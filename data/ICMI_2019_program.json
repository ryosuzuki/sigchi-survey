{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10038,
    "startDate": 1571011200000,
    "endDate": 1571356800000,
    "shortName": "ICMI",
    "name": "ICMI 2019",
    "year": 2019,
    "fullName": "21st ACM International Conference on Multimodal Interaction",
    "url": "https://icmi.acm.org/2019",
    "location": "Suzhou, Jiangsu, China",
    "timeZoneOffset": 480,
    "logoUrl": "https://files.sigchi.org/conference/logo/296feceb-3379-ab98-cc36-57a114a8c969.png",
    "timeZoneName": "Asia/Shanghai"
  },
  "sponsors": [
    {
      "id": 10076,
      "name": "Openstream",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/046f16eb-8346-4a7d-a6ea-07daf62db4aa.png",
      "levelId": 10057,
      "order": 1
    },
    {
      "id": 10077,
      "name": "Microsoft",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/59888e95-1636-9145-5635-dc7da3dea0cd.png",
      "levelId": 10058,
      "order": 3
    },
    {
      "id": 10078,
      "name": "Alibaba Group",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0fa46602-d136-4568-90f7-6b0a387e1e55.png",
      "levelId": 10058,
      "order": 2
    },
    {
      "id": 10079,
      "name": "AISpeech",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/ff7d32a1-185e-9b5d-aeb4-5d0db4e28afe.png",
      "levelId": 10059,
      "order": 4
    },
    {
      "id": 10080,
      "name": "SenseTime",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/d797e5fe-a8c3-4982-144f-965d055d959c.png",
      "levelId": 10059,
      "order": 6
    },
    {
      "id": 10081,
      "name": "Baidu",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/59dbf65d-60ae-b32a-a967-cb6eea330128.png",
      "levelId": 10059,
      "order": 5
    },
    {
      "id": 10082,
      "name": "Tencent YouTu Lab",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/bd9faded-2ed1-a2e2-0525-ebd3f0478900.png",
      "levelId": 10059,
      "order": 7
    },
    {
      "id": 10083,
      "name": "NSFC",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/3216aaf3-da3c-2e9e-fa81-431abebab233.png",
      "levelId": 10060,
      "order": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10057,
      "name": "Platinum",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10058,
      "name": "Gold",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10059,
      "name": "Silver",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10060,
      "name": "General Support",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10051,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10079,
      "name": "Ground Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/9a32615b-607c-ba9d-9d12-20522094bbae.png",
      "roomIds": [
        10301,
        10305,
        10304,
        10303,
        10321,
        10320,
        10325,
        10326,
        10322,
        10323
      ]
    }
  ],
  "rooms": [
    {
      "id": 10301,
      "name": "E3",
      "typeId": 11458,
      "setup": "Special"
    },
    {
      "id": 10305,
      "name": "M1",
      "typeId": 11465,
      "setup": "Classroom"
    },
    {
      "id": 10304,
      "name": "M2",
      "typeId": 11465,
      "setup": "Classroom"
    },
    {
      "id": 10303,
      "name": "M3",
      "typeId": 11465,
      "setup": "Classroom"
    },
    {
      "id": 10321,
      "name": "E3 Foyer",
      "typeId": 11462,
      "setup": "Theatre"
    },
    {
      "id": 10320,
      "name": "GB Foyer",
      "typeId": 11462,
      "setup": "Special"
    },
    {
      "id": 10325,
      "name": "Grand Ballroom A (GA)",
      "typeId": 11462,
      "setup": "Theatre"
    },
    {
      "id": 10326,
      "name": "Grand Ballroom B (GB)",
      "typeId": 11462,
      "setup": "Theatre"
    },
    {
      "id": 10322,
      "name": "M3 Foyer",
      "typeId": 11462,
      "setup": "Theatre"
    },
    {
      "id": 10323,
      "name": "Multi-Function Room 1",
      "typeId": 11462,
      "setup": "Theatre"
    }
  ],
  "tracks": [
    {
      "id": 10681,
      "name": "ICMI 2019 Late Breaking Results",
      "typeId": 11482
    },
    {
      "id": 10658,
      "name": "ICMI 2019 Demonstrations",
      "typeId": 11478
    },
    {
      "id": 10659,
      "name": "ICMI 2019 Doctoral Consortium",
      "typeId": 11479
    },
    {
      "id": 10657,
      "name": "ICMI 2019 Chinese Audio-Textual SLU Challenge",
      "typeId": 11480
    },
    {
      "id": 10677,
      "typeId": 11480
    },
    {
      "id": 10680,
      "typeId": 11465
    },
    {
      "id": 10678,
      "typeId": 11465
    },
    {
      "id": 10683,
      "typeId": 11479
    },
    {
      "id": 10682,
      "typeId": 11480
    },
    {
      "id": 10656,
      "name": "ICMI 2019 Long and Short Papers",
      "typeId": 11462
    }
  ],
  "contentTypes": [
    {
      "id": 11482,
      "name": "Late Breaking Results",
      "color": "#969696",
      "duration": 0
    },
    {
      "id": 11456,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11457,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11458,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11459,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 11460,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11461,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 11462,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 11463,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 11464,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11465,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 20,
      "displayName": "Workshops"
    },
    {
      "id": 11478,
      "name": "Demo & Exhibits",
      "color": "#969696",
      "duration": 60
    },
    {
      "id": 11479,
      "name": "Doctoral Consortium",
      "color": "#969696",
      "duration": 60
    },
    {
      "id": 11480,
      "name": "Grand Challenge",
      "color": "#969696",
      "duration": 20
    }
  ],
  "timeSlots": [
    {
      "id": 11260,
      "type": "SESSION",
      "startDate": 1571043600000,
      "endDate": 1571054400000
    },
    {
      "id": 11261,
      "type": "SESSION",
      "startDate": 1571043600000,
      "endDate": 1571055300000
    },
    {
      "id": 11262,
      "type": "SESSION",
      "startDate": 1571043600000,
      "endDate": 1571056200000
    },
    {
      "id": 11264,
      "type": "LUNCH",
      "startDate": 1571056200000,
      "endDate": 1571061600000
    },
    {
      "id": 11265,
      "type": "SESSION",
      "startDate": 1571061600000,
      "endDate": 1571072400000
    },
    {
      "id": 11266,
      "type": "SESSION",
      "startDate": 1571061600000,
      "endDate": 1571072700000
    },
    {
      "id": 11267,
      "type": "SESSION",
      "startDate": 1571061600000,
      "endDate": 1571073900000
    },
    {
      "id": 11268,
      "type": "BREAK",
      "startDate": 1571066100000,
      "endDate": 1571067900000
    },
    {
      "id": 11269,
      "type": "SESSION",
      "startDate": 1571076000000,
      "endDate": 1571083200000
    },
    {
      "id": 11263,
      "type": "BREAK",
      "startDate": 1571048100000,
      "endDate": 1571049900000
    },
    {
      "id": 11271,
      "type": "BREAK",
      "startDate": 1571134500000,
      "endDate": 1571136300000
    },
    {
      "id": 11272,
      "type": "LUNCH",
      "startDate": 1571143200000,
      "endDate": 1571148000000
    },
    {
      "id": 11273,
      "type": "SESSION",
      "startDate": 1571148000000,
      "endDate": 1571153700000
    },
    {
      "id": 11274,
      "type": "SESSION",
      "startDate": 1571153700000,
      "endDate": 1571160600000
    },
    {
      "id": 11275,
      "type": "SESSION",
      "startDate": 1571160600000,
      "endDate": 1571164200000
    },
    {
      "id": 11278,
      "type": "BREAK",
      "startDate": 1571220000000,
      "endDate": 1571221800000
    },
    {
      "id": 11280,
      "type": "SESSION",
      "startDate": 1571234400000,
      "endDate": 1571239200000
    },
    {
      "id": 11281,
      "type": "SESSION",
      "startDate": 1571239200000,
      "endDate": 1571246400000
    },
    {
      "id": 11282,
      "type": "BREAK",
      "startDate": 1571239200000,
      "endDate": 1571246400000
    },
    {
      "id": 11283,
      "type": "SESSION",
      "startDate": 1571246400000,
      "endDate": 1571250000000
    },
    {
      "id": 11285,
      "type": "BREAK",
      "startDate": 1571306400000,
      "endDate": 1571308200000
    },
    {
      "id": 11286,
      "type": "LUNCH",
      "startDate": 1571314800000,
      "endDate": 1571320800000
    },
    {
      "id": 11287,
      "type": "SESSION",
      "startDate": 1571320800000,
      "endDate": 1571326200000
    },
    {
      "id": 11289,
      "type": "BREAK",
      "startDate": 1571326200000,
      "endDate": 1571328000000
    },
    {
      "id": 11270,
      "type": "SESSION",
      "startDate": 1571136300000,
      "endDate": 1571143200000
    },
    {
      "id": 11276,
      "type": "BREAK",
      "startDate": 1571153700000,
      "endDate": 1571160600000
    },
    {
      "id": 11291,
      "type": "SESSION",
      "startDate": 1571216400000,
      "endDate": 1571220000000
    },
    {
      "id": 11277,
      "type": "SESSION",
      "startDate": 1571221800000,
      "endDate": 1571227800000
    },
    {
      "id": 11279,
      "type": "LUNCH",
      "startDate": 1571227800000,
      "endDate": 1571234400000
    },
    {
      "id": 11292,
      "type": "SESSION",
      "startDate": 1571302800000,
      "endDate": 1571306400000
    },
    {
      "id": 11284,
      "type": "SESSION",
      "startDate": 1571308200000,
      "endDate": 1571314800000
    },
    {
      "id": 11293,
      "type": "SESSION",
      "startDate": 1571328000000,
      "endDate": 1571328900000
    },
    {
      "id": 11294,
      "type": "SESSION",
      "startDate": 1571328900000,
      "endDate": 1571331600000
    },
    {
      "id": 11295,
      "type": "SESSION",
      "startDate": 1571331600000,
      "endDate": 1571332500000
    },
    {
      "id": 11296,
      "type": "SESSION",
      "startDate": 1571389200000,
      "endDate": 1571401800000
    },
    {
      "id": 11297,
      "type": "BREAK",
      "startDate": 1571392800000,
      "endDate": 1571394600000
    },
    {
      "id": 11298,
      "type": "SESSION",
      "startDate": 1571389200000,
      "endDate": 1571392800000
    },
    {
      "id": 11299,
      "type": "SESSION",
      "startDate": 1571394600000,
      "endDate": 1571401800000
    },
    {
      "id": 11290,
      "type": "SESSION",
      "startDate": 1571130000000,
      "endDate": 1571130900000
    },
    {
      "id": 11309,
      "type": "SESSION",
      "startDate": 1571130900000,
      "endDate": 1571134500000
    }
  ],
  "sessions": [
    {
      "id": 1540,
      "name": "Doctoral Consortium PM",
      "typeId": 11479,
      "roomId": 10303,
      "chairIds": [
        16333
      ],
      "contentIds": [
        24682,
        4814,
        4030,
        3546,
        4568
      ],
      "timeSlotId": 11267
    },
    {
      "id": 2403,
      "name": "Demo & Exhibit",
      "typeId": 11478,
      "roomId": 10301,
      "chairIds": [
        23613
      ],
      "contentIds": [
        3824,
        4746,
        6006,
        3527,
        3360,
        3531,
        6430
      ],
      "timeSlotId": 11281
    },
    {
      "id": 24569,
      "name": "The 1st Mandarin Audio-Visual Speech Recognition Challenge",
      "typeId": 11480,
      "roomId": 10303,
      "chairIds": [
        24733
      ],
      "contentIds": [
        24506,
        24508
      ],
      "timeSlotId": 11298
    },
    {
      "id": 2378,
      "name": "Session 5: Sound and interaction",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        18478
      ],
      "contentIds": [
        5238,
        4333,
        4620,
        6379,
        4679,
        7140
      ],
      "timeSlotId": 11284
    },
    {
      "id": 1562,
      "name": "Poster Session 1",
      "typeId": 11462,
      "roomId": 10301,
      "chairIds": [
        24734
      ],
      "contentIds": [
        6133,
        3473,
        4302,
        6740,
        5093,
        3379,
        6511,
        3541,
        4811,
        4473
      ],
      "timeSlotId": 11274
    },
    {
      "id": 1926,
      "name": "The 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU)",
      "typeId": 11480,
      "roomId": 10303,
      "chairIds": [
        11198,
        10428
      ],
      "contentIds": [
        6433,
        5255,
        4349,
        6382,
        3681
      ],
      "timeSlotId": 11299
    },
    {
      "id": 1882,
      "name": "Session 1: Human Behavior  ",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        22204
      ],
      "contentIds": [
        3731,
        5780,
        8102,
        7203,
        7848,
        8100
      ],
      "timeSlotId": 11270
    },
    {
      "id": 24528,
      "name": "NeuroManagement and Intelligent Computing Method on Multimodal Interaction",
      "typeId": 11465,
      "roomId": 10305,
      "chairIds": [
        24427
      ],
      "contentIds": [
        24521,
        24520,
        24524,
        24523,
        24516,
        24514,
        24519
      ],
      "timeSlotId": 11261
    },
    {
      "id": 24670,
      "name": "Late Breaking Results",
      "typeId": 11482,
      "roomId": 10301,
      "chairIds": [
        24732
      ],
      "contentIds": [
        24659,
        24662,
        24667,
        24668,
        24669,
        24657,
        24661,
        24663,
        24665,
        24666,
        24660,
        24664,
        24655,
        24656,
        24658
      ],
      "timeSlotId": 11274
    },
    {
      "id": 1857,
      "name": "Session 2: Artificial Agents",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        20452
      ],
      "contentIds": [
        2967,
        4904,
        2774,
        7069,
        3257
      ],
      "timeSlotId": 11273
    },
    {
      "id": 1503,
      "name": "Session 6: Multiparty interaction",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        22954
      ],
      "contentIds": [
        7666,
        6670,
        3600,
        4610,
        5964
      ],
      "timeSlotId": 11287
    },
    {
      "id": 1597,
      "name": "Poster Session 2",
      "typeId": 11462,
      "roomId": 10301,
      "chairIds": [
        24734
      ],
      "contentIds": [
        7331,
        7549,
        7405,
        6207,
        3529,
        4798,
        3488,
        7477,
        5151
      ],
      "timeSlotId": 11281
    },
    {
      "id": 1606,
      "name": "Doctoral Consortium AM",
      "typeId": 11479,
      "roomId": 10303,
      "chairIds": [
        16333
      ],
      "contentIds": [
        24681,
        4965,
        5469,
        5653,
        5189,
        3311
      ],
      "timeSlotId": 11262
    },
    {
      "id": 24530,
      "name": "The 7th Emotion Recognition in the Wild Challenge (EmotiW)",
      "typeId": 11480,
      "roomId": 10305,
      "chairIds": [
        23613
      ],
      "contentIds": [
        24511,
        24512,
        24509,
        24510,
        24517,
        24518,
        24513,
        24515,
        24505,
        24507
      ],
      "timeSlotId": 11296
    },
    {
      "id": 24680,
      "name": "Doctoral Spotlight Posters",
      "typeId": 11479,
      "roomId": 10301,
      "chairIds": [
        16333
      ],
      "contentIds": [
        24678,
        24676,
        24677,
        24675,
        24671,
        24674,
        24679,
        24672,
        24673
      ],
      "timeSlotId": 11274
    },
    {
      "id": 2048,
      "name": "Session 3: Touch and Gesture",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        23613
      ],
      "contentIds": [
        2915,
        5632,
        5886,
        6222,
        3794
      ],
      "timeSlotId": 11277
    },
    {
      "id": 24531,
      "name": "Media Analytics for Societal Trends: Closing the loop with impact and affect in human-media interactions",
      "typeId": 11465,
      "roomId": 10305,
      "chairIds": [
        24731
      ],
      "contentIds": [
        24525,
        24574,
        24573,
        24527,
        24526
      ],
      "timeSlotId": 11266
    },
    {
      "id": 2470,
      "name": "Session 4: Physiological Modeling  ",
      "typeId": 11462,
      "roomId": 10326,
      "chairIds": [
        16333
      ],
      "contentIds": [
        3822,
        5507,
        6918,
        5696
      ],
      "timeSlotId": 11280
    },
    {
      "id": 24741,
      "name": "Grand Challenge Posters",
      "typeId": 11480,
      "roomId": 10301,
      "chairIds": [
        23613
      ],
      "contentIds": [
        24743,
        24742,
        24745,
        24744,
        24746,
        24747,
        24748,
        24749,
        24750,
        24751,
        24752,
        24753,
        24754,
        24755,
        24756,
        24757,
        24758
      ],
      "timeSlotId": 11281
    }
  ],
  "events": [
    {
      "id": 2624,
      "name": "Lunch",
      "typeId": 11458,
      "roomId": 10325,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571143200000,
      "endDate": 1571148000000,
      "presenterIds": []
    },
    {
      "id": 2649,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10321,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571239200000,
      "endDate": 1571246400000,
      "presenterIds": []
    },
    {
      "id": 2549,
      "name": "Lunch",
      "typeId": 11458,
      "roomId": 10325,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571227800000,
      "endDate": 1571234400000,
      "presenterIds": []
    },
    {
      "id": 2605,
      "name": "Keynote 3: Challenges of Multimodal Interaction in the Era of Human-Robot Coexistence",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [
        24330
      ],
      "contentIds": [],
      "startDate": 1571302800000,
      "endDate": 1571306400000,
      "description": "With the rapid progress in computing and sensory technologies, we will enter the era of human-robot coexistence in the not-too-distant future, and it is time to address the challenges of multimodal interaction. Should a robot take the form of humanoid? Is it better for robots to behave as a second-class citizen or as an equal part of the society as human? Should the communication between human and robot be symmetric or is it okay to be asymmetric? And how about the communication between robots with human presence? What does it mean by emotional intelligence for robots? With the inevitable physical interaction between human and robot, how to guarantee safety? What is the ethical and moral model for robots and how do they follow?",
      "presenterIds": [
        24740
      ]
    },
    {
      "id": 2640,
      "name": "Lunch",
      "typeId": 11458,
      "roomId": 10325,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571314800000,
      "endDate": 1571320800000,
      "presenterIds": []
    },
    {
      "id": 2602,
      "name": "Closing of ICMI 2019 & Introduction of ICMI 2020",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571331600000,
      "endDate": 1571332500000,
      "presenterIds": [
        24684,
        24735
      ]
    },
    {
      "id": 24576,
      "name": "Lunch",
      "typeId": 11458,
      "roomId": 10323,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571056200000,
      "endDate": 1571061600000,
      "presenterIds": []
    },
    {
      "id": 24575,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10322,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571048100000,
      "endDate": 1571049900000,
      "presenterIds": []
    },
    {
      "id": 2574,
      "name": "Reception",
      "typeId": 11458,
      "roomId": 10323,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571076000000,
      "endDate": 1571083200000,
      "presenterIds": []
    },
    {
      "id": 2603,
      "name": "Tour groups 1 & 2",
      "typeId": 11458,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571405400000,
      "endDate": 1571405400000,
      "presenterIds": []
    },
    {
      "id": 2585,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10320,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571306400000,
      "endDate": 1571308200000,
      "presenterIds": []
    },
    {
      "id": 2657,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10320,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571326200000,
      "endDate": 1571328000000,
      "presenterIds": []
    },
    {
      "id": 2565,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10320,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571220000000,
      "endDate": 1571221800000,
      "presenterIds": []
    },
    {
      "id": 2599,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10321,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571153700000,
      "endDate": 1571160600000,
      "presenterIds": []
    },
    {
      "id": 2595,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10320,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571134500000,
      "endDate": 1571136300000,
      "presenterIds": []
    },
    {
      "id": 2561,
      "name": "Opening Session",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571130000000,
      "endDate": 1571130900000,
      "presenterIds": [
        24686,
        24330
      ]
    },
    {
      "id": 2653,
      "name": "Keynote 2: A Brief History of Intelligence",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [
        14933
      ],
      "contentIds": [],
      "startDate": 1571216400000,
      "endDate": 1571220000000,
      "description": "Intelligence is the deciding factor of how human beings become the most dominant life forms on earth. Throughout history, human beings have developed tools and technologies which help civilizations evolve and grow. Computers, and by extension, artificial intelligence (AI), has played important roles in that continuum of technologies. Recently artificial intelligence has garnered much interest and discussion. As artificial intelligence are tools that can enhance human capability, a sound understanding of what the technology can and cannot do is also necessary to ensure their appropriate use. While developing artificial intelligence, we also found out the definition and understanding of our own human intelligence continue evolving. The debates of the race between human and artificial intelligence have been ever growing. In this talk, I will describe the history of both artificial intelligence and human intelligence (HI). From the great insights of the such historical perspectives, [...]",
      "presenterIds": [
        24739
      ]
    },
    {
      "id": 2579,
      "name": "Tutorial 1: Spoken Dialogue Processing for Multimodal Human-Robot Interaction",
      "typeId": 11458,
      "roomId": 10304,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571043600000,
      "endDate": 1571054400000,
      "description": "The tutorial first addresses the desirable tasks and interactions conducted by humanoid robots engaged in spoken dialogue. These obviously depend on the character design of the robots, and I will focus on long and deep interactions such as counseling and interview, which have a definite task but do not have observable goals. They will expand the potential of communicative robots. The second part of the tutorial will focus on the methodology and technical aspects of spoken dialogue processing including speech recognition and synthesis for human-robot interaction. A comprehensive review on spoken language understanding and dialogue management is provided. Then, non-verbal processing is also addressed. In particular, smooth turn-taking and real-time feedback including backchannels are critically important for keeping the user engaged in the dialogue, so the interaction will be duplex consisting of not only speaking but also attentive listening.",
      "presenterIds": [
        10175
      ]
    },
    {
      "id": 2622,
      "name": "Banquet at Songhelou",
      "typeId": 11458,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571252400000,
      "endDate": 1571259600000,
      "location": "Songhelou",
      "description": "Bus leaves conference venue at 6:45 PM",
      "presenterIds": []
    },
    {
      "id": 2606,
      "name": "Keynote 1: Socially-Aware User Interfaces: Can Genuine Sensitivity Be Learnt at all?",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [
        24686
      ],
      "contentIds": [],
      "startDate": 1571130900000,
      "endDate": 1571134500000,
      "description": "Recent years have initiated a paradigm shift from pure task-based human-machine interfaces towards socially-aware interaction. Advances in deep learning have led to anthropomorphic interfaces with robust sensing capabilities that come close to or even exceed human performance. In some cases, these interfaces may convey to humans the illusion of a sentient being that cares for them. At the same time, there is the risk that - at some point - these systems may have to reveal their lack of true comprehension of the situative context and the user’s needs with serious consequences to user trust. The talk will discuss challenges that arise when designing multimodal interfaces that hide the underlying complexity from the user, but still demonstrate a transparent and plausible behavior. It will argue for hybrid AI approaches that look beyond deep learning to encompass a theory of mind to obtain a better understanding of the rationale behind human behaviors.",
      "presenterIds": [
        16788
      ]
    },
    {
      "id": 2555,
      "name": "Sustained Achievement Awardee Talk: Connecting Humans with Humans: Multimodal, Multilingual, Multiparty Mediation",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [
        24736
      ],
      "contentIds": [],
      "startDate": 1571246400000,
      "endDate": 1571250000000,
      "description": "Behind much of my research work over 4 decades has been the simple observation that people like people and love interacting with other people more than they like interacting with machines. Technologies that truly support such social desires are more likely to be adopted broadly. Consider email, texting, chat rooms, social media, video conferencing, the internet, speech translation, even videogames with a social element (e.g., Fortnite): we enjoy the technology whenever it brings us closer to our fellow humans, instead of imposing attention-grabbing clutter. If so, how then can we build better technologies that improve, encourage, support human- human interaction? ",
      "presenterIds": [
        24738
      ]
    },
    {
      "id": 2592,
      "name": "Tutorial 2: Getting Virtually Personal: Power Conversational AI to Fulfill Tasks and Personalize Chitchat for Real-World Applications",
      "typeId": 11458,
      "roomId": 10304,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571061600000,
      "endDate": 1571072400000,
      "description": "This tutorial will first review the state-of-art approaches to conversational AI and challenges for building conversational AI for real-world applications. It will then introduce a model-based framework that combines symbolic approaches and deep learning to support conversational AI for fulfilling tasks and personalizing chitchat. The tutorial will also teach conversational AI design patterns and discuss auto-evaluation of conversational AI. This will be a fun, hands-on tutorial during which participants will design, build, test, and deploy their own conversational AI to achieve certain tasks and chitchat. This tutorial is especially suitable for those who are interested in building conversational AI for real-world applications or exploring complex applications of conversational AI (e.g., AI companions, AI caretakers, and AI interviewers).",
      "presenterIds": []
    },
    {
      "id": 24578,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10322,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571392800000,
      "endDate": 1571394600000,
      "presenterIds": []
    },
    {
      "id": 24577,
      "name": "Coffee Break",
      "typeId": 11458,
      "roomId": 10322,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571066100000,
      "endDate": 1571067900000,
      "presenterIds": []
    },
    {
      "id": 2629,
      "name": "Panel Discussion",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [
        24686
      ],
      "contentIds": [],
      "startDate": 1571160600000,
      "endDate": 1571164200000,
      "presenterIds": [
        22204,
        22954,
        22500,
        24759
      ]
    },
    {
      "id": 2672,
      "name": "Grand Challenge Overview",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571328000000,
      "endDate": 1571328900000,
      "presenterIds": [
        23613
      ]
    },
    {
      "id": 2674,
      "name": "Town Hall Meeting",
      "typeId": 11458,
      "roomId": 10326,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1571328900000,
      "endDate": 1571331600000,
      "presenterIds": [
        22500
      ]
    }
  ],
  "contents": [
    {
      "id": 5632,
      "typeId": 11462,
      "title": "VisualTouch: Enhancing Affective Touch Communication with Multi-modality Stimulation",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "As one of the most important non-verbal communication channel, touch plays an essential role in interpersonal affective communication. Although some researchers have started exploring the possibility of using wearable devices for conveying emotional information, most of the existing devices still lack the capability to support affective and dynamic touch in interaction. In this paper, we explore the effect of dynamic visual cues on the emotional perception of vibrotactile signals. For this purpose, we developed VisualTouch, a haptic sleeve consisting of a haptic layer and a visual layer.\r\nWe hypothesized that visual cues would enhance the interpretation of tactile cues when both types of cues are congruent. We first carried out an experiment and selected 4 stimuli producing substantially different responses. Based on that, a second experiment was conducted with 12 participants rating the valence and arousal of 36 stimuli using SAM scales.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Telecom Paristech",
              "dsl": ""
            }
          ],
          "personId": 18498
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Télécom-Paristech",
              "dsl": "I3"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Télécom-Paristech",
              "dsl": "I3"
            }
          ],
          "personId": 20563
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris Saclay",
              "dsl": "LTCI, Télécom ParisTech"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris Saclay",
              "dsl": "LTCI, Télécom ParisTech"
            }
          ],
          "personId": 20245
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS - telecom ParisTech",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS - telecom ParisTech",
              "dsl": ""
            }
          ],
          "personId": 8457
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Télécom ParisTech",
              "dsl": "I3"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Télécom ParisTech",
              "dsl": "I3"
            }
          ],
          "personId": 13020
        }
      ],
      "sessionIds": [
        2048
      ],
      "eventIds": []
    },
    {
      "id": 4610,
      "typeId": 11462,
      "title": "Emergent Leadership Detection Across Datasets",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Automatic detection of emergent leaders in small groups from nonverbal behaviour is a growing research topic in social signal processing but existing methods were evaluated on single datasets - an unrealistic assumption for real-world applications in which systems are required to also work in settings unseen at training time. It therefore remains unclear whether current methods for emergent leadership detection generalise to similar but new settings and to which extent. To overcome this limitation, we are the first to study a cross-dataset evaluation setting for the emergent leadership detection task. We provide evaluations for within- and cross-dataset prediction using two current datasets (PAVIS and MPIIGroupInteraction), as well as an investigation on the robustness of commonly used feature channels and online prediction in the cross-dataset setting. Our evaluations show that using pose and eye contact based features, cross-dataset prediction is possible with an accuracy of 0.68, as such providing another important piece of the puzzle towards real-world emergent leadership detection.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "Max Planck Institute for Informatics"
            }
          ],
          "personId": 11892
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Institute for Visualisation and Interactive Systems"
            }
          ],
          "personId": 15266
        }
      ],
      "sessionIds": [
        1503
      ],
      "eventIds": []
    },
    {
      "id": 6918,
      "typeId": 11462,
      "title": "Controlling for Confounders in Multimodal Emotion Classification via Adversarial Learning",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Various psychological factors affect how individuals express emotions. Yet, when we collect data intended for use in building emotion recognition systems, we often try to do so by creating paradigms that are designed just with a focus on eliciting emotional behavior. Algorithms trained with these types of data are unlikely to function outside of controlled environments because our emotions naturally change as a function of these other factors. In this work, we study how the multimodal expressions of emotion change when an individual is under varying levels of stress. We hypothesize that stress produces modulations that can hide the true underlying emotions of individuals and that we can make emotion recognition algorithms more generalizable by controlling for variations in stress. To this end, we use adversarial networks to decorrelate stress modulations from emotion representations. We study how stress alters acoustic and lexical emotional predictions, paying special attention to how modulations due to stress affect the transferability of learned emotion recognition models across domains. Our results show that stress is indeed encoded in trained emotion classifiers and that this encoding varies across levels of emotions and across the lexical and acoustic modalities. Our results also show that emotion recognition models that control for stress during training have better generalizability when applied to new domains, compared to models that do not control for stress during training. We conclude that is is necessary to consider the effect of extraneous psychological factors when building and testing emotion recognition models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 16711
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 23072
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 15705
        }
      ],
      "sessionIds": [
        2470
      ],
      "eventIds": []
    },
    {
      "id": 4620,
      "typeId": 11462,
      "title": "Evidence for communicative compensation in debt advice with reduced multimodality ",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Evidence exists that shows professional advice accompanied by empathy displays and signals of active listening lead to more successful outcomes (Zolnierek & DiMatteo, 2009). These communicative skills are typically displayed through visual and paralinguistic multimodal nonverbal signals; in situations where there is reduced opportunity for multimodal communication alternative strategies are required. In particular, when visual cues are not present there is a need to compensate for the lack of visual nonverbal information. Providing debt advice is often a highly emotional communicative scenario. The current study aims to compare explicit emotional content (as expressed verbally) and implicit emotional content (as expressed through paralinguistic cues) in face to face (FTF) and telephone debt advice recordings. Twenty-two debt advice recordings were coded to differentiate emotional or functional sections of the conversation, and then processed using the emotional dimensions from the OpenSMILE speech feature extraction software. The analysis found that FTF recordings included more explicit emotion than telephone recordings did. However, linear mixed effects modelling found substantially higher levels of arousal and slightly lower levels of valence in telephone advice. Interaction analyses found that emotional speech in FTF advice was characterised by lower levels of arousal than during functional speech, whereas emotional speech in telephone advice had higher levels of arousal than in functional speech. We conclude that there are differences in emotional content when comparing full and reduced multimodal debt advice. Furthermore, as telephone advice cannot avail of visual nonverbal signals, it seems to compensate using stronger nonverbal paralinguistic cues present in the voice. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Belfast",
              "institution": "Queen's University Belfast",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 15267
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Belfast",
              "institution": "Queen's University Belfast",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 13676
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Belfast",
              "institution": "Queen's University Belfast",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 20505
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 6670,
      "typeId": 11462,
      "title": "Smile and Laugh Dynamics in Naturalistic Dyadic Interactions: Arousal Levels, Sequences and Roles",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Smiles and laughs have been the subject of many studies over the past decades,\r\ndue to their frequent occurrence in interactions, as well as their social and\r\nemotional functions in dyadic conversations. In this paper we push forward\r\nprevious work by providing a first study on the influence one interacting\r\npartner's smiles and laughs have on their interlocutor's, taking into account\r\nthe arousal levels of expressions. Our second contribution is a study on the\r\npatterns of laugh and smile sequences during the dialogs, again taking the\r\narousal level into account. Finally, we discuss the effect of the interlocutor's\r\nrole on smiling and laughing. In order to achieve this, we use a database of\r\nnaturalistic dyadic conversations which was collected and annotated for the\r\npurpose of this study. The details of the collection and annotation are also\r\nreported here to enable reproduction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "numediart",
              "dsl": "University of Mons"
            }
          ],
          "personId": 11376
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Electrical Engineering"
            }
          ],
          "personId": 8759
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Glendale",
              "institution": "Disney Research Los Angeles",
              "dsl": ""
            }
          ],
          "personId": 17075
        }
      ],
      "sessionIds": [
        1503
      ],
      "eventIds": []
    },
    {
      "id": 3600,
      "typeId": 11462,
      "title": "Task-independent Multimodal Prediction of Group Performance Based on  Product Dimensions",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Computational analysis to predict group performance\r\nhas attracted attention in the multimodal interaction domain.\r\nThough many previous studies focus on group performance by modeling on one problem solving task, this paper proposes an approach to develop models for predicting the performance for multiple group meeting tasks, such as business meetings, where the model has no clear correct answer. This paper adopts \"product dimensions\" [Hackman 1967 et al.] (PD) which is proposed as a set of dimensions for describing the general properties of written passages that are generated by a group,\r\nas a metric measuring group output. \r\nThis study enhanced the group discussion corpus called the MATRICS corpus including multiple discussion sessions by annotating the performance metric of PD. We extract group-level linguistic features including vocabulary level features using a word embedding technique, topic segmentation techniques, and functional features with dialog act and parts of speech on the word level. We also extracted nonverbal features from the speech turn, prosody, and head movement.\r\nWith a corpus including multiple discussion data and an annotation of the group performance,\r\nwe conduct two types of experiments thorough classification and regression modeling to predict the PD.\r\nThe first experiment is to evaluate the task-dependent prediction accuracy, in the situation that the samples obtained from the same discussion task are included in both the training and testing.\r\nThe second experiments is to evaluate the task-independent prediction accuracy, in the situation that \r\nthe type of discussion task is different between the training samples and testing samples.\r\nIn this situation, regression models are developed to infer the performance in an unknown discussion task. The experimental results show that a support vector regression model archived a 0.76 correlation in the discussion-task-dependent setting and 0.55 in the task-independent setting. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 9334
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 18478
        }
      ],
      "sessionIds": [
        1503
      ],
      "eventIds": []
    },
    {
      "id": 5653,
      "typeId": 11479,
      "title": "Tailoring Motion Recognition Systems to Children’s Motions",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Motion-based applications are becoming increasingly popular among children and require accurate motion recognition to ensure meaningful interactive experiences. However, motion recognizers are usually trained on adults’ motions. Children and adults differ in terms of their body proportions and stages of development of their neuromuscular systems, so children and adults will likely perform motions differently. Therefore, motion recognizers tailored to adults will likely perform poorly for children. My PhD thesis will focus on identifying features that characterize children’s and adults’ motions. This set of features will provide a model that can be used to understand children’s natural motion qualities and will serve as the first step in tailoring recognizers to children’s motions. This paper describes my past and ongoing work toward this end and outlines the next steps in my PhD work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "CISE department"
            }
          ],
          "personId": 17564
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 6430,
      "typeId": 11478,
      "title": "A Proxemics Measurement Tool Integrated into VAIF and Unity",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "When people interact with an embodied conversational agent (ECA), although an agent may have a range of animations, the agent usually remains fixed within the environment. However, if the agent does move, it may do so in a way that does not consider the user, adding a sense of unnaturalness to the user's experience. Implementing proxemics measurements (the distance between conversants) will help further research on how users are being influenced in a free-movement virtual world while interacting with ECAs. Knowing how proxemics affect human-ECA interaction will enable the prediction of the distance taken by the human towards the ECA, and on which factors this depends. This will then help create an adaptable ECA that can react properly to the user's movement in a virtual reality setting while providing greater naturalness and engagement. The Market Scene, where the user is placed in a market with other agents and walks up to ECAs to have conversations, is an example of an application where proxemics matters. But without support for measuring human-ECA proxemics, it is difficult to advance the study of human-ECA proxemics. Accordingly, this paper presents a tool for proxemics, integrated in VAIF that enables automatic measurement and recording of human-ECA proxemics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "El Paso",
              "institution": "The University of Texas at El Paso",
              "dsl": "Interactive Systems Group"
            }
          ],
          "personId": 14083
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "El Paso",
              "institution": "The University of Texas at El Paso",
              "dsl": "Interactive Systems Group"
            }
          ],
          "personId": 9553
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "El Paso",
              "institution": "The University of Texas at El Paso",
              "dsl": "Interactive Systems Group"
            }
          ],
          "personId": 18485
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "El Paso",
              "institution": "University of Texas at El Paso",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9868
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "El Paso",
              "institution": "The University of Texas at El Paso",
              "dsl": "Interactive Systems Group"
            }
          ],
          "personId": 11623
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 5151,
      "typeId": 11462,
      "title": "DeepReviewer: Collaborative Grammar and Innovation Neural Network for Automatic Paper Review",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, there are more and more papers submitted to various periodicals and conferences. Typically, reviewers need to read through the paper and give a review comment and score to it based on somehow certain criterion. This review process is labor intensive and time-consuming. Recently, AI technology is widely used to alleviate human labor burden. Can machine learn from human to review papers automatically? In this paper, we propose a collaborative grammar and innovation model - DeepReviewer to achieve automatic paper review. This model learning the semantic, grammar and innovative features of an article by three main well-designed components simultaneously. Moreover, these three factors are integrated by an attention layer to get the final review score of the paper. We crawled paper review data from Openreview and built a real data set. Experimental results demonstrate that our model exceeds many baselines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "BeiJing",
              "institution": "Renmin University of China",
              "dsl": "School of Information"
            }
          ],
          "personId": 8208
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Renmin University of China",
              "dsl": "School of Information"
            }
          ],
          "personId": 23731
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Renmin University of China",
              "dsl": "School of Information"
            }
          ],
          "personId": 9482
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 3360,
      "typeId": 11478,
      "title": "Seeing is believing but feeling is the truth",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "Ultrasound is beyond the range of human hearing and tactile perception. In the past few years, several modulation techniques have been invented to overcome this and evoke perceptible tactile sensations of shapes and textures that can be felt, but not seen. Therefore, mid-air haptic technology has found use in several human computer interaction applications and is the focus of multiple research efforts. Visualising the induced acoustic pressure field can help understand and optimise how different modulation techniques translate into tactile sensations. Here, rather than using acoustic simulation tools to do that, we exploit the micro-displacement of a thin layer of oil to visualize the impinging acoustic pressure field outputted from an ultrasonic phased array device. Our demo uses a light source to illuminate the oil displacement and project it onto a screen to produce an interactive lightbox display. Interaction is facilitated via optical hand-tracking technology thus enabling an instantaneous and aesthetically pleasing visualisation of mid-air haptics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Gloucestershire",
              "city": "BRISTOL",
              "institution": "ULTRAHAPTICS LTD",
              "dsl": ""
            }
          ],
          "personId": 22598
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Ultrahaptics",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bournemouth",
              "institution": "Bournemouth University",
              "dsl": ""
            }
          ],
          "personId": 18375
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Ultrahaptics",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "Mathematics"
            }
          ],
          "personId": 16042
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 6433,
      "typeId": 11480,
      "title": "CATSLU: The 1st Chinese Audio-Textual Spoken Language Understanding Challenge",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "Spoken language understanding (SLU) is a key component of conversational dialogue systems, which converts user utterances into semantic representations. The previous works almost focus on parsing semantic from textual inputs (top hypothesis of speech recognition and even manual transcripts) while losing information hidden in the audio. We herein describe the \\textsc{1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU)} which introduces a new dataset with audio-textual information, multiple domains and domain knowledge. We introduce two scenarios of audio-textual SLU in which participants are encouraged to utilize data of other domains or not. In this paper, we will describe the challenge and results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 19303
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University ",
              "dsl": ""
            }
          ],
          "personId": 20020
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 11198
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Automation, Chinese Academy of Sciences",
              "dsl": "National Laboratory of Pattern Recognition (NLPR)"
            }
          ],
          "personId": 10428
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Minhang",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14933
        }
      ],
      "sessionIds": [
        1926
      ],
      "eventIds": []
    },
    {
      "id": 7203,
      "typeId": 11462,
      "title": "FACIAL EXPRESSION RECOGNITION VIA RELATION-BASED CONDITIONAL GENERATIVE ADVERSARIAL NETWORK",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Recognizing emotions by adapting to various human identities is very difficult. In order to solve this problem, this paper proposes a relation-based conditional generative adversarial network (RcGAN), which recognizes facial expressions by using the difference (or relation) between neutral face and expressive face. The proposed method can recognize facial expression or emotion independently of human identity. Experimental results show that the proposed method provides higher accuracies of 97.93% and 82.86% for CK+ and MMI databases, respectively than conventional method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Incheon",
              "institution": "Inha University",
              "dsl": "Electronic Engineering"
            }
          ],
          "personId": 13497
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Incheon",
              "city": "Incheon",
              "institution": "Inha University",
              "dsl": "Electronic Engineering"
            }
          ],
          "personId": 23557
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Incheon",
              "institution": "Inha University",
              "dsl": "Electronic Engineering"
            }
          ],
          "personId": 15576
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 4904,
      "typeId": 11462,
      "title": "A High Fidelity Open Embodied Avatar with LipSyncing and Expression Capabilities",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Embodied virtual agents have many applications and provide benefits over disembodied agents, allowing nonverbal social and interaction cues to be leveraged, in a similar manner to how humans interact with each other.\r\nWe present an open embodied avatar built upon the Unreal Engine that can be controlled via a simple python programming interface. The avatar has lip syncing (phoneme control), head gesture and facial expression (using either facial action units or cardinal emotion categories) capabilities. We release code and models to illustrate how the agent can be controlled like a puppet or used to create a simple conversational agent using public application programming interfaces (APIs). GITHUB Link: [REMOVED FOR ANONYMIZATION]",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 9104
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "redmond",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 16333
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 19777
        }
      ],
      "sessionIds": [
        1857
      ],
      "eventIds": []
    },
    {
      "id": 3379,
      "typeId": 11462,
      "title": "Modeling Emotion Influence Using Attention-based Graph Convolutional Recurrent Network",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "User emotion modeling is a vital problem of social media analysis. In previous studies, content and topology information of social networks have been considered in emotion modeling tasks, but the inflence of current emotion states of other users was not considered. We define emotion influence as the emotional impact from user's friends in social networks, which is determined by both network structure and node attributes (the features of friends). In this paper, we try to model the emotion influence to help analyze user's emotion. The key challenges to this problem are:  1) how to combine content features and network structures together to model emotion influence;  2) how to selectively focus on the major social network information related to emotion influence. To tackle these challenges, we propose an attention-based graph convolutional recurrent network to bring in emotion influence and content data. Firstly, we use an attention-based graph convolutional network to selectively aggregate the features of the user's friends with specific attention. Then an LSTM model is used to learn user's own content features and emotion influence. The model we proposed is more capable of quantifying the emotion influence in social networks as well as combining them together to analyze the user emotion status. We conduct emotion classification experiments to evaluate the effectiveness of our model on a real world dataset called Sina Weibo. Results show that our model outperforms several state-of-the-art methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 18040
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 15973
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 11408
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 7477,
      "typeId": 11462,
      "title": "Engagement Modeling in Dyadic Interaction",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "In the recent years, engagement modeling has gained increasing attention due the important role it plays in human-agent interaction. The agent should be able to detect, in real time, the engagement level of the user in order to react accordingly. In this context, our goal is to develop a computational model to predict engagement level of the user in real time. Relying on previous findings, we use facial expressions, head movements and gaze direction as predictive features. Moreover, engagement is not only measured from single cues, but from the combination of several cues that arise over a certain time window. Thus, for better engagement prediction, we consider the variation of multimodal behaviors over time. To this end, we rely on LSTM that can jointly model the temporality and the sequentiality of multimodal behaviors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "PARIS",
              "institution": "CNRS",
              "dsl": "ISIR"
            }
          ],
          "personId": 13212
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "ISIR",
              "dsl": "Sorbonne Universités, UPMC, CNRS"
            }
          ],
          "personId": 21466
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 6207,
      "typeId": 11462,
      "title": "Interaction Process Label Recognition in Group Discussion",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "In qualifying and analyzing the performance of group interaction, interaction processing analysis (IPA) defined by Bale is considered a useful approach. IPA is a system for labeling a total of 12 interaction categories for the interaction process. Automatic IPA can manually encompass the gap in spending manpower and can efficiently qualify group performance. In this paper, we present computational interaction processing analysis by developing a model to recognize categories of IPA. We extract both verbal features and nonverbal features for IPA category recognition modeling with SVM, RF, DNN and LSTM machine learning algorithms and analyze the contribution of multimodal features and unimodal features for the total data and each label. We also investigate the effect of context information by training sequences with different lengths with an LSTM and evaluating them. The results show that multimodal features achieve the best performance with an F1 score of 0.601 for the recognition of 12 IPA categories using the total data. Multimodal features are better than the unimodal features for the total data and most labels. The results of investigating context information show that a suitable length of sequence enables a longer sequence to achieve the best F1 score of 0.602 and a better performance for recognition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 22171
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 18478
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ishikawa",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 9747
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 5696,
      "typeId": 11462,
      "title": "Multimodal Classification of EEG During Physical Activity",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Brain Computer Interfaces (BCIs) typically utilize electroencephalography (EEG) to enable control of a computer through brain signals. However, EEG is susceptible to a large amount of noise, especially from muscle activity, making it difficult to use in ubiquitous computing environments where mobility and physicality are important features. In this work, we present a novel multimodal approach for classifying the P300 event related potential (ERP) component by coupling EEG signals with nonscalp electrodes (NSE) that measure ocular and muscle artifacts. We demonstrate the effectiveness of our approach on a new dataset where the P300 signal was evoked with participants on a stationary bike under three conditions of physical activity: rest, low-intensity, and high-intensity exercise. We show that intensity of physical activity impacts the performance of both our proposed model and existing state-of-the-art models. After incorporating signals from nonscalp electrodes our proposed model performs significantly better for the physical activity conditions. Our results suggest that the incorporation of additional modalities related to eye-movements and muscle activity may improve the efficacy of mobile EEG-based BCI systems, creating the potential for ubiquitous BCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California - Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 13720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 19429
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California -- Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9927
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California -- Santa Barbara",
              "dsl": "Psychology & Brain Sciences"
            }
          ],
          "personId": 12119
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Fresno",
              "institution": "California State University, Fresno",
              "dsl": "Computer Science"
            }
          ],
          "personId": 11758
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UC Santa Barbara",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 20442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California -- Santa Barbara",
              "dsl": "Psychology & Brain Sciences"
            }
          ],
          "personId": 8764
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 21658
        }
      ],
      "sessionIds": [
        2470
      ],
      "eventIds": []
    },
    {
      "id": 5189,
      "typeId": 11479,
      "title": "Multi-modal fusion methods for robust emotion recognition using body-worn physiological sensors in mobile environments",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "High-accuracy physiological emotion recognition typically requires participants to wear or attach obtrusive sensors (e.g., Electroencephalograph). To achieve precise emotion recognition using only wearable body-worn physiological sensors, my doctoral work focuses on researching and developing a robust sensor fusion system among different physiological sensors. Developing such fusion system has three problems: 1) how to pre-process signals with different temporal characteristics and noise models, 2) how to train the fusion system with limited labeled data and 3) how to fuse multiple signals with inaccurate and inexact ground truth. To overcome these challenges, I plan to explore semi-supervised, weakly supervised and unsupervised machine learning methods to obtain precise emotion recognition in mobile environments. By developing such techniques, we can measure the user engagement with larger amounts of participants and apply the emotion recognition techniques in a variety of scenarios such as mobile video watching and online education. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": "Distributed and Interactive Systems"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Intelligent Systems and Pattern Recognition Laboratory"
            }
          ],
          "personId": 11941
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 4679,
      "typeId": 11462,
      "title": "Smooth turn-taking by a robot using an online continuous model to generate turn-taking cues",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Turn-taking in human-robot interaction is a crucial part of spoken dialogue systems, but current models do not allow for human-like turn-taking speed seen in natural conversation. In this work we propose combining two independent prediction models. A continuous model predicts the upcoming end of the turn in order to generate gaze aversion and fillers as turn-taking cues. This prediction is done while the user is speaking, so turn-taking can be done with little silence between turns, or even overlap. Once a speech recognition result has been received at a later time, a second model uses the lexical information to decide if or when the turn should actually be taken. We constructed the continuous model using the speaker's prosodic features as inputs and evaluated its online performance. We then conducted a subjective experiment in which we implemented our model in an android robot and asked participants to compare it to one without turn-taking cues, which produces a response when a speech recognition result is received. We found that using both gaze aversion and a filler was preferred when the continuous model correctly predicted the upcoming end of turn, while using only gaze aversion was better if the prediction was wrong.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 18740
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 9539
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 10175
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 5964,
      "typeId": 11462,
      "title": "A Multimodal Robot-Driven Meeting Facilitation System for Group Decision-Making Sessions",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Group meetings are ubiquitous, with millions of meetings held across the world every day. However, meeting quality, group performance, and outcomes are challenged by a variety of dysfunctional behaviors, unproductive social dynamics, and lack of experience in conducting efficient and productive meetings. Previous studies have shown that meeting facilitators can be advantageous in helping groups reach their goals more effectively, but many groups do not have access to human facilitators due to a lack of resources or other barriers. In this paper, we describe the development of a multimodal robotic meeting facilitator that can improve the quality of small group decision-making meetings. This automated group facilitation system uses multimodal sensor inputs (user gaze, speech, prosody, and proxemics), as well as inputs from a tablet application, to intelligently enforce meeting structure, promote time management, balance group participation, and facilitate group decision-making processes. Results of a between-subject study of 20 user groups (N=40) showed that the robot facilitator is accepted by group members, is effective in enforcing meeting structure, and users found it helpful in balancing group participation. We also report design implications derived from the findings of our study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Sciences"
            }
          ],
          "personId": 21638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Khoury College of Computer Sciences"
            }
          ],
          "personId": 20750
        }
      ],
      "sessionIds": [
        1503
      ],
      "eventIds": []
    },
    {
      "id": 6222,
      "typeId": 11462,
      "title": "Creativity Support and Multimodal Pen-based Interaction",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Creativity as a  skill is associated with a potential to drive both  productivity and psychological wellbeing. Since multimodality can foster cognitive ability, multimodal digital tools should also be ideal to support creativity as an essentially cognitive skill. In this paper, we explore this notion by presenting a multimodal pen-based interaction technique and studying how it  supports creativity. The multimodal solution uses micro-controller-technology to augment a digital pen with RGB LEDs and a Leap Motion sensor to enable bimanual input. We report on a user study with 26 participants demonstrating that the multimodal technique is indeed perceived as supporting creativity significantly more than  a  baseline condition. We conclude with a critical discussion of our results, considering  implications for creativity support through multimodal interaction techniques and the culture and materiality surrounding lived practices of pen-based sketching. To this end, we utilize insights based on our own experience observing and engaging with various sketching communities in our town, including the urban sketchers. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": "Human-Centered Multimedia Lab"
            }
          ],
          "personId": 23123
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": ""
            }
          ],
          "personId": 8663
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": ""
            }
          ],
          "personId": 12808
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Human Centered Multimedia Lab",
              "dsl": "University of Augsburg"
            }
          ],
          "personId": 14057
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": ""
            }
          ],
          "personId": 15106
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": ""
            }
          ],
          "personId": 21712
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": ""
            }
          ],
          "personId": 16788
        }
      ],
      "sessionIds": [
        2048
      ],
      "eventIds": []
    },
    {
      "id": 24655,
      "typeId": 11482,
      "title": "Floor Apportionment Function of Speaker’s Gaze in Grounding Acts",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Mutual visual attention between the current and next speaker were analyzed from the viewpoints of grounding in communication and linguistic proficiency. Each utterance was categorized according to the grounding acts in the dialogue [21], and the gazing activities of the current and next speakers were compared between native (L1) and second language (L2) conversations. A correlation analysis showed that mutual visual attention was less prominent in utterances that ground the previous utterances without adding new information in L2, but not necessarily so in L1 conversations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Garden Air Tower, 3-10-10, Iidabashi, Chiyoda-ku",
              "institution": "KDDI Research, Inc.",
              "dsl": "Interaction Design Group"
            }
          ],
          "personId": 24642
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "AIST",
              "dsl": ""
            }
          ],
          "personId": 24611
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyotanabe-shi",
              "institution": "Doshisha University",
              "dsl": ""
            }
          ],
          "personId": 24629
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyotanabe-shi",
              "institution": "Doshisha University",
              "dsl": ""
            }
          ],
          "personId": 24608
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24656,
      "typeId": 11482,
      "title": "Real-Time Multimodal Classification of Internal and External Attention",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "The current attentional state can be divided into several categories, for example, the direction of attention. Often, this state is subconscious or its constant report impossible. Thus, an automated surveillance of the attentional state could be beneficial. In this paper, we performed a classification of multimodal data (EEG and eye tracking) to model internally- and externally-directed attention. 10 participants performed 6 different tasks of which 3 were associated with internal and 3 with external attention. In the first step, we showed that a combination of the two modalities led to an improvement of classification accuracy (average 72.67%) compared to single modality classifications. In a second step, the analysis was performed in real-time. The system was tested on one participant with an average accuracy of 60.87%. These results allow for an optimistic outlook on a reliable real-time multimodal classification system of internal and external attention.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 14035
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 24640
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "University of Graz",
              "dsl": ""
            }
          ],
          "personId": 24607
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "University of Graz",
              "dsl": ""
            }
          ],
          "personId": 24621
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 17410
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24657,
      "typeId": 11482,
      "title": "Multimodal Biometric Authentication for VR/AR using EEG and Eye Tracking",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Electroencephlogram (EEG) signals can enable an additional non-intrusive input modality especially when paired with a wearable headset (i.e. AR/VR). A great challenge in using EEG data for Brain-Computer Interface (BCI) algorithms is its poor generalization performance across users. Taking advantage of these inter-user differences, we investigate the potential to use this technology for user authentication -- similar to how face recognition works for mobile phones. We perform a correlation analysis on the feasibility of using EEG for authentication. Additionally, we evaluate this in combination with eye tracking data which is also readily available in the same headsets. We formulate a novel evaluation paradigm using publicly available EEG motor imagery and eye tracking data and demonstrate strong feasibility towards using EEG and eye tracking for authentication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California - Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 24638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California - Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 13720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California -- Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9927
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 21658
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24658,
      "typeId": 11482,
      "title": "Sensory Substitution Device Stabilizing Human Voice Production",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "The aim of the study has been to pursue a project on the development of a sensory substitution device capable of stabilizing the human voice production. This device is an alternative solution to the problem with voice production (e.g., singing) among Hearing Impairment Community. The information about the frequency of the emitted sound is passed through tactile stimuli. The project proposes a prototype of complete device with some innovative solutions. Vibration motors in the form of a loop are located on the user’s arm. Therefore all sounds of chosen music scale can be presented by keeping a strongly limited size of display.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Warsaw",
              "institution": "University of Warsaw",
              "dsl": ""
            }
          ],
          "personId": 24635
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24659,
      "typeId": 11482,
      "title": "Towards More Intelligent Conversational Agents: Fusing Dialogue and Gaze From Discussions of 2D and 3D Scenes",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Conversation partners rely on inference using each other's gaze and utterances to negotiate shared meaning. In contrast, dialogue systems still operate mostly with unimodal question or command and response interactions. To realize systems that can intuitively discuss and collaborate with humans, we should consider other sensory information. We begin to address this limitation with an innovative study that acquires, analyzes, and fuses interlocutors' discussion and gaze. Introducing a discussion-based elicitation task, we collect gaze with remote and wearable eye trackers alongside dialogue as interlocutors come to consensus on questions about an on-screen 2D image and a real-world 3D scene. We analyze the visual-linguistic patterns, and also map the modalities onto the visual environment by extending a multimodal image region annotation framework using statistical machine translation for multimodal fusion, applying three ways of fusing speakers' gaze and discussion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Oakland",
              "institution": "Mills College",
              "dsl": ""
            }
          ],
          "personId": 24649
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Tacoma",
              "institution": "University of Puget Sound",
              "dsl": ""
            }
          ],
          "personId": 24618
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 48925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 24613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 24625
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24660,
      "typeId": 11482,
      "title": "Interactive Upper Limb Training Device for Arm-Reaching and Finger Pointing Exercise",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "In order to restore muscle strength, stroke patients should adhere to regular and intensive rehabilitation exercises, especially visually-guided reaching and pointing movements. Most of patients are very passive when using traditional tools for these repetitive training. With the intention to support multiple training exercises actively, we proposed an upper limb rehabilitation device consisting of a tabletop device and a color map. This device can guide patients to do arm-reaching exercise and finger pointing exercise at the same time. The exercise data can be recorded and sent to computer via Bluetooth to track rehabilitation progress. Compared with the traditional training methods, the device enhances the fun of training through interactive forms such as visual feedbacks (LEDs) and voice instructions. Preliminary usability testing suggested that the device is attractive to patients, and easy to understand and play.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": ""
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": ""
            }
          ],
          "personId": 24616
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 24652
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Industrial Design"
            },
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 24639
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": ""
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": ""
            }
          ],
          "personId": 24609
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 6740,
      "typeId": 11462,
      "title": "TouchGazePath: Multimodal Interaction with Touch and Gaze Path for Secure Yet Efficient PIN Entry",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "We present TouchGazePath, a multimodal method for entering personal identification numbers (PINs). Using a touch-sensitive display showing a virtual keypad, the user initiates input with a touch at any location, glances with their eye gaze on the keys bearing the PIN numbers, then terminates input by lifting their finger. TouchGazePath is not susceptible to security attacks, such as shoulder surfing, thermal attacks, or smudge attacks. In a user study with 18 participants, TouchGazePath was compared with the traditional Touch-Only method and the multimodal Touch+Gaze method, the latter using eye gaze for targeting and touch for selection. The average time to enter a PIN with TouchGazePath was 3.3 s. This was not as fast as Touch-Only (as expected), but was about twice as fast the Touch+Gaze. TouchGazePath was also more accurate than Touch+Gaze. TouchGazePath had high user ratings as a secure PIN input method and was the preferred PIN input method for 11 of 18 participants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Koblenz",
              "institution": "University of Koblenz",
              "dsl": ""
            }
          ],
          "personId": 8767
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Koblenz",
              "institution": "University of Koblenz",
              "dsl": ""
            }
          ],
          "personId": 12356
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Koblenz",
              "institution": "University of Koblenz",
              "dsl": ""
            }
          ],
          "personId": 9280
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "York University",
              "dsl": "Department of Electrical Engineering and Computer Science"
            }
          ],
          "personId": 11986
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Koblenz",
              "institution": "University of Koblenz-Landau",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": ""
            }
          ],
          "personId": 8872
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 24661,
      "typeId": 11482,
      "title": "An Approach to Reading Assistance with Eye Tracking Data and Text Features",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Unknown words and long and difficult sentences with complex structures often cause difficulties in reading comprehension. This paper proposed a reading assistance approach based on analyzing the eye tracking data and the text features of the gaze area in the process of reading. This approach could automatically detect the user's intention in terms of word translation or long sentence summary, and then display the meaning of the word or the summary of the sentence in the form of annotations. The pilot study results showed that the average accuracy of this approach reached 80.6%±6.3%, and the automatically generated annotation improved the user's reading efficiency and subjective experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University of Technology",
              "dsl": ""
            }
          ],
          "personId": 24623
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhoou",
              "institution": "Zhejiang University of Technology",
              "dsl": ""
            }
          ],
          "personId": 24646
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24662,
      "typeId": 11482,
      "title": "Multimodal Anticipated versus Actual Perceptual Reactions",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce an experimental method where participants watch and rate humerous vs neutral videos while their reactions are collected in three modalities: non-linguistic verbalizations (laughter), facial expressions, and galvanic skin response. We use unimodal analysis and predictive modeling to examine the relationship between the perceptions and reactions anticipated by experimenters and the subjects' actual ones. This confirmed expected associations between facial expressions, but not skin response, and amusement. Laughter, while relatively infrequent, strongly suggests amusement when it occurs. Our approach can apply generally to comparisons of anticipated vs actual responses when collecting data for learning affective human response.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland, Baltimore County",
              "dsl": ""
            }
          ],
          "personId": 24634
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Hamilton",
              "institution": "Colgate University",
              "dsl": ""
            }
          ],
          "personId": 24624
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 24650
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 24620
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 24625
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24663,
      "typeId": 11482,
      "title": "Evaluation of Dominant and Non-Dominant Hand Movements For Volleyball Action Modelling",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we assess the use of Inertial Measurement Units (IMU)  in recognising different volleyball-specific actions. Analysis of the results suggests that all sensors in the IMU (i.e. magnetometer, accelerometer, barometer and gyroscope)  contribute unique information in the classification of volleyball-specific actions. We demonstrate that while the accelerometer feature set provides the best Unweighted Average Recall (UAR) overall, \"decision fusion'' of the accelerometer with the magnetometer improves UAR slightly from 85.86% to 86.9%. Interestingly, it is also demonstrated that the non-dominant hand provides better UAR than the dominant hand. These results are even more marked with \"decision fusion''.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 9037
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Biomedical Signals and Systems"
            }
          ],
          "personId": 13006
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Turkey",
              "institution": "Abdullah Gul University",
              "dsl": ""
            }
          ],
          "personId": 22718
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": ""
            }
          ],
          "personId": 12611
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Izmir",
              "institution": "Izmir University of Economics",
              "dsl": ""
            }
          ],
          "personId": 13452
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Istanbul Technical University",
              "dsl": ""
            }
          ],
          "personId": 18570
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 9657
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interactiom"
            }
          ],
          "personId": 20948
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 12332
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Biomedical Signals and Systems"
            }
          ],
          "personId": 15991
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 23094
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24664,
      "typeId": 11482,
      "title": "Multimodal Assessment on Teaching Skills via Neural Networks",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Repeated training of teaching skills for student teachers is difficult as many collaborators should be needed for rehearsal environment.\r\nTherefore, we are studying a teaching training system using virtual classroom which is constructed by virtual agents as students.\r\nIn order to construct such a training system, an automatic assessment of human behaviour by the system is required.\r\nOn the other hand, it is difficult to assess teaching skills in term of educational environment due to complex interactions with many students and subjectivity of the task of teaching assessment. \r\nIn this study, we propose an assessment model by neural networks (NN) to learn more potential assessment features using multi-modal information: gesture and prosodic information, facial expressions, and the teacher's intention.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "RIKEN Center for Advanced Intelligence",
              "dsl": ""
            }
          ],
          "personId": 24648
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "RIKEN Center for Advanced Intelligence",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 24647
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kusatu",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 24615
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "RIKEN Center for Advanced Intelligence",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 8687
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24665,
      "typeId": 11482,
      "title": "Are Humans Biased in Assessment of Video Interviews?",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Supervised systems require human labels for training. But, are humans themselves always impartial during the annotation process? We examine this question in the context of automated assessment of human behavioral tasks. Specifically, we investigate whether human ratings themselves can be trusted at their face value when scoring video-based structured interviews, and whether such ratings can impact machine learning models that use them as training data. We present preliminary empirical evidence that indicates there are biases in such annotations, most of which are visual in nature.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": "NLP, Speech and Multimodal"
            }
          ],
          "personId": 24612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": ""
            }
          ],
          "personId": 24643
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Educational Testing Service",
              "dsl": "R&D"
            }
          ],
          "personId": 24633
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": ""
            }
          ],
          "personId": 24632
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": "Academic to Career"
            }
          ],
          "personId": 24617
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Educational Testing Service",
              "dsl": "NLP, Speech and Multimodal"
            }
          ],
          "personId": 24653
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Educational Testing Service",
              "dsl": "NLP, Speech and Multimodal"
            }
          ],
          "personId": 24651
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": "NLP, Speech and Multimodal"
            }
          ],
          "personId": 24637
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Educational Testing Service",
              "dsl": "Cognitive and Tech Sciences"
            }
          ],
          "personId": 24641
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24666,
      "typeId": 11482,
      "title": "Lemusade: Make Lemonade using music",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we presented a prototype designed to explore whether our audio senses can guide us as sufficiently as visual cues to create food tastes of our choices. We designed a lemonade-mixer box which allowed users to make lemonade based on only music output. The adjustments of essential ingredients of lemonade were mapped to changes in the melody separately. Our user testing showed that sounds have significant influences in making food. We also discussed possible future research directions on this topic.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "School of Information",
              "dsl": "University of California, Berkeley"
            }
          ],
          "personId": 24626
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "berkeley",
              "institution": "University of California, Berkeley ",
              "dsl": ""
            }
          ],
          "personId": 24645
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "School of Information"
            }
          ],
          "personId": 24636
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24667,
      "typeId": 11482,
      "title": "Measuring Affective Sharing between Two People by EEG Hyperscanning",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "Empathy plays an important role in human social interaction. For example, It promotes stronger relationships and collaboration. As empathy relies on inter-brain neural synchronization, psychophysiological measurement of empathy should be possible. In this study, we measured affective sharing, one of the main components of empathy, from EEG signals. To elicit affective sharing, we conducted an experiment in which participants communicated using facial expressions of joy, sadness, and neutrality. EEG signals were simultaneously recorded from both participants during the experiment. The result showed the correlations of the EEG powers were significantly higher under the joy and sadness conditions in the alpha-mu band. This result demonstrates that it is possible to measure affective sharing in response to emotional faces from the correlation of EEG powers. \r\nTo our knowledge, this is the first EEG hyperscanning study that investigates affective sharing in response to emotional faces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24630
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24622
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24614
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24654
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24668,
      "typeId": 11482,
      "title": "Detecting Syntactic Violations from Single-trial EEG using Recurrent Neural Networks",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "We proposes a method with neural network models to detect language anomalies using electroencephalogram (EEG) signals. To the best of our knowledge, there have been few studies on classifying single-trial EEG signals related to language processing such as syntactic processing in sentence comprehension. We evaluated neural network models, i.e., stacked autoencoder (SAE)  and long-short term memory (LSTM)  for detecting syntactically anomalous sentences from single-trial EEG signals. 18 participants listened to sentences, some of which are syntactically anomalous, and responded by pressing a key on a keyboard. To compare SAE and LSTM with a traditional model, support vector machine (SVM), we trained all three with the recorded EEG data and tested them on unseen participants. The LSTM exhibited higher accuracy (61.3%) than SAE (58.3%) and SVM (58.4%). We found that LSTM performs better for single-trial of EEG signals during syntactic processing. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24628
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24622
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24654
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 5469,
      "typeId": 11479,
      "title": "Multimodal Machine Learning for Interactive Mental Health Therapy",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Mental health disorders and substance abuse are among the leading causes of disability. Despite its prevalence and importance, there is a large gap between the needs and resources available for mental health assessment and treatment. Automatic behavior analysis for computer-aided mental health assessment can augment clinical resources in the diagnosis and treatment of patients. Intelligent systems like virtual agents and social robots can have a large impact by deploying multimodal machine learning to perceive and interact with patients in interactive scenarios for probing behavioral cues of mental health disorders. In this paper, we propose our plan for developing multimodal machine learning methods for augmenting embodied interactive agents with emotional intelligence for the perception of mental health disorders. We aim to develop a new generation of intelligent agents that can create engaging interactive experiences for assisting with mental health assessments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies, University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 10730
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 24669,
      "typeId": 11482,
      "title": "Detecting Dementia from Face in Human-Agent Interaction",
      "trackId": 10681,
      "tags": [],
      "keywords": [],
      "abstract": "This paper proposes an approach to automatically detect dementia from a human face. Although some works have detected dementia from speech and language attributes, there are few studies focusing on facial expression in dementia patients. We recorded the human-agent interaction data of spoken dialogues from 24 participants (12 with dementia and 12 without) and extracted the face features. Our objective was to classify dementia by L1 regularized logistic regression. The facial features and the L1 logistic regression were then used to classify the participants into two groups with 0.82 detection performance, as measured by the areas under the receiver operating characteristic curve. We also identified various contributing features, such as action units, eye gaze, and lip activity. These results demonstrate that our system has the potential to detect dementia from the face through spoken dialog systems and as such, can be of assistance to health care workers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24622
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Health and Counseling Center"
            }
          ],
          "personId": 24644
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nankoku",
              "institution": "Kochi Medical School",
              "dsl": "Department of Neuropsychiatry"
            }
          ],
          "personId": 24627
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": "Graduate School of Medicine"
            }
          ],
          "personId": 24631
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Health and Counseling Center"
            }
          ],
          "personId": 24619
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 24654
        }
      ],
      "sessionIds": [
        24670
      ],
      "eventIds": []
    },
    {
      "id": 24671,
      "typeId": 11479,
      "title": "Detecting Temporal Phases of Anxiety in The Wild: Toward Continuously Adaptive Self-Regulation Technologies",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Anxiety disorders are becoming more prevalent; therefore, the demand for mobile anxiety self-regulation technologies is rising. However, the existing regulation technologies have not yet reached the ability to guide suitable interventions to a user in a timely manner. This is mainly due to the lack of maturity in the anxiety detection area. Hence, this research aims to (1) identify potential temporal phases of anxiety which could become effective personalization parameters for regulation technologies, (2) detect such phases through collecting and analyzing multimodal indicators of anxiety, and (3) design self-regulation technologies that can guide suitable interventions for the detected anxiety phase. Based on an exploratory study that was conducted with therapists treating anxiety disorders, potential temporal phases and common indicators of anxiety were identified. The design of anxiety detection and regulation technologies is currently in progress. The proposed research methodology and expected contributions are further discussed in this paper.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Notting Hill",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            },
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Notting Hill",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 12076
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24672,
      "typeId": 11479,
      "title": "Multimodal Machine Learning for Interactive Mental Health Therapy",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Mental health disorders and substance abuse are among the leading causes of disability. Despite its prevalence and importance, there is a large gap between the needs and resources available for mental health assessment and treatment. Automatic behavior analysis for computer-aided mental health assessment can augment clinical resources in the diagnosis and treatment of patients. Intelligent systems like virtual agents and social robots can have a large impact by deploying multimodal machine learning to perceive and interact with patients in interactive scenarios for probing behavioral cues of mental health disorders. In this paper, we propose our plan for developing multimodal machine learning methods for augmenting embodied interactive agents with emotional intelligence for the perception of mental health disorders. We aim to develop a new generation of intelligent agents that can create engaging interactive experiences for assisting with mental health assessments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies, University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 10730
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24673,
      "typeId": 11479,
      "title": "Tailoring Motion Recognition Systems to Children’s Motions",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Motion-based applications are becoming increasingly popular among children and require accurate motion recognition to ensure meaningful interactive experiences. However, motion recognizers are usually trained on adults’ motions. Children and adults differ in terms of their body proportions and stages of development of their neuromuscular systems, so children and adults will likely perform motions differently. Therefore, motion recognizers tailored to adults will likely perform poorly for children. My PhD thesis will focus on identifying features that characterize children’s and adults’ motions. This set of features will provide a model that can be used to understand children’s natural motion qualities and will serve as the first step in tailoring recognizers to children’s motions. This paper describes my past and ongoing work toward this end and outlines the next steps in my PhD work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "CISE department"
            }
          ],
          "personId": 17564
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 3681,
      "typeId": 11480,
      "title": "Multi-Classification Model for Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "  The spoken language understanding (SLU) is an important part of spoken dialogue system (SDS). In the paper, we focus on how to extract a set of act-slot-value triples from users' utterances in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU). This paper adopts the pretrained BERT model to encode users' utterances and builds multiple classifiers to get the required triples. In our framework, finding acts and values of slots are recognized as classification tasks respectively. Such multi-task training is expected to help the encoder to get better understanding of the utterance. Since the system is built on the transcriptions given by automatic speech recognition (ASR), some tricks are applied to correct the errors of the triples. We also found that using the minimum edit distance (MED) between results and candidates to rebuild the triples was beneficial in our experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Heifei",
              "institution": "University of Science and Technology of China",
              "dsl": ""
            }
          ],
          "personId": 19426
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hefei",
              "institution": "USTC",
              "dsl": ""
            }
          ],
          "personId": 8230
        }
      ],
      "sessionIds": [
        1926
      ],
      "eventIds": []
    },
    {
      "id": 24674,
      "typeId": 11479,
      "title": "Multi-modal fusion methods for robust emotion recognition using body-worn physiological sensors in mobile environments",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "High-accuracy physiological emotion recognition typically requires participants to wear or attach obtrusive sensors (e.g., Electroencephalograph). To achieve precise emotion recognition using only wearable body-worn physiological sensors, my doctoral work focuses on researching and developing a robust sensor fusion system among different physiological sensors. Developing such fusion system has three problems: 1) how to pre-process signals with different temporal characteristics and noise models, 2) how to train the fusion system with limited labeled data and 3) how to fuse multiple signals with inaccurate and inexact ground truth. To overcome these challenges, I plan to explore semi-supervised, weakly supervised and unsupervised machine learning methods to obtain precise emotion recognition in mobile environments. By developing such techniques, we can measure the user engagement with larger amounts of participants and apply the emotion recognition techniques in a variety of scenarios such as mobile video watching and online education. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": "Distributed and Interactive Systems"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Intelligent Systems and Pattern Recognition Laboratory"
            }
          ],
          "personId": 11941
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24675,
      "typeId": 11479,
      "title": "Communicative Signals and Social Contextual Factors in Multimodal Affect Recognition.",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "One research branch in Affective Computing focuses on using multimodal `emotional' expressions (e.g. facial expressions or non-verbal vocalisations) to automatically detect emotions and affect experienced by persons. The field is increasingly interested in using contextual factors to better infer emotional expressions rather than solely relying on the emotional expressions by themselves. We are interested in expressions that occur in a social context. In our research we plan to investigate how we can; a) use communicative signals that are displayed during interactions to recognise social contextual factors that affect emotion expression and in turn b) predict/recognise what these emotion expressions are most likely communicating considering that context. To achieve this, we formulate three main research questions: I) How do communicative signals such as emotion expressions co-ordinate behaviours and knowledge between interlocutors in interactive settings?, II) Can we use communicative signals to predict/recognise social contextual factors? and III) Can we use social contextual factors and communicative signals to predict what emotion experience is linked to an emotion expression in the same conversation? ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Overijssel ",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 20445
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 2915,
      "typeId": 11462,
      "title": "Dynamic Adaptive Gesturing Predicts Domain Expertise in Mathematics",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Embodied Cognition theorists believe that mathematics thinking is embodied in physical activity, like gesturing while explaining math solutions. This research asks the question whether expertise in mathematics can be detected by analyzing students’ rate and type of manual gestures. The results reveal several unique findings, including that math experts reduced their total rate of gesturing by 50%, compared with non-experts. They also dynamically increased their rate of gesturing on harder problems. Although experts reduced their rate of gesturing overall, they selectively produced 62% more iconic gestures. Iconic gestures are strategic because they assist with retaining spatial information in working memory, so that inferences can be extracted to support correct problem solving. The present results on representation-level gesture patterns are convergent with recent findings on signal-level handwriting and physical activity patterns, while also contributing a causal understanding of how and why experts adapt their manual activity during problem solving.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "CHIC, Information Technology"
            }
          ],
          "personId": 20332
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "CHIC, Information Technology"
            }
          ],
          "personId": 18171
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "CHIC, Information Technology"
            }
          ],
          "personId": 22954
        }
      ],
      "sessionIds": [
        2048
      ],
      "eventIds": []
    },
    {
      "id": 24676,
      "typeId": 11479,
      "title": "Co-located Collaboration Analytics",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Collaboration is an important skill of the 21st century. It can be in an online (or remote) setting or in a co-located (or face-to-face) setting. With the large scale adoption of sensor use, studies on colocated collaboration (CC) has gained momentum. CC takes place in physical spaces where the group members share each other’s social and epistemic space. This involves subtle multimodal interactions such as gaze, gestures, speech, discourse which are complex in nature. The aim of this PhD is to detect these interactions and then use these insights to build an automated real-time feedback system to facilitate co-located collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Limburg",
              "city": "Heerlen",
              "institution": "Open Universiteit",
              "dsl": "Technology Enhanced Learning and Innovations"
            }
          ],
          "personId": 13829
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24677,
      "typeId": 11479,
      "title": "Coalescing Narrative and Dialogue for Grounded Pose Forecasting",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "This research aims to create a data-driven end-to-end model for multimodal forecasting body pose and gestures of virtual avatars. A novel aspect of this research is to coalesce both narrative and dialogue for pose forecasting. In a narrative, language is used in a third person view to describe the avatar actions. In dialogue both first and second person views need to be integrated to accurately forecast avatar pose. Gestures and poses of a speaker are linked to other modalities: language and acoustics. We use these correlations to better predict the avatar's pose.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 21261
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 4965,
      "typeId": 11479,
      "title": "Detecting Temporal Phases of Anxiety in The Wild: Toward Continuously Adaptive Self-Regulation Technologies",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Anxiety disorders are becoming more prevalent; therefore, the demand for mobile anxiety self-regulation technologies is rising. However, the existing regulation technologies have not yet reached the ability to guide suitable interventions to a user in a timely manner. This is mainly due to the lack of maturity in the anxiety detection area. Hence, this research aims to (1) identify potential temporal phases of anxiety which could become effective personalization parameters for regulation technologies, (2) detect such phases through collecting and analyzing multimodal indicators of anxiety, and (3) design self-regulation technologies that can guide suitable interventions for the detected anxiety phase. Based on an exploratory study that was conducted with therapists treating anxiety disorders, potential temporal phases and common indicators of anxiety were identified. The design of anxiety detection and regulation technologies is currently in progress. The proposed research methodology and expected contributions are further discussed in this paper.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Notting Hill",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            },
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Notting Hill",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 12076
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 24678,
      "typeId": 11479,
      "title": "Attention-driven Interaction Systems for Augmented Reality",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented reality (AR) glasses enable the embedding of visual\r\ncontent in real-world surroundings. In this PhD project, I will\r\nimplement user interfaces which adapt to the cognitive state of the\r\nuser, for example by avoiding distractions or re-directing the user’s\r\nattention towards missed information. For this purpose, sensory\r\ndata from the user is captured (via EEG, eye tracking,...) and mod-\r\neled with machine learning techniques. The focus of the cognitive\r\nstate estimation is centered around attention related aspects. The\r\nmain task is to build models for an estimation of a person’s atten-\r\ntional state from the combination and classification of multimodal\r\ndata streams and context information, as well as their evaluation.\r\nFurthermore, the goal is to develop prototypical user interfaces for\r\nAR glasses and to test their usability in different scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 14035
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24679,
      "typeId": 11479,
      "title": "Multimodal Driver Interaction with Gesture, Gaze and Speech",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "The ever-growing research in computer vision has created new avenues for user interaction. Speech commands and gesture recognition are already being applied in various touch-based inputs. It is, therefore, foreseeable, that the use of multimodal input methods for user interaction is the next phase in development. In this paper, I propose a research plan of novel methods for the use of multimodal inputs for the semantic interpretation of human-computer interaction, specifically applied to a car driver. A fusion methodology has to be designed that adequately makes use of a recognized gesture (specifically finger pointing), eye gaze and head pose  for the identification of reference objects, while using the semantics from speech for a natural interactive environment for the driver. The proposed plan includes different techniques based on artificial neural networks for the fusion of the camera-based modalities (gaze, head and gesture). It then combines features extracted from speech with the fusion algorithm to determine the intent of the driver.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Universität des Saarlandes",
              "dsl": "Saarland Informatics Campus"
            },
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "BMW",
              "dsl": "Intelligent Personal Assistant"
            }
          ],
          "personId": 23568
        }
      ],
      "sessionIds": [
        24680
      ],
      "eventIds": []
    },
    {
      "id": 24681,
      "typeId": 11479,
      "title": "Invited Talk 1",
      "trackId": 10683,
      "tags": [],
      "keywords": [],
      "abstract": "TBD",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 24330
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 24682,
      "typeId": 11479,
      "title": "Invited Talk 2",
      "trackId": 10683,
      "tags": [],
      "keywords": [],
      "abstract": "TBD",
      "authors": [],
      "sessionIds": [
        1540
      ],
      "eventIds": []
    },
    {
      "id": 6511,
      "typeId": 11462,
      "title": "WiBend: Recognizing Bending Interaction for Passive Deformable Surfaces with Wi-Fi",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes WiBend, a system that recognizes bend- ing gestures as the input modalities for interacting on non-instrumented and deformable surfaces using WiFi signals. Without cameras or any internal/external sensors, WiBend recognizes different bending gestures that can define an input modality for discrete or continuous interaction on de- formable devices, in particular, on devices with slate form factors. WiBend takes advantage of off-the-shelf 802.11 (Wi-Fi) devices and Channel State Information (CSI) measurements of packet transmissions when the user is placed and interacting between a Wi-Fi transmitter and a receiver. WiBend extracts relevant features from CSI measurements to repre- sent different bending gestures in the form of Hidden Markov Models (HMM) during a training phase. During interaction, to recognize a gesture, WiBend extracts its features from CSI and computes their likelihood with respect to the pre- computed HMM models. We have performed extensive user experiments in an instrumented laboratory to obtain data for training the HMM models and for evaluating the precision of WiBend. During the experiments, participants performed 12 distinct bending gestures with three surface sizes, two bending speeds and two different directions. The perfor- mance evaluation results show that WiBend can distinguish between 12 bending gestures with a precision of 84% on average.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": "LIG"
            }
          ],
          "personId": 22971
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "CNRS",
              "dsl": "LIG"
            }
          ],
          "personId": 16142
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": "Equipe IIHM du Laboratoire d'Informatique de Grenoble"
            }
          ],
          "personId": 19519
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Grenoble Institute of Technology",
              "dsl": "LIG Lab"
            }
          ],
          "personId": 8758
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 5238,
      "typeId": 11462,
      "title": "\"Paint that object yellow\": Multimodal Interaction to Enhance Creativity During Design Tasks in VR",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual Reality (VR) has always been considered a promising medium to support designers with alternative work environments. Still, graphical user interfaces are prone to induce attention shifts between the user interface and the manipulated target objects which hampers the creative process. This work proposes a speech-and-gesture-based interaction paradigm for creative tasks in VR. We developed a multimodal toolbox (MTB) for VR-based design applications and compared it to a typical unimodal menu-based toolbox (UTB). The comparison uses a design-oriented use-case and measures flow, usability, and presence as relevant characteristics for a VR-based design process. The multimodal approach (1) led to a lower perceived task duration and a higher reported feeling of flow. It (2) provided a higher intuitive use and a lower mental workload while not being slower than an UTB. Finally, it (3) generated a higher feeling of presence. Overall, our results confirm significant advantages of the proposed multimodal interaction paradigm and the developed MTB for important characteristics of design processes in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Universit of Würzburg, Department of Computer Science, HCI Group",
              "dsl": ""
            }
          ],
          "personId": 16499
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Institute Human-Computer Media",
              "dsl": ""
            }
          ],
          "personId": 16610
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg, Department of Computer Science, HCI Group",
              "dsl": ""
            }
          ],
          "personId": 13012
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 21950
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 17071
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 6006,
      "typeId": 11478,
      "title": "Hang Out with the Language Assistant",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "AI assistants have found their place in households but most of the existing assistants use single modal interaction. We present a language assistant for kids called Hola (Hang out with the Language Assistant) which is a true multimodal assistant. Hola is a small mobile robot based assistant capable of understanding the objects around it and responding to questions about objects that it can see. Hola is also able to adjust the camera position and its own position to make an extra attempt to understand the object using robot control mechanism. The technology behind it uses a combination of natural language understanding, object detection, and hand pose detection. In addition, Hola also supports reading book in the form of storytelling for kids using OCR. Children can ask a question about any word that they do not understand and Hola can retrieve the information from the internet and tells the meaning, other details of the word. After reading the book or a page, the robot asks the child based on the words used in the book to confirm the child’s understanding of the book.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 15548
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 12664
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 9921
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 14783
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 9406
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 18500
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 19933
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 18565
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "SK Telecom",
              "dsl": "AI Center"
            }
          ],
          "personId": 11511
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 4473,
      "typeId": 11462,
      "title": "Exploring transfer learning between scripted and spontaneous speech for emotion recognition",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Emerging technologies related to the Internet of Things yield large amounts of real-life speech data that can reflect individuals’ emotions. Yet, labelled data of human emotion from spontaneous speech are extremely limited due to the difficulties in the collection and annotation of such large volumes of audio samples. A potential way to address this limitation is to augment emotion models of spontaneous speech with fully annotated data collected using scripted scenarios. We investigate whether and to what extent knowledge related to speech emotional content can be transferred between datasets of scripted and spontaneous speech. We implement transfer learning through: (1) a feed-forward neural network trained on the source data and whose last layers are fine-tuned based on the target data; and (2) a progressive neural network retain- ing a pool of pre-trained models and learning lateral connections between source and target task. We explore the effectiveness of the proposed approach using four publicly available datasets of emotional speech: IEMOCAP, RAVDESS, eNTERFACE’05, and CREMA-D. Our results indicate that transfer learning can effectively leverage corpora of scripted data to improve the performance of emotion recognition systems for spontaneous speech and that such systems can bene- fit even from a limited amount of source data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            }
          ],
          "personId": 23162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            }
          ],
          "personId": 15364
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 7549,
      "typeId": 11462,
      "title": "Multimodal Behavioral Markers Exploring Suicidal Intent in Social Media Videos",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Suicide is one of the leading causes of death in the modern world. In this digital age, individuals are increasingly using social media to express themselves and often use these platforms to express suicidal intent. Various studies have inspected suicidal intent behavioral markers in controlled environments but it is still unexplored if such markers will generalize to suicidal intent expressed on social media.  In this work, we set out to study multimodal behavioral markers related to suicidal intent when expressed on social media videos. We explore verbal, acoustic and visual behaviour markers in context of identifying individuals at higher risk of suicidal attempt. Our analysis reveals a set of predominant multimodal behavioral markers indicative of suicidal intent on social media videos. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies"
            }
          ],
          "personId": 11525
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies"
            }
          ],
          "personId": 10191
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies"
            }
          ],
          "personId": 15375
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies"
            }
          ],
          "personId": 17064
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 22500
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 5507,
      "typeId": 11462,
      "title": "Driving Anomaly Detection with Conditional Generative Adversarial Network using Physiological and CAN-Bus Data",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "New developments in advanced driver assistance systems (ADAS) can help drivers deal with risky driving maneuvers, preventing potential hazard scenarios. A key challenge in these systems is to determine when to intervene. While there are situations where the needs for intervention or feedback is clear (e.g., lane departure), it is often difficult to determine scenarios that deviate from normal driving conditions. These scenarios can appear due to errors by the drivers, presence of pedestrian or bicycles, or maneuvers from other vehicles. We formulate this problem as a driving anomaly detection, where the goal is to automatically identify cases that require intervention.  Towards addressing this challenging but important goal, we propose a multimodal system that considers (1) physiological signals from the driver, and (2) vehicle information obtained from the controller area network (CAN) bus sensor. The system relies on conditional generative adversarial networks (GAN) where the models are constrained by the signals previously observed. The difference of the scores in the discriminator between the predicted and actual signals is used as a metric for detecting driving anomalies. We collected and annotated a novel dataset for driving anomaly detection tasks, which is used to validate our proposed models. We present the analysis of the results, and perceptual evaluations which demonstrate the discriminative power of this unsupervised approach for detecting driving anomalies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "University of Texas at Dallas",
              "dsl": "Department of Electrical Engineering/Multimodal Signal Processing (MSP) Laboratory"
            }
          ],
          "personId": 19878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Honda Research Institute USA, Inc.",
              "dsl": ""
            }
          ],
          "personId": 8339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Electrical and Computer Engineering "
            }
          ],
          "personId": 10345
        }
      ],
      "sessionIds": [
        2470
      ],
      "eventIds": []
    },
    {
      "id": 5255,
      "typeId": 11480,
      "title": "Transfer Learning Methods for Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a series of methods to improve the performance of spoken language understanding in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU 2019) which is aimed to improve robustness to automatic speech recognition (ASR) errors and solve the problem of not enough labelled data in new domain. We combine word information and char information to improve the performance of the semantic parser. We also use some transfer learning methods like correlation alignments and adversarial learning to improve robustness of spoken language understanding system. Then we merge the rule method and the neural network method to raise system output performance. In the video and weather domains with few training data, we use both the transfer learning model which train on multi-domain data and the rule-based approach. Our approaches achieve F1 scores of 86.83%, 92.84%, 94.16%, and 93.04% in the test sets of map, music, video and weather domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 14220
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 16346
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 10353
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 23231
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 9322
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 15517
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 11198
        }
      ],
      "sessionIds": [
        1926
      ],
      "eventIds": []
    },
    {
      "id": 4746,
      "typeId": 11478,
      "title": "A Real-Time Scene Recognition System Based on RGB-D Video Streams",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "Depth data captured by the cameras such as Microsoft Kinect can bring depth information than traditional RGB data, which is also more robust to different environments, such as dim or dark lighting conditions. In this technical demonstration, we build a scene recognition system based on real-time processing of RGB-D video streams. Our system recognizes the scenes with video clips, where three types of threads are implemented to ensure the realtime. This system first buffers the frames of both RGB and depth videos with the capturing threads. When the buffered videos reach the certain length, the frames will be packed into clips and forwarded in a pre-trained C3D model to predict scene labels with the scene recognition thread. Finally, the predicted scene labels and captured videos are illustrated in our user interface with illustration thread.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beihang University",
              "dsl": ""
            }
          ],
          "personId": 14224
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "ICT",
              "dsl": "IIP"
            }
          ],
          "personId": 21415
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "ICT",
              "dsl": "IIP"
            }
          ],
          "personId": 16292
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing National Day School",
              "dsl": ""
            }
          ],
          "personId": 8486
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "ICT",
              "dsl": "IIP"
            }
          ],
          "personId": 13999
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 3473,
      "typeId": 11462,
      "title": "Evaluation of Ultrasound Haptics as a Supplementary Feedback Cue for Grasping in Virtual Environments",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an evaluation of ultrasound mid-air haptics as a supplementary feedback cue for grasping and lifting virtual objects in Virtual Reality (VR). We present a user study with 27 participants and evaluate 6 different object sizes ranging from 40 mm to 100 mm. We compare three  supplementary feedback cues in VR; mid-air haptics, visual feedback (glow effect) and no supplementary feedback. We report on precision metrics (time to completion, grasp aperture and grasp accuracy) and interaction metrics (post-test questionnaire, observations and feedback) to understand general trends and preferences. The results showed an overall preference for visual cues for bigger objects (> 60 mm) while ultrasound mid-air haptics were preferred for small virtual targets (50 mm).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 8398
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 16690
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": ""
            }
          ],
          "personId": 23298
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "West Midlands",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 14392
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 3731,
      "typeId": 11462,
      "title": "Multi-modal Active Learning Using Reinforcement Learning",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Human behavior is inherently multi-modal, showing its different\r\nfacets through face, body, audio, autonomic physiology\r\nand other modalities. For achieving meaningful human-computer\r\nand human-robot interactions, multi-modal models\r\nfor estimating the user states (e.g.engagement) are therefore\r\nneeded. Most of the existing works that try to build classifiers\r\nof the user’s states during interactions with computers\r\nor robots assume that the data to train the models are fully\r\nlabeled. Nevertheless, data labeling is costly and tedious, and\r\nalso prone to subjective interpretations by the human coders.\r\nThis is even more pronounced when the data are multi-modal:\r\nsome users are more expressive with their facial expressions,\r\nsome with their voice. Thus, building models that can\r\naccurately estimate the user’s states during an interaction is\r\nchallenging. To tackle the challenges of learning from multimodal\r\ndata, we propose a novel multi-modal active learning\r\n(AL) approach. We formulate our approach using the notion\r\nof reinforcement learning (RL) to find an optimal policy for\r\nactive selection of the user’s data, while largely reducing the\r\nneed for labels needed to train the target (modality-specific)\r\nmodels. We also investigate different multi-modal data fusion\r\nstrategies, showing that the proposed model-level fusion\r\ncoupled with RL outperforms the feature-level and modality-specific\r\nmodels, also outperforming the naive AL strategies\r\nsuch as random sampling, and the standard heuristics such as\r\nuncertainty sampling. We show the benefits of our approach\r\non the task of engagement estimation from real-world data\r\nof child-robot interactions during an autism therapy. Lastly,\r\nwe show that the proposed multi-modal AL approach can\r\nbe used to efficiently personalize the engagement classifiers\r\nto the target user using a small amount of actively selected\r\ndata of that user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 17991
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Computing"
            }
          ],
          "personId": 19999
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "GLAM - Group on Language, Audio & Music",
              "dsl": "Imperial College London"
            }
          ],
          "personId": 22795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 14286
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 5780,
      "typeId": 11462,
      "title": "Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile navigation apps are among the most used mobile applications and are often used as a baseline to evaluate new mobile navigation technologies in field studies. As field studies often introduce external factors that are hard to control for, we investigate how pedestrian navigation methods can be evaluated in virtual reality (VR). We present a study comparing navigation methods in real life (RL) and VR to evaluate if VR environments are a viable alternative to RL environments when it comes to testing these. In a series of studies, participants navigated a real and a virtual environment using a paper map and a navigation app on a smartphone. We measured the differences in navigation performance, task load and spatial knowledge acquisition between RL and VR. From these we formulate guidelines for the improvement of pedestrian navigation systems in VR like improved legibility for small screen devices. We furthermore discuss appropriate low-cost and low-space VR-locomotion techniques and discuss more controllable locomotion techniques. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 16882
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 9296
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 23204
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 16692
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 13421
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 11711
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 9793
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 12139
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 13121
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 17469
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 20794
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 21353
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 22260
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 20844
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 9715
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 10145
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "University of Applied Sciences Bonn-Rhein-Sieg",
              "dsl": ""
            }
          ],
          "personId": 12006
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 14422
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 2967,
      "typeId": 11462,
      "title": "Multimodal Analysis and Estimation of Intimate Self-Disclosure",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Self-disclosure to others has a proven benefit for one's mental health. It is shown that disclosure to computers can be similarly beneficial for emotional and psychological well-being. In this paper, we analyzed verbal and nonverbal behavior associated with self-disclosure in two datasets containing structured human-human and human-agent interviews from more than 200 participants. Correlation analysis of verbal and nonverbal behavior revealed that linguistic features such as affective and cognitive content in verbal behavior, and nonverbal behavior such as head gestures are associated with intimate self-disclosure. A multimodal deep neural network was developed to automatically estimate the level of intimate self-disclosure from verbal and nonverbal behavior. Between modalities, verbal behavior was the best modality for estimating self-disclosure within-corpora achieving r=0.66. However, the cross-corpus evaluation demonstrated that nonverbal behavior are more robust and can outperform language modality in cross-corpus evaluation. Such automatic models can be deployed in interactive virtual agents or social robots to evaluate rapport and guide their conversational strategy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "USC",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 20452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 15549
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Playa Vista",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 16734
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 21516
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Playa Vista",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 19253
        }
      ],
      "sessionIds": [
        1857
      ],
      "eventIds": []
    },
    {
      "id": 7069,
      "typeId": 11462,
      "title": "Multitask Prediction of Exchange-level Annotations for Multimodal Dialogue Systems",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents multimodal computational modeling of three labels that are independently annotated per exchange to implement an adaptation mechanism of dialogue strategy in spoken dialogue systems based on recognizing user sentiment by multimodal signal processing.\r\nThe three labels include (1) user's interest label pertaining to the current topic, (2) user's sentiment label, and (3) topic continuance denoting whether the system should continue the current topic or change it.\r\nPredicting the three types of labels that capture different aspects of the user's sentiment level and the system's next action contribute to adopting a dialogue strategy based on the user's sentiment.\r\n For this purpose, we enhanced shared multimodal dialogue data by annotating impressed sentiment labels and the topic continuance labels.\r\n With the corpus, we develop a multimodal prediction model for the three labels.\r\nA multitask learning technique is applied for binary classification tasks of the three labels considering the partial similarities among them. The prediction model was efficiently trained even with a small data set (less than 2000 samples) thanks to the multitask learning framework. \r\nExperimental results show that the multitask deep neural network (DNN) model trained with multimodal features including linguistics, facial expressions, body and head motions, and acoustic features, outperformed those trained as single-task DNNs by 1.6 points at the maximum.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 10186
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 18478
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 23516
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 15875
        }
      ],
      "sessionIds": [
        1857
      ],
      "eventIds": []
    },
    {
      "id": 3488,
      "typeId": 11462,
      "title": "Improved Visual Focus of Attention Estimation and Prosodic Features for Analyzing Group Interactions",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Collaborative group tasks require efficient and productive verbal and non-verbal interactions among the participants. Studying such interaction patterns could help groups perform more efficiently, but the detection and measurement of human behavior is challenging since it is inherently multimodal and changes on a millisecond time frame. In this paper, we present a method to study groups performing a collaborative decision-making task using non-verbal behavioral cues. First, we present a novel algorithm to estimate the visual focus of attention (VFOA) of participants using frontal cameras. The algorithm can be used in various group settings, and performs with a state-of-the-art accuracy of 90%.  Secondly, we present prosodic features for non-verbal speech analysis. These features are commonly used in speech/music classification tasks, but are rarely used in human group interaction analysis. We validate our algorithms on a multimodal dataset of 14 group meetings with 45 participants, and show that a combination of VFOA-based visual metrics and prosodic-feature-based metrics can predict emergent group leaders with 64% accuracy and dominant contributors with 86% accuracy. We also report our findings on the correlations between the non-verbal behavioral metrics with gender, emotional intelligence, and the Big 5 personality traits. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": ""
            }
          ],
          "personId": 10216
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": ""
            }
          ],
          "personId": 20066
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": ""
            }
          ],
          "personId": 20508
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 21178
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": ""
            }
          ],
          "personId": 16093
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 20058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Communication Studies"
            }
          ],
          "personId": 23548
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": ""
            }
          ],
          "personId": 24193
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 7331,
      "typeId": 11462,
      "title": "What's behind a choice? Understanding Modality Choices under Changing Environmental Conditions",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Interacting with the physical and digital environment multimodally enhances user flexibility and adaptability to different scenarios. A body of research has focused on comparing the efficiency and effectiveness of different interaction modalities in digital environments. However, little is known about user behavior in an environment that provides freedom to choose from a range of modalities. That is why, we take a closer look at the factors that influence input modality choices. Building on the work by Jameson & Kristensson, our goal is to understand how different factors influence user choices. In this paper, we present a study that aims to explore modality choices in a hands-free interaction environment, wherein participants can choose and combine freely three hands-free modalities (Gaze, Head movements, Speech) to execute point and select actions in a 2D interface. On the one hand, our results show that users avoid switching modalities more often than we expected, particularly, under conditions that should prompt modality switching. On the other hand, when users make a modality switch, user characteristics and consequences of the experienced interaction have a higher impact on the choice, than the changes in environmental conditions. Further, when users switch between modalities, we identified different types of switching behaviors. Users who deliberately try to find and choose an optimal modality (single switcher), users who try to find optimal combinations of modalities (multiple switcher), and a switching behavior triggered by error occurrence (error biased switcher). We believe that these results help to further understand when and how to design for multimodal interaction in real-world systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 14508
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": "computer science"
            }
          ],
          "personId": 10419
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences  ",
              "dsl": ""
            }
          ],
          "personId": 18614
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 11931
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 8100,
      "typeId": 11462,
      "title": "Effect of Feedback on Users' Emotions: Analysis of Facial Expressions during a Simulated Target Detection Task",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Safety-critical systems (e.g., UAV systems) often incorporate warning modules that alert users regarding imminent hazards (e.g., system failures). However, these warning systems are often not perfect, and trigger false alarms, which can lead to negative emotions and affect subsequent system usage. Although various feedback mechanisms have been studied in the past to counter the possible negative effects of system errors, the effect of such feedback mechanisms and system errors on users' immediate emotions and task performance is not clear. To investigate the influence of affective feedback on participants' immediate emotions, we designed a 2 (warning reliability: high/low) x 2 (feedback: present/absent) between-group study where participants interacted with a simulated UAV system to identify and neutralize enemy vehicles under time constraint. Task performance along with participants' facial expressions were analyzed. Results indicated that giving feedback decreased fear emotions during the task whereas warning increased frustration for high reliability groups compared to low reliability groups. Finally, feedback was found not to affect task performance. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 13518
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 15778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 21702
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 10897
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Farmington",
              "institution": "University of Connecticut Health Center",
              "dsl": "Health Disparities Institute"
            }
          ],
          "personId": 10962
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut",
              "dsl": "Communication and Psychological Sciences"
            }
          ],
          "personId": 13580
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 24742,
      "typeId": 11480,
      "title": "Bootstrap Model Ensemble and Rank Loss for Engagement Intensity Regression",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24456
        },
        {
          "affiliations": [
            {
              "institution": "Nanyang Technological University"
            }
          ],
          "personId": 24455
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24472
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24471
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24473
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24486
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 8102,
      "typeId": 11462,
      "title": "Video and Text-Based Affect Analysis of Children in Play Therapy",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Play therapy is an approach to psychotherapy where a child is engaging in play activities. Because of the strong affective component of play, it provides a natural setting to analyze feelings and coping strategies of the child. In this paper, we investigate an approach to track the affective state of a child during a play therapy session. We assume a simple, camera-based sensor setup, and describe the challenges of this application scenario. We use fine-tuned off-the-shelf deep convolutional neural networks for the processing of the child's face during sessions to automatically extract valence and arousal dimensions of affect, as well as basic emotional expressions. We further investigate text-based and body-movement based affect analysis. We evaluate these modalities separately and in conjunction with play therapy videos in natural sessions, discussing the results of such analysis and how it aligns with the professional clinicians’ assessments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": "Department of Information and Computing Sciences"
            }
          ],
          "personId": 16107
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "ISTANBUL",
              "institution": "Boğaziçi University",
              "dsl": "Computer Engineering Department"
            }
          ],
          "personId": 19881
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 11904
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "ISTANBUL",
              "institution": "Bilgi University",
              "dsl": ""
            }
          ],
          "personId": 20903
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": "Department of Information and Computing Sciences"
            }
          ],
          "personId": 22204
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 24743,
      "typeId": 11480,
      "title": "EmotiW 2019: Automatic Emotion, Engagement and Cohesion Prediction Tasks",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Monash University"
            }
          ],
          "personId": 24465
        },
        {
          "affiliations": [
            {
              "institution": "University of Canberra"
            }
          ],
          "personId": 24462
        },
        {
          "affiliations": [
            {
              "institution": "Indian Institute of Technology Ropar"
            }
          ],
          "personId": 24453
        },
        {
          "affiliations": [
            {
              "institution": "Australian National University"
            }
          ],
          "personId": 24451
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24744,
      "typeId": 11480,
      "title": "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24478
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "University of Chinese Academy of Sciences"
            }
          ],
          "personId": 24504
        },
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24502
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Key Lab of Computer Vision and Pattern Recognition"
            },
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 24426
        },
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24421
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24419
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24425
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 7848,
      "typeId": 11462,
      "title": "Continuous Emotion Recognition in Videos by Fusing Facial Expression, Head Pose and Eye Gaze",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Continuous emotion recognition is of great significance in affective computing and human-computer interaction. Most of existing methods for video based continuous emotion recognition utilize facial expression. However, besides facial expression, other clues including head pose and eye gaze are also closely related to continuous emotion recognition, but have not been well explored. On the one hand, head pose and eye gaze could result in different degrees of credibility of facial expression features. On the other hand, head pose and eye gaze carry emotional clues themselves, which is complementary to facial expression. Accordingly, in this paper we propose two ways to incorporate these two clues into continuous emotion recognition. They are respectively an attention mechanism based on head pose and eye gaze clues to guide the utilization of facial features in continuous emotion recognition, and an auxiliary line which helps extract more useful emotion information from head pose and eye gaze.\r\nExperiments are conducted on RECOLA, a database for continuous emotion recognition, and the results show that our framework outperforms other state-of-the-art methods due to the full use of head pose and eye gaze clues in addition to facial expression for continuous emotion recognition. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "beihang university",
              "dsl": ""
            }
          ],
          "personId": 11802
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "beijing",
              "institution": "beihang university",
              "dsl": ""
            }
          ],
          "personId": 10992
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "beijing",
              "institution": "beihang university",
              "dsl": ""
            }
          ],
          "personId": 14536
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "beijing",
              "institution": "beihang university",
              "dsl": ""
            }
          ],
          "personId": 16327
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "beijing",
              "institution": "beihang university",
              "dsl": ""
            }
          ],
          "personId": 22591
        }
      ],
      "sessionIds": [
        1882
      ],
      "eventIds": []
    },
    {
      "id": 24745,
      "typeId": 11480,
      "title": "Exploring Regularizations with Face, Body and Image Cues for Group Cohesion Prediction",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24484
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24489
        },
        {
          "affiliations": [
            {
              "institution": "Nanyang Technological University"
            }
          ],
          "personId": 24488
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24476
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24474
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24481
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24746,
      "typeId": 11480,
      "title": "Engagement Intensity Prediction with Facial Behavior Features",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24423
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24413
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24411
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24417
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24747,
      "typeId": 11480,
      "title": "Group-level Cohesion Prediction using Deep Learning Models with A Multi-stream Hybrid Network",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24415
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24438
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24448
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24447
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24748,
      "typeId": 11480,
      "title": "Automatic Group Cohesiveness Detection with Multi-modal Features",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24444
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24443
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24446
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24445
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24749,
      "typeId": 11480,
      "title": "Multi-feature and Multi-instance Learning with Anti-overfitting Strategy for Engagement Intensity Prediction",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24440
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24439
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24442
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24441
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24468
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24750,
      "typeId": 11480,
      "title": "Bi-modality Fusion for Emotion Recognition in the Wild",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24460
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24458
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24466
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24463
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24454
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24452
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24457
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24491
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24751,
      "typeId": 11480,
      "title": "Multi-Attention Fusion Network for Video-based Emotion Recognition",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24490
        },
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24493
        },
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24492
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24752,
      "typeId": 11480,
      "title": "Spotting Visual Keywords from Temporal Sliding Windows",
      "trackId": 10682,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 24431
        },
        {
          "affiliations": [],
          "personId": 24432
        },
        {
          "affiliations": [],
          "personId": 24429
        },
        {
          "affiliations": [],
          "personId": 24430
        },
        {
          "affiliations": [],
          "personId": 24435
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24753,
      "typeId": 11480,
      "title": "Deep Audio-visual System for Closed-set Word-level Speech Recognition",
      "trackId": 10682,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 24436
        },
        {
          "affiliations": [],
          "personId": 24433
        },
        {
          "affiliations": [],
          "personId": 24434
        },
        {
          "affiliations": [],
          "personId": 24437
        },
        {
          "affiliations": [],
          "personId": 24449
        },
        {
          "affiliations": [],
          "personId": 24450
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24754,
      "typeId": 11480,
      "title": "CATSLU: The 1st Chinese Audio-Textual Spoken Language Understanding Challenge",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "Spoken language understanding (SLU) is a key component of conversational dialogue systems, which converts user utterances into semantic representations. The previous works almost focus on parsing semantic from textual inputs (top hypothesis of speech recognition and even manual transcripts) while losing information hidden in the audio. We herein describe the \\textsc{1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU)} which introduces a new dataset with audio-textual information, multiple domains and domain knowledge. We introduce two scenarios of audio-textual SLU in which participants are encouraged to utilize data of other domains or not. In this paper, we will describe the challenge and results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 19303
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University ",
              "dsl": ""
            }
          ],
          "personId": 20020
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 11198
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Automation, Chinese Academy of Sciences",
              "dsl": "National Laboratory of Pattern Recognition (NLPR)"
            }
          ],
          "personId": 10428
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Minhang",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14933
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24755,
      "typeId": 11480,
      "title": "Transfer Learning Methods for Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a series of methods to improve the performance of spoken language understanding in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU 2019) which is aimed to improve robustness to automatic speech recognition (ASR) errors and solve the problem of not enough labelled data in new domain. We combine word information and char information to improve the performance of the semantic parser. We also use some transfer learning methods like correlation alignments and adversarial learning to improve robustness of spoken language understanding system. Then we merge the rule method and the neural network method to raise system output performance. In the video and weather domains with few training data, we use both the transfer learning model which train on multi-domain data and the rule-based approach. Our approaches achieve F1 scores of 86.83%, 92.84%, 94.16%, and 93.04% in the test sets of map, music, video and weather domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 14220
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 16346
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 10353
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 23231
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 9322
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 15517
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Heilongjiang",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": "School of Computer Science and Technology"
            }
          ],
          "personId": 11198
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24756,
      "typeId": 11480,
      "title": "Streamlined Decoder for Chinese Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "As a critical component of Spoken Dialog System (SDS), spoken language understanding (SLU) attracts a lot of attention, especially for methods based on unaligned data, which require only sentence-level annotation which is easy to obtain. Recently, a new approach has been proposed that utilizes the hierarchical relationship between act-slot-value triples and achieved state-of-the-art result on DSTC2 dataset. However, it ignores the transfer of internal information between hierarchies, which may record the intermediate information of the upper level when making prediction, which may contribute to the prediction of the lower level. So, we propose a novel streamlined decoding structure with attention mechanism, which uses three successively connected RNN to decode act, slot and value respectively. On the first Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU), our model exceeds state-of-the-art model on an unaligned multi-turn task-oriented Chinese spoken dialogue dataset provided by the contest.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 20937
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 21121
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 20040
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24757,
      "typeId": 11480,
      "title": "Robust Spoken Language Understanding with Acoustic and Domain Knowledge",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "Spoken language understanding (SLU) converts user utterances into structured semantic forms. There are still two main issues for SLU: robustness to ASR-errors and the data sparsity of new and extended domains. In this paper, we propose a robust SLU system by leveraging both acoustic and domain knowledge. We extract audio features by training ASR models on a large number of utterances without semantic annotations. For exploiting domain knowledge, we design lexicon features from the domain ontology and propose an error elimination algorithm to help predicted values recovered from ASR-errors. The results of CATSLU challenge show that our systems can outperform all of the other teams across four domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Minhang",
              "dsl": "Shanghai Jiao Tong University"
            }
          ],
          "personId": 19819
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 11328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 19303
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Minhang",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14933
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24758,
      "typeId": 11480,
      "title": "Multi-Classification Model for Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "  The spoken language understanding (SLU) is an important part of spoken dialogue system (SDS). In the paper, we focus on how to extract a set of act-slot-value triples from users' utterances in the 1st Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU). This paper adopts the pretrained BERT model to encode users' utterances and builds multiple classifiers to get the required triples. In our framework, finding acts and values of slots are recognized as classification tasks respectively. Such multi-task training is expected to help the encoder to get better understanding of the utterance. Since the system is built on the transcriptions given by automatic speech recognition (ASR), some tricks are applied to correct the errors of the triples. We also found that using the minimum edit distance (MED) between results and candidates to rebuild the triples was beneficial in our experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Heifei",
              "institution": "University of Science and Technology of China",
              "dsl": ""
            }
          ],
          "personId": 19426
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hefei",
              "institution": "USTC",
              "dsl": ""
            }
          ],
          "personId": 8230
        }
      ],
      "sessionIds": [
        24741
      ],
      "eventIds": []
    },
    {
      "id": 24505,
      "typeId": 11480,
      "title": "Bi-modality Fusion for Emotion Recognition in the Wild",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24460
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24458
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24466
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24463
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24454
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24452
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24457
        },
        {
          "affiliations": [
            {
              "institution": "Southeast University"
            }
          ],
          "personId": 24491
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 3257,
      "typeId": 11462,
      "title": "Multimodal Learning for Identifying Opportunities for Empathetic Reponses",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Embodied interactive agents possessing emotional intelli-\r\ngence and empathy can create natural and engaging social\r\ninteractions. Providing appropriate responses by interactive\r\nvirtual agents requires the ability to perceive users’ emo-\r\ntional states. In this paper, we study and analyze behavioral\r\ncues that indicate an opportunity to provide an empathetic\r\nresponse. Emotional tone in language in addition to facial\r\nexpressions are strong indicators of dramatic sentiment in\r\nconversation that warrants an empathetic response. To auto-\r\nmatically recognize such instances, we develop a multimodal\r\ndeep neural network for identifying opportunities when the\r\nagent should express positive or negative empathetic re-\r\nsponses. We train and evaluate our model using audio, video\r\nand language from human-agent interactions in a wizard-of-\r\nOz setting, using the wizard’s empathetic responses and Me-\r\nchanical Turk annotations as ground-truth labels. Our model\r\noutperforms the text-based baseline achieving F1-score of\r\n0.71 on a three-class classification. We further investigate\r\nthe results and evaluate the capability of such a model to be\r\ndeployed for real-world human-agent interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Institute for Creative Technologies, University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 10730
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 15549
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 19707
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 13472
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "USC",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 20452
        }
      ],
      "sessionIds": [
        1857
      ],
      "eventIds": []
    },
    {
      "id": 24506,
      "typeId": 11480,
      "title": "Spotting Visual Keywords from Temporal Sliding Windows",
      "trackId": 10682,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 24431
        },
        {
          "affiliations": [],
          "personId": 24432
        },
        {
          "affiliations": [],
          "personId": 24429
        },
        {
          "affiliations": [],
          "personId": 24430
        },
        {
          "affiliations": [],
          "personId": 24435
        }
      ],
      "sessionIds": [
        24569
      ],
      "eventIds": []
    },
    {
      "id": 24507,
      "typeId": 11480,
      "title": "Multi-Attention Fusion Network for Video-based Emotion Recognition",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24490
        },
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24493
        },
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc."
            }
          ],
          "personId": 24492
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24508,
      "typeId": 11480,
      "title": "Deep Audio-visual System for Closed-set Word-level Speech Recognition",
      "trackId": 10682,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 24436
        },
        {
          "affiliations": [],
          "personId": 24433
        },
        {
          "affiliations": [],
          "personId": 24434
        },
        {
          "affiliations": [],
          "personId": 24437
        },
        {
          "affiliations": [],
          "personId": 24449
        },
        {
          "affiliations": [],
          "personId": 24450
        }
      ],
      "sessionIds": [
        24569
      ],
      "eventIds": []
    },
    {
      "id": 24509,
      "typeId": 11480,
      "title": "Exploring Regularizations with Face, Body and Image Cues for Group Cohesion Prediction",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24484
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24489
        },
        {
          "affiliations": [
            {
              "institution": "Nanyang Technological University"
            }
          ],
          "personId": 24488
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24476
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24474
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24481
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24510,
      "typeId": 11480,
      "title": "Exploring Emotion Features and Fusion Strategies for Audio-Video Emotion Recognition",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24478
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "University of Chinese Academy of Sciences"
            }
          ],
          "personId": 24504
        },
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24502
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Key Lab of Computer Vision and Pattern Recognition"
            },
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 24426
        },
        {
          "affiliations": [
            {
              "institution": "National Engineering Laboratory for Speech and Language Information Processing (NEL-SLIP)"
            },
            {
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 24421
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24419
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24425
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 4798,
      "typeId": 11462,
      "title": "DIF : Dataset of Perceived Intoxicated Faces for Drunk Person Identification",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Traffic accidents cause over a million deaths every year, of which a large fraction is attributed to drunk driving. Automated intoxicated driver detection systems in vehicles will be useful in reducing accidents and the related financial costs. Existing solutions require special equipment such as electrocardiogram, infrared cameras or breathalyzers. In this work, we propose a new dataset called DIF (Dataset of perceived Intoxicated Faces) containing audio-visual data of intoxicated and sober people obtained from online sources. To the best of our knowledge this is the first work for automatic bimodal non-invasive intoxication detection. Convolutional Neural Networks (CNN) and deep neural networks are trained for computing the video and audio baselines. 3D CNN is used to exploit the spatio-temporal change in the video. A simple variation of the traditional 3D convolution block is proposed based on inducing non-linearity between the spatial and temporal channels. Extensive experiments are performed in validating the approach and baselines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Roopnagar",
              "institution": "Indian Institute of Technology Ropar",
              "dsl": ""
            }
          ],
          "personId": 13857
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Roopnagar",
              "institution": "Indian Institute of Technology Ropar",
              "dsl": ""
            }
          ],
          "personId": 18727
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Punjab",
              "city": "Rupnagar",
              "institution": "Indian Institute of Technology Ropar",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 13631
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Punjab",
              "city": "Rupnagar",
              "institution": "Indian Institute of Technology Ropar",
              "dsl": ""
            }
          ],
          "personId": 23613
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 4030,
      "typeId": 11479,
      "title": "Coalescing Narrative and Dialogue for Grounded Pose Forecasting",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "This research aims to create a data-driven end-to-end model for multimodal forecasting body pose and gestures of virtual avatars. A novel aspect of this research is to coalesce both narrative and dialogue for pose forecasting. In a narrative, language is used in a third person view to describe the avatar actions. In dialogue both first and second person views need to be integrated to accurately forecast avatar pose. Gestures and poses of a speaker are linked to other modalities: language and acoustics. We use these correlations to better predict the avatar's pose.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 21261
        }
      ],
      "sessionIds": [
        1540
      ],
      "eventIds": []
    },
    {
      "id": 24511,
      "typeId": 11480,
      "title": "EmotiW 2019: Automatic Emotion, Engagement and Cohesion Prediction Tasks",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Monash University"
            }
          ],
          "personId": 24465
        },
        {
          "affiliations": [
            {
              "institution": "University of Canberra"
            }
          ],
          "personId": 24462
        },
        {
          "affiliations": [
            {
              "institution": "Indian Institute of Technology Ropar"
            }
          ],
          "personId": 24453
        },
        {
          "affiliations": [
            {
              "institution": "Australian National University"
            }
          ],
          "personId": 24451
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24512,
      "typeId": 11480,
      "title": "Bootstrap Model Ensemble and Rank Loss for Engagement Intensity Regression",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24456
        },
        {
          "affiliations": [
            {
              "institution": "Nanyang Technological University"
            }
          ],
          "personId": 24455
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24472
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24471
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Science"
            }
          ],
          "personId": 24473
        },
        {
          "affiliations": [
            {
              "institution": "Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences"
            },
            {
              "institution": "The Chinese University of Hong Kong"
            }
          ],
          "personId": 24486
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24513,
      "typeId": 11480,
      "title": "Automatic Group Cohesiveness Detection with Multi-modal Features",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24444
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24443
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24446
        },
        {
          "affiliations": [
            {
              "institution": "University of Delaware"
            }
          ],
          "personId": 24445
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24514,
      "typeId": 11465,
      "title": "Single-trial based EEG classification of the dynamic representation of speaker stance: A preliminary study with representational similarity analysis",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tongji University"
            }
          ],
          "personId": 24497
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 24515,
      "typeId": 11480,
      "title": "Multi-feature and Multi-instance Learning with Anti-overfitting Strategy for Engagement Intensity Prediction",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24440
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24439
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24442
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24441
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo AI Team"
            }
          ],
          "personId": 24468
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24516,
      "typeId": 11465,
      "title": "A supra-modal decoding mechanism: Evidence from Chinese speakers learning English",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tamkang University, Twaiwan, China"
            }
          ],
          "personId": 24494
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai University of Sport, China"
            }
          ],
          "personId": 24495
        },
        {
          "affiliations": [
            {
              "institution": "Fudan University"
            }
          ],
          "personId": 24496
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 24517,
      "typeId": 11480,
      "title": "Engagement Intensity Prediction with Facial Behavior Features",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24423
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24413
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24411
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24417
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 24518,
      "typeId": 11480,
      "title": "Group-level Cohesion Prediction using Deep Learning Models with A Multi-stream Hybrid Network",
      "trackId": 10677,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24415
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24438
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24448
        },
        {
          "affiliations": [
            {
              "institution": "Chonnam National University"
            }
          ],
          "personId": 24447
        }
      ],
      "sessionIds": [
        24530
      ],
      "eventIds": []
    },
    {
      "id": 3527,
      "typeId": 11478,
      "title": "A Searching and Automatic Video Tagging Tool for Events of Interest during Volleyball Training Sessions",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "Quick and easy access to performance data during matches and training sessions is important for both players and coaches. While there are many video tagging systems available, these systems require manual effort. This paper proposes a system architecture that automatically supplements video recording by detecting events of interests in volleyball matches and training sessions to provide tailored and interactive multi-modal feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 13006
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 9037
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Turkey",
              "institution": "Abdullah Gul University",
              "dsl": ""
            }
          ],
          "personId": 22718
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": ""
            }
          ],
          "personId": 12611
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Izmir",
              "institution": "Izmir University of Economics",
              "dsl": ""
            }
          ],
          "personId": 13452
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Istanbul Technical University",
              "dsl": ""
            }
          ],
          "personId": 18570
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 9657
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 20948
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 12332
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 23094
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 15991
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 24519,
      "typeId": 11465,
      "title": "Affective computation of students' behaviors under classroom scenes",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Hunan University of Arts and Science, China"
            }
          ],
          "personId": 24475
        },
        {
          "affiliations": [
            {
              "institution": "Fudan University, China"
            }
          ],
          "personId": 24477
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai University of Sport, China"
            }
          ],
          "personId": 24480
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai Changhai Hospital, China"
            }
          ],
          "personId": 24483
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 24520,
      "typeId": 11465,
      "title": "Multisensory integration of emotions in a face-prosody-semantics Stroop task",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Shanghai Jiao Tong University"
            }
          ],
          "personId": 24428
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai Jiao Tong University"
            }
          ],
          "personId": 24412
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 24521,
      "typeId": 11465,
      "title": "NeuroManagement and Intelligent Computing Method on Multimodal Interaction",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Fudan University"
            }
          ],
          "personId": 24427
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 3529,
      "typeId": 11462,
      "title": "Unsupervised Deep Fusion Cross-modal Hashing",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Approximate cross-modal nearest neighbors searching attracted a lot of attention in the era of big data.  To handle the large-scale data in terms of storage and searching time, learning to hash becomes popular due to its efficiency and effectiveness. Most existing unsupervised cross-modal hashing methods, to shorten the semantic gap, try to simultaneously minimize the loss of intra-modal similarity and the loss of inter-modal similarity. Although these methods achieved promising performance, their models can not guarantee in theory these two losses are simultaneously minimized. Moreover, most previous methods use sign function to obtain hash codes, which may result a large quantization loss. In this paper, we first theoretically prove that cross-modal hashing could be implemented by protecting both intra-modal and inter-modal similarity with the aid of variational inference technique and point out the problem that maximizing intra and inter-modal similarity is mutually restrictive. In this case, we propose an unsupervised cross-modal hashing framework named as Unsupervised Deep Fusion Cross-modal Hashing (UDFCH) which leverages the data fusion to capture the underlying manifold across modalities to avoid above problem. What's more, in order to reduce the quantization loss, we assume that each bit of the hash codes independently obeys the Bernoulli distribution and obtains the hash codes directly by reparameterization trick. Our UDFCH framework has two stages. The first stage aims at mining the the intra-modal structure of each modality, which is implemented via applying auto-encoder on each modal data so that the original information can be maintained as much as possible. The second stage aims to determine the modality-aware hash code by sufficiently considering the correlation and manifold structure among modalities. A series of experiments have been conducted on three benchmark datasets. The results have shown that the proposed UDFCH framework outperforms the state-of-the-art methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Beijing Jiaotong University",
              "dsl": "School of Computer and Information Technology"
            }
          ],
          "personId": 23723
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University",
              "dsl": ""
            }
          ],
          "personId": 19100
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Beijing Key Lab of Traffic Data Analysis and Mining",
              "dsl": "Beijing Jiaotong University"
            }
          ],
          "personId": 17187
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 24523,
      "typeId": 11465,
      "title": "Machine learning in human-computer nonverbal communication",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National University of Defense Technology, China"
            }
          ],
          "personId": 24418
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai Changhai Hospital, China"
            }
          ],
          "personId": 24420
        },
        {
          "affiliations": [
            {
              "institution": "Vanderbilt University, USA"
            }
          ],
          "personId": 24422
        },
        {
          "affiliations": [
            {
              "institution": "Zhejiang Sci-Tech University, China"
            }
          ],
          "personId": 24424
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 4811,
      "typeId": 11462,
      "title": "Generative Model of Agent's behaviors in Human-Agent Interaction",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "A social interaction implies a social exchange between two or more persons, where they adapt and adjust their behaviors in response to their interaction partners. With the growing interest in human-agent interactions, it is desirable to make these interactions more natural and human like. In this context, we aim at enhancing the quality of the interaction between user and Embodied Conversational Agent (ECA) by endowing  ECA with the capacity to adapt its behavior in real time according the user's behavior. The novelty of our approach is to model the agent's  nonverbal behaviors as a function of both agent's and user's  behaviors jointly with the agent's communicative intentions. Moreover, we encompass the variation of  behavior over time through a LSTM-based model. Our model IL-LSTM (Interaction Loop LSTM) predicts the next agent's behavior taking into account the behavior that both, the agent and the user, have displayed within a time window. We have conducted an evaluation study involving an agent interacting with visitors in a science museum. Results of our study show that participants have better experience and are more engaged in the interaction when the agent adapts its behaviors to theirs, thus creating an interactive loop.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "PARIS",
              "institution": "CNRS",
              "dsl": "ISIR"
            }
          ],
          "personId": 13212
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "ISIR",
              "dsl": "Sorbonne Universités, UPMC, CNRS"
            }
          ],
          "personId": 21466
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 3531,
      "typeId": 11478,
      "title": "Chemistry Pods: A Real Time Multimodal tool for the Classroom",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "Instructors are often multitasking in the classroom. This makes it increasingly difficult for them to pay attention to each individual's engagement especially during activities where students are working in groups. In this paper, we describe a system that aids instructors in supporting group collaboration by utilizing a centralized, easy-to-navigate dashboard connected to multiple pods dispersed among groups of students in a classroom or laboratory. This allows instructors to check multiple qualities of the discussion such as: the usage of instructor specified keywords, relative participation of each individual, the speech acts students are using and different emotional characteristics of group language.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evantson",
              "institution": "Northwestern University",
              "dsl": "Computer Science - Tiilt Lab"
            }
          ],
          "personId": 13526
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "Sony",
              "dsl": ""
            }
          ],
          "personId": 23050
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Sony Electronics Inc.",
              "dsl": ""
            }
          ],
          "personId": 22139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 21067
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "Sony",
              "dsl": ""
            }
          ],
          "personId": 20001
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "Sony",
              "dsl": ""
            }
          ],
          "personId": 16549
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 24524,
      "typeId": 11465,
      "title": "A synergy study of metaphoric gestures on rhetorical behavior construction: Based on the corpus of \"AI\"-themed public speeches",
      "trackId": 10678,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Guandong University of Foreign Studies, China"
            }
          ],
          "personId": 24414
        },
        {
          "affiliations": [
            {
              "institution": "Xi'an International Studies University, China"
            }
          ],
          "personId": 24416
        }
      ],
      "sessionIds": [
        24528
      ],
      "eventIds": []
    },
    {
      "id": 24525,
      "typeId": 11465,
      "title": "Victim or Perpetrator Analysis of Violent Character Portrayals from Movie Scripts",
      "trackId": 10680,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 24501
        },
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 24503
        },
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 24479
        },
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 24485
        },
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 24487
        }
      ],
      "sessionIds": [
        24531
      ],
      "eventIds": []
    },
    {
      "id": 24526,
      "typeId": 11465,
      "title": "RWF-2000: A Large Video Database for Violence Detection",
      "trackId": 10680,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Duke Kunshan University"
            }
          ],
          "personId": 24498
        },
        {
          "affiliations": [
            {
              "institution": "Duke Kunshan University"
            }
          ],
          "personId": 24499
        },
        {
          "affiliations": [
            {
              "institution": "Duke Kunshan University"
            }
          ],
          "personId": 24500
        }
      ],
      "sessionIds": [
        24531
      ],
      "eventIds": []
    },
    {
      "id": 4302,
      "typeId": 11462,
      "title": "Understanding the Attention Demand of Touch and Tangible Interaction on a Composite Task",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Bimanual input is frequently used on touch and tangible interaction on tabletop surfaces. Considering a composite task, such as moving a set of objects, attention, decision making and fine motor control have to be phased with the coordination of the two hands. However, attention demand is an important factor to design easy to learn and recall interaction techniques. This, determining what interaction modality demands less attention and which one performs better in these conditions is important to improve design. In this work, we present the first empirical results on this matter. We report that users are consistent in their assessments of the attention demand for both touch and tangible modalities, even under different hands synchronicity, and different population sizes and densities. Our findings indicate that the one hand condition and small populations demand less attention comparing to respectively, two hands conditions and bigger populations. Also, we show that tangible modality reduces significantly the attention when using two hands synchronous movements or when moving the sparse populations, decreases the movement time over touch modality, without compromising the traveled distance. We use our findings to outline a set of guidelines to assist touch and tangible design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Valenciennes",
              "institution": "Univ. Polytechnique Hauts-de-France, CNRS UMR 8201",
              "dsl": "LAMIH"
            }
          ],
          "personId": 15156
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Valenciennes",
              "institution": "Univ. Polytechnique Hauts-de-France, CNRS UMR 8201",
              "dsl": "LAMIH"
            }
          ],
          "personId": 19263
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Valenciennes",
              "institution": "Univ. Polytechnique Hauts-de-France, CNRS UMR 8201",
              "dsl": "LAMIH"
            }
          ],
          "personId": 11750
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 4814,
      "typeId": 11479,
      "title": "Co-located Collaboration Analytics",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Collaboration is an important skill of the 21st century. It can be in an online (or remote) setting or in a co-located (or face-to-face) setting. With the large scale adoption of sensor use, studies on colocated collaboration (CC) has gained momentum. CC takes place in physical spaces where the group members share each other’s social and epistemic space. This involves subtle multimodal interactions such as gaze, gestures, speech, discourse which are complex in nature. The aim of this PhD is to detect these interactions and then use these insights to build an automated real-time feedback system to facilitate co-located collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Limburg",
              "city": "Heerlen",
              "institution": "Open Universiteit",
              "dsl": "Technology Enhanced Learning and Innovations"
            }
          ],
          "personId": 13829
        }
      ],
      "sessionIds": [
        1540
      ],
      "eventIds": []
    },
    {
      "id": 24527,
      "typeId": 11465,
      "title": "Multi-queries based Attention for Facial Emotion Recognition",
      "trackId": 10680,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tianjin Normal University"
            }
          ],
          "personId": 24459
        },
        {
          "affiliations": [
            {
              "institution": "Tianjin Normal University"
            }
          ],
          "personId": 24461
        },
        {
          "affiliations": [
            {
              "institution": "Tianjin Normal University"
            }
          ],
          "personId": 24464
        },
        {
          "affiliations": [
            {
              "institution": "Tianjin Normal University"
            }
          ],
          "personId": 24467
        }
      ],
      "sessionIds": [
        24531
      ],
      "eventIds": []
    },
    {
      "id": 3794,
      "typeId": 11462,
      "title": "Motion Eavesdropper: Smartwatch-based Handwriting Recognition Using Deep Learning",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "This paper focuses on the real-life scenario that people are handwriting while wearing small mobile devices on their wrists. We explore the possibility of eavesdropping privacy-related information based on motion signals. To achieve this, we elaborately develop a new deep-learning based motion sensing framework with four major components, i.e., recorder, signal preprocessor, feature extractor and handwriting recognizer. First, we integrate a series of simple yet effective signal processing techniques to purify the sensory data to reflect the kinetic property of a handwriting motion. Then we take advantage of properties of Multimodal Convolutional Neural Network (MCNN) to extract abstract features. After that, a bidirectional Long Short-Term Memory (BLSTM) network is exploited to model temporal dynamics. Finally, we incorporate Connectionist Temporal Classification (CTC) algorithm to realize end-to-end handwriting recognition. We prototype our design using a commercial off-the-shelf smartwatch and carry out extensive experiments. The encouraging results reveal that our system can robustly achieve an average accuracy of 64\\% at character-level and 71.9\\% at word-level, and 56.6\\% accuracy rate for words unseen in the training set under certain conditions, which expose the danger of privacy disclosure in daily lives.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14614
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 10650
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14563
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 8896
        }
      ],
      "sessionIds": [
        2048
      ],
      "eventIds": []
    },
    {
      "id": 3541,
      "typeId": 11462,
      "title": "ElderReact: A Multimodal Dataset for Recognizing Emotional Response in Aging Adults",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Automatic emotion recognition plays a critical role in technologies such as intelligent agents and social robots and is increasingly being deployed in applied settings such as education and healthcare. Most research to date has focused on recognizing the emotional expressions of young and middle-aged adults and, to a lesser extent, children and adolescents. Very few studies have examined automatic emotion recognition in older adults (i.e., elders), which represent a large and growing population worldwide. Given that aging causes many changes in facial shape and appearance and has been found to alter patterns of nonverbal behavior, there is strong reason to believe that automatic emotion recognition systems may need to be developed specifically (or augmented) for the elder population. To promote and support this type of research, we introduce a newly collected multimodal dataset of elders reacting to emotion elicitation stimuli. Specifically, it contains 1323 video clips of 46 unique individuals with human annotations of six discrete emotions: anger, disgust, fear, happiness, sadness, and surprise as well as valence. We present a detailed analysis of the most indicative features for each emotion. We also establish several baselines using unimodal and multimodal features on this dataset. Finally, we show that models trained on dataset of another age group do not generalize well on elders. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 14485
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 11167
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 9404
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 21469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 14761
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 22500
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 2774,
      "typeId": 11462,
      "title": "To React or not to React: End-to-End Visual Pose Forecasting of a Personalized Avatar during Dyadic Conversations",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Non verbal behaviours such as gestures, facial expressions, body posture, and para-linguistic cues have been shown to complement or clarify verbal messages. Hence to improve telepresence, in form of an avatar, it is important to model these behaviours, especially in dyadic interactions. Creating such personalized avatars not only requires to model intrapersonal dynamics between a avatar's speech and their body pose, but it also needs to model interpersonal dynamics with the interlocutor present in the conversation. \r\n\r\nIn this paper, we introduce a neural architecture named Dyadic Residual-Attention Model (DRAM), which integrates intrapersonal (monadic) and interpersonal (dyadic) dynamics using selective attention to generate sequences of body pose conditioned on audio and body pose of the interlocutor and audio of the human operating the avatar. We evaluate our proposed model on dyadic conversational data consisting of pose and audio of both participants, confirming the importance of adaptive attention between monadic and dyadic dynamics when predicting avatar pose. We also conduct a user study to analyze judgments of human observers. Our results confirm that the generated body pose is more natural, models intrapersonal dynamics and interpersonal dynamics better than non-adaptive monadic/dyadic models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 21261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            }
          ],
          "personId": 8887
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Language Technologies Institute"
            }
          ],
          "personId": 22500
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Facebook Reality Labs",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 14277
        }
      ],
      "sessionIds": [
        1857
      ],
      "eventIds": []
    },
    {
      "id": 4568,
      "typeId": 11479,
      "title": "Multimodal Driver Interaction with Gesture, Gaze and Speech",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "The ever-growing research in computer vision has created new avenues for user interaction. Speech commands and gesture recognition are already being applied in various touch-based inputs. It is, therefore, foreseeable, that the use of multimodal input methods for user interaction is the next phase in development. In this paper, I propose a research plan of novel methods for the use of multimodal inputs for the semantic interpretation of human-computer interaction, specifically applied to a car driver. A fusion methodology has to be designed that adequately makes use of a recognized gesture (specifically finger pointing), eye gaze and head pose  for the identification of reference objects, while using the semantics from speech for a natural interactive environment for the driver. The proposed plan includes different techniques based on artificial neural networks for the fusion of the camera-based modalities (gaze, head and gesture). It then combines features extracted from speech with the fusion algorithm to determine the intent of the driver.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Universität des Saarlandes",
              "dsl": "Saarland Informatics Campus"
            },
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "BMW",
              "dsl": "Intelligent Personal Assistant"
            }
          ],
          "personId": 23568
        }
      ],
      "sessionIds": [
        1540
      ],
      "eventIds": []
    },
    {
      "id": 3546,
      "typeId": 11479,
      "title": "Attention-driven Interaction Systems for Augmented Reality",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented reality (AR) glasses enable the embedding of visual\r\ncontent in real-world surroundings. In this PhD project, I will\r\nimplement user interfaces which adapt to the cognitive state of the\r\nuser, for example by avoiding distractions or re-directing the user’s\r\nattention towards missed information. For this purpose, sensory\r\ndata from the user is captured (via EEG, eye tracking,...) and mod-\r\neled with machine learning techniques. The focus of the cognitive\r\nstate estimation is centered around attention related aspects. The\r\nmain task is to build models for an estimation of a person’s atten-\r\ntional state from the combination and classification of multimodal\r\ndata streams and context information, as well as their evaluation.\r\nFurthermore, the goal is to develop prototypical user interfaces for\r\nAR glasses and to test their usability in different scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 14035
        }
      ],
      "sessionIds": [
        1540
      ],
      "eventIds": []
    },
    {
      "id": 7140,
      "typeId": 11462,
      "title": "Towards Automatic Detection of Misinformation in Online Medical Videos",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Recent years have witnessed a significant increase in the online sharing of medical information, with videos representing a large fraction of such online sources. Previous studies have however shown that more than half of the health-related videos on platforms such as YouTube contain misleading information and biases. Hence, it is crucial to build computational tools that can help evaluate the quality of these videos so that users can obtain accurate information to help inform their decisions. In this study, we focus on the automatic detection of misinformation in YouTube videos. We select prostate cancer videos as our entry point to tackle this problem. The contribution of this paper is twofold. First, we introduce a new dataset consisting of 250 videos related to prostate cancer manually annotated for misinformation. Second, we explore the use of linguistic, acoustic, and user engagement features for the development of classification models to identify misinformation. Using a series of ablation experiments, we show that we can build automatic models with accuracies of up to 74%, corresponding to a 76.5% precision and 73.2% recall for misinformative instances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann  Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 18045
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science Engineering"
            }
          ],
          "personId": 13699
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Department of Urology and Population Health"
            }
          ],
          "personId": 13448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science Engineering"
            }
          ],
          "personId": 18394
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 5093,
      "typeId": 11462,
      "title": "Determining Iconic Gesture Forms based on Entity Image Representation",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Iconic gestures are used to depict physical objects mentioned in speech, and the gesture form is assumed to be based on the image of a given object in a speaker’s mind. Based on this idea, this study proposes a model that learns iconic gesture forms from image representation obtained from pictures of physical entities. First, we collect the pictures of each entity on the web and create an average image representation from them. Subsequently, the average image representation is fed to a fully connected neural network to decide the gesture form. In the model evaluation experiment, our two-step gesture form selection method can classify seven types of gesture forms with over 62% accuracy. Furthermore, we demonstrate an example of gesture generation in a virtual agent system in which our model is used to create a gesture dictionary that assigns a gesture form for each entry word in the dictionary.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 23115
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 18052
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Media Intelligence Laboratories"
            }
          ],
          "personId": 21276
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 6379,
      "typeId": 11462,
      "title": "Speaker-Independent Speech-Driven Visual Speech Synthesis using Domain-Adapted Acoustic Models",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Speech-driven visual speech synthesis involves mapping features extracted from acoustic speech to the corresponding lip anima- tion controls for a face model. This mapping can take many forms, but a powerful approach is to use deep neural networks (DNNs). However, a limitation is the lack of synchronized audio, video, and depth data required to reliably train the DNNs, especially for speaker-independent models. In this paper, we investigate adapting an automatic speech recognition (ASR) acoustic model (AM) for the visual speech synthesis problem. We train the AM on ten thousand hours of audio-only data. The AM is then adapted to the visual speech synthesis domain using ninety hours of synchronized audio-visual speech. Using a subjective assessment test, we compared the performance of the AM-initialized DNN to one with a random initialization. The results show that viewers significantly prefer animations generated from the AM-initialized DNN than the ones generated using the randomly initialized model. This demonstrates that visual speech synthesis can significantly benefit from the powerful representation of speech in the ASR acoustic models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": "Apple Inc."
            }
          ],
          "personId": 20617
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple Inc.",
              "dsl": ""
            }
          ],
          "personId": 22290
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 20291
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 13372
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurick",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 10056
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 8851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 15402
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Cupertino",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 22420
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 7405,
      "typeId": 11462,
      "title": "Estimating Uncertainty in Task-Oriented Dialogue",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Situated multimodal systems that instruct humans need to handle user uncertainties, as expressed in behaviour, and plan their actions accordingly. Speakers’ decision to reformulate or repair previous utterances depends greatly on the listeners’ signals of uncertainty. In this paper, we estimate uncertainty in a situated guided task, as leveraged in non-verbal cues expressed by the listener, and predict that the speaker will reformulate their utterance. We use a corpus where people instruct how to assemble furniture, and extract their multimodal features. While uncertainty is in cases verbally expressed, most instances are expressed non-verbally, which indicates the importance of multimodal approaches. In this work, we present a model for uncertainty estimation. Our findings indicate that uncertainty estimation from non- verbal cues works well, and can exceed human annotator performance when verbal features cannot be perceived.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 13067
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 8903
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 24203
        }
      ],
      "sessionIds": [
        1597
      ],
      "eventIds": []
    },
    {
      "id": 4333,
      "typeId": 11462,
      "title": "VCMNet: Weakly Supervised Learning for Automatic Infant Vocalisation Maturity Analysis",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "Using neural networks to classify infant vocalisations into important subclasses (such as crying versus speech) is an emergent task in speech technology. One of the biggest roadblocks standing in the way of progress lies in the datasets: The performance of a learning model is affected by the labelling quality and size of the dataset used, and infant vocalisation datasets with good quality labels tend to be small. In this paper, we assess the performance of a model for infant VoCalisations Maturity (VCM)  trained with a large dataset annotated automatically using a purpose-built classifier; and a small dataset annotated by highly trained human coders. The two datasets are used in three different training strategies, whose performance is compared against a baseline model. The first training strategy investigates adversarial training, while the second exploits multi-task learning as the neural network trains on both datasets simultaneously. In the final strategy, we integrate adversarial training and multi-task learning. All of the training strategies outperform the baseline, with the adversarial training strategy yielding the best results on the development set.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Group on Language, Audio & Music"
            }
          ],
          "personId": 20628
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Group on Language, Audio & Music"
            }
          ],
          "personId": 23297
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "ENS, EHESS, Centre National de la Recherche Scientifique PSL Research University",
              "dsl": "Laboratoire de Sciences Cognitives et Psycholinguistique, Département d’Études Cognitives"
            }
          ],
          "personId": 18076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": "Communications"
            }
          ],
          "personId": 16330
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "ZD.B Embedded Intelligence for Health Care and Wellbeing",
              "dsl": "University of Augsburg"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "GLAM - Group on Language, Audio & Music",
              "dsl": "Imperial College London"
            }
          ],
          "personId": 22795
        }
      ],
      "sessionIds": [
        2378
      ],
      "eventIds": []
    },
    {
      "id": 6382,
      "typeId": 11480,
      "title": "Robust Spoken Language Understanding with Acoustic and Domain Knowledge",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "Spoken language understanding (SLU) converts user utterances into structured semantic forms. There are still two main issues for SLU: robustness to ASR-errors and the data sparsity of new and extended domains. In this paper, we propose a robust SLU system by leveraging both acoustic and domain knowledge. We extract audio features by training ASR models on a large number of utterances without semantic annotations. For exploiting domain knowledge, we design lexicon features from the domain ontology and propose an error elimination algorithm to help predicted values recovered from ASR-errors. The results of CATSLU challenge show that our systems can outperform all of the other teams across four domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Minhang",
              "dsl": "Shanghai Jiao Tong University"
            }
          ],
          "personId": 19819
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 11328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 19303
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Minhang",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 14933
        }
      ],
      "sessionIds": [
        1926
      ],
      "eventIds": []
    },
    {
      "id": 3822,
      "typeId": 11462,
      "title": "Predicting cognitive load in an emergency simulation based on behavioral and physiological measures",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "The reliable estimation of cognitive load is an integral step towards real-time adaptivity of learning or gaming environments. We introduce a novel and robust machine learning method for cognitive load assessment based on behavioral and physiological measures in a combined within- and cross-participant approach. 47 participants completed different scenarios of a commercially available emergency personnel simulation game realizing several levels of difficulty based on cognitive load. Using interaction metrics, pupil dilation, eye-fixation behavior, and heart rate data, we trained individual, participant-specific forests of extremely randomized trees differentiating between low and high cognitive load. We achieved an average classification accuracy of 72\\%. We then apply these participant-specific classifiers in a novel way, using similarity between participants, normalization, and relative importance of individual features to successfully achieve the same level of classification accuracy in cross-participant classification. These results indicate that a combination of behavioral and physiological indicators allows for reliable prediction of cognitive load in an emergency simulation game, opening up new avenues for adaptivity and interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "LEAD Graduate School and Research Network"
            }
          ],
          "personId": 23807
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Leibniz-Institut für Wissensmedien",
              "dsl": ""
            }
          ],
          "personId": 15021
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "LEAD Graduate School & Research Network"
            }
          ],
          "personId": 20164
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Leibniz-Institut für Wissensmedien",
              "dsl": ""
            }
          ],
          "personId": 9987
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Leibniz-Institut für Wissensmedien",
              "dsl": ""
            }
          ],
          "personId": 23075
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Leibniz-Institut für Wissensmedien",
              "dsl": ""
            }
          ],
          "personId": 13770
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tubingen",
              "institution": "University of Tubingen",
              "dsl": ""
            }
          ],
          "personId": 17789
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Leibniz-Institut für Wissensmedien",
              "dsl": ""
            }
          ],
          "personId": 11476
        }
      ],
      "sessionIds": [
        2470
      ],
      "eventIds": []
    },
    {
      "id": 3311,
      "typeId": 11479,
      "title": "Communicative Signals and Social Contextual Factors in Multimodal Affect Recognition.",
      "trackId": 10659,
      "tags": [],
      "keywords": [],
      "abstract": "One research branch in Affective Computing focuses on using multimodal `emotional' expressions (e.g. facial expressions or non-verbal vocalisations) to automatically detect emotions and affect experienced by persons. The field is increasingly interested in using contextual factors to better infer emotional expressions rather than solely relying on the emotional expressions by themselves. We are interested in expressions that occur in a social context. In our research we plan to investigate how we can; a) use communicative signals that are displayed during interactions to recognise social contextual factors that affect emotion expression and in turn b) predict/recognise what these emotion expressions are most likely communicating considering that context. To achieve this, we formulate three main research questions: I) How do communicative signals such as emotion expressions co-ordinate behaviours and knowledge between interlocutors in interactive settings?, II) Can we use communicative signals to predict/recognise social contextual factors? and III) Can we use social contextual factors and communicative signals to predict what emotion experience is linked to an emotion expression in the same conversation? ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Overijssel ",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 20445
        }
      ],
      "sessionIds": [
        1606
      ],
      "eventIds": []
    },
    {
      "id": 3824,
      "typeId": 11478,
      "title": "The Dyslexperience: Use of Projection Mapping to Simulate Dyslexia",
      "trackId": 10658,
      "tags": [],
      "keywords": [],
      "abstract": "There is a lack of awareness about dyslexia among people in our society. More often than not, there are many misconceptions surrounding the diagnosis of dyslexia, leading to misjudgements and misunderstanding about dyslexics from the workplace to school. This paper presents a multimodal interactive installation designed to communicate the emotional ordeal faced by dyslexics, allowing those who do not understand to see through the lens of those with dyslexia. The main component of this installation is a projection mapping technique used to enhance typography, simulating the experience of dyslexia. Projection mapping makes it possible to create a natural augmented information presentation method on the tangible surface of a specially designed printed book. The user interface combines a color–tracking sensor and a projection to create a camera–projector system. The described system performs tabletop object detection and automatic projection mapping, using page flipping as user interaction. Such a system can be adapted to fit different contexts and installation spaces, for the purpose of education and awareness. There is also the potential to conduct further research with real dyslexia patients. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design, School of Design & Environment"
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design, School of Design & Environment"
            }
          ],
          "personId": 12578
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design,School of Design and Environment"
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design,School of Design and Environment"
            }
          ],
          "personId": 23795
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design, School of Design and Envirionemtn"
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Division of Industrial Design, School of Design and Envirionemtn"
            }
          ],
          "personId": 16119
        }
      ],
      "sessionIds": [
        2403
      ],
      "eventIds": []
    },
    {
      "id": 7666,
      "typeId": 11462,
      "title": "Modeling Team-level Multimodal Dynamics during Multiparty Collaboration",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "We adopt a multimodal approach to investigating team interactions in the context of remote collaborative problem solving (CPS). Our goal is to understand multimodal patterns that emerge and their relation with collaborative outcomes. We measured speech rate, body movement, and galvanic skin response from 101 triads (303 participants) who used video conferencing software to collaboratively solve challenging levels in an educational physics game. We use multi-dimensional recurrence quantification analysis (MdRQA) to quantify patterns of team-level regularity, or repeated patterns of activity in these three modalities. We found that teams exhibit significant regularity above chance baselines. Regularity was unaffected by task factors. but had a quadratic relationship with session time in that it initially increased but then decreased as the session progressed. Importantly, teams that produce more varied behavioral patterns (irregularity) reported higher emotional valence and performed better on a subset of the problem solving tasks. Regularity did not predict arousal or subjective perceptions of the collaboration. We discuss implications of our findings for the design of systems that aim to improve collaborative outcomes by monitoring the ongoing collaboration and intervening accordingly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 15136
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 23992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 16075
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Psychology and Neuroscience"
            }
          ],
          "personId": 24173
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Glendale",
              "institution": "Arizona State University",
              "dsl": ""
            }
          ],
          "personId": 23091
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tallahassee",
              "institution": "Florida State Univiersity",
              "dsl": ""
            }
          ],
          "personId": 14545
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tallahassee",
              "institution": "Florida State University",
              "dsl": ""
            }
          ],
          "personId": 16744
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Glendale",
              "institution": "Arizona State University",
              "dsl": ""
            }
          ],
          "personId": 20924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 20249
        }
      ],
      "sessionIds": [
        1503
      ],
      "eventIds": []
    },
    {
      "id": 6133,
      "typeId": 11462,
      "title": "CorrFeat: Correlation-based Feature Extraction Algorithm using Skin Conductance and Pupil Diameter for Emotion Recognition",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "High-accuracy physiological emotion recognition typically requires participants to wear or attach obtrusive sensors (e.g., Electroencephalograph). To recognize emotions using less obtrusive wearable sensors, we present a novel emotion recognition method that uses only pupil diameter and electrodermal activity. Psychological studies show that these two signals are related to the attention level of humans exposed to visual stimuli. Based on this, we propose a feature extraction algorithm that maximizes the correlation coefficient of pupil diameter and skin conductance response for participants watching the same video clip. To boost performance given limited data, we implement an incremental learning system without a deep architecture to classify emotions along the arousal and valence dimensions. We test our method on the MAHNOB-HCI database, and achieve accuracies of 82.9% and 82.1% for arousal and valence, respectively. Our method outperforms not only state-of-art approaches, but also widely-used traditional and deep learning methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": "Distributed and Interactive Systems"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Intelligent Systems and Pattern Recognition Laboratory"
            }
          ],
          "personId": 11941
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 11635
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Xinhuanet",
              "dsl": "Future Media Convergence Institute"
            }
          ],
          "personId": 13621
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Xinhuanet",
              "dsl": "Future Media Convergence Institute"
            }
          ],
          "personId": 11519
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "CWI",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Intelligent Systems and Pattern Recognition Laboratory"
            }
          ],
          "personId": 13498
        }
      ],
      "sessionIds": [
        1562
      ],
      "eventIds": []
    },
    {
      "id": 24573,
      "typeId": 11465,
      "title": "Invited Talk 2:Multimodal Behavioral Analytics: What Hand Movemenets Reveal about Domain Expertise",
      "trackId": 10680,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Monash University"
            }
          ],
          "personId": 24572
        }
      ],
      "sessionIds": [
        24531
      ],
      "eventIds": []
    },
    {
      "id": 4349,
      "typeId": 11480,
      "title": "Streamlined Decoder for Chinese Spoken Language Understanding",
      "trackId": 10657,
      "tags": [],
      "keywords": [],
      "abstract": "As a critical component of Spoken Dialog System (SDS), spoken language understanding (SLU) attracts a lot of attention, especially for methods based on unaligned data, which require only sentence-level annotation which is easy to obtain. Recently, a new approach has been proposed that utilizes the hierarchical relationship between act-slot-value triples and achieved state-of-the-art result on DSTC2 dataset. However, it ignores the transfer of internal information between hierarchies, which may record the intermediate information of the upper level when making prediction, which may contribute to the prediction of the lower level. So, we propose a novel streamlined decoding structure with attention mechanism, which uses three successively connected RNN to decode act, slot and value respectively. On the first Chinese Audio-Textual Spoken Language Understanding Challenge (CATSLU), our model exceeds state-of-the-art model on an unaligned multi-turn task-oriented Chinese spoken dialogue dataset provided by the contest.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 20937
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 21121
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 20040
        }
      ],
      "sessionIds": [
        1926
      ],
      "eventIds": []
    },
    {
      "id": 24574,
      "typeId": 11465,
      "title": "Invited Talk 1: Understanding emotions and sentiment in multimedia",
      "trackId": 10680,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "USC Institute for Creative Technologies"
            }
          ],
          "personId": 24571
        }
      ],
      "sessionIds": [
        24531
      ],
      "eventIds": []
    },
    {
      "id": 5886,
      "typeId": 11462,
      "title": "TouchPhoto: Enabling Independent Picture Taking and Understanding for Visually-Impaired Users",
      "trackId": 10656,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents TouchPhoto, which provides visual-audio-tactile assistive features to enable visually-impaired users to take and understand photographs independently. A user can take photographs under auditory guidance and record audio tags to aid later recall of\r\nthe photographs’ contents. For comprehension, the user can listen to audio tags embedded in a photograph while touching salient features, e.g., human faces, using an electrovibration display. We conducted two user studies with visually-impaired users, one for picture taking and the other for understanding and recall, in a two-month interval. They considered auditory assistance as very useful for taking and understanding photographs and tactile features as helpful but to a limited extent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongsangbuk-do",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Haptics and Virtual Reality Laboratory"
            }
          ],
          "personId": 12380
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongsangbuk-do",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 8803
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongsangbuk-do",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Haptics and Virtual Reality Laboratory"
            }
          ],
          "personId": 14602
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongbuk",
              "city": "Pohang",
              "institution": "Pohang University of Science and Technology (POSTECH)",
              "dsl": "Computer Science and Engineering / Haptics and Virtual Reality Laboratory"
            }
          ],
          "personId": 18524
        }
      ],
      "sessionIds": [
        2048
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 17410,
      "firstName": "Felix",
      "lastName": "Putze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15364,
      "firstName": "Theodora",
      "lastName": "Chaspari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23557,
      "firstName": "Min Kyu",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21516,
      "firstName": "Jan",
      "lastName": "Ondras",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15375,
      "firstName": "Vaibhav",
      "lastName": "Vaibhav",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23568,
      "firstName": "Abdul Rafey",
      "lastName": "Aftab",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8208,
      "firstName": "Youfang",
      "lastName": "Leng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20505,
      "firstName": "Gary",
      "lastName": "McKeown",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20508,
      "firstName": "Indrani",
      "lastName": "Bhattacharya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24607,
      "firstName": "Mathias",
      "lastName": "Benedek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24608,
      "firstName": "Seiichi",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24609,
      "firstName": "Yung C.",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24611,
      "firstName": "Koki",
      "lastName": "Ijuin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24612,
      "firstName": "Chee Wee",
      "lastName": "Leong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24613,
      "firstName": "Reynold",
      "lastName": "Bailey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24614,
      "firstName": "Koichiro",
      "lastName": "Yoshino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8230,
      "firstName": "zhenhua",
      "lastName": "ling",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24615,
      "firstName": "Kazuhiro",
      "lastName": "Kuwabara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24616,
      "firstName": "XUEZHOU",
      "lastName": "YANG",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24617,
      "firstName": "Harrison",
      "lastName": "Kell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24618,
      "firstName": "Bradley",
      "lastName": "Olson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15402,
      "firstName": "Thibaut",
      "lastName": "Weise",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24619,
      "firstName": "Takashi",
      "lastName": "Kudo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12332,
      "firstName": "Dennis",
      "lastName": "Reidsma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24620,
      "firstName": "Christopher",
      "lastName": "Homan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24621,
      "firstName": "Sonja",
      "lastName": "Walcher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18478,
      "firstName": "Shogo",
      "lastName": "Okada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24622,
      "firstName": "Hiroki",
      "lastName": "Tanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24623,
      "firstName": "Wei",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24624,
      "firstName": "Tyrell",
      "lastName": "Roberts",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24625,
      "firstName": "Cecilia",
      "lastName": "Alm",
      "middleInitial": "Ovesdotter",
      "affiliations": []
    },
    {
      "id": 24626,
      "firstName": "Jiexin",
      "lastName": "Lyu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24627,
      "firstName": "Hiroaki",
      "lastName": "Kazui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24628,
      "firstName": "Shunnosuke",
      "lastName": "Motomura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24629,
      "firstName": "Tsuneo",
      "lastName": "Kato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18485,
      "firstName": "Laura J.",
      "lastName": "Hinojos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24630,
      "firstName": "Taiki",
      "lastName": "Kinoshita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24631,
      "firstName": "Manabu",
      "lastName": "Ikeda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24632,
      "firstName": "Michelle P.",
      "lastName": "Martin-Raugh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14392,
      "firstName": "Ian",
      "lastName": "Williams",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24633,
      "firstName": "Vikram",
      "lastName": "Ramanarayanan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24634,
      "firstName": "Monali",
      "lastName": "Saraf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24635,
      "firstName": "Agnieszka",
      "lastName": "Kulesza",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24636,
      "firstName": "Ankit",
      "lastName": "Bansal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13372,
      "firstName": "Gabriele",
      "lastName": "Fanelli",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17469,
      "firstName": "Martin",
      "lastName": "Schmeißer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24637,
      "firstName": "Zydrune",
      "lastName": "Mladineo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23613,
      "firstName": "Abhinav",
      "lastName": "Dhall",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24638,
      "firstName": "Vrishab",
      "lastName": "Krishna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24639,
      "firstName": "Jie",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19519,
      "firstName": "Laurence",
      "lastName": "Nigay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22591,
      "firstName": "yunhong",
      "lastName": "wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24640,
      "firstName": "Moritz",
      "lastName": "Schult",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11328,
      "firstName": "Chen",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9280,
      "firstName": "Raphael",
      "lastName": "Menges",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24641,
      "firstName": "Laura",
      "lastName": "McCulla",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18498,
      "firstName": "Zhuoming",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24642,
      "firstName": "Ichiro",
      "lastName": "Umata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24643,
      "firstName": "Katrina",
      "lastName": "Roohr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24644,
      "firstName": "Hiroyoshi",
      "lastName": "Adachi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12356,
      "firstName": "Daniyal",
      "lastName": "Akbari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18500,
      "firstName": "Wooseung",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24645,
      "firstName": "Jiawen",
      "lastName": "Yao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24646,
      "firstName": "shiwei",
      "lastName": "cheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22598,
      "firstName": "Abdenaceur",
      "lastName": "Abdouni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24647,
      "firstName": "Hung-Hsuan",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24648,
      "firstName": "Masato",
      "lastName": "Fukuda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24649,
      "firstName": "Regina",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24650,
      "firstName": "Raymond",
      "lastName": "Ptucha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24651,
      "firstName": "Yao",
      "lastName": "Qian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24652,
      "firstName": "yang",
      "lastName": "zou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24653,
      "firstName": "Rutuja",
      "lastName": "Ubale",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24654,
      "firstName": "Satoshi",
      "lastName": "Nakamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9296,
      "firstName": "Niklas",
      "lastName": "Emanuel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20563,
      "firstName": "Robin",
      "lastName": "Héron",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14422,
      "firstName": "Johannes",
      "lastName": "Schöning",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12380,
      "firstName": "Jongho",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18524,
      "firstName": "Seungmoon",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10345,
      "firstName": "Carlos",
      "lastName": "Busso",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9322,
      "firstName": "Zhuolin",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24684,
      "firstName": "Wen",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 13421,
      "firstName": "Marvin",
      "lastName": "Lange",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 24686,
      "firstName": "Helen",
      "lastName": "Meng",
      "middleInitial": "Mei Ling",
      "affiliations": []
    },
    {
      "id": 11376,
      "firstName": "Kevin",
      "lastName": "El Haddad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10353,
      "firstName": "Xiaotian",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16499,
      "firstName": "Erik",
      "lastName": "Wolf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9334,
      "firstName": "Go",
      "lastName": "Miura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18565,
      "firstName": "Jaeyon",
      "lastName": "Hwang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21638,
      "firstName": "Ameneh",
      "lastName": "Shamekhi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13448,
      "firstName": "Stacy",
      "lastName": "Loeb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20617,
      "firstName": "Ahmed",
      "lastName": "Hussen Abdelaziz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18570,
      "firstName": "Kubra",
      "lastName": "Cengiz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13452,
      "firstName": "Izem",
      "lastName": "Tengiz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11408,
      "firstName": "Zhiyong",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8339,
      "firstName": "Teruhisa",
      "lastName": "Misu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20628,
      "firstName": "Najla",
      "lastName": "Al Futaisi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14485,
      "firstName": "Kaixin",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21658,
      "firstName": "Tobias",
      "lastName": "Höllerer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24731,
      "firstName": "Naveen",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 17564,
      "firstName": "Aishat",
      "lastName": "Aloba",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24732,
      "firstName": "Shiguang",
      "lastName": "Shan",
      "affiliations": []
    },
    {
      "id": 15517,
      "firstName": "Dequan",
      "lastName": "Zheng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24733,
      "firstName": "Shuang",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24734,
      "firstName": "Mengyue",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 24735,
      "firstName": "Khiet",
      "lastName": "Truong",
      "affiliations": []
    },
    {
      "id": 13472,
      "firstName": "David",
      "lastName": "Traum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24736,
      "firstName": "Crowley",
      "lastName": "James",
      "affiliations": []
    },
    {
      "id": 24738,
      "firstName": "Alexander",
      "lastName": "Waibel",
      "affiliations": []
    },
    {
      "id": 24739,
      "firstName": "Hsiao-Wuen",
      "lastName": "Hon",
      "affiliations": []
    },
    {
      "id": 24740,
      "firstName": "Zhengyou",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 16549,
      "firstName": "Steve",
      "lastName": "Brenneman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23723,
      "firstName": "Jiaming",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14508,
      "firstName": "Stephanie",
      "lastName": "Arevalo Arboleda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10419,
      "firstName": "Stanislaw",
      "lastName": "Miller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23731,
      "firstName": "Li",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18614,
      "firstName": "Martha",
      "lastName": "Janka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24759,
      "firstName": "Xilin",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 13497,
      "firstName": "Byung Cheol",
      "lastName": "Song",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13498,
      "firstName": "Pablo",
      "lastName": "Cesar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10428,
      "firstName": "Chengqing",
      "lastName": "Zong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9404,
      "firstName": "Xinru",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15548,
      "firstName": "Jin-hwan",
      "lastName": "Oh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15549,
      "firstName": "Kalin",
      "lastName": "Stefanov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22718,
      "firstName": "Sena Busra Yengec",
      "lastName": "Tasdemir",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9406,
      "firstName": "Jeonghwa",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21702,
      "firstName": "Theodore",
      "lastName": "Jensen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14536,
      "firstName": "weixin",
      "lastName": "li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13518,
      "firstName": "Md Abdullah Al",
      "lastName": "Fahim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8398,
      "firstName": "Maite",
      "lastName": "Frutos-Pascual",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21712,
      "firstName": "Marius",
      "lastName": "Pfeil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14545,
      "firstName": "Chen",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11476,
      "firstName": "Peter",
      "lastName": "Gerjets",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13526,
      "firstName": "Khalil",
      "lastName": "Anderson",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 15576,
      "firstName": "Dong Yoon",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16610,
      "firstName": "Sara",
      "lastName": "Wolf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14563,
      "firstName": "Qian",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23795,
      "firstName": "Ai Ling,",
      "lastName": "Ng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11511,
      "firstName": "Seongwon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19707,
      "firstName": "Setareh",
      "lastName": "Nasihati Gilani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11519,
      "firstName": "Xintong",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23807,
      "firstName": "Tobias",
      "lastName": "Appel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11525,
      "firstName": "Ankit Parag",
      "lastName": "Shah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8457,
      "firstName": "Françoise",
      "lastName": "Detienne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14602,
      "firstName": "Hanseul",
      "lastName": "Cho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9482,
      "firstName": "Jie",
      "lastName": "Xiong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22795,
      "firstName": "Bjorn",
      "lastName": "Schuller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13580,
      "firstName": "Ross",
      "lastName": "Buck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20750,
      "firstName": "Timothy",
      "lastName": "Bickmore",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14614,
      "firstName": "Hao",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12578,
      "firstName": "Zi Fong",
      "lastName": "Yong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8486,
      "firstName": "Jia'ning",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18727,
      "firstName": "Sai Srinadhu",
      "lastName": "Katta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16690,
      "firstName": "Jake",
      "lastName": "Harrison",
      "middleInitial": "Michael",
      "affiliations": []
    },
    {
      "id": 16692,
      "firstName": "Felix",
      "lastName": "Kroll",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18740,
      "firstName": "Divesh",
      "lastName": "Lala",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13621,
      "firstName": "Chen",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20794,
      "firstName": "Nicolai",
      "lastName": "Schütz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13631,
      "firstName": "Devendra",
      "lastName": "Yadav",
      "middleInitial": "Pratap",
      "affiliations": []
    },
    {
      "id": 19777,
      "firstName": "Shital",
      "lastName": "Shah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12611,
      "firstName": "Vahid",
      "lastName": "Naghashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9539,
      "firstName": "Koji",
      "lastName": "Inoue",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16711,
      "firstName": "Mimansa",
      "lastName": "Jaiswal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9553,
      "firstName": "Adriana",
      "lastName": "Camacho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15705,
      "firstName": "Emily",
      "lastName": "Mower Provost",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16734,
      "firstName": "Sinhwa",
      "lastName": "Kang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11623,
      "firstName": "David",
      "lastName": "Novick",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16744,
      "firstName": "Valerie",
      "lastName": "Shute",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19819,
      "firstName": "Hao",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20844,
      "firstName": "Kerstin",
      "lastName": "Bub",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13676,
      "firstName": "Aidan",
      "lastName": "Feeney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11635,
      "firstName": "Abdallah",
      "lastName": "El Ali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12664,
      "firstName": "Sudhakar",
      "lastName": "Sah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17789,
      "firstName": "Enkelejda",
      "lastName": "Kasneci",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13699,
      "firstName": "Veronica",
      "lastName": "Perez-Rosas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16788,
      "firstName": "Elisabeth",
      "lastName": "André",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13720,
      "firstName": "Yi",
      "lastName": "Ding",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10650,
      "firstName": "Dong",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15778,
      "firstName": "Mohammad Maifi Hasan",
      "lastName": "Khan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19878,
      "firstName": "Yuning",
      "lastName": "Qiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20903,
      "firstName": "Sibel",
      "lastName": "Halfon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19881,
      "firstName": "Batıkan",
      "lastName": "Türkmen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14761,
      "firstName": "Jeffrey",
      "lastName": "Girard",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 22954,
      "firstName": "Sharon",
      "lastName": "Oviatt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23992,
      "firstName": "Angela",
      "lastName": "Stewart",
      "middleInitial": "E.B.",
      "affiliations": []
    },
    {
      "id": 9657,
      "firstName": "Dees",
      "lastName": "Postma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22971,
      "firstName": "Mira",
      "lastName": "Sarkis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20924,
      "firstName": "Nicholas",
      "lastName": "Duran",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 21950,
      "firstName": "Jean-Luc",
      "lastName": "Lugrin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11711,
      "firstName": "Matthis",
      "lastName": "Laudan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14783,
      "firstName": "Yoori",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20937,
      "firstName": "heyan",
      "lastName": "huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13770,
      "firstName": "Manuel",
      "lastName": "Ninaus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20948,
      "firstName": "Robby",
      "lastName": "van Delden",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8663,
      "firstName": "Katharina",
      "lastName": "Weitz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19933,
      "firstName": "Myeongsoo",
      "lastName": "Shin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11750,
      "firstName": "Christophe",
      "lastName": "Kolski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10730,
      "firstName": "Leili",
      "lastName": "Tavabi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11758,
      "firstName": "Hubert",
      "lastName": "Cecotti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8687,
      "firstName": "Toyoaki",
      "lastName": "Nishida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16882,
      "firstName": "Gian-Luca",
      "lastName": "Savino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9715,
      "firstName": "Thorsten",
      "lastName": "Kluss",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15875,
      "firstName": "Kazunori",
      "lastName": "Komatani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13829,
      "firstName": "Sambit",
      "lastName": "Praharaj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12808,
      "firstName": "Ruben",
      "lastName": "Schlagowski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23050,
      "firstName": "Theodore",
      "lastName": "Dubiel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9747,
      "firstName": "Jianwu",
      "lastName": "Dang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11802,
      "firstName": "suowei",
      "lastName": "wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19999,
      "firstName": "Meiru",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23072,
      "firstName": "Zakaria",
      "lastName": "Aldeneh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13857,
      "firstName": "Vineet",
      "lastName": "Mehta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20001,
      "firstName": "Cody",
      "lastName": "Poultney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23075,
      "firstName": "Korbinian",
      "lastName": "Moeller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23091,
      "firstName": "Amanda",
      "lastName": "Michaels",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20020,
      "firstName": "Zijian",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23094,
      "firstName": "Saturnino",
      "lastName": "Luz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8758,
      "firstName": "Andrzej",
      "lastName": "Duda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8759,
      "firstName": "Sandeep Nallan",
      "lastName": "Chakravarthula",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8764,
      "firstName": "Barry",
      "lastName": "Giesbrecht",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8767,
      "firstName": "Chandan",
      "lastName": "Kumar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9793,
      "firstName": "Rieke",
      "lastName": "Leder",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17991,
      "firstName": "Ognjen",
      "lastName": "Rudovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20040,
      "firstName": "Puhai",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23115,
      "firstName": "Fumio",
      "lastName": "Nihei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21067,
      "firstName": "Marcelo",
      "lastName": "Worsley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23123,
      "firstName": "Ilhan",
      "lastName": "Aslan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14933,
      "firstName": "Kai",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20058,
      "firstName": "Christoph",
      "lastName": "Riedl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20066,
      "firstName": "Mallory",
      "lastName": "Morgan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8803,
      "firstName": "Yongjae",
      "lastName": "Yoo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15973,
      "firstName": "Jia",
      "lastName": "Jia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24173,
      "firstName": "Caroline",
      "lastName": "Reinhardt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11892,
      "firstName": "Philipp",
      "lastName": "Müller",
      "middleInitial": "Matthias",
      "affiliations": []
    },
    {
      "id": 15991,
      "firstName": "Bert-Jan",
      "lastName": "van Beijnum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18040,
      "firstName": "Yulan",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23162,
      "firstName": "Qingqing",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22139,
      "firstName": "Kenji",
      "lastName": "Tanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18045,
      "firstName": "Rui",
      "lastName": "Hou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11904,
      "firstName": "Eda Aydın",
      "lastName": "Oktay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21121,
      "firstName": "Xianling",
      "lastName": "Mao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24193,
      "firstName": "Richard",
      "lastName": "Radke",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 18052,
      "firstName": "Yukiko",
      "lastName": "Nakano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24203,
      "firstName": "Joakim",
      "lastName": "Gustafson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9868,
      "firstName": "Mahdokht",
      "lastName": "Afravi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10897,
      "firstName": "Yusuf",
      "lastName": "Albayram",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8851,
      "firstName": "Nick",
      "lastName": "Apostoloff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22171,
      "firstName": "Sixia",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11931,
      "firstName": "Jens",
      "lastName": "Gerken",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18076,
      "firstName": "Alejandrina",
      "lastName": "Cristia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19100,
      "firstName": "Chen",
      "lastName": "Min",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23204,
      "firstName": "Steven",
      "lastName": "Kowalzik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11941,
      "firstName": "Tianyi",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17064,
      "firstName": "Mahmoud",
      "lastName": "Alismail",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8872,
      "firstName": "Steffen",
      "lastName": "Staab",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16042,
      "firstName": "Orestis",
      "lastName": "Georgiou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15021,
      "firstName": "Natalia",
      "lastName": "Sevcenko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17071,
      "firstName": "Marc Erich",
      "lastName": "Latoschik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13999,
      "firstName": "Shuqiang",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17075,
      "firstName": "James",
      "lastName": "Kennedy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8887,
      "firstName": "Shugao",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21178,
      "firstName": "Michael",
      "lastName": "Foley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22204,
      "firstName": "Albert Ali",
      "lastName": "Salah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23231,
      "firstName": "Xuancai",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8896,
      "firstName": "Run",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9921,
      "firstName": "Jihoon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20164,
      "firstName": "Franz",
      "lastName": "Wortha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9927,
      "firstName": "Aiwen",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8903,
      "firstName": "Andre",
      "lastName": "Pereira",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16075,
      "firstName": "Mary",
      "lastName": "Amon",
      "middleInitial": "Jean",
      "affiliations": []
    },
    {
      "id": 13006,
      "firstName": "Fahim",
      "lastName": "Salim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10962,
      "firstName": "Emil",
      "lastName": "Coman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11986,
      "firstName": "Scott",
      "lastName": "MacKenzie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14035,
      "firstName": "Lisa-Marie",
      "lastName": "Vortmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13012,
      "firstName": "Chris",
      "lastName": "Zimmerer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13020,
      "firstName": "Stéphane",
      "lastName": "Safin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16093,
      "firstName": "Jonas",
      "lastName": "Braasch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12006,
      "firstName": "Ernst",
      "lastName": "Kruijff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14057,
      "firstName": "Simon",
      "lastName": "Flutura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16107,
      "firstName": "Metehan",
      "lastName": "Doyran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10992,
      "firstName": "zhengyin",
      "lastName": "du",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22260,
      "firstName": "Zihe",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16119,
      "firstName": "Yuta",
      "lastName": "Nakayama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18171,
      "firstName": "Jionghao",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23297,
      "firstName": "Zixing",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23298,
      "firstName": "Chris",
      "lastName": "Creed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15106,
      "firstName": "Susana Garcia",
      "lastName": "Valesco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9987,
      "firstName": "Katerina",
      "lastName": "Tsarava",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14083,
      "firstName": "Aaron",
      "lastName": "Rodriguez",
      "middleInitial": "E",
      "affiliations": []
    },
    {
      "id": 24330,
      "firstName": "Yale",
      "lastName": "Song",
      "affiliations": []
    },
    {
      "id": 13067,
      "firstName": "Dimosthenis",
      "lastName": "Kontogiorgos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21261,
      "firstName": "Chaitanya",
      "lastName": "Ahuja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16142,
      "firstName": "Céline",
      "lastName": "Coutrix",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22290,
      "firstName": "Barry-John",
      "lastName": "Theobald",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20245,
      "firstName": "Eric",
      "lastName": "Lecolinet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20249,
      "firstName": "Sidney",
      "lastName": "D'Mello",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21276,
      "firstName": "Ryo",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 48925,
      "firstName": "Preethi",
      "lastName": "Vaidyanathan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15136,
      "firstName": "Lucca",
      "lastName": "Eloy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17187,
      "firstName": "Liping",
      "lastName": "Jing",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12076,
      "firstName": "Hashini",
      "lastName": "Senaratne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15156,
      "firstName": "Yosra",
      "lastName": "Rekik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19253,
      "firstName": "Jonathan",
      "lastName": "Gratch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19263,
      "firstName": "Walid",
      "lastName": "Merrad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13121,
      "firstName": "Dayana",
      "lastName": "Markhabayeva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20291,
      "firstName": "Justin",
      "lastName": "Binder",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10056,
      "firstName": "Paul",
      "lastName": "Dixon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9037,
      "firstName": "Fasih",
      "lastName": "Haider",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12119,
      "firstName": "Tom",
      "lastName": "Bullock",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24411,
      "firstName": "Guee-Sang",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 24412,
      "firstName": "Hongwei",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 24413,
      "firstName": "Soo-Hyung",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 24414,
      "firstName": "Lang",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 24415,
      "firstName": "Tien",
      "lastName": "Dang",
      "middleInitial": "Xuan",
      "affiliations": []
    },
    {
      "id": 24416,
      "firstName": "Liqin",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 24417,
      "firstName": "Hyung-Jeong",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24418,
      "firstName": "Shuangping",
      "lastName": "Gong",
      "affiliations": []
    },
    {
      "id": 24419,
      "firstName": "Kai",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24420,
      "firstName": "Huajuan",
      "lastName": "Mao",
      "affiliations": []
    },
    {
      "id": 24421,
      "firstName": "Jun",
      "lastName": "Du",
      "affiliations": []
    },
    {
      "id": 24422,
      "firstName": "Yihang",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24423,
      "firstName": "Van",
      "lastName": "Huynh",
      "middleInitial": "Thong",
      "affiliations": []
    },
    {
      "id": 19303,
      "firstName": "Su",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24424,
      "firstName": "Anran",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 24425,
      "firstName": "Yu",
      "lastName": "Qiao",
      "affiliations": []
    },
    {
      "id": 21353,
      "firstName": "Carolin",
      "lastName": "Stellmacher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24426,
      "firstName": "Xiaojiang",
      "lastName": "Peng",
      "affiliations": []
    },
    {
      "id": 12139,
      "firstName": "Zhanhua",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24427,
      "firstName": "Weihui",
      "lastName": "Dai",
      "affiliations": []
    },
    {
      "id": 24428,
      "firstName": "Yi",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 20332,
      "firstName": "Abishek",
      "lastName": "Sriramulu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24429,
      "firstName": "Heming",
      "lastName": "Du",
      "affiliations": []
    },
    {
      "id": 24430,
      "firstName": "Liang",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 24431,
      "firstName": "Yue",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 24432,
      "firstName": "Tianyu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24433,
      "firstName": "Wei",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 24434,
      "firstName": "Minhao",
      "lastName": "Fan",
      "affiliations": []
    },
    {
      "id": 24435,
      "firstName": "Tom",
      "lastName": "Gedeon",
      "affiliations": []
    },
    {
      "id": 24436,
      "firstName": "Yougen",
      "lastName": "Yuan",
      "affiliations": []
    },
    {
      "id": 24437,
      "firstName": "Yue",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 24438,
      "firstName": "Soo-Hyung",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 24439,
      "firstName": "Zhiguang",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 24440,
      "firstName": "Jianming",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 24441,
      "firstName": "Yi",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 24442,
      "firstName": "Yanan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24443,
      "firstName": "Xin",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 24444,
      "firstName": "Bin",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 24445,
      "firstName": "Charles",
      "lastName": "Boncelet",
      "affiliations": []
    },
    {
      "id": 24446,
      "firstName": "Kenneth",
      "lastName": "Barner",
      "affiliations": []
    },
    {
      "id": 24447,
      "firstName": "Guee-Sang",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 24448,
      "firstName": "Hyung-Jeong",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24449,
      "firstName": "Peng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24450,
      "firstName": "Lei",
      "lastName": "Xie",
      "affiliations": []
    },
    {
      "id": 24451,
      "firstName": "Tom",
      "lastName": "Gedeon",
      "affiliations": []
    },
    {
      "id": 24452,
      "firstName": "Xingxun",
      "lastName": "Jiang",
      "affiliations": []
    },
    {
      "id": 24453,
      "firstName": "Shreya",
      "lastName": "Ghosh",
      "affiliations": []
    },
    {
      "id": 24454,
      "firstName": "Chuangao",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 24455,
      "firstName": "Jianfei",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24456,
      "firstName": "Kai",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24457,
      "firstName": "Jiateng",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 24458,
      "firstName": "Wenming",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 24459,
      "firstName": "Jinlong",
      "lastName": "Jiao",
      "affiliations": []
    },
    {
      "id": 24460,
      "firstName": "Sunan",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 14220,
      "firstName": "Xu",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24461,
      "firstName": "Wan",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 24462,
      "firstName": "Roland",
      "lastName": "Goecke",
      "affiliations": []
    },
    {
      "id": 24463,
      "firstName": "Cheng",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 24464,
      "firstName": "Dongyan",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 9104,
      "firstName": "Deepali",
      "lastName": "Aneja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14224,
      "firstName": "Yuyun",
      "lastName": "Hua",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24465,
      "firstName": "Abhinav",
      "lastName": "Dhall",
      "affiliations": []
    },
    {
      "id": 24466,
      "firstName": "Yuan",
      "lastName": "Zong",
      "affiliations": []
    },
    {
      "id": 24467,
      "firstName": "Ziping",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 24468,
      "firstName": "Yusuke",
      "lastName": "Uchida",
      "affiliations": []
    },
    {
      "id": 22420,
      "firstName": "Sachin",
      "lastName": "Kajareker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24471,
      "firstName": "Kaipeng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24472,
      "firstName": "Da",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 24473,
      "firstName": "Xiaojiang",
      "lastName": "Peng",
      "affiliations": []
    },
    {
      "id": 24474,
      "firstName": "Xiaojiang",
      "lastName": "Peng",
      "affiliations": []
    },
    {
      "id": 24475,
      "firstName": "Jiaolong",
      "lastName": "Fu",
      "affiliations": []
    },
    {
      "id": 24476,
      "firstName": "Kaipeng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13212,
      "firstName": "Soumia",
      "lastName": "Dermouche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24477,
      "firstName": "Tingting",
      "lastName": "Ge",
      "affiliations": []
    },
    {
      "id": 24478,
      "firstName": "Hengshun",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 24479,
      "firstName": "Karan",
      "lastName": "Singla",
      "affiliations": []
    },
    {
      "id": 11167,
      "firstName": "Xinyu",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24480,
      "firstName": "Meilin",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 24481,
      "firstName": "Yu",
      "lastName": "Qiao",
      "affiliations": []
    },
    {
      "id": 10145,
      "firstName": "Jaime",
      "lastName": "Maldonado",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15266,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24483,
      "firstName": "Xiaohua",
      "lastName": "Hu",
      "affiliations": []
    },
    {
      "id": 15267,
      "firstName": "Nicole",
      "lastName": "Andelic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24484,
      "firstName": "Da",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 16292,
      "firstName": "Xinhang",
      "lastName": "Song",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24485,
      "firstName": "Yalda",
      "lastName": "Uhls",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 24486,
      "firstName": "Yu",
      "lastName": "Qiao",
      "affiliations": []
    },
    {
      "id": 24487,
      "firstName": "Shri",
      "lastName": "Narayanan",
      "affiliations": []
    },
    {
      "id": 21415,
      "firstName": "Sixian",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24488,
      "firstName": "Jianfei",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24489,
      "firstName": "Kai",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24490,
      "firstName": "Yanan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24491,
      "firstName": "Wanchuang",
      "lastName": "Xia",
      "affiliations": []
    },
    {
      "id": 24492,
      "firstName": "Keiichiro",
      "lastName": "Hoashi",
      "affiliations": []
    },
    {
      "id": 24493,
      "firstName": "Jianming",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 24494,
      "firstName": "Youhui",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24495,
      "firstName": "Lanqing",
      "lastName": "Dong",
      "affiliations": []
    },
    {
      "id": 24496,
      "firstName": "Weihui",
      "lastName": "Dai",
      "affiliations": []
    },
    {
      "id": 24497,
      "firstName": "Xiaoming",
      "lastName": "Jing",
      "affiliations": []
    },
    {
      "id": 24498,
      "firstName": "Ming",
      "lastName": "Cheng",
      "affiliations": []
    },
    {
      "id": 24499,
      "firstName": "Wilton",
      "lastName": "Fok",
      "middleInitial": "W.T.",
      "affiliations": []
    },
    {
      "id": 24500,
      "firstName": "Ming",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 24501,
      "firstName": "Victor",
      "lastName": "Martinez",
      "affiliations": []
    },
    {
      "id": 24502,
      "firstName": "Yuanyuan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24503,
      "firstName": "Krishna",
      "lastName": "Somandepalli",
      "affiliations": []
    },
    {
      "id": 24504,
      "firstName": "Debin",
      "lastName": "Meng",
      "affiliations": []
    },
    {
      "id": 11198,
      "firstName": "Tiejun",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10175,
      "firstName": "Tatsuya",
      "lastName": "Kawahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14277,
      "firstName": "Yaser",
      "lastName": "Sheikh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16327,
      "firstName": "Di",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18375,
      "firstName": "Rory",
      "lastName": "Clark",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10186,
      "firstName": "Yuki",
      "lastName": "Hirano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16330,
      "firstName": "Anne",
      "lastName": "Warlaumont",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16333,
      "firstName": "Daniel",
      "lastName": "McDuff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14286,
      "firstName": "Rosalind",
      "lastName": "Picard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10191,
      "firstName": "Vasu",
      "lastName": "Sharma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16346,
      "firstName": "Chengda",
      "lastName": "Tang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21466,
      "firstName": "Catherine",
      "lastName": "Pelachaud",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20442,
      "firstName": "Matthew",
      "lastName": "Turk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18394,
      "firstName": "Rada",
      "lastName": "Mihalcea",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23516,
      "firstName": "Haruto",
      "lastName": "Nishimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20445,
      "firstName": "Michel-Pierre",
      "lastName": "Jansen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21469,
      "firstName": "Mingtong",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19426,
      "firstName": "Chaohong",
      "lastName": "Tan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22500,
      "firstName": "Louis-philippe",
      "lastName": "Morency",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20452,
      "firstName": "Mohammad",
      "lastName": "Soleymani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19429,
      "firstName": "Brandon",
      "lastName": "Huynh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10216,
      "firstName": "Lingyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24571,
      "firstName": "Mohammad",
      "lastName": "Soleymani",
      "affiliations": []
    },
    {
      "id": 24572,
      "firstName": "Sharon",
      "lastName": "Oviatt",
      "affiliations": []
    },
    {
      "id": 23548,
      "firstName": "Brooke",
      "lastName": "Foucault Welles",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 46,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 07:46:42+00"
  }
}