{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10108,
    "shortName": "IUI",
    "displayShortName": "",
    "year": 2024,
    "startDate": 1710720000000,
    "endDate": 1710979200000,
    "fullName": "29th annual ACM conference on Intelligent User Interfaces",
    "url": "https://iui.acm.org/2024/",
    "location": "Greenville, South Carolina",
    "timeZoneOffset": -240,
    "timeZoneName": "America/New_York",
    "logoUrl": "https://files.sigchi.org/conference/logo/10108/7569f632-799e-3258-4605-b3babdc96d7a.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10108/830400cc-e7ce-5816-5984-528100d331a7.html",
    "name": "IUI 2024"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 37,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": true,
    "isRegistrationEnabled": true,
    "publicationDate": "2024-03-18 21:13:00+00"
  },
  "sponsors": [
    {
      "id": 10478,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10108/logo/6a1c5d9c-1af6-419a-eb51-be8da979c5d8.png",
      "levelId": 10286,
      "url": "https://iui.acm.org/2024/sponsorship.html",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10480,
      "name": "ACM SIGAI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10108/logo/59cfaa14-74cb-cd3a-7719-71d1edab43ea.png",
      "levelId": 10286,
      "url": "https://iui.acm.org/2024/sponsorship.html",
      "order": 1,
      "extraPadding": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10286,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    },
    {
      "id": 10295,
      "name": "Platinum",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10296,
      "name": "Gold",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10297,
      "name": "Silver",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10298,
      "name": "Bronze",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10299,
      "name": "Conference Sponsor",
      "rank": 6,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10263,
      "name": "Hotel Main - 1st floor",
      "roomIds": [
        11433,
        11438,
        11439,
        11440,
        11443
      ]
    },
    {
      "id": 10264,
      "name": "Hotel Workshop level - 2nd floor",
      "roomIds": [
        11434,
        11442
      ]
    },
    {
      "id": 10265,
      "name": "Bank of America",
      "roomIds": [
        11437
      ]
    }
  ],
  "rooms": [
    {
      "id": 11433,
      "name": "Reedy Hall",
      "setup": "THEATRE",
      "typeId": 13294,
      "capacity": 350,
      "note": ""
    },
    {
      "id": 11434,
      "name": "Loft",
      "setup": "ROUNDS",
      "typeId": 13291,
      "capacity": 34,
      "note": ""
    },
    {
      "id": 11437,
      "name": "BofA",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "capacity": 50,
      "note": ""
    },
    {
      "id": 11438,
      "name": "Reedy 1",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "capacity": 50,
      "note": ""
    },
    {
      "id": 11439,
      "name": "Reedy 2",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "capacity": 50,
      "note": ""
    },
    {
      "id": 11440,
      "name": "Reedy 3",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "capacity": 50,
      "note": ""
    },
    {
      "id": 11442,
      "name": "Soft",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "capacity": 30,
      "note": ""
    },
    {
      "id": 11443,
      "name": "Pre-function area",
      "setup": "SPECIAL",
      "typeId": 13298,
      "note": ""
    },
    {
      "id": 11523,
      "name": "Jasmine",
      "setup": "CLASSROOM",
      "typeId": 13297,
      "note": ""
    }
  ],
  "tracks": [
    {
      "id": 12508,
      "typeId": 13294
    },
    {
      "id": 12509,
      "name": "IUI 2022 Workshops and Tutorials"
    },
    {
      "id": 12510,
      "name": "IUI 2024 Doctoral Consortium",
      "typeId": 13291
    },
    {
      "id": 12511,
      "name": "IUI 2024 Workshops",
      "typeId": 13297
    },
    {
      "id": 12512,
      "name": "IUI 2024 Tutorials",
      "typeId": 13289
    },
    {
      "id": 12513,
      "name": "IUI 2022 Posters and Demos"
    },
    {
      "id": 12514,
      "name": "IUI 2024 Papers",
      "typeId": 13294
    },
    {
      "id": 12515,
      "name": "IUI 2024 Posters and Demos",
      "typeId": 13290
    },
    {
      "id": 12520,
      "typeId": 13292
    },
    {
      "id": 12522,
      "typeId": 13292
    }
  ],
  "contentTypes": [
    {
      "id": 13289,
      "name": "Course",
      "displayName": "Courses",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 13290,
      "name": "Demo",
      "displayName": "Demos",
      "color": "#006d2c",
      "duration": 5
    },
    {
      "id": 13291,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13292,
      "name": "Event",
      "displayName": "Events",
      "color": "#ffc034",
      "duration": 0
    },
    {
      "id": 13293,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 13294,
      "name": "Paper",
      "displayName": "Papers",
      "color": "#0d42cc",
      "duration": 20
    },
    {
      "id": 13295,
      "name": "Poster",
      "displayName": "Posters",
      "color": "#ff7a00",
      "duration": 5
    },
    {
      "id": 13296,
      "name": "Work-in-Progress",
      "displayName": "Works-In-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 13297,
      "name": "Workshop",
      "displayName": "Workshops",
      "color": "#f60000",
      "duration": 240
    },
    {
      "id": 13298,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    }
  ],
  "timeSlots": [
    {
      "id": 13652,
      "type": "SESSION",
      "startDate": 1710842700000,
      "endDate": 1710847500000
    },
    {
      "id": 13653,
      "type": "BREAK",
      "startDate": 1710847500000,
      "endDate": 1710848400000
    },
    {
      "id": 13654,
      "type": "SESSION",
      "startDate": 1710848400000,
      "endDate": 1710853200000
    },
    {
      "id": 13655,
      "type": "LUNCH",
      "startDate": 1710853200000,
      "endDate": 1710858600000
    },
    {
      "id": 13656,
      "type": "SESSION",
      "startDate": 1710858600000,
      "endDate": 1710863400000
    },
    {
      "id": 13657,
      "type": "BREAK",
      "startDate": 1710863400000,
      "endDate": 1710864600000
    },
    {
      "id": 13658,
      "type": "SESSION",
      "startDate": 1710864600000,
      "endDate": 1710869400000
    },
    {
      "id": 13659,
      "type": "SESSION",
      "startDate": 1710929100000,
      "endDate": 1710933900000
    },
    {
      "id": 13660,
      "type": "BREAK",
      "startDate": 1710933900000,
      "endDate": 1710934800000
    },
    {
      "id": 13661,
      "type": "SESSION",
      "startDate": 1710934800000,
      "endDate": 1710939600000
    },
    {
      "id": 13662,
      "type": "LUNCH",
      "startDate": 1710939600000,
      "endDate": 1710945000000
    },
    {
      "id": 13663,
      "type": "SESSION",
      "startDate": 1710945000000,
      "endDate": 1710949800000
    },
    {
      "id": 13664,
      "type": "BREAK",
      "startDate": 1710949800000,
      "endDate": 1710951000000
    },
    {
      "id": 13665,
      "type": "SESSION",
      "startDate": 1710951000000,
      "endDate": 1710955800000
    },
    {
      "id": 13666,
      "type": "SESSION",
      "startDate": 1711015500000,
      "endDate": 1711020300000
    },
    {
      "id": 13667,
      "type": "BREAK",
      "startDate": 1711020300000,
      "endDate": 1711021200000
    },
    {
      "id": 13668,
      "type": "SESSION",
      "startDate": 1711021200000,
      "endDate": 1711026000000
    },
    {
      "id": 13669,
      "type": "LUNCH",
      "startDate": 1711026000000,
      "endDate": 1711031400000
    },
    {
      "id": 13670,
      "type": "SESSION",
      "startDate": 1711031400000,
      "endDate": 1711036200000
    },
    {
      "id": 13671,
      "type": "BREAK",
      "startDate": 1711036200000,
      "endDate": 1711037400000
    },
    {
      "id": 13682,
      "type": "SESSION",
      "startDate": 1710752400000,
      "endDate": 1710757800000
    },
    {
      "id": 13683,
      "type": "BREAK",
      "startDate": 1710757800000,
      "endDate": 1710758700000
    },
    {
      "id": 13684,
      "type": "SESSION",
      "startDate": 1710758700000,
      "endDate": 1710765000000
    },
    {
      "id": 13685,
      "type": "LUNCH",
      "startDate": 1710765000000,
      "endDate": 1710770400000
    },
    {
      "id": 13686,
      "type": "SESSION",
      "startDate": 1710770400000,
      "endDate": 1710776700000
    },
    {
      "id": 13687,
      "type": "BREAK",
      "startDate": 1710776700000,
      "endDate": 1710777600000
    },
    {
      "id": 13688,
      "type": "SESSION",
      "startDate": 1710777600000,
      "endDate": 1710783000000
    },
    {
      "id": 13689,
      "type": "SESSION",
      "startDate": 1710783000000,
      "endDate": 1710786600000
    },
    {
      "id": 13690,
      "type": "SESSION",
      "startDate": 1710837900000,
      "endDate": 1710841500000
    },
    {
      "id": 13691,
      "type": "BREAK",
      "startDate": 1710841500000,
      "endDate": 1710842700000
    },
    {
      "id": 13692,
      "type": "SESSION",
      "startDate": 1710924300000,
      "endDate": 1710927900000
    },
    {
      "id": 13693,
      "type": "SESSION",
      "startDate": 1710927900000,
      "endDate": 1710929100000
    },
    {
      "id": 13694,
      "type": "SESSION",
      "startDate": 1710874800000,
      "endDate": 1710882000000
    },
    {
      "id": 13696,
      "type": "SESSION",
      "startDate": 1710837000000,
      "endDate": 1710837900000
    },
    {
      "id": 13697,
      "type": "SESSION",
      "startDate": 1710923400000,
      "endDate": 1710924300000
    },
    {
      "id": 13698,
      "type": "SESSION",
      "startDate": 1710961200000,
      "endDate": 1710968400000
    },
    {
      "id": 13699,
      "type": "SESSION",
      "startDate": 1711009800000,
      "endDate": 1711010700000
    },
    {
      "id": 13700,
      "type": "SESSION",
      "startDate": 1711010700000,
      "endDate": 1711014300000
    },
    {
      "id": 13701,
      "type": "SESSION",
      "startDate": 1711037400000,
      "endDate": 1711040400000
    },
    {
      "id": 13702,
      "type": "SESSION",
      "startDate": 1710786600000,
      "endDate": 1710793800000
    },
    {
      "id": 13703,
      "type": "SESSION",
      "startDate": 1711014300000,
      "endDate": 1711015500000
    },
    {
      "id": 13712,
      "type": "SESSION",
      "startDate": 1710752400000,
      "endDate": 1710783000000
    },
    {
      "id": 13713,
      "type": "SESSION",
      "startDate": 1710752400000,
      "endDate": 1710765000000
    },
    {
      "id": 13714,
      "type": "SESSION",
      "startDate": 1710770400000,
      "endDate": 1710783000000
    },
    {
      "id": 13802,
      "type": "SESSION",
      "startDate": 1710835200000,
      "endDate": 1710837000000
    },
    {
      "id": 13803,
      "type": "SESSION",
      "startDate": 1710921600000,
      "endDate": 1710923400000
    },
    {
      "id": 13804,
      "type": "SESSION",
      "startDate": 1710750600000,
      "endDate": 1710752400000
    }
  ],
  "sessions": [
    {
      "id": 139327,
      "name": "Multimodal Models and Interaction",
      "isParallelPresentation": false,
      "importedId": "10058",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        139727
      ],
      "contentIds": [
        139191,
        139194,
        139234,
        139185,
        139222,
        139207
      ],
      "source": "SYS",
      "timeSlotId": 13654
    },
    {
      "id": 139328,
      "name": "AI in Peer Review and Decision Making",
      "isParallelPresentation": false,
      "importedId": "10059",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        138926
      ],
      "contentIds": [
        139179,
        139189,
        139181,
        139197,
        139235,
        139228
      ],
      "source": "SYS",
      "timeSlotId": 13656
    },
    {
      "id": 139329,
      "name": "Annotated Dataset Construction and HCAI Interaction with Predictive Models",
      "isParallelPresentation": false,
      "importedId": "10060",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        138728
      ],
      "contentIds": [
        139233,
        139226,
        139221,
        139198,
        139212,
        139220
      ],
      "source": "SYS",
      "timeSlotId": 13658
    },
    {
      "id": 139330,
      "name": "HCAI, Bias and Fairness in AI",
      "isParallelPresentation": false,
      "importedId": "10061",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        139459
      ],
      "contentIds": [
        139209,
        139190,
        139192,
        139239,
        139232,
        139201
      ],
      "source": "SYS",
      "timeSlotId": 13659
    },
    {
      "id": 139331,
      "name": "AI in Data Annotation and Analysis",
      "isParallelPresentation": false,
      "importedId": "10062",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        138853
      ],
      "contentIds": [
        139188,
        139206,
        139204,
        139223,
        139210,
        139225
      ],
      "source": "SYS",
      "timeSlotId": 13661
    },
    {
      "id": 139332,
      "name": "AI Tools, User Interfaces and Interaction",
      "isParallelPresentation": false,
      "importedId": "10063",
      "typeId": 13294,
      "chairIds": [
        138948
      ],
      "contentIds": [
        139186,
        139184,
        139231,
        139196,
        139195,
        139236
      ],
      "source": "SYS",
      "timeSlotId": 13663
    },
    {
      "id": 139333,
      "name": "AI in Personalization, Recommendation and Search",
      "isParallelPresentation": false,
      "importedId": "10064",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        138832
      ],
      "contentIds": [
        139208,
        139213,
        139237,
        139202,
        139224
      ],
      "source": "SYS",
      "timeSlotId": 13652
    },
    {
      "id": 139334,
      "name": "Generative AI: Theory and Applications",
      "isParallelPresentation": false,
      "importedId": "10065",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        153767
      ],
      "contentIds": [
        139180,
        139218,
        139216,
        139219,
        139356,
        139215
      ],
      "source": "SYS",
      "timeSlotId": 13670
    },
    {
      "id": 139335,
      "name": "AI for Health",
      "isParallelPresentation": false,
      "importedId": "10066",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        139352
      ],
      "contentIds": [
        139205,
        139182,
        139193,
        139230,
        139238,
        139354
      ],
      "source": "SYS",
      "timeSlotId": 13665
    },
    {
      "id": 139336,
      "name": "Information visualization and Visual Analytics",
      "isParallelPresentation": false,
      "importedId": "10067",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        153766
      ],
      "contentIds": [
        139183,
        139200,
        139217,
        139227,
        139355,
        139357
      ],
      "source": "SYS",
      "timeSlotId": 13666
    },
    {
      "id": 139337,
      "name": "Applications of Language Models",
      "isParallelPresentation": false,
      "importedId": "10068",
      "typeId": 13294,
      "roomId": 11433,
      "chairIds": [
        139151
      ],
      "contentIds": [
        139229,
        139214,
        139199,
        139187,
        139211,
        139203
      ],
      "source": "SYS",
      "timeSlotId": 13668
    },
    {
      "id": 139568,
      "name": "Poster and Demo Reception",
      "isParallelPresentation": false,
      "importedId": "10116",
      "typeId": 13290,
      "roomId": 11433,
      "chairIds": [],
      "contentIds": [
        139510,
        139521,
        139506,
        139495,
        139485,
        139509,
        139523,
        139489,
        139482,
        139518,
        139491,
        139486,
        139494,
        139496,
        139499,
        139519,
        139500,
        139488,
        139498,
        139520,
        139514,
        139497,
        139501,
        139515
      ],
      "source": "SYS",
      "timeSlotId": 13694
    },
    {
      "id": 139628,
      "name": "Adaptive XAI",
      "isParallelPresentation": false,
      "importedId": "10119",
      "typeId": 13297,
      "roomId": 11523,
      "chairIds": [],
      "contentIds": [
        139480
      ],
      "source": "SYS",
      "timeSlotId": 13713
    },
    {
      "id": 139630,
      "name": "EPIC",
      "isParallelPresentation": false,
      "importedId": "10121",
      "typeId": 13289,
      "roomId": 11437,
      "chairIds": [],
      "contentIds": [
        139487
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139634,
      "name": "Doctoral Consortium",
      "isParallelPresentation": true,
      "importedId": "10125",
      "typeId": 13291,
      "roomId": 11434,
      "chairIds": [
        139727,
        139728
      ],
      "contentIds": [
        139490,
        139522,
        139517,
        139493,
        139511,
        139492,
        139513,
        139512,
        139516,
        139508
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139638,
      "name": "Welcome Reception",
      "isParallelPresentation": false,
      "importedId": "10129",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        138806,
        139560
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13689
    },
    {
      "id": 139639,
      "name": "Doctoral Consortium Dinner",
      "isParallelPresentation": false,
      "importedId": "10130",
      "typeId": 13292,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13702
    },
    {
      "id": 139640,
      "name": "Opening Keynote: Prof. Margaret M. Burnett",
      "isParallelPresentation": false,
      "importedId": "10131",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        138806,
        139560
      ],
      "contentIds": [
        139729
      ],
      "source": "SYS",
      "timeSlotId": 13690
    },
    {
      "id": 139641,
      "name": "Keynote: Prof. Krzysztof Gajos",
      "isParallelPresentation": false,
      "importedId": "10132",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        139560,
        138806
      ],
      "contentIds": [
        139686
      ],
      "source": "SYS",
      "timeSlotId": 13692
    },
    {
      "id": 139642,
      "name": "Panel: Impact Paper Award",
      "isParallelPresentation": false,
      "importedId": "10133",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        139560,
        139645,
        138906,
        138737,
        139644,
        139125
      ],
      "contentIds": [
        139664
      ],
      "source": "SYS",
      "timeSlotId": 13700
    },
    {
      "id": 139646,
      "name": "HAI-GEN 2024",
      "isParallelPresentation": false,
      "importedId": "10134",
      "typeId": 13297,
      "roomId": 11438,
      "chairIds": [],
      "contentIds": [
        139507
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139648,
      "name": "HUMANIZE'24/SOCIALIZE'24",
      "isParallelPresentation": false,
      "importedId": "10136",
      "typeId": 13297,
      "roomId": 11523,
      "chairIds": [],
      "contentIds": [
        139502
      ],
      "source": "SYS",
      "timeSlotId": 13714
    },
    {
      "id": 139652,
      "name": "IUI Metaverse",
      "isParallelPresentation": false,
      "importedId": "10140",
      "typeId": 13297,
      "roomId": 11439,
      "chairIds": [],
      "contentIds": [
        139483
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139656,
      "name": "Past Meets Future (PMF)",
      "isParallelPresentation": false,
      "importedId": "10144",
      "typeId": 13297,
      "roomId": 11440,
      "chairIds": [],
      "contentIds": [
        139481
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139660,
      "name": "Speech as Interactive Design Material (SIDM)",
      "isParallelPresentation": false,
      "importedId": "10148",
      "typeId": 13297,
      "roomId": 11442,
      "chairIds": [],
      "contentIds": [
        139503
      ],
      "source": "SYS",
      "timeSlotId": 13712
    },
    {
      "id": 139666,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10153",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13683
    },
    {
      "id": 139667,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10154",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13687
    },
    {
      "id": 139668,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10155",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13691
    },
    {
      "id": 139669,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10156",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13657
    },
    {
      "id": 139670,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10157",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13693
    },
    {
      "id": 139671,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10158",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13664
    },
    {
      "id": 139672,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10159",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13703
    },
    {
      "id": 139673,
      "name": "Coffee Break with light refreshments",
      "isParallelPresentation": false,
      "importedId": "10160",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13671
    },
    {
      "id": 139674,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "10161",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13653
    },
    {
      "id": 139675,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "10162",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13660
    },
    {
      "id": 139676,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "10163",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13667
    },
    {
      "id": 139678,
      "name": "Lunch break (lunch is not provided)",
      "isParallelPresentation": false,
      "importedId": "10165",
      "typeId": 13298,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13685
    },
    {
      "id": 139679,
      "name": "Lunch break (lunch is not provided)",
      "isParallelPresentation": false,
      "importedId": "10166",
      "typeId": 13298,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13655
    },
    {
      "id": 139680,
      "name": "Lunch break (lunch is not provided)",
      "isParallelPresentation": false,
      "importedId": "10167",
      "typeId": 13298,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13662
    },
    {
      "id": 139681,
      "name": "Lunch break (lunch is not provided)",
      "isParallelPresentation": false,
      "importedId": "10168",
      "typeId": 13298,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13669
    },
    {
      "id": 139683,
      "name": "Dinner Banquet",
      "isParallelPresentation": false,
      "importedId": "10170",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13698
    },
    {
      "id": 139730,
      "name": "Town Hall and Closing",
      "isParallelPresentation": false,
      "importedId": "10172",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13701
    },
    {
      "id": 153768,
      "name": "Opening Session",
      "isParallelPresentation": false,
      "importedId": "11507",
      "typeId": 13292,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS"
    },
    {
      "id": 153769,
      "name": "Welcome Session",
      "isParallelPresentation": false,
      "importedId": "11508",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        138806,
        139560
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13696
    },
    {
      "id": 153770,
      "name": "Welcome Session",
      "isParallelPresentation": false,
      "importedId": "11509",
      "typeId": 13292,
      "roomId": 11433,
      "chairIds": [
        138806,
        139560
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13697
    },
    {
      "id": 153771,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "11510",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13802
    },
    {
      "id": 153772,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "11511",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13803
    },
    {
      "id": 153773,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "11512",
      "typeId": 13298,
      "roomId": 11443,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13804
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 139179,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2250",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil's advocate in the AI-assisted group decision making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities exhibited by modern large language models (LLMs), we design four different styles of devil's advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil's advocates that argue against the AI model's decision recommendation have the potential to promote groups' appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil's advocate usually does not lead to substantial increases in people's perceived workload for completing the group decision making tasks, while interactive LLM-powered devil's advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue university",
              "dsl": ""
            }
          ],
          "personId": 139168
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 138958
        }
      ]
    },
    {
      "id": 139180,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Appropriate Incongruity Driven Human-AI Collaborative Tool to Assist Novices in Humorous Content Generation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7346",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "Creating humorous content has been shown to improve an individual's emotional well-being by decreasing stress, overcoming anxiety, and enhancing interpersonal relationships. However, it is common knowledge that a good sense of humor is not common. In this paper, we propose a natural language processing (NLP) driven collaborative tool based on appropriate incongruity theory to assist novices in writing humorous content. We use cartoon-caption writing as the use case since it is a popular method where people engage in creating humorous content. The paper describes the design of our co-authoring tool and findings from a two-part user study where (1) 20 participants used our tool to co-author cartoon captions and (2) 66 participants evaluated those captions. Our findings show that the tool helped participants to identify incongruous visual elements in the cartoon, support ideation, and expand the narrative, resulting in co-authored captions frequently rated funnier than those written without the tool. This approach can be appropriated to other humor generation applications including creative writing, creating memes, sketch comedy, and advertising.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Sri Lanka",
              "state": "",
              "city": "Moratuwa",
              "institution": "University of Moratuwa",
              "dsl": "Department of Electronic and Telecommunication"
            }
          ],
          "personId": 139123
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Sri Lanka",
              "state": "Western Province",
              "city": "Moratuwa",
              "institution": "University of Moratuwa",
              "dsl": "Department of Electronic and Telecommunication Engineering"
            }
          ],
          "personId": 139062
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 138930
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": ""
            }
          ],
          "personId": 139138
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 139059
        }
      ]
    },
    {
      "id": 139181,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "ReviewFlow: Intelligent Scaffolding to Support Academic Peer Reviewing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4195",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "Peer review is a cornerstone of science. Research communities conduct peer reviews to assess contributions and to improve the overall quality of science work. Every year, new community members are recruited as peer reviewers for the first time. \r\nHow could technology help novices adhere to their community's practices and standards for peer reviewing? To better understand peer review practices and challenges, we conducted a formative study with 10 novices and 10 experts. We found that many experts adopt a workflow of annotating, note-taking, and synthesizing notes into well-justified reviews that align with community standards. Novices lack timely guidance on how to read and assess submissions and how to structure paper reviews. To support the peer review process, we developed ReviewFlow -- an AI-driven workflow that scaffolds novices with contextual reflections to critique and annotate submissions, in-situ knowledge support to assess novelty, and notes-to-outline synthesis to help align peer reviews with community expectations. In a within-subjects experiment, 16 inexperienced reviewers wrote reviews using ReviewFlow and a baseline environment with minimal guidance. Participants produced more comprehensive reviews using ReviewFlow than the baseline, calling out more pros and cons, but they still struggled to provide actionable suggestions to address the weaknesses. While participants appreciated the streamlined process support from ReviewFlow, they also expressed concerns about using AI as part of the scientific review process. We discuss the implications of using AI to scaffold peer review process on scientific work and beyond.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 139071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "UCSD Design Lab"
            }
          ],
          "personId": 139047
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UCSD",
              "dsl": "ProtoLab (Design Lab)"
            }
          ],
          "personId": 139005
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 139052
        }
      ]
    },
    {
      "id": 139182,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Personalizing an AR-based Communication System for Nonspeaking Autistic Users",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-6099",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "Nonspeaking autistic individuals (\"nonspeakers\") represent about one-third of the autistic population, and most are never provided with an effective alternative to speech, hindering their educational, employment, and social opportunities. Some individuals have learned to spell words and sentences by pointing to letters on a physical letterboard held vertically in their field of view by a trained human assistant. While this method is effective, nonspeakers have expressed to us a desire to transition towards a more independent communication method that relies less on a human assistant, which would provide them with more autonomy and privacy. Augmented Reality (AR) based communication systems have the potential to address this objective. For example, an AR-based communication system can lessen the reliance on a human assistant by employing a virtual letterboard that is automatically and adaptively placed in a personalized manner that considers a given user's unique motor skills and movement patterns. In this paper, we explore the use of Behavioural Cloning (BC) to derive such a personalized placement policy. Specifically, we observe finger, palm, head, and physical letterboard poses during real-life interactions between a nonspeaker and their assistant. These observations are then used to train a BC Machine Learning (ML) model that can adapt the placement of a virtual letterboard for that user within an AR environment. Results show that our approach can accurately replicate the actions of the human assistant of any given user, outperforming a non-ML baseline personalized placement policy in both positional and rotational accuracies. This work represents a foundational step toward enabling more autonomous and private communications for nonspeakers, thereby opening up new opportunities for them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Electrical and Software Engineering"
            }
          ],
          "personId": 139000
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Electrical and Software Engineering"
            }
          ],
          "personId": 138972
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Jaswal Lab"
            }
          ],
          "personId": 139090
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Psychology"
            }
          ],
          "personId": 139178
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 139134
        }
      ]
    },
    {
      "id": 139183,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Assessing User Trust in Active Learning Systems: Insights from Query Policy and Uncertainty Visualization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7743",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "Active learning systems have become increasingly popular for various applications in machine learning (ML), including medical imaging, environmental monitoring, and geospatial analysis. These systems rely on inputs dynamically queried from people to enhance classification. Ensuring appropriate analyst trust in these systems remains a significant obstacle, as analysts may over-rely or under-rely on the system. Common active learning (AL) strategies enhance classification models by asking an analyst to provide labels for data points with the highest degree of uncertainty. However, model-centric policies do not consider potential priming effects on the analyst and how they will affect people's trust in the system post-training. In this paper, we present an empirical study assessing how AL query policies and visualizations that enhance transparency in a classifier’s certainty influence trust in automated image classifiers. We found that query policy may significantly influence an analyst’s perception of the system’s capabilities, while the level of visual transparency into classifier certainty may influence an analyst’s ability to perform the classification task. Our study informs the design of interactive labeling systems to help mitigate the effects of over-reliance. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel HIll",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 139132
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Durham",
              "institution": "Duke University",
              "dsl": "Department of Statistical Science"
            }
          ],
          "personId": 139085
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina-Chapel Hill",
              "dsl": ""
            }
          ],
          "personId": 139141
        }
      ]
    },
    {
      "id": 139184,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "MathAssist: A Handwritten Mathematical Expression Autocomplete Technique",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5524",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "Writing and editing mathematical expressions with complicated structures in computer system is difficult and time-consuming. To address this, we proposed MathAssist, a mathematical expression autocomplete technique that recommends full formulas in real-time based on the user's input strokes. Our technique identifies user's input purpose by matching the structure of the current user input to the structure of formulas in a database. To facilitate such process, we propose a novel tree-based formalization to represent formula. In comparison to a mathematical expression recognition algorithm (SRD) and a commercial MicroSoft Ink Equation (InkEqu), our approach outperformed both of them on task completion time (reduced by 37.14% and 37.58%) and accuracy (32.78% and 10.55% higher). We also discuss our findings in using autocomplete to assist formula editing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "University of Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 139112
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences",
              "dsl": "Institute of Software"
            }
          ],
          "personId": 139092
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "University of Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 139164
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "University of Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 139165
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 138976
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences",
              "dsl": "Institute of Software"
            }
          ],
          "personId": 139106
        }
      ]
    },
    {
      "id": 139185,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and Exploration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3981",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "    Synthesizers are powerful tools that allow musicians to create dynamic and original sounds. Existing commercial interfaces for synthesizers typically require musicians to interact with complex low-level parameters or to manage large libraries of premade sounds. To address these challenges, we implement SynthScribe --- a fullstack system that uses multimodal deep learning to let users express their intentions at a much higher level. We implement features which address a number of difficulties, namely 1) searching through existing sounds, 2) creating completely new sounds, 3) making small but meaningful modifications to a given sound. This is achieved with three main features: a multimodal search engine for a large library of synthesizer sounds; a user centered genetic algorithm by which completely new sounds can be created and selected given the users preferences; a sound editing support feature which highlights and gives examples for key control parameters with respect to a text or audio based query.  The results of our user studies show SynthScribe is capable of reliably retrieving and modifying sounds while also affording the ability to create completely new sounds that expand a musicians creative horizon. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138907
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139068
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science "
            }
          ],
          "personId": 139174
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Music"
            }
          ],
          "personId": 138905
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139035
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139104
        }
      ]
    },
    {
      "id": 139186,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SpaceEditing: A Latent Space Editing Interface for Integrating Human Knowledge into Deep Neural Networks",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3025",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "Human-centered AI aims to bridge the gap between machine decision-making and human understanding. However, even for classification tasks where deep neural networks have achieved superb performance, there are currently few methods that link humans and AI well, especially on domain-specific tasks. In this paper, we propose SpaceEditing, a 2D spatial layout tool that enables human users to interact with the latent space of deep neural networks. During the interaction process, the tool's algorithm automatically processes user movements and feedback into the network to learn from user-modified information. We evaluate SpaceEditing with three case studies: (1) an archaeology researcher uses a bronze dataset; (2) a deep learning researcher uses a garbage classification dataset; (3) six deep learning beginners use a head pose dataset. The experimental results demonstrate the effectiveness of our tool in integrating human knowledge and improving network performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Chang Chun",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 139150
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139003
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ishikawa",
              "institution": "JAIST",
              "dsl": ""
            }
          ],
          "personId": 139065
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139155
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Chang Chun",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 139096
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jilin",
              "city": "Changchun",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 139159
        }
      ]
    },
    {
      "id": 139187,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Empirical Evidence on Conversational Control of GUI in Semantic Automation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5124",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "This research explores integration of a Large Language Model (LLM) fine-tuned to conversationally control the user interface (UI) for a Semantic Automation Layer (SAL). We condense SAL capabilities from prior work and prioritize with business analysts and data engineers via a Kano model, before implementing a prototypical UI.  We augment the UI with our conversational engine and propose In-situ Prompt Engineering and learn from Human Feedback to smoothen the interaction and manipulation of UI through natural language commands. To evaluate the efficacy and usability of conversational control in various use-case scenarios, we conduct and report on an empirical interaction design user study. Our findings provide evidence supporting enhanced user engagement and satisfaction. We also observe significant increase of trust in AI after working with our conversational UI. This work generates areas for further refinement and research towards more intelligent, highly-integrated conversational UIs even beyond our application within Semantic Automation. We discuss our findings and point out next steps paving the way for future research and development in creating more intuitive and adaptive user interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139017
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 138981
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "",
              "city": "La Plata",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 138925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139158
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 138778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "University of Massachusetts Boston",
              "dsl": ""
            }
          ],
          "personId": 139028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Coral Gables",
              "institution": "IBM Software",
              "dsl": ""
            }
          ],
          "personId": 139121
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Böblingen",
              "institution": "IBM Software",
              "dsl": ""
            }
          ],
          "personId": 139122
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Böblingen",
              "institution": "IBM Software",
              "dsl": ""
            }
          ],
          "personId": 139097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139080
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 138989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139038
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139146
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139088
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139039
        }
      ]
    },
    {
      "id": 139188,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "PDFChatAnnotator: A Human-LLM Collaborative Multi-Modal Data Annotation Tool for PDF-Format Catalogs",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8631",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "The document contains substantial unannotated data, necessitating extensive manual labeling efforts. To address this issue, we introduce PDFChatAnnotator, a human-LLM collaborative tool to collect multi-modal data from PDF catalogs. Initially, PDFChatAnnotator automatically employs our proposed multi-modal binding rules to link related data from different modalities and harnesses the information extraction capabilities of large language models (LLMs) to extract specific information from text descriptions. Furthermore, the tool empowers users to guide and refine the LLM's annotations. During the annotation process, users can influence the LLM through multiple rounds of communication and example establishment via the provided interfaces. To assess the effectiveness of PDFChatAnnotator's techniques, we conducted a technical evaluation using three catalogs with typical layouts as experimental data. The results showed that all accuracy rates for multi-modal binding exceeded 90%, and both the proposed \"example establishment\" and \"interactive adjustment of requirements\" contributed to enhanced accuracy rates. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Jilin",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 139166
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139155
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jilin",
              "city": "Changchun",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 139159
        }
      ]
    },
    {
      "id": 139189,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Enhancing Peer Review with AI-Powered Suggestion Generation Assistance: Investigating the Design Dynamics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-1329",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "While writing peer reviews resembles an important task in science, education, and large organizations, providing fruitful suggestions to peers is not a straightforward task, as different user interaction designs of text suggestion interfaces can have diverse effects on user behaviors when writing the review text. Generative language models might be able to support humans in formulating reviews with textual suggestions. Previous systems use two designs for providing text suggestions, but do not empirically evaluate them: inline and list of suggestions. To investigate the effects of embedding NLP text generation models in the two designs, we collected user requirements to implement Hamta as an example of assistants providing reviewers with text suggestions. Our experiment on comparing the two designs on 31 participants indicates that people using the inline interface provided longer reviews on average, while participants using the list of suggestions experienced more ease of use in using our tool. The results shed light on important design findings for embedding text generation models in user-centered assistants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": ""
            }
          ],
          "personId": 139013
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Bern",
              "institution": "Institute for Digital Technology Management",
              "dsl": ""
            }
          ],
          "personId": 139026
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": ""
            }
          ],
          "personId": 139173
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Bern",
              "institution": "Bern University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 138984
        }
      ]
    },
    {
      "id": 139190,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Do Crowdsourced Fairness Preferences Correlate with Risk Perceptions?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2978",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "With the increasing prevalence of automatic decision-making systems, concerns regarding the fairness of these systems also arise. Without a universally agreed-upon definition of fairness, given an automated decision-making scenario, researchers often adopt a crowdsourced approach to solicit people’s preferences across multiple fairness definitions. However, it is often found that crowdsourced fairness preferences are highly context-dependent, making it intriguing to explore the driving factors behind these preferences. One plausible hypothesis is that people’s fairness preferences reflect their perceived risk levels for different decision-making mistakes, such that the fairness definition that equalizes across groups the type of mistakes that are perceived as most serious will be preferred. To test this conjecture, we conduct a human-subject study (N =213) to study people’s fairness perceptions in three societal contexts. In particular, these three societal contexts differ on the expected level of risk associated with different types of decision mistakes, and we elicit both people’s fairness preferences and risk perceptions for each context. Our results show that people can often distinguish between different levels of decision risks across different societal contexts. However, we find that people’s fairness preferences do not vary significantly across the three selected societal contexts, except for within a certain subgroup of people (e.g., people with a certain racial background). As such, we observe minimal evidence suggesting that people’s risk perceptions of decision mistakes correlate with their fairness preference. These results highlight that fairness preferences are highly subjective and nuanced, and they might be primarily affected by factors other than the perceived risks of decision mistakes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138985
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139108
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 138958
        }
      ]
    },
    {
      "id": 139191,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Looking for a better fit? An Incremental Learning Multimodal Object Referencing Framework adapting to Individual Drivers",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4516",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "The rapid advancement of the automotive industry towards automated and semi-automated vehicles has rendered traditional methods of vehicle interaction, such as touch-based and voice command systems, inadequate for a widening range of non-driving related tasks, such as referencing objects outside of the vehicle. Consequently, research has shifted toward gestural input (e.g., hand, gaze, and head pose gestures) as a more suitable mode of interaction during driving. However, due to the dynamic nature of driving and individual variation, there are significant differences in drivers' gestural input performance. While, in theory, this inherent variability could be moderated by substantial data-driven machine learning models, prevalent methodologies lean towards constrained, single-instance trained models for object referencing. These models show a limited capacity to continuously adapt to the divergent behaviors of individual drivers and the variety of driving scenarios. \r\nTo address this, we propose IcRegress, a novel regression-based incremental learning approach that adapts to changing behavior and the unique characteristics of drivers engaged in the dual task of driving and referencing objects. We suggest a more personalized and adaptable solution for multimodal gestural interfaces, employing continuous lifelong learning to enhance driver experience, safety, and convenience. Our approach was evaluated using an outside-the-vehicle object referencing use case, which highlighted the superiority of the adapted incremental learning models over a single trained model across various driver traits such as handedness, driving experience, and numerous driving conditions. Finally, to facilitate reproducibility, ease deployment, and promote further research, we offer our approach as an open-source framework.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus\t",
              "dsl": ""
            }
          ],
          "personId": 139147
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 139154
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 138992
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 138962
        }
      ]
    },
    {
      "id": 139192,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "BiasEye: A Bias-Aware Real-time Interactive Material Screening System for Impartial Candidate Assessment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3592",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "In the process of evaluating competencies for job or student recruitment through material screening, decision-makers can be influenced by inherent cognitive biases, such as the screening order or anchoring information, leading to inconsistent outcomes. To tackle this challenge, we conducted interviews with seven experts to understand their challenges and needs for support in the screening process. Building on their insights, we introduce BiasEye, a bias-aware real-time interactive material screening visualization system. BiasEye enhances awareness of cognitive biases by improving information accessibility and transparency. It also aids users in identifying and mitigating biases through a machine learning (ML) approach that models individual screening preferences. Findings from a mixed-design user study with 20 participants demonstrate that, compared to a baseline system lacking our bias-aware features, BiasEye increases participants' bias awareness and boosts their confidence in making final decisions. At last, we discuss the potential of ML and visualization in mitigating biases during human decision-making tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "ShanghaiTech University",
              "dsl": "School of Information Science and Technology"
            }
          ],
          "personId": 138908
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghaitech",
              "dsl": "School of Information science and technology, Shanghaitech University"
            }
          ],
          "personId": 139101
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghaitech University",
              "dsl": "School of Information Science and Technology"
            }
          ],
          "personId": 139091
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 139050
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 139049
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "ShanghaiTech University",
              "dsl": "School of Information Science and Technology"
            }
          ],
          "personId": 139036
        }
      ]
    },
    {
      "id": 139193,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Getting on the Right Foot: Using Observational and Quantitative Methods to Evaluate Movement Disorders",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5375",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "Currently doctors rely on tools such as the Unified Parkinson’s Disease Rating Scale (MDS-UDPRS) and the Scale for the Assessment and Rating of Ataxia (SARA) to make diagnoses for movement disorders based on clinical observations of a patient’s motor movement. Observation-based assessments however are inherently subjective and can differ by person. Moreover, different movement disorders show overlapping symptoms, challenging neurologists to make a correct diagnosis based on eyesight alone. In this work, we create an intelligent interface to highlight movements and gestures that are indicative of a movement disorder to observing doctors. First, we analyzed the walking patterns of 43 participants with Parkinson's Disease (PD), 60 participants with ataxia, and 52 participants with no movement disorder to find 10 metrics that can be used to distinguish PD from ataxia. Next, we designed an interface that provides context to the gestures that are relevant to a movement disorder diagnosis. Finally, we surveyed two neurologists (one who specializes in PD and the other who specializes in ataxia) on how useful this interface is for making a diagnosis. Our results not only showcase additional metrics that can be used to evaluate movement disorders quantitatively but also outline steps to be taken when designing an interface for these kinds of diagnostic tasks.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139137
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": ""
            }
          ],
          "personId": 138935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Houston Methodist",
              "dsl": "Neurology"
            }
          ],
          "personId": 138936
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139124
        }
      ]
    },
    {
      "id": 139194,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Conan's Bow Tie: A Streaming Voice Conversion for Real-Time VTuber Livestreaming",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8885",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "Recent years have witnessed a dramatic growing trend of Virtual YouTubers (VTubers) as a new business on social media, such as YouTube, Twitch, and TikTok. However, due to the health problems or retirement of voice actors, maintaining the recognizable voices of VTuber avatars becomes a critical problem that remains unsolved. One potential solution has been depicted as Conan's Bow Tie voice changer in the popular animation Case Closed (i.e., Detective Conan). To make this a reality, we introduce VTuberBowTie, a user-friendly streaming voice conversion system for real-time VTuber livestreaming. We propose an innovative streaming voice conversion approach that tackles the challenges of limited context modeling and bidirectional context dependence inherent to conventional real-time voice conversion. Rather than individually processing the voice stream in data chunks, our approach adopts a fully sequential structure that leverages contextual information preceding the input chunk, thereby expanding the perceptual range and enabling seamless concatenation. Moreover, we developed a ready-to-use interaction interface for VTuberBowTie and deployed it on various computing platforms. The experimental results show that VTuberBowTie can achieve high-quality voice conversion in a streaming manner with a latency of 179.1ms on CPU and 70.8ms on GPU while providing users a friendly interactive experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Cyber Science and Technology"
            }
          ],
          "personId": 138950
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 138900
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Cyber Science and Technology"
            }
          ],
          "personId": 139031
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 138940
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 139008
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 138971
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 138966
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "School of Cyber Science and Technology"
            }
          ],
          "personId": 138922
        }
      ]
    },
    {
      "id": 139195,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8005",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "Informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike. When producing informational videos, editors edit videos by overlaying text/images or trimming footage to enhance the video quality and make it more engaging. However, video editing can be difficult and time-consuming, especially for novice video editors who often struggle with expressing and implementing their editing ideas. To address this challenge, we first explored how multimodality — natural language (NL) and sketching, which are natural modalities humans use for expression—can be utilized to support video editors in expressing video editing ideas. We gathered 176 multimodal expressions of editing commands from 10 video editors, which revealed the patterns of use of NL and sketching in describing edit intents. Based on the findings, we present ExpressEdit, a system that enables editing videos via NL text and sketching on the video frame. Powered by LLM and vision models, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL command and spatial references from sketching. The system implements the interpreted edits, which then the user can iterate on. An observational study (N=10) showed that ExpressEdit enhanced the ability of novice video editors to express and implement their edit ideas. The system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user’s multimodal edit commands and supporting iterations on the editing commands. This work offers insights into the design of future multimodal interfaces and AI-based pipelines for video editing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139094
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": ""
            }
          ],
          "personId": 139145
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139027
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139057
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138912
        }
      ]
    },
    {
      "id": 139196,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Chart4Blind: An Intelligent Interface for Chart Accessibility Conversion",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8644",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "In a world driven by data visualization, ensuring the inclusive accessibility of charts for Blind and Visually Impaired (BVI) individuals remains a significant challenge. Charts are usually presented as raster graphics without textual and visual metadata needed for an equivalent exploration experience for BVI people. Additionally, converting these charts into accessible formats requires considerable effort from sighted individuals. Digitizing charts with metadata extraction is just one aspect of the issue; transforming it into accessible modalities, such as tactile graphics, presents another difficulty. To address these disparities, we propose Chart4Blind, an intelligent user interface that converts bitmap image representations of line charts into universally accessible formats. Chart4Blind achieves this transformation by generating Scalable Vector Graphics (SVG), Comma-Separated Values (CSV), and alternative text exports, all comply with established accessibility standards. Through interviews and a formal user study, we demonstrate that even inexperienced sighted users can make charts accessible in an average of 4 minutes using Chart4Blind, achieving a System Usability Scale rating of 90%.  In comparison to existing approaches, Chart4Blind provides a comprehensive solution, generating end-to-end accessible SVGs suitable for assistive technologies such as embossed prints (papers and laser cut), 2D tactile displays, and screen readers. For additional information, including open-source codes and demos, please visit our project page https://moured.github.io/chart4blind/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Karlsruhe",
              "institution": "Institute for Anthropomatics and Robotics ",
              "dsl": "Computer Vision for Human-Computer Interaction Lab"
            },
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": "Center for Digital Accessibility and Assistive Technology"
            }
          ],
          "personId": 138998
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": "Center for Digital Accessibility and Assistive Technology"
            }
          ],
          "personId": 139046
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Center for Digital Accessibility and Assistive Technology"
            }
          ],
          "personId": 138949
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": "Intelligent Sensing and Perception Group"
            }
          ],
          "personId": 139133
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Center for Digital Accessibility and Assistive Technology",
              "dsl": "Karlsruhe Institute of Technology"
            }
          ],
          "personId": 139110
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Center for Digital Accessibility and Assistive Technology"
            }
          ],
          "personId": 139073
        }
      ]
    },
    {
      "id": 139197,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8248",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "In settings where users both need high accuracy and are time-pressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to no-time-pressure settings. We also find that a user's overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user's overreliance rate (which is related to the user's trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "School of Engineering and Applied Sciences"
            }
          ],
          "personId": 138987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "School of Engineering and Applied Sciences"
            }
          ],
          "personId": 138926
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Allston",
              "institution": "Harvard University",
              "dsl": "School of Engineering and Applied Sciences"
            }
          ],
          "personId": 138894
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "SEAS"
            }
          ],
          "personId": 138994
        }
      ]
    },
    {
      "id": 139198,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "VMS: Interactive Visualization to Support the Sensemaking and Selection of Predictive Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2061",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "To compare and select machine learning models, relying on performance measures alone may not always be sufficient. This is particularly the case where different subsets, features, and predicted results may vary in importance relative to the task at hand. Explanation and visualization techniques are required to support model sensemaking and informed decision-making. However, a review shows that existing systems are mostly designed for model developers and not evaluated with target users in their effectiveness. To address this issue, this research proposes an interactive visualization, VMS (Visualization for Model Sensemaking and Selection), for users of the model to compare and select predictive models. VMS integrates performance-, instance-, and feature-level analysis to evaluate models from multiple angles. Particularly, a feature view integrating the value and contribution of hundreds of features supports model comparison on local and global scales. We exemplified this method for comparing models predicting patients' hospital length of stay through time-series health records and evaluated the prototype with 16 participants from the medical field. Results reveal evidence that VMS supports users to rationalize models in multiple ways and enables users to select the optimal models with a small sample size. User feedback suggested future directions on incorporating domain knowledge in model training, such as for different patient groups considering different sets of features as important.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138975
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138920
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Department of Computer Science",
              "dsl": "Aalto University"
            }
          ],
          "personId": 138961
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139087
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Engineering University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 138952
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 138939
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Helsinki University Hospital",
              "dsl": ""
            }
          ],
          "personId": 139100
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139029
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Helsinki Institute for Information Technology HIIT, University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138959
        }
      ]
    },
    {
      "id": 139199,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "ConstitutionMaker: Interactively Critiquing Large Language Models by Converting Feedback into Principles",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4162",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "Large language model (LLM) prompting is a promising new approach for users to create and customize their own chatbots. However, current methods for steering a chatbot's outputs, such as prompt engineering and fine-tuning, do not support users in converting their natural feedback on the model's outputs to changes in the prompt or model. In this work, we explore how to enable users to interactively refine model outputs through their feedback, by helping them convert their feedback into a set of principles (i.e. a constitution) that dictate the model's behavior. From a formative study, we (1) found that users needed support converting their feedback into principles for the chatbot and (2) classified the different principle types desired by users. Inspired by these findings, we developed ConstitutionMaker, an interactive tool for converting user feedback into principles, to steer LLM-based chatbots. With ConstitutionMaker, users can provide either positive or negative feedback in natural language, select auto-generated feedback, or rewrite the chatbot’s response; each mode of feedback automatically generates a principle that is inserted into the chatbot’s prompt. In a user study with 14 participants, we compare ConstitutionMaker to an ablated version, where users write their own principles. With ConstitutionMaker, participants felt that their principles could better guide the chatbot, that they could more easily convert their feedback into principles, and that they could write principles more efficiently, with less mental demand. ConstitutionMaker helped users identify ways to improve the chatbot, formulate their intuitive responses to the model into feedback, and convert this feedback into specific and clear principles. Together, these findings inform future tools that support the interactive critiquing of LLM outputs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 139161
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 139033
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 139001
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 139099
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 138946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 138917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 139069
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 138977
        }
      ]
    },
    {
      "id": 139200,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "iScore: Visual Analytics for Interpreting How Language Models Automatically Score Summaries",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8845",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "The recent explosion in popularity of large language models (LLMs) has inspired learning engineers to incorporate them into adaptive educational tools that automatically score summary writing. Understanding and evaluating LLMs is vital before deploying them in critical learning environments, yet their unprecedented size and expanding number of parameters inhibits transparency and impedes trust when they underperform. Through a collaborative user-centered design process with several learning engineers building and deploying summary scoring LLMs, we characterized fundamental design challenges and goals around interpreting their models, including aggregating large text inputs, tracking score provenance, and scaling LLM interpretability methods. To address their concerns, we developed iScore, an interactive visual analytics tool for learning engineers to upload, score, and compare multiple summaries simultaneously. Tightly integrated views allow users to iteratively revise the language in summaries, track changes in the resulting LLM scores, and visualize model weights at multiple levels of abstraction. To validate our approach, we deployed iScore with three learning engineers over the course of a month. We present a case study where interacting with iScore led a learning engineer to improve their LLM's score accuracy by three percentage points. Finally, we conducted qualitative interviews with the learning engineers that revealed how iScore enabled them to understand, evaluate, and build trust in their LLMs during deployment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139093
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University",
              "dsl": ""
            }
          ],
          "personId": 138968
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University",
              "dsl": ""
            }
          ],
          "personId": 139119
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia State University",
              "dsl": "Applied Linguistics"
            }
          ],
          "personId": 139086
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University",
              "dsl": ""
            }
          ],
          "personId": 139160
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139016
        }
      ]
    },
    {
      "id": 139201,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Understanding Users’ Dissatisfaction with ChatGPT Responses: Types, Resolving Tactics, and the Effect of Knowledge Level",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-6944",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "Large language models (LLMs) with chat-based capabilities, such as ChatGPT, are widely used in various workflows. However, due to a limited understanding of these large-scale models, users struggle to use this technology and experience different kinds of dissatisfaction. Researchers have introduced several methods, such as prompt engineering, to improve model responses. However, they focus on enhancing the model's performance in specific tasks, and little has been investigated on how to deal with the user dissatisfaction resulting from the model's responses. Therefore, with ChatGPT as the case study, we examine users' dissatisfaction along with their strategies to address the dissatisfaction. After organizing users' dissatisfaction with LLM into seven categories based on a literature review, we collected 511 instances of dissatisfactory ChatGPT responses from 107 users and their detailed recollections of dissatisfactory experiences, which we released as a publicly accessible dataset. Our analysis reveals that users most frequently experience dissatisfaction when ChatGPT fails to grasp their intentions, while they rate the severity of dissatisfaction related to accuracy the highest. We also identified four tactics users employ to address their dissatisfaction and their effectiveness. We found that users often do not use any tactics to address their dissatisfaction, and even when using tactics, 72% of dissatisfaction remained unresolved. Moreover, we found that users with low knowledge of LLMs tend to face more dissatisfaction on accuracy while they often put minimal effort in addressing dissatisfaction. Based on these findings, we propose design implications for minimizing user dissatisfaction and enhancing the usability of chat-based LLM.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of AI"
            }
          ],
          "personId": 139116
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": ""
            }
          ],
          "personId": 138892
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139064
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Sejong",
              "institution": "Korea Development Institute",
              "dsl": "School of Public Policy and Management"
            }
          ],
          "personId": 138910
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138912
        }
      ]
    },
    {
      "id": 139202,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "DataDive: Supporting Readers' Contextualization of Statistical Statements with Data Exploration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5457",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139333
      ],
      "eventIds": [],
      "abstract": "Statistical statements that refer to data to support narratives or claims are commonly used to inform readers about the magnitude of social issues.\r\nWhile contextualizing statistical statements with relevant data supports readers in building their own interpretation of statements, the complexity of finding contextual information on the web and linking statistical statements with it impedes readers' efforts to do so.\r\nWe present DataDive, an interactive tool for contextualizing statistical statements for the readers of online texts.\r\nBased on users' selections of statistical statements, our tool uses an LLM-powered pipeline to generate candidates of relevant contexts and poses them as guiding questions to the user as potential contexts for exploration.\r\nWhen the user selects a question, DataDive employs visualizations to further help the user compare and explore contextually relevant data.\r\nA technical evaluation shows that DataDive generates important and diverse questions that facilitate exploration around statistical statements and retrieves relevant data for comparison.\r\nMoreover, a user study with 21 participants suggests that DataDive facilitates users to explore diverse contexts and to be more aware of how statistical data could relate to the text.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139143
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Ho Chi Minh ",
              "institution": "Ho Chi Minh City University of Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139014
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore University of Technology and Design",
              "dsl": ""
            }
          ],
          "personId": 139020
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Information & Electronics Research Institute"
            }
          ],
          "personId": 139002
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138895
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138912
        }
      ]
    },
    {
      "id": 139203,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "JAMPL8: Exploring LLM-Enhanced Templates for Idea Reflection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8441",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "Advances in AI, particularly large language models (LLMs), can transform creative work. When developing a new idea, LLMs can help designers gather information, find competitors, and generate alternatives. However, LLM responses tend to be long-winded or contain inaccuracies, placing a burden on users to carefully synthesize information. In our formative studies with 52 students and five instructors, we find that novice designers typically lack guidance on how to compose prompts, reflect critically on LLM responses, and extract key information to help shape an idea. Building on these insights, we explore an alternative approach for interacting with LLMs, not via chat, but rather through structured templates. Collaborative design templates are a well-established strategy for helping novices think, organize information, and reflect on creative work. Developed as a digital whiteboard plugin, Jamplate integrates LLM capabilities into design templates, streamlining the collection and organization of user-generated content and LLM responses within the template structure. In a preliminary study with 8 novice designers, participants expressed that Jamplate's reflective questions and in-situ guidance improved their ability to think critically and improve ideas more effectively. We discuss the potential of designing LLM-enhanced templates to instigate critical reflection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 139111
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine ",
              "institution": "University of California, Irvine",
              "dsl": ""
            }
          ],
          "personId": 139128
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 139118
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 138891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": ""
            }
          ],
          "personId": 138909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 139083
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Dept of Cognitive Science"
            }
          ],
          "personId": 139052
        }
      ]
    },
    {
      "id": 139204,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Elicitating Challenges and User Needs Associated with Annotation Software for Plant Phenotyping",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7871",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "Artificial Intelligence (AI) has been enhancing data analysis efficiency and accuracy during plant phenotyping, which is vital for tackling global agricultural and environmental challenges. Designing a reliable AI system to assist precise plant phenotyping begins with high-quality phenotypic feature annotation, which usually involves collaboration between plant scientists and AI specialists. However, due to the high level of diversity in these researchers' backgrounds, it is likely that they have differing user needs from a fine-grained plant feature annotation system. We conducted semi-structured interviews with 8 experienced annotators from diverse backgrounds, and observed how they interact with their preferred annotation system, to elucidate the challenges faced when annotating plant features and identify user needs. We collected qualitative responses to the interview questions, and conducted a quantitative evaluation of the agreement of their annotations on the given images. By analyzing the participants’ behaviors and the collected data, we identified common user needs and derived implications for the design of an AI-assisted annotation system, including providing a range of annotation options, the flexibility to adapt annotations, and functions to help addressing uncertainty. Our research contributes to the design of systems that make annotations efficient and reliable, not only benefiting plant phenotyping, but also other interdisciplinary fields that rely on user-driven annotations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 138978
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Dept of CISE"
            }
          ],
          "personId": 139114
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Clinton",
              "institution": "Hamilton College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Dept of CISE"
            }
          ],
          "personId": 139172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 139105
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Agronomy Departmen"
            }
          ],
          "personId": 139019
        }
      ]
    },
    {
      "id": 139205,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Comparing Teaching Strategies of a Machine Learning-based Prosthetic Arm",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2660",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "Pattern-recognition-based arm prostheses rely on recognizing muscle activation to trigger movements. The effectiveness of this approach depends not only on the performance of the machine learner but also on the user’s understanding of its recognition capabilities, allowing them to adapt and work around recognition failures. We investigate how different model training strategies to select gesture classes and record respective muscle contractions impact model accuracy and user comprehension. We report on a lab experiment where participants performed hand gestures to train a classifier under three conditions: (1) the system cues gesture classes randomly (control), (2) the user selects gesture classes (teacher-led), (3) the system queries gesture classes based on their separability (learner-led). After training, we compare the models’ accuracy and test participants’ predictive understanding of the prosthesis’ behavior. We found that teacher-led and learner-led strategies yield faster and greater performance increases, respectively.\r\nCombining two evaluation methods, we found that participants developed a more accurate mental model when the system queried the least separable gesture class (learner-led). Our results conclude that, in the context of machine learning-based myoelectric prosthesis control, guiding the user to focus on class separability during training can improve recognition performances and support users’ mental models about the system’s behavior. We discuss our results in light of several research fields : myoelectric prosthesis control, motor learning, human-robot interaction, and interactive machine teaching.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Institut Des Systèmes Intelligents et de Robotique",
              "dsl": ""
            }
          ],
          "personId": 138945
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, UPMC, CNRS",
              "dsl": "Institut des Systémes Intelligents et de Robotique"
            }
          ],
          "personId": 138896
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Hochschule München University of Applied Sciences",
              "dsl": "Munich Center for Digital Sciences and AI"
            }
          ],
          "personId": 139098
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 139040
        }
      ]
    },
    {
      "id": 139206,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Closing the Knowledge Gap in Designing Data Annotation Interfaces for AI-powered Disaster Management Analytic Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3835",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "Data annotation interfaces predominantly leverage ground truth labels to guide annotators toward accurate responses. With the growing adoption of Artificial Intelligence (AI) in domain-specific professional tasks, it has become increasingly important to help beginning annotators identify how their early-stage knowledge can lead to inaccurate answers, which in turn, helps to ensure quality annotations at scale. To investigate this issue, we conducted a formative study involving eight individuals from the field of disaster management, each possessing varying levels of expertise. The goal was to understand the prevalent factors contributing to disagreements among annotators when classifying Twitter messages related to disasters and to analyze their respective responses. Our analysis identified two primary causes of disagreement between expert and beginner annotators: 1) a lack of contextual knowledge or uncertainty about the situation, and 2) the absence of visual or supplementary cues. Based on these findings, we designed a Context interface, which generates aids that help beginners identify potential mistakes and provide the hidden context of the presented tweet. The summative study compares Context design with two widely used designs in data annotation UI, Highlight and Reasoning based interfaces. We found significant differences between these designs in terms of attitudinal and behavioral data. We conclude with implications for designing future interfaces aiming at closing the knowledge gap among annotators.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 139148
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 138955
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 138916
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 139063
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Gaithersburg",
              "institution": "Montgomery County Community Emergency Response Team",
              "dsl": ""
            }
          ],
          "personId": 139115
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Provo",
              "institution": "Brigham Young University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 139070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 139142
        }
      ]
    },
    {
      "id": 139207,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "From Text to Pixels: Enhancing User Understanding through Text-to-Image Model Explanations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3913",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "Recent progress in Text-to-Image (T2I) models promises transformative applications in art, design, education, medicine, and entertainment. These models, exemplified by Dall-e, Imagen, and Stable Diffusion, have the potential to revolutionize various industries. However, a primary concern is their operation as a 'black-box' for many users. Without understanding the underlying mechanics, users are unable to harness the full potential of these models. This study focuses on bridging this gap by developing and evaluating explanation techniques for T2I models, targeting inexperienced end users. While prior works have delved into Explainable AI (XAI) methods for classification or regression tasks, T2I generation poses distinct challenges. Through formative studies with experts, we identified unique explanation goals and subsequently designed tailored explanation strategies. We then empirically evaluated these methods with a cohort of 473 participants from Amazon Mechanical Turk (AMT) across three tasks. Our results highlight users' ability to learn new keywords through explanations, a preference for example-based explanations, and challenges in comprehending explanations that significantly shift the image's theme. Moreover, findings suggest users benefit from a limited set of concurrent explanations. Our main contributions include a curated dataset for evaluating T2I explainability techniques, insights from a comprehensive AMT user study, and observations critical for future T2I model explainability research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California Los Angeles",
              "dsl": "Electrical and Computer Engineering, Human Computer Interface Group"
            }
          ],
          "personId": 139084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139032
        }
      ]
    },
    {
      "id": 139208,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Sample, Nudge and Rank: Exploiting Interpretable GAN Controls for Exploratory Search",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3910",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139333
      ],
      "eventIds": [],
      "abstract": "Exploratory search is characterized by open-ended search tasks and uncertainty with respect to the clarity of users' information needs. In the context of image retrieval, generative adversarial networks (GANs) present numerous opportunities for satisfying the information needs of users engaged in exploratory search compared to a collection of images. In this article, we present a novel approach for performing exploratory search on a GAN's image space using interpretable GAN controls that can be summarized as \\textit{sample}, \\textit{nudge}, and \\textit{rank}. At each search iteration, we \\textit{sample} images from the GAN's latent space. We implement faceted search by \\textit{nudging} the sampled images towards regions of the latent space containing the attributes associated with selected facets. Lastly, we \\textit{rank} the nudged images using reinforcement learning with relevance feedback. We present a comprehensive evaluation of the proposed approach, incorporating results from simulations and a user study. In simulation, we show that our approach efficiently adapts to user preferences, while preserving a high-level of image diversity. In the user study (N=30), a majority of participants (23/30) preferred our system to the baseline. Concordant with simulation results, users reported both higher perceived search efficiency and image diversity compared to the baseline. Indeed, due to the baseline system's dependence on a warm-start procedure, users of our system examined significantly fewer images while achieving task outcomes of similar subjective quality.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139078
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Compute Science"
            }
          ],
          "personId": 138963
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138906
        }
      ]
    },
    {
      "id": 139209,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Why and When LLM-Based Assistants Can Go Wrong: Investigating the Effectiveness of Prompt-Based Interactions for Software Help-Seeking",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2821",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "Large Language Model (LLM) assistants, such as ChatGPT, have emerged as potential alternatives to search methods for helping users navigate complex, feature-rich software. LLMs use vast training data from domain-specific texts, software manuals, and code repositories to mimic human-like interactions, offering tailored assistance, including step-by-step instructions. In this work, we investigated LLM-generated software guidance through a within-subject experiment with 16 participants and follow-up interviews. We compared a baseline LLM assistant with an LLM optimized for particular software contexts, SoftAIBot, which also offered guidelines for constructing appropriate prompts. We assessed task completion, perceived accuracy, relevance, and trust. Surprisingly, although SoftAIBot outperformed the baseline LLM, our results revealed no significant difference in LLM usage and user perceptions with or without prompt guidelines and the integration of domain context. Most users struggled to understand how the prompt's text related to the LLM's responses and often followed the LLM's suggestions verbatim, even if they were incorrect. This resulted in difficulties when using the LLM's advice for software tasks, leading to low task completion rates. Our detailed analysis also revealed that users remained unaware of inaccuracies in the LLM's responses, indicating a gap between their lack of software expertise and their ability to evaluate the LLM's assistance. With the growing push for designing domain-specific LLM assistants, we emphasize the importance of incorporating explainable, context-aware cues into LLMs to help users understand prompt-based interactions, identify biases, and maximize the utility of LLM assistants. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science "
            }
          ],
          "personId": 138954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 138893
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "Computing Science"
            }
          ],
          "personId": 138948
        }
      ]
    },
    {
      "id": 139210,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Annota: Peer-based AI Hints Towards Learning Qualitative Coding at Scale",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8293",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "Learning qualitative analysis requires personalized feedback and in-depth discussion not possible for educators to provide in a large course, resulting in many students obtaining only a shallow exposure to qualitative user research and interpretative skills. To overcome this challenge, we introduce a learnersourcing method that builds on the Dawid-Skene expectation maximization (EM) algorithm to generate peer-based AI hints that support students in one aspect of qualitative analysis: determining what sentences are relevant to the research question. After one annotation round, class-wide annotations are used to predict relevant sentences and to generate hints prompting students to revisit missed or incorrectly annotated sentences. An in-the-wild deployment within a large course (N=122) showed that our algorithm converged to comparatively high accuracy despite noisy student labels, and after only ~20 students. An analysis of student interviews found that peer-based AI hints helped improve understanding of research questions, led to more careful examination of transcript annotations, and improved understanding of when they were over-annotating or under-annotating the transcript.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 139170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 138974
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 138995
        }
      ]
    },
    {
      "id": 139211,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "FigurA11y: AI Assistance for Writing Scientific Alt Text",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-6198",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "High-quality alt text is crucial for making scientific figures accessible to blind and low-vision readers. Crafting complete, accurate alt text is challenging even for domain experts, as published figures often depict complex visual information and readers have varied informational needs. These challenges, along with high diversity in figure types and domain-specific details, also limit the usefulness of fully automated approaches. Consequently, the prevalence of high-quality alt text is very low in scientific papers today. We investigate whether and how human-AI collaborative editing systems can help address the difficulty of writing high-quality alt text for complex scientific figures. We present FigurA11y, an interactive system that generates draft alt text and provides suggestions for author revisions using a pipeline driven by extracted figure and paper metadata. We test two versions, motivated by prior work on visual accessibility and writing support. The base Draft+Revise version provides authors with an automatically generated draft description to revise, along with extracted figure metadata and figure-specific alt text guidelines to support the revision process. The full Interactive Assistance version further adds contextualized suggestions: text snippets to iteratively produce descriptions, and hypothetical user questions with possible answers to reveal potential ambiguities and resolutions. In a study of authors (N=14), we found the system assisted them in efficiently producing descriptive alt text. Generated drafts and interface elements enabled authors to quickly initiate and edit detailed descriptions. Additionally, interactive suggestions from the full system prompted more iteration and highlighted aspects for authors to consider, resulting in greater deviation from the drafts without increased average cognitive load or manual effort.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139066
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for AI",
              "dsl": ""
            }
          ],
          "personId": 139011
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 138929
        }
      ]
    },
    {
      "id": 139212,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Benefits of Human-AI Interaction for Expert Users Interacting with Prediction Models: a Study on Marathon Running",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-2191",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "Users with large domain knowledge can be reluctant to use prediction models. This also applies to the sports domain, where running coaches rarely rely on marathon prediction tools for race-plan advice for their runners' next marathon. This paper studies the effect of adding interactivity to such prediction models, to incorporate and acknowledge users' domain knowledge. In think-aloud sessions and an online study, we tested an interactive machine learning tool that allowed coaches to indicate the importance of earlier races feeding into the model. Our results show that coaches deploy rich knowledge when working with the model on runners familiar to them, and their adaptations improved model accuracy. Those coaches who could interact with the model displayed more trust and acceptance in the resulting predictions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction group"
            }
          ],
          "personId": 139153
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Den Bosch",
              "institution": "Jheronimus Academy of Data Science",
              "dsl": ""
            }
          ],
          "personId": 139125
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139045
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Technical university of Eindhoven",
              "dsl": "Human Technology Interaction"
            }
          ],
          "personId": 139144
        }
      ]
    },
    {
      "id": 139213,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Leveraging ChatGPT for Automated Human-centered Explanations in Recommender Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5380",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139333
      ],
      "eventIds": [],
      "abstract": "The adoption of recommender systems (RSs) in various domains has become increasingly popular, but concerns have been raised about their lack of transparency and interpretability. While significant advancements have been made in creating explainable RSs, there is still a shortage of automated approaches that can deliver meaningful and contextual human-centered explanations. Numerous researchers have evaluated explanations based on human-generated recommendations and explanations to address this gap. However, such approaches do not scale for real-world systems. Building on recent research that exploits Large Language Models (LLMs) for RSs, we propose leveraging the conversational capabilities of ChatGPT to provide users with personalized, human-like, and meaningful explanations for recommended items. Our paper presents one of the first user studies that measure users' perceptions of ChatGPT-generated explanations while acting as an RS. Regarding recommendations, we assess whether users prefer ChatGPT over random (but popular) recommendations. Concerning explanations, we assess users' perceptions of personalization, effectiveness, and persuasiveness.  Our findings reveal that users tend to prefer ChatGPT-generated recommendations over popular ones. Additionally, personalized rather than generic explanations prove to be more effective when the recommended item is unfamiliar.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Paraíba",
              "city": "Campina Grande",
              "institution": "Federal University of Campina Grande",
              "dsl": ""
            }
          ],
          "personId": 138988
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Paraíba",
              "city": "Campina Grande",
              "institution": "Federal University of Campina Grande",
              "dsl": ""
            }
          ],
          "personId": 138899
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Gothenburg",
              "institution": "University of Gothenburg",
              "dsl": ""
            }
          ],
          "personId": 139176
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction"
            }
          ],
          "personId": 139125
        }
      ]
    },
    {
      "id": 139214,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "AI for Low-Code for AI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-1024",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "Low-code programming allows citizen developers to create programs with minimal coding effort, typically via visual (e.g. drag-and-drop) interfaces. At the same time, recent language-model powered tools such as Copilot and ChatGPT generate programs from natural language instructions. We argue that these modalities are complementary: tools like ChatGPT greatly reduce the need to memorize large APIs but still require their users to read (and modify) textual code, whereas visual tools abstract away most or all textual code but struggle to provide easy access to large APIs. At their intersection, we propose LowCoder, the first low-code tool for developing machine-learning pipelines that supports both a visual programming interface (LowCoder_VP) and a natural language interface (LowCoder_NL). We develop a novel task formulation and benchmark several language models to develop LowCoder_NL. We leverage this tool to provide some of the first insights into whether and how these two modalities help programmers by conducting a user study. We task 20 developers with varying levels of expertise with implementing ML pipelines using LowCoder. Overall, we find that LowCoder is especially useful for (i)~Discoverability: using LowCoder_NL, participants discovered new operators in 75% of the tasks, compared to just 32.5% and 27.5% using web search or scrolling through options respectively, and (ii)~Iterative Composition: 82.5% of tasks were successfully completed and many initial pipelines were further successfully improved. Qualitative analysis shows that language models helped users discover how to implement constructs when they know what to do, but still failed to support novices when they lack clarity on what they want to accomplish. Overall, we demonstrate the benefits of combining the power of language models with visual low-code programming by building LowCoder and conducting a user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 139004
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T. J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 139107
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T. J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 138901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 139030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T. J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 138931
        }
      ]
    },
    {
      "id": 139215,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "LAVE: LLM-Powered Agent Assistance and Language Augmentation for Video Editing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-9782",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "Video creation has become increasingly popular, yet the expertise and effort required for editing often pose barriers to beginners. In this paper, we explore the integration of large language models (LLMs) into the video editing workflow to reduce these barriers. Our design vision is embodied in LAVE, a novel system that provides LLM-powered agent assistance and language-augmented editing features. LAVE automatically generates language descriptions for the user's footage, serving as the foundation for enabling the LLM to process videos and assist in editing tasks. When the user provides editing objectives, the agent plans and executes relevant actions to fulfill them. Moreover, LAVE allows users to edit videos through either the agent or direct UI manipulation, providing flexibility and enabling manual refinement of agent actions. Our user study, which included eight participants ranging from novices to proficient editors, demonstrated LAVE's effectiveness. The results also shed light on user perceptions of the proposed LLM-assisted editing paradigm and its impact on users' creativity and sense of co-creation. Based on these findings, we propose design implications to inform the future development of agent-assisted content editing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139068
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Meta",
              "dsl": "Reality Lab Research"
            }
          ],
          "personId": 139095
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Meta",
              "dsl": "Meta Reality Labs Research"
            }
          ],
          "personId": 139061
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Department of Cognitive Science and Design Lab"
            }
          ],
          "personId": 139103
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 139044
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Facebook",
              "dsl": "Facebook Reality Labs"
            }
          ],
          "personId": 138986
        }
      ]
    },
    {
      "id": 139216,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SwipeGANSpace: Swipe-to-Compare Image Generation via Efficient Latent Space Exploration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-9144",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "Generating preferred images using generative adversarial networks (GANs) is challenging owing to the high-dimensional nature of latent space. In this study, we propose a novel approach that uses simple user-swipe interactions to generate preferred images for users. To effectively explore the latent space with only swipe interactions, we apply principal component analysis to the latent space of the StyleGAN, creating meaningful subspaces. We use a multi-armed bandit algorithm to decide the dimensions to explore, focusing on the preferences of the user. Experiments show that our method is more efficient in generating preferred images than the baseline methods. Furthermore, changes in preferred images during image generation or the display of entirely different image styles were observed to provide new inspirations, subsequently altering user preferences. This highlights the dynamic nature of user preferences, which our proposed approach recognizes and enhances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139055
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 138943
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139024
        }
      ]
    },
    {
      "id": 139217,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Using Machine Learning to Improve Interactive Visualizations for Large Collected Traffic Detector Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3885",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "In traffic engineering, cities rely on large detector datasets to manage traffic. Visualizing these big, multi-dimensional datasets poses challenges such as overplotting and dimension reduction, often rendering traditional visualization techniques inadequate. To address this, we added two machine learning (ML) algorithms (Local Outlier Factor algorithm and K-Prototypes clustering) to an interactive time series visualization to improve exploration by both domain experts and non-experts. We used an original detector dataset of a mid-sized German city. Our findings reveal that the ML algorithms greatly enhanced data exploration in these interactive visualizations, particularly for users with limited domain knowledge. This research directly contributes to the design of traffic data analysis tools, offering a foundation for traffic detection hardware and software improvements, but also advancing complex dataset visualization in general. It will ultimately lead to more informed decisions, improved traffic management, and has the potential to reduce air pollutants, thus counteracting climate change.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 139135
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 138915
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 139175
        }
      ]
    },
    {
      "id": 139218,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "FrameKit: A Tool for Authoring Adaptive UIs Using Keyframes",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5148",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "Adaptive user interfaces (AUIs) can improve user experience by adapting how information and functionality are presented in a user interface.\r\nHowever, the dynamic nature and potentially numerous variations of AUIs make them challenging to author.\r\nIn this paper, we present a generalized framework for defining adaptation as interpolations between UIs and introduce a computational approach for intelligently generating new variations of a UI from a small set of designs.\r\nBased on this approach, we built FrameKit, an authoring tool with a programming-by-example interface that retains flexibility and control afforded by manual authoring while reducing effort through automatic generation.\r\nWe demonstrate that FrameKit can support adaptations that typically require domain-specific toolkits, such as those found in context-aware applications, responsive UIs, and ability-based adaptation.\r\nWe evaluated FrameKit with ten front-end developers, who successfully authored AUIs after a short tutorial session and suggested that FrameKit provides an effective mental model for AUI authoring.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 139015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Reality Labs Research",
              "dsl": ""
            }
          ],
          "personId": 138903
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta Reality Labs Research",
              "dsl": ""
            }
          ],
          "personId": 138999
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 138996
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 139117
        }
      ]
    },
    {
      "id": 139219,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "AI Comes Out of the Closet: Using AI-Generated Virtual Characters to Help Individuals Practice LGBTQIA+ Advocacy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-9467",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "Despite significant historical progress, discrimination and social stigma continue to impact the lives of LGBTQIA+ individuals. The use of AI-generated virtual characters offers a unique opportunity to facilitate advocacy by engaging individuals in simulated conversations that can foster understanding, education, and empathy. This paper explores the potential of AI simulations in helping individuals practice LGBTQIA+ advocacy, while also acknowledging the need for ethical considerations and addressing concerns about oversimplification or perpetuation of stereotypes. By combining technological innovation with a commitment to inclusivity, we aim to contribute to the ongoing struggle for equality in both the legal framework and the hearts and minds of the community. We present a study evaluating virtual characters driven by generative conversational AI simulating the social interactions surrounding ‘coming out of the closet’, a rite of passage associated with LGBTQIA+ communities. In our study, virtual characters embodied as queer individuals engage with users in a text-based conversation simulation paired with visual representations. We investigate how the interactions between the virtual characters and a user influence the user’s comfort, confidence, empathy and sympathy. We developed an AI simulation with distinct visual personas and deployed a series of conditions. We explore the potential of these interfaces for simulating queer social interactions to enhance LGBTQIA+ potential and cultural acceptance. We present findings from such deployments involving 323 users. Finally, we discuss the design implications of our work on the potential future of embodied, self-actuated and openly LGBTQIA+ intelligent agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab, MIT, Cambridge "
            }
          ],
          "personId": 138921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 138997
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": ""
            }
          ],
          "personId": 139140
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 138990
        }
      ]
    },
    {
      "id": 139220,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Creating an African American-Sounding TTS: Guidelines, Technical Challenges, and Surprising Evaluations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-1629",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "Representations of AI agents in user interfaces and robotics are predominantly White, not only in terms of facial and skin features but also in the synthetic voices they use. In this paper we explore some unexpected challenges in the representation of race we found in the process of developing an U.S. English Text-to-Speech (TTS) system aimed to sound like an educated, professional, regional accent-free African American woman. The paper starts by presenting the results of focus groups with African American IT professionals where guidelines and challenges for the creation of a representative and appropriate TTS system were discussed and gathered, followed by a discussion about some of the technical difficulties faced by the TTS system developers. We then describe two studies with U.S. English speakers where the participants were not able to attribute the correct race to the African American TTS voice while overwhelmingly correctly recognizing the race of a White TTS system of similar quality. A focus group with African American IT workers not only confirmed the representativeness of the African American voice we built but also suggested that the surprising recognition results may have been caused by the inability or the latent prejudice from non-African Americans to associate educated, non-vernacular, professionally-sounding voices to Black people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "São Paulo",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139151
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139152
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "São Paulo",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139023
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "São Paulo",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139129
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139082
        }
      ]
    },
    {
      "id": 139221,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "DynamicLabels: Supporting Informed Construction of Machine Learning Label Sets with Crowd Feedback",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-1942",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "Label set construction—deciding on a group of distinct labels—is an essential stage in building a supervised machine learning (ML) application, as a badly designed label set negatively affects subsequent stages, such as training dataset construction, model training, and model deployment. Despite its significance, it is challenging for ML practitioners to come up with a well-defined label set, especially when no external references are available. Through our formative study (n=8), we observed that even with the help of external references or domain experts, ML practitioners still need to go through multiple iterations to gradually improve the label set. In this process, there exist challenges in collecting helpful feedback and utilizing it to make optimal refinement decisions. To support informed refinement, we present DynamicLabels, a system that aims to support a more informed label set-building process with crowd feedback. Crowd workers provide annotations and label suggestions to the ML practitioner’s label set, and the ML practitioner can review the feedback through multi-aspect analysis and refine the label set with crowd-made labels. Through a within-subjects study (n=16) using two datasets, we found that DynamicLabels enables better understanding and exploration of the collected feedback and supports a more structured and flexible refinement process. The crowd feedback helped ML practitioners explore diverse perspectives, spot current weaknesses, and shop from crowd-generated labels. Metrics and label suggestions in DynamicLabels helped in obtaining a high-level overview of the feedback, gaining assurance, and spotting surfacing conflicts and edge cases that could have been overlooked.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daegu",
              "institution": "DGIST",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 138951
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139043
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139157
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 139109
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138912
        }
      ]
    },
    {
      "id": 139222,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Sensing the Machine: Evaluating Multi-modal Interaction for Intelligent Dynamic Guidance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3206",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "Recent research has demonstrated the potential for representing intelligent guidance using multi-modal cues, yet few guidelines or processes exist to guide the design of such a system. In this paper, we seek to address this gap by investigating the design of multi-modality assistant systems for setting the optimal parameters in industrial plants. We present the results of our study conducted with 22 participants to evaluate the effectiveness and experience of different combinations of visual (Highlights and Ambient lights) and haptic (Clicks and Vibration) modalities for providing intelligent dynamic guidance. Our findings demonstrate that providing the intelligent guidance with the multi modality of Highlights+Ambient resulted in shorter task duration and higher practicality than Ambient lights alone. Moreover, Highlights+Ambient+Vibration guidance was rated with lower usability than Highlights+Ambient as well as higher mental demand than merely Highlights.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology",
              "dsl": "Center for Technology Experience"
            }
          ],
          "personId": 139037
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology GmbH",
              "dsl": "Center for Technology Experience"
            }
          ],
          "personId": 139007
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology",
              "dsl": "Center for Technology Experience"
            }
          ],
          "personId": 139171
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technnology",
              "dsl": "Center for Technology Experience"
            }
          ],
          "personId": 138932
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg & AIT",
              "institution": "University of Salzburg & AIT",
              "dsl": ""
            }
          ],
          "personId": 139163
        }
      ]
    },
    {
      "id": 139223,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Understanding Novice's Annotation Process For 3D Semantic Segmentation Task With Human-In-The-Loop",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8381",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "Large-scale 3D point clouds are often used as training data for 3D semantic segmentation, but the labor-intensive nature of the annotation process challenges the acquisition of sufficient labeled data. Meanwhile, there has been limited research on introducing novice annotators to acquire the labeled data by enhancing their annotation performance and user experience. Therefore, in this study, we explored solutions involving two dimensions: the presence of AI assistance and the number of classes visualized simultaneously in model's segmentation results in HITL. We conducted a user study with 16 novice annotators who had no prior experience in 3D semantic segmentation, asking them to perform annotation tasks. The results revealed an interaction effect between the two dimensions on annotation accuracy and labeling efficiency. We also found that displaying multiple classes at once reduced the time taken for annotation. Moreover, visualizing multiple classes at once or the absence of AI assistance led to a greater increase in model accuracy compared to our baselines. The best user experience was observed when the visualization showed a single class at a time with AI assistance. Based on these findings, we discuss which environments can enhance novice annotators' annotation performance and user experience in 3D semantic segmentation tasks within HITL contexts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139018
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "Artificial Intelligence Convergence"
            }
          ],
          "personId": 138938
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 138919
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 138904
        }
      ]
    },
    {
      "id": 139224,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Toward Faceted Skill Recommendation in Intelligent Personal Assistants",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7455",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139333
      ],
      "eventIds": [],
      "abstract": "Research continuously shows that, despite the wide range of skills developed for Intelligent Personal Assistants (IPAs), users tend to engage with only a small number of them. One reason for this discrepancy is the issue of skill discoverability, which is commonly addressed through conversational recommendations. Current recommendation strategies, however, are limited due to information asymmetry, lack of interactivity, and an underdeveloped understanding of appropriate grouping of available skills. In this paper, we explore opportunities for interactive faceted skill recommendations using voice interfaces. Through an open card sort user study and semi-structured interviews, we identify and describe five facets driving users’ natural grouping of IPA skills (Thematic, Procedural, Cross-system, Environmental, and Recipient) and demonstrate the need for simultaneous support of these facets. We then discuss the implications of these findings for advancing the discoverability of IPA skills through the design of interactive conversational recommendations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Information"
            }
          ],
          "personId": 139042
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139131
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Information"
            }
          ],
          "personId": 138964
        }
      ]
    },
    {
      "id": 139225,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Snapper: Accelerating Bounding Box Annotation in Object Detection Tasks with Find-and-Snap Tooling",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7774",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139331
      ],
      "eventIds": [],
      "abstract": "Object detection tasks are central to the development of datasets and algorithms in computer vision and machine learning. Despite its centrality, object detection remains tedious and time-consuming due to the inherent interactions that are often associated with drawing precise annotations. In this paper, we introduce Snapper, an interactive and intelligent annotation tool that intercepts bounding box annotations as they're drawn and \"snaps\" them to the nearby object edges in real-time. Through a mixed-design user study with 18 full-time annotators, we compare Snapper’s annotation mode to alternative modes of annotation and find that Snapper enables participants to complete object detection tasks 39% more quickly without diminishing annotation quality. Further, we find that participants perceive Snapper as a tool that is interactively intuitive, trustworthy, and helpful. We conclude by discussing the implications of our findings as they relate to augmenting annotators' conventions for drawing annotations in practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139131
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139021
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 138898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139034
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139130
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139079
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 138967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 138956
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139113
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Santa Clara",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 139127
        }
      ]
    },
    {
      "id": 139226,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Slicing, Chatting, and Refining: A Concept-Based Approach for Machine Learning Model Validation with ConceptSlicer",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7610",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "As machine learning (ML) gains wider adoption in real-world applications, the validation of ML models becomes fundamental for its productization, particularly in safety-critical applications. Recently, data slice finding has emerged as a popular method for validating ML models, but it requires additional metadata or cross-modal embeddings for the slices to be interpretable. We propose ConceptSlicer, an integrated workflow that facilitates the slicing of computer vision models using visual concepts. This approach breaks down the image dataset into interpretable visual concepts, serving as metadata in the slice finding process. Our system offers insights into model issues and enables a deeper understanding of computer vision models' strengths and weaknesses. We evaluate ConceptSlicer through interviews with eight domain experts and machine learning practitioners, and fine-tune the ML models based on their feedback. Our study also highlights varied attitudes towards large foundational models, encouraging contemplation of the challenges and opportunities presented by this technological advancement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "ETH AI Center"
            }
          ],
          "personId": 139041
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 138911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 139009
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 139025
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139022
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California at Davis",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 139120
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Robert Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 139126
        }
      ]
    },
    {
      "id": 139227,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SlopeSeeker: A Search Tool for Exploring a Dataset of Quantifiable Trends",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8501",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "Natural language and search interfaces intuitively facilitate data exploration and provide visualization responses to diverse analytical queries based on the underlying datasets. However, these interfaces often fail to interpret more complex analytical intents, such as discerning subtleties and quantifiable differences between terms like “bump” and “spike” in the context of COVID cases, for example. We address this gap by extending the capabilities of a data exploration search interface for interpreting semantic concepts in time series trends. We first create a comprehensive dataset of semantic concepts by mapping quantifiable univariate data trends such as slope and angle to crowdsourced, semantically meaningful trend labels. The dataset contains quantifiable properties that capture the slope-scalar effect of semantic modifiers like “sharply” and “gradually”, as well as multi-line trends (e.g., “peak,” “valley”). We demonstrate the utility of this dataset in SlopeSeeker, a tool that supports natural language querying of quantifiable trends, such as “show me stocks that tanked in 2010.” The tool incorporates novel scoring and ranking techniques based on semantic relevance and visual prominence to present relevant trend chart responses containing these semantic trend concepts. In addition, SlopeSeeker provides a faceted search interface for users to navigate a semantic hierarchy of concepts from general trends (e.g., “increase”) to more specific ones (e.g., “sharp increase”). A preliminary user evaluation of the tool demonstrates that the search interface supports greater expressivity of queries containing concepts that describe data trends. We identify potential future directions for leveraging our publicly available quantitative semantics dataset in other data domains and for novel visual analytics interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139067
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 139060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 139012
        }
      ]
    },
    {
      "id": 139228,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Impact of Voice Fidelity on Decision Making: A Potential Dark Pattern?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3690",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "Manipulative design in user interfaces (conceptualized as dark patterns) has emerged as a significant impediment to the ethical design of technology and a threat to user agency and freedom of choice. While previous research focused on exploring these patterns in the context of graphical user interfaces, the impact of speech has largely been overlooked. We conducted a listening test (𝑁 = 50) to elicit participants’ preferences regarding different synthetic voices that varied in terms of synthesis method (concatenative vs. neural) and prosodic qualities (speech pace and pitch variance), and then evaluated their impact in an online decision-making study (𝑁 = 101). Our results indicate a significant effect of voice qualities on the participant’s choices, independently from the content of the available options. Our results also indicate that the voice’s perceived engagement, ease of understanding, and domain fit directly translate to its impact on participants’ behavior in decision-making tasks.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 139075
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": "Department of Behavioural and Cognitive Sciences"
            }
          ],
          "personId": 138991
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 139136
        }
      ]
    },
    {
      "id": 139229,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SenseMate: An Accessible and Beginner-Friendly Human-AI Platform for Qualitative Data Analysis",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5030",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139337
      ],
      "eventIds": [],
      "abstract": "Community organizations face challenges in harnessing the power of qualitative data analysis, or sensemaking, to understand the diverse perspectives and needs brought up by their constituents. One of the most time-consuming and tedious parts of sensemaking is qualitative coding, or the process of identifying themes across a large and unstructured corpus of community input. A challenge in qualitative coding is attaining high intercoder reliability, especially between expert and beginner sensemakers. In this work, we present SenseMate, a novel human-AI system designed to help with qualitative coding. SenseMate leverages rationale extraction models, a new machine learning strategy to semi-automate sensemaking, which produces theme recommendations and human-interpretable explanations. The models were trained on a dataset of people’s experiences living in Boston, which was annotated for themes by expert sensemakers. We integrated rationale extraction models into SenseMate through an iterative, human-centered design process revolving around four key design principles derived from an extensive literature review. The design process consisted of three iterations with continuous feedback from seven people associated with community organizations. Through an online experiment involving 180 novice sensemakers, we aimed to determine whether AI-generated recommendations and rationales would decrease coding time, increase intercoder reliability (i.e. Cohen’s kappa), and minimize differences between novice and expert coding decisions (i.e. F-score of participant answers compared to expert gold labels). We found that though the model recommendations and explanations increased coding time by 49 seconds per unit of analysis, they raised intercoder reliability by 29% and coding F-score by 10%. Regarding the effectiveness of SenseMate’s design, participants reported that the platform was generally easy to use. In summary, Sensemate is (1) built for beginner sensemakers without a technical background, a user group that prior work doesn’t focus on, (2) implements rationale extraction models to recommend themes and generate explanations, which has advantages over large language models in terms of user privacy and control, and (3) contains original and intuitive features created from user feedback that can be applied to future QDA systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Center for Constructive Communication"
            }
          ],
          "personId": 139053
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 138933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 139077
        }
      ]
    },
    {
      "id": 139230,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Utilizing a Dense Video Captioning Technique for Generating Image Descriptions of Comics for People with Visual Impairments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4062",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "To improve the accessibility of visual figures, auto-generation of text description of individual images has been studied. However, it cannot be directly applied to comics as the descriptions can be redundant as similar scenes appear in a row. To address this issue, we propose generating the descriptions per group of related images and demonstrate how an dense captioning technique for videos can be utilized for this purpose and ways to improve its performance. To assess the effectiveness of our approach and to identify factors affecting the quality of text descriptions of comics, we conducted a preliminary study with 3 sighted evaluators and a main user study with 12 participants with visual impairments. The results show that text descriptions generated per group of images are perceived to be better than those generated per image in terms of accuracy, clarity, understandability, length, informativeness and preference for sighted groups, when annotator is human. In the same conditions, when the annotator is AI, it exhibited better performance in terms of length. Also, people with visual impairments prefer group descriptions because of conciseness, smooth connectivity of sentences, and non-repetitive features. Based on the findings, we provide design recommendations for generating accessible comic descriptions at a scale for blind users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "The Department of Artificial Intelligence Convergence"
            }
          ],
          "personId": 138982
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "The Department of Artificial Intelligence Convergence"
            }
          ],
          "personId": 139177
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "The Department of Artificial Intelligence Convergence, Ewha Womans University",
              "dsl": ""
            }
          ],
          "personId": 139072
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Ewha Womans University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 138904
        }
      ]
    },
    {
      "id": 139231,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SoK: An Exhaustive Taxonomy of Display Issues for Mobile Applications",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8826",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "Display issues, often arising from design inconsistencies or software problems, can have a significant impact on both user experience and system functionality. This study focuses on three primary challenges in the field of display issues: the absence of a standardized classification system, the limitations of existing detection tools, and the inadequacy of available data. To systematically address these challenges, we introduce a Comprehensive Display Issue Analysis Framework (DIS). Utilizing this framework, we construct a comprehensive and industry-validated taxonomy for display issues. When evaluating the capabilities of existing detection tools and the completeness of available data against this taxonomy, we find that current mainstream tools can identify only 77\\% of the cataloged display issues. This finding suggests that, although the field has received some attention from the industry, there is still room for further improvement and research. This study not only deepens our understanding of the classification of display issues and the capabilities of detection tools, but also provides valuable insights for future research and applications in this domain.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen Technology University",
              "dsl": ""
            }
          ],
          "personId": 139149
        },
        {
          "affiliations": [
            {
              "country": "Nigeria",
              "state": "Kano",
              "city": "Wudil",
              "institution": "Aliko Dangote University of Science and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138902
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Nanyang Technological University",
              "dsl": ""
            }
          ],
          "personId": 139081
        }
      ]
    },
    {
      "id": 139232,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Take It, Leave It, or Fix It: Measuring Productivity and Trust in Human-AI Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-6328",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "Although recent developments in generative AI have greatly enhanced the capabilities of conversational agents such as Google's Bard or OpenAI's ChatGPT, it's unclear whether the usage of these agents aids users across various contexts. To better understand how access to conversational AI affects productivity and trust, we conducted a mixed-methods, task-based user study, observing 76 software engineers (N=76) as they completed a programming exam with and without access to Bard. Effects on performance, efficiency, satisfaction, and trust vary depending on user expertise, question type (open-ended \"solve\" questions vs. definitive \"search\" questions), and measurement type (demonstrated vs. self-reported). Our findings include evidence of automation complacency, increased reliance on the AI over the course of the task, and increased performance for novices on “solve”-type questions when using the AI. We discuss common behaviors, design recommendations, and impact considerations to improve collaborations with conversational AI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google",
              "dsl": "Google Research"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 138947
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google",
              "dsl": "Google Research"
            }
          ],
          "personId": 139001
        }
      ]
    },
    {
      "id": 139233,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Studying Collaborative Interactive Machine Teaching in Image Classification",
      "addons": {
        "config": {
          "type": "config",
          "virtualPresentation": true
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-3136",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139329
      ],
      "eventIds": [],
      "abstract": "While human-centered approaches to machine learning explore various human roles within the interaction loop, the notion of Interactive Machine Teaching (IMT) emerged with a focus on leveraging the teaching skills of humans as a teacher to build machine learning systems. However, most systems and studies are devoted to single users. In this article, we study collaborative interactive machine teaching in the context of image classification to analyze how people can structure the teaching process collectively and to understand their experience. Our contributions are threefold. First, we developed a web application called TeachTOK that enables groups of users to curate data and train a model together incrementally. Second, we conducted a study in which ten participants were divided into three teams that competed to build an image classifier in nine days. Qualitative results of participants' discussions in focus groups reveal the emergence of collaboration patterns in the machine teaching task, how collaboration helps revise teaching strategies and participants' reflections on their interaction with the TeachTOK application. From these findings we provide implications for the design of more interactive, collaborative and participatory machine learning-based systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "CNRS, Laboratoire Interdisciplinaire des Sciences du Numérique",
              "dsl": "Université Paris-Saclay"
            }
          ],
          "personId": 138923
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, Laboratoire Interdisciplinaire des Sciences du Numérique",
              "dsl": ""
            }
          ],
          "personId": 138983
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "CNRS, Laboratoire Interdisciplinaire des Sciences du Numérique",
              "dsl": "Université Paris-Saclay"
            }
          ],
          "personId": 138927
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, Institut des Systèmes Intelligents et de Robotique, ISIR",
              "dsl": ""
            }
          ],
          "personId": 139040
        }
      ]
    },
    {
      "id": 139234,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "AiCommentator: A Multimodal Conversational Agent for Embedded Visualization in Football Viewing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-1037",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139327
      ],
      "eventIds": [],
      "abstract": "Traditionally, sports commentators provide viewers with diverse information, encompassing in-game developments and player performances. Yet young adult football viewers increasingly use mobile devices for deeper insights during football matches. Such insights into players on the pitch and performance statistics support viewers’ understanding of game stakes, creating a more engaging viewing experience. Inspired by commentators’ traditional roles and to incorporate information into a single platform, we developed AiCommentator, a Multimodal Conversational Agent (MCA) for embedded visualization and conversational interactions in football broadcast video. AiCommentator integrates embedded visualization, either with an automated non-interactive or with a responsive interactive commentary mode. Our system builds upon multimodal techniques, integrating computer vision and large language models, to demonstrate ways for designing tailored, interactive sports-viewing content. AiCommentator’s event system infers game states based on a multi-object tracking algorithm and computer vision backend, facilitating automated responsive commentary. We address three key topics: evaluating young adults’ satisfaction and immersion across the two viewing modes, enhancing viewer understanding of in-game events and players on the pitch, and devising methods to present this information in a usable manner. In a mixed-method evaluation (n=16) of AiCommentator, we found that the participants appreciated aspects of both system modes but preferred the interactive mode, expressing a higher degree of engagement and satisfaction. Our paper reports on our development of AiCommentator and presents the results from our user study, demonstrating the promise of interactive MCA for a more engaging sports viewing experience. Systems like AiCommentator could be pivotal in transforming the interactivity and accessibility of sports content, revolutionizing how sports viewers engage with video content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": "MediaFutures"
            }
          ],
          "personId": 139074
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": ""
            }
          ],
          "personId": 138944
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "The University of Bergen",
              "dsl": ""
            }
          ],
          "personId": 138941
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Tromso",
              "institution": "NORCE",
              "dsl": ""
            }
          ],
          "personId": 138914
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": "Department of information science and media studies"
            }
          ],
          "personId": 138913
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Miyagi",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 138924
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": ""
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Gothenburg",
              "institution": "Chalmers University of Technology",
              "dsl": "t2i lab, Interaction Design, CSE"
            }
          ],
          "personId": 139051
        }
      ]
    },
    {
      "id": 139235,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "The Impact of Explanations on Fairness in Human-AI Decision-Making: Protected vs Proxy Features",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-8661",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139328
      ],
      "eventIds": [],
      "abstract": "AI systems have been known to amplify biases in real-world data. Explanations may help human-AI teams address these biases for fairer decision-making. Typically, explanations focus on salient input features. If a model is biased against some protected group, explanations may include features that demonstrate this bias, but when biases are realized through proxy features, the relationship between this proxy feature and the protected one may be less clear to a human. In this work, we study the effect of the presence of protected and proxy features on participants' perception of model fairness and their ability to improve demographic parity over an AI alone. Further, we examine how different treatments---explanations, model bias disclosure and proxy correlation disclosure---affect fairness perception and parity. We find that explanations help people detect direct biases but not indirect biases. Additionally, regardless of bias type, explanations tend to increase agreement with model biases. Disclosures can help mitigate this effect for indirect biases, improving both unfairness recognition and decision-making fairness. We hope that our findings can help guide further research into advancing explanations in support of fair human-AI decision-making.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 138979
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 138918
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland College Park",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138969
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 138960
        }
      ]
    },
    {
      "id": 139236,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "SCAINs Presenter: Preventing Miscommunication by Detecting Context-Dependent Utterances in Spoken Dialogue",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-7498",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139332
      ],
      "eventIds": [],
      "abstract": "When individuals are talking while performing multiple tasks at the same time, it is sometimes easy to miss parts of a conversation and misinterpret subsequent statements or have difficulty following the conversation. In this work, we aim to identify statements that may lead to misinterpretation of the subsequent statement if missed and to prevent communication discrepancies. Although there have been several attempts to present images and text that provide topics to support conversation, there is currently no system that supports conversation by taking interpretability into account. We propose a conversation support system SCAINs Presenter that presents Statements Crucial for Awareness of Interpretive Nonsense (SCAINs), which are statements that are important for interpreting other sentences and are extracted by reproducing the interpretations of those who missed part of the conversation and those who did not. The unique point of the SCAINs Presenter is to display extracted sentences that influence the context of the subsequent dialogue by taking into account their interpretability. In particular, since SCAINs are sentences that may cause misinterpretation of the subsequent dialogue if they are absent, the SCAINs Presenter helps the users to be aware of the possibility of a conversation gap coming from the misinterpretation.  Our experiments show that when SCAINs are omitted, the intention of the following statements often becomes unclear, and the meaning of the following statements changes. We also found that SCAINs can capture a unique aspect different from the merely important statements. Moreover, the results of case studies in a realistic setting suggest that looking at SCAINs encourages conversation participants to switch their focus from a subtask chat to an ongoing conversation that is a primary task. Our research clarifies the linguistic processing underlying the identification of high-context utterances and demonstrates the effectiveness of using them to support real person-to-person interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 139139
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 138934
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 139169
        }
      ]
    },
    {
      "id": 139237,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "The Trust Recovery Journey. The Effect of the Timing of Errors on the Willingness to Follow AI Advice.",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4508",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139333
      ],
      "eventIds": [],
      "abstract": "Complementing human decision-making with AI advice offers substantial advantages. However, humans do not always trust AI advice appropriately and are overly sensitive to incidental AI errors, even in cases with overall good performance. Today's research still needs to uncover the underlying aspects of trust decline and recovery over time in repeated human-AI interactions. Our work investigates the consequences of incidental AI error on (self-reported) trust and participants' reliance on AI advice. Results from our experiment, where 208 participants evaluated 14 legal cases before and after receiving algorithmic advice, showed that trust significantly decreased after early and late errors but was rapidly restored in both scenarios. Reliance significantly dropped only for early errors but not for late errors. In both scenarios, reliance was able to be restored. Results suggest that late (compared to early) errors are less drastic in trust loss and allow quicker recovery. These findings align with an interpretation in which humans can build up trust over time if a system is performing well, making them more tolerant of incidental AI errors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Department of Industrial Engineering & Innovation Sciences",
              "dsl": "Human-Technology Interaction Group, Eindhoven University of Technology"
            }
          ],
          "personId": 139076
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction"
            }
          ],
          "personId": 138928
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technoloy",
              "dsl": "Human-Technology Interaction Group"
            }
          ],
          "personId": 139089
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Den Bosch",
              "institution": "Jheronimus Academy of Data Science",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction"
            }
          ],
          "personId": 139125
        }
      ]
    },
    {
      "id": 139238,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "The effect of personalizing a psychotherapy conversational agent on therapeutic bond and usage intentions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-5913",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "While 33.6% of college students suffer from mental health problems, only 24.6% of these students with symptoms would seek professional help due to their personal attitudes or costs associated with therapy. Psychotherapy chatbots may offer a solution as they are always available, anonymous, and cost-effective. Research has shown that these chatbots can significantly reduce symptoms of anxiety and depression. However, there is a lack of understanding about the personalization preferences of users and the effects of personalization on health outcomes. To investigate this, we developed a personalizable psychotherapy chatbot designed to provide personalized help. In a randomized controlled trial (n=54), participants were either assigned to a personalizable condition or a non-personalizable control condition. After 1 week of usage, participants had a significantly higher therapeutic bond with the personalized version compared to the baseline. In fact, the therapeutic bond was similar to that between a psychologist and his client. This is a promising result, as a high therapeutic bond has been linked to therapeutic success in psychotherapy. Participants reported that the therapy style, personality, and avatar were the most important personalizable aspects of the chatbot. Participants also liked the chatbot's usage of their name and the transparency about what the chatbot had learned about them. These features are likely important for establishing a strong therapeutic bond with users. However, the ability to personalize the chatbot had no impact on the usage intentions of the participants. This can be explained by the fact that users from both conditions equally reported that the chatbot was able to help them with their mental health. 53 participants also indicated that they would be willing to use a psychotherapy chatbot when integrated with a human therapist. These findings indicate the potential of psychotherapy chatbots and the need for further research on their integration with traditional psychotherapy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139054
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139102
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 138937
        }
      ]
    },
    {
      "id": 139239,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "FARPLS: A Feature-Augmented Robot Trajectory Preference Labeling System to Assist Human Labelers’ Preference Elicitation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24a-4945",
      "source": "PCS",
      "trackId": 12514,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139330
      ],
      "eventIds": [],
      "abstract": "Preference-based learning aims to align robot task objectives with human values. One of the most common methods to infer human preferences is by pairwise comparisons of robot task trajectories. Traditional comparison-based preference labeling systems seldom support labelers to digest and identify critical differences between complex trajectories recorded in videos.  Our formative study (N = 12) suggests that individuals may overlook non-salient task features and establish biased preference criteria during their preference elicitation process because of partial observations. In addition, they may experience mental fatigue when given many pairs to compare, causing their label quality to deteriorate. To mitigate these issues, we propose FARPLS, a Feature-Augmented Robot trajectory Preference Labeling System. FARPLS highlights potential outliers in a wide variety of task features that matter to humans and extracts the corresponding video keyframes for easy review and comparison. It also dynamically adjusts the labeling order according to users’ familiarities, difficulties of the trajectory pair, and level of disagreements. At the same time, the system monitors labelers’ consistency and provides feedback on labeling progress to keep labelers engaged. . A between-subjects study (N = 42, 105 pairs of robot pick-and-place trajectories per person) shows that FARPLS can help users establish preference criteria more easily and notice more relevant details in the presented trajectories than the conventional interface. FARPLS also improves labeling consistency and engagement, mitigating challenges in preference elicitation without raising cognitive loads significantly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 138965
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 139167
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 139056
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 138993
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": ""
            }
          ],
          "personId": 139156
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong Productivity Council",
              "dsl": "Robotics and AI Division"
            }
          ],
          "personId": 138957
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong Industrial Artificial Intelligence and Robotics Centre (FLAIR)",
              "dsl": ""
            }
          ],
          "personId": 138953
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138970
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong Productivity Council",
              "dsl": "Robotics and Artificial Intelligence Division"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong Industrial Artificial Intelligence and Robotics Centre (FLAIR)",
              "dsl": ""
            }
          ],
          "personId": 138897
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 138942
        }
      ]
    },
    {
      "id": 139354,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "How Do Users Experience Traceability of AI Systems? Examining Subjective Information Processing Awareness in Automated Insulin Delivery (AID) Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "TiiS2",
      "source": "CSV",
      "trackId": 12508,
      "tags": [],
      "keywords": [
        "AI Traceability"
      ],
      "sessionIds": [
        139335
      ],
      "eventIds": [],
      "abstract": "When interacting with artificial intelligence (AI) in the medical domain, users frequently face automated information processing, which can remain opaque to them. For example, users with diabetes may interact daily with automated insulin delivery (AID). However, effective AID therapy requires traceability of automated decisions for diverse users. Grounded in research on human-automation interaction, we study Subjective Information Processing Awareness (SIPA) as a key construct to research users’ experience of explainable AI. The objective of the present research was to examine how users experience differing levels of traceability of an AI algorithm. We developed a basic AID simulation to create realistic scenarios for an experiment with N = 80, where we examined the effect of three levels of information disclosure on SIPA and performance. Attributes serving as the basis for insulin needs calculation were shown to users, who predicted the AID system’s calculation after over 60 observations. Results showed a difference in SIPA after repeated observations, associated with a general decline of SIPA ratings over time. Supporting scale validity, SIPA was strongly correlated with trust and satisfaction with explanations. The present research indicates that the effect of different levels of information disclosure may need several repetitions before it manifests. Additionally, high levels of information disclosure may lead to a miscalibration between SIPA and performance in predicting the system’s results. The results indicate that for a responsible design of XAI, system designers could utilize prediction tasks in order to calibrate experienced traceability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Universit?t zu L?beck"
            }
          ],
          "personId": 139348
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Universit?t zu L?beck"
            }
          ],
          "personId": 139339
        }
      ]
    },
    {
      "id": 139355,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "Visual Analytics of Co-Occurrences to Discover Subspaces in Structured Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "TiiS1",
      "source": "CSV",
      "trackId": 12508,
      "tags": [],
      "keywords": [
        "Visual Analytics"
      ],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "We present an approach that shows all relevant subspaces of categorical data condensed in a single picture. We model the categorical values of the attributes as co-occurrences with data partitions generated from structured data using pattern mining. We show that these co-occurrences are a-priori, allowing us to greatly reduce the search space, effectively generating the condensed picture where conventional approaches filter out several subspaces as these are deemed insignificant. The task of identifying interesting subspaces is common but difficult due to exponential search spaces and the curse of dimensionality. One application of such a task might be identifying a cohort of patients defined by attributes such as gender, age, and diabetes type that share a common patient history, which is modeled as event sequences. Filtering the data by these attributes is common but cumbersome and often does not allow a comparison of subspaces. We contribute a powerful multi-dimensional pattern exploration approach (MDPE-approach) agnostic to the structured data type that models multiple attributes and their characteristics as co-occurrences, allowing the user to identify and compare thousands of subspaces of interest in a single picture. In our MDPE-approach, we introduce two methods to dramatically reduce the search space, outputting only the boundaries of the search space in the form of two tables. We implement the MDPE-approach in an interactive visual interface (MDPE-vis) that provides a scalable, pixel-based visualization design allowing the identification, comparison, and sense-making of subspaces in structured data. Our case studies using a gold-standard dataset and external domain experts confirm our approach’s and implementation’s applicability. A third use case sheds light on the scalability of our approach and a user study with 15 participants underlines its usefulness and power.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "University of Konstanz"
            }
          ],
          "personId": 139350
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "4Soft GmbH"
            }
          ],
          "personId": 139353
        },
        {
          "affiliations": [
            {
              "country": "The Netherlands",
              "institution": "Utrecht University"
            }
          ],
          "personId": 139352
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "institution": "ETH AI Center"
            }
          ],
          "personId": 139347
        },
        {
          "affiliations": [
            {
              "country": "United States of America",
              "institution": "University of California-Davis"
            }
          ],
          "personId": 139346
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "University of Konstanz"
            }
          ],
          "personId": 139349
        }
      ]
    },
    {
      "id": 139356,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "GRAFS: Graphical Faceted Search System to Support Conceptual Understanding in Exploratory Search",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "TiiS4",
      "source": "CSV",
      "trackId": 12508,
      "tags": [],
      "keywords": [
        "Graphical Search"
      ],
      "sessionIds": [
        139334
      ],
      "eventIds": [],
      "abstract": "When people search for information about a new topic within large document collections, they implicitly construct a mental model of the unfamiliar information space to represent what they currently know and guide their exploration into the unknown. Building this mental model can be challenging as it requires not only finding relevant documents but also synthesizing important concepts and the relationships that connect those concepts both within and across documents. This article describes a novel interactive approach designed to help users construct a mental model of an unfamiliar information space during exploratory search. We propose a new semantic search system to organize and visualize important concepts and their relations for a set of search results. A user study (n=20) was conducted to compare the proposed approach against a baseline faceted search system on exploratory literature search tasks. Experimental results show that the proposed approach is more effective in helping users recognize relationships between key concepts, leading to a more sophisticated understanding of the search topic while maintaining similar functionality and usability as a faceted search system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States of America",
              "state": "NC",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill"
            }
          ],
          "personId": 139340
        },
        {
          "affiliations": [
            {
              "country": "United States of America",
              "state": "NC",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill"
            }
          ],
          "personId": 139341
        },
        {
          "affiliations": [
            {
              "country": "United States of America",
              "state": "NC",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill"
            }
          ],
          "personId": 139344
        },
        {
          "affiliations": [
            {
              "country": "United States of America",
              "state": "NC",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill"
            }
          ],
          "personId": 139345
        }
      ]
    },
    {
      "id": 139357,
      "typeId": 13294,
      "durationOverride": 13,
      "title": "XAutoML: A Visual Analytics Tool for Understanding and Validating Automated Machine Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "TiiS3",
      "source": "CSV",
      "trackId": 12508,
      "tags": [],
      "keywords": [
        "Visual Analytics"
      ],
      "sessionIds": [
        139336
      ],
      "eventIds": [],
      "abstract": "In the last 10 years, various automated machine learning (AutoML) systems have been proposed to build end-to-end machine learning (ML) pipelines with minimal human interaction. Even though such automatically synthesized ML pipelines are able to achieve competitive performance, recent studies have shown that users do not trust models constructed by AutoML due to missing transparency of AutoML systems and missing explanations for the constructed ML pipelines. In a requirements analysis study with 36 domain experts, data scientists, and AutoML researchers from different professions with vastly different expertise in ML, we collect detailed informational needs for AutoML. We propose XAutoML, an interactive visual analytics tool for explaining arbitrary AutoML optimization procedures and ML pipelines constructed by AutoML. XAutoML combines interactive visualizations with established techniques from explainable artificial intelligence (XAI) to make the complete AutoML procedure transparent and explainable. By integrating XAutoML with JupyterLab, experienced users can extend the visual analytics with ad-hoc visualizations based on information extracted from XAutoML. We validate our approach in a user study with the same diverse user group from the requirements analysis. All participants were able to extract useful information from XAutoML, leading to a significantly increased understanding of ML pipelines produced by AutoML and the AutoML optimization itself.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "USU Software AG"
            }
          ],
          "personId": 139338
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Institute of Ubiquitous Mobility Systems, Karlsruhe University of Applied Sciences"
            }
          ],
          "personId": 139351
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Institute of Ubiquitous Mobility Systems, Karlsruhe University of Applied Sciences"
            }
          ],
          "personId": 139342
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "institution": "Institute of Industrial Manufacturing and Management IFF, University of Stuttgart"
            }
          ],
          "personId": 139343
        }
      ]
    },
    {
      "id": 139480,
      "typeId": 13297,
      "durationOverride": 210,
      "title": "Adaptive XAI: Towards Intelligent Interfaces for Tailored AI Explanations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1039",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139628
      ],
      "eventIds": [],
      "abstract": "As the integration of Artificial Intelligence into daily decision-making processes intensifies, the need for clear communication between humans and AI systems becomes crucial. The Adaptive XAI (AXAI) workshop focuses on the design and development of intelligent interfaces that can adaptively explain AI's decision-making processes and our engagement with those processes.\r\nIn line with the human-centric principles of the Future Artificial Intelligence Research (FAIR) project, this workshop seeks to explore, understand and develop interfaces that dynamically adapt, thereby creating explanations of AI-based systems that both relate to and resonate with a range of users with different explanation-based requirements. As AI's role in our lives becomes ever more embedded, the ways in which such systems explain elements about the system need to be malleable and responsive to the ever-evolving individual's cognitive state, relating to contextual needs/focus and to the social setting.\r\nFor instance, easy to use and effective interaction modalities like Visual Languages can provide users with intuitive mechanisms to interact with, adjust, and reshape AI narratives. This ensures that a richer, more tailored understanding can be provided, allowing explanations to emerge in line with the users' demands and the ever-shifting contexts they find themselves in, both as individuals and as part of a group.\r\nThe Adaptive XAI workshop extends an invitation to scholars, designers, and tech-nologists to collaboratively shape the future of human-XAI interplay.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Pisa",
              "institution": "University of Pisa",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139380
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Pisa",
              "institution": "University of Pisa",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "Molde",
              "institution": "Molde University College",
              "dsl": "Faculty of Logistics"
            }
          ],
          "personId": 139438
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Pisa",
              "institution": "CNR-ISTI",
              "dsl": "HIIS Laboratory"
            }
          ],
          "personId": 139389
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "NL",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Cognitive Psychology and Ergonomics"
            }
          ],
          "personId": 139439
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Lab"
            }
          ],
          "personId": 139434
        }
      ]
    },
    {
      "id": 139481,
      "typeId": 13297,
      "durationOverride": 510,
      "title": "Past Meets Future: Human-AI Interaction for Digital History and Cultural Heritage",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1038",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139656
      ],
      "eventIds": [],
      "abstract": "Digital History and Cultural Heritage encapsulate invaluable societal narratives, yet scholars and practitioners face challenges in data quality, accessibility, and engagement. Human-AI Interaction (HAI) holds promise to address these challenges, fostering enhanced analysis, discoverability, and storytelling at scale. However, its potential remains largely untapped by the HAI community. This workshop aimed to bridge this gap, inviting inviting scholars and practitioners from fields such as human-computer interaction (HCI), artificial intelligence (AI), history, cultural heritage, and GLAMs (galleries, libraries, archives, and museums) to explore innovative HAI methodologies and frameworks tailored to these domains. Through interdisciplinary dialogue, we aimed to propose tractable solutions, enriching both the Digital History and Cultural Heritage sectors, as well as the HAI field, while nurturing a fertile ground for historical storytelling and meaningful engagement with our shared past.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Arlington",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139422
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 139462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            },
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "Library of Congress",
              "dsl": ""
            }
          ],
          "personId": 139410
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 139367
        }
      ]
    },
    {
      "id": 139482,
      "typeId": 13290,
      "title": "Exploring AI-assisted Ideation and Prototyping for Choreography",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-9293",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Choreography creation is a multimodal endeavor, demanding cognitive abilities to develop creative ideas and technical expertise to convert choreographic ideas into physical dance movements. Previous research has sought to reduce the complexities in the choreography creation process in both dimensions. Among them, non-AI systems have focused on reinforcing cognitive activities by assisting in analyzing and understanding dance movements and supporting physical capabilities by enhancing body expressivity. In contrast, AI-based methods have helped to address the creation of novel materials with generative AI algorithms. The choreography creation process is constrained by time and requires a rich set of resources to stimulate novel ideas, but the need for iterative prototyping and the physical dependence have not been adequately addressed in prior work. Recognizing these challenges and the research gap, we present an innovative AI-based choreography-support system. Our primary goal is to facilitate rapid ideation by utilizing a generative AI model that can produce diverse and novel dance sequences. The system is designed to support digital dance prototyping in an iterative fashion through an interactive web-based user interface that enables editing and modification of generated motion. We evaluated our system by inviting six choreographers to analyze its limitations and benefits and present the results along with directions for future work. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UC Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139427
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UCSB",
              "dsl": "Computer Science "
            }
          ],
          "personId": 138990
        }
      ]
    },
    {
      "id": 139483,
      "typeId": 13297,
      "durationOverride": 510,
      "title": "Intelligent User Interface for Metaverse",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1035",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139652
      ],
      "eventIds": [],
      "abstract": "Recent research in eXtended Reality (XR) Technologies, Computer Vision and AI are redefining interaction with electronic media in new ways which was not thought before. From its inception at the 1956 Dartmouth Workshop, Artificial Intelligence (AI) revitalized itself many times by finding new theories and applications. The newly popularized concept of Metaverse can be an apt platform to merge AI and XR technologies together. This workshop is planned to investigate user interface and interaction issues with Metaverse and scoping the use of AI tools and technology for improving UI/UX for Metaverse. As part of the workshop, the very concept of Metaverse will be explored in details with member from academia, industry and standardization bodies.  Paper and poster submissions will be solicited and interactive demonstration sessions will be organized to explore various use cases and related UI/UX challenges in Metaverse. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 138759
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science Bangalore",
              "dsl": "I3D Lab"
            },
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Siemens",
              "dsl": ""
            }
          ],
          "personId": 139436
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Ipswich",
              "institution": "BT",
              "dsl": "Research Labs"
            }
          ],
          "personId": 139469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Meta",
              "dsl": "Reality Labs Research"
            }
          ],
          "personId": 139460
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Bellaterra",
              "institution": "Universitat Autònoma de Barcelona",
              "dsl": ""
            }
          ],
          "personId": 139458
        }
      ]
    },
    {
      "id": 139485,
      "typeId": 13290,
      "title": "CARE: An Infrastructure for Evaluation of Carousel-Based Recommender Interfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-5924",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Carousel-based interfaces are integral to enhancing user experience in online recommender systems such as streaming services or e-commerce platforms, yet their usability evaluation often lacks a standardized approach. \r\nWhile there is existing work on supporting the process of evaluation of recommender systems - from toolkits to infrastructure - they predominantly concentrate on assessing the recommendation algorithms rather than the user experience. This focus results in a limited understanding of the overall effectiveness of modern recommender systems, as it overlooks the role of user interface design, particularly carousel-based interfaces, in overall user experience. \r\nIn response to this gap, this paper introduces a web-based infrastructure specifically designed for the usability assessment of carousel-based interfaces. Our infrastructure is versatile, suitable for various domains and experimental setups, and holds the potential for expansion due to its modular design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 139429
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 139405
        }
      ]
    },
    {
      "id": 139486,
      "typeId": 13290,
      "title": "Generative AI syntheses of platform, content, visuals, and kinetics for cyberphysical computationally-mediated posters and broader applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-1902",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "We demonstrate a modular display devices that interweaves artificial intelligence, tangible interaction, and an ACM IUI corpus. Our design incorporates a low-cost, shape-changing tangible interface with physical input buttons and a Raspberry Pi-driven display, within a structure of varying scale, sidedness, and materiality. The demonstration illustrates generative AI syntheses of varying-scale physical and virtual platforms; of rich content, drawing upon all published ACM IUI articles and Wikipedia; visuals, automatically synthesized for varying-scale & medium displays; and kinetics, as well as illustrating paths for generatively upcycling second-life computational technologies.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human Centered Computing division, School of Computing"
            }
          ],
          "personId": 139392
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139411
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human Centered Computing division, School of Computing, Tangible Visualization Lab"
            }
          ],
          "personId": 139409
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Architecture"
            }
          ],
          "personId": 139476
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Department of Genetics & Biochemistry"
            }
          ],
          "personId": 139478
        }
      ]
    },
    {
      "id": 139487,
      "typeId": 13289,
      "durationOverride": 510,
      "title": "EPIC: Enhanced Privacy and Integrity Considerations for Research (Tutorial)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24d-1004",
      "source": "PCS",
      "trackId": 12512,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139630
      ],
      "eventIds": [],
      "abstract": "We propose a full-day tutorial on enhancing the privacy and integrity of human subjects research at the point of study design. Participants will receive hands-on training in how to determine sample size and collect participant demographics in a way that prioritizes data integrity, participant privacy, and sample representativeness. The tutorial also invites participants to discuss and troubleshoot the unique challenges to and opportunities for designing robust and ethical human-centered AI research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": ""
            }
          ],
          "personId": 139463
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139421
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "HCC/School of Computing"
            }
          ],
          "personId": 138858
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139440
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Data Science Institute"
            }
          ],
          "personId": 139372
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139453
        }
      ]
    },
    {
      "id": 139488,
      "typeId": 13290,
      "title": "NotebookGPT – Facilitating and Monitoring Explicit Lightweight Student GPT Help Requests During Programming Exercises",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-1903",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "The improved performance of GPT on coding tasks has forced educators to consider the impact of GPT and similar models on teaching programming. Students' use of GPT to solve programming problems can hinder their learning. However, they might also get significant benefits such as quality feedback on programming style, explanations of how a given piece of code works, help with debugging code, and the ability to see valuable alternatives to existing code solutions. We propose a new design for interacting with GPT called Mediated GPT with the goals of (a) providing students with access to GPT but allowing instructors to programmatically modify responses to prevent hindrances to student learning and combat common GPT response concerns, (b) helping students generate and learn to create effective prompts to GPT, and (c) tracking how students use GPT to get help on programming exercises. We demonstrate a first-pass implementation of this design called NotebookGPT. In the future, we intend to evaluate if our implementation meets the design goals and if students using this implementation reap the proposed benefits of GPT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139425
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139459
        }
      ]
    },
    {
      "id": 139489,
      "typeId": 13290,
      "title": "EvaluLLM: LLM assisted evaluation of generative outputs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-5843",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "With the rapid improvement in large language model (LLM) capabilities, its becoming more difficult to measure the quality of outputs generated by natural language generation (NLG) systems. Conventional metrics such as BLEU and ROUGE are bound to reference data, and are generally unsuitable for tasks that require creative or diverse outputs. Human evaluation is an option, but manually evaluating generated text is difficult to do well, and expensive to scale and repeat as requirements and quality criteria change. Recent work has focused on the use of LLMs as customize-able NLG evaluators, and initial results are promising. In this demonstration we present EvaluLLM, an application designed to help practitioners setup, run and review evaluation over sets of NLG outputs, using an LLM as a custom evaluator. Evaluation is formulated as a series of choices between pairs of generated outputs conditioned on a user provided evaluation criteria. This approach simplifies the evaluation task and obviates the need for complex scoring algorithms. The system can be applied to general evaluation, human assisted evaluation, and model selection problems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139473
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": "Thomas J. Watson Center"
            }
          ],
          "personId": 139471
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139388
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 139466
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI X-Lab"
            }
          ],
          "personId": 139452
        }
      ]
    },
    {
      "id": 139490,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Advancing GUI for Generative AI: Charting the Design Space of Human-AI Interactions through Task Creativity and Complexity",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1003",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "West Hyattsville",
              "institution": "University of Maryland",
              "dsl": "College of Information Studies"
            }
          ],
          "personId": 139457
        }
      ]
    },
    {
      "id": 139491,
      "typeId": 13290,
      "title": "Evaluating ADHD Users’ Experience with Recommender Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-2399",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Although there is a significant increase in research on the user experience of recommender systems, they do not delineate varied experiences based on cognitive abilities. In this paper, we evaluate the impact of recommender systems on users with the neurodevelopmental classification of Attention Deficit Hyperactivity Disorder (ADHD). Through constructivist grounded theory analysis of six contextual interviews, we formulate an initial theory explaining how personalized recommendations exacerbate ADHD users' self-regulatory challenges leading to overarching detrimental consequences in ADHD users' interpersonal lives. Furthermore, though participants found community and social support through personalized recommendations, the challenges of personalized recommendations outweighed the benefits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human Centered Computing/HATLAB"
            }
          ],
          "personId": 139378
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human-Centered Computing/School of Computing"
            }
          ],
          "personId": 139406
        }
      ]
    },
    {
      "id": 139492,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Human-Centered Evaluation of Explanations in AI-Assisted Decision-Making",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1004",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 139365
        }
      ]
    },
    {
      "id": 139493,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Enhancing Digital Privacy Education for Older Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1005",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Department of Human-Centered Computing"
            }
          ],
          "personId": 139401
        }
      ]
    },
    {
      "id": 139494,
      "typeId": 13290,
      "title": "Eye-Gaze-Enabled Assistive Robotic Stamp Printing System for Individuals with Severe Speech and Motor Impairment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-2336",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Robotics is a trailblazing technology that has found extensive applications in the field of assistive aids for individuals with severe speech and motor impairment (SSMI). This article describes the design and development of an eye gaze-controlled user interface to manipulate the robotic arm. User studies were reported to engage users through eye gaze input to select stamps from the two designs and select the stamping location on cards using three designated boxes present in the User Interface. The entire process, from stamp selection to stamping location selection, is controlled by eye movements. The user interface contains the print button to initiate the robotic arm that enables the user to independently create personalized stamped cards. Extensive user interface trials revealed that individuals with severe speech and motor impairment showed improvements with a 33.2% reduction in the average time taken and a 42.8% reduction in the standard deviation for the completion of the task. This suggests the effectiveness and potential to enhance the autonomy and creativity of individuals with SSMI, contributing to the development of inclusive assistive technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "Department of Design and Manufacturing"
            }
          ],
          "personId": 139475
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "Department of Design and Manufacturing"
            }
          ],
          "personId": 139394
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian institute of science",
              "dsl": "Robert Bosch Centre for Cyber Physical Systems"
            }
          ],
          "personId": 139373
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "Department of Design and Manufacturing"
            }
          ],
          "personId": 139470
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 138759
        }
      ]
    },
    {
      "id": 139495,
      "typeId": 13290,
      "title": "Assistant Dashboard Plus – Enhancing an Existing Instructor Dashboard with Difficulty Detection and GPT-based Code Clustering",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-5763",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "As interest in programming as a major grows, instructors must accommodate more students in their programming courses. One particularly challenging aspect of this growth is providing quality assistance to students during in-class and out-of-class programming exercises. Prior work proposes using instructor dashboards to help instructors combat these challenges. Further, the introduction of ChatGPT represents an exciting avenue to assist instructors with programming exercises but needs a delivery method for this assistance. We propose a new instructor dashboard Assistant Dashboard Plus that extends an existing dashboard with two new features: (a) identifying students in difficulty so that instructors can effectively assist them, and (b) providing instructors with pedagogically relevant groupings of students' exercise solutions with similar implementations so that instructors can provide overlapping code style feedback to students within the same group. For difficulty detection, it uses a state-of-the-art algorithm for which a visualization has not been created. For code clustering, it uses GPT. We present a first-pass implementation of this dashboard.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139425
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139386
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139391
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139390
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139464
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139396
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139459
        }
      ]
    },
    {
      "id": 139496,
      "typeId": 13290,
      "title": "Human-AI Collaboration in a Student Discussion Forum",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-6794",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "The recent public releases of AI tools such as ChatGPT have forced computer science educators to reconsider how they teach. These tools have demonstrated considerable ability to generate code and answer conceptual questions, rendering them incredibly useful for completing CS coursework. While overreliance on AI tools could hinder students’ learning, we believe they have the potential to be a helpful resource for both students and instructors alike. We propose a novel system for instructor-mediated GPT interaction in a class discussion board. By automatically generating draft responses to student forum posts, GPT can help Teaching Assistants (TAs) respond to student questions in a more timely manner, giving students an avenue to receive fast, quality feedback on their solutions without turning to ChatGPT directly. Additionally, since they are involved in the process, instructors can ensure that the information students receive is accurate, and can provide students with incremental hints that encourage them to engage critically with the material, rather than just copying an AI-generated snippet of code. We utilize Piazza—a popular educational forum where TAs help students via text exchanges—as a venue for GPT-assisted TA responses to student questions. These student questions are sent to GPT-4 alongside assignment instructions and a customizable prompt, both of which are stored in editable instructor-only Piazza posts. We demonstrate an initial implementation of this system, and provide examples of student questions that highlight its benefits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": ""
            }
          ],
          "personId": 139423
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chappel Hill",
              "institution": "UNC",
              "dsl": "Department of Compuser Science"
            }
          ],
          "personId": 139459
        }
      ]
    },
    {
      "id": 139497,
      "typeId": 13290,
      "title": "Utilizing Large Language Models in Tribal Emergency Management",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-8510",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "This paper explores the unique challenges faced by tribal communities in the context of emergency management, encompassing natural disasters and the preservation of their rich cultural heritage. The study aims to investigate both the potential advantages\r\nand hurdles associated with the adoption of large language models (LLMs) in tribal emergency management. Our primary goal is to qualitatively assess Indigenous perspectives on the suitability and acceptability of deploying an LLM-powered chatbot in this specific domain. To achieve this objective, we employ a think-aloud interview methodology involving 18 tribal members. This qualitative research approach captures participants’ cognitive processes and decision-making as they engage with the language model’s responses in real-time. Through thematic analysis of these verbalized thoughts and the prompts submitted, the study sheds light on various aspects, including usability, information-seeking behavior, and the incorporation of tribal culture considerations when integrating large language models into tribal emergency management practices. The paper concludes with a discussion of potential design implications and contributions to the fields of AI and HCI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Omaha",
              "institution": "University of Nebraska Omaha",
              "dsl": "College of Information Science & Technology"
            }
          ],
          "personId": 139442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Omaha",
              "institution": "University of Nebraska at Omaha",
              "dsl": "School of Public Administration/Digital Governance and Analytics Lab"
            }
          ],
          "personId": 139383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Omaha",
              "institution": "University of Nebraska at Omaha",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 139368
        }
      ]
    },
    {
      "id": 139498,
      "typeId": 13290,
      "title": "Perceived Trustworthiness of Human vs. AI Instructors in Digital Privacy Education for Older Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-4276",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Recent work highlights digital privacy education as a crucial component in overcoming the digital literacy gap among seniors, but also shows that seniors distrust AI systems and prefer a more personable education experience. To this end, we conducted a within-subjects experiment to explore the pivotal role of trust in digital privacy education for older adults, with a specific focus on how the physical characteristics of instructors---human and AI---influence trust levels.  \r\n\r\nIn our study, 36 younger and 27 older participants evaluated 9 introductions to a video tutorial on digital privacy, featuring 3 AI and 6 human instructors (the latter varying by age and gender). \r\n\r\nAnalysis revealed that trust towards the AI instructors was lower than towards the human instructors. Among the AI instructors, a robot with human-like features was the most trusted, while among the human instructors, the older and middle-aged female instructors were the most trusted. Furthermore, participant demographics such as gender and rurality were found to moderate trust levels. \r\n\r\nThis research has implications for instructional design and technology acceptance, particularly in addressing privacy concerns and fostering inclusive digital literacy among the senior population.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139401
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "College of Behavioral, Social and Health Sciences"
            }
          ],
          "personId": 139382
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138806
        }
      ]
    },
    {
      "id": 139499,
      "typeId": 13290,
      "title": "Impact of Personalised AI Chat Assistant on Mediated Human-Human Textual Conversations: Exploring Female-Male Differences",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-3120",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "In text-based communication, it is of utmost importance to prioritise the enhancement of user experiences during the earliest stages of contact, particularly while establishing rapport. This research investigates differences between males and females in the utilisation of an AI conversation assistant at the beginning of a conversation. The system has text ``Recommendation'' and ``Polishing'' capabilities and can customise the linguistic style, selecting either a \\textit{humorous} or a \\textit{respectful} tone. They also have a choice of three distinct levels of AI extraversion. In user evaluation studies, the system received a favourable usability rating, confirming its efficacy. Both male and female participants indicated an elevated sense of comfort and a greater inclination to sustain connections while utilising the AI system. It is noteworthy to mention that male users generally report a greater level of user experience compared to female users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham Univeristy",
              "dsl": ""
            }
          ],
          "personId": 139437
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": ""
            }
          ],
          "personId": 139451
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "School of Electronics & Computer Science"
            }
          ],
          "personId": 139363
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Newcastle upon Tyne",
              "institution": "Newcastle University",
              "dsl": "Open Lab, School of Computing"
            }
          ],
          "personId": 139456
        }
      ]
    },
    {
      "id": 139500,
      "typeId": 13290,
      "title": "Multimodal Target Prediction for Rapid Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-7121",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Intent prediction finds widespread applications in user interface (UI/UX) design to predict target icons, in automotive industry to\r\nanticipate driver’s intent, and in understanding human motion during human-robot interactions (HRI). Predicting human intent\r\ninvolves analyzing factors such as hand motion, eye gaze movement, and gestures. This paper introduces a multimodal intent prediction\r\nalgorithm involving hand and eye gaze using Bayesian fusion. Inverse reinforcement learning was leveraged to learn human preferences\r\nfor the human-robot handover task. Results demonstrate that the proposed approach achieves the highest prediction accuracy of 99.9%\r\nat 60% task completion as compared to state-of-the-art (SOTA) methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            }
          ],
          "personId": 139432
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Tamil Nadu",
              "city": "Vellore",
              "institution": "Vellore Institute of Technology",
              "dsl": "CSE Core"
            }
          ],
          "personId": 139408
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "RBCCPS"
            }
          ],
          "personId": 139419
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bengaluru",
              "institution": "Indian Institute of Sciences",
              "dsl": "I3D Labs"
            }
          ],
          "personId": 139418
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            }
          ],
          "personId": 138848
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 138741
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139433
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 138759
        }
      ]
    },
    {
      "id": 139501,
      "typeId": 13290,
      "title": "(X)AI-SPOT: an (X)AI-Supported Production Process Optimization Tool",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-9680",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "We demonstrate (X)AI-SPOT - (X)AI-Supported Process Optimization Tool - that aims to encourage, facilitate, and enhance AI usage for process engineering and optimization in the production industry. Furthermore, (X)AI-SPOT seeks not to become a one-size-fits-all approach but a framework where each user archetype (Shop Floor Worker, Field Expert, and AI Expert) can receive tailored XAI functionality suited to their unique requirements. Currently, (X)AI-SPOT handles the Shop Floor Worker user archetype, with initial support for the Field Expert. We also describe our tool's architecture w.r.t. extendibility and support of different user archetypes; we share our findings from an expert user interview and conclude with a discussion of design decisions and future work. Our application is available at http://exait.know-center.at/mv-ui.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": "Human AI Interaction"
            }
          ],
          "personId": 139477
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": "Human AI Interaction"
            }
          ],
          "personId": 139387
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Technische Universität Graz",
              "dsl": ""
            }
          ],
          "personId": 139414
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Kapfenberg",
              "institution": "voestalpine",
              "dsl": "Böhler Aerospace"
            }
          ],
          "personId": 139444
        }
      ]
    },
    {
      "id": 139502,
      "typeId": 13297,
      "durationOverride": 210,
      "title": "HUMANIZE'24: Seventh Edition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1005",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139648
      ],
      "eventIds": [],
      "abstract": "HUMANIZE aims to investigate the potential of combining the quantitative, data-driven approaches with the qualitative, theory-driven approaches. We solicit work from researchers that incorporate variables grounded in psychological theory into their adaptive/intelligent systems. These variables allow for designing adaptive systems from a more user-centered approach in terms of requirements or needs based on user characteristics rather than solely interaction behavior, which allow for the creation of systems that take into account explainability, fairness, transparency, and bias aspects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Jönköping",
              "institution": "Jönköping University",
              "dsl": "Department of Computer Science and Informatics"
            }
          ],
          "personId": 138838
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Walldorf",
              "institution": "SAP SE",
              "dsl": "UX, Mobile & Business Services "
            }
          ],
          "personId": 139413
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Koper",
              "institution": "University of Primorska",
              "dsl": "Faculty of Mathematics, Natural Sciences and Information Technologies"
            }
          ],
          "personId": 138832
        }
      ]
    },
    {
      "id": 139503,
      "typeId": 13297,
      "durationOverride": 510,
      "title": "Speech as Interactive Design Material (SIDM): How to design and evaluate task-tailored synthetic voices?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1045",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139660
      ],
      "eventIds": [],
      "abstract": "The aim of this workshop is two-fold. First, it aims to establish a research community focused on design and evaluation of synthetic speech (TTS) interfaces that are tailored not only to goal oriented tasks (e.g., food ordering, online shopping) but also personal growth and resilience promoting applications (e.g., coaching, mindful reflection, and tutoring). Second, through discussion and collaborative efforts, to establish a set of practices and standards that will help to improve ecological validity of TTS evaluation. In particular, the workshop will explore the topics such as: interaction design of voice-based conversational interfaces; the interplay between prosodic aspects (e.g., pitch variance, loudness, jitter) of TTS and its impact on voice perception. This workshop will serve as a platform on which to build a community that is better equipped to tackle the dynamic field of interactive TTS interfaces, which remains understudied, yet increasingly pertinent to everyday lives of users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 139402
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt University",
              "dsl": "Mathematics and Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "-Select-",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 139371
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": "Institute of Information Management"
            }
          ],
          "personId": 139395
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 139450
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Bern",
              "institution": "Bern University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 138984
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Human Centered Design & Engineering"
            }
          ],
          "personId": 139416
        }
      ]
    },
    {
      "id": 139506,
      "typeId": 13290,
      "title": "A Human-AI Collaborative System to Support Mitosis Assessment in Pathology",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-3714",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "This demo presents a human-AI collaborative system to assist pathologists in examining the pathological pattern of mitosis, a critical factor in tumor diagnosis. Traditionally, pathologists face challenges in assessing mitoses due to the task's inherent complexity. The demonstrated system aims to address the problem by designing an enhanced human-artificial intelligence workflow. Firstly, it can guide a pathologist user to regions of interest that have flexible morphologies. Then, inside each region of interest, the system highlights each AI-detected mitosis event along with enriched forms of explainable AI evidence. The system can potentially improve the efficiency and correctness of pathologists' mitosis assessment by enabling them to leverage the power of AI while retaining their clinical expertise and judgment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139399
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Kansas City",
              "institution": "University of Kansas Medical Center",
              "dsl": "Pathology"
            }
          ],
          "personId": 139400
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA David Geffen School of Medicine",
              "dsl": "Department of Pathology and Laboratory Medicine"
            }
          ],
          "personId": 139364
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Baylor College of Medicine and Ben Taub Hospital",
              "dsl": "Department of Pathology & Immunology and Internal Medicine"
            }
          ],
          "personId": 139474
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139032
        }
      ]
    },
    {
      "id": 139507,
      "typeId": 13297,
      "durationOverride": 510,
      "title": "HAI-GEN 2024: 5thWorkshop on Human-AI Co-Creation with Generative Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24c-1040",
      "source": "PCS",
      "trackId": 12511,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139646
      ],
      "eventIds": [],
      "abstract": "Generative AI has rapidly entered the public consciousness with the development of applications such as ChatGPT, Midjourney, and GitHub Copilot. Nielsen recently argued that we have entered a new era of human-computer interaction in which users need only specify what they want and not how it should be produced. This paradigm of intent-based outcome specification shifts control over from people to AI, enabling new forms of co-creativity and co-creation. Although these systems are capable of holding fluent conversations and producing high-fidelity images, difficulties remain regarding their ability to produce outputs that satisfy their users' needs. Our workshop will bring together researchers and practitioners from both the HCI and AI disciplines to explore the implications of this shift in control, deepen our understanding of the human-AI co-creative process, and examine how we can design, build, use, and evaluate human-AI co-creative systems that are both effective and safe.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 138834
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "University of North Carolina at Charlotte",
              "dsl": "Computing and Informatics"
            }
          ],
          "personId": 139461
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 138792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 138822
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 138787
        }
      ]
    },
    {
      "id": 139508,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Developing Resilience through Retiree Volunteerism",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1013",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            },
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Institute for Engaged Aging"
            }
          ],
          "personId": 139417
        }
      ]
    },
    {
      "id": 139509,
      "typeId": 13290,
      "title": "CHARM: a Group Decision Making Support Chatbot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-3631",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Messaging apps, such as Telegram and WhatsApp, are routinely used to communicate, chat and make decisions. Group recommender systems (GRSs) have been introduced as self standing tools to support group interactions and decision-making. We present here a TelegramBot (CHARM) that supports groups to make decision on an arbitrary topic, by leveraging GRSs techniques. CHARM helps elici the group members’ preferences, ranks the items that the members have suggested to be considered, provides a summary of the current status of the discussion, and finally recommends a fair choice. A focus group study has revealed that the designed functionality includes features that users expect to find in a bot for group decision making.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Bosnia and Herzegovina",
              "state": "",
              "city": "Sarajevo",
              "institution": "University of Sarajevo",
              "dsl": ""
            }
          ],
          "personId": 139369
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "BZ",
              "city": "Bolzano",
              "institution": "Free University of Bozen Bolzano",
              "dsl": ""
            }
          ],
          "personId": 139385
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "ngoc.nguyen@udayton.edu",
              "institution": "University of Dayton",
              "dsl": ""
            }
          ],
          "personId": 139377
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 139398
        }
      ]
    },
    {
      "id": 139510,
      "typeId": 13290,
      "title": "A Bicycle Navigation System for Analyzing the Comfort Level of the Cyclist",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-4324",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "In recent years, due to COVID-19, the number of cyclists has increased and the use of bicycles has been promoted worldwide as a means of transportation in Mobility as a Service (MaaS). In this study, we construct a navigation system that recommends the shortest route for a cyclist. During the traversing through the recommended route, the cyclist's facial images are captured and unevenness of the route is recorded periodically; through the camera and an accelerometer of the smartphone. The system analyzes the level of comfort the cyclist experienced by estimating the facial expressions and the degree of unevenness throughout the recommended route. Finally, the information on the experienced level of comfort for the recommended route is annotated in Google Maps. We conducted real-life experiments for two consecutive days in which four subjects took part; where each of the subjects rode bicycles for about 1.5 hours per day following the route recommended by the navigation system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": ""
            }
          ],
          "personId": 139384
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": ""
            }
          ],
          "personId": 139443
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hikone",
              "institution": "Shiga University",
              "dsl": ""
            }
          ],
          "personId": 139366
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Aomori",
              "institution": "Aomori University",
              "dsl": ""
            }
          ],
          "personId": 139379
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 139374
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Sangyo University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Suita",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 139447
        }
      ]
    },
    {
      "id": 139511,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Enhancing Spatial Problem-Solving Through Data-Driven Methods: A Practical Approach Utilizing Immersive Technologies and Multimodal Physical Tracking",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1014",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Flensburg",
              "institution": "Flensburg University of Applied Sciences",
              "dsl": "Faculty for Information and Communication / Usability Lab"
            }
          ],
          "personId": 139375
        }
      ]
    },
    {
      "id": 139512,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Netizen A11y: Engaging Internet Users in Making Visual Media Accessible",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1015",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 139010
        }
      ]
    },
    {
      "id": 139513,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Investigating How to Design Inclusive Data-driven Systems for Diverse User Groups",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1016",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139064
        }
      ]
    },
    {
      "id": 139514,
      "typeId": 13290,
      "title": "Training Adults with Mild to Moderate Dementia in ChatGPT: Exploring Best Practices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-5956",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Amid the surge in interest in generative AI applications, particularly for enhancing daily life, limited attention has been paid to including people with disabilities, such as those with dementia, in the development of emerging AI systems. With a considerable population affected by dementia, this exclusion raises concerns about equity and accessibility in adopting potentially beneficial assistive technologies. The study aims to delineate best practices for teaching individuals with mild to moderate dementia how they can integrate the use of ChatGPT as a support mechanism into their everyday activities. The research was conducted in two remote focus groups which were then followed by optional individual training sessions. The data collected from these sessions uncovered three best practices: 1) employing conversational learning in groups, 2) facilitating proxy interactions, and 3) providing one-on-one guided walkthroughs. The study showcases the interest and capacity of individuals with dementia to learn and engage with generative AI, presenting pathways for incorporating such technology into their lives. These findings not only contribute best practices for teaching individuals with cognitive differences to use emerging generative AI technologies but also highlight the potential for inclusive design of AI systems for people with mild to moderate dementia.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human-Centered Computing/School of Computing"
            }
          ],
          "personId": 139404
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Central",
              "institution": "Clemson University",
              "dsl": "CS/School of Computing/EquiTech "
            }
          ],
          "personId": 139376
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139479
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human-Centered Computing/School of Computing"
            }
          ],
          "personId": 139406
        }
      ]
    },
    {
      "id": 139515,
      "typeId": 13290,
      "title": "XAIVIER: Time Series Classifier Verification with Faithful Explainable AI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-1176",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Ensuring that a machine learning model performs as intended is a critical step before it can be used in practice. This is commonly done by measuring a model's predictive performance (e.g., accuracy). \r\nHowever, in high-stakes settings it is often necessary to verify on which data aspects the model actually relies on. \r\nThis demo presents XAIVIER, the eXplainable AI VIsual Explorer and Recommender, \r\na web application for interactive XAI on time series data. \r\nXAIVIER supports dataset exploration and model inspection, allowing users to explain model predictions using various explainers. \r\nAn explainer recommender is provided to advise users which explainer delivers most faithful explanations for their dataset and model.\r\nFinally, explanation-based grouping is provided to reveal the model's underlying decision-making strategies.\r\nA demo of XAIVIER is available at https://xai-explorer-demo.know-center.at",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": ""
            }
          ],
          "personId": 139428
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": ""
            }
          ],
          "personId": 139441
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": ""
            }
          ],
          "personId": 139455
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": ""
            }
          ],
          "personId": 139446
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": ""
            }
          ],
          "personId": 139387
        }
      ]
    },
    {
      "id": 139516,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Revolutionizing Digital Consent: An Automated Approach to Simplifying and Deciphering Privacy Policies for Empowered User Understanding",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1017",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Penn State University",
              "dsl": "College of IST"
            }
          ],
          "personId": 139472
        }
      ]
    },
    {
      "id": 139517,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "Bridging Domain Expertise and AI through Data Understanding",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1019",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139420
        }
      ]
    },
    {
      "id": 139518,
      "typeId": 13290,
      "title": "ExpressEdit: Video Editing with Natural Language and Sketching",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-7992",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Informational videos serve as a crucial source of conceptual and procedural knowledge. While it is important to make the video more informative and engaging, editing such videos (e.g., trimming, overlaying text/image, etc.) can be difficult and time-consuming. Especially for novice video editors, who often struggle with expressing and executing their editing ideas. We present ExpressEdit, a system that facilitates editing informational videos via natural language text and sketching directly on the video frame by interpreting multimodal editing commands and suggesting applicable edits. Powered by a multimodal technical pipeline, the system interprets (1) temporal, (2) spatial, and (3) operational references in an NL editing command and spatial references from sketching. This work offers insights into building multimodal interfaces for video editing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139094
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": ""
            }
          ],
          "personId": 139145
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139027
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139057
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 138912
        }
      ]
    },
    {
      "id": 139519,
      "typeId": 13290,
      "title": "Measuring Heterogeneity in Language Modeling for Enhanced Chatbot Personalization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-5770",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Federated Learning (FL) is a large-scale distributed paradigm for machine learning characterized by decentralized data and limited communication. Data heterogeneity, or differences in data across clients, is a central challenge in FL that can significantly slow down the learning process. Language modeling involves predicting the probability of a sequence of words in context and is the foundation for Chatbots such as ChatGPT. Currently, the effect of data heterogeneity is not well understood in practical settings such as in language modeling. For instance, (i) it is unclear whether the conventional assumption of bounded gradient dissimilarity holds during training of language models. Furthermore, (ii) algorithms introduced to handle data heterogeneity rely on assumptions which may not hold in practice. An example of this is the SCAFFOLD algorithm relying on smoothness of the objective function, a condition that is often violated by language models. In this work, the role of heterogeneity is investigated while training Recurrent Neural Networks (RNNs) for language modeling by experimentally addressing (i) and (ii) above. To address point (i), the gradient dissimilarity and its correlation to performance degradation under data heterogeneity is measured, with the goal of testing the theoretical assumption above. To address point (ii), SCAFFOLD is compared against baselines to determine whether techniques to mitigate heterogeneity are helpful for language modeling tasks.  Our findings are relevant for the design of adaptive interfaces that leverage language models to interpret and predict user needs more effectively, leading to more engaging and efficient human-computer interaction. By learning about the impact of data heterogeneity in the context on language modeling, we can create more personalized and accurate chatbots to enhance user experience. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Alexandria",
              "institution": "Thomas Jefferson High School for Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 139448
        }
      ]
    },
    {
      "id": 139520,
      "typeId": 13290,
      "title": "Search Result Presentation for Non-Native Language Documents",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-8885",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Many document search systems display search result lists with brief\r\nexcerpts from the matched documents called snippets.  It helps users\r\nquickly locate relevant documents in the search result list, but when\r\nusers search for documents in non-native languages, they cannot read\r\nsnippets quickly.  In this paper, we compare two methods of presenting\r\na search result to users whose native language is different from the\r\nlanguage in which the matched document is written.  One is to show a\r\nsnippet translated into the user's native language, whereas the other\r\nis to show the original snippet together with several keyphrases that\r\nare extracted from the snippet and translated into the user's native\r\nlanguage.  The result of our experiment shows that the former does not\r\nimprove the users' efficiency in locating relevant documents from\r\nsearch result lists, while the latter does.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 139431
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 139403
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            }
          ],
          "personId": 139449
        }
      ]
    },
    {
      "id": 139521,
      "typeId": 13290,
      "title": "A Demonstration of BLIP: A System to Explore Undesirable Consequences of Digital Technologies",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-3472",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "Digital technologies have positively transformed society, but they have also led to undesirable consequences not anticipated at the time of design or development. Insights into past undesirable consequences can help researchers and practitioners gain awareness and anticipate adverse effects, and make informed decisions when developing new technology. To facilitate such exploration, we demonstrate BLIP, a research prototype system that extracts, summarizes, and categorizes undesirable consequences from online articles and academic papers, and present them in an interactive web-based interface. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 139445
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 139454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 139426
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 139412
        }
      ]
    },
    {
      "id": 139522,
      "typeId": 13291,
      "durationOverride": 510,
      "title": "An Estimation of Three-Layered Human’s Trust in Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24e-1012",
      "source": "PCS",
      "trackId": 12510,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139634
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": ""
            }
          ],
          "personId": 139407
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139415
        }
      ]
    },
    {
      "id": 139523,
      "typeId": 13290,
      "title": "Can LLMs Infer Domain Knowledge from Code Exemplars? A Preliminary Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "iui24b-6582",
      "source": "PCS",
      "trackId": 12515,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139568
      ],
      "eventIds": [],
      "abstract": "As organizations recognize the potential of Large Language Models (LLMs), bespoke domain-specific solutions are emerging, which inherently face challenges of knowledge gaps and contextual accuracy. Prompt engineering techniques such as chain-of-thoughts and few-shot prompting are proposed to enhance LLMs’ capabilities by dynamically presenting relevant exemplars. This raises the question: Are LLMs able to infer domain knowledge from code exemplars involving similar domain concepts and analyze the data correctly? To investigate this, we curated a synthetic dataset containing 45 tabular databases, each with domain concepts and definitions, natural language data analysis queries, and responses in the form of Python code, visualizations, and insights. Using this dataset, we conducted a within-subjects experiment to evaluate the effectiveness of domain-specific exemplars versus randomly selected, generic exemplars. Our study underscores the significance of tailored exemplars in enhancing LLMs’ accuracy and contextual understanding in domain-specific tasks, paving the way for more intuitive and effective data analysis solutions.\r\n\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 139370
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 139462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research North America",
              "dsl": ""
            }
          ],
          "personId": 139465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 139025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Robert Bosch Research",
              "dsl": ""
            }
          ],
          "personId": 139126
        }
      ]
    },
    {
      "id": 139664,
      "typeId": 13292,
      "title": "Implicit interest indicators",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "IPA",
      "source": "CSV",
      "trackId": 12520,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139642
      ],
      "eventIds": [],
      "abstract": "Recommender systems provide personalized suggestions about items that users will find interesting. Typically, recommender systems require a user interface that can ``intelligently'' determine the interest of a user and use this information to make suggestions. The common solution, ``explicit ratings'', where users tell the system what they think about a piece of information, is well-understood and fairly precise. However, having to stop to enter explicit ratings can alter normal patterns of browsing and reading. A more ``intelligent'' method is to useimplicit ratings, where a rating is obtained by a method other than obtaining it directly from the user. These implicit interest indicators have obvious advantages, including removing the cost of the user rating, and that every user interaction with the system can contribute to an implicit rating.",
      "authors": [
        {
          "affiliations": [],
          "personId": 139644
        },
        {
          "affiliations": [],
          "personId": 139645
        }
      ]
    },
    {
      "id": 139686,
      "typeId": 13292,
      "durationOverride": 60,
      "title": "The State of Design Knowledge in Human-AI Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "KN2",
      "source": "CSV",
      "trackId": 12522,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139641
      ],
      "eventIds": [],
      "abstract": "My research is at the intersection of HCI and AI. I design, build and evaluate interactive systems that have some kind of machine intelligence under the hood. I strive to build intelligent interactive systems that are useful, that give people a meaningful sense of control, and whose behavior aligns with the mental models held by their users. This is challenging because the underlying AI technology can be occasionally wrong, it delivers the most value if it is allowed to act proactively, and it frequently behaves in unexpected ways. In the past two decades, the human-AI interaction community has grown and has made substantial progress in producing useful design knowledge that addresses these challenges. Machine intelligence is now present in many real-world interactive systems from nearly invisible (like predictive text helping with mobile text entry), to highly consequential (like AI-powered decision-support systems). However, there are also some important gaps in our knowledge. For example, the results of our behavioral experiments indicate that adaptive user interfaces require more cognitive effort to operate than we had assumed, predictive text changes the content of what people write instead of just making text entry more efficient, and decision makers presented with AI-generated decision recommendations and explanations rarely engage cognitively with the content of what the AI communicates. Meanwhile the results of our other studies point out some unverified assumptions underlying the common choices of what (and whose) problems we solve with AI in clinical and public sector settings. This said, I believe that we can design useful and usable AI-powered interactive systems but the relevant design knowledge is relatively knew and is still a work in progress. The contemporary enthusiasm for using machine intelligence in interactive systems is an opportunity to grow our knowledge. It is also a danger in that it creates conditions where following the �best practices� of others, without having the time or opportunity to examine them, can turn unverified assumptions into fundamental principles of our field.",
      "authors": [
        {
          "affiliations": [],
          "personId": 139684
        }
      ]
    },
    {
      "id": 139729,
      "typeId": 13292,
      "durationOverride": 60,
      "title": "Mission: To enable diverse mere mortals to assess an AI agent's \"goodness\" for their own needs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "KN1",
      "source": "CSV",
      "trackId": 12522,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        139640
      ],
      "eventIds": [],
      "abstract": "As AI agents become more and more prevalent in everyday technology, more and more individuals -- from every walk of life, at every level of education, across the entire socioeconomic spectrum, of every gender, race, ethnicity, and age -- will need to make decisions about which agent(s) to use, and to what extent using them is the best path forward. The \"mission\" this talk explores is how we can enable such diverse individuals to make such decisions in ways that make their lives better instead of worse. For example, should I use an agent to enable me to be a remote caregiver for my grandmother, or should I move in with her? Should I buy a semi-self-driving car X, or a semi-self-driving car Y, or stay entirely manual? Will using one of these systems cost someone's life? Will it so destroy someone's privacy that their lives become filled with fear and harassment? Will my child become less intelligent over time if I give her access to LLM-powered \"homework helpers\"? In this talk, I do not show how to answer any of these questions. But I show a few paths forward that may point to way(s) toward answering them and at least one path on how not to answer them.",
      "authors": [
        {
          "affiliations": [],
          "personId": 139685
        }
      ]
    }
  ],
  "people": [
    {
      "id": 138728,
      "firstName": "Jonathan",
      "lastName": "Dodge",
      "middleInitial": "",
      "importedId": "b7Yrlft2emMk8VrUaYuwHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138737,
      "firstName": "Tsvi",
      "lastName": "Kuflik",
      "middleInitial": "",
      "importedId": "PYti2xcGj_4LyHsRw3_zjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138741,
      "firstName": "Murthy",
      "lastName": "L R D",
      "middleInitial": "",
      "importedId": "fmpbAz2XBei3sQeKhxmtQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138759,
      "firstName": "Pradipta",
      "lastName": "Biswas",
      "middleInitial": "",
      "importedId": "92_ObkqDmujSR1dJzL_Q-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138778,
      "firstName": "Hendrik",
      "lastName": "Strobelt",
      "middleInitial": "",
      "importedId": "HT083xWRbLUW2zxmA1U1Eg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138787,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "importedId": "-PmisC4rRA8oAfHrwVVdYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138792,
      "firstName": "Justin",
      "lastName": "Weisz",
      "middleInitial": "D.",
      "importedId": "oEGSNyBE7LAVFkwQ0slwxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138806,
      "firstName": "Bart",
      "lastName": "Knijnenburg",
      "middleInitial": "",
      "importedId": "DwKb12yLvWsGrrnrAPm-og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138822,
      "firstName": "Lydia",
      "lastName": "Chilton",
      "middleInitial": "B",
      "importedId": "jM77DxTwyp1U8TtuVyYGCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138832,
      "firstName": "Marko",
      "lastName": "Tkalcic",
      "middleInitial": "",
      "importedId": "QKk8S97GSmcVVbZEpyY30w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138834,
      "firstName": "Werner",
      "lastName": "Geyer",
      "middleInitial": "",
      "importedId": "lBErfIQtxVMOfhoBk4ucJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138838,
      "firstName": "Bruce",
      "lastName": "Ferwerda",
      "middleInitial": "",
      "importedId": "n9zf8m9tq_uwL2bVvEjpxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138848,
      "firstName": "Abhishek",
      "lastName": "Mukhopadhyay",
      "middleInitial": "",
      "importedId": "kiqqR4i2maicog6_0I8U3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138853,
      "firstName": "Seth",
      "lastName": "Polsley",
      "middleInitial": "",
      "importedId": "IjnQd0p0YpZNohttHB_aFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138858,
      "firstName": "Mehtab",
      "lastName": "Iqbal",
      "middleInitial": "",
      "importedId": "nVI-NtwXmnsJX_VWuvGc3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138891,
      "firstName": "Jenny",
      "lastName": "Mar",
      "middleInitial": "",
      "importedId": "DV5cY8gJkt5aRC3Wttb4HA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138892,
      "firstName": "Jueon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "ITkUXOLzsi-W8CJSHuV9qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138893,
      "firstName": "Hariharan",
      "lastName": "Subramonyam",
      "middleInitial": "",
      "importedId": "8uB_Jqw_cfCMBk7zG0Bx_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138894,
      "firstName": "Krzysztof",
      "lastName": "Gajos",
      "middleInitial": "Z.",
      "importedId": "KRI177WW5kh28FLEt6-HJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138895,
      "firstName": "Yoo Jin",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "8Rl06uMgy52txSGvCJWg7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138896,
      "firstName": "Nathanael",
      "lastName": "Jarasse",
      "middleInitial": "",
      "importedId": "crDJWqFuAGpBuV__C0dLag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138897,
      "firstName": "Ming",
      "lastName": "Ge",
      "middleInitial": "",
      "importedId": "j3NA3JaOfqmjRibBXPNAmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138898,
      "firstName": "Jonathan",
      "lastName": "Buck",
      "middleInitial": "",
      "importedId": "_IBqqcVt7UdKJJIsjg9tTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138899,
      "firstName": "Leandro",
      "lastName": "Marinho",
      "middleInitial": "",
      "importedId": "ndEq2V7my8Petjr_X-74AA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138900,
      "firstName": "Zhehan",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "itMYk6PZq9_vNvn2k_Og5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138901,
      "firstName": "Kiran",
      "lastName": "Kate",
      "middleInitial": "",
      "importedId": "qceTCP6Ncb6T74CUSdnMMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138902,
      "firstName": "Kabir Sulaiman",
      "lastName": "SAID",
      "middleInitial": "",
      "importedId": "mjY49CNwVPnABS0yZgMFUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138903,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "middleInitial": "",
      "importedId": "biWjVLs70rX0liHWYQRenQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138904,
      "firstName": "Uran",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "Yk6-nBYNLc_uTNvUe5TGuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138905,
      "firstName": "Gregory Lee",
      "lastName": "Newsome",
      "middleInitial": "",
      "importedId": "vMUQtaiXVYoaKh29uNNiZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138906,
      "firstName": "Dorota",
      "lastName": "Głowacka",
      "middleInitial": "",
      "importedId": "gQ_ngZr37ypkrk6ZywKXfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138907,
      "firstName": "Stephen",
      "lastName": "Brade",
      "middleInitial": "",
      "importedId": "KVnKgrTeEwC-MBLdTGRLoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138908,
      "firstName": "Qianyu",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "4E5efwtDKObXciJa40DyZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138909,
      "firstName": "Sydney",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "Z7ipNRDXg4hRi3OhWGtXQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138910,
      "firstName": "Jaehyuk",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "Yo9XsLuygSCNoRuXQ3yhag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138911,
      "firstName": "Jorge",
      "lastName": "Piazentin Ono",
      "middleInitial": "H",
      "importedId": "MYaiE-RZcdqMRiylCYGVbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138912,
      "firstName": "Juho",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "20MXUK-HxVwenJX_VlbU8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138913,
      "firstName": "Frode",
      "lastName": "Guribye",
      "middleInitial": "",
      "importedId": "tq-it2GQQcwjVSbOZiqVpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138914,
      "firstName": "Njål",
      "lastName": "Borch",
      "middleInitial": "",
      "importedId": "q_4F3xyFOVC15a6ltKhngQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138915,
      "firstName": "Pia",
      "lastName": "Hammer",
      "middleInitial": "",
      "importedId": "5Q9HLeX0TW5LThp5_an8BQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138916,
      "firstName": "Sungsoo Ray",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "Ofpd6HjnSFOWOD8CPkbw3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138917,
      "firstName": "Nitesh",
      "lastName": "Goyal",
      "middleInitial": "",
      "importedId": "hc6CF6L8TGeWh9vymMgWnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138918,
      "firstName": "Connor",
      "lastName": "Baumler",
      "middleInitial": "",
      "importedId": "X6SZKQ-YXAHPKaOGeKE1-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138919,
      "firstName": "Yunjung",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "SOfIK4CGbg7D0RbYBpPXvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138920,
      "firstName": "Vishnu",
      "lastName": "Raj",
      "middleInitial": "",
      "importedId": "mQG6BVKnPjVbvwER6N3O9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138921,
      "firstName": "Daniel",
      "lastName": "Pillis",
      "middleInitial": "",
      "importedId": "HGZf_UerVJ4bPTtX7p172A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138922,
      "firstName": "Kui",
      "lastName": "Ren",
      "middleInitial": "",
      "importedId": "_S83YGGPLcJFjYlUMa0HBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138923,
      "firstName": "Behnoosh",
      "lastName": "Mohammadzadeh",
      "middleInitial": "",
      "importedId": "d-hfoEsfHlb2veJiXyqVEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138924,
      "firstName": "Kazuyuki",
      "lastName": "Fujita",
      "middleInitial": "",
      "importedId": "e-KfkiPFbFPUYASETBkUtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138925,
      "firstName": "Abel",
      "lastName": "Valente",
      "middleInitial": "N.",
      "importedId": "0xJOtnYz9sCFycpHKacoPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138926,
      "firstName": "Zana",
      "lastName": "Buçinca",
      "middleInitial": "",
      "importedId": "EqvDgTP0S5WQ9V50kOA4tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138927,
      "firstName": "michele",
      "lastName": "gouiffes",
      "middleInitial": "",
      "importedId": "Zyqpa9X4pcCiI0vyCqH05w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138928,
      "firstName": "Gerrit",
      "lastName": "Rooks",
      "middleInitial": "",
      "importedId": "0e8sr5u8dkkKBQ2ex6DUYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138929,
      "firstName": "Jonathan",
      "lastName": "Bragg",
      "middleInitial": "",
      "importedId": "5juTx1AoMViy24GcPbDLsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138930,
      "firstName": "Alister",
      "lastName": "Palmer",
      "middleInitial": "",
      "importedId": "axQiDO7da4Yv9vuysKPM5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138931,
      "firstName": "Martin",
      "lastName": "Hirzel",
      "middleInitial": "",
      "importedId": "vzZ5OgiYQ3VGaVAV_UQNvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138932,
      "firstName": "Jaison",
      "lastName": "Puthenkalam",
      "middleInitial": "",
      "importedId": "j7qz2nzLbsoQVKZNGAr7cQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138933,
      "firstName": "Dimitra",
      "lastName": "Dimitrakopoulou",
      "middleInitial": "",
      "importedId": "kbdBlCdmMfuV1f0IBamCJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138934,
      "firstName": "Tomoyuki",
      "lastName": "Maekawa",
      "middleInitial": "",
      "importedId": "qQOH0J0Lj98pfoj7-RQbKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138935,
      "firstName": "Sarah",
      "lastName": "Chen",
      "middleInitial": "A",
      "importedId": "EdkoCVVE7qAst4a3nBliWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138936,
      "firstName": "Tetsuo",
      "lastName": "Ashizawa",
      "middleInitial": "",
      "importedId": "R2I3QJeInSQqTay8ISfd_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138937,
      "firstName": "Katrien",
      "lastName": "Verbert",
      "middleInitial": "",
      "importedId": "PNdaX17L-jK6PSG60qgujQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138938,
      "firstName": "Eunyeoul",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "qy1hdGG5oaNf9mcx3Eiemg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138939,
      "firstName": "Laura-Maria",
      "lastName": "Peltonen",
      "middleInitial": "",
      "importedId": "m0QXWQothATRMWX8-Yguew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138940,
      "firstName": "Xiangyu",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "qD3JurHzHF_IyYkftqONKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138941,
      "firstName": "Stephanie",
      "lastName": "Zubicueta Portales",
      "middleInitial": "",
      "importedId": "fvyXBuOcdrjL0CRnC-xMSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138942,
      "firstName": "Xiaojuan",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "tOS6C4kPL7t30lwzz_2A4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138943,
      "firstName": "Mingzhe",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "ASzOHOuYZwJ_wJTZZOHCoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138944,
      "firstName": "Oda Elise",
      "lastName": "Nordberg",
      "middleInitial": "",
      "importedId": "Zshfc5JZnmuARm0EurBJCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138945,
      "firstName": "Vaynee",
      "lastName": "Sungeelee",
      "middleInitial": "",
      "importedId": "ackchfMP9pEsPwunvG0gWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138946,
      "firstName": "Aaron",
      "lastName": "Donsbach",
      "middleInitial": "",
      "importedId": "hQAFxoAvY1AVvyC6rNzjrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138947,
      "firstName": "Crystal",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "wZU0uCPiRxq8b51H6M_6Ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138948,
      "firstName": "Parmit",
      "lastName": "Chilana",
      "middleInitial": "K",
      "importedId": "tN_Q0RA96u9zr4DWfu3Lvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138949,
      "firstName": "Karin",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "_AMGvlXnC4jsbs63Td8g1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138950,
      "firstName": "Qianniu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "vmrxOqnjptA27qUsl3FCmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138951,
      "firstName": "Jeongeon",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "yWqT8o9HN1Ge5XufQ7AhQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138952,
      "firstName": "Chen",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "7MwQkQDDk3C-FhXXoBbhFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138953,
      "firstName": "Yingchi",
      "lastName": "LI",
      "middleInitial": "",
      "importedId": "SRdIaSMHu7l9e58nXUpFOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138954,
      "firstName": "Anjali",
      "lastName": "Khurana",
      "middleInitial": "",
      "importedId": "_VvAWqjzhHyEP2x3Dd7n3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138955,
      "firstName": "Hossein",
      "lastName": "Salemi",
      "middleInitial": "",
      "importedId": "7pyt6u8CIsDDB4RA0qLe5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138956,
      "firstName": "Patrick",
      "lastName": "Haffner",
      "middleInitial": "",
      "importedId": "7e9U1Gh1qXGisH_CTS8QYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138957,
      "firstName": "Leiliang",
      "lastName": "Gong",
      "middleInitial": "",
      "importedId": "vUoiEZLkiRs1d5EyjUTY4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138958,
      "firstName": "Ming",
      "lastName": "Yin",
      "middleInitial": "",
      "importedId": "rC6aAtG76yfXtx2U0v5dwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138959,
      "firstName": "Giulio",
      "lastName": "Jacucci",
      "middleInitial": "",
      "importedId": "DRnY16S61QwHMLVm_3_gNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138960,
      "firstName": "Hal",
      "lastName": "Daumé III",
      "middleInitial": "",
      "importedId": "izofVtFEL_aMRNPaPSkGIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138961,
      "firstName": "Hans",
      "lastName": "Moen",
      "middleInitial": "",
      "importedId": "mNZPWhYruVfX1FNckaq2yA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138962,
      "firstName": "Antonio",
      "lastName": "Krüger",
      "middleInitial": "",
      "importedId": "7BVGaBoskVivG5iBzHVi8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138963,
      "firstName": "Alan",
      "lastName": "Medlar",
      "middleInitial": "",
      "importedId": "dwgusbmZWs0Sl_ByCj7dcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138964,
      "firstName": "Anastasia",
      "lastName": "Kuzminykh",
      "middleInitial": "",
      "importedId": "GkorhbP9GGlTsJ542N4FSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138965,
      "firstName": "Hanfang",
      "lastName": "Lyu",
      "middleInitial": "",
      "importedId": "n8A6mWXCTDNHxyP3_4volw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138966,
      "firstName": "Zhenguang",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "1qaMLZ0jB9XkG87H1cl-zQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138967,
      "firstName": "Matthew",
      "lastName": "Lease",
      "middleInitial": "",
      "importedId": "Qckb_3KNrA8DgjMK10FG7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138968,
      "firstName": "Langdon",
      "lastName": "Holmes",
      "middleInitial": "",
      "importedId": "QEqe3xxpISibYgpaFoQv7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138969,
      "firstName": "Tin",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "0hPrUbss8x5phxxtY0P5Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138970,
      "firstName": "Mingfei",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "piwnzb8Fm87pFXK7y99r4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138971,
      "firstName": "Feng",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "qpuybavMIX1nVNzqyZ2atg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138972,
      "firstName": "Lorans",
      "lastName": "Alabood",
      "middleInitial": "",
      "importedId": "OWFsnh2af6XNMj804eKkGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138974,
      "firstName": "Giridhar",
      "lastName": "Vadhul",
      "middleInitial": "",
      "importedId": "jS9Sh4bZcVNnzgbIHRZFjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138975,
      "firstName": "Chen",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "FHVVBpgWK-3Dl0xCJmAyWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138976,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "e4x280wrNe6DjEzW6pjtvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138977,
      "firstName": "Michael",
      "lastName": "Terry",
      "middleInitial": "",
      "importedId": "ZqUu0aTfbEHO8z3pZkwwew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138978,
      "firstName": "Xiaolei",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "zQQQtiRhxUuJPanMp2wiZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138979,
      "firstName": "Navita",
      "lastName": "Goyal",
      "middleInitial": "",
      "importedId": "fQl6Ag8EhCTm-1MqpUTA-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138980,
      "firstName": "Sarah",
      "lastName": "Morrison-Smith",
      "middleInitial": "",
      "importedId": "_1cEVBBuHnGGAXEA_wNrkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138981,
      "firstName": "Mauro",
      "lastName": "Martino",
      "middleInitial": "",
      "importedId": "HUHYJMYBZ2qfY73qjkiweA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138982,
      "firstName": "Suhyun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "fSNIcTjbDSRIl5daOnAzLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138983,
      "firstName": "Jules",
      "lastName": "Françoise",
      "middleInitial": "",
      "importedId": "dDnawGbJTecstXua-HRDng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138984,
      "firstName": "Thiemo",
      "lastName": "Wambsganss",
      "middleInitial": "",
      "importedId": "8cDSzbKQYZMwmaiq8efqiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138985,
      "firstName": "Chowdhury Mohammad Rakin",
      "lastName": "Haider",
      "middleInitial": "",
      "importedId": "eccBsREPa4IyqrOaF5AwtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138986,
      "firstName": "Raj",
      "lastName": "Sodhi",
      "middleInitial": "",
      "importedId": "BgFQbW15rOo8wet41p4KSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138987,
      "firstName": "Siddharth",
      "lastName": "Swaroop",
      "middleInitial": "",
      "importedId": "nqy4aFvkMdvGFTap1YAXcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138988,
      "firstName": "Ítallo",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "72s9rUSTzN9ZaajZKzd83A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138989,
      "firstName": "Nandana",
      "lastName": "Mihindukulasooriya",
      "middleInitial": "",
      "importedId": "TcEE0AYrGkkNBcY8F301OA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138990,
      "firstName": "Misha",
      "lastName": "Sra",
      "middleInitial": "",
      "importedId": "Ud_BsQ2Tnwvc9smICZF5hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138991,
      "firstName": "Anastasia",
      "lastName": "Sergeeva",
      "middleInitial": "",
      "importedId": "gKSLCF2miIgHDrWAJEbG3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138992,
      "firstName": "Michael",
      "lastName": "Feld",
      "middleInitial": "",
      "importedId": "nxEMdJRc1rie0_Xm7vSq3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138993,
      "firstName": "Ujaan",
      "lastName": "Das",
      "middleInitial": "",
      "importedId": "gTHa8Ykv3S8aHgjCGkbYfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138994,
      "firstName": "Finale",
      "lastName": "Doshi-Velez",
      "middleInitial": "",
      "importedId": "wqTJP6yIgYU0GrPZPI-tCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138995,
      "firstName": "David",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "opN2sb38wb0DO8y-MROXPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138996,
      "firstName": "Brad",
      "lastName": "Myers",
      "middleInitial": "A",
      "importedId": "2PlVnFcPRlVXwHeaH5ipHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138997,
      "firstName": "Pat",
      "lastName": "Pataranutaporn",
      "middleInitial": "",
      "importedId": "dNtiyFe0Lvv0cHDSi3oqeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138998,
      "firstName": "Omar",
      "lastName": "Moured",
      "middleInitial": "",
      "importedId": "iBPwcftqOM3kXb4ctUFhHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 138999,
      "firstName": "Joannes",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "eNeZgWLty5S2IIYSgAtceA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139000,
      "firstName": "Ahmadreza",
      "lastName": "Nazari",
      "middleInitial": "",
      "importedId": "h9YjAQMA6l7uRXA9UnYuRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139001,
      "firstName": "James",
      "lastName": "Wexler",
      "middleInitial": "",
      "importedId": "njFQiex8HTqld2Fkvv9iTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139002,
      "firstName": "Dae Hyun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "elh4YJbzNb0DcwEMRQzzmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139003,
      "firstName": "Ding",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "xjWfBNc8zI1fuHFhumeAgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139004,
      "firstName": "Nikitha",
      "lastName": "Rao",
      "middleInitial": "",
      "importedId": "eGUsOBRETlEKxb9TxMFAnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139005,
      "firstName": "Yun seo",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "-pxA2Mm-hlZKrRvVz2GQGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139006,
      "firstName": "Gregory",
      "lastName": "Bramble",
      "middleInitial": "",
      "importedId": "JOgzJOvL_TkJPuuET7PPIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139007,
      "firstName": "Katja",
      "lastName": "Galllhuber",
      "middleInitial": "",
      "importedId": "6KFHaf3cr0DLHWSTLXlr2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139008,
      "firstName": "Zhongjie",
      "lastName": "Ba",
      "middleInitial": "",
      "importedId": "qobhLUHZqZV68OA6iMs20Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139009,
      "firstName": "Wenbin",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "hB_EpMbvJtcIKl18OFmOsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139010,
      "firstName": "Ruolin",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "3Dg6PbJi9flxlFRWGC9d1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139011,
      "firstName": "Lucy Lu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "iUJJT_9gF0brJFnyQCs1Dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139012,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "middleInitial": "",
      "importedId": "dxPeY6Jxn1PhelTJJyXnjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139013,
      "firstName": "Seyed Parsa",
      "lastName": "Neshaei",
      "middleInitial": "",
      "importedId": "ua-iai1rTp-ETBvAWzjKyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139014,
      "firstName": "Khanh Duy",
      "lastName": "Le",
      "middleInitial": "",
      "importedId": "nAEtJW6mTOcd0hjV-_bw4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139015,
      "firstName": "Jason",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "AX66TvfSdtUHX3ZgCBIB1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139016,
      "firstName": "Alex",
      "lastName": "Endert",
      "middleInitial": "",
      "importedId": "ZJ_loGbNmPLvlVON5QH6vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139017,
      "firstName": "Daniel",
      "lastName": "Weidele",
      "middleInitial": "Karl I.",
      "importedId": "vpk5rumm2ev9Z9NoZpen7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139018,
      "firstName": "Yujin",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "8dJnyUN5qVeHPjoijezRpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139019,
      "firstName": "Yangyang",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "9enWTPOvBftrM80VIUvFtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139020,
      "firstName": "Gionnieve",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "9i9ZrddBZWJ5YQtXEOUPJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139021,
      "firstName": "Min",
      "lastName": "Bai",
      "middleInitial": "",
      "importedId": "u3m25pb0dFMgY7W9BegXrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139022,
      "firstName": "Mrinmaya",
      "lastName": "Sachan",
      "middleInitial": "",
      "importedId": "NftprGOUaj53nKG-YaU30w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139023,
      "firstName": "Marcelo",
      "lastName": "Carpinette Grave",
      "middleInitial": "",
      "importedId": "Ds0HI2KBpTH8zKFmai7V-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139024,
      "firstName": "Yukino",
      "lastName": "Baba",
      "middleInitial": "",
      "importedId": "c7-mIgWFgUbGRRIJpepPAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139025,
      "firstName": "Liang",
      "lastName": "Gou",
      "middleInitial": "",
      "importedId": "l9uGy_GjXf1ccghQQ-yWDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139026,
      "firstName": "Roman",
      "lastName": "Rietsche",
      "middleInitial": "",
      "importedId": "igAmzRHRH_Tss8cKU4O4fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139027,
      "firstName": "Michal",
      "lastName": "Lewkowicz",
      "middleInitial": "A",
      "importedId": "hdH483JJ0QOo1uzruqHf4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139028,
      "firstName": "Loraine",
      "lastName": "Franke",
      "middleInitial": "",
      "importedId": "ckrtW247CsnyTw8Cxt_gSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139029,
      "firstName": "Pekka",
      "lastName": "Marttinen",
      "middleInitial": "",
      "importedId": "bPkFyLnYyMP9dyzFz2BbpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139030,
      "firstName": "Vincent",
      "lastName": "Hellendoorn",
      "middleInitial": "",
      "importedId": "ba7EQ7Vlq11aQoAg02GiEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139031,
      "firstName": "Li",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "ysxzTVrlqmVbEQ7jjdc0dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139032,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "KpO_0vxU-ZNKmw9QA3hTuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139033,
      "firstName": "Benjamin",
      "lastName": "Wedin",
      "middleInitial": "D",
      "importedId": "TG2mWv380_dx_Q9zibJiWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139034,
      "firstName": "Tristan",
      "lastName": "McKinney",
      "middleInitial": "J",
      "importedId": "VbLybeAsP4NxggmNsXOz3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139035,
      "firstName": "Sageev",
      "lastName": "Oore",
      "middleInitial": "",
      "importedId": "6qD9qIDehJTPx2obR2pOMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139036,
      "firstName": "Quan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "EMaNOYI4P5ij7ZGTXbHLZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139037,
      "firstName": "Setareh",
      "lastName": "Zafari",
      "middleInitial": "",
      "importedId": "KLSScp_R3bFPQyAU79FWZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139038,
      "firstName": "Md Faisal Mahbub",
      "lastName": "Chowdhury",
      "middleInitial": "",
      "importedId": "aL4HISVI_gihhSGUivDpvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139039,
      "firstName": "Lisa",
      "lastName": "Amini",
      "middleInitial": "",
      "importedId": "4rg91w_qACDInwsBw9H1KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139040,
      "firstName": "Baptiste",
      "lastName": "Caramiaux",
      "middleInitial": "",
      "importedId": "uYJTuMpA4oVzyCRCHqwYoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139041,
      "firstName": "Xiaoyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "HDATxcPKSGVcfxx-KBvfaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139042,
      "firstName": "Manveer",
      "lastName": "Kalirai",
      "middleInitial": "",
      "importedId": "_3ppatRLXjf_6w4uddb2_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139043,
      "firstName": "Eun-Young",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "DU1zbG8y-d1STqRItxmkUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139044,
      "firstName": "Yan",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "fP1r-93kL6JIkVuFSujaHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139045,
      "firstName": "Barry",
      "lastName": "Smyth",
      "middleInitial": "",
      "importedId": "fNVdtEw5-9qm-njwrclDMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139046,
      "firstName": "Morris",
      "lastName": "Baumgarten-Egemole",
      "middleInitial": "",
      "importedId": "tkXHvSmqO2kvn6BeLf4gpw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139047,
      "firstName": "Aaron",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "Sl0yPww4WjKAxkRx3AKciw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139048,
      "firstName": "Zhuoran",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "g6zSlFdd3PLJzLYRwPL5Bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139049,
      "firstName": "Zhenhui",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "QLB33PjBTuHKiEDUE3PJJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139050,
      "firstName": "Qiushi",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "qOFQrBymOEj5PIOrlG3bsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139051,
      "firstName": "Morten",
      "lastName": "Fjeld",
      "middleInitial": "",
      "importedId": "_z6XIixtXWgHAmNFrilevg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139052,
      "firstName": "Steven",
      "lastName": "Dow",
      "middleInitial": "P.",
      "importedId": "8QHJbjDXFqKcG-dlzKVHwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139053,
      "firstName": "Cassandra",
      "lastName": "Overney",
      "middleInitial": "",
      "importedId": "ncr8UwP7QXXIfMq192vLRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139054,
      "firstName": "Wout",
      "lastName": "Vossen",
      "middleInitial": "",
      "importedId": "FpI8jDRZ-afSaGCpu9jcrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139055,
      "firstName": "Yuto",
      "lastName": "Nakashima",
      "middleInitial": "",
      "importedId": "ce2_BPXrwlbBAjNzqcSgxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139056,
      "firstName": "Xin",
      "lastName": "LIANG",
      "middleInitial": "",
      "importedId": "WOjhRketAuHHr_hlwUuDkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139057,
      "firstName": "Alex",
      "lastName": "Suryapranata",
      "middleInitial": "",
      "importedId": "ukMV1hu8VdyC1Kv4tZs6aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139058,
      "firstName": "Belén",
      "lastName": "Saldías",
      "middleInitial": "",
      "importedId": "r326tsXmqGo14-QA6w3O-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139059,
      "firstName": "Anusha",
      "lastName": "Withana",
      "middleInitial": "",
      "importedId": "80-gbgUjsm6bpgIfu0McLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139060,
      "firstName": "Dennis",
      "lastName": "Bromley",
      "middleInitial": "",
      "importedId": "QlgK2FV2cua2hROfpwoU1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139061,
      "firstName": "Zhaoyang",
      "lastName": "Lv",
      "middleInitial": "",
      "importedId": "CDcDVpa_nO7nGQ36c6sldg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139062,
      "firstName": "Amashi",
      "lastName": "Niwarthana",
      "middleInitial": "",
      "importedId": "cetv_9jgWuBWvFwuxdS_8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139063,
      "firstName": "Yasas",
      "lastName": "Senarath",
      "middleInitial": "",
      "importedId": "nEdcnbgsBquarODorS3_VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139064,
      "firstName": "Seoyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "xpdqDwaJpf4rX1VJcS14PA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139065,
      "firstName": "Haoran",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "Jh_Ci9ZAnCWwEqNjBvaKwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139066,
      "firstName": "Nikhil",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "3jIezK4Ua5HHK0an5pzbcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139067,
      "firstName": "Alexander",
      "lastName": "Bendeck",
      "middleInitial": "",
      "importedId": "T4P5HzR0DXTMF2Q_ChffTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139068,
      "firstName": "Bryan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HPOKEIUSZ5soMnVIZFkXqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139069,
      "firstName": "Carrie",
      "lastName": "Cai",
      "middleInitial": "J",
      "importedId": "IpbG6xKKCWj1t6fVN2m8Qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139070,
      "firstName": "Amanda",
      "lastName": "Hughes",
      "middleInitial": "Lee",
      "importedId": "3eSoKsyYFo-QotoZ9iW9vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139071,
      "firstName": "Lu",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "yPoNaQkqveODccA_QPNhlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139072,
      "firstName": "Kyungok",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "ixlW2wcLyg1s_DrIklzefw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139073,
      "firstName": "Rainer",
      "lastName": "Stiefelhagen",
      "middleInitial": "",
      "importedId": "LAMQG7E_y_kfeWDtW5TmrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139074,
      "firstName": "Peter",
      "lastName": "Andrews",
      "middleInitial": "",
      "importedId": "QM8fyQQ18eWEr5OqNXCSHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139075,
      "firstName": "Mateusz",
      "lastName": "Dubiel",
      "middleInitial": "",
      "importedId": "6x8WfAMnoDiLpjzAbnfnGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139076,
      "firstName": "Patricia",
      "lastName": "Kahr",
      "middleInitial": "K.",
      "importedId": "BzfOiih6iqhVdgk66r6suQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139077,
      "firstName": "Deb",
      "lastName": "Roy",
      "middleInitial": "",
      "importedId": "KveS2CkO4-M5733Kl2rI5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139078,
      "firstName": "Yang",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "qXh3thaJGkWMfqrjohHxkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139079,
      "firstName": "Koushik",
      "lastName": "Kalyanaraman",
      "middleInitial": "",
      "importedId": "-cL2lejndV1-t9TxFvEGzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139080,
      "firstName": "Sugato",
      "lastName": "Bagchi",
      "middleInitial": "",
      "importedId": "MWzjGvYjoqUYsSDuHzSYVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139081,
      "firstName": "Ming",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "hWdDZxBf7wdyperDTlYzKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139082,
      "firstName": "Ron",
      "lastName": "Hoory",
      "middleInitial": "",
      "importedId": "vYp9xrl1vZwA2KBQAuj8lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139083,
      "firstName": "Jane",
      "lastName": "E",
      "middleInitial": "L",
      "importedId": "pexOf3_TPLCTA3pprxU4tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139084,
      "firstName": "Noyan",
      "lastName": "Evirgen",
      "middleInitial": "",
      "importedId": "tSrY4sH7Qo1QjfyLBmm5pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139085,
      "firstName": "Song Young",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "w1_WmzVSt8puuN1VNwMNaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139086,
      "firstName": "Joon Suh",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "XWIYCp96mRra4CgW9F7fvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139087,
      "firstName": "Tommi",
      "lastName": "Gröhn",
      "middleInitial": "",
      "importedId": "r_B5hX8PSrKPRopKPdCYSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139088,
      "firstName": "Alfio",
      "lastName": "Gliozzo",
      "middleInitial": "",
      "importedId": "_BmIfInOH1aRb0w4nt4Ttg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139089,
      "firstName": "Chris",
      "lastName": "Snijders",
      "middleInitial": "",
      "importedId": "0Dmo9eGNaaSkW0SZdZRF-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139090,
      "firstName": "Kaylyn",
      "lastName": "Feeley",
      "middleInitial": "B",
      "importedId": "7LFkxyz1qukvMxWzNWvN4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139091,
      "firstName": "Zihao",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "sGiNlnZFUTW86UOLrc-vaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139092,
      "firstName": "Jin",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "z1aNj6qw9Bqb0_gaOOx1Dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139093,
      "firstName": "Adam",
      "lastName": "Coscia",
      "middleInitial": "",
      "importedId": "HRAFgGCjqOQEG5AqC33V0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139094,
      "firstName": "Bekzat",
      "lastName": "Tilekbay",
      "middleInitial": "",
      "importedId": "t53ngrGyc3HhF58dMlXAeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139095,
      "firstName": "Yuliang",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "ybQ2XlgTUMC3Unz9CBs7ZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139096,
      "firstName": "Chuntao",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "oMIOAIkMKPN8Arfk6Aduyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139097,
      "firstName": "Robin",
      "lastName": "Auer",
      "middleInitial": "",
      "importedId": "R9G63xQWAcU9e3zrM3p91A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139098,
      "firstName": "Téo",
      "lastName": "Sanchez",
      "middleInitial": "",
      "importedId": "OMUoER6uOt_GHxnhjj1NBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139099,
      "firstName": "Mahima",
      "lastName": "Pushkarna",
      "middleInitial": "",
      "importedId": "uCt7Btttl3UnPUV-K7DNwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139100,
      "firstName": "Saila",
      "lastName": "Koivusalo",
      "middleInitial": "",
      "importedId": "jc30QxpTm_C5LeYoAEWS5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139101,
      "firstName": "Haoran",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "QtMSnZ5VZ4koTO4DJO5jZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139102,
      "firstName": "Maxwell",
      "lastName": "Szymanski",
      "middleInitial": "",
      "importedId": "dxceOKOPCE8ho-FNiIyQpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139103,
      "firstName": "Haijun",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "qR18CZssIG8qNcFRCGcljg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139104,
      "firstName": "Tovi",
      "lastName": "Grossman",
      "middleInitial": "",
      "importedId": "y0gW9n9_FRp81xTibIb1pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139105,
      "firstName": "Alina",
      "lastName": "Zare",
      "middleInitial": "",
      "importedId": "CBuxi4WhSUHcvGwtovL0KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139106,
      "firstName": "Guozhong",
      "lastName": "Dai",
      "middleInitial": "",
      "importedId": "3yEH9CZX-cgX3Lb86wyCiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139107,
      "firstName": "Jason",
      "lastName": "Tsay",
      "middleInitial": "",
      "importedId": "uTK1prsyorO4PPPvmTRPxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139108,
      "firstName": "Christopher",
      "lastName": "Clifton",
      "middleInitial": "",
      "importedId": "ooYWldvNWKu-NJ0zo_u4KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139109,
      "firstName": "Jinyeong",
      "lastName": "Yim",
      "middleInitial": "",
      "importedId": "HvwAdXIYQFdKZ8WYr4Cd5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139110,
      "firstName": "Thorsten",
      "lastName": "Schwarz",
      "middleInitial": "",
      "importedId": "p7MtaVuLwKsICdbd0-Wj8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139111,
      "firstName": "Xiaotong",
      "lastName": "Xu",
      "middleInitial": "(Tone)",
      "importedId": "zBafE1xBRiVhfozJXWfWsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139112,
      "firstName": "Wenhui",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "6Xtbuf5bkq5zzzalz4H4Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139113,
      "firstName": "Xiong",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "ZVTlaOxRqgGEbPGn0gkZLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139114,
      "firstName": "Qing",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "jBk4WEwZVUogS8hglf2K-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139115,
      "firstName": "Steve",
      "lastName": "Peterson",
      "middleInitial": "",
      "importedId": "vLxTsdPJxTU8eLwllLB8Qw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139116,
      "firstName": "Yoonsu",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "TozbMnA_aeB9lG3Rgnb15Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139117,
      "firstName": "Ben",
      "lastName": "Lafreniere",
      "middleInitial": "",
      "importedId": "ZCec69xnESZNP5_GEGvG_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139118,
      "firstName": "Catherine",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "pFp2HHy64smiZfDzvD1idw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139119,
      "firstName": "Wesley",
      "lastName": "Morris",
      "middleInitial": "",
      "importedId": "l0VzLe9vBgAKsdj1vsgjew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139120,
      "firstName": "Kwan-Liu",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "1iDHQv_R398XjgDeuux2aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139121,
      "firstName": "Kathryn",
      "lastName": "Alvero",
      "middleInitial": "",
      "importedId": "rMCLwROqJmumOIeQx1iI-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139122,
      "firstName": "Shayenna",
      "lastName": "Misko",
      "middleInitial": "",
      "importedId": "RCcQMZ_CKnL6Lldw9pAFpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139123,
      "firstName": "Hasindu",
      "lastName": "Kariyawasam",
      "middleInitial": "",
      "importedId": "mHsffKYC74Nh2hgvaG98pQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139124,
      "firstName": "Ehsan",
      "lastName": "Hoque",
      "middleInitial": "",
      "importedId": "dIHsUNSuOeQE07JMWpGo0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139125,
      "firstName": "Martijn",
      "lastName": "Willemsen",
      "middleInitial": "C.",
      "importedId": "_nbct3F3wAvrGaJh0Dmtiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139126,
      "firstName": "Liu",
      "lastName": "Ren",
      "middleInitial": "",
      "importedId": "aUDpIWTI4-jRw8RAD8sGkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139127,
      "firstName": "Erran",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "5DPj7NHS7DzygQIb8hegsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139128,
      "firstName": "Jiayu",
      "lastName": "Yin",
      "middleInitial": "",
      "importedId": "87tJZNgVppNB4WgyPsVrYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139129,
      "firstName": "Julio",
      "lastName": "Nogima",
      "middleInitial": "",
      "importedId": "CIza8Cygf6OrDAH5ooZJDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139130,
      "firstName": "Amy",
      "lastName": "Rechkemmer",
      "middleInitial": "",
      "importedId": "Is2Q0msIzd5RY_E9xDLAHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139131,
      "firstName": "Alex",
      "lastName": "Williams",
      "middleInitial": "C",
      "importedId": "zinenbm-EstP9Dctp4syug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139132,
      "firstName": "Ian",
      "lastName": "Thomas",
      "middleInitial": "",
      "importedId": "uGnxggjnlCnq0H1Ck2ImMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139133,
      "firstName": "Alina",
      "lastName": "Roitberg",
      "middleInitial": "",
      "importedId": "DfP7GP7Za_CrDWAk0vmOoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139134,
      "firstName": "Diwakar",
      "lastName": "Krishnamurthy",
      "middleInitial": "",
      "importedId": "0M17D6F9vaidpvlZlS3xyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139135,
      "firstName": "Rifat Mehreen",
      "lastName": "Amin",
      "middleInitial": "",
      "importedId": "TrHI-NduSQgUXIBGSqkFmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139136,
      "firstName": "Luis",
      "lastName": "Leiva",
      "middleInitial": "A.",
      "importedId": "zEw1asvR5_8wPjGUPy5Fbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139137,
      "firstName": "James",
      "lastName": "Spann",
      "middleInitial": "",
      "importedId": "G-Wu7YCS2MCJ4W79PvHVKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139138,
      "firstName": "Judy",
      "lastName": "Kay",
      "middleInitial": "",
      "importedId": "SEECRD18onioOT2p4FuaKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139139,
      "firstName": "Aoto",
      "lastName": "Tsuchiya",
      "middleInitial": "",
      "importedId": "HktyCscXa5BdOx3p43hvAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139140,
      "firstName": "Pattie",
      "lastName": "Maes",
      "middleInitial": "",
      "importedId": "Apx00vbso3FewB_L4nWE_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139141,
      "firstName": "Danielle",
      "lastName": "Szafir",
      "middleInitial": "Albers",
      "importedId": "N_P0ylpQQQOp3CJQoagxsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139142,
      "firstName": "Hemant",
      "lastName": "Purohit",
      "middleInitial": "",
      "importedId": "duDEdF4b6LfcIGWmKuo3Lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139143,
      "firstName": "Hyunwoo",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "ZDwp6Eb0MfC-1nlBdcvXmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139144,
      "firstName": "Wijnand",
      "lastName": "IJsselsteijn",
      "middleInitial": "",
      "importedId": "wIdgfbLScR_gBQLw_nzkXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139145,
      "firstName": "Saelyne",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "KqqAYbATMsEk3CeesDFqUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139146,
      "firstName": "Horst",
      "lastName": "Samulowitz",
      "middleInitial": "",
      "importedId": "0qdbO4OtdPVqVbfHXbjgYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139147,
      "firstName": "Amr",
      "lastName": "Gomaa",
      "middleInitial": "",
      "importedId": "oYch6aFzsGIjQYAVOHPepA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139148,
      "firstName": "Zinat",
      "lastName": "Ara",
      "middleInitial": "",
      "importedId": "tUdJJNPxpaCZphtYAazsdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139149,
      "firstName": "Liming",
      "lastName": "Nie",
      "middleInitial": "",
      "importedId": "quP6uzPtVfkLzNwxEHfzkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139150,
      "firstName": "Jiafu",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "1FOIBPrR3XoDz0dLEszRQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139151,
      "firstName": "Claudio",
      "lastName": "Pinhanez",
      "middleInitial": "Santos",
      "importedId": "EgInCQ0uPNx1aLALF8epJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139152,
      "firstName": "Raul",
      "lastName": "Fernandez",
      "middleInitial": "",
      "importedId": "cAv2pXhynkIderByX5EMZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139153,
      "firstName": "Heleen",
      "lastName": "Muijlwijk",
      "middleInitial": "",
      "importedId": "6onEIzzT_JfE8Kotpt7Ucg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139154,
      "firstName": "Guillermo",
      "lastName": "Reyes",
      "middleInitial": "",
      "importedId": "reL5BLFrhSTgKuYkbNtnCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139155,
      "firstName": "Chia-Ming",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "aiJ5verWvtUnd1viC9qM7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139156,
      "firstName": "Chuhan",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "LukoRLUMLYuIgAqn1Fg-Vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139157,
      "firstName": "Yeon Su",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "nqxO9C4J232GEhhiXW6fKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139158,
      "firstName": "Gaetano",
      "lastName": "Rossiello",
      "middleInitial": "",
      "importedId": "ad26Nz2zb1X4PLN3n9W7hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139159,
      "firstName": "Xi",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "D5n1I60650pRwUxSfWcCLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139160,
      "firstName": "Scott",
      "lastName": "Crossley",
      "middleInitial": "",
      "importedId": "YSSj-mv0Vq8no5HLDlDRjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139161,
      "firstName": "Savvas",
      "lastName": "Petridis",
      "middleInitial": "",
      "importedId": "IjWutxpu2XUWrOfp-uRUNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139162,
      "firstName": "Chun-Wei",
      "lastName": "Chiang",
      "middleInitial": "",
      "importedId": "PEC7h3clcoJR1G14at3ixg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139163,
      "firstName": "Manfred",
      "lastName": "Tscheligi",
      "middleInitial": "",
      "importedId": "H0xTlBTd7quX3RMy28fTEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139164,
      "firstName": "Qingshan",
      "lastName": "Tong",
      "middleInitial": "",
      "importedId": "8mDBIvMlqWdRwXKEndrArA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139165,
      "firstName": "Qiang",
      "lastName": "Fu",
      "middleInitial": "",
      "importedId": "CSlplUheVdwtaAzt5_pFkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139166,
      "firstName": "Yi",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "SZicHuOG25ANmT7uxiFJkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139167,
      "firstName": "Yuanchen",
      "lastName": "Bai",
      "middleInitial": "",
      "importedId": "gfO8cXftst7pOODYghSjQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139168,
      "firstName": "Zhuoyan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "4jDbOrgFEaVYRLl-fFTtyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139169,
      "firstName": "Michita",
      "lastName": "Imai",
      "middleInitial": "",
      "importedId": "zdFVVnvXRUgxbJUy9e4Dtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139170,
      "firstName": "Dustin",
      "lastName": "Palea",
      "middleInitial": "",
      "importedId": "n2kqb4XUGQ35tmMjDrkYsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139171,
      "firstName": "Lukas",
      "lastName": "Kröninger",
      "middleInitial": "",
      "importedId": "TZdFArr1rnjIAdTM7HYguA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139172,
      "firstName": "Lisa",
      "lastName": "Anthony",
      "middleInitial": "",
      "importedId": "HPmCRYLdBQ5k_FoMcUnekQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139173,
      "firstName": "Xiaotian",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "7Vr3aL_yAgCvs8p7sHTanA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139174,
      "firstName": "Mauricio",
      "lastName": "Sousa",
      "middleInitial": "",
      "importedId": "WRWNWFmuweggtf9ISKQDxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139175,
      "firstName": "Andreas",
      "lastName": "Butz",
      "middleInitial": "Martin",
      "importedId": "CWYpLecThpHr6-Hh2dxUZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139176,
      "firstName": "Alan",
      "lastName": "Said",
      "middleInitial": "",
      "importedId": "k3JI8JK455ecs_9uoJqUyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139177,
      "firstName": "Semin",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "2J4NqdkuN4E8mJ770qY5EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139178,
      "firstName": "Vikram",
      "lastName": "Jaswal",
      "middleInitial": "K.",
      "importedId": "z53YHoOVOQ7G7rB1FZHshQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139338,
      "firstName": "Marc-Andr?",
      "lastName": "Z?ller",
      "importedId": "TiiS_auth9",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139339,
      "firstName": "ThomasÃ¯Â¿Â½",
      "lastName": "Franke",
      "importedId": "TiiS_auth8",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139340,
      "firstName": "MengtianÃ¯Â¿Â½",
      "lastName": "Guo",
      "importedId": "TiiS_auth13",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139341,
      "firstName": "ZhilanÃ¯Â¿Â½",
      "lastName": "Zhou",
      "importedId": "TiiS_auth14",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139342,
      "firstName": "ThomasÃ¯Â¿Â½",
      "lastName": "Schlegel",
      "importedId": "TiiS_auth11",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139343,
      "firstName": "MarcoÃ¯Â¿Â½",
      "lastName": "Huber",
      "middleInitial": "F",
      "importedId": "TiiS_auth12",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139344,
      "firstName": "DavidÃ¯Â¿Â½",
      "lastName": "Gotz",
      "importedId": "TiiS_auth15",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139345,
      "firstName": "YueÃ¯Â¿Â½",
      "lastName": "Wang",
      "importedId": "TiiS_auth16",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139346,
      "firstName": "Kwan-Liu",
      "lastName": "Ma",
      "importedId": "TiiS_auth5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139347,
      "firstName": "MennatallahÃ¯Â¿Â½",
      "lastName": "El-Assady",
      "importedId": "TiiS_auth4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139348,
      "firstName": "Tim",
      "lastName": "Schrills",
      "importedId": "TiiS_auth7",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139349,
      "firstName": "DanielÃ¯Â¿Â½",
      "lastName": "Keim",
      "importedId": "TiiS_auth6",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139350,
      "firstName": "Wolfgang",
      "lastName": "Jentner",
      "importedId": "TiiS_auth1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139351,
      "firstName": "Waldemar",
      "lastName": "Titov",
      "importedId": "TiiS_auth10",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139352,
      "firstName": "HannaÃ¯Â¿Â½",
      "lastName": "Hauptmann",
      "importedId": "TiiS_auth3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139353,
      "firstName": "GiulianaÃ¯Â¿Â½",
      "lastName": "Lindholz",
      "importedId": "TiiS_auth2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139363,
      "firstName": "Zhaoxing",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "5k7LiLoYMk_1xzEl4on6Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139364,
      "firstName": "Shino",
      "lastName": "Magaki",
      "middleInitial": "",
      "importedId": "J02thSnnQdFaUD1P3rNxZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139365,
      "firstName": "Xinru",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "2pG5t-nBoxmUqMO4MTpG1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139366,
      "firstName": "Tomoki",
      "lastName": "Yoshihisa",
      "middleInitial": "",
      "importedId": "64Q-514kzMxeMPV6cNO4Cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139367,
      "firstName": "Ioanna",
      "lastName": "Lykourentzou",
      "middleInitial": "",
      "importedId": "DkW3v1baghXLkEZfUSUtmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139368,
      "firstName": "Chunhua",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "UTeJLxzPywFCnsItWFchUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139369,
      "firstName": "Amra",
      "lastName": "Delic",
      "middleInitial": "",
      "importedId": "pabfj2B7FKmeRkZ8g_lW5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139370,
      "firstName": "Jiajing",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "UQHfdG8AR2kyztbUvjG0eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139371,
      "firstName": "Matthew",
      "lastName": "Aylett",
      "middleInitial": "Peter",
      "importedId": "QDjVwvn1C9zWcvJshearvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139372,
      "firstName": "Susan",
      "lastName": "McGregor",
      "middleInitial": "E",
      "importedId": "IU6CuRkz5TcBYAnmroqV8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139373,
      "firstName": "Himanshu",
      "lastName": "Vishwakarma",
      "middleInitial": "",
      "importedId": "ENR1aFpTjqI_yl7usxQfdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139374,
      "firstName": "Keisuke",
      "lastName": "Murashige",
      "middleInitial": "",
      "importedId": "kdJvpYfchYs-e4MY89_Uvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139375,
      "firstName": "Henner",
      "lastName": "Bendig",
      "middleInitial": "",
      "importedId": "WjpPhaGY0aRWYpX56JPLAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139376,
      "firstName": "Sushant",
      "lastName": "Kot",
      "middleInitial": "",
      "importedId": "3Rbah5LO53j73-MsgQmNqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139377,
      "firstName": "Thuy Ngoc",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "ZcsoiU4ByoO2JuMXXIja8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139378,
      "firstName": "Sushmita",
      "lastName": "Khan",
      "middleInitial": "",
      "importedId": "wAyVwGn-os-2zgUxzC09Sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139379,
      "firstName": "Shinji",
      "lastName": "Shimojo",
      "middleInitial": "",
      "importedId": "UcAapjxfEJ9HRlmgSQtfHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139380,
      "firstName": "Tommaso",
      "lastName": "Turchi",
      "middleInitial": "",
      "importedId": "3KXQIDVeu_2tWRSGoUxEDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139382,
      "firstName": "Kaileigh Angela",
      "lastName": "Byrne",
      "middleInitial": "",
      "importedId": "LqjHULMXkJbHpeuZDQVA8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139383,
      "firstName": "Yu-che",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "_TWmtmLIm-4iDt6_NtwSMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139384,
      "firstName": "Ryuta",
      "lastName": "Yamaguchi",
      "middleInitial": "",
      "importedId": "6QfXJBIZdqatnkpYSuNuzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139385,
      "firstName": "Hanif",
      "lastName": "Emamgholizadeh",
      "middleInitial": "",
      "importedId": "qPQocoUZjHr4ianzX5dFRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139386,
      "firstName": "Tao",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "p-mvnXaqidF1qNSECNDitw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139387,
      "firstName": "Vedran",
      "lastName": "Sabol",
      "middleInitial": "",
      "importedId": "t8YxgBUyTelW7g-FRQcevw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139388,
      "firstName": "Qian",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "UjB4apdI2SB6NeYx_yB3Xw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139389,
      "firstName": "Fabio",
      "lastName": "Paternò",
      "middleInitial": "",
      "importedId": "KnKTr0CmXVvzaE4nW3dyNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139390,
      "firstName": "Gabriel",
      "lastName": "Schell",
      "middleInitial": "",
      "importedId": "esE7V3SLeym3KU6qs3yx5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139391,
      "firstName": "Chandler",
      "lastName": "Robinson",
      "middleInitial": "",
      "importedId": "8J9OUmWU0uPvJCROfqGmKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139392,
      "firstName": "Sida",
      "lastName": "Dai",
      "middleInitial": "",
      "importedId": "wd9WEgvOshBkEt3lQmuj5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139394,
      "firstName": "Anamika",
      "lastName": "J H",
      "middleInitial": "",
      "importedId": "_PN8E0uKL1-rgGU3LZ-wHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139395,
      "firstName": "Anuschka",
      "lastName": "Schmitt",
      "middleInitial": "",
      "importedId": "xr4nz3jI1wh8IjGVYIETkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139396,
      "firstName": "Zeqi",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "dyg37fXEirJkUCAQDKt2bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139398,
      "firstName": "Francesco",
      "lastName": "Ricci",
      "middleInitial": "",
      "importedId": "ADxxr1k0PSV_cKqXyeE8jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139399,
      "firstName": "Chunxu",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "K75Z5QZ2nfQAzUxafZrGiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139400,
      "firstName": "Mohammad",
      "lastName": "Haeri",
      "middleInitial": "",
      "importedId": "Vu7xubhUiVfeZUWuYv1VKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139401,
      "firstName": "Heba",
      "lastName": "Aly",
      "middleInitial": "",
      "importedId": "fs06vlU2aBe5NEoAB94AiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139402,
      "firstName": "Mateusz",
      "lastName": "Dubiel",
      "middleInitial": "",
      "importedId": "FHogJhGDv_yR9gaDmm1QVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139403,
      "firstName": "Yoko",
      "lastName": "Yamakata",
      "middleInitial": "",
      "importedId": "gYlL1UVOlOcN3-YMKhFLKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139404,
      "firstName": "Elizabeth",
      "lastName": "Gilman",
      "middleInitial": "S",
      "importedId": "W6XOAmpix0A4J9sB_AxSkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139405,
      "firstName": "Peter",
      "lastName": "Brusilovsky",
      "middleInitial": "",
      "importedId": "Dd6y6kmumJiPz-0pXmThcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139406,
      "firstName": "Emma",
      "lastName": "Dixon",
      "middleInitial": "",
      "importedId": "2-MCkMk1vkqQLTsSXsHlXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139407,
      "firstName": "Abdullah",
      "lastName": "ALZAHRANI",
      "middleInitial": "",
      "importedId": "zP5A24MWHvH4ZA5Oz7Gaog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139408,
      "firstName": "Ameya",
      "lastName": "Patil",
      "middleInitial": "Avinash",
      "importedId": "zy7ZRerIpQzmemO7ouIdqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139409,
      "firstName": "Brygg",
      "lastName": "Ullmer",
      "middleInitial": "",
      "importedId": "FOOsNKFUPyq5qWUBEnceTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139410,
      "firstName": "Benjamin",
      "lastName": "Lee",
      "middleInitial": "Charles Germain",
      "importedId": "RKIQCmgvfY0SvFN3FcY19w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139411,
      "firstName": "Jasmine",
      "lastName": "Davidson",
      "middleInitial": "",
      "importedId": "50rJUF1v_fWA2EHHg4zL-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139412,
      "firstName": "Katharina",
      "lastName": "Reinecke",
      "middleInitial": "",
      "importedId": "TZQicXpNUJczMG71r_hdFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139413,
      "firstName": "Panagiotis",
      "lastName": "Germanakos",
      "middleInitial": "",
      "importedId": "Fx3puSjVBbvbSGfINQM40Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139414,
      "firstName": "Hanna",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "A8Iyk_6HmJRZVrec8dB2gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139415,
      "firstName": "Muneeb",
      "lastName": "Ahmad",
      "middleInitial": "",
      "importedId": "feseg9RuKvN3nDpWcJJKBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139416,
      "firstName": "Gary",
      "lastName": "Hsieh",
      "middleInitial": "",
      "importedId": "SzGMytUMUsCvzI-hvVb-bA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139417,
      "firstName": "Dan",
      "lastName": "Roberts",
      "middleInitial": "",
      "importedId": "igsyBHiSS9yMDXSVwB-uew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139418,
      "firstName": "Gyanig",
      "lastName": "Kumar",
      "middleInitial": "",
      "importedId": "LAdd0FKzvIBmkzCfm_YYWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139419,
      "firstName": "GVS",
      "lastName": "Mothish",
      "middleInitial": "",
      "importedId": "j8O8l13p55Jn6YXZk8tUaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139420,
      "firstName": "Joshua",
      "lastName": "Holstein",
      "middleInitial": "",
      "importedId": "AxKeqiK-yAREDKbGHZ04EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139421,
      "firstName": "Kelly",
      "lastName": "Caine",
      "middleInitial": "",
      "importedId": "7DfAKEEU8g-zp3S0CSxAsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139422,
      "firstName": "Kurt",
      "lastName": "Luther",
      "middleInitial": "",
      "importedId": "u1Q_0Cnkq4k1c3ycNGvonQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139423,
      "firstName": "Mason",
      "lastName": "Laney",
      "middleInitial": "",
      "importedId": "VkjrfVn2Ef-fAwop16_j9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139425,
      "firstName": "Samuel",
      "lastName": "George",
      "middleInitial": "D",
      "importedId": "cDWkY90WHUyEvUmodUsxRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139426,
      "firstName": "Rene",
      "lastName": "Just",
      "middleInitial": "",
      "importedId": "NbY1mbtzb-eS32GjR4glNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139427,
      "firstName": "Yimeng",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "dwkz9rYgN23KEsKOVCGQNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139428,
      "firstName": "Ilija",
      "lastName": "Simic",
      "middleInitial": "",
      "importedId": "--PHvElldxg-7ChZbBCIDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139429,
      "firstName": "Behnam",
      "lastName": "Rahdari",
      "middleInitial": "",
      "importedId": "qktCVsiEeGeWJIOKKyG2XQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139431,
      "firstName": "Tetsuya",
      "lastName": "Sasaoka",
      "middleInitial": "",
      "importedId": "HvaSxBzmcXI39RVFKgbzmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139432,
      "firstName": "Mukund",
      "lastName": "Mitra",
      "middleInitial": "",
      "importedId": "ImXmgXefc07tH0TXyQMong",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139433,
      "firstName": "Partha Pratim",
      "lastName": "Chakraborty",
      "middleInitial": "",
      "importedId": "1vt83UhWX9YM9ErxDEP9_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139434,
      "firstName": "Alan",
      "lastName": "Chamberlain",
      "middleInitial": "",
      "importedId": "Hw9zVvFP4L1tKJpHsOErFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139435,
      "firstName": "Hongyan",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "wxMYxUmErC9DjRBtCpKDSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139436,
      "firstName": "Vinay",
      "lastName": "Sharma",
      "middleInitial": "Krishna",
      "importedId": "SkbdWTxGYbZvFlHySbf09w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139437,
      "firstName": "Jindi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "4ohbgMxOIr6Q6vT3dV0AsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139438,
      "firstName": "Alessio",
      "lastName": "Malizia",
      "middleInitial": "",
      "importedId": "zmZrK6jcLT_i9AFlTZ5Hlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139439,
      "firstName": "Simone",
      "lastName": "Borsci",
      "middleInitial": "",
      "importedId": "NzvR0w2YYHeIoLbGoHbC1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139440,
      "firstName": "Hansen",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "6p_5SUscOpba5knNiXr8Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139441,
      "firstName": "Santokh",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "WSAGi9U3yoENQQ-lYRYGsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139442,
      "firstName": "Srishti",
      "lastName": "Gupta",
      "middleInitial": "",
      "importedId": "OaS8y87ZjBTOQl_BxQGGTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139443,
      "firstName": "Faisal",
      "lastName": "Mehmood",
      "middleInitial": "",
      "importedId": "oxjmSIAOr-d1VmVrKpiysQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139444,
      "firstName": "Johannes Georg",
      "lastName": "Hoffer",
      "middleInitial": "",
      "importedId": "C-2bpuIFpCKdqG8PB6-pbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139445,
      "firstName": "Rock",
      "lastName": "Pang",
      "middleInitial": "Yuren",
      "importedId": "9Y7Uf_DHONgcEYpis1oKTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139446,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "middleInitial": "",
      "importedId": "Yt-D2LzWNlB8U57O8tv1nA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139447,
      "firstName": "Yukiko",
      "lastName": "Kawai",
      "middleInitial": "",
      "importedId": "LKF-7RTA_fA48ZpaasrQdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139448,
      "firstName": "Marina",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "vDe1f6SKqzX34y0I7V5Evw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139449,
      "firstName": "Keishi",
      "lastName": "Tajima",
      "middleInitial": "",
      "importedId": "LM1zHYr6FJrGU1uVvZkZWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139450,
      "firstName": "Zilin",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "PtYrYgktu51Bckpoqf_FUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139451,
      "firstName": "Ioannis",
      "lastName": "Ivrissimtzis",
      "middleInitial": "",
      "importedId": "G9w42FwbQWmCDc3T-qyQOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139452,
      "firstName": "James",
      "lastName": "Johnson",
      "middleInitial": "M.",
      "importedId": "cpZ4g4bPFDedERSrmsl_jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139453,
      "firstName": "Emily",
      "lastName": "Sidnam-Mauch",
      "middleInitial": "",
      "importedId": "NBCo629XRft6nkLxUCchfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139454,
      "firstName": "Sebastin",
      "lastName": "Santy",
      "middleInitial": "",
      "importedId": "DHT7uuJRWVIWAwMNv7TuLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139455,
      "firstName": "Christian",
      "lastName": "Partl",
      "middleInitial": "",
      "importedId": "-xNkVQ39laCxocpRx7Om8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139456,
      "firstName": "Lei",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "PKzNw7gcJP61AjlCOU6WjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139457,
      "firstName": "Zijian",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "si762MKT2eYFguLsSElEQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139458,
      "firstName": "Pilar",
      "lastName": "Orero",
      "middleInitial": "",
      "importedId": "zLwIFP1-FI88_7pp89xfTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139459,
      "firstName": "Prasun",
      "lastName": "Dewan",
      "middleInitial": "",
      "importedId": "Ry_2kw3kw06hjZAgRfUTBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139460,
      "firstName": "Eryn",
      "lastName": "Whitworth",
      "middleInitial": "",
      "importedId": "n_wEtJKlgAGrihbyB2Zymw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139461,
      "firstName": "Mary Lou",
      "lastName": "Maher",
      "middleInitial": "",
      "importedId": "gpodBdQLvZ6G2zX9ciI0Fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139462,
      "firstName": "Vikram",
      "lastName": "Mohanty",
      "middleInitial": "",
      "importedId": "HtluANIhjfbY3ost-ikcMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139463,
      "firstName": "Hanna",
      "lastName": "Alzughbi",
      "middleInitial": "",
      "importedId": "yq3lhffvKmCaj8N-MrGzyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139464,
      "firstName": "Wei",
      "lastName": "Shan",
      "middleInitial": "",
      "importedId": "WOHo6VMvy01a49ar6ERtww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139465,
      "firstName": "Hongtao",
      "lastName": "Hao",
      "middleInitial": "",
      "importedId": "kqIkAVaLkRzxvNNHFmjNdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139466,
      "firstName": "Casey",
      "lastName": "Dugan",
      "middleInitial": "",
      "importedId": "gqc44QPKDXhf2KLy6DOSFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139468,
      "firstName": "Ziqian",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "2zUCHzGJYt7fJhcVvoWdXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139469,
      "firstName": "Anasol",
      "lastName": "Pena-Rios",
      "middleInitial": "",
      "importedId": "kTF-5Kj_UqeguIofuSVQbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139470,
      "firstName": "Kudrat",
      "lastName": "Kashyap",
      "middleInitial": "",
      "importedId": "z52s7m3tV5a6t8XZNiD0DA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139471,
      "firstName": "Zahra",
      "lastName": "Ashktorab",
      "middleInitial": "",
      "importedId": "bIa2SNlIglTkznKv3Yl2IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139472,
      "firstName": "Shikha",
      "lastName": "Soneji",
      "middleInitial": "",
      "importedId": "Po2blPO42yb0FgqPzYSmIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139473,
      "firstName": "Michael",
      "lastName": "Desmond",
      "middleInitial": "",
      "importedId": "Os78HB8il8q2OTI_W8YhQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139474,
      "firstName": "Neda",
      "lastName": "Zarrin-Khameh",
      "middleInitial": "",
      "importedId": "93rJFylLArUP5FoaL-pZoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139475,
      "firstName": "Anujith",
      "lastName": "Muraleedharan",
      "middleInitial": "",
      "importedId": "_icudtJmgS-pA1HcbC49cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139476,
      "firstName": "Winifred Elysse",
      "lastName": "Newman",
      "middleInitial": "",
      "importedId": "awQg7Yu58usE0aCo7pKOIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139477,
      "firstName": "Inti Gabriel",
      "lastName": "Mendoza Estrada",
      "middleInitial": "",
      "importedId": "Uh03wRo5qqpZ7oe9wzj6QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139478,
      "firstName": "Miriam",
      "lastName": "Konkel",
      "middleInitial": "K",
      "importedId": "cdeeGc9VYtCFGu-G9pDKMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139479,
      "firstName": "Margi",
      "lastName": "Engineer",
      "middleInitial": "",
      "importedId": "kEq4l91YTuOjOYIFJo2dug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139560,
      "firstName": "Osnat",
      "lastName": "Mokryn",
      "importedId": "Chair",
      "source": "CSV",
      "affiliations": [
        {
          "country": "Israel",
          "institution": "University of Haifa"
        }
      ]
    },
    {
      "id": 139644,
      "firstName": "Phong",
      "lastName": "Le",
      "importedId": "IPA1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139645,
      "firstName": "Mark",
      "lastName": "Claypool",
      "importedId": "IPA2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 139684,
      "firstName": "Krzysztof",
      "lastName": "Gajos",
      "importedId": "KN2_a",
      "source": "CSV",
      "affiliations": [
        {
          "country": "United States of America",
          "institution": "Harvard University"
        }
      ]
    },
    {
      "id": 139685,
      "firstName": "Margaret",
      "lastName": "Burnett",
      "middleInitial": "M.",
      "importedId": "KN1_a",
      "source": "CSV",
      "affiliations": [
        {
          "country": "United States of America",
          "institution": "Oregon State University"
        }
      ]
    },
    {
      "id": 139727,
      "firstName": "Paul",
      "lastName": "Taele",
      "importedId": "DC2",
      "source": "CSV",
      "affiliations": [
        {
          "country": "United States of America",
          "city": "Texas",
          "institution": "Texas A&M University"
        }
      ]
    },
    {
      "id": 139728,
      "firstName": "JŸrgen",
      "lastName": "Ziegler",
      "importedId": "DC1",
      "source": "CSV",
      "affiliations": [
        {
          "country": "Germany",
          "institution": "University of Duisburg-Essen (UDE)"
        }
      ]
    },
    {
      "id": 153766,
      "firstName": "Axel",
      "lastName": "Soto",
      "importedId": "11505",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 153767,
      "firstName": "Dezhi",
      "lastName": "Wu",
      "importedId": "11506",
      "source": "SYS",
      "affiliations": []
    }
  ],
  "recognitions": [
    {
      "id": 10074,
      "name": "Impact Paper Award",
      "iconName": "diamond"
    }
  ]
}