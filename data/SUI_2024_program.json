{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10123,
    "shortName": "SUI",
    "displayShortName": "",
    "year": 2024,
    "startDate": 1728259200000,
    "endDate": 1728432000000,
    "fullName": "12th ACM Symposium on Spatial User Interaction",
    "url": "https://sui.acm.org/2024/",
    "location": "Trier, Germany",
    "timeZoneOffset": 120,
    "timeZoneName": "Europe/Berlin",
    "logoUrl": "https://files.sigchi.org/conference/logo/10123/ba257578-4d74-7bbe-db9d-48e8e47f58ea.png",
    "name": "SUI 2024"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 8,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2024-10-03 09:44:44+00"
  },
  "sponsors": [
    {
      "id": 10629,
      "name": "Association for Computing Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/17a2e7a7-b80f-9122-1df7-ecbde3e16c48.png",
      "levelId": 10376,
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10630,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/3e2af7de-306f-ca60-ced7-ee96ede6a9f1.png",
      "levelId": 10376,
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10631,
      "name": "ACM SIGGRAPH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/6f5bf0d7-a5d8-0fec-0e8a-62b693a9847d.png",
      "levelId": 10376,
      "order": 2,
      "extraPadding": 8
    },
    {
      "id": 10632,
      "name": "Ubiquitous Media Technology Lab",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/500886b7-7521-ec58-925c-c4bbc87436fa.png",
      "levelId": 10376,
      "order": 3,
      "extraPadding": 8
    },
    {
      "id": 10633,
      "name": "Human Computer Interaction Lab",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/fe8675fc-efb0-d76b-5a04-521b340bee91.png",
      "levelId": 10376,
      "order": 4,
      "extraPadding": 8
    },
    {
      "id": 10634,
      "name": "Deutsche Forschungsgemeinschaft",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/c1f2d2e4-747c-86d5-c601-91fc3ab8ff9f.png",
      "levelId": 10376,
      "order": 5,
      "extraPadding": 8
    },
    {
      "id": 10635,
      "name": "Deutsches Forschungszentrum für Künstliche Intelligenz GmbH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10123/logo/65c589eb-5fe3-a1aa-cbaa-adac31835dd4.png",
      "levelId": 10376,
      "order": 6,
      "extraPadding": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10376,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10364,
      "name": "Ground Floor",
      "roomIds": [
        11792,
        11793
      ]
    },
    {
      "id": 10365,
      "name": "1st Floor",
      "roomIds": [
        11794,
        11795
      ]
    }
  ],
  "rooms": [
    {
      "id": 11792,
      "name": "Kapelle (K 101)",
      "setup": "CLASSROOM",
      "typeId": 13784,
      "note": ""
    },
    {
      "id": 11793,
      "name": "Lecture Hall (HS 11)",
      "setup": "CLASSROOM",
      "typeId": 13786,
      "note": ""
    },
    {
      "id": 11794,
      "name": "Seminar Room (HZ 202)",
      "setup": "NO_ROOM",
      "typeId": 13786
    },
    {
      "id": 11795,
      "name": "Seminar Room (HZ 203)",
      "setup": "NO_ROOM",
      "typeId": 13786
    }
  ],
  "tracks": [
    {
      "id": 13115,
      "name": "SUI 2024 Papers",
      "typeId": 13786
    },
    {
      "id": 13116,
      "name": "SUI 2024 Demos",
      "typeId": 13782
    },
    {
      "id": 13117,
      "name": "SUI 2024 Posters",
      "typeId": 13787
    },
    {
      "id": 13118,
      "typeId": 13829
    },
    {
      "id": 13119,
      "typeId": 13832
    },
    {
      "id": 13120,
      "typeId": 13833
    },
    {
      "id": 13121,
      "typeId": 13790
    },
    {
      "id": 13122,
      "typeId": 13824
    },
    {
      "id": 13123,
      "typeId": 13825
    },
    {
      "id": 13124,
      "typeId": 13826
    },
    {
      "id": 13125,
      "typeId": 13829
    },
    {
      "id": 13126,
      "typeId": 13828
    },
    {
      "id": 13127,
      "typeId": 13830
    },
    {
      "id": 13128,
      "typeId": 13784
    },
    {
      "id": 13129,
      "typeId": 13784
    },
    {
      "id": 13130,
      "typeId": 13831
    },
    {
      "id": 13131,
      "typeId": 13830
    },
    {
      "id": 13132,
      "typeId": 13827
    },
    {
      "id": 13133,
      "typeId": 13834
    }
  ],
  "contentTypes": [
    {
      "id": 13781,
      "name": "Course",
      "displayName": "Courses",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 13782,
      "name": "Demo",
      "displayName": "Demos",
      "color": "#006d2c",
      "duration": 60
    },
    {
      "id": 13783,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13784,
      "name": "Event",
      "displayName": "Events",
      "color": "#ffc034",
      "duration": 0
    },
    {
      "id": 13785,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 13786,
      "name": "Paper",
      "displayName": "Papers",
      "color": "#0d42cc",
      "duration": 18
    },
    {
      "id": 13787,
      "name": "Poster",
      "displayName": "Posters",
      "color": "#ff7a00",
      "duration": 60
    },
    {
      "id": 13788,
      "name": "Work-in-Progress",
      "displayName": "Works-In-Progress",
      "color": "#ff99ca",
      "duration": 5
    },
    {
      "id": 13789,
      "name": "Workshop",
      "displayName": "Workshops",
      "color": "#f60000",
      "duration": 240
    },
    {
      "id": 13790,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 30
    },
    {
      "id": 13824,
      "name": "Welcome Reception",
      "color": "#32d923",
      "duration": 150
    },
    {
      "id": 13825,
      "name": "Registration",
      "color": "#acadb9",
      "duration": 60
    },
    {
      "id": 13826,
      "name": "Keynotes",
      "color": "#ff99ca",
      "duration": 75
    },
    {
      "id": 13827,
      "name": "Lunch Break",
      "color": "#8e008b",
      "duration": 90
    },
    {
      "id": 13828,
      "name": "Lunch Break 2",
      "color": "#8e008b",
      "duration": 60
    },
    {
      "id": 13829,
      "name": "Coffee Break",
      "color": "#8e008b",
      "duration": 30
    },
    {
      "id": 13830,
      "name": "Coffee Break 2",
      "color": "#8e008b",
      "duration": 15
    },
    {
      "id": 13831,
      "name": "Registration 2",
      "color": "#acadb9",
      "duration": 30
    },
    {
      "id": 13832,
      "name": "Special Topic Session",
      "color": "#acadb9",
      "duration": 60
    },
    {
      "id": 13833,
      "name": "Closing",
      "color": "#ff99ca",
      "duration": 30
    },
    {
      "id": 13834,
      "name": "Keynote 2",
      "color": "#ff99ca",
      "duration": 60
    }
  ],
  "timeSlots": [
    {
      "id": 14387,
      "type": "SESSION",
      "startDate": 1728293400000,
      "endDate": 1728297900000
    },
    {
      "id": 14388,
      "type": "BREAK",
      "startDate": 1728297900000,
      "endDate": 1728299700000
    },
    {
      "id": 14389,
      "type": "SESSION",
      "startDate": 1728299700000,
      "endDate": 1728304200000
    },
    {
      "id": 14390,
      "type": "LUNCH",
      "startDate": 1728304200000,
      "endDate": 1728309600000
    },
    {
      "id": 14391,
      "type": "SESSION",
      "startDate": 1728309600000,
      "endDate": 1728315000000
    },
    {
      "id": 14392,
      "type": "BREAK",
      "startDate": 1728315000000,
      "endDate": 1728315900000
    },
    {
      "id": 14393,
      "type": "SESSION",
      "startDate": 1728315900000,
      "endDate": 1728319500000
    },
    {
      "id": 14394,
      "type": "SESSION",
      "startDate": 1728319500000,
      "endDate": 1728324000000
    },
    {
      "id": 14402,
      "type": "SESSION",
      "startDate": 1728289800000,
      "endDate": 1728293400000
    },
    {
      "id": 14403,
      "type": "SESSION",
      "startDate": 1728376200000,
      "endDate": 1728378000000
    },
    {
      "id": 14404,
      "type": "SESSION",
      "startDate": 1728378000000,
      "endDate": 1728383400000
    },
    {
      "id": 14405,
      "type": "BREAK",
      "startDate": 1728383400000,
      "endDate": 1728384300000
    },
    {
      "id": 14406,
      "type": "SESSION",
      "startDate": 1728384300000,
      "endDate": 1728387900000
    },
    {
      "id": 14407,
      "type": "SESSION",
      "startDate": 1728387900000,
      "endDate": 1728393300000
    },
    {
      "id": 14408,
      "type": "LUNCH",
      "startDate": 1728393300000,
      "endDate": 1728396900000
    },
    {
      "id": 14409,
      "type": "SESSION",
      "startDate": 1728396900000,
      "endDate": 1728401400000
    },
    {
      "id": 14410,
      "type": "BREAK",
      "startDate": 1728401400000,
      "endDate": 1728403200000
    },
    {
      "id": 14411,
      "type": "SESSION",
      "startDate": 1728403200000,
      "endDate": 1728406800000
    },
    {
      "id": 14413,
      "type": "SESSION",
      "startDate": 1728406800000,
      "endDate": 1728408600000
    },
    {
      "id": 14414,
      "type": "BREAK",
      "startDate": 1728408600000,
      "endDate": 1728410400000
    },
    {
      "id": 14415,
      "type": "SESSION",
      "startDate": 1728410400000,
      "endDate": 1728419400000
    },
    {
      "id": 14460,
      "type": "SESSION",
      "startDate": 1728464400000,
      "endDate": 1728468000000
    }
  ],
  "sessions": [
    {
      "id": 174994,
      "name": "Session 1 (Perception)",
      "isParallelPresentation": false,
      "importedId": "14920",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174833,
        174839,
        174837,
        174840
      ],
      "source": "SYS",
      "timeSlotId": 14389
    },
    {
      "id": 174995,
      "name": "Session 2 (Selection & Manipulation)",
      "isParallelPresentation": false,
      "importedId": "14921",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174846,
        174850,
        174827,
        174832,
        174831
      ],
      "source": "SYS",
      "timeSlotId": 14391
    },
    {
      "id": 174996,
      "name": "Demo Session",
      "isParallelPresentation": true,
      "importedId": "14922",
      "typeId": 13782,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        174879,
        174883,
        174875,
        174877,
        174882,
        174876,
        174878,
        174880,
        174881
      ],
      "source": "SYS",
      "timeSlotId": 14393
    },
    {
      "id": 174997,
      "name": "Poster Session",
      "isParallelPresentation": true,
      "importedId": "14923",
      "typeId": 13787,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        174986,
        174988,
        174992,
        174980,
        174969,
        174993,
        174982,
        174989,
        174991,
        174985,
        174972,
        174974,
        174971,
        174984,
        174968,
        174979,
        174983,
        174977,
        174973,
        174990,
        174981,
        174987,
        174975,
        174976,
        174970,
        174978
      ],
      "source": "SYS",
      "timeSlotId": 14406
    },
    {
      "id": 174998,
      "name": "Session 3 (Multi-User)",
      "isParallelPresentation": false,
      "importedId": "14924",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174842,
        174825,
        174847,
        174826
      ],
      "source": "SYS",
      "timeSlotId": 14394
    },
    {
      "id": 174999,
      "name": "Session 4 (Augmented & Mixed Reality)",
      "isParallelPresentation": false,
      "importedId": "14925",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174838,
        174841,
        174843,
        174845,
        174848
      ],
      "source": "SYS",
      "timeSlotId": 14404
    },
    {
      "id": 175000,
      "name": "Session 5 (Assistance, Accessibility & Guidance)",
      "isParallelPresentation": false,
      "importedId": "14926",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174830,
        174834,
        174824,
        174828,
        174829
      ],
      "source": "SYS",
      "timeSlotId": 14407
    },
    {
      "id": 175001,
      "name": "Session 6 (Application & Games)",
      "isParallelPresentation": false,
      "importedId": "14927",
      "typeId": 13786,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        174836,
        174835,
        174849,
        174844
      ],
      "source": "SYS",
      "timeSlotId": 14409
    }
  ],
  "events": [
    {
      "id": 175017,
      "name": "Break",
      "isParallelPresentation": false,
      "importedId": "14928",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175005
      ],
      "startDate": 1728408600000,
      "endDate": 1728410400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175018,
      "name": "Closing & Award Ceremony",
      "isParallelPresentation": false,
      "importedId": "14929",
      "typeId": 13784,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        175004
      ],
      "startDate": 1728406800000,
      "endDate": 1728408600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175019,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "14930",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175002
      ],
      "startDate": 1728401400000,
      "endDate": 1728403200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175020,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "14931",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175011
      ],
      "startDate": 1728315000000,
      "endDate": 1728315900000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175021,
      "name": "Coffee Break (incl. Demo Setup)",
      "isParallelPresentation": false,
      "importedId": "14932",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175009
      ],
      "startDate": 1728297900000,
      "endDate": 1728299700000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175022,
      "name": "Coffee Break (incl. Poster Setup)",
      "isParallelPresentation": false,
      "importedId": "14933",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175015
      ],
      "startDate": 1728383400000,
      "endDate": 1728384300000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175023,
      "name": "Gather at Bus Pick-Up Location",
      "isParallelPresentation": false,
      "importedId": "14934",
      "typeId": 13784,
      "chairIds": [],
      "contentIds": [
        175012
      ],
      "startDate": 1728324000000,
      "endDate": 1728325800000,
      "location": "In Front of Venue",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175024,
      "name": "Joint Reception & Poster Session (SUI & VRST)",
      "isParallelPresentation": false,
      "importedId": "14935",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175006
      ],
      "startDate": 1728410400000,
      "endDate": 1728419400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175025,
      "name": "Lunch Break",
      "isParallelPresentation": false,
      "importedId": "14936",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175010
      ],
      "startDate": 1728393300000,
      "endDate": 1728396900000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175026,
      "name": "Lunch Break",
      "isParallelPresentation": false,
      "importedId": "14937",
      "typeId": 13784,
      "roomId": 11792,
      "chairIds": [],
      "contentIds": [
        175016
      ],
      "startDate": 1728304200000,
      "endDate": 1728309600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175027,
      "name": "Opening & Keynote",
      "isParallelPresentation": false,
      "importedId": "14938",
      "typeId": 13784,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        175008
      ],
      "startDate": 1728293400000,
      "endDate": 1728297900000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175028,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14939",
      "typeId": 13784,
      "chairIds": [],
      "contentIds": [
        175007
      ],
      "startDate": 1728289800000,
      "endDate": 1728293400000,
      "location": "Conference Building",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175029,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14940",
      "typeId": 13784,
      "chairIds": [],
      "contentIds": [
        175014
      ],
      "startDate": 1728376200000,
      "endDate": 1728378000000,
      "location": "Conference Building",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175030,
      "name": "Special Topic Session",
      "isParallelPresentation": false,
      "importedId": "14941",
      "typeId": 13784,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        175003
      ],
      "startDate": 1728403200000,
      "endDate": 1728406800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175031,
      "name": "Wine Cellar Tour & Dinner",
      "isParallelPresentation": false,
      "importedId": "14942",
      "typeId": 13784,
      "chairIds": [],
      "contentIds": [
        175013
      ],
      "startDate": 1728325800000,
      "endDate": 1728325800000,
      "location": "Weingut-Weinstube-Restaurant von Nell",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 175033,
      "name": "Joint Keynote (SUI & VRST)",
      "isParallelPresentation": false,
      "importedId": "14943",
      "typeId": 13784,
      "roomId": 11793,
      "chairIds": [],
      "contentIds": [
        175032
      ],
      "startDate": 1728464400000,
      "endDate": 1728468000000,
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 174824,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Improving Video Navigation for Spatial Task Tutorials by Spatially Segmenting and Situating How-To Videos",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1077",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175000
      ],
      "eventIds": [],
      "abstract": "How-to videos are widely used for accessing instructional content. Many of tasks covered in these videos are spatial, requiring movement between locations within a physical space to complete different parts of the activity.\r\nConventional linear video interfaces, which often only allow time-based  navigational techniques, like scrubbing, prove inefficient and cumbersome for such tasks.\r\nTo address this, we investigate an approach for video browsing and navigation optimized for how-to videos involving spatial tasks by chaptering videos based on the location where tasks take place and using augmented reality to anchor these video segments to their physical locations via virtual signposts.\r\nThrough two studies, we demonstrate that our approach outperforms both standard and chaptered video interfaces in terms of speed and ease.\r\nOur work contributes empirical evidence that spatially segmenting and situating tutorials is a promising strategy for improving video navigation.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "SPECTRAL, Computer Science"
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174806
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Saskatchewan",
              "city": "Saskatoon",
              "institution": "University of Saskatchewan",
              "dsl": ""
            }
          ],
          "personId": 174741
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174752
        }
      ]
    },
    {
      "id": 174825,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Social VR for Professional Networking: A Spatial Perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1099",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174998
      ],
      "eventIds": [],
      "abstract": "One essential function of professional events, such as industry trade shows and academic conferences, is to foster and extend a person's connections to others within a specific community. In this paper, we delve into the emerging practice of transitioning these events from physical venues to social VR as a new medium. Specifically, we ask: how does the spatial design in social VR affect the attendee’s networking at these events? To this end, we conducted qualitative research, obtaining observation notes and interview responses from 13 participants. Each had attended at least one professional event taking place in social VR. We identified four elements of (virtual) spatial design that participants commonly relied upon to navigate their social interactions during the events. Some of these elements were interpreted differently depending on the role of the participant (i.e., event hosts vs. regular attendees). We concluded this paper by outlining four design implications derived from our findings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland ",
              "dsl": "Information Science"
            }
          ],
          "personId": 174746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "College of Information Studies"
            }
          ],
          "personId": 174804
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174742
        }
      ]
    },
    {
      "id": 174826,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Where to Draw the Line: Physical Space Partitioning and View Privacy in AR-based Co-located Collaboration for Immersive Analytics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1151",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174998
      ],
      "eventIds": [],
      "abstract": "This paper investigates two main aspects of co-located collaboration using Augmented Reality (AR) for Immersive Analytics (IA): physical space partitioning and view privacy. AR-based collaborative work in IA can greatly benefit from direct conversational awareness cues and enhanced mutual understanding between users. However, some challenges still exist, particularly in enabling efficient interaction for IA tasks such as analysis and decision-making on complex data within limited physical space. Moreover, collaborative IA often involves both cooperative and individual tasks with experts of diverse backgrounds, necessitating effective workspace management. To address spatial proximity issues in limited space such as offices or meeting rooms, we explored a workspace partitioning approach that divided physical space with virtual boundaries on the floor. We conducted a user study to examine workspace management approaches (partitioning and non-partitioning) in conjunction with view privacy policies (public and private view). Findings suggest that under private view conditions, individual tasks were completed more quickly, and non-partitioning facilitated faster placement of shared objects. Additionally, public view improved object arrangement time in partitioned space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN, VENISE Team",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN, VENISE Team",
              "dsl": ""
            }
          ],
          "personId": 174727
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 174774
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Essonne",
              "city": "Orsay",
              "institution": "LISN, ARAI team",
              "dsl": "Université Paris-Saclay, CNRS"
            },
            {
              "country": "France",
              "state": "Essonne",
              "city": "Orsay",
              "institution": "LISN, ARAI team",
              "dsl": "Université Paris-Saclay, CNRS"
            }
          ],
          "personId": 174749
        }
      ]
    },
    {
      "id": 174827,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Guiding Handrays in Virtual Reality: Comparison of Gaze and Object Based Assistive Raycast Redirection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1074",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174995
      ],
      "eventIds": [],
      "abstract": "Handray selection is widely used for hand-tracking-based interactions in head-mounted displays, as it is a simple and straightforward interaction technique. However, selection performance decreases for small and distant objects. It is also negatively affected by input inaccuracies due to natural hand tremors, tracking issues, and movement caused by pinch gestures. Recent work introduced assistive raycast redirection for controller raycasting which facilitates object selection in virtual reality. It applies a gradual proximity and gain-based redirection of the ray towards the target center within a predefined redirection zone. Inspired by this approach, we implemented two redirection techniques for improving handray selection while maintaining ease of use: \\textit{RayToTarget} and the gaze-assisted \\textit{RayToGaze}. We evaluated them together with classic handray in a Fitts' Law user study. Our findings suggest that both redirection techniques perform significantly better than classic handray, but performance for RayToGaze decreases at greater target depth compared to RayToTarget. Generally, handray redirection was well received and did not decrease the sense of agency. However, different individual preferences and target acquisition strategies affect the user experience for both redirection techniques and might impact selection performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human Computer Interaction"
            },
            {
              "country": "Germany",
              "state": "Hamburg",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Understanding Written Artefacts"
            }
          ],
          "personId": 174770
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 174729
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174755
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            },
            {
              "country": "Germany",
              "state": "Hamburg",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Understanding Written Artefacts"
            }
          ],
          "personId": 174739
        }
      ]
    },
    {
      "id": 174828,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Automatic Video-to-Audiotactile Conversion of Golf Broadcasting on A Refreshable Pin Array",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1097",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175000
      ],
      "eventIds": [],
      "abstract": "Video accessibility is an important but challenging research question.\r\nIn this study, we implemented and evaluated a system that converts video content into audio clips and tactile icons without losing context using a refreshable pin array display.\r\nThe suggested system converts contextual information of the video to audio description and tactile scenes, allowing users to hear and touch.\r\nAs an initial target, we selected golf broadcasting, which has a clear context yet relies heavily on visual features and provides limited information through audio.\r\nWe extracted contextual information through computer vision to deliver information such as scores and the trajectories and results of shots.\r\nThen, we converted them to audio via Text-to-Speech and tactile icons on the pin array.\r\nWe evaluated the system by conducting a perception experiment and a usability survey, and the results showed that the system effectively converted the information. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 174759
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi-do",
              "city": "Ansan",
              "institution": "Hanyang University ERICA",
              "dsl": "School of Computing"
            }
          ],
          "personId": 174723
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi-do",
              "city": "Ansan",
              "institution": "Hanyang University ERICA",
              "dsl": "School of Computing"
            }
          ],
          "personId": 174769
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi-do",
              "city": "Ansan-si",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 174792
        }
      ]
    },
    {
      "id": 174829,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Annorama: Enabling Immersive At-Desk Annotation Experiences in Virtual Reality with 3D Point Cloud Dioramas",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1131",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175000
      ],
      "eventIds": [],
      "abstract": "Point cloud annotation plays a pivotal role in computer vision and machine learning by facilitating the creation of volumetric annotations in 3D space. While prior research has explored point cloud annotation in VR environments, its practical implementation in space-constrained office settings, where data annotation is typically conducted, remains an open question. In this paper, we introduce Annorama, a interactive system that translates 3D point cloud scenes into miniature desk-scale dioramas, enabling annotation using a unique family of keyboard-assisted mid-air gestures inspired by direct manipulation. Through a within-subjects study with 16 participants, we demonstrate the feasibility of our system by assessing the efficacy of four types of mid-air gestures for drawing cuboid annotations. Our findings suggest that Annorama allows for rapid and accurate annotation of point cloud data, particularly with the Sizing and Two Point Gestures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Amazon Web Services",
              "dsl": "Amazon Science"
            }
          ],
          "personId": 174800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 174785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 174809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 174747
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 174765
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 174778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Santa Clara",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 174784
        }
      ]
    },
    {
      "id": 174830,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "TAGGAR: General-Purpose Task Guidance from Natural Language in Augmented Reality using Vision-Language Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1054",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175000
      ],
      "eventIds": [],
      "abstract": "Augmented reality (AR) task guidance systems provide assistance for procedural tasks, which require a sequence of physical actions, by rendering virtual guidance visuals within the real-world environment. Current AR task guidance systems are limited in that they require AR system experts to manually place visuals, require CAD models of real-world objects, or only function for limited types of tasks or environments. We propose a general-purpose AR task guidance approach and proof-of-concept system to generate guidance for tasks defined by natural language. Our approach allows an operator to take pictures of relevant objects and write task instructions for an end user, which are used by the system to determine where to place guidance visuals. Then, an end user can receive and follow guidance even if objects change locations or environments. Our approach utilizes current vision-language machine learning models for text and image semantic understanding and object localization. We built a proof-of-concept system, called TAGGAR (Text to Automated General-purpose Guidance in AR), using our approach and tested its accuracy and usability in a user study. We found that all operators were able to generate clear guidance for tasks in an office, and end users were able to follow the guidance visuals to complete the expected action 85.7% of the time without any knowledge of the tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 174775
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 174798
        }
      ]
    },
    {
      "id": 174831,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Ubiquitous BlowClick: Non-verbal Vocal Input for Confirmation with Hand-held Mobile Devices in the Field",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1136",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174995
      ],
      "eventIds": [],
      "abstract": "Mobile devices have become integral to modern life, with tasks often requiring one-handed interaction. However, conventional methods, such as tapping, face limitations, especially in scenarios where hands-free operation is crucial, like driving or using AR glasses. Alternative approaches, including speech commands and non-verbal vocal input (NVVI), have been explored to address these challenges. While speech commands suffer from inherent delays, NVVI, characterized by simple audio signatures, presents a promising solution. This study aims to integrate and evaluate NVVI on mobile devices compared to traditional tapping interaction. We introduce machine learning-based NVVI classification into an Android-based architecture, providing a fast and resource-efficient pipeline. Through empirical evaluations, including reaction time tasks and ISO 9241:411 Fitts's law selection tasks, we assess the feasibility and effectiveness of NVVI. Our findings indicate that while tapping exhibits a slight speed advantage in conventional use, blowing is consistently detected and executed with efficiency, resulting in significantly faster input, particularly when the hands are initially distant.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 174803
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 174745
        }
      ]
    },
    {
      "id": 174832,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Evaluation of Retrieval Techniques for Out-of-Range VR Objects, Contrasting Controller-Based and Free-Hand Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1115",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174995
      ],
      "eventIds": [],
      "abstract": "In VR environments like ``sandbox'' applications or interactive molecule simulators for education, physics-based objects can move beyond a user's natural reach. We evaluate four interaction methods to support efficient and intuitive retrieval of objects for such events, including one that has not been evaluated previously and three others derived from well-known techniques. Considering the proliferation of camera-based hand tracking in VR headsets, there is a large interest in contrasting held-controller and free-hand interaction methods, so our evaluation considers both input types to further understand tradeoffs. We gathered performance data and subjective impressions from 52 subjects in a representative game-like puzzle task. A hand-extension (``go-go''-type) technique was least promising, with image-plane and pointing-based techniques being more promising. The recent tether-handle technique was roughly on-par with others for controller input (details vary by metric); it simply uses the same underlying grab metaphor as the main interaction. For free-hand interaction, its performance is reduced in a way that reflects broader problems of grab detection and manipulation methods for whole-hand interaction in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 174788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 174721
        }
      ]
    },
    {
      "id": 174833,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Mapping Real World Locomotion Speed to the Virtual World in Large Field of View Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1092",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174994
      ],
      "eventIds": [],
      "abstract": "In virtual environments, the tracking of physical movements in the\r\nreal world and mapping them to movement in the virtual world\r\nincreases immersion and the experience of presence. For example,\r\nwalking on a treadmill in the physical world may be mapped to the\r\nmovement of the camera in a first-person view of the virtual world.\r\nHowever, due to interrelated factors relating to the field of view and\r\ndistortion of objects in the virtual environment, matching the feeling\r\nof a user’s physical movement speed to their movement speed in\r\nthe virtual world can be complex. This possible mismatch between\r\nperceived physical speed and virtual movement speed is detrimen-\r\ntal as it can induce motion sickness and reduce the experience of\r\npresence. Although this has been investigated with head-mounted\r\ndisplays, there is little information about how to overcome this mis-\r\nmatch when using large 2D screens that provide a very different\r\nviewing environment. To address this gap, we investigate how a\r\n180-degree display that nearly fills the entire human FOV impacts\r\nthis perceptual mismatch while moving (walking and running on a\r\ntreadmill). Our results show that people prefer camera speeds that\r\nactually exceed their physical movement speed, and increasingly\r\nso at higher speeds. Interestingly, though, people’s tolerance for\r\ndeviations from the ideal camera speed mapping does not change\r\nwith movement speed. Inter-participant differences suggest that a\r\nsingle universal model for matching VE speed to movement speed\r\nis insufficient, so we propose a simple personalized linear model\r\nthat can be quickly calibrated for each user. This work therefore\r\nprovides important findings to inform and improve the design of\r\nvirtual environments for an improved user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174762
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 174807
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174752
        }
      ]
    },
    {
      "id": 174834,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Goldilocks Zoning: Evaluating a Gaze-Aware Approach to Task-Agnostic VR Notification Placement",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1071",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175000
      ],
      "eventIds": [],
      "abstract": "While virtual reality (VR) offers immersive experiences, users need to remain aware of notifications from outside VR. However, inserting notifications into a VR experience can result in distraction or breaks in presence, since existing notification systems in VR use static placement and lack situational awareness. We address this challenge by introducing a novel notification placement technique, Goldilocks Zoning, which leverages a 360-degree heatmap generated using gaze data to place notifications near salient areas of the environment without obstructing the primary task. To investigate the effectiveness of this technique, we conducted a dual-task experiment comparing Goldilocks Zoning to common notification placement techniques. Our findings reveal that for simple iconic notifications, placing the notification within the user's visual field is more critical than choosing an ideal location. The findings also demonstrate the feasibility of utilizing gaze history as a proxy for understanding user attention and task engagement in the VR setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human-Computer Interaction"
            }
          ],
          "personId": 174738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 174728
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 174798
        }
      ]
    },
    {
      "id": 174835,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Trustful Trading in Shared Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1072",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175001
      ],
      "eventIds": [],
      "abstract": "If the Metaverse is the visionary space that is poised to greatly expand human activity, building of interpersonal trust within the Metaverse must be possible. Trust has been described as a \"lubricative\" for business, innovation, resilience, and even general enjoyment. In its most basic dyadic form, peer-to-peer trading of virtual items with the\r\nmeans of shared augmented reality should be possible in a trustworthy manner. Here, we investigate how specific design choices to facilitate such a trade impact trust.\r\n\r\nA user study with 36 participants showed that the entailment of a mutual confirmation of an item exchange improves both trust towards the software system, as well as interpersonal trust. We further found that perceived closeness towards the trade peer remains a much greater influence on trust than any other effect. We also found strong correlations between user experience and trust.\r\n\r\nIn summary, our research shows that shared augmented reality can provide a great environment for trade and bartering among physically co-located peers, because potentially defrauding behaviours can be impeded by explicitly displaying item ownership and safeguarding the transfer of ownership.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HITLab NZ"
            }
          ],
          "personId": 174814
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "School of Psychology, Speech and Hearing"
            }
          ],
          "personId": 174822
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174815
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174797
        }
      ]
    },
    {
      "id": 174836,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Construction of SVS: Scale of Virtual Twin's Similarity to Physical Counterpart in Simple Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1050",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175001
      ],
      "eventIds": [],
      "abstract": "Owing to the absence of a universally accepted definition of the term “virtual twin\", varying degrees of similarity between physical prototypes and their virtual counterparts across different research papers. Consequently, we introduced a questionnaire, SVS, intended to quantify the similarity between a virtual twin and its physical counterpart in simple environments in terms of visual fidelity, physical fidelity, environmental fidelity, and functional fidelity. This paper describes the methodology employed in formulating the indicator items and provides an initial examination through between-subjects user studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174811
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174776
        }
      ]
    },
    {
      "id": 174837,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Investigating Presence Across Rendering Style and Ratio of Virtual to Real Content in Mixed Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1143",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174994
      ],
      "eventIds": [],
      "abstract": "We investigate how the amount and rendering style of virtual content impact self-reported presence and subjective preference in an extended reality environment. In a within-subjects experiment, we vary the ratio of virtual to real content across three conditions: low (mostly real with some virtual elements), medium (a balanced mix of both), and high (mostly virtual with no real visual elements). For each ratio, we use two different rendering styles for virtual content: realistic and stylized (cartoon-like), evaluating presence through standardized questionnaires (SUS, WS). Our results suggest that different ratios of virtual to real content minimally affect presence, with realistic renderings evoking stronger presence than stylized ones. Participants preferred higher amounts of virtual content and realistic virtual content over stylized versions. These findings imply that coherence and quality of virtual content may contribute more to presence in mixed reality settings than amount of virtual content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 174724
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Newfoundland and Labrador",
              "city": "St. John's",
              "institution": "Memorial University of Newfoundland",
              "dsl": "Department of Comptuer Science"
            }
          ],
          "personId": 174756
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 174801
        }
      ]
    },
    {
      "id": 174838,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Keep Track! Supporting Spatial Tasks with Augmented Reality Overviews",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1066",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174999
      ],
      "eventIds": [],
      "abstract": "In industrial environments, efficient management of task takeovers is critical, especially when tasks are complex and workers face frequent interruptions such as shift changes. These takeovers often increase the cognitive load and the potential for errors. \r\nWe propose the integration of Augmented Reality (AR) to assist users in handling takeovers of complex, and spatial distributed tasks. \r\nWe developed three visualization techniques with increasing levels of spatial registration - Diagram View as Baseline, Map View, and Location-bound View - ranging from a simple overview to spatially anchored to enhance spatial and temporal awareness during task takeovers.\r\nA comparative user study (n=24) was conducted to evaluate the effectiveness of these techniques in improving task takeovers efficiency. Our results showed that the Location-bound View, in particular, significantly reduced cognitive load and improved task performance by integrating contextual information directly into the user's field of view. This work provides insights for AR system designers and suggests that AR overviews with increasing levels of spatial registration can effectively support the takeover of complex distributed tasks in industrial settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 174799
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174737
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "OFFIS - Institute for Information Technology",
              "dsl": ""
            }
          ],
          "personId": 174772
        }
      ]
    },
    {
      "id": 174839,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Difficulties in Perceiving and Understanding Robot Reliability Changes in a Sequential Binary Task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1122",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174994
      ],
      "eventIds": [],
      "abstract": "Human-robot teams push the boundaries of what both humans and robots can accomplish. In order for the team to function well, the human must accurately assess the robot's capabilities to calibrate the trust between the human and robot. In this paper, we use virtual reality (VR), a widely accepted tool in studying human-robot interaction (HRI), to study human behaviors affecting their detection and understanding of changes in a simulated robot's reliability. We present a human-subject study to see how different reliability change factors may affect this process. Our results demonstrate that participants make judgements about robot reliability before they have accumulated sufficient evidence to make objectively high-confidence inferences about robot reliability. We show that this reliability change observation behavior diverges from behavior expectations based on the probability distribution functions used to describe observation outcomes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 174791
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "Trentino Alto Adige",
              "city": "Trento",
              "institution": "University of Trento",
              "dsl": "CIMeC"
            }
          ],
          "personId": 174819
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 174743
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 174823
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 174783
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 174733
        }
      ]
    },
    {
      "id": 174840,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Augmenting Virtual Spatial UIs with Physics- and Direction-Based Visual Motion Cues to Non-Disruptively Mitigate Motion Sickness",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1144",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174994
      ],
      "eventIds": [],
      "abstract": "The use of Virtual Reality (VR) technology in moving platforms such as vehicles can be difficult due to significant issues around motion sickness, partly due to the physical motion being occluded in VR. The use of visual cues within VR can mitigate this motion sickness. However, these additional visual cues can disrupt users. This paper presents two studies conducted on a yaw-motion platform, investigating the effectiveness of our efforts to manipulate the visually perceived motion of spatial UIs within VR environments using novel physics-based cues, reducing motion sickness with less distraction on tasks. The first study validates our design's effectiveness, while the second compares it with existing solutions (speed/direction-base cues) regarding motion sickness and distraction levels among VR users. Our findings show that our design can relieve rotational motion sickness while concurrently diminishing distraction. This study serves as a valuable starting point for research into non-disruptively interleaving motion cues with spatial UI components within VR environments to mitigate motion sickness, emphasizing the delicate equilibrium between motion sickness mitigation and preserving the user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow ",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 174812
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 174730
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "Computing Science"
            }
          ],
          "personId": 174744
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 174810
        }
      ]
    },
    {
      "id": 174841,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Support Lines and Grids for Depth Ordering in Indoor Augmented Reality using Optical See-Through Head-Mounted Displays",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1068",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174999
      ],
      "eventIds": [],
      "abstract": "X-ray vision is a technique where Augmented Reality is used to display occluded real world objects, giving users the impression of being able to see through objects or humans. However, displaying occluded objects means that occlusion is not a reliable depth cue anymore which can lead to incorrect depth ordering. Additional depth cues like support lines and grids showed predominantly positive results with respect to depth perception in AR in previous studies. However, their impact on the ordinal depth estimation in x-ray vision applications has not been evaluated yet. While multiple different designs for lines and grids have been proposed, they have not been compared against each other. We conducted a within-subject user study with 48 participants to explore different support line and grid combinations for x-ray vision in indoor environments. Our results suggest that additional depth cues can result in an increased mental demand and should be selected carefully.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bonn",
              "institution": "Fraunhofer FKIE",
              "dsl": "Human Systems Engineering"
            }
          ],
          "personId": 174734
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bonn",
              "institution": "Fraunhofer FKIE",
              "dsl": "Human Systems Engineering"
            }
          ],
          "personId": 174818
        }
      ]
    },
    {
      "id": 174842,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Design Considerations for Shared, Co-located Augmented Reality Narratives",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1047",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174998
      ],
      "eventIds": [],
      "abstract": "Augmented reality (AR) allows users to experience stories together in the same physical space. However, little is known about the impact of shared AR narratives on an individual's experience. Much of our current understanding is derived from VR contexts, which can differ significantly in presence, social interaction, and spatial awareness from stories and other entertainment experienced using AR head-worn displays. To understand the perceptions of multi-user, co-located, AR storytelling, we conducted an exploratory study involving three original AR narratives. Participants experienced each AR narrative alone or in pairs via the Microsoft Hololens 2. We collected qualitative and quantitative data from 39 participants through questionnaires and post-experience semi-structured interviews. Results indicate participants enjoyed experiencing AR narratives together and illuminated five themes relevant to the design of multi-user, co-located AR narratives. We discuss the implications of these themes and provide design recommendations for AR experience designers and storytellers revolving around interaction purpose, interaction balance, scene transition timing, and blocking of the experience. Our findings highlight the importance of exploring both user interactions and pair interactions as factors in AR storytelling research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 174767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 174753
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Sault Ste. Marie",
              "institution": "Algoma University",
              "dsl": ""
            }
          ],
          "personId": 174789
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": ""
            },
            {
              "country": "Brazil",
              "state": "",
              "city": "Rio de Janeiro",
              "institution": "Pontifical Catholic University of Rio de Janeiro",
              "dsl": ""
            }
          ],
          "personId": 174805
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 174768
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": ""
            }
          ],
          "personId": 174786
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 174794
        }
      ]
    },
    {
      "id": 174843,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Immersive Speculative Enactment to Investigate the Impact of Near-Future Mixed Reality Contact Lenses on Users' Lives Through Focus Groups",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1069",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174999
      ],
      "eventIds": [],
      "abstract": "In this paper we investigate the impact of near-future Mixed Reality (MR) contact lenses on user's everyday lives, through an Immersive Speculative Enactment (ISE) and focus groups. If or when MR technology advances to the same level of ubiquitousness of current smartphones, this is likely to have a large impact on people's everyday lives. To gain qualitative insight on this impact, we created an ISE in which participants could experience a simulated MR lens prototype together in groups of four, thereby expanding the ISE method to multiple participants for the first time. Participants were then part of a focus group, in which the impact of the MR lenses was discussed. Participants raised concerns about the future of social interactions and expressing agency over the device, while also recognising how it could have practical applications. Based on these findings we formulate three guidelines for future MR contact lenses.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174757
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174732
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, University of Lisbon",
              "dsl": "ITI / LARSyS"
            }
          ],
          "personId": 174750
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 174776
        }
      ]
    },
    {
      "id": 174844,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Becoming Q: Using Design Workshops to Explore Everyday Objects as Interaction Devices for an Augmented Reality Spy Game",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1086",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175001
      ],
      "eventIds": [],
      "abstract": "The work in this paper extends state of the art research in the field of interaction design exploring the use of everyday objects as interaction devices in Augmented Reality (AR), by taking a user defined approach to explore how users understand everyday objects as interaction devices in an AR game. A survey (n = 16) and workshop (n = 10) were conducted with members of the general public. The survey asked participants to select several everyday objects from their day to day life, answer questions regarding the object's normal function, and to think of and consider what the object could do if it was a spy gadget. The workshop followed up on this survey, participants were asked to bring their selected objects along, and during the workshop participants considered the  objects they and other participants brought to collaboratively create new ideas about how these objects could be used if they were spy gadgets. The workshops were recorded and reviewed using reflexive thematic analysis, and four themes were identified to consider when designing AR game experiences using everyday objects: `what players look for in objects', `how players want to use objects', `what players want their objects to be capable of in game' and `concerns players have about object use'.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174796
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "School of Product Design"
            }
          ],
          "personId": 174740
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174797
        }
      ]
    },
    {
      "id": 174845,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "The Effect of Augmented Reality on Performance, Task Loading, and Situational Awareness in Construction Inspection Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1142",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174999
      ],
      "eventIds": [],
      "abstract": "The construction industry is characterized by the need to perform detail-oriented tasks in complex environments -- requiring tools and systems that prioritize precision, efficiency and safety. While Augmented Reality (AR) has emerged as a potential avenue for these tools, its effectiveness and impact on performance and situation awareness, as well as the challenges it may introduce, are yet to be fully understood. This research aims to investigate the efficacy of AR’s use in this domain through the representative task of inspecting prefabricated concrete panel casts, using studies complete with visual and auditory distraction simulations to explore two new AR schematic visualization systems. This work employs a dual-task user study (N = 18) to measure the impact of the AR on Situation Awareness, Task Loading, and Task Performance when compared to the conventional standard of paper blueprints. We find that AR solutions can lower perceived mental and temporal demands without negatively affecting situation awareness. Further, the AR solutions reduced the rate of false negatives and required less time than paper blueprints, suggesting that AR holds promise for improving construction workflows through increased performance and speed without impacting the safety provided by maintaining situation awareness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "FREDERICTON",
              "institution": "UNIVERSITY OF NEW BRUNSWICK",
              "dsl": "COMPUTER SCIENCE"
            }
          ],
          "personId": 174802
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174731
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174758
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": ""
            }
          ],
          "personId": 174773
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fred",
              "institution": "University of New Brunswick",
              "dsl": "Department of Civil Engineering"
            }
          ],
          "personId": 174748
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Department of Civil Engineering"
            }
          ],
          "personId": 174763
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 174752
        }
      ]
    },
    {
      "id": 174846,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Evaluating Node Selection Techniques for Network Visualizations in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1048",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174995
      ],
      "eventIds": [],
      "abstract": "The visual analysis of networks is crucial for domain experts to understand their structure, investigate attributes, and formulate new hypotheses. Effective visual exploration relies heavily on interaction, particularly the selection of individual nodes. While node selection in 2D environments is relatively straightforward, immersive 3D environments like Virtual Reality (VR) introduce additional challenges such as clutter, occlusion, and depth perception, complicating node selection. State-of-the-art VR network analysis systems predominantly utilize a ray-based selection method controlled via VR controllers. Although effective for small and sparse graphs, this method struggles with larger and denser network visualizations. To address this limitation and enhance node selection in cluttered immersive environments, we present and compare six distinct node selection techniques through a user study involving 18 participants. Our findings reveal significant differences in the efficiency, physical effort, and user preference of these techniques, particularly in relation to graph complexity. Notably, the filter plane metaphor emerged as the superior method for selecting nodes in dense graphs. These insights advance the field of effective network exploration in immersive environments and provide a foundation for future research on general object manipulation in virtual 3D spaces. Our work informs the design of more efficient and user-friendly VR tools, ultimately enhancing the usability and effectiveness of immersive network analysis systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 174754
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 174781
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174725
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174761
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 174735
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 174821
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": ""
            }
          ],
          "personId": 174782
        }
      ]
    },
    {
      "id": 174847,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "Social VR Activities Should Support Ongoing Conversation - Comparing Older and Young Adults Desires and Requirements",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1103",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174998
      ],
      "eventIds": [],
      "abstract": "Social VR can contribute to keeping in touch with friends and relatives, and social contacts are important for the well-being of older adults. Yet fairly little is known about the needs and desires of this demographic for social VR, especially regarding the types of activity they would like to engage in. We present findings from interviews with older adults and  young adults. Here, we compare these to identify differences, commonalities, and opportunities for inter-generational social VR activities. Despite the favoring of cultural (seniors) and sport (younger adults) interactions, we found that both users groups prefer low-intensity and game-like activities that allow for ongoing conversation and 'sharing the moment'. To appeal to this demographic, ease of use, realistic avatars and the mitigation of age-related differences were core requirements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 174766
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Usability"
            }
          ],
          "personId": 174813
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "VR and Visualisation Research Group"
            }
          ],
          "personId": 174771
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 174793
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "University of Weimar",
              "dsl": "Usability"
            }
          ],
          "personId": 174760
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": ""
            }
          ],
          "personId": 174726
        }
      ]
    },
    {
      "id": 174848,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "OnArmQWERTY: An Empirical Evaluation of On-Arm Tap Typing for AR HMDs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1148",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174999
      ],
      "eventIds": [],
      "abstract": "Text entry is an essential and frequent task in Augmented Reality (AR) applications, yet developing an effective and user-friendly method remains a challenge. This paper introduces OnBodyQWERTY, a text entry technique for AR HMDs that allows users to project a virtual QWERTY keyboard onto various locations on their non-dominant hand, including the palm, the back of the hand, and both the anterior and posterior sides of the forearm. Users interact with this overlaid keyboard on their skin by tapping with the index finger of the dominant hand, benefiting from the inherent self-haptic feedback of on-body interaction. A user study involving 13 participants evaluated the performance of OnBodyQWERTY compared to a traditional mid-air virtual keyboard. The results demonstrate that OnBodyQWERTY significantly improves typing speed and accuracy. Specifically, typing on the palm location outperforms all other on-body locations, achieving a mean typing speed of 20.18 WPM and a mean error rate of 0.71\\%, which underscores the importance of comfortable, ergonomic typing postures and effective tactile feedback as key factors enhancing text entry performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS",
              "dsl": ""
            }
          ],
          "personId": 174777
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "Department of Computing"
            }
          ],
          "personId": 174817
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 174751
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": ""
            }
          ],
          "personId": 174790
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            }
          ],
          "personId": 174820
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 174722
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "School of Computing and Digital Technology, Birmingham City University",
              "dsl": "DMT Lab"
            }
          ],
          "personId": 174808
        }
      ]
    },
    {
      "id": 174849,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "3D Flight Planning Using Extended Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1082",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        175001
      ],
      "eventIds": [],
      "abstract": "For all pilots, before getting on their aircraft, they plan their flight. Through this, they preview flight information and assess the flight situation. However, despite the three-dimensional nature of flying, the majority of planning processes still rely on 2D map-based software. Extended Reality (XR) technology has been widely used in aviation, in particular in pilot and crew member training. Yet, there have been very few studies or applications that incorporate 3D visualization and XR into flight planning. To address this gap, our research focuses on the implementation of a 3D flight planning application in XR. Through a user study (N=16), we demonstrate that flight planning in 3D XR outperforms traditional 2D environments in terms of user experience. The study also provides insights into the preferred method of route planning: creating flight routes through a series of waypoints has better usability compared to drawing flight routes freely in a 3D XR environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Institute of Cartography and Geoinformation"
            }
          ],
          "personId": 174779
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zurich",
              "dsl": "Institute of Cartography and Geoinformation"
            }
          ],
          "personId": 174764
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "ZH",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Institute of Cartography and Geoinformation"
            }
          ],
          "personId": 174787
        }
      ]
    },
    {
      "id": 174850,
      "typeId": 13786,
      "durationOverride": 18,
      "title": "PhoneCanvas: 3D Sketching System Using a Depth Camera-Equipped Smartphone as a Canvas",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24a-1060",
      "source": "PCS",
      "trackId": 13115,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174995
      ],
      "eventIds": [],
      "abstract": "We present PhoneCanvas, an intuitive 3D modeling system for beginners using a depth camera-equipped smartphone. Users can draw lines, erase lines, and draw surfaces by varying their hand gestures and rotating 3D models by rotating their smartphones. These functions allow users to intuitively and quickly perform 3D modeling while freely rotating 3D models. PhoneCanvas is cost-effective in introducing and deploying, and allows simultaneous 3D modeling by multiple users through image recognition of hand movements. In this work, we conducted studies investigating the system's usability for beginners. The study results show that the system enables quick and efficient modeling of simple objects, and facilitates collaborative 3D modeling discussions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki ",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 174816
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 174736
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 174795
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 174780
        }
      ]
    },
    {
      "id": 174875,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "Exploration of On-skin Interfaces to Enhance Immersive Experiences and Promote Inclusive Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1016",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "The Cave Automatic Virtual Environment system (CAVE-like system)  system provides a physically immersive environment for both normal users and users with disabilities. However, the commonly used handheld controllers, while effective, limit user immersion and exclude some individuals with physical disabilities. We would like to propose an innovative interface device aimed at improving user immersion and accessibility in a CAVE-like system. The interface utilizes a nanogenerator-based pulse for signal generation and incorporates a nexus for real-time signal analysis and instruction generation with assistance of tracking system in CAVE-like system. The device provides users and designers with considerable flexibility to develop interactions tailored to specific gestures, physical capabilities, and ergonomic considerations, effectively overcoming the constraints associated with traditional handheld controllers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong",
              "dsl": ""
            }
          ],
          "personId": 174871
        }
      ]
    },
    {
      "id": 174876,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "Real-Time Bidirectional Head Rotation Sharing for Collaborative Interaction Enhancement",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1019",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "Remote collaboration is becoming more prevalent, yet it often struggles with effectively conveying the spatial orientation of a remote participant. We introduce an innovative communication method that enables users to share their head direction. While traditional methods like written text and spoken language suit most situations, new approaches are necessary for scenarios lacking sufficient visual or auditory cues. For instance, how can hearing-impaired individuals share directional information during a remote collaborative game? This research presents an interactive system that induces head rotation based on the other user's head direction, allowing users to grasp each other's intended direction intuitively. This system improves communication by offering an additional means to share directional cues, especially in settings where visual and auditory cues are inadequate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 174864
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 174873
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 174872
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 174851
        }
      ]
    },
    {
      "id": 174877,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "FreelForce: Reel-type Force Feedback Device with HMD for Smooth and Quick Transitions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1009",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "Many force feedback devices have been proposed to enhance the immersive experience of VR.\r\nHowever, force feedback is not always necessary in VR and the user sometimes tries to touch physical objects with their bare hands when in the pass-through mode.\r\nWe propose “FreelForce,” a device that combines an HMD and a reel strap to enable smooth and quick transitions from a hands-free state to a state where force feedback can be experienced.\r\nThree types of accessories and applications are implemented to expand the input functionality for the mixed environment of VR/MR and everyday life and to explore the design space of the technique.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174852
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174861
        }
      ]
    },
    {
      "id": 174878,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "RubiXR: Demonstration of dynamic task augmentation through co-design of interactive 3D content and 3D user interfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1020",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "Augmenting manual tasks with digital content is a major area of interest to HCI researchers investigating applications of augmented reality (AR). Taking a research-through-design approach in HCI, in this work we propose and demonstrate a novel state-driven and dynamic approach to augmenting such tasks with interactive 3D content, considering the case study of augmenting user experiences in manipulating a color-coded 3D puzzle cube.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 174865
        }
      ]
    },
    {
      "id": 174879,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "Adaptive Immersion: Mixed Reality Map Visualization with Gradual Transition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1022",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "In this demo, we present an interactive system aiming to improve the visualization of geospatial data by allowing users to gradually transition from a conventional 2D view, to a 2D view augmented with 3D elements, up to a fully immersive 3D experience. To achieve this, we combine a multi-touch table with a mixed reality head-mounted display. Here, we utilize the Magic Leap 2 and its dynamic dimming feature, which allows both, the augmentation of the MTT with 3D elements, and a fully immersive visualization.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "HCI"
            }
          ],
          "personId": 174854
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "HCI"
            }
          ],
          "personId": 174857
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174739
        }
      ]
    },
    {
      "id": 174880,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "Virtual Breeding Nursery: Towards a VR Digital Twin for Plant Breeding",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1021",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "In this demo, we present the Virtual Breeding Nursery, a VR application designed for plant breeders to do remote assessment of plant traits in a virtual reality environment. Large-scale plant breeding trials are essential for identifying seed varieties with desirable traits, such as higher yield and resistance to diseases and pests. These trials, conducted over several years and across various locations, require frequent and labor-intensive evaluations. The Virtual Breeding Nursery application, developed in collaboration with plant breeders, provides photo-realistic visualizations of 3D digital twins of breeding candidates within an immersive VR environment. It allows for efficient remote assessment of plant traits, side-by-side comparisons of breeding candidates from the same or different locations, and tracking their development over time. An autonomous robotic system was developed to regularly capture the GPS-localized multimodal data of breeding candidates, including high-resolution images, 3D laser scans, and multispectral data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Saarland",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 174863
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 174859
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Osnabrück",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 174860
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Holtsee",
              "institution": "NPZ Innovation GmbH (NPZi)",
              "dsl": ""
            }
          ],
          "personId": 174867
        }
      ]
    },
    {
      "id": 174881,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "Webcam-based Hand- and Object-Tracking for a Desktop Workspace in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1013",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "As virtual reality overlays the user's view, challenges arise when interaction with their physical surroundings is still needed. In a seated workspace environment interaction with the physical surroundings can be essential to enable productive working. Interaction with e.g. physical mouse and keyboard can be difficult when no visual reference is given to where they are placed. This demo shows a combination of computer vision-based marker detection with machine-learning-based hand detection to bring users' hands and arbitrary objects into VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH",
              "dsl": "ITC/VR-Group"
            }
          ],
          "personId": 174874
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH",
              "dsl": "ITC/VR-Group"
            }
          ],
          "personId": 174855
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 174870
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Visual Computing Institute"
            }
          ],
          "personId": 174868
        }
      ]
    },
    {
      "id": 174882,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "OSIRITap: Whole-Body Interface that Recognizes Tap Input Using Only an HMD and Wrist-Mounted IMUs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1012",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "Virtual and mixed reality (VR/MR) technologies have recently enhanced gesture recognition and motion tracking. While on-body interaction has been widely explored for system input, it suffers from input area limitations and the need for customized or expensive devices. Our proposal, OSIRITap, utilizes only an HMD and wrist-worn IMUs to realize full-body tap input. It performs whole body and hand tracking by the HMD when the hands are within the field of view (FoV) and by combining HMD and IMU when the hands are outside the FoV. Experiments confirm that the technique can recognize taps on various body parts. Additionally, we implement two applications to showcase the effectiveness of our approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174853
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174861
        }
      ]
    },
    {
      "id": 174883,
      "typeId": 13782,
      "durationOverride": 60,
      "title": "An Eye Tracking Concussion Assessment System with Integrated MR-based Sports Vision Training ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24b-1015",
      "source": "PCS",
      "trackId": 13116,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174996
      ],
      "eventIds": [],
      "abstract": "Concussions can be fatal or permanent if they occur twice in a short period. This risk has also been emphasized in ball games, and concussion assessment (CA) such as VOMS have been developed in recent years, but they contain ambiguities. Therefore, this study developed a system to digitalize CA from the gaze and head movement using an MR HMD with eye tracking function. In particular, we focused on the fact that the original ability may not be measured due to unfamiliarity with use or nervousness. We developed a CA system for daily use with a sports vision training (SVT) function to train sports-related eye movements to resolve or reduce these problems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University ",
              "dsl": ""
            }
          ],
          "personId": 174862
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Komazawa University",
              "dsl": ""
            }
          ],
          "personId": 174866
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Takamatsu",
              "institution": "Kagawa University",
              "dsl": ""
            }
          ],
          "personId": 174856
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hyogo",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 174858
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nishinomiya",
              "institution": "SMART SYSTEM STRENGTH",
              "dsl": ""
            }
          ],
          "personId": 174869
        }
      ]
    },
    {
      "id": 174968,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "FoldAR: Using the Bended Screen of Foldable Smartphones for Depth Discrimination in Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1008",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Mobile augmented reality is used in research and many commercial applications. Handheld-based AR has been widely investigated. Interacting in 3D space, however, has shortcomings when interacting along the depth axis on a flat 2D display. This paper presents the concept of FoldAR, which uses the two screen parts of foldable smartphones to display a camera view and a map view. With foldable smartphones users can spatially align the display's two parts with the dimensions of interaction. This allows to manipulate objects on two differently oriented screen parts within all three dimensions at once. We contribute a prototype and discuss possibilities and applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "Technische Hochschule Köln",
              "dsl": "moxd lab"
            }
          ],
          "personId": 174949
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "Technische Hochschule Köln",
              "dsl": ""
            }
          ],
          "personId": 174940
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "Technische Hochschule Köln",
              "dsl": ""
            }
          ],
          "personId": 174918
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "Technische Hochschule Köln",
              "dsl": "moxd lab"
            }
          ],
          "personId": 174896
        }
      ]
    },
    {
      "id": 174969,
      "typeId": 13787,
      "title": "Back to (Virtual) Reality: Preferences and Effects of Entry and Exit Transitions to Virtual Experiences for Older Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1028",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Each immersive mixed reality (MR) experience starts with a transition from the real world to virtual reality (VR) and ends with the opposite transition back. The way these transitions are performed can have a significant impact on presence and the overall user experience. However, studies have mostly been conducted with younger users, excluding older adults. \r\nIn this study, four enter and exit transitions were analyzed regarding presence, user experience, and preference for older adults: (i) a direct transition, (ii) fade transition, (iii) feedback transition, and (iv) active control transition. Results indicate that older adults preferred the active control and the feedback transition over the two traditional ones, with the active control receiving the highest user experience ratings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174951
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174938
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174739
        }
      ]
    },
    {
      "id": 174970,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Training for Ultrasound Imaging: An Evaluation of SonoGame",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1007",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Ultrasound examination is an important imaging method used across the medical field. However, during medical education, practical training for ultrasound imaging is not yet widely available, while good potential exists for gamification. SonoGame is a hands-on platform for medical students developed to train ultrasound skills using a game-based approach. It consists of mini games that challenge the player to examine, identify, and recreate 2D cross-sectional images of a 3D virtual volume. SonoGame provides a step-by-step introduction to ultrasound imaging. It aims at improving ultrasound skills by learning how to orient in an ultrasound image and by developing necessary spatial visualization as well as hand-eye coordination skills.\r\n\r\nWe present and discuss a prospective, single-center study of the effects of SonoGame-based training on ultrasound skills. Medical students without previous practical ultrasound experience repeatedly played SonoGame over a period of four weeks. They significantly improved in their ultrasound skills, particularly with complex anatomical structures, and, post-training, they reported feeling more familiar with and confident in their use of sonography. What is more, they considered SonoGame a valuable supportive tool for ultrasound skills training. \r\n\r\nWe conclude that SonoGame-based training can usefully supplement conventional ultrasound education of medical students, as it promotes and supports the skill set required for ultrasound examinations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174943
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174931
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174899
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174942
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174909
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University Hospital Tuebingen",
              "dsl": ""
            }
          ],
          "personId": 174937
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Flensburg",
              "institution": "Flensburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 174928
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Flensburg",
              "institution": "Flensburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 174958
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Flensburg",
              "institution": "Flensburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 174964
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Flensburg",
              "institution": "Flensburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 174959
        }
      ]
    },
    {
      "id": 174971,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Exploring User-Defined Interactions for Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1029",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) is becoming increasingly common. Interactions in VR environments are typically facilitated through controllers or hand-tracking approaches, which often lack user configurability and adaptability. To address this, we explore user-defined interactions for custom proxy objects. We conducted an elicitation study to understand how users would create their own interaction metaphors for common VR tasks, i.e., text typing and color picking. Participants were presented with a 3D-printed object selected from a set of initial requirements and were asked to define their own interaction mappings. In this work, we discuss the resulting concepts and their implications for future developments in personalized interactions within immersive virtual environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174905
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Saarland",
              "city": "Saarbrücken",
              "institution": "Deutsche Hochschule für Prävention und Gesundheitsmanagement (DHfPG)",
              "dsl": ""
            }
          ],
          "personId": 174927
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 174939
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI), Saarland Informatics Campus",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 174886
        }
      ]
    },
    {
      "id": 174972,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Exploring Gaze-Based Menu Navigation in Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1026",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "With the integration of eye tracking technologies in Augmented Reality (AR) and Virtual Reality (VR) headsets, gaze-based interactions have opened up new possibilities for user interface design, including menu navigation. Prior research in gaze-based menu navigation in VR has predominantly focused on pie menus, yet recent studies indicate a user preference for list layouts. However, the comparison of gaze-based interactions on list menus is lacking in the literature. This work aims to fill this gap by exploring the viability of list menus for multi-level gaze-based menu navigation in VR and evaluating the efficiency of various gaze-based interactions, such as dwelling and border-crossing, against traditional controller navigation and multi-modal interaction using gaze and button press.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            }
          ],
          "personId": 174902
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": ""
            }
          ],
          "personId": 174919
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Oldenburg University",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 174932
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 174910
        }
      ]
    },
    {
      "id": 174973,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Mix n' Match Senses, Add or Remove VR: A Customized Approach towards Optimal Visualization Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1005",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "This research investigates the efficacy and usability of multisensory modeling in visualization with the option for virtual reality (VR) integration. With a flexible multisensory mapping and customized interface, users were asked to mix and match senses (visual, audio-visual, visual-haptic, audio-haptic, audio-visual-haptic, and more) and VR options (VR vs non-VR). Research results demonstrated user preference for a VR-enhanced multimodal exploration of virtual objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Spokane",
              "institution": "Eastern Washington University",
              "dsl": "Computer Science and Electrical Engineering"
            }
          ],
          "personId": 174936
        }
      ]
    },
    {
      "id": 174974,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Exploring Gesture Interaction in Underwater Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1027",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "An underwater virtual reality (UVR) system with gesture-based controls was developed to facilitate navigation and interaction while submerged. The system uses a waterproof head-mounted display and camera-based gesture recognition, originally trained for above-water conditions, employing three gestures: grab for navigation, pinch for single interactions, and point for continuous interactions. In an experimental study, we tested gesture recognition both above and underwater, and evaluated participant interaction within an immersive underwater scene. Results showed that underwater conditions slightly affected gesture accuracy, but the system maintained high performance. Participants reported a strong sense of presence and found the gestures intuitive while highlighting the need for further refinement to address usability challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Institute of Visual Computing",
              "dsl": "Bonn-Rhein-Sieg University of Applied Sciences"
            }
          ],
          "personId": 174926
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Institute of Visual Computing",
              "dsl": "Bonn-Rhein-Sieg University of Applied Sciences"
            }
          ],
          "personId": 174891
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Cybernetics and Reality Engineering Laboratory"
            }
          ],
          "personId": 174888
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Cybernetics and Reality Engineering Laboratory"
            }
          ],
          "personId": 174898
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "North Rhine-Westphalia",
              "city": "Sankt Augustin",
              "institution": "Institute of Visual Computing",
              "dsl": "Bonn-Rhein-Sieg University of Applied Sciences"
            }
          ],
          "personId": 174893
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Cybernetics & Reality Engineering"
            }
          ],
          "personId": 174955
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Nara",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Cybernetics and Reality Engineering Laboratory"
            }
          ],
          "personId": 174887
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Cybernetics and Reality Engineering Laboratory"
            }
          ],
          "personId": 174947
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Bonn-Rhein-Sieg University of Applied Sciences ",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Surrey",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts and Technology"
            }
          ],
          "personId": 174911
        }
      ]
    },
    {
      "id": 174975,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Simulating Spatial Disorientation in Pilots: Integrated VR and Rotating Chair-Based Approach",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1025",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Spatial Disorientation (SD) is a critical issue in aviation, often leading to severe accidents when pilots misinterpret their aircraft's attitude. This paper proposes a novel simulator system designed to induce unrecognized SD using Virtual Reality (VR) and a rotating chair. The system focuses on two cases of SD: False Horizon and Leans. The VR scenario includes multiple phases that challenge participants' SD by integrating visual and rotational stimuli based on an aerodynamic model. The experimental setup and procedure are designed to simulate realistic flight conditions, providing a controlled environment to study SD dynamics. Future work will involve implementing the prototype system and collecting physiological responses to enhance real-time SD detection and training protocols.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174960
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174917
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Artificial Intelligence"
            }
          ],
          "personId": 174900
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174952
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 174957
        }
      ]
    },
    {
      "id": 174976,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "The Avatar Dressing Room - a VR environment for strengthening physical and psychological embodiment with avatars",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1045",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "With the rise of remote teams, there is an increasing need for effective ways to foster positive team dynamics and outcomes across distances. One tool created for enhancing team dynamics is the Six Hats method, which encourages team members to tackle problems from various perspectives, each symbolized by a different colored hat. Inspired by this method and recognizing the potential of VR to facilitate rich spatial interactions in remote team meetings, we propose a VR meeting environment where members embody avatars with distinct characteristics and perspectives, enabling spatial and interpersonal dynamics. To efficiently increase the physical and psychological embodiment of participants with their avatars, we introduce the Avatar Dressing Room (ADR) - a modular virtual dressing room space for participants to gradually embody their avatars and adopt their characteristics. This poster details the design of the ADR spaces and space components, and presents findings from a preliminary evaluation involving nine participants. The results offer insights into the optimal characteristics of the space, avatar choices, and activity durations to maximize the effectiveness of the ADR in team ideation and spatial interaction contexts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Ramat Gan",
              "institution": "Shenkar - Engineering. Design. Art",
              "dsl": "Kadar Design and Technology Center"
            }
          ],
          "personId": 174912
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Ramat Gan",
              "institution": "Shenkar",
              "dsl": "Kadar Design and Technology Center"
            }
          ],
          "personId": 174922
        }
      ]
    },
    {
      "id": 174977,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Minimizing Errors in Eyes-Free Target Acquisition in Virtual Reality through Auditory Feedback",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1023",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "This study aims to support eyes-free target acquisition in virtual\r\nreality by enhancing the user’s three-dimensional spatial aware-\r\nness through sonification mapping pan, frequency, and amplitude\r\nto the x, y, and z axes, respectively. Our method provides two types\r\nof changing sounds. When multiple targets are sparsely arranged,\r\nthe sound changes based on the distance between the user’s hand\r\nand the targets. When multiple targets are densely arranged, the\r\nsound changes based on the distance between the user’s hand and\r\nthe central coordinate of the target group. Our two user studies,\r\nin which the targets were arranged sparsely and densely, respec-\r\ntively, showed that changing the sound exponentially and discretely\r\nminimized errors in eyes-free target acquisition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Graduate School of Engineering, The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 174913
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 174894
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Engineering"
            }
          ],
          "personId": 174950
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 174895
        }
      ]
    },
    {
      "id": 174978,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "XR4LAW: Implementing an Immersive Ergonomic User Interface for Legislative and Deliberative Institutions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1042",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "XR4LAW is an innovative project included in the ERC project HyperModeLex that aims to investigate a new methodology of work of parliamentarians through the use of Virtual Reality (VR) and Mixed Reality (MR) technologies. XR4LAW creates a Virtual Dashboard, with the goal of integrating legislative work into the metaverse. The bidimensional metaphor used for navigating the legislative documentation is limited considering the complexity of the material that a member of parliament should navigate and search. For this reason, the current project intends to provide an immersive environment where to find relevant documents using an easy human-computer interaction interface. The application is connected to the eXist-db database, where all legislative documents are available in XML format using the Akoma Ntoso OASIS XML standard applied to the European legislation. The primary goal is to develop an ergonomic and intuitive user interface that capitalizes on MR's capabilities, such as real-world visibility and utilizing physical spaces to overlay virtual elements. This immersive environment empowers end-users to explore and analyze legal documents in a whole new way, improving the accessibility and efficiency of parliamentary work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of Legal Studies"
            }
          ],
          "personId": 174924
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 174953
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of Legal Studies"
            }
          ],
          "personId": 174903
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of the Arts"
            }
          ],
          "personId": 174929
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of Legal Studies"
            }
          ],
          "personId": 174916
        }
      ]
    },
    {
      "id": 174979,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Hands-on VR Approach to Teach 3D Spatial Concepts of Molecular Reactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1043",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Molecules require a certain relative orientation and proximity to react with each other, a concept that is often difficult for beginners to grasp. We present a virtual reality system designed to help users learn about chemistry reactions with hands-on manipulation and interaction with molecules in a simulated lab environment. Our system emphasizes intuitive molecular interactions over a full molecular dynamics simulation, minimizing erratic molecule motion and sensitivity, in order to help users better understand spatial requirements underlying chemistry reactions. The system uses a novel approach to mapping reagent and product molecules to facilitate reaction detection and animation. We propose to evaluate our system by training a machine learning classifier on motion features (head, hand, and eye) to estimate user understanding of chemistry reactions, thereby enabling adaptive system feedback, such as providing visual hints.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": ""
            }
          ],
          "personId": 174948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 174788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Louisiana",
              "city": "Lafayette",
              "institution": "University of Louisiana at Lafayette",
              "dsl": "Center for Advanced Computer Studies"
            }
          ],
          "personId": 174721
        }
      ]
    },
    {
      "id": 174980,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Administering VR Questionnaires Generated in Google Forms",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1041",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Recent VR research has advocated for deploying questionnaires in VR (in-VRQs) rather than out of VR (out-VRQs). We present a workflow and Unity plugin that streamlines the process of incorporating questionnaires in VR environments. Our approach presents questionnaires as a user-fixed watch in VR that leverages bi-manual interaction for input.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": ""
            }
          ],
          "personId": 174920
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Newfoundland and Labrador",
              "city": "St. John's",
              "institution": "Memorial University of Newfoundland",
              "dsl": "Department of Comptuer Science"
            }
          ],
          "personId": 174756
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 174801
        }
      ]
    },
    {
      "id": 174981,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "PuzzleAide: Comparing Audio and Embodied Assistants for MR Puzzle-Solving",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1017",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Conversational Virtual Agents (CVA) have great potential to assist users in their task performance. Whether these agents need to be embodied or not was the main research question of the present pilot study. To answer this question, we designed and developed\r\na Mixed Reality (MR) application for solving a physical puzzle while interacting with a CVA. Eleven participants took part in our between-subject pilot study and interacted with two different representations of a CVA (i.e., voice-only and embodied agent).\r\nIn this short paper, we descriptively report on the participants’ problem-solving time, the number of assistance requests they made to the CVAs and the social presence they perceived while doing so.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "Università di Bologna",
              "dsl": ""
            }
          ],
          "personId": 174953
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174908
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 174933
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Hamburg",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer-Interaction"
            }
          ],
          "personId": 174907
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 174946
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": "Department of the Arts"
            }
          ],
          "personId": 174925
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 174739
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": ""
            }
          ],
          "personId": 174929
        }
      ]
    },
    {
      "id": 174982,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Emotion Estimation Using Laban Feature Values Based on a Multi-scale Kinesphere for Tasks with Various Body Motions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1039",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Laban's theory, which was originally proposed for body expression, describes the relationships between body movements and emotions. In our previous study, we proposed our original Laban feature values and demonstrated their effectiveness in estimating emotions. However, they have a limitation in that they were only effective in certain situations. To solve the limitation, we propose novel Laban feature values based on the size of the kinesphere (movable range of the body). We estimated emotions with Laban feature values. The results show the importance of the small kinesphere, which comprises the shoulders, back, and waist. This study shows the shoulders' critical influence in the accurate estimation of emotions, suggesting that the central body regions are integral to expressing emotional expressions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hyogo",
              "institution": "Kwansei Gakuin",
              "dsl": ""
            }
          ],
          "personId": 174961
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 174930
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Takamatsu",
              "institution": "Kagawa University",
              "dsl": ""
            }
          ],
          "personId": 174856
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Komazawa University",
              "dsl": ""
            }
          ],
          "personId": 174866
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hyogo",
              "city": "Sanda",
              "institution": "Kwansei Gakuin University",
              "dsl": ""
            }
          ],
          "personId": 174858
        }
      ]
    },
    {
      "id": 174983,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Improving Spatial Awareness in Video Mirror-Mediated XR Telementoring through Visual Cues",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1015",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "The quality of collaboration between an on-site novice user and a remote expert in cross-reality (XR) settings depends on mutual spatial awareness. Non-stereo video mirrors in Augmented Reality (AR) - Virtual Reality (VR) telementoring settings lack depth cues to meaningfully implement 3D interaction techniques like gesture guiding and gaze cues. This work introduces a visual cue for non-static video streams that acts as an add-on to these interaction techniques, drawing from principles of monocular depth perception. Descriptive results from a comparative within-design user study indicate that the \"illumination cue\" can improve spatial awareness primarily for the remote expert.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 174965
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Bonn-Rhein-Sieg University of Applied Sciences ",
              "dsl": ""
            }
          ],
          "personId": 174911
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Köln",
              "institution": "TH Köln",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 174954
        }
      ]
    },
    {
      "id": 174984,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Eye-Tracking Analysis for Cognitive Load Estimation in Wearable Mixed Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1037",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Previous research has indicated that cognitive load significantly affects performance and user experience in mixed reality (MR) applications. This paper analyses eye-tracking data to investigate their relationship with cognitive load in wearable MR (on HoloLens2 headset). The aim is to determine whether fixation and saccadic features, together with age, visual condition and previous experience with MR systems, can be used to infer cognitive load levels in a specific application scenario. The analysis is done on a user study with 17 individuals performing a discovery task in an w-MR application designed for building occupancy monitoring. Results reveal that participants can be grouped into two distinct groups that separate individuals by experience and may match two different cognitive load levels, confirming that longer fixation frequency and saccade duration appear to be significant predictors of lower cognitive load in w-MR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Spain/Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "Information Processing and Telecommunication Center"
            }
          ],
          "personId": 174934
        }
      ]
    },
    {
      "id": 174985,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Enhancing the Entertainment Value of Old Maid Card Game with AR Technology",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1016",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Non-digital games provide the tactile and spatial interaction of moving objects in real space, an aspect absent in digital games. This study considers card games as interactive media and introduces an “AR card game” using an optical see-through HMD. The proposed “AR Card Game” leverages AR to enhance digital information within the actual card game, preserving its inherent interactivity, and augmenting the entertainment value by controlling player information. This paper outlines the development of a card game incorporating AR-based functionality into the game \"Old Maid\". ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 174901
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Cyberscience Center",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 174884
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Cyberscience Center",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Graduate School of Information Sciences"
            }
          ],
          "personId": 174962
        }
      ]
    },
    {
      "id": 174986,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "A Literature Review of Indoor Wayfinding in Virtual Environment: Comparability to Real Environment and Ecological Validity",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1035",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) technology offers a novel approach to studying human wayfinding behavior under controlled conditions. However, it remains uncertain whether the findings from VR settings are ecologically valid in real environments.\r\nWe conducted a systematic literature review, compiling an analysis table of experimental factors, conditions, methodologies, and results from relevant studies. We categorized ecological validity into three types: Proven, Extrapolated, and Relevant. This categorization aids wayfinding researchers in understanding the scope and limitations of VR-based studies and supports the effective use of VR technology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua university",
              "dsl": "The Future laboratory"
            }
          ],
          "personId": 174904
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 174890
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University Future Lab",
              "dsl": ""
            }
          ],
          "personId": 174963
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174914
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "--- Select One ---",
              "city": "Beijing",
              "institution": "School of Digital Media & Design Arts",
              "dsl": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 174941
        }
      ]
    },
    {
      "id": 174987,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Ranking Realism in Mixed Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1014",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "We ranked \"realism\" by giving numerical values to physical plausibility, polygon count, texture detail, and shadows in Mixed Reality. The Elo ranking system shows that each graphical element has a distinct and consistent impact on overall realism ranking, suggesting such ranking systems may reliably quantify graphical realism.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 174724
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Carleton University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 174801
        }
      ]
    },
    {
      "id": 174988,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "A Nail-tip Device for Gesture and Force Input",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1036",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Micro-gestures are critical to expanding the personal computing environment. A nail-mounted device enabling micro-gesture and force inputs via the tip of a fingernail is proposed. A prototype with three touch sensors and a strain gauge to support five gestures and directional force input is fabricated. Experiments confirm successful gesture recognition. We introduce three applications to discuss the design space of the proposal.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174956
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 174861
        }
      ]
    },
    {
      "id": 174989,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Enable Natural User Interactions in Handheld Mobile Augmented Reality through Image Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1012",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR) has transformed user interaction with the digital world by blending virtual elements with the physical environment. While various AR interaction methods have been studied, most research has concentrated on evaluating interactions within controlled experimental settings, lacking validation in practical application scenarios. This project aims to address this gap by using Unity to develop a handheld mobile AR application that simulates the pottery-making process. Two versions of the application were developed: a conventional touch-based version and a mid-air natural interaction version based on hand detection and tracking. We conducted a user study of 40 participants adopting the mixed methods design. By comparing the data from the touch-based interaction group (N=20) to the mid-air natural interaction group (N=20), the study found that the touch-based version had better usability, while the mid-air version provided a more immersive experience, as acknowledged by the participants. Future work should focus on addressing areas for improvement in the mid-air interaction, which could include incorporating better distance indications and interaction feedback, thus further bridging the gap between virtual and real-world interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong Polytechnic University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong Polytechnic University",
              "dsl": ""
            }
          ],
          "personId": 174906
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University ",
              "dsl": ""
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University ",
              "dsl": ""
            }
          ],
          "personId": 174885
        }
      ]
    },
    {
      "id": 174990,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "On the Impact of a Simulated Cognitive Augmentation to Detect Deception on Decision-making Confidence",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1034",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "This paper explores the simulated integration of cognitive augmentation (CA) and augmented reality (AR) in deception detection within negotiation contexts. It assesses how AR visualizations of deception probabilities impact decision-making confidence and user acceptance. The study reveals a strong positive correlation exists between users' comfort with CA technologies and their decision-making confidence. This underscores the importance of user-centred design and familiarity with technology for effective CA implementations. The research also addresses public perceptions and ethical considerations, suggesting cautious optimism toward these technologies in high-stakes environments. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Phoenix",
              "institution": "Arizona State University",
              "dsl": "School of Social and Behavioral Sciences"
            }
          ],
          "personId": 174945
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Canterbury",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 174797
        }
      ]
    },
    {
      "id": 174991,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Enhancing Airport Wayfinding for Older Adults through Gesture-Based Navigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1031",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "With the increasing global trend of aging, the proportion of older adults is steadily rising, posing significant challenges to their mobility, especially in large and complex environments such as airports. These environments are often overwhelming due to high information complexity, causing older adults to experience difficulties in navigation. Although numerous navigation tools are available, their high cognitive load and complexity render them less accessible for older adults. Common issues include difficulty in reading maps, interpreting signage, and operating digital devices.\r\n\r\nGesture-based navigation systems offer a promising solution to these challenges by providing a more intuitive and user-friendly interface. Such systems leverage natural human gestures, reducing the learning curve and minimizing cognitive load. Our research introduces a gesture-based navigation aid specifically designed for older adults to enhance their airport experience.\r\n\r\nThe contributions of this study are threefold: First, it proposes a navigation method that significantly reduces navigation errors among older adults. Second, the system is easy to learn, making it accessible even to those with limited technological proficiency. Third, the interaction through gestures is more natural, promoting a seamless and stress-free navigation experience. This work aims to improve the confidence and independence of older adults during air travel, thereby enhancing their overall travel experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua university",
              "dsl": "The Future laboratory"
            }
          ],
          "personId": 174904
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Academy of Arts & Design",
              "dsl": "Tsinghua University"
            }
          ],
          "personId": 174921
        }
      ]
    },
    {
      "id": 174992,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "ARCube: Hybrid Spatial Interaction for Immersive Audio",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1032",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "The ARCube is an augmented reality (AR) interface for three-dimensional spatial control, designed to be used next to physical control devices. The user study evaluated an AR interface combined with a MIDI controller in an immersive audio environment. Key issues identified included faulty gesture detection and problems with spatial sound perception. Despite these challenges, most participants reported enhanced engagement and immersion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Center of Music Technology"
            }
          ],
          "personId": 174967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Music, Lab for Interaction & Immersion"
            }
          ],
          "personId": 174915
        }
      ]
    },
    {
      "id": 174993,
      "typeId": 13787,
      "durationOverride": 60,
      "title": "Dropping Hints: Visual Hints for Improving Learning using Mobile Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "sui24c-1030",
      "source": "PCS",
      "trackId": 13117,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        174997
      ],
      "eventIds": [],
      "abstract": "In traditional learning, students frequently need help mastering mathematical concepts due to the complexity of the content and static and non-interactive teaching methods. However, the ubiquity of mobile phones and their enabling impact on augmented reality (AR) technology improves learning experiences. AR specifically fosters spatial thinking through visual augmentations and enables interactions with dynamic content. In this paper, we propose a mobile AR learning system that utilizes these visual augmentations to present educational hints of different textual- and graphical types. The system considers teachers’ and students’ perspectives and provides respective interfaces. These interfaces are a web-based authoring tool for teachers and a mobile AR application for students. We conclude our work with ideas for future AR-based learning systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174923
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174737
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Specification of Software Systems"
            }
          ],
          "personId": 174966
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "Universität Duisburg Essen",
              "dsl": ""
            }
          ],
          "personId": 174892
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174889
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174897
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174905
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 174944
        }
      ]
    },
    {
      "id": 175002,
      "typeId": 13829,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "11",
      "source": "CSV",
      "trackId": 13118,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175019
      ],
      "authors": []
    },
    {
      "id": 175003,
      "typeId": 13832,
      "title": "The Future of Spatial Computing: Opportunities and Challenges",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "12",
      "source": "CSV",
      "trackId": 13119,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175030
      ],
      "authors": [
        {
          "affiliations": [],
          "personId": 175041
        },
        {
          "affiliations": [],
          "personId": 175042
        },
        {
          "affiliations": [],
          "personId": 175043
        },
        {
          "affiliations": [],
          "personId": 175044
        }
      ]
    },
    {
      "id": 175004,
      "typeId": 13833,
      "title": "Closing & Award Ceremony",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "13",
      "source": "CSV",
      "trackId": 13120,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175018
      ],
      "authors": []
    },
    {
      "id": 175005,
      "typeId": 13790,
      "title": "Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14",
      "source": "CSV",
      "trackId": 13121,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175017
      ],
      "authors": []
    },
    {
      "id": 175006,
      "typeId": 13824,
      "title": "Joint Reception & Poster Session (SUI & VRST)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "15",
      "source": "CSV",
      "trackId": 13122,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175024
      ],
      "authors": []
    },
    {
      "id": 175007,
      "typeId": 13825,
      "title": "Registration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 13123,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175028
      ],
      "authors": []
    },
    {
      "id": 175008,
      "typeId": 13826,
      "title": "Opening & Keynote",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "2",
      "source": "CSV",
      "trackId": 13124,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175027
      ],
      "authors": []
    },
    {
      "id": 175009,
      "typeId": 13829,
      "title": "Coffee Break (incl. Demo Setup)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3",
      "source": "CSV",
      "trackId": 13125,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175021
      ],
      "authors": []
    },
    {
      "id": 175010,
      "typeId": 13828,
      "title": "Lunch Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "4",
      "source": "CSV",
      "trackId": 13126,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175025
      ],
      "authors": []
    },
    {
      "id": 175011,
      "typeId": 13830,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "5",
      "source": "CSV",
      "trackId": 13127,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175020
      ],
      "authors": []
    },
    {
      "id": 175012,
      "typeId": 13784,
      "title": "Gather at Bus Pick-Up Location",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "6",
      "source": "CSV",
      "trackId": 13128,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175023
      ],
      "authors": []
    },
    {
      "id": 175013,
      "typeId": 13784,
      "title": "Wine Cellar Tour & Dinner",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "7",
      "source": "CSV",
      "trackId": 13129,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175031
      ],
      "authors": []
    },
    {
      "id": 175014,
      "typeId": 13831,
      "title": "Registration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "8",
      "source": "CSV",
      "trackId": 13130,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175029
      ],
      "authors": []
    },
    {
      "id": 175015,
      "typeId": 13830,
      "title": "Coffee Break (incl. Poster Setup)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "9",
      "source": "CSV",
      "trackId": 13131,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175022
      ],
      "authors": []
    },
    {
      "id": 175016,
      "typeId": 13827,
      "title": "Lunch Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "10",
      "source": "CSV",
      "trackId": 13132,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175026
      ],
      "authors": []
    },
    {
      "id": 175032,
      "typeId": 13834,
      "title": "Joint Keynote (SUI & VRST)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "16",
      "source": "CSV",
      "trackId": 13133,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        175033
      ],
      "authors": []
    }
  ],
  "people": [
    {
      "id": 174721,
      "firstName": "Christoph",
      "lastName": "Borst",
      "middleInitial": "W",
      "importedId": "M8Y4nRtYSeSlgBIX-wY4FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174722,
      "firstName": "Wenge",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "W6PiX1e31UCewBthWe540Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174723,
      "firstName": "Minho",
      "lastName": "Chung",
      "middleInitial": "",
      "importedId": "N4FrajigPhGL_05GV_4ztw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174724,
      "firstName": "Eric",
      "lastName": "DeMarbre",
      "middleInitial": "",
      "importedId": "2BnDy3_5ZSmx7P_E-M4rdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174725,
      "firstName": "Jonathan",
      "lastName": "Wieland",
      "middleInitial": "",
      "importedId": "SrDwt8DyP_6DbsGJnsWs9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174726,
      "firstName": "Eva",
      "lastName": "Hornecker",
      "middleInitial": "",
      "importedId": "f1FCmv1ffkE8IC4qXcwyuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174727,
      "firstName": "Inoussa",
      "lastName": "OUEDRAOGO",
      "middleInitial": "",
      "importedId": "6QxJgBnGiqKtD0IG-F6sEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174728,
      "firstName": "Stephen",
      "lastName": "DiVerdi",
      "middleInitial": "",
      "importedId": "4xl52Mh6Tk2ffvRTUO7LOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174729,
      "firstName": "Susanne",
      "lastName": "Schmidt",
      "middleInitial": "",
      "importedId": "nty9T-FExBWpAOmqzwYWjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174730,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "importedId": "ob1mmWPesH21Yfh_UxGqsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174731,
      "firstName": "Colin",
      "lastName": "Brett",
      "middleInitial": "A.",
      "importedId": "lkPxdN5rfz6yCwJ0NEvqXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174732,
      "firstName": "Renée",
      "lastName": "Venema",
      "middleInitial": "",
      "importedId": "Z8zWNfnfX2ijdT4O_oA0gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174733,
      "firstName": "Greg",
      "lastName": "Welch",
      "middleInitial": "",
      "importedId": "lkkZ7KrKLEQK6RLXPh4x6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174734,
      "firstName": "Lisa",
      "lastName": "Prinz",
      "middleInitial": "Marie",
      "importedId": "MQaOhZzSqKg2ZX3R-foZUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174735,
      "firstName": "Daniel",
      "lastName": "Keim",
      "middleInitial": "",
      "importedId": "JFTbBcXPXDKm76l0n4qJlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174736,
      "firstName": "Kousei",
      "lastName": "Nagayama",
      "middleInitial": "",
      "importedId": "CUj4N4TBoNvj_8zLtyW3nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174737,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "middleInitial": "",
      "importedId": "oLvfjZm0xHwl7reMcJOYcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174738,
      "firstName": "Cory",
      "lastName": "Ilo",
      "middleInitial": "",
      "importedId": "URdPJF5D3mOjn0JbEggptA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174739,
      "firstName": "Frank",
      "lastName": "Steinicke",
      "middleInitial": "",
      "importedId": "w2YEfsGpoTqbpROww8WcOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174740,
      "firstName": "Adrian",
      "lastName": "Clark",
      "middleInitial": "James",
      "importedId": "zmtSwCuQsRx4T5xlS-29Rw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174741,
      "firstName": "Carl",
      "lastName": "Gutwin",
      "middleInitial": "",
      "importedId": "RO5QJHzbRVEdIOe2THdnIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174742,
      "firstName": "Huaishu",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "cISP1gspzEJZEfcOkN6sIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174743,
      "firstName": "Zubin",
      "lastName": "Datta Choudhary",
      "middleInitial": "",
      "importedId": "4kICmQRekR3J6aZH0SJAlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174744,
      "firstName": "Katharina",
      "lastName": "Pöhlmann",
      "middleInitial": "Margareta Theresa",
      "importedId": "6XgZl_EajTtLW3SuB2FxAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174745,
      "firstName": "Javier Alejandro",
      "lastName": "Jaquez Lora",
      "middleInitial": "",
      "importedId": "zHZVcbElxPvaqlr1SrUKSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174746,
      "firstName": "Victoria",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "K8wnaSN7RjHb7jZ3QnLA1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174747,
      "firstName": "Satyugjit",
      "lastName": "Virk",
      "middleInitial": "",
      "importedId": "MHe1I2qB8a2vk0UlWC5Nsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174748,
      "firstName": "Lloyd",
      "lastName": "Waugh",
      "middleInitial": "",
      "importedId": "aCDHG-MVGGTPNLM0sBfVUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174749,
      "firstName": "Patrick",
      "lastName": "Bourdot",
      "middleInitial": "",
      "importedId": "W6TFUKBczgZtcou4wzykLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174750,
      "firstName": "Augusto",
      "lastName": "Esteves",
      "middleInitial": "",
      "importedId": "7f2iQ_8H8O1G87Aj2KDagw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174751,
      "firstName": "Xinan",
      "lastName": "Yan",
      "middleInitial": "",
      "importedId": "eRp2WJDmVANS33Cu2AHenQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174752,
      "firstName": "Scott",
      "lastName": "Bateman",
      "middleInitial": "",
      "importedId": "QL2eMuGvs5t0tzMsPt35LQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174753,
      "firstName": "Eric",
      "lastName": "Schoenborn",
      "middleInitial": "Cade",
      "importedId": "Oae_CWHD6PQGntO5I4Z8BA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174754,
      "firstName": "Lucas",
      "lastName": "Joos",
      "middleInitial": "",
      "importedId": "rfgxbp9OCFUH1zFENSSv_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174755,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "middleInitial": "",
      "importedId": "NrpIHYjQBiGY3BcwqIdH0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174756,
      "firstName": "Jay",
      "lastName": "Henderson",
      "middleInitial": "",
      "importedId": "VTCOtUSYyPJgRrNbe0bXcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174757,
      "firstName": "Robbe",
      "lastName": "Cools",
      "middleInitial": "",
      "importedId": "Yv1BovThwzMR4S9cQdQFwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174758,
      "firstName": "Pouya",
      "lastName": "Pournasir",
      "middleInitial": "",
      "importedId": "O3QSc-NBBMLOENyAve4xkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174759,
      "firstName": "Haerim",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "eDFaLyrV4GSVF0e_pipaBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174760,
      "firstName": "Jan",
      "lastName": "Ehlers",
      "middleInitial": "",
      "importedId": "szhhtjBZoN6GSDsT0Ivk0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174761,
      "firstName": "Harald",
      "lastName": "Reiterer",
      "middleInitial": "",
      "importedId": "nhpL_dT_DvMypkFF8KWU_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174762,
      "firstName": "Ian",
      "lastName": "Smith",
      "middleInitial": "",
      "importedId": "iVQCcI3td2rkTB1BpF0Gcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174763,
      "firstName": "Zhen",
      "lastName": "Lei",
      "middleInitial": "",
      "importedId": "WIoMWfvbaZAnVKW7Hlta0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174764,
      "firstName": "Adrian",
      "lastName": "Sarbach",
      "middleInitial": "",
      "importedId": "cwdxjjqgDbSwvnW7yhFGQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174765,
      "firstName": "Patrick",
      "lastName": "Haffner",
      "middleInitial": "",
      "importedId": "7e9U1Gh1qXGisH_CTS8QYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174766,
      "firstName": "Laura",
      "lastName": "Simon",
      "middleInitial": "",
      "importedId": "f_d0bMJoTwx2ABYIlD9S0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174767,
      "firstName": "Cherelle",
      "lastName": "Connor",
      "middleInitial": "",
      "importedId": "XM2FQh6WB4yi3BOQRXTKUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174768,
      "firstName": "Cameron",
      "lastName": "Moore",
      "middleInitial": "",
      "importedId": "s9vP0k5Ehz8nywO2qAdojw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174769,
      "firstName": "Eunchae",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "dmtT_f0UTgdAgy3-8ZYmqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174770,
      "firstName": "Jenny",
      "lastName": "Gabel",
      "middleInitial": "",
      "importedId": "dFoECjaoOFc5pkfaC3FQTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174771,
      "firstName": "Anton",
      "lastName": "Lammert",
      "middleInitial": "Benjamin",
      "importedId": "4L4-HGv2afqJmlVhpq63oQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174772,
      "firstName": "Wilko",
      "lastName": "Heuten",
      "middleInitial": "",
      "importedId": "Ygh9bEaN9mZKSF3jc-kNAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174773,
      "firstName": "Daniel",
      "lastName": "Rea",
      "middleInitial": "J.",
      "importedId": "tTLRUmONojKOxWEw8DpcKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174774,
      "firstName": "Huyen",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "UH8kKG-jMBE5KZaF0JM13w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174775,
      "firstName": "Daniel",
      "lastName": "Stover",
      "middleInitial": "",
      "importedId": "V6NTUi4s0dJnb7R9F298kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174776,
      "firstName": "Adalberto",
      "lastName": "Simeone",
      "middleInitial": "L.",
      "importedId": "I_JDJuvxoql-qJ3DHyV0SQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174777,
      "firstName": "Rajkumar",
      "lastName": "Darbar",
      "middleInitial": "",
      "importedId": "Eg_gLc-iEbJCKgYi96dDLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174778,
      "firstName": "Matthew",
      "lastName": "Lease",
      "middleInitial": "",
      "importedId": "Qckb_3KNrA8DgjMK10FG7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174779,
      "firstName": "Shupeng",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HbpFHQmbSrOpTJz4vnLqRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174780,
      "firstName": "Buntarou",
      "lastName": "Shizuki",
      "middleInitial": "",
      "importedId": "TJAWIctVjvJEKegv8Zj6KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174781,
      "firstName": "Uzay",
      "lastName": "Durdu",
      "middleInitial": "",
      "importedId": "37GByzZzvWXQxXX5HPh-5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174782,
      "firstName": "Maximilian",
      "lastName": "Fischer",
      "middleInitial": "T.",
      "importedId": "ytBnhclK-iMaiqberdxIMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174783,
      "firstName": "Gerd",
      "lastName": "Bruder",
      "middleInitial": "",
      "importedId": "tZQ7gLt4C6wd0tuTsCuv8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174784,
      "firstName": "Erran",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "5DPj7NHS7DzygQIb8hegsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174785,
      "firstName": "Alex",
      "lastName": "Williams",
      "middleInitial": "C",
      "importedId": "zinenbm-EstP9Dctp4syug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174786,
      "firstName": "Derek",
      "lastName": "Reilly",
      "middleInitial": "",
      "importedId": "O6WkTPxcYzluCAJZ9JFXLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174787,
      "firstName": "Martin",
      "lastName": "Raubal",
      "middleInitial": "",
      "importedId": "_md2mtWi6yb-3yB3XyIXZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174788,
      "firstName": "David",
      "lastName": "Broussard",
      "middleInitial": "Michael",
      "importedId": "j_vyAxq5u1Z9SN4jUkok6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174789,
      "firstName": "Sathaporn \"Hubert\"",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "KdZAW-_WiRRWuYdwmoeTaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174790,
      "firstName": "Yushi",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "JJYtyHv7IxnYffs-fW65_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174791,
      "firstName": "Hiroshi",
      "lastName": "Furuya",
      "middleInitial": "",
      "importedId": "KSKorSmO1AmFRwXebkZsXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174792,
      "firstName": "Yongjae",
      "lastName": "Yoo",
      "middleInitial": "",
      "importedId": "WZ0U-W1x8-8OYVoY5Wy_3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174793,
      "firstName": "Bernd",
      "lastName": "Froehlich",
      "middleInitial": "",
      "importedId": "ZrSKlpvVHOHGINAPpsT0bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174794,
      "firstName": "Wallace",
      "lastName": "Lages",
      "middleInitial": "Santos",
      "importedId": "EBpxxge7kxsOcJcZyOJsgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174795,
      "firstName": "Myungguen",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "V25s4iNy8gqfNrB_V6LQMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174796,
      "firstName": "Mac",
      "lastName": "Greenslade",
      "middleInitial": "",
      "importedId": "i1QD6IZlKI4929yovL7flg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174797,
      "firstName": "Stephan",
      "lastName": "Lukosch",
      "middleInitial": "",
      "importedId": "cdVi3RFjgfbhht_Pm4NJag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174798,
      "firstName": "Doug",
      "lastName": "Bowman",
      "middleInitial": "",
      "importedId": "vt2L_KgrYMbRY_hbo_B1rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174799,
      "firstName": "Jannike",
      "lastName": "Illing",
      "middleInitial": "",
      "importedId": "OdLNWdOZNF3UMpz7OioSyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174800,
      "firstName": "Subramanian",
      "lastName": "Chidambaram",
      "middleInitial": "",
      "importedId": "M-h_QSnBuXtPkc-N18lLBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174801,
      "firstName": "Robert",
      "lastName": "Teather",
      "middleInitial": "J",
      "importedId": "pRK54b4NGsl7VVxnIqf5Vg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174802,
      "firstName": "oyindolapo",
      "lastName": "komolafe",
      "middleInitial": "OLABISI",
      "importedId": "FkYffk65AR1HKoBZJTSL0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174803,
      "firstName": "Daniel",
      "lastName": "Zielasko",
      "middleInitial": "",
      "importedId": "A3jicF6_24m1PAN3FScFrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174804,
      "firstName": "Ge",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "FWLlgLqOMq2xmLYJTe-zfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174805,
      "firstName": "Thiago",
      "lastName": "Malheiros Porcino",
      "middleInitial": "",
      "importedId": "ct1qq87_X-Pn1TVx_vne9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174806,
      "firstName": "Book",
      "lastName": "Sadprasid",
      "middleInitial": "",
      "importedId": "Qg34YN28n7Prh0L6tD487w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174807,
      "firstName": "Erik",
      "lastName": "Scheme",
      "middleInitial": "J",
      "importedId": "aFlV-dzrZOYbUoRWUBNQKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174808,
      "firstName": "Sayan",
      "lastName": "Sarcar",
      "middleInitial": "",
      "importedId": "OsqrQn04FEjYmh8m4cLKWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174809,
      "firstName": "Min",
      "lastName": "Bai",
      "middleInitial": "",
      "importedId": "u3m25pb0dFMgY7W9BegXrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174810,
      "firstName": "Stephen",
      "lastName": "Brewster",
      "middleInitial": "Anthony",
      "importedId": "5oR5tpEfy7_GSFNHxRxKQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174811,
      "firstName": "Xuesong",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "u52w4RXpiMx4cCypKR9u4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174812,
      "firstName": "Zhanyan",
      "lastName": "Qiu",
      "middleInitial": "",
      "importedId": "SPkUjID3EA_bXOK5qKpSbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174813,
      "firstName": "Lina",
      "lastName": "Klass",
      "middleInitial": "",
      "importedId": "g1tnoIizTvIz69piIsszaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174814,
      "firstName": "Marko",
      "lastName": "Ritter",
      "middleInitial": "",
      "importedId": "d_C6jow23F2oDvN_iIBsRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174815,
      "firstName": "Yasas Sri",
      "lastName": "Wickramasinghe",
      "middleInitial": "",
      "importedId": "mZbglHppXrodWIoQyADBCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174816,
      "firstName": "Yuki",
      "lastName": "Takeyama",
      "middleInitial": "",
      "importedId": "gbBDSnGh2l3Ndz5HCOIcJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174817,
      "firstName": "Xuning",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "njrbKFBQgs8LEpB3o_0ZRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174818,
      "firstName": "Tintu",
      "lastName": "Mathew",
      "middleInitial": "",
      "importedId": "3FtDL48XC5S6-owx_UD9HQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174819,
      "firstName": "Laura",
      "lastName": "Battistel",
      "middleInitial": "",
      "importedId": "6coc1grsHxUc-Gcy_g-gLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174820,
      "firstName": "Hai-Ning",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "GSX8DqeKtRcdDxphGPXHxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174821,
      "firstName": "Johannes",
      "lastName": "Fuchs",
      "middleInitial": "",
      "importedId": "MxGf7c-6oGazlmDcwbaFfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174822,
      "firstName": "Kongmeng",
      "lastName": "Liew",
      "middleInitial": "",
      "importedId": "VhTl8KA8AB0GtjLM0bv-oQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174823,
      "firstName": "Matt",
      "lastName": "Gottsacker",
      "middleInitial": "",
      "importedId": "2DvYg340wgRhBzjPWqP5RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174851,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "importedId": "2Guk0EWER7zUs-hWrmCSsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174852,
      "firstName": "Leo",
      "lastName": "Shimoda",
      "middleInitial": "",
      "importedId": "vJ6XhQJTgJkTtOjA_D0Jbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174853,
      "firstName": "Naoki",
      "lastName": "Kunieda",
      "middleInitial": "",
      "importedId": "w6yMAUlMb_K6t4l-EWueLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174854,
      "firstName": "Julia",
      "lastName": "Hertel",
      "middleInitial": "",
      "importedId": "KepUoRxfP-6c4rFr_KWnVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174855,
      "firstName": "Jonathan Heinrich",
      "lastName": "Beierle",
      "middleInitial": "",
      "importedId": "NRdmEwOP9rwTJiruEBQH8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174856,
      "firstName": "Satoshi",
      "lastName": "Fukumori",
      "middleInitial": "",
      "importedId": "NrIyrCWJAxBiAgY0TKci1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174857,
      "firstName": "Solmaz",
      "lastName": "Goodarzi",
      "middleInitial": "",
      "importedId": "6FVn0E78GPB4u69n7-TynA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174858,
      "firstName": "Michiya",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "importedId": "t4Ihlo6LPMiXqstSJh0Uzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174859,
      "firstName": "Florian",
      "lastName": "Daiber",
      "middleInitial": "",
      "importedId": "V7xTeq008LZvqi1nDsfEZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174860,
      "firstName": "Christoph",
      "lastName": "Tieben",
      "middleInitial": "",
      "importedId": "UtOPZwp19UghnMGsCx8D7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174861,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "middleInitial": "",
      "importedId": "GPbdshGJqVFsnL-tpVbWZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174862,
      "firstName": "Kenjiro",
      "lastName": "Okada",
      "middleInitial": "",
      "importedId": "p5QJiT5tga0BEvXtM1i7QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174863,
      "firstName": "Muhammad Moiz",
      "lastName": "Sakha",
      "middleInitial": "",
      "importedId": "FmxFiS3KF9ARRPSwtER8eg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174864,
      "firstName": "Wanhui",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "dorqXTsYOVHnG5CBZGSmaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174865,
      "firstName": "Gabriel",
      "lastName": "Lipkowitz",
      "middleInitial": "",
      "importedId": "hsKOI7-Wl5vZVlVFUuIJkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174866,
      "firstName": "Saizo",
      "lastName": "Aoyagi",
      "middleInitial": "",
      "importedId": "u_9AkAVjK-bxV8vXSyU5wA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174867,
      "firstName": "Matthias",
      "lastName": "Enders",
      "middleInitial": "",
      "importedId": "hn5Lx87aIKMupcFUKuhGLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174868,
      "firstName": "Tim",
      "lastName": "Weissker",
      "middleInitial": "",
      "importedId": "j20hHOW2RY54EBAd1fWzgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174869,
      "firstName": "Hiroyuki",
      "lastName": "Abutani",
      "middleInitial": "",
      "importedId": "b1_ynjs96y3SorFttr0EnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174870,
      "firstName": "Torsten",
      "lastName": "Kuhlen",
      "middleInitial": "Wolfgang",
      "importedId": "X04CYKYDVKKapfPiPeGdsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174871,
      "firstName": "Bo",
      "lastName": "Hui",
      "middleInitial": "",
      "importedId": "m0N7Kf0N5iktDjTUtpE78A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174872,
      "firstName": "Qing",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "Q6BVkFDfzqdL3mLrZqrBIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174873,
      "firstName": "Takuto",
      "lastName": "Nakamura",
      "middleInitial": "",
      "importedId": "bNfAZvqyGjS4a4WZsKSsrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174874,
      "firstName": "Sebastian",
      "lastName": "Pape",
      "middleInitial": "",
      "importedId": "Tbj03rOfoCrO7Aud7pQM7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174884,
      "firstName": "Toru",
      "lastName": "Abe",
      "middleInitial": "",
      "importedId": "ilCPV1Op9HkRXH3rmPtjTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174885,
      "firstName": "Chen",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "HQ7hzUOZ7bKMIfH3MH70-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174886,
      "firstName": "Antonio",
      "lastName": "Krüger",
      "middleInitial": "",
      "importedId": "7BVGaBoskVivG5iBzHVi8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174887,
      "firstName": "Hideaki",
      "lastName": "Uchiyama",
      "middleInitial": "",
      "importedId": "olAN1SqMZ-mNqKD0YU0EjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174888,
      "firstName": "Hiromu",
      "lastName": "Otsubo",
      "middleInitial": "",
      "importedId": "TGRGnCChOHKwPNQwB2OzfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174889,
      "firstName": "Florian",
      "lastName": "Rademaker",
      "middleInitial": "",
      "importedId": "oR_RQiK1De8_GEwg5NiVlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174890,
      "firstName": "Ruishen",
      "lastName": "Zheng",
      "middleInitial": "",
      "importedId": "zEFqFo5laMl-h86bDR4myA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174891,
      "firstName": "Marvin",
      "lastName": "Lehnort",
      "middleInitial": "",
      "importedId": "mmX3270AlTKOPqTiorvxhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174892,
      "firstName": "Mak",
      "lastName": "Krvavac",
      "middleInitial": "",
      "importedId": "2WCaieKSMKXIk1ouKQ5Wkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174893,
      "firstName": "Melissa",
      "lastName": "Steininger",
      "middleInitial": "",
      "importedId": "2dSIPmPm12Jm4bEQQ5CeTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174894,
      "firstName": "Arinobu",
      "lastName": "Niijima",
      "middleInitial": "",
      "importedId": "1hXdXCGzoscaaHtE_sgpbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174895,
      "firstName": "Takefumi",
      "lastName": "Ogawa",
      "middleInitial": "",
      "importedId": "xqqIRrOeTICQPcpViSNvHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174896,
      "firstName": "Matthias",
      "lastName": "Böhmer",
      "middleInitial": "",
      "importedId": "dI6rSgSnFLE8QkDz59Hliw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174897,
      "firstName": "Johannes",
      "lastName": "Waltmann",
      "middleInitial": "",
      "importedId": "oRD6mP1Y4X8tWiluH5I8NQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174898,
      "firstName": "Monica",
      "lastName": "Perusquia-Hernandez",
      "middleInitial": "",
      "importedId": "GZ1r7eyqLjexGA4_p7AYfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174899,
      "firstName": "Christian",
      "lastName": "Gall",
      "middleInitial": "",
      "importedId": "FVEMlbCx2x379ROH0esQGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174900,
      "firstName": "Suchun",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "yk9Tkuqau4DmAQrvGroIhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174901,
      "firstName": "Yuki",
      "lastName": "Hamaguchi",
      "middleInitial": "",
      "importedId": "pRUGOJ5ydJVVu8YbB4J9xA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174902,
      "firstName": "László",
      "lastName": "Kopácsi",
      "middleInitial": "",
      "importedId": "CTXU2FnWF-xpeKk-TP0Q6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174903,
      "firstName": "Salvatore",
      "lastName": "Sapienza",
      "middleInitial": "",
      "importedId": "CmW-rlq1Q0dQ1-3lReyaSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174904,
      "firstName": "Minghui",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "pYCcyJT__LIFnYuc7_xzxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174905,
      "firstName": "Donald",
      "lastName": "Degraen",
      "middleInitial": "",
      "importedId": "wCv-aRujLDWpCOAMkRiA0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174906,
      "firstName": "Qinyang",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "HYZGNt42lhIEM4pTL1MpVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174907,
      "firstName": "Anton",
      "lastName": "Lux",
      "middleInitial": "",
      "importedId": "jUui683-8l150sdL4DE_MA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174908,
      "firstName": "Fariba",
      "lastName": "Mostajeran",
      "middleInitial": "",
      "importedId": "wRobmt2RbvU0QwRZonf7ZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174909,
      "firstName": "Sara",
      "lastName": "Brucker",
      "middleInitial": "",
      "importedId": "RyuH5z9KmdtECnDmZskqsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174910,
      "firstName": "Daniel",
      "lastName": "Sonntag",
      "middleInitial": "",
      "importedId": "j13cRyEGzXiAR8ZvE_auKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174911,
      "firstName": "Ernst",
      "lastName": "Kruijff",
      "middleInitial": "",
      "importedId": "1ImzhVT49igpl263mFJeaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174912,
      "firstName": "Alon",
      "lastName": "Rosenbaum",
      "middleInitial": "",
      "importedId": "tAabWGPG0ENt5nktP-E5EQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174913,
      "firstName": "Yota",
      "lastName": "Takahara",
      "middleInitial": "",
      "importedId": "HB_ODQf0jwpWVKWBCqzZdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174914,
      "firstName": "Yirui",
      "lastName": "Zuo",
      "middleInitial": "",
      "importedId": "4SfQSbaghUZrQaih3qFksQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174915,
      "firstName": "Henrik",
      "lastName": "von Coler",
      "middleInitial": "",
      "importedId": "Rpz9wCO5gkxeSIIrPaFP9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174916,
      "firstName": "Monica",
      "lastName": "Palmirani",
      "middleInitial": "",
      "importedId": "lWpwRcBEBRxzEf4ntyprsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174917,
      "firstName": "Jiyeong",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "wjgp5P09155kpHWKGww9IA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174918,
      "firstName": "Vimal Darius",
      "lastName": "Seetohul",
      "middleInitial": "",
      "importedId": "uFbJEQ30pzYAU395mJgdRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174919,
      "firstName": "Albert",
      "lastName": "Klimenko",
      "middleInitial": "",
      "importedId": "tbqj6ibdcbBqn28sSSKpNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174920,
      "firstName": "Naz",
      "lastName": "Al Kassm",
      "middleInitial": "Mokhamed",
      "importedId": "9c2z3Bg2_9OtjJFj-1aYBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174921,
      "firstName": "Yu’an",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "BoKl5XHsiHO40JaBtruKlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174922,
      "firstName": "Michal",
      "lastName": "Rinott",
      "middleInitial": "",
      "importedId": "PmUqXqg9qrflZhaSVnc0iA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174923,
      "firstName": "Nick",
      "lastName": "Wittig",
      "middleInitial": "",
      "importedId": "HOR4tRUhEEh_JulLnZW6WA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174924,
      "firstName": "Giuseppe",
      "lastName": "Di Maria",
      "middleInitial": "",
      "importedId": "c7ZCvnKnG0RJqEStQz6DoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174925,
      "firstName": "Pasquale",
      "lastName": "Cascarano",
      "middleInitial": "",
      "importedId": "x-ZO6LKhx59PF8lqQ9eOMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174926,
      "firstName": "Alexander",
      "lastName": "Marquardt",
      "middleInitial": "",
      "importedId": "gVINTWLixpmSYkK6nW6uDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174927,
      "firstName": "Marco",
      "lastName": "Speicher",
      "middleInitial": "",
      "importedId": "cZgVpqupprcGrD2d1OioHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174928,
      "firstName": "Sven",
      "lastName": "Bertel",
      "middleInitial": "",
      "importedId": "z0Ey53-0oWMQqQfLiIHYcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174929,
      "firstName": "Gustavo",
      "lastName": "Marfia",
      "middleInitial": "",
      "importedId": "ju027Xou6h2U2SowuD4SVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174930,
      "firstName": "Sota",
      "lastName": "Fujiwara",
      "middleInitial": "",
      "importedId": "jxGORizR4GGb8rxpJ-ZC5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174931,
      "firstName": "Victoria",
      "lastName": "Henze",
      "middleInitial": "",
      "importedId": "jqPvKtJy3r2MNsbutYpNLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174932,
      "firstName": "Michael",
      "lastName": "Barz",
      "middleInitial": "",
      "importedId": "WsChlJFPL7ZlsvnGI05r6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174933,
      "firstName": "Kevin",
      "lastName": "Heuer",
      "middleInitial": "",
      "importedId": "l4_RriHgKcu15a-MU6zfnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174934,
      "firstName": "Paula",
      "lastName": "López",
      "middleInitial": "",
      "importedId": "WLKGrNXQ1pFsMWGGJsEUfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174935,
      "firstName": "Amali",
      "lastName": "Seneviratne",
      "middleInitial": "",
      "importedId": "ki2stWbVRSk80y2crNjZrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174936,
      "firstName": "Shamima",
      "lastName": "Yasmin",
      "middleInitial": "",
      "importedId": "mK7zifQGoMmqXtLUyY2ulw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174937,
      "firstName": "Markus",
      "lastName": "Hahn",
      "middleInitial": "",
      "importedId": "MBMnxIvulz1PYxD-LxHPNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174938,
      "firstName": "Leah",
      "lastName": "Knaack",
      "middleInitial": "",
      "importedId": "9_Qch5QDB92hqtnZUGnmKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174939,
      "firstName": "André",
      "lastName": "Zenner",
      "middleInitial": "",
      "importedId": "Y1e1zISAmrQLhVJEP-fAhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174940,
      "firstName": "David",
      "lastName": "Petersen",
      "middleInitial": "",
      "importedId": "3Je7chAlVZuR3-m-eMgsgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174941,
      "firstName": "Weiwei",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "8WziwpFGxWm2UdiWupkMXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174942,
      "firstName": "Birgitt",
      "lastName": "Schoenfisch",
      "middleInitial": "",
      "importedId": "mbgHHToL7CHVxd3u2MTyLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174943,
      "firstName": "Selin",
      "lastName": "Guergan",
      "middleInitial": "",
      "importedId": "NEA2ARlYtQ5UupTJaig5aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174944,
      "firstName": "Stefan",
      "lastName": "Schneegass",
      "middleInitial": "",
      "importedId": "QMo-7Iuectu0tMAaNm6edg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174945,
      "firstName": "Bethany",
      "lastName": "Growns",
      "middleInitial": "",
      "importedId": "afmyEKwRqWx2owGWEfV0FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174946,
      "firstName": "Gil Otis",
      "lastName": "Mends-Cole",
      "middleInitial": "",
      "importedId": "q4yaxCwCqXQbcZXc2ZqyUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174947,
      "firstName": "Kiyoshi",
      "lastName": "Kiyokawa",
      "middleInitial": "",
      "importedId": "ZAR0Zh2XgSbboh7cGkzI_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174948,
      "firstName": "Adil",
      "lastName": "Khokhar",
      "middleInitial": "",
      "importedId": "v8stbIqgSLt_HwHTcl9OWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174949,
      "firstName": "Max-Julien Gabriel",
      "lastName": "Teetz",
      "middleInitial": "",
      "importedId": "3ZHHSFfNVm7BTELdK3CKwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174950,
      "firstName": "Chanho",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "RZtXVoY_Irmd2-gkY-o-bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174951,
      "firstName": "Lucie",
      "lastName": "Kruse",
      "middleInitial": "",
      "importedId": "iTcdY0WvIkKo2I3IVyth3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174952,
      "firstName": "Kyungphil",
      "lastName": "Ryoo",
      "middleInitial": "",
      "importedId": "A5g4je9h7SQRHRFMVTudpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174953,
      "firstName": "Shirin",
      "lastName": "Hajahmadi",
      "middleInitial": "",
      "importedId": "QXZVOXeYtcLBJCkb-KYg_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174954,
      "firstName": "Jonas",
      "lastName": "Schild",
      "middleInitial": "",
      "importedId": "S5WOqalPEIP1xzoail7hQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174955,
      "firstName": "Felix",
      "lastName": "Dollack",
      "middleInitial": "",
      "importedId": "UDz1EvPZoGUm4PueCz55og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174956,
      "firstName": "Tsubasa",
      "lastName": "Otaki",
      "middleInitial": "",
      "importedId": "1Xt2LNgXW84beN9CVyKnQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174957,
      "firstName": "Kyoungwoo",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "z9TV1DHs41TWdg473t5Jtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174958,
      "firstName": "Lukas",
      "lastName": "Mayer",
      "middleInitial": "",
      "importedId": "RkS_ky-72kXn0gkyMrKrtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174959,
      "firstName": "Michael",
      "lastName": "Teistler",
      "middleInitial": "",
      "importedId": "cXThNfxniWs8zVVdxqFJHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174960,
      "firstName": "Seoyeon",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "nhx6KWm5MXzJQmGc6Onp0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174961,
      "firstName": "Akari",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "PHrJRM4LAHPp_dpB5jeFUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174962,
      "firstName": "Takuo",
      "lastName": "Suganuma",
      "middleInitial": "",
      "importedId": "ENW57TuqT0MmhTefn3stuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174963,
      "firstName": "Ruowen",
      "lastName": "Niu",
      "middleInitial": "",
      "importedId": "tmwI1FdqmhQaDD7C5DzXtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174964,
      "firstName": "Matthias",
      "lastName": "Suencksen",
      "middleInitial": "",
      "importedId": "7iNihUcRcwsx1QGJFiYBow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174965,
      "firstName": "Jan-Michel",
      "lastName": "Nöring",
      "middleInitial": "",
      "importedId": "s1lQkaNNe-js8g8VkBjE1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174966,
      "firstName": "Lukas",
      "lastName": "Glaser",
      "middleInitial": "",
      "importedId": "cM81BhPbdb-BYpzGLZfvFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 174967,
      "firstName": "Hyunkyung",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "xldbLuTMs36XWmaXTX8msQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 175041,
      "firstName": "Frank",
      "lastName": "Steinicke",
      "importedId": "1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 175042,
      "firstName": "Benjamin",
      "lastName": "Weyers",
      "importedId": "2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 175043,
      "firstName": "Stephan",
      "lastName": "Lukosch",
      "importedId": "3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 175044,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "importedId": "4",
      "source": "CSV",
      "affiliations": []
    }
  ],
  "recognitions": []
}