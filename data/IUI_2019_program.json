{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10023,
    "startDate": 1552694400000,
    "endDate": 1553040000000,
    "shortName": "IUI",
    "name": "IUI 2019",
    "year": 2019,
    "fullName": "ACM IUI 2019, the 24th Intelligent User Interfaces Conference ",
    "url": "https://iui.acm.org/2019/",
    "location": "Los Angeles, USA",
    "timeZoneOffset": -420,
    "logoUrl": "https://files.sigchi.org/conference/logo/8ca2e99a-4627-2cb5-3804-b8468e4cebec.png",
    "timeZoneName": "America/Los_Angeles"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10018,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10166,
      "name": "Pacific I & II",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10167,
      "name": "Pacific III",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10168,
      "name": "Sierra I & II",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10169,
      "name": "Catalina",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10170,
      "name": "Malibu/Santa Monica",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10171,
      "name": "Palisades",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10172,
      "name": "Pacific II",
      "typeId": 10158,
      "setup": "Theatre"
    },
    {
      "id": 10173,
      "name": "Pacific I",
      "typeId": 10158,
      "setup": "Theatre"
    }
  ],
  "tracks": [
    {
      "id": 10073,
      "name": "IUI 2019 Papers",
      "typeId": 10158
    },
    {
      "id": 10072,
      "name": "IUI 2019 Student Volunteers",
      "typeId": 11296
    },
    {
      "id": 10074,
      "typeId": 10158
    },
    {
      "id": 10075
    }
  ],
  "contentTypes": [
    {
      "id": 10151,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 10152,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 10153,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 10155,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 10156,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 10157,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 10158,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 10159,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 10160,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 10154,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11296,
      "name": "Unused",
      "color": "#90979a",
      "duration": 0
    },
    {
      "id": 11297,
      "name": "Keynote",
      "color": "#90979a",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 10822,
      "type": "SESSION",
      "startDate": 1552813200000,
      "endDate": 1552815000000
    },
    {
      "id": 10823,
      "type": "SESSION",
      "startDate": 1552815000000,
      "endDate": 1552819500000
    },
    {
      "id": 10824,
      "type": "BREAK",
      "startDate": 1552819500000,
      "endDate": 1552821300000
    },
    {
      "id": 10825,
      "type": "SESSION",
      "startDate": 1552821300000,
      "endDate": 1552824900000
    },
    {
      "id": 10826,
      "type": "LUNCH",
      "startDate": 1552824900000,
      "endDate": 1552831200000
    },
    {
      "id": 10827,
      "type": "SESSION",
      "startDate": 1552831200000,
      "endDate": 1552836600000
    },
    {
      "id": 10828,
      "type": "BREAK",
      "startDate": 1552836600000,
      "endDate": 1552838400000
    },
    {
      "id": 10829,
      "type": "SESSION",
      "startDate": 1552838400000,
      "endDate": 1552842000000
    },
    {
      "id": 10830,
      "type": "BREAK",
      "startDate": 1552842000000,
      "endDate": 1552842900000
    },
    {
      "id": 10831,
      "type": "SESSION",
      "startDate": 1552842900000,
      "endDate": 1552849200000
    },
    {
      "id": 10832,
      "type": "SESSION",
      "startDate": 1552897800000,
      "endDate": 1552902300000
    },
    {
      "id": 10833,
      "type": "BREAK",
      "startDate": 1552902300000,
      "endDate": 1552904100000
    },
    {
      "id": 10834,
      "type": "SESSION",
      "startDate": 1552904100000,
      "endDate": 1552908600000
    },
    {
      "id": 10835,
      "type": "LUNCH",
      "startDate": 1552908600000,
      "endDate": 1552914900000
    },
    {
      "id": 10836,
      "type": "SESSION",
      "startDate": 1552914900000,
      "endDate": 1552919700000
    },
    {
      "id": 10837,
      "type": "BREAK",
      "startDate": 1552919700000,
      "endDate": 1552921500000
    },
    {
      "id": 10838,
      "type": "SESSION",
      "startDate": 1552921500000,
      "endDate": 1552926000000
    },
    {
      "id": 10839,
      "type": "BREAK",
      "startDate": 1552926000000,
      "endDate": 1552926900000
    },
    {
      "id": 10840,
      "type": "SESSION",
      "startDate": 1552926900000,
      "endDate": 1552928400000
    },
    {
      "id": 10841,
      "type": "SESSION",
      "startDate": 1552928400000,
      "endDate": 1552930200000
    },
    {
      "id": 10842,
      "type": "SESSION",
      "startDate": 1552986000000,
      "endDate": 1552990500000
    },
    {
      "id": 10843,
      "type": "BREAK",
      "startDate": 1552990500000,
      "endDate": 1552992300000
    },
    {
      "id": 10845,
      "type": "LUNCH",
      "startDate": 1552995900000,
      "endDate": 1553002200000
    },
    {
      "id": 10846,
      "type": "SESSION",
      "startDate": 1553002200000,
      "endDate": 1553007000000
    },
    {
      "id": 10847,
      "type": "BREAK",
      "startDate": 1553007000000,
      "endDate": 1553008800000
    },
    {
      "id": 10848,
      "type": "SESSION",
      "startDate": 1553008800000,
      "endDate": 1553013600000
    },
    {
      "id": 10849,
      "type": "BREAK",
      "startDate": 1553013600000,
      "endDate": 1553015400000
    },
    {
      "id": 10850,
      "type": "SESSION",
      "startDate": 1553015400000,
      "endDate": 1553019000000
    },
    {
      "id": 10851,
      "type": "SESSION",
      "startDate": 1553072400000,
      "endDate": 1553077800000
    },
    {
      "id": 10852,
      "type": "BREAK",
      "startDate": 1553077800000,
      "endDate": 1553079600000
    },
    {
      "id": 10853,
      "type": "SESSION",
      "startDate": 1553079600000,
      "endDate": 1553085000000
    },
    {
      "id": 10854,
      "type": "LUNCH",
      "startDate": 1553085000000,
      "endDate": 1553090400000
    },
    {
      "id": 10855,
      "type": "SESSION",
      "startDate": 1553090400000,
      "endDate": 1553095800000
    },
    {
      "id": 10856,
      "type": "BREAK",
      "startDate": 1553095800000,
      "endDate": 1553097600000
    },
    {
      "id": 10857,
      "type": "SESSION",
      "startDate": 1553097600000,
      "endDate": 1553103000000
    },
    {
      "id": 10844,
      "type": "SESSION",
      "startDate": 1552992300000,
      "endDate": 1552995900000
    }
  ],
  "sessions": [
    {
      "id": 1318,
      "name": "Interactive Machine Learning II ",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        3893,
        5194,
        4660,
        6261
      ],
      "timeSlotId": 10848
    },
    {
      "id": 1357,
      "name": "Keynote DARPA’s Explainable Artificial Intelligence (XAI) Program",
      "typeId": 11297,
      "roomId": 10168,
      "chairIds": [
        9237
      ],
      "contentIds": [
        7171
      ],
      "timeSlotId": 10832
    },
    {
      "id": 1002,
      "name": "Keynote Innovating with AI",
      "typeId": 11297,
      "roomId": 10168,
      "chairIds": [
        16053
      ],
      "contentIds": [
        4329
      ],
      "timeSlotId": 10842
    },
    {
      "id": 1356,
      "name": "Interactive Machine Learning I",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        6197,
        4698,
        2684,
        4685
      ],
      "timeSlotId": 10846
    },
    {
      "id": 1823,
      "name": "Keynote Getting Virtually Personal: Making Responsible and Empathetic “Her” for Everyone",
      "typeId": 11297,
      "roomId": 10168,
      "chairIds": [
        25026
      ],
      "contentIds": [
        7532
      ],
      "timeSlotId": 10823
    },
    {
      "id": 1571,
      "name": "Assistive IUIs",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        4200,
        4694,
        7448
      ],
      "timeSlotId": 10827
    },
    {
      "id": 1038,
      "name": "Automated Driving",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        6915,
        3345,
        6286,
        4134
      ],
      "timeSlotId": 10829
    },
    {
      "id": 1288,
      "name": "Agent-based IUIs",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        5726,
        7651,
        8086,
        5452,
        6856
      ],
      "timeSlotId": 10827
    },
    {
      "id": 1465,
      "name": "Multi-Modal Interfaces & Experience Transfer",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        3175,
        7655,
        8034
      ],
      "timeSlotId": 10829
    },
    {
      "id": 2490,
      "name": "User-Adaptive IUIs",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        3089,
        3853,
        2836,
        5956
      ],
      "timeSlotId": 10836
    },
    {
      "id": 1417,
      "name": "Recommender Systems",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        5101,
        5302,
        3677,
        7952
      ],
      "timeSlotId": 10838
    },
    {
      "id": 2143,
      "name": "Intelligent Visualization",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        5734,
        6611,
        5815,
        6203
      ],
      "timeSlotId": 10846
    },
    {
      "id": 1078,
      "name": "Trust in Automation",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        7245,
        4862,
        6229,
        3491
      ],
      "timeSlotId": 10836
    },
    {
      "id": 1355,
      "name": "IUIs for Wearable and Mobile",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        6182,
        4630,
        6024,
        5470
      ],
      "timeSlotId": 10838
    },
    {
      "id": 2305,
      "name": "Augmented and Mixed Reality",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        3838,
        3087,
        5978,
        3006
      ],
      "timeSlotId": 10846
    },
    {
      "id": 2366,
      "name": "Evaluation of IUIs",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        5419,
        5217,
        4243,
        4785
      ],
      "timeSlotId": 10848
    },
    {
      "id": 2102,
      "name": "Big Data and Analytics",
      "typeId": 10158,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [
        4582,
        5004,
        6586
      ],
      "timeSlotId": 10850
    },
    {
      "id": 2043,
      "name": "Explanations in Recommender Systems",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        5711,
        4853,
        3446,
        4383
      ],
      "timeSlotId": 10836
    },
    {
      "id": 2185,
      "name": "Affective and Aesthetic IUI",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        7831,
        3588,
        7338,
        6274
      ],
      "timeSlotId": 10838
    },
    {
      "id": 2013,
      "name": "Natural Language and Speech",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        7162,
        7587,
        5559
      ],
      "timeSlotId": 10850
    },
    {
      "id": 1407,
      "name": "Explainable AI II",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        2792,
        4540,
        8024
      ],
      "timeSlotId": 10829
    },
    {
      "id": 2174,
      "name": "Explainable AI I",
      "typeId": 10158,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [
        3207,
        4386,
        3464,
        6807,
        5418
      ],
      "timeSlotId": 10827
    },
    {
      "id": 2477,
      "name": "Collaborative Interfaces I",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        7924,
        3496,
        6780,
        2741
      ],
      "timeSlotId": 10848
    },
    {
      "id": 1365,
      "name": "Collaborative Interfaces II",
      "typeId": 10158,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [
        4531,
        6792,
        2794
      ],
      "timeSlotId": 10850
    },
    {
      "id": 2211,
      "name": "Session",
      "chairIds": [],
      "contentIds": []
    }
  ],
  "events": [
    {
      "id": 2615,
      "name": "Panel Hype or Hope: AI Meets Humans in the Real World",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552992300000,
      "endDate": 1552995900000,
      "description": "During the past few years, AI has been resurrected and become the \"darling\" subject of research and development in Computer Science. On the one hand, startups, enterprises, and academia alike have invested heavily in AI research and applications. On the ot",
      "presenterIds": [
        19666,
        14643,
        21271,
        19754
      ]
    },
    {
      "id": 2552,
      "name": "Panel Fostering Women Entrepreneurship",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552821300000,
      "endDate": 1552824900000,
      "description": "Starting one's own venture not only is gutsy but also requires discipline and resourcefulness. In this panel, several entrepreneurs and an NSF program director whose focus is to foster the U.S. entrepreneurial ecosystem will share their experience and guid",
      "presenterIds": [
        25026,
        19666,
        14643,
        23355
      ]
    },
    {
      "id": 2554,
      "name": "Reception",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552755600000,
      "endDate": 1552770000000,
      "location": "at the University of Southern California, Information Sciences Institute",
      "presenterIds": []
    },
    {
      "id": 2601,
      "name": "Registration",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552807800000,
      "endDate": 1552813200000,
      "presenterIds": []
    },
    {
      "id": 2654,
      "name": "Welcome and Opening Plenary",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552813200000,
      "endDate": 1552815000000,
      "presenterIds": []
    },
    {
      "id": 2634,
      "name": "Student Consortium 2",
      "typeId": 10154,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552842900000,
      "endDate": 1552849200000,
      "presenterIds": []
    },
    {
      "id": 2583,
      "name": "Student Consortium 1",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552842900000,
      "endDate": 1552849200000,
      "presenterIds": []
    },
    {
      "id": 2594,
      "name": "W7 User Interfaces for Spatial-Temporal Data Analysis",
      "typeId": 10154,
      "roomId": 10167,
      "chairIds": [
        13179,
        21338,
        23344
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553083200000,
      "description": "Humanity generates many large and complex datasets that tend to be complex, heterogeneous (texts, images, videos, etc.), huge and are rapidly growing and changing over space and time dimensions. Hence, special, dedicated solutions for visualizing ...",
      "presenterIds": []
    },
    {
      "id": 2677,
      "name": "Registration",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552894200000,
      "endDate": 1552897800000,
      "presenterIds": []
    },
    {
      "id": 2633,
      "name": "W8 User-Aware Conversational Agents",
      "typeId": 10154,
      "roomId": 10169,
      "chairIds": [
        25053,
        23539,
        9040
      ],
      "contentIds": [],
      "startDate": 1553090400000,
      "endDate": 1553104800000,
      "description": "Conversational agent systems present an extremely rich and challenging research space for many topics of user awareness and adaptation, such as user profiles, contexts, personalities, emotions, social dynamics, conversational styles, etc. ",
      "presenterIds": []
    },
    {
      "id": 2562,
      "name": "Most Impactful paper panel ",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552904100000,
      "endDate": 1552908600000,
      "presenterIds": []
    },
    {
      "id": 2584,
      "name": "Poster Madness",
      "typeId": 10154,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552926900000,
      "endDate": 1552928400000,
      "presenterIds": []
    },
    {
      "id": 2644,
      "name": "Townhall Meeting ",
      "typeId": 10154,
      "roomId": 10166,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552928400000,
      "endDate": 1552930200000,
      "presenterIds": []
    },
    {
      "id": 2546,
      "name": "W4 Theory-Informed User Modeling for Tailoring and Personalizing Interfaces",
      "typeId": 10154,
      "roomId": 10171,
      "chairIds": [
        20947,
        10884,
        10294,
        9900
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553085000000,
      "description": "The HUMANIZE workshop aims to provide a venue for scholars to discover and discuss research findings on how to incorporate psychological theory into personalized interfaces. Personalization is often done through mining behavior data for patterns...",
      "presenterIds": []
    },
    {
      "id": 2660,
      "name": "W9 Intelligent User Interfaces for Algorithmic Transparency in Emerging Technologies",
      "typeId": 10154,
      "roomId": 10169,
      "chairIds": [
        19725,
        23975,
        12459,
        13222,
        22192
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553083200000,
      "description": "IUI ATEC’s goal is to focus on three principles that describe approaches to combating algorithmic biases that can be applied by researchers, even without access to a given system’s inter-workings: - Awareness: Raise stakeholders’ ...",
      "presenterIds": []
    },
    {
      "id": 2572,
      "name": "W5 Intelligent User Interfaces for Internet of Things",
      "typeId": 10154,
      "roomId": 10171,
      "chairIds": [
        14171,
        18876,
        17554,
        11169
      ],
      "contentIds": [],
      "startDate": 1553090400000,
      "endDate": 1553104800000,
      "description": "IoT technologies (e.g. smart homes, m-health, public tracking) are revolutionizing the way we interact with our environments. Designing interfaces for IoTs is challenging due to inherent nature of IoT which accommodates several devices in an environment. ",
      "presenterIds": []
    },
    {
      "id": 2557,
      "name": "Award Ceremony and Conference Banquet ",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1553020200000,
      "endDate": 1553029200000,
      "location": "Banquet Room",
      "presenterIds": []
    },
    {
      "id": 2636,
      "name": "Student Consortium 3 ",
      "typeId": 10154,
      "roomId": 10167,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552842900000,
      "endDate": 1552849200000,
      "presenterIds": []
    },
    {
      "id": 2578,
      "name": "Poster and Demo Reception",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552932900000,
      "endDate": 1552939200000,
      "presenterIds": []
    },
    {
      "id": 2540,
      "name": "Registration",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552980600000,
      "endDate": 1552986000000,
      "presenterIds": []
    },
    {
      "id": 2673,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552819500000,
      "endDate": 1552821300000,
      "presenterIds": []
    },
    {
      "id": 2539,
      "name": "Lunch (on your own) + Fostering Women Entrepreneurship Lunch",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552824900000,
      "endDate": 1552831200000,
      "location": "TBD",
      "presenterIds": []
    },
    {
      "id": 2664,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552836600000,
      "endDate": 1552838400000,
      "presenterIds": []
    },
    {
      "id": 2580,
      "name": "Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552842000000,
      "endDate": 1552842900000,
      "presenterIds": []
    },
    {
      "id": 2661,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552902300000,
      "endDate": 1552904100000,
      "presenterIds": []
    },
    {
      "id": 2630,
      "name": "Lunch (on your own) + Steering committee lunch",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552908600000,
      "endDate": 1552914900000,
      "location": "TBD",
      "presenterIds": []
    },
    {
      "id": 2596,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552919700000,
      "endDate": 1552921500000,
      "presenterIds": []
    },
    {
      "id": 2612,
      "name": "Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552926000000,
      "endDate": 1552926900000,
      "presenterIds": []
    },
    {
      "id": 2577,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552990500000,
      "endDate": 1552992300000,
      "presenterIds": []
    },
    {
      "id": 2608,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1553007000000,
      "endDate": 1553008800000,
      "presenterIds": []
    },
    {
      "id": 2586,
      "name": "Coffee Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1553013600000,
      "endDate": 1553015400000,
      "presenterIds": []
    },
    {
      "id": 2548,
      "name": "W6 User Interactions for Building Knowledge",
      "typeId": 10154,
      "roomId": 10170,
      "chairIds": [
        10457,
        12259,
        22662,
        22346,
        18276,
        23485
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553085000000,
      "description": "Building sets of complete, correct, and unbiased information – whether it is domain knowledge or training data – is an iterative and ongoing process that is necessary for producing systems that have the requisite knowledge to be effective in their environm",
      "presenterIds": []
    },
    {
      "id": 2623,
      "name": "Break",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552930200000,
      "endDate": 1552932900000,
      "presenterIds": []
    },
    {
      "id": 2588,
      "name": "Walk to Banquet Room",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1553019000000,
      "endDate": 1553020200000,
      "presenterIds": []
    },
    {
      "id": 2543,
      "name": "Workshops",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553103000000,
      "presenterIds": []
    },
    {
      "id": 2571,
      "name": "Lunch (on your own)",
      "typeId": 10154,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1552995900000,
      "endDate": 1553002200000,
      "presenterIds": []
    },
    {
      "id": 2573,
      "name": "W1 Intelligent Music Interfaces for Listening and Creation",
      "typeId": 10154,
      "roomId": 10172,
      "chairIds": [
        15611,
        23024
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553104800000,
      "description": "Both music creation and music listening interfaces heavily rely on and benefit from intelligent approaches that enable users to access sound and music in unprecedented manners. This ongoing trend draws from manifold areas...",
      "presenterIds": []
    },
    {
      "id": 2658,
      "name": "W2 Exploratory Search and Interactive Data Analytics",
      "typeId": 10154,
      "roomId": 10173,
      "chairIds": [
        19318,
        15912,
        13315,
        24036,
        8853
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553104800000,
      "description": "The aim of this workshop is to explore new methods and interface/system design for interactive data analytics and management in various domains, including specialised text collections (e.g. legal, medical, scientific), multimedia, and bioinformatics...",
      "presenterIds": []
    },
    {
      "id": 2659,
      "name": "W3 Explainable Smart Systems",
      "typeId": 10154,
      "roomId": 10168,
      "chairIds": [
        18085,
        19700,
        23977
      ],
      "contentIds": [],
      "startDate": 1553072400000,
      "endDate": 1553104800000,
      "description": "This workshop will follow on from the very successful ExSS 2018 workshop held at IUI. It will bring together researchers in academia and industry who have an interest in making smart systems explainable to users and therefore more intelligible ...",
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 4353,
      "typeId": 11296,
      "title": "yyhe@g.clemson.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": ""
            }
          ],
          "personId": 11169
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7171,
      "title": "DARPA’s Explainable Artificial Intelligence (XAI) Program",
      "trackId": 10075,
      "tags": [],
      "keywords": [],
      "abstract": "Dramatic success in machine learning (ML) has led to a new wave of artificial intelligence (AI) applications (e.g., transportation, security, medicine, finance, and defense) that offer tremendous benefits, but cannot explain their decisions and actions to human users. DARPA’s Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychological requirements for effective explanations. XAI’s developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychological theories of explanation to assist the XAI evaluator with defining a suitable evaluation framework, which the developer teams will use to test their AI systems. XAI completed the first phase of the program in February 2019. During this phase, each developer team conducted a user evaluation to assess their AI system’s ability to improve user understanding, trust, and user task performance. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "DARPA I2O"
            }
          ],
          "personId": 9237
        }
      ],
      "sessionIds": [
        1357
      ],
      "eventIds": []
    },
    {
      "id": 6915,
      "typeId": 10158,
      "title": "Improving Take-Over Quality in Automated Driving By Interrupting Non-Driving Tasks",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "With automated driving advancing, first production models started to incorporate the technology. However, until full autonomy is achieved, drivers always need to stay available to take over control from the car. This requirement has proven challenging: increased levels of automation reduce drivers’ situational awareness and driving performance can suffer, especially in the critical moments after take-over. While manual-driving research introduced strategies to direct drivers’ attention back to the road, notably interruptions of the non-driving task, the efficacy of these interventions on automated driving remain unclear. To investigate this, 53 participants drove in an automated simulator while performing tasks on an IVIS. With task interruptions, they reported increased situational awareness and showed improved reaction times during take-over, particularly for low-effort tasks (watching movies). Different to manual driving, halting tasks did not suffice; instead, we displayed the driving scene. Results question effects of situational awareness on take-over and offer solutions for manufacturers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University Munich",
              "dsl": ""
            }
          ],
          "personId": 23362
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Chair for Information Systems (i17)"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Chair for Information Systems (i17)"
            }
          ],
          "personId": 23601
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Santa Clara University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Santa Clara University",
              "dsl": ""
            }
          ],
          "personId": 14573
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Chair for Information Systems"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Chair for Information Systems"
            }
          ],
          "personId": 14394
        }
      ],
      "sessionIds": [
        1038
      ],
      "eventIds": []
    },
    {
      "id": 3588,
      "typeId": 10158,
      "title": "Does Emotion Influence the Use of Auto-suggest during Smartphone Typing?",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Typing based interfaces are common across many mobile applications, especially messaging apps. To reduce the difficulty of typing using keyboard applications on smartphones, smartwatches with restricted space, several techniques, such as auto-complete, auto-suggest, are implemented. Although helpful, these techniques do add more cognitive load on the user. Hence beyond the importance to improve the word recommendations, it is useful to understand the pattern of use of auto-suggestions during typing. Among several factors that may influence use of auto-suggest, the role of emotion has been mostly overlooked, often due to the difficulty of unobtrusively inferring emotion.\r\nWith advances in affective computing, and ability to infer user's emotional states accurately, it is imperative to investigate how auto-suggest can be guided by emotion aware decisions. In this work, we investigate correlations between user emotion and usage of auto-suggest i.e. whether users prefer to use auto-suggest in specific emotion states. We developed an Android keyboard application, which records auto-suggest usage and collects emotion self-reports from users in a 3-week in-the-wild study. Analysis of the dataset reveals relationship between user reported emotion state and use of auto-suggest. We used the data to train personalized models for predicting use of auto-suggest in specific emotion state. The model can predict use of auto-suggest with an average accuracy (AUCROC) of 82% showing the feasibility of emotion-aware auto-suggestion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 11063
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 13397
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 19885
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 14164
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Statesboro",
              "institution": "Georgia Southern University",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 17486
        }
      ],
      "sessionIds": [
        2185
      ],
      "eventIds": []
    },
    {
      "id": 2821,
      "typeId": 11296,
      "title": "homangab@iitk.ac.in",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Uttar Pradesh",
              "city": "Kanpur",
              "institution": "IIT Kanpur",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 11332
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5893,
      "typeId": 11296,
      "title": "lucoba@unibz.it",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bolzano",
              "dsl": ""
            }
          ],
          "personId": 19094
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3338,
      "typeId": 11296,
      "title": "js2812@york.ac.uk",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 18739
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3853,
      "typeId": 10158,
      "title": "TiiS: Proactive Information Retrieval by Inferring Search Intent from Primary Task Context",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        2490
      ],
      "eventIds": []
    },
    {
      "id": 3087,
      "typeId": 10158,
      "title": "PATI: A Projection-based Augmented Table-Top Interface for Robot Programming",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "As robots begin to provide daily assistance to individuals in human environments, their end-users, who do not necessarily have substantial technical training or backgrounds in robotics or programming, will ultimately need to program and ``re-task'' their robots to perform a variety of custom tasks. In this work, we present PATI---a Projection-based Augmented Table-top Interface for robot programming---through which users are able to use simple, common gestures (e.g., pinch gestures) and tools (e.g., shape tools) to specify table-top manipulation tasks (e.g., pick-and-place) for a robot manipulator. PATI allows users to interact with the environment directly when providing task specifications; for example, users can utilize gestures and tools to annotate the environment with task-relevant information, such as specifying target landmarks and selecting objects of interest. We conducted a user study to compare PATI with a state-of-the-art, standard industrial method for end-user robot programming. Our results show that participants needed significantly less training time before they felt confident in using our system than they did for the industrial method. Moreover, participants were able to program a robot manipulator to complete a pick-and-place task significantly faster with PATI. This work indicates a new direction for end-user robot programming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University ",
              "dsl": "Laboratory for Computational Sensing and Robotics"
            }
          ],
          "personId": 11746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 11024
        }
      ],
      "sessionIds": [
        2305
      ],
      "eventIds": []
    },
    {
      "id": 7952,
      "typeId": 10158,
      "title": "SAM: A Modular Framework for Self-Adapting Web Menus",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents SAM, a modular and extensible JavaScript framework for self-adapting menus on webpages. SAM allows control of two elementary aspects for adapting web menus: (1) the target policy, which assigns scores to menu items for adaptation, and (2) the adaptation style, which specifies how they are adapted on display. By decoupling them, SAM enables the exploration of different combinations independently. Several policies from literature are readily implemented, and paired with adaptation styles such as reordering and highlighting. The process—including user data logging—is local, offering privacy benefits and eliminating the need for server-side modifications. Researchers can use SAM to experiment adaptation policies and styles, and benchmark techniques in an ecological setting with real webpages. Practitioners can make websites self-adapting, and end-users can dynamically personalise typically static web menus.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communication and Networking"
            }
          ],
          "personId": 14493
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communications and Networking"
            }
          ],
          "personId": 22168
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 25054
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 19149
        }
      ],
      "sessionIds": [
        1417
      ],
      "eventIds": []
    },
    {
      "id": 3089,
      "typeId": 10158,
      "title": "RL-KLM: Automating Keystroke-level Modeling with Reinforcement Learning",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "The Keystroke-Level Model (KLM) is a popular model for predicting users' task completion times with graphical user interfaces. KLM predicts task completion times as a linear function of elementary operators. However, the policy, or the assumed sequence of the operators that the user executes, needs to be prespecified by the analyst. This paper investigates Reinforcement Learning (RL) as an algorithmic method to obtain the policy automatically. We define the KLM as an Markov Decision Process, and  show that when solved with RL methods, this approach yields user-like policies in simple but realistic interaction tasks. RL-KLM offers a quick way to obtain a global upper bound for user performance. It opens up new possibilities to use KLM in computational interaction.\r\nHowever, scalability and validity remain open issues.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Signal Processing and Acoustics"
            }
          ],
          "personId": 22257
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 19149
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University ",
              "dsl": ""
            }
          ],
          "personId": 15171
        }
      ],
      "sessionIds": [
        2490
      ],
      "eventIds": []
    },
    {
      "id": 3345,
      "typeId": 10158,
      "title": "Assessing Public Perception of Self-Driving Cars: the Autonomous Vehicle Acceptance Model",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce the Autonomous Vehicle Acceptance Model (AVAM), a model of user acceptance for autonomous vehicles, adapted from existing models of user acceptance for generic technologies. A 26-item questionnaire is developed in accordance with the model and a survey conducted to evaluate 6 autonomy scenarios. In a pilot survey (n=54) and follow-up survey (n=187), the AVAM presented good internal consistency and replicated patterns from previous surveys. Results showed that users were less accepting of high autonomy levels and displayed significantly lower intention to use highly autonomous vehicles. We also assess expected driving engagement of hands, feet and eyes which are shown to be lower for full autonomy compared with all other autonomy levels. This highlighted that partial autonomy, regardless of level, is perceived to require uniformly higher driver engagement than full autonomy. These results can inform experts regarding public perception of autonomy across SAE levels. The AVAM and associated questionnaire enable standardised evaluation of AVs across studies, allowing for meaningful assessment of changes in perception over time and between different technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge ",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 25035
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 19259
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 13160
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 19700
        }
      ],
      "sessionIds": [
        1038
      ],
      "eventIds": []
    },
    {
      "id": 4627,
      "typeId": 11296,
      "title": "d8191101@u-aizu.ac.jp",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Fukushima",
              "city": "Aizu Wakamatsu",
              "institution": "University of Aizu",
              "dsl": "Computer Arts Lab"
            }
          ],
          "personId": 20495
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5651,
      "typeId": 11296,
      "title": "tooba.ahsen@tufts.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford, Massachusetts",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 12130
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7955,
      "typeId": 11296,
      "title": "jomatorr@espol.edu.ec",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ecuador",
              "state": "",
              "city": "Guayaquil",
              "institution": "ESPOL University",
              "dsl": ""
            }
          ],
          "personId": 22501
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2836,
      "typeId": 10158,
      "title": "Transformer: A Database-Driven Approach to Generating Forms for Constrained Interaction",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Form-based data insertion or querying is often one of the most time-consuming steps in data-driven workflows. The small screen and lack of physical keyboard in devices such as smartphones and smartwatches introduce imprecision during user input. This can lead to data quality issues such as incomplete responses and errors, increasing user input time.  We present Transformer, a system that leverages the contents of the database to automatically optimize forms for constrained input settings. Our cost function models the user input effort based on the schema and data distribution. This is used by Transformer to find the user interface (UI) widget and layout with ideal input cost for each form field. We demonstrate through user studies that Transformer provides a significantly improved user experience, with up to 50% and 57% reduction in form completion time for smartphones and smartwatches respectively. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Department of Computer Science/Interactive Data Systems"
            }
          ],
          "personId": 19031
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 16945
        }
      ],
      "sessionIds": [
        2490
      ],
      "eventIds": []
    },
    {
      "id": 3093,
      "typeId": 11296,
      "title": "dorisjunglinlee@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois, Urbana-Champaign",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 13057
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4630,
      "typeId": 10158,
      "title": "Smell Pittsburgh: Community-Empowered Mobile Smell Reporting System",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Urban air pollution has been linked to various human health considerations, including cardiopulmonary diseases. Communities who suffer from poor air quality often rely on experts to identify pollution sources due to the lack of accessible tools. Taking this into account, we developed Smell Pittsburgh, a system that enables community members to report odors and track where these odors are frequently concentrated. All smell report data are publicly accessible online. These reports are also sent to the local health department and visualized on a map along with air quality data from monitoring stations. This visualization provides a comprehensive overview of the local pollution landscape. Additionally, with these reports and air quality data, we developed a model to predict upcoming smell events and send push notifications to inform communities. Our evaluation of this system demonstrates that engaging residents in documenting their experiences with pollution odors can help identify local air pollution patterns, and can empower communities to advocate for better air quality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 14333
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 17286
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 15293
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 13317
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 15533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 23540
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University ",
              "dsl": ""
            }
          ],
          "personId": 13230
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 15063
        }
      ],
      "sessionIds": [
        1355
      ],
      "eventIds": []
    },
    {
      "id": 4632,
      "typeId": 11296,
      "title": "valladaresm5783@uhcl.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "University of Houston Clear Lake",
              "dsl": ""
            }
          ],
          "personId": 15795
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7448,
      "typeId": 10158,
      "title": "Learning to Assess the Quality of Stroke Rehabilitation Exercises",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Due to the limited number of therapists, task-oriented exercises are often prescribed for post-stroke survivors as in-home rehabilitation. During in-home rehabilitation, a patient may become unmotivated or confused to comply prescriptions without the feedback of a therapist. To address this challenge, this paper proposes an automated method that can achieve not only qualitative, but also quantitative assessment of stroke rehabilitation exercises. Specifically, we explored a threshold model that utilizes the outputs of binary classifiers to quantify the correctness of a movements into a performance score. We collected movements of 11 healthy subjects and 15 post-stroke survivors using a Kinect sensor and ground truth scores from primary and secondary therapists. The proposed method achieves the following agreement with the primary therapist: 0.8436, 0.8264, and 0.7976 F1-scores on three task-oriented exercises. Experimental results show that our approach performs equally well or better than multi-class classification, regression, or the evaluation of the secondary therapist. Furthermore, we found a strong correlation ($R^2$ = 0.95) between the sum of computed exercise scores and the Fugl-Meyer Assessment scores, clinically validated motor impairment index of post-stroke survivors. Our results demonstrate a feasibility of automatically assessing stroke rehabilitation exercises with the decent agreement levels and clinical relevance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 9951
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 17210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 23496
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Tecnico",
              "dsl": ""
            }
          ],
          "personId": 11365
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Funchal",
              "institution": "Universidade da Madeira",
              "dsl": "Faculdade de Ciências Exatas e da Engenharia / Madeira Interactive Technologies Institute"
            }
          ],
          "personId": 14509
        }
      ],
      "sessionIds": [
        1571
      ],
      "eventIds": []
    },
    {
      "id": 4383,
      "typeId": 10158,
      "title": "The Effect of Explanations and Algorithmic Accuracy on Visual Recommender Systems of Artistic Images",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "There are very few works about explaining content-based recommendations of images in the artistic domain. Current works do not provide a perspective of the many variables involved in the user perception of several aspects of the system such as domain knowledge, relevance, explainability, and trust. In this paper, we aim to fill this gap by studying three interfaces, with different levels of explainability, for artistic image recommendation. Our experiments with N=121 users confirm that explanations of recommendations in the image domain are useful and increase user satisfaction, perception of explainability and relevance. Furthermore, our results show that the observed effects are also dependent on the underlying recommendation algorithm used. We tested two algorithms: Deep Neural Networks (DNN), which has high accuracy, and Attractiveness Visual Features (AVF) with high transparency but lower accuracy. Our results indicate that algorithms should not be studied in isolation, but rather in conjunction with interfaces, since both play a significant role in the perception of explainability and trust for image recommendation. Finally, using the framework by Knijnenburg et al., we provide a comprehensive model which synthesizes the effects between different variables involved in the user experience with explainable visual recommender systems of artistic images.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Pontificia Universidad Católica de Chile",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Millennium Institute for Foundational Research on Data",
              "dsl": ""
            }
          ],
          "personId": 14524
        },
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Pontificia Universidad Catolica de Chile",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Millennium Institute for Foundational Research on Data",
              "dsl": ""
            }
          ],
          "personId": 13541
        },
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Pontificia Universidad Católica de Chile",
              "dsl": ""
            },
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Conversica",
              "dsl": ""
            }
          ],
          "personId": 12618
        },
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Pontificia Universidad Catolica de Chile",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Millennium Institute for Foundational Research on Data",
              "dsl": ""
            }
          ],
          "personId": 24036
        }
      ],
      "sessionIds": [
        2043
      ],
      "eventIds": []
    },
    {
      "id": 4386,
      "typeId": 10158,
      "title": "I Can Do Better Than Your AI:  Expertise and Explanations",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Intelligent assistants, such as navigation, recommender, and expert systems, are most helpful in situations where users lack domain knowledge.  Despite this, recent research in cognitive psychology has revealed that lower-skilled individuals may maintain a sense of illusory superiority, which might suggest that users with the highest need for advice may be the least likely to defer judgment.  Explanation interfaces -- a method for persuading users to take a system's advice -- are thought by many to be the solution for instilling trust, but do their effects hold for self-assured users?  To address this knowledge gap, we conducted a quantitative study (N=529) wherein participants played a binary decision-making game with help from an intelligent assistant.  Participants were profiled in terms of both actual (measured) expertise and reported familiarity with the task concept.  The presence of explanations, level of automation, and number of errors made by the intelligent assistant were manipulated while observing changes in user acceptance of advice.  An analysis of cognitive metrics lead to three findings for research in intelligent assistants:  1) higher reported familiarity with the task simultaneously predicted more reported trust but less adherence, 2) explanations only swayed people who reported very low task familiarity, and 3) showing explanations to people who reported more task familiarity led to automation bias.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Army Research Laboratory",
              "dsl": "Battlefield Information Processing Branch"
            }
          ],
          "personId": 20210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 16392
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Adelphi",
              "institution": "Army Research Laboratory",
              "dsl": "Battlefield Information Processing Branch"
            }
          ],
          "personId": 11828
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Army Research Laboratory",
              "dsl": "Battlefield Information Processing Branch"
            }
          ],
          "personId": 21468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 25042
        }
      ],
      "sessionIds": [
        2174
      ],
      "eventIds": []
    },
    {
      "id": 6182,
      "typeId": 10158,
      "title": "Background Perception and Comprehension of Symbols Conveyed through Vibrotactile Wearable Displays",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Previous research has demonstrated the feasibility of conveying vibrotactile encoded information efficiently using wearable devices. Users can understand vibrotactile encoded symbols and complex messages combining such symbols. Such wearable devices can find applicability in many multitasking use cases. Nevertheless, for multitasking, it would be necessary for the perception and comprehension of vibrotactile information to be less attention demanding and not interfere with other parallel tasks. We present a user study which investigates whether high speed vibrotactile encoded messages can be perceived in the background while performing other concurrent attention-demanding primary tasks. The vibrotactile messages used in the study were limited to symbols representing letters of English Alphabet. We observed that users could very accurately comprehend vibrotactile such encoded messages in the background and other parallel tasks did not affect users performance. Additionally, the comprehension of such messages did also not affect the performance of the concurrent primary task as well.  Our results promote the use of vibrotactile information transmission to facilitate multitasking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know Center",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know Center",
              "dsl": ""
            }
          ],
          "personId": 22693
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know Center GmbH",
              "dsl": "Knowledge Visualization"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know Center GmbH",
              "dsl": "Knowledge Visualization"
            }
          ],
          "personId": 12744
        }
      ],
      "sessionIds": [
        1355
      ],
      "eventIds": []
    },
    {
      "id": 4134,
      "typeId": 10158,
      "title": "S(C)ENTINEL - Monitoring Automated Vehicles with Olfactory Reliability Displays",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Overreliance in technology is safety-critical and it is assumed that this could have been a main cause of severe accidents with automated vehicles. To ease the complex task of permanently monitoring vehicle behavior in the driving environment, researchers have proposed to implement reliability/uncertainty displays. Such displays allow to estimate whether or not an upcoming intervention is likely. However, presenting uncertainty just adds more visual workload on drivers, who might also be engaged in secondary tasks. We suggest to use olfactory displays as a potential solution to communicate system uncertainty and conducted a user study (N=25) in a high-fidelity driving simulator. Results of the experiment (conditions: no reliability display, purely visual reliability display, and visual-olfactory reliability display) comping both objective (task performance) and subjective (technology acceptance model, trust scales, semi-structured interviews) measures suggest that olfactory notifications could become a valuable extension for calibrating trust in automated vehicles. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "CARISSMA"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 25050
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "SCHI Lab, Creative Technology Research Group, School of Engineering and Informatics"
            }
          ],
          "personId": 20098
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 25046
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 25047
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "SCHI Lab, School of Engineering and Informatics"
            }
          ],
          "personId": 13367
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "SCHI Lab, Creative Technology Research Group, School of Engineering and Informatics"
            }
          ],
          "personId": 48933
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 25051
        }
      ],
      "sessionIds": [
        1038
      ],
      "eventIds": []
    },
    {
      "id": 7209,
      "typeId": 11296,
      "title": "tobyli@cs.cmu.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 25033
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5418,
      "typeId": 10158,
      "title": "Automated Rationale Generation: a Technique for Explainable AI and its Effects on Human Perceptions",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "{\\em Automated rationale generation} is an approach for real-time explanation generation whereby a computational model learns to translate an autonomous agent's internal state and action data representations into natural language. \r\nTraining on human explanation data can enable agents to learn to generate human-like explanations for their behavior.\r\nIn this paper, using the context of an ] agent that plays {\\em Frogger}, we describe \r\n(a) how to collect a corpus of explanations, \r\n(b) how to train a neural rationale generator to produce different styles of rationales, and\r\n(c) how people perceive these rationales.\r\nWe conducted two user studies. \r\nThe first study establishes the plausibility of each type of generated rationale and situates their user perceptions along the dimensions of \\textit{confidence}, \\textit{humanlike-ness}, \\textit{adequate justification}, and \\textit{understandability}.\r\nThe second study further explores user preferences between the generated rationales with regard to \\textit{confidence} in the autonomous agent, communicating \\textit{failure} and \\textit{unexpected behavior}. \r\nOverall, we find alignment between the intended differences in features of the generated rationales and the perceived differences by users.\r\nMoreover, context permitting, participants preferred detailed rationales to  form a stable mental model of the agent's behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Interactive Computing"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 21674
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Interactive Computing"
            }
          ],
          "personId": 8349
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Interactive Computing"
            }
          ],
          "personId": 16280
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kentucky",
              "city": "Lexington",
              "institution": "University of Kentucky",
              "dsl": "Computer Science"
            }
          ],
          "personId": 23628
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Interactive Computing"
            }
          ],
          "personId": 14033
        }
      ],
      "sessionIds": [
        2174
      ],
      "eventIds": []
    },
    {
      "id": 5419,
      "typeId": 10158,
      "title": "When People and Algorithms Meet: User-reported Problems in Intelligent Everyday Applications",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "The complex nature of intelligent systems motivates work on supporting users during interaction, for example through explanations. However, there is yet little empirical evidence on specific problems users face in such systems in everyday use.\r\nThis paper investigates such problems as reported by users: We analysed 35,448 reviews of three apps on the Google Play Store (Facebook, Netflix and Google Maps) with sentiment analysis and topic modelling to reveal problems during interaction that can be attributed to the apps' algorithmic decision-making. We enriched this data with users' coping and support strategies through a follow-up online survey (N=286). In particular, we found problems and strategies related to content, algorithm, user choice, and feedback.\r\nWe discuss corresponding implications for designing user support, highlighting the importance of user control and explanations of output, not processes.\r\nOur work thus contributes empirical evidence to facilitate understanding of users' everyday problems with intelligent systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 9153
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 18607
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 20082
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 22784
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 25048
        }
      ],
      "sessionIds": [
        2366
      ],
      "eventIds": []
    },
    {
      "id": 3372,
      "typeId": 11296,
      "title": "mirko.gelsomini@polimi.it",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Milan",
              "institution": "Politecnico di Milano",
              "dsl": "Department of Design"
            }
          ],
          "personId": 17514
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2863,
      "typeId": 11296,
      "title": "tmahmood@uncc.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": ""
            }
          ],
          "personId": 12415
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4660,
      "typeId": 10158,
      "title": "Peripheral Vision: A New Killer App for Smart Glasses",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Most smart glasses have a small and limited field of view. The head-mounted display often spreads between the human central and peripheral vision. In this paper, we exploit this characteristic to display information in the peripheral vision of the user. We introduce a mobile peripheral vision model, which can be used on any smart glasses with a head-mounted display without any additional hardware requirement. This model taps into the blocked peripheral vision of a user and simplifies multi-tasking when using smart glasses. To display the potential applications of this model, we implement an application for indoor and outdoor navigation. We conduct an experiment on 20 people on both smartphone and smart glass to evaluate our model on indoor and outdoor conditions. Users report to have spent at least 50% less time looking at the screen by exploiting their peripheral vision with smart glass. 90% of the users Agree}that using the model for navigation is more practical than standard navigation applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "SymLab"
            }
          ],
          "personId": 24083
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 18862
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "Kowloon",
              "city": "ClearWater Bay",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Symlab"
            }
          ],
          "personId": 18450
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Computer Science"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "CSE"
            }
          ],
          "personId": 12724
        }
      ],
      "sessionIds": [
        1318
      ],
      "eventIds": []
    },
    {
      "id": 3893,
      "typeId": 10158,
      "title": "TiiS: A Visual Approach for Interactive Keyterm-based Clustering",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1318
      ],
      "eventIds": []
    },
    {
      "id": 6197,
      "typeId": 10158,
      "title": "Towards Rapid Interactive Machine Learning: Evaluating Tradeoffs of Classification without Representation",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Our contribution is the design and evaluation of an interactive machine learning interface that rapidly provides the user with model feedback after every interaction. To address visual scalability, this interface communicates with the user via a ``tip of the iceberg'' approach, where the user interacts with a small set of recommended instances for each class. To address computational scalability, we developed an $O(n)$ classification algorithm that incorporates user feedback incrementally, and without consulting the data's underlying representation matrix. Our computational evaluation showed that this algorithm has similar accuracy to several off-the-shelf classification algorithms with small amounts of labeled data. Empirical evaluation revealed that users performed better using our design compared to an equivalent active learning setup.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Richland",
              "institution": "Pacific Northwest National Laboratory",
              "dsl": "Visual Analytics"
            }
          ],
          "personId": 22890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Richland",
              "institution": "Pacific Northwest National Laboratory",
              "dsl": "Data Sciences"
            }
          ],
          "personId": 20115
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 15853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Richland",
              "institution": "PNNL",
              "dsl": "Data Sciences and Analytics"
            }
          ],
          "personId": 22414
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 21140
        }
      ],
      "sessionIds": [
        1356
      ],
      "eventIds": []
    },
    {
      "id": 6203,
      "typeId": 10158,
      "title": "The Role of User Differences in Customization: A Case Study in Personalization for Infovis-Based Content",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Although there is extensive evidence that personalization of interactive systems can improve the user’s experience and satisfaction, it is also known that the two main approaches to deliver personalization, namely via customization or system-driven adaptation, have limitations. In particular, many users do not use customize mechanisms, while adaptation can be perceived as intrusive and opaque. In this paper, we explore an intermediary approach to personalization, namely delivering system-driven support to customization. To this end, we study a customization mechanism allowing to choose the type and amount of information displayed by means of information visualizations in a system for decision making, and examine the impact of user differences on the effectiveness of this mechanism. Our results show that, for the users who did use the customization mechanism, customization effectiveness was impacted by their levels of visualization literacy and locus of control. These results suggest that the customization mechanism could be improved by system-driven assistance to customize depending on the user’s level of visualization literacy and locus of control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 20808
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "UBC",
              "dsl": "Computer Science"
            }
          ],
          "personId": 23068
        }
      ],
      "sessionIds": [
        2143
      ],
      "eventIds": []
    },
    {
      "id": 3133,
      "typeId": 11296,
      "title": "thanhan91@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "Computer Science"
            }
          ],
          "personId": 16644
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7742,
      "typeId": 11296,
      "title": "domina.robert@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "-Select-",
              "city": "Jönköping",
              "institution": "Jönköping University",
              "dsl": ""
            }
          ],
          "personId": 20174
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5956,
      "typeId": 10158,
      "title": "SearchLens: Composing and Capturing Complex User Interests for Exploratory Search",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Whether figuring out where to eat in an unfamiliar city or deciding which apartment to live in, consumer generated data (i.e. reviews and forum posts) are often an important influence in online decision making. To make sense of these rich repositories of diverse opinions, searchers need to sift through a large number of reviews to characterize each item based on aspects that they care about. We introduce a novel system, SearchLens, where searchers build up a collection of \"Lenses\" that reflect their different latent interests, and compose the Lenses to find relevant items across different contexts. Based on the Lenses, SearchLens generates personalized interfaces with visual explanations that promotes transparency and enables deeper exploration. While prior work found searchers may not wish to put in effort specifying their goals without immediate and sufficient benefits, results from a controlled lab study suggest that our approach incentivized participants to express their interests more richly than in a baseline condition, and a field study showed that participants found benefits in SearchLens while conducting their own tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "HCII and LTI",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 13257
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institue"
            }
          ],
          "personId": 25043
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 16608
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 25024
        }
      ],
      "sessionIds": [
        2490
      ],
      "eventIds": []
    },
    {
      "id": 5190,
      "typeId": 11296,
      "title": "gmountk@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 21650
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5194,
      "typeId": 10158,
      "title": "TiiS: A Human-in-the-loop System for Sound Event Detection and Annotation",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1318
      ],
      "eventIds": []
    },
    {
      "id": 5452,
      "typeId": 10158,
      "title": "Who Should Be My Teammates: Using A Conversational Agent to Understand Individual Differences and Help Teaming",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "We are building an intelligent agent to help teaming efforts. In this paper, we investigate the real-world use of such an agent to understand students deeply and help student team formation in a large university class involving about 200 students and 40 teams. Specifically, the agent interacted with each student in a text-based conversation at the beginning and end of the class. We show how the intelligent agent was able to elicit in-depth information from the students, infer the students' personality traits, and reveal the complex relationships between team personality compositions and team results. We also report on the students' behavior with and impression of the agent. We discuss the benefits and limitations of such an intelligent agent in helping team formation, and the design considerations for creating intelligent agents for aiding in teaming efforts.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 25049
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Juji, Inc.",
              "dsl": ""
            }
          ],
          "personId": 25026
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "urbana",
              "institution": "university of illinois",
              "dsl": ""
            }
          ],
          "personId": 25021
        }
      ],
      "sessionIds": [
        1288
      ],
      "eventIds": []
    },
    {
      "id": 7245,
      "typeId": 10158,
      "title": "TiiS: A Classification Model for Sensing Human Trust in Machines Using EEG and GSR",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1078
      ],
      "eventIds": []
    },
    {
      "id": 4685,
      "typeId": 10158,
      "title": "TiiS: A Review of User Interface Design for Interactive Machine Learning",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1356
      ],
      "eventIds": []
    },
    {
      "id": 5711,
      "typeId": 10158,
      "title": "TiiS: Crowdsourcing Ground Truth for Medical Relation Extraction",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        2043
      ],
      "eventIds": []
    },
    {
      "id": 7505,
      "typeId": 11296,
      "title": "ber58@pitt.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 17719
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6229,
      "typeId": 10158,
      "title": "Do I Trust My Machine Teammate? An Investigation from Perception to Decision",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "In the human-machine collaboration context, understanding the reason behind each human decision is critical for interpreting the performance of the human-machine team. Via an experimental study of a system with varied levels of accuracy, we describe how human trust interplays with system performance, human perception and decisions. It is revealed that humans are able to perceive the performance of automatic systems and themselves, and adjust their trust levels according to the accuracy of systems. The 70% system accuracy suggests to be a threshold between increasing and decreasing human trust and system usage. We have also shown that trust can be derived from a series of users’ decisions rather than from a single one, and relates to the perceptions of users. A general framework depicting how trust and perception affect human decision making is proposed, which can be used as future guidelines for human-machine collaboration design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Eveleigh",
              "institution": "CSIRO",
              "dsl": "Data61"
            }
          ],
          "personId": 14462
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "CSIRO",
              "dsl": ""
            }
          ],
          "personId": 12438
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "Data61 - CSIRO",
              "dsl": ""
            }
          ],
          "personId": 11006
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": ""
            }
          ],
          "personId": 16869
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO",
              "dsl": ""
            }
          ],
          "personId": 8358
        }
      ],
      "sessionIds": [
        1078
      ],
      "eventIds": []
    },
    {
      "id": 4694,
      "typeId": 10158,
      "title": "Guided Play: Digital Sensing and Coaching for Stereotypical Play Behavior in Children with Autism",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Restricted and repetitive behaviors (RRBs) are a core symptom and an early marker of autism. Current research and intervention for RRB heavily rely on professional experience and effort. Guided Play is a technology that uses instrumented games and toys as a platform to understand children's play behavior and facilitate behavioral intervention during play. This paper presents the design and implementation of a prototype based on the technology, as well as an evaluation on 6 children with autism. The results show that children with RRBs in physical world activities also exhibit similar patterns in a similar digital activity, and that digital coaching can reduce RRBs by expanding children's play skill repertoire and promoting symbolic play.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Fujitsu Laboratories of America",
              "dsl": ""
            }
          ],
          "personId": 12893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Fujitsu Labs of America",
              "dsl": ""
            }
          ],
          "personId": 14869
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Sunnyvale",
              "institution": "Fujistu Laboratories of America",
              "dsl": ""
            }
          ],
          "personId": 24156
        }
      ],
      "sessionIds": [
        1571
      ],
      "eventIds": []
    },
    {
      "id": 8024,
      "typeId": 10158,
      "title": "Decision Making Strategies Differ in the Presence of Collaborative Explanations: Two Conjoint Studies",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Rating-based summary statistics are ubiquitous in e-commerce, and often are crucial components in personalized recommendation mechanisms. Especially visual rating summarizations have been identified as important means to explain, why an item is presented or proposed to an user. Largely left unexplored, however, is the issue to what extent the descriptives of these rating summary statistics influence decision making of the online consumer. Therefore, we conducted a series of two conjoint experiments to explore how different summarizations of rating distributions (i.e., in the form of number of ratings, mean, variance, skewness, bimodality, or origin of the ratings) impact users' decision making. In a first study with over 200 participants, we identified that users are primarily guided by the mean and the number of ratings, and -- to lesser degree -- by the variance and origin of a rating. When probing the maximizing behavioral tendencies of our participants, other sensitivities regarding the summary of rating distributions became apparent. We thus instrumented a follow-up eye-tracking study to explore in more detail, how the choices of participants vary in terms of their decision making strategies. This second round with over 40 additional participants supported our hypothesis that users, who usually experience higher decision difficulty, follow compensatory decision strategies, and focus more on the decisions they make. We conclude by outlining how the results of these studies can guide algorithm development, and counterbalance presumable biases in implicit user feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": ""
            }
          ],
          "personId": 19094
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": ""
            }
          ],
          "personId": 9288
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": ""
            }
          ],
          "personId": 14562
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": ""
            }
          ],
          "personId": 9696
        }
      ],
      "sessionIds": [
        1407
      ],
      "eventIds": []
    },
    {
      "id": 4185,
      "typeId": 11296,
      "title": "kai.hollaender@ifi.lmu.de",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [],
          "personId": 25037
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5978,
      "typeId": 10158,
      "title": "Walking with Adaptive Augmented Reality Workspaces: Design and Usage Patterns",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile augmented reality may eventually replace our smartphones as the primary way of accessing information on the go. However, current interfaces provide little support to walking and to the variety of actions we perform in the real world. To achieve its full potential, augmented reality interfaces must support the fluid way we move and interact in the physical world. We explored how different adaptation strategies can contribute towards this goal. We evaluated design alternatives through contextual studies and identified the key interaction patterns that interfaces for walking should support. We also identified desirable properties of adaptation-based interface techniques, which can be used to guide the design of the next-generation walking-centered augmented reality workspaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 25044
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 25052
        }
      ],
      "sessionIds": [
        2305
      ],
      "eventIds": []
    },
    {
      "id": 4698,
      "typeId": 10158,
      "title": "Where can my career take me? Harnessing Dialogue for Interactive Career Goal Recommendations",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Career goals represent a special case for recommender systems and require considering both short and long term goals. Recommendations must represent a trade off between relevance to the user, achievability and aspirational goals to move the user forward in their career. Users may have different motivations and concerns when looking for a new long term goal, so involving the user in the recommender process becomes all the more important than in other domains. Additionally, the cost to the user of making a bad decision is much higher than investing two hours in watching a movie they don't like or listening to an unappealing song. As a result, we feel career recommendations is a unique opportunity to truly engage the user in an interactive recommender as we believe they will invest the cognitive load. In this paper, we present an interactive career goal recommender framework that leverages the power of dialogue to allow the user interactively improve the recommendations and bring their own preferences to the system. \r\nThe underlying recommendation algorithm is a novel solution that suggests both short and long term goals through utilizing the sequential patterns extracted from career trajectories that are enhanced with features of the supporting user profiles. The effectiveness of the proposed solution is demonstrated with extensive experiments on two real world data sets. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 21105
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 9914
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 23346
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "",
              "city": "La Plata",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Argentina",
              "state": "",
              "city": "La Plata",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 16231
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "",
              "city": "Buenos Aires",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Argentina",
              "state": "",
              "city": "Buenos Aires",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 19661
        }
      ],
      "sessionIds": [
        1356
      ],
      "eventIds": []
    },
    {
      "id": 3677,
      "typeId": 10158,
      "title": "Rasch-based Tailored Goals for Nutrition Assistance Systems",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Choosing adequate goals plays is central to the success of a task. With this study, we investigate tailoring the goals of a nutrition assistance system to the user's abilities according to a Rasch scale. To that end, we evaluated two versions of a mobile system that offers dietary tracking, visual feedback, and personalized recipe recommendations. The original version targets optimal nutritional behavior and focuses on the six least optimal nutrients (N=51). The adapted version targets only improved nutritional behavior compared to the status quo and thus tailors the advice to the next six achievable nutrients according to a Rasch scale (N=47). Results of the two-week study indicate that the tailored advice leads to higher success for the focused nutrients, and is perceived to be more diverse and personalized, and thus more effective.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "Department of Informatics",
              "dsl": "Technical University of Munich"
            }
          ],
          "personId": 16990
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction Group"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "'s-Hertogenbosch",
              "institution": "Jheronimus Academy of Data Science",
              "dsl": ""
            }
          ],
          "personId": 10136
        }
      ],
      "sessionIds": [
        1417
      ],
      "eventIds": []
    },
    {
      "id": 5470,
      "typeId": 10158,
      "title": "ShopEye: Fusing RFID and Smartwatch for Multi-relation Excavation in Physical stores",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Smart retail stores open new possibilities for enabling a variety of physical analytics, such as users' shopping trajectories and preferences for certain items. This paper aims to excavate three kinds of relations in physical stores, i.e. user-item, user-user and item-item, which provide abundant information for enhancing users' shopping experiences and boosting retailers' sales. We present ShopEye, a hybrid RFID and smartwatch system to delve into these relations in an implicit and non-intrusive manner. The intuition is that inertial sensors embedded in smartwatches and RFID tags attached to items can capture the user behaviors and the item motions, respectively. ShopEye first pairs users with corresponding items according to  correlations between inertial signals and RFID signals, and then incorporates these pairs with the motion behaviors of users to further profile user-user and item-item relations. We have tested the system extensively in our lab environment which mimics the real retail store. Experimental results demonstrate the effectiveness and robustness of ShopEye in excavating these relations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT lab"
            }
          ],
          "personId": 25038
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT lab"
            }
          ],
          "personId": 25023
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 25025
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT lab"
            }
          ],
          "personId": 9329
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT Lab"
            }
          ],
          "personId": 9310
        }
      ],
      "sessionIds": [
        1355
      ],
      "eventIds": []
    },
    {
      "id": 5726,
      "typeId": 10158,
      "title": "Digital Survivor of Sexual Assault",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "The Digital Survivor of Sexual Assault (DS2A) is an interface that allows a user to have a conversational experience with a survivor of sexual assault, using Artificial Intelligence technology and recorded videos. The application uses a statistical classifier to retrieve contextually appropriate pre-recorded video utterances by the survivor, together with dialogue management policies which enable users to conduct simulated conversations with the survivor about the sexual assault, its aftermath, and other pertinent topics. The content in the application has been specifically elicited to support the needs for the training of U.S. Army professionals in the Sexual Harassment/Assault Response and Prevention (SHARP) Program, and the application comes with an instructional support package. The system has been tested with approximately 200 users, and is presently being used in the SHARP Academy's capstone course.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 16470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 19071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 17502
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 21611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Making Things 3D",
              "dsl": ""
            }
          ],
          "personId": 18380
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 16547
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 21728
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Fort Leavenworth",
              "institution": "U.S. Army",
              "dsl": "SHARP Academy"
            }
          ],
          "personId": 21358
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Fort Leavenworth",
              "institution": "U.S. Army",
              "dsl": "SHARP Academy"
            }
          ],
          "personId": 18149
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Institute for Creative Technologies"
            }
          ],
          "personId": 25040
        }
      ],
      "sessionIds": [
        1288
      ],
      "eventIds": []
    },
    {
      "id": 5217,
      "typeId": 10158,
      "title": "Progressive Disclosure: Empirically Motivated Approaches to Designing Effective Transparency ",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "As we increasingly delegate important decisions to intelligent systems, it is essential that users understand how algorithmic decisions are made. Prior work has often taken a technocentric approach to transparency. In contrast, we explore empirical user-centric methods to better understand user reactions to transparent systems. We assess user reactions to transparency in two studies. In Study 1, users anticipated that a more transparent system would perform better, but retracted this evaluation after experience with the system. Qualitative data suggest this arose because transparency is distracting and undermines simple heuristics users form about system operation. Study 2 explored these effects in depth, suggesting that users may benefit from initially simplified feedback that hides potential system errors and assists users in building working heuristics about system operation. We use these findings to motivate new progressive disclosure principles for transparency in intelligent systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 12854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 15775
        }
      ],
      "sessionIds": [
        2366
      ],
      "eventIds": []
    },
    {
      "id": 8034,
      "typeId": 10158,
      "title": "Exemplar Based Experience Transfer",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Banners are present in several forms and a person might be inspired by one or more of these. However, designing banners is a non-trivial task, especially for novices. Starting from a blank canvas can often be overwhelming, and exploring alternatives is time-consuming. In this paper, we propose an automatic approach to transfer a novice user's content into an example banner. Our algorithm begins with extracting the template of the example banner via a semantic segmentation approach. This is followed by an energy-based optimization framework to combine multiple design elements and arrive at an optimal layout. A crowd-sourced experiment comparing our automatic results against banners designed by creative professionals indicates the viability of the proposed work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16883
        },
        {
          "affiliations": [],
          "personId": 14660
        },
        {
          "affiliations": [],
          "personId": 14971
        },
        {
          "affiliations": [],
          "personId": 16703
        },
        {
          "affiliations": [],
          "personId": 13211
        },
        {
          "affiliations": [],
          "personId": 20928
        }
      ],
      "sessionIds": [
        1465
      ],
      "eventIds": []
    },
    {
      "id": 5734,
      "typeId": 10158,
      "title": "StoryPrint: an Interactive Visualization of Stories",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose StoryPrint, an interactive visualization of creative storytelling that facilitates individual and comparative structural analyses. This visualization method is intended for script-based media, which has suitable metadata. The pre-visualization process involves parsing the script into different metadata categories and analyzing the sentiment on a character and scene basis. For each scene, the setting, character presence, character prominence, and character emotion of a film are represented as a StoryPrint. The visualization is presented as a radial diagram of concentric rings wrapped around a circular time axis. A user then has the ability to toggle a difference overlay to assist in the cross-comparison of two different scene inputs. \r\n\r\nWe evaluated our visualization tool with two different user study groups. A larger group study consisting of 15-minute interviews of 100 naive users tested usability and intuitiveness of design while a smaller group study consisting of hour-long interviews with expert users tested both usability and usefulness as a tool for the writing process and industry. Naive users found the visualization tool to be effective in its portrayal of emotion, characterization, and setting. In addition, naive users showed that the difference overlay was a better visualization for comparative visual analytics than the traditional side-by-side comparison. In the expert study, 4 out of 5 experts supported the use of StoryPrint as a tool during the writing process, and all five found the tool useful for comparing scripts. We conclude that this tool effectively fills the gap in the interactive visualization of individual and comparative analysis research and could introduce a useful tool for writing and comparing scripts for users of all types of experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Disney Research",
              "dsl": ""
            }
          ],
          "personId": 12719
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Disney Research",
              "dsl": ""
            }
          ],
          "personId": 17980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "New Brunswick",
              "institution": "Rutgers University",
              "dsl": "Computer Science Dept"
            }
          ],
          "personId": 15276
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "New Brunswick",
              "institution": "Rutgers University",
              "dsl": ""
            }
          ],
          "personId": 16785
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Swiss Federal Institute of Technology in Zurich (ETHZ)",
              "dsl": ""
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Disney Research",
              "dsl": ""
            }
          ],
          "personId": 12295
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23954
        }
      ],
      "sessionIds": [
        2143
      ],
      "eventIds": []
    },
    {
      "id": 3175,
      "typeId": 10158,
      "title": "MyoSign: Enabling End-to-End Sign Language Recognition with Wearables",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Automatic sign language recognition is an important milestone in facilitating the communication between the deaf community and hearing people. Existing approaches are either intrusive or susceptible to ambient environments and user diversity. Moreover, most of them perform only isolated word recognition, not sentence-level sequence translation. In this paper, we present MyoSign, a deep learning based system that enables end-to-end American Sign Language (ASL) recognition at both word and sentence levels. We leverage a lightweight wearable device which can provide inertial and electromyography signals to non-intrusively capture signs. First, we propose a multimodal Convolutional Neural Network (CNN) to abstract representations from inputs of different sensory modalities. Then, a bidirectional Long Short Term Memory (LSTM) is exploited to model temporal dependences. On the top of the networks, we employ Connectionist Temporal Classification (CTC) to get around temporal segments and achieve end-to-end continuous sign language recognition. We evaluate MyoSign on 70 commonly used ASL words and 100 ASL sentences from 15 volunteers. Our system achieves an average accuracy of 93.7% at word-level and 93.1% at sentence-level in user-independent settings. In addition, MyoSign can recognize sentences unseen in the training set with 92.4% accuracy. The encouraging results indicate that MyoSign can be a meaningful buildup in the advancement of sign language recognition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT lab"
            }
          ],
          "personId": 25038
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT lab"
            }
          ],
          "personId": 25023
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 25025
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "RFID and IOT Lab"
            }
          ],
          "personId": 9310
        }
      ],
      "sessionIds": [
        1465
      ],
      "eventIds": []
    },
    {
      "id": 4200,
      "typeId": 10158,
      "title": "Scene Text Access: A Comparison of  Mobile OCR Modalities for Blind Users",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "We present a study with seven blind participants using three different mobile OCR apps to find text posted in various indoor environments. The first app considered was Microsoft SeeingAI in its Short Text mode, which reads any text in sight with a minimalistic interface. The second app was Spot+OCR, a custom application that separates the task of text detection from OCR proper. Upon detection of text in the image, Spot+OCR generates a short vibration; as soon as the user stabilizes the phone, a high-resolution snapshot is taken and OCR-processed. The third app, Guided OCR, was designed to guide the user in taking several pictures in a 360º span at the maximum resolution available by the camera, with minimum overlap between pictures. Quantitative results (in terms of true positive ratios and traversal speed) were recorded. Along with the qualitative observation and outcomes from an exit survey, these results allow us to identify and assess the different strategies used by our participants, as well as the challenges of operating these systems without sight.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California",
              "dsl": ""
            }
          ],
          "personId": 20681
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California",
              "dsl": ""
            }
          ],
          "personId": 23702
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California",
              "dsl": ""
            }
          ],
          "personId": 8639
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 20313
        }
      ],
      "sessionIds": [
        1571
      ],
      "eventIds": []
    },
    {
      "id": 7532,
      "title": "Getting Virtually Personal: Making Responsible and Empathetic “Her” for Everyone",
      "trackId": 10075,
      "tags": [],
      "keywords": [],
      "abstract": "Have you watched the movie Her? Have you ever wondered or wished to have your own AI companion just like Samantha, who could understand you better than you know about yourself, and could tell you what you really are, whom your best partner may be, and which career path would be best for you? In this talk, I will present a computational framework for building responsible and empathetic Artificial Intelligent (AI) agents who can deeply understand their users as unique individuals and responsibly guide their behavior in both virtual and real world.\n\nStarting with a live demo of showing how an AI interviewer chats with a user to automatically derive his/her personality characteristics and provide personalized recommendations, I will highlight the technical advances of the framework in two aspects. First, I will present a computational, evidence-based approach to Big 5 personality inference, which enables an AI agent to deeply understand a user’s unique characteristics by analyzing the user’s chat text on the fly. Second, I will describe a topic-based conversation engine that couples deep learning with rules to support a natural conversation and rapid customization of a conversational agent.\n\nI will describe the initial applications of our AI agents in the real world, from talent selection to student teaming to user experience research. Finally, I will discuss the wider implications of our work on building hyper-personalized systems and their impact on our lives.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Juji, Inc"
            }
          ],
          "personId": 15326
        }
      ],
      "sessionIds": [
        1823
      ],
      "eventIds": []
    },
    {
      "id": 8046,
      "typeId": 11296,
      "title": "martijn.millecamp@cs.kuleuven.be",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23385
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4467,
      "typeId": 11296,
      "title": "bwilliford@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 13195
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6261,
      "typeId": 10158,
      "title": "Investigating the Feasibility of Finger Identification on Capacitive Touchscreens using Deep Learning",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Touchscreens enable intuitive mobile interaction. However, touch input is limited to 2D touch locations which makes it challenging to provide shortcuts and secondary actions similar to hardware keyboards and mice. Previous work presented a wide range of approaches to provide secondary actions by identifying which finger touched the display. While these approaches are based on external sensors which are inconvenient, we use capacitive images from mobile touchscreens to investigate the feasibility of finger identification. We collected a dataset of low-resolution fingerprints and trained convolutional neural networks that classify touches from eight combinations of fingers. We focused on combinations that involve the thumb and index finger as these are mainly used for interaction. As a result, we achieved an accuracy of over 92% for a position-invariant differentiation between left and right thumbs. We evaluated the model and two use cases that users find useful and intuitive. \r\nWe publicly share our data set (CapFingerId) comprising 455,709 capacitive images of touches from each finger on a representative mutual capacitive touchscreen and our models to enable future work using and improving them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": ""
            }
          ],
          "personId": 23461
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "University of Stuttgart",
              "dsl": ""
            }
          ],
          "personId": 14575
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 25022
        }
      ],
      "sessionIds": [
        1318
      ],
      "eventIds": []
    },
    {
      "id": 3446,
      "typeId": 10158,
      "title": "To Explain or not to Explain: the Effects of Personal Characteristics when Explaining Music Recommendations",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems have been increasingly used in online services that we consume daily, such as Facebook, Netflix, YouTube, and Spotify. However, these systems are often presented to users as a ``black box'', i.e. the rationale for providing individual recommendations remains unexplained to users. In recent years, various attempts have been made to address this black box issue by providing textual explanations or interactive visualisations that enable users to explore the provenance of recommendations. Among other things, results demonstrated benefits in terms of precision and user satisfaction. Previous research had also indicated that personal characteristics such as domain knowledge, trust propensity and persistence may also play an important role on such perceived benefits. Yet, to date, little is known about the effects of personal characteristics on explaining recommendations. To address this gap, we developed a music recommender system with explanations and conducted an online study using a within-subject design. We captured various personal characteristics of participants and administered both qualitative and quantitative evaluation methods. Results indicate that personal characteristics have significant influence on the interaction and perception of recommender systems, and that this influence changes by adding explanations. For people with a low need for cognition are the explained recommendations the most beneficial. For people with a high need for cognition, we observed that explanations could create a lack of confidence. Based on these results, we present some design implications for explaining recommendations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23385
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 17983
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "UBC",
              "dsl": "Computer Science"
            }
          ],
          "personId": 23068
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9340
        }
      ],
      "sessionIds": [
        2043
      ],
      "eventIds": []
    },
    {
      "id": 7803,
      "typeId": 11296,
      "title": "protiva.r@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Department of Computer Science/Interactive Data Systems"
            }
          ],
          "personId": 19031
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6780,
      "typeId": 10158,
      "title": "Assisting Group Activity Analysis through Hand Detection and Identification in Multiple Egocentric Videos",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Research in group activity analysis has put attention to monitor the work and evaluate group and individual performance, which can be reflected towards potential improvements in future group interactions. As a new means to examine individual or joint actions in the group activity, our work investigates the potential of detecting and disambiguating hands of each person in first-person points-of-view videos. Based on the recent developments in automated hand-region extraction from videos, we develop a new multiple-egocentric-video browsing interface that gives easy access to the frames of 1) individual action when only the hands of the viewer are detected, 2) joint action when collective hands are detected, and 3) the viewer checking the others' action as only their hands are detected. We take the evaluation process to explore the effectiveness of our interface with proposed hand-related features which can help perceive actions of interests in the complex analysis of videos involving co-occurred behaviors of multiple people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 19132
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 22999
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 11334
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 20763
        }
      ],
      "sessionIds": [
        2477
      ],
      "eventIds": []
    },
    {
      "id": 2684,
      "typeId": 10158,
      "title": "Towards Human-Guided Machine Learning",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user’s knowledge about the data available.  We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 13702
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Harvard University",
              "dsl": ""
            }
          ],
          "personId": 18248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 15899
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 9668
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Dallas",
              "institution": "University of Texas at Dallas",
              "dsl": ""
            }
          ],
          "personId": 20042
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Information Sciences Institute"
            }
          ],
          "personId": 20840
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 13557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 18731
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 11906
        }
      ],
      "sessionIds": [
        1356
      ],
      "eventIds": []
    },
    {
      "id": 6274,
      "typeId": 10158,
      "title": "Piano Genie",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "We present Piano Genie, an intelligent controller which allows non-musicians to improvise on the piano. With Piano Genie, a user performs on a simple interface with eight buttons, and their performance is decoded into the space of plausible piano music in real time. To learn a suitable mapping procedure for this problem, we train recurrent neural network autoencoders with discrete bottlenecks: an encoder learns an appropriate sequence of buttons corresponding to a piano piece, and a decoder learns to map this sequence back to the original piece. During performance, we substitute a user's input for the encoder output, and play the decoder's prediction each time the user presses a button. To improve the intuitiveness of Piano Genie's performance behavior, we impose musically meaningful constraints over the encoder's outputs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Department of Music"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Brain",
              "dsl": ""
            }
          ],
          "personId": 11948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Brain",
              "dsl": ""
            }
          ],
          "personId": 17414
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "DeepMind",
              "dsl": ""
            }
          ],
          "personId": 16509
        }
      ],
      "sessionIds": [
        2185
      ],
      "eventIds": []
    },
    {
      "id": 3207,
      "typeId": 10158,
      "title": "What can AI do for me: Evaluating Machine Learning Interpretations in Cooperative Play",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Machine learning is an important tool for decision making, but its ethical and responsible application requires rigorous vetting of its interpretability and utility: an understudied problem, particularly for natural language processing models.  We propose an evaluation of interpretation on a real task with real human users, where the effectiveness of interpretation is measured by how much it improves human performance.  We design a grounded, realistic human-computer cooperative setting using a question answering task, Quizbowl.  We recruit both trivia experts and novices to play this game with computer as their teammate, who communicates its prediction via three different interpretations.  We also provide design guidance for natural language processing human-in-the-loop settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park ",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 21418
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "CLIP"
            }
          ],
          "personId": 9353
        }
      ],
      "sessionIds": [
        2174
      ],
      "eventIds": []
    },
    {
      "id": 6792,
      "typeId": 10158,
      "title": "TiiS: Evaluation and Refinement of Clustered Search Results with the Crowd",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1365
      ],
      "eventIds": []
    },
    {
      "id": 6024,
      "typeId": 10158,
      "title": "Towards a Generalizable Method for Detecting Fluid Intake with Wrist-Mounted Sensors and Adaptive Segmentation",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Over the last decade, advances in mobile technologies have enabled the development of intelligent systems that attempt to recognize and model a variety of health-related human behaviors. While automated dietary monitoring based on passive sensors has been an area of increasing research activity for many years, much less attention has been given to tracking fluid intake. In this work, we apply an adaptive segmentation technique on a continuous stream of inertial data captured with a practical, off-the-shelf wrist-mounted device to detect fluid intake gestures passively. We evaluated our approach in a study with 30 participants where 561 drinking instances were recorded. Using a leave-one-participant-out (LOPO), we were able to detect drinking episodes with 90.3% precision and 91.0% recall, demonstrating the generalizability of our approach. In addition to our proposed method, we also contribute an anonymized and labeled dataset of drinking and non-drinking gestures to encourage further work in the field.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 21650
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "The Pennsylvania State University",
              "dsl": "Kinesiology"
            }
          ],
          "personId": 18681
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 8490
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "The Pennsylvania State University",
              "dsl": ""
            }
          ],
          "personId": 11499
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "The Pennsylvania State University",
              "dsl": "Kinesiology"
            }
          ],
          "personId": 9650
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 13548
        }
      ],
      "sessionIds": [
        1355
      ],
      "eventIds": []
    },
    {
      "id": 3464,
      "typeId": 10158,
      "title": "Explainability Scenarios: Towards Scenario-based XAI Design",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Integral to the adoption and uptake of AI systems in real-world settings is the ability for people to make sense of and evaluate such systems, a growing area of development and design efforts known as XAI (Explainable AI). Recent work has advanced the state of the art, yet a key challenge remains in understanding unique requirements that might arise when XAI systems are deployed into complex settings of use. In helping envision such requirements, this paper turns to scenario-based design, a method that anticipates and leverages scenarios of possible use early on in system development. To demonstrate the value of the scenario-based design method to XAI design, this paper presents a case study of aging-in-place monitoring. Introducing the concept of “explainability scenarios” as resources in XAI design, this paper sets out an forward-facing agenda for further attention to the emergent requirements of explainability-in-use. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "IBM Research",
              "dsl": "Almaden"
            }
          ],
          "personId": 25032
        }
      ],
      "sessionIds": [
        2174
      ],
      "eventIds": []
    },
    {
      "id": 5004,
      "typeId": 10158,
      "title": "Flux Capacitors for JavaScript DeLoreans: Approximate Caching for Physics-based Data Interaction",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Interactive visualizations have become an effective and pervasive mode of allowing users to explore the data in a visual, fluid, and immersive manner. \r\nWhile modern web, mobile, touch, and gesture-driven next-generation interfaces such as Leap Motion allow for highly interactive experiences, they pose unique and unprecedented workloads to the underlying data platform. \r\nUsually, these visualizations do not need precise results for most queries generated during an interaction, and the users require the intermediate results as feedback only to guide them towards their goal query.\r\nWe present a middleware component - \\emph{Flux Capacitor}, that insulates the backend from bursty and query-intensive workloads. \r\n\\emph{Flux Capacitor} uses prefetching and caching strategies devised by exploiting the inherent physics-metaphor of UI widgets such as friction and inertia in range sliders, and typical user-interaction. \r\nThis enables low interaction response times while intelligently trading off accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 12161
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 16945
        }
      ],
      "sessionIds": [
        2102
      ],
      "eventIds": []
    },
    {
      "id": 6286,
      "typeId": 10158,
      "title": "Why Do You Like To Drive Automated? A Context-Dependent Analysis of Highly Automated Driving to Elaborate Requirements for Intelligent User Interfaces",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Technology acceptance is a critical factor influencing the adoption of automated vehicles. Consequently, manufacturers feel obliged to design automated driving systems in a way to account for negative effects of automation on user experience. Recent publications confirm that full automation will potentially lack in the satisfaction of important user needs. To counteract, the adoption of Intelligent User Interfaces (IUIs) could play an important role. In this work, we focus on the evaluation of the impact of scenario type (represented by variations of road type and traffic volume) on the fulfillment of psychological needs. Results of a qualitative study (N=30) show that the scenario has a high impact on how users perceive the automation. Based on this, we discuss the potential of adaptive IUIs in the context of automated driving. In detail, we look at the aspects trust, acceptance, and user experience and its impact on IUIs in different driving situations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 25047
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "CARISSMA"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 25050
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Human-Computer Interaction Group",
              "dsl": "Technische Hochschule Ingolstadt"
            }
          ],
          "personId": 22226
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 25051
        }
      ],
      "sessionIds": [
        1038
      ],
      "eventIds": []
    },
    {
      "id": 7570,
      "typeId": 11296,
      "title": "richterr3777@uhcl.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "La Porte",
              "institution": "University of Houston-Clear Lake",
              "dsl": "Human Factors Psychology"
            }
          ],
          "personId": 21933
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4243,
      "typeId": 10158,
      "title": "Supporting job mediator and job seeker through an actionable dashboard",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Job mediation services can assist job seekers in finding suitable employment through a personalised approach. Consultation or mediation sessions, supported by personal profile data of the job seeker, help job mediators understand personal situation and requests. Prediction and recommendation systems can directly provide job seekers with possible job vacancies. However, incorrect or unrealistic suggestions, and bad interpretations can result in bad decisions or demotivation of the job seeker. This paper explores how an interactive dashboard visualising prediction and recommendation output can help support the dialogue between job mediator and job seeker, by increasing the \"explainability\" and providing mediators with control over the information that is shown to job seekers. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 23047
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 13528
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 9340
        }
      ],
      "sessionIds": [
        2366
      ],
      "eventIds": []
    },
    {
      "id": 8086,
      "typeId": 10158,
      "title": "An Intelligent Assistant for Mediation Analysis in Visual Analytics",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Mediation analysis is commonly performed using regressions or Bayesian network analysis in statistics, psychology, and health science; however, it is not effectively supported in existing visualization tools. The lack of assistance poses great risks when people use visualizations to explore causal relationships and make data-driven decisions, as spurious correlations or seemingly conflicting visual patterns might occur. In this paper, we focused on the causal reasoning task over three variables and investigated how an interface could help users reason more efficiently. We developed an interface that facilitates two processes involved in causal reasoning: 1) detecting inconsistent trends, which guides users' attention to important visual evidence, and 2) interpreting visualizations, by providing assisting visual cues and allowing users to compare key visualizations side by side. Our preliminary study showed that the features are potentially beneficial. We discuss design implications and how the features could be generalized for more complex causal analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 20073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois",
              "dsl": "Computer Science"
            }
          ],
          "personId": 15068
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "urbana",
              "institution": "university of illinois",
              "dsl": ""
            }
          ],
          "personId": 25021
        }
      ],
      "sessionIds": [
        1288
      ],
      "eventIds": []
    },
    {
      "id": 6807,
      "typeId": 10158,
      "title": "The Effects of Example-Based Explanations in a Machine Learning Interface",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "The black-box nature of machine learning algorithms can make their predictions difficult to understand and explain to end-users. In this paper, we propose and evaluate two kinds of example-based explanations in the visual domain, normative explanations and comparative explanations (Figure 1), which automatically surface examples from the training set of a deep neural net sketch-recognition algorithm. To investigate their effects, we deployed these explanations to 1150 users on QuickDraw, an online platform where users draw images and see whether a recognizer has correctly guessed the intended drawing. When the algorithm failed to recognize the drawing, those who received normative explanations felt they had a better understanding of the system, and perceived the system to have higher capability. However, comparative explanations did not always improve perceptions of the algorithm, possibly because they sometimes exposed limitations of the algorithm and may have led to surprise. These findings suggest that examples can serve as a vehicle for explaining algorithmic behavior, but point to relative advantages and disadvantages of using different kinds of examples, depending on the goal.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 25027
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 12098
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 19814
        }
      ],
      "sessionIds": [
        2174
      ],
      "eventIds": []
    },
    {
      "id": 7831,
      "typeId": 10158,
      "title": "Paralinguistic Recommendations for Affective Word Clouds",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Word clouds are widely used for non-analytic purposes, such as introducing a topic to students, or creating a gift with personally meaningful text. Surveys show that users prefer tools that yield word clouds with a stronger emotional impact. Fonts and color palettes are powerful paralinguistic signals that may determine this impact, but, typically, the expectation is that they are chosen by the users. We present an affect-aware font and color palette selection methodology that aims to facilitate more informed choices. We induce associations of fonts with a set of eight affects, and evaluate the resulting data in a series of user studies both on individual words as well as in word clouds. Relying on a recent study to procure affective color palettes, we carry out a similar user study to understand the impact of color choices on word clouds. Our findings suggest that both fonts and color palettes are powerful tools contributing to the affect associated with a word cloud. The experiments further confirm that the novel datasets we propose are successful in enabling this. Based on this data, we implement a prototype that allows users to specify a desired affect and recommends congruent fonts and color palettes for the word cloud.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": ""
            }
          ],
          "personId": 19858
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": ""
            }
          ],
          "personId": 13786
        }
      ],
      "sessionIds": [
        2185
      ],
      "eventIds": []
    },
    {
      "id": 7587,
      "typeId": 10158,
      "title": "Inferencing Underspecified Natural Language Utterances in Visual Analysis",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Handling ambiguity and underspecification of users’ utterances is challenging, particularly for natural language inter- faces that help with visual analytical tasks. Constraints in the underlying analytical platform and the users’ expectations of high precision and recall require thoughtful inferencing to help generate useful responses. In this paper, we introduce a system to resolve partial utterances based on syntactic and semantic constraints of the underlying analytical expressions. We extend inferencing based on best practices in information visualization to generate useful visualization responses. We employ heuristics to help constrain the solution space of possible inferences, and apply ranking logic to the interpretations based on relevancy. We evaluate the quality of inferred interpretations based on relevancy and analytical usefulness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Software",
              "dsl": ""
            }
          ],
          "personId": 48932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Research",
              "dsl": ""
            }
          ],
          "personId": 17241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Tableau Software",
              "dsl": ""
            }
          ],
          "personId": 19428
        }
      ],
      "sessionIds": [
        2013
      ],
      "eventIds": []
    },
    {
      "id": 3491,
      "typeId": 10158,
      "title": "Effects of the Source of Advice and Decision Task on Decisions to Request Expert Advice",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Automation has become a deeply integrated aspect of our everyday activities. Many factors affect whether we rely on and comply with recommendations that we receive, from both human and automated experts. In the present study, participants were presented with advice from either a human or automated expert to complete one of two decision tasks: assigning teams to find human survivors or assigning teams to find and repair oil wells. Participants played 1 of 4 modified versions of the Search and Rescue video game and, on each trial, were asked to choose 3 of 12 locations to which to send search teams. Participants could request advice from a drone or human expert (confederate), depending on the condition to which they were assigned. Participants utilized automation more consistently than the human expert regardless of the decision task. We discuss possible explanations of our results and how they affect design considerations for automation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "University of Houston-Clear Lake",
              "dsl": "Human Factors Psychology"
            }
          ],
          "personId": 21933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "University of Houston-Clear Lake",
              "dsl": "Human Factors Psychology"
            }
          ],
          "personId": 24311
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "University of Houston-Clear Lake",
              "dsl": ""
            }
          ],
          "personId": 22825
        }
      ],
      "sessionIds": [
        1078
      ],
      "eventIds": []
    },
    {
      "id": 3496,
      "typeId": 10158,
      "title": "Popup: Reconstructing 3D Video Using Particle Filtering to Aggregate Crowd Responses",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Collecting a sufficient amount of 3D training data for autonomous vehicles to handle rare, but critical, traffic events (e.g., collisions) may take decades of deployment. Abundant video data of such events from municipal traffic cameras and video sharing sites (e.g., YouTube) could provide a potential alternative, but generating realistic training data in the form of 3D video reconstructions is a challenging task beyond the current capabilities of computer vision. Crowdsourcing the annotation of necessary information could bridge this gap, but the level of accuracy required to obtain usable reconstructions makes this task nearly impossible for non-experts. In this paper, we propose a novel hybrid intelligence method that combines annotations from workers viewing different instances (video frames) of the same target (3D object), and uses particle filtering to aggregate responses. Our approach can leveraging temporal dependencies between video frames, enabling higher quality through more aggressive filtering.The proposed method results in a 33% reduction in the relative error of position estimation compared to a state-of-the-art baseline. Moreover, our method enables skipping(self-filtering) challenging annotations, reducing the total annotation time for hard-to-annotate frames by 16%. Our approach provides a generalizable means of aggregating more accurate crowd responses in settings where annotation is especially challenging or error-prone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "EECS"
            }
          ],
          "personId": 25020
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 20492
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Human-Computer Interaction Institute",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 25029
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "EECS"
            }
          ],
          "personId": 17838
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 25045
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "EECS"
            }
          ],
          "personId": 11103
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 25055
        }
      ],
      "sessionIds": [
        2477
      ],
      "eventIds": []
    },
    {
      "id": 7338,
      "typeId": 10158,
      "title": "Prediction of Music Pairwise Preferences from Facial Expressions",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Users of a recommender system may be requested to express their preferences about items either with evaluations of items (e.g. a rating) or with comparisons of item pairs. In this work we focus on the acquisition of pairwise preferences in the music domain. Asking the user to explicitly compare music, i.e., which, among two listened tracks, is preferred, requires some user effort. We have therefore developed a novel approach for automatically extracting these preferences from the analysis of the facial expressions of the users while listening to the compared tracks. We have trained a predictor that infers user's pairwise preferences by using features extracted from these data. We show that the predictor performs better than a commonly used baseline, which leverages the user's listening duration of the tracks to infer pairwise preferences. Furthermore, we show that there are differences in the accuracy of the proposed method between users with different personalities and we have therefore adapted the trained model accordingly. Our work shows that by introducing a low user effort preference elicitation approach, which, however, requires to access information that may raise potential privacy issues (face expression), one can obtain good prediction accuracy of pairwise music preferences. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 10294
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Milano",
              "institution": "Vodafone",
              "dsl": ""
            }
          ],
          "personId": 17238
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Ljubljana",
              "institution": "University of Ljubljana, Slovenia",
              "dsl": "Faculty of Computer Science and Informatics"
            }
          ],
          "personId": 14995
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bozen - Bolzano",
              "institution": "Free University of Bozen - Bolzano",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 15586
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bolzano",
              "institution": "Free University of Bozen-Bolzano",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 10200
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Ljubljana",
              "institution": "University of Ljubljana",
              "dsl": ""
            }
          ],
          "personId": 14162
        }
      ],
      "sessionIds": [
        2185
      ],
      "eventIds": []
    },
    {
      "id": 5806,
      "typeId": 11296,
      "title": "fabio.catania@polimi.it",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Milan",
              "institution": "Politecnico di Milano",
              "dsl": "Department of Electronics, Information and Bioengineering"
            }
          ],
          "personId": 13661
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6320,
      "typeId": 11296,
      "title": "mali7@cs.rochester.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester",
              "dsl": "Computer Science"
            }
          ],
          "personId": 16041
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4785,
      "typeId": 10158,
      "title": "TiiS: Seven Metrics that Matter when Modeling Expert Sketching Ability",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        2366
      ],
      "eventIds": []
    },
    {
      "id": 3506,
      "typeId": 11296,
      "title": "mbonham@udel.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Delaware",
              "city": "Newark",
              "institution": "University of Delaware",
              "dsl": "Computer and Information Sciences/ HCI@UD Lab"
            }
          ],
          "personId": 19694
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5298,
      "typeId": 11296,
      "title": "alspring@ucsc.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 12854
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4531,
      "typeId": 10158,
      "title": "Personalized Explanations for Hybrid Recommender Systems",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems have become pervasive on the web, shaping the way users see information and thus the decisions they make. As these systems get more complex, there is a growing need for transparency.   In this paper, we study the problem of generating and visualizing personalized explanations for hybrid recommender systems, which incorporate many different data sources. We build upon a hybrid probabilistic graphical model and develop an approach to generate real-time recommendations along with personalized explanations.  To study the benefits of explanations for hybrid recommender systems, we  conduct a crowd-sourced user study where our system generates personalized recommendations and explanations for real users of the last.fm music platform.  We experiment with 1) different explanation styles (e.g., user-based, item-based), 2) manipulating the number of explanation styles presented, and 3) manipulating the presentation format (e.g., textual vs. visual). We apply a mixed model statistical analysis to consider user personality traits as a control variable and demonstrate the usefulness of our approach in creating personalized hybrid explanations with different style, number, and format.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 9614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Army Research Laboratory",
              "dsl": "Battlefield Information Processing Branch"
            }
          ],
          "personId": 20210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los ANgeles",
              "institution": "University of Southern California",
              "dsl": "Computer Science"
            }
          ],
          "personId": 23600
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 16392
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California Santa Cruz",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 14928
        }
      ],
      "sessionIds": [
        1365
      ],
      "eventIds": []
    },
    {
      "id": 2741,
      "typeId": 10158,
      "title": "Explainable Modeling of Annotations in Crowdsourcing",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Aggregation models for improving the quality of annotations collected via crowdsourcing have been widely studied, but far less has been done to explain why annotators make the mistakes that they do. To this end, we propose a joint aggregation and worker clustering model that detects patterns underlying crowd worker labels to characterize varieties of labeling errors. We evaluate our approach on a Named Entity Recognition dataset labeled by Mechanical Turk workers in both a retrospective experiment and a small human study. The former shows that our joint model improves the quality of clusters vs. aggregation followed by clustering. Results of the latter suggest that clusters aid human sense-making in interpreting worker labels and predicting worker mistakes. By enabling better explanation of annotator mistakes, our model creates a new opportunity to help Requesters improve task instructions and to help crowd annotators learn from their mistakes. Source code, data, and supplementary material is shared online.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "Computer Science"
            }
          ],
          "personId": 18713
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "College of Computer and Information Science"
            }
          ],
          "personId": 13077
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 16765
        }
      ],
      "sessionIds": [
        2477
      ],
      "eventIds": []
    },
    {
      "id": 5302,
      "typeId": 10158,
      "title": "Intelligently Recommending Key Bindings on Physical Keyboards with Demonstrations in Emacs",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Physical keyboards have been peripheral input devices to electronic computers since early 1970s and become ubiquitous during the past few decades, especially in professional areas such as software programming, professional game playing, and document processing. In these real-world applications, key bindings, a fundamental vehicle for human to interact with software systems using physical keyboards, play a critical role in users' productivity. However, as essential applications of artificial intelligence research, research on intelligent user interfaces and recommender systems barely relates to key bindings on physical keyboards. In this paper, we develop a recommender system (referred to as EKBRS) for intelligently recommending key bindings with demonstration in Emacs, which we use as a base user interface. This is a brand new direction of intelligent user interface research and also a novel application of recommender systems. To the best of our knowledge, this is the world's first intelligent user interface that heavily exploits key bindings of physical keyboards and the world's first recommender system for recommending key bindings. We empirically show the effectiveness of our recommender system and briefly discuss the applicability of this recommender system to other software systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley ",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 19909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 21075
        }
      ],
      "sessionIds": [
        1417
      ],
      "eventIds": []
    },
    {
      "id": 5815,
      "typeId": 10158,
      "title": "Visualizing Authorship and Contribution of Collaborative Writing in e-Learning Environments",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, several productivity platforms provide effective capabilities to edit collaboratively the content of a document.  \r\nIn educational settings, e-Learning approaches have taken advantage of this functionality to encourage students to join others to complete projects that include the writing of text documents.  \r\nAlthough collaborative writing may foster interaction among students, the existing analytical metrics on these platforms are limited and can slow down the process of review by instructors in trying to determine the level of contribution of each student in the document.  \r\nIn this paper, we describe an analytic framework to measure and visualize the contribution in collaborative writing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ecuador",
              "state": "",
              "city": "Guayaquil",
              "institution": "ESPOL University",
              "dsl": ""
            }
          ],
          "personId": 22501
        },
        {
          "affiliations": [
            {
              "country": "Ecuador",
              "state": "Guayas",
              "city": "Guayaquil",
              "institution": "Escuela Superior Politécnica del Litoral - ESPOL",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 16596
        },
        {
          "affiliations": [
            {
              "country": "Ecuador",
              "state": "GUAYAS",
              "city": "GUAYAQUIL",
              "institution": "ESPOL",
              "dsl": "ELECTRICAL AND COMPUTER ENGINEERING"
            }
          ],
          "personId": 20471
        }
      ],
      "sessionIds": [
        2143
      ],
      "eventIds": []
    },
    {
      "id": 5559,
      "typeId": 10158,
      "title": "Empathic Dialogue System based on Emotions Extracted from Tweets",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Empathic conversations have increasingly been important for dialogue systems to improve the users’ experience, and increase their engagement with the system, which is difficult for many existing monotonous systems. Existing empathic dialogue systems are designed for limited domain dialogues.  They respond fixed phrases toward observed user emotions.  In open domain conversations, however, generating empathic responses for a wide variety of topics is required. In this paper, we draw on psychological studies about empathy, and propose an empathic dialogue system in open domain conversations. The proposed system generates empathic utterances based on observed emotions in user utterances, thus is able to build empathy with users. Our experiments have proven that users were able to feel more empathy from the proposed system, especially when their emotions were explicitly expressed in their utterances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Fujimino",
              "institution": "KDDI Research, Inc.",
              "dsl": ""
            }
          ],
          "personId": 10075
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Fujimino",
              "institution": "KDDI Research, Inc.",
              "dsl": ""
            }
          ],
          "personId": 11853
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Fujimino",
              "institution": "KDDI Research, Inc.",
              "dsl": ""
            }
          ],
          "personId": 10727
        }
      ],
      "sessionIds": [
        2013
      ],
      "eventIds": []
    },
    {
      "id": 6586,
      "typeId": 10158,
      "title": "Avoiding Drill-down Fallacies with VisPilot: Assisted Exploration of Data Subsets",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "As datasets continue to grow in size and complexity, exploring multi-dimensional datasets remain challenging for analysts. A common operation during this exploration is drill-down---understanding the behavior of data subsets by progressively adding filters. While widely used, in the absence of careful attention towards confounding factors,\r\ndrill-downs could lead to inductive fallacies. Specifically, an analyst may end up being \"deceived\" into thinking that a deviation in trend is attributable to a local change, when in fact it is a more general phenomenon; we term this the drill-down fallacy. One way to avoid falling prey to drill-down fallacies is to exhaustively explore all potential drill-down paths, which quickly becomes infeasible on complex datasets with many attributes. We present VisPilot, an accelerated visual data exploration tool that guides analysts through the key insights in a dataset, while avoiding drill-down fallacies. Our user study results show that VisPilot helps analysts discover interesting visualizations, understand attribute importance, and predict unseen visualizations better than other multidimensional data analysis baselines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois, Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 13057
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana–Champaign",
              "dsl": ""
            }
          ],
          "personId": 10693
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois, Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 23084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google, Inc.",
              "dsl": ""
            }
          ],
          "personId": 12749
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois",
              "dsl": ""
            }
          ],
          "personId": 21887
        }
      ],
      "sessionIds": [
        2102
      ],
      "eventIds": []
    },
    {
      "id": 4540,
      "typeId": 10158,
      "title": "What Data Should I Protect? Recommender and Planning Support for Data Security Analysts",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Major breaches of sensitive company data, as for Facebook's 50 million user accounts in 2018 or Equifax's 143 million user accounts in 2017, are showing the limitations of reactive data security technologies. Companies and government organizations are turning to proactive data security technologies that secure sensitive data at source. However, data security analysts still face two fundamental challenges in data protection decisions: 1) the information overload from the growing number of data repositories and protection techniques to consider; 2) the optimization of protection plans given the current goals and available resources in the organization. In this work, we propose an intelligent user interface for security analysts that recommends what data to protect, visualizes simulated protection impact, and helps build protection plans. In a domain with limited access to expert users and practices, we elicited user requirements from security analysts in industry and modeled data risks based on architectural and conceptual attributes. Our preliminary evaluation suggests that the design improves the understanding and trust of the recommended protections and helps convert risk information in protection plans.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 25030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google",
              "dsl": "UX Research"
            }
          ],
          "personId": 10536
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Redwood City",
              "institution": "Informatica",
              "dsl": "UX Team"
            }
          ],
          "personId": 20199
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Redwood City",
              "institution": "Informatica",
              "dsl": "UX Team"
            }
          ],
          "personId": 23037
        }
      ],
      "sessionIds": [
        1407
      ],
      "eventIds": []
    },
    {
      "id": 3006,
      "typeId": 10158,
      "title": "Cross-Platform Immersive Web Browsing for Online 3D Neuron Database Exploration",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Web services have become one major way for people to obtain and explore information nowadays. However, web browsers currently only offer limited data analysis capabilities, especially for large-scale 3D datasets. This project presents a method of immersive web browsing  (ImWeb) to enable effective exploration of multiple datasets over the web with augmented reality (AR) techniques. The ImWeb system allows inputs from both the web browser and AR and provides a set of immersive analytics methods for enhanced web browsing, exploration, comparison, and summary tasks. We have also integrated 3D neuron mining and abstraction approaches to support efficient analysis functions. The architecture of ImWeb system flexibly separates the tasks on web browser and AR and supports smooth networking among the system, so that ImWeb can be adopted by different platforms, such as desktops, large displays, and tablets. We use an online 3D neuron database to demonstrate that ImWeb enables new experiences of exploring 3D datasets over the web. We expect that our approach can be applied to various other online databases and become one useful addition to future web services.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 18404
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": ""
            }
          ],
          "personId": 12415
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 15262
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 13989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee",
              "dsl": ""
            }
          ],
          "personId": 17952
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "University of North Carolina at Charlotte",
              "dsl": ""
            }
          ],
          "personId": 23398
        }
      ],
      "sessionIds": [
        2305
      ],
      "eventIds": []
    },
    {
      "id": 6594,
      "typeId": 11296,
      "title": "viktor.schlegel@manchester.ac.uk",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "Univesity of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 19964
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6856,
      "typeId": 10158,
      "title": "BigBlueBot: Teaching Strategies for Successful Human-Agent Interactions",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Chatbots are becoming quite popular, with many brands developing conversational experiences using platforms such as IBM's Watson Assistant and Facebook Messenger. However, previous research reveals that users' expectations of what conversational agents can understand and do far outpace their actual technical capabilities. Our work seeks to bridge the gap between these expectations and reality by designing a fun learning experience with several goals: explaining how chatbots work by mapping utterances to a set of intents, teaching strategies for avoiding conversational breakdowns, and increasing desire to use chatbots by creating feelings of empathy toward them. Our experience, called BigBlueBot, consists of interactions with two chatbots in which breakdowns occur and the user (or chatbot) must recover using one or more repair strategies. In a Mechanical Turk evaluation (N=88), participants learned strategies for having successful human-agent interactions, reported feelings of empathy toward the chatbots, and expressed a  desire to interact with chatbots in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 25028
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 19690
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 21796
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 11756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 10991
        }
      ],
      "sessionIds": [
        1288
      ],
      "eventIds": []
    },
    {
      "id": 7625,
      "typeId": 11296,
      "title": "chelsea.m.myers@drexel.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Drexel University",
              "dsl": "Digital Media"
            }
          ],
          "personId": 12376
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3533,
      "typeId": 11296,
      "title": "Akarduni@uncc.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 19585
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3790,
      "typeId": 11296,
      "title": "surjya.ghosh@gmail.com",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 11063
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6611,
      "typeId": 10158,
      "title": "Vulnerable to Misinformation? Verifi!",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "We present Verifi2, a visual analytic system to support the investigation of misinformation on social media. Various models and studies have emerged from multiple disciplines to detect or understand the effects of misinformation. However, there is still a lack of intuitive and accessible tools that help social media users distinguish misinformation from verified news. Verifi2 uses state-of-the-art computational methods to highlight linguistic, network, and image features that can distinguish suspicious news accounts. By exploring news on a source and document level in Verifi2, users can interact with the complex dimensions that characterize misinformation and contrast how real and suspicious news outlets differ on these dimensions. To evaluate Verifi2, we conduct interviews with experts in digital media, communications, education, and psychology who study misinformation. Our interviews highlight the complexity of the problem of combating misinformation and show promising potential for Verifi2 as an educational tool on misinformation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 19585
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "University of North Carolina at Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 13729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 15853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 20247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Richland",
              "institution": "PNNL",
              "dsl": "Data Sciences and Analytics"
            }
          ],
          "personId": 22414
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Richland",
              "institution": "Pacific Northwest National Laboratory",
              "dsl": "Visual Analytics"
            }
          ],
          "personId": 22890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC - Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 12398
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "UNC Charlotte",
              "dsl": "Computer Science"
            }
          ],
          "personId": 21140
        }
      ],
      "sessionIds": [
        2143
      ],
      "eventIds": []
    },
    {
      "id": 3034,
      "typeId": 11296,
      "title": "ppaudyal@asu.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "Impact"
            }
          ],
          "personId": 13046
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4061,
      "typeId": 11296,
      "title": "tianyili@vt.edu",
      "trackId": 10072,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 25030
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7651,
      "typeId": 10158,
      "title": "Induction of an active attitude by short speech reaction time toward interaction for decision-making with multiple agents",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "An interactive decision-making is useful to put our ambiguous desires into concrete through the interaction with others. However, in human-agent interaction, the agents are often not regarded as well-experienced consultants but rather as human-centered interfaces that provide information. We aimed to induce an active human attitude toward decision-making interactions with agents by controlling the speech reaction time (SRT) of the agents in order to consider the agents as reliable consultants. We conducted an experiment to investigate whether the SRT could influence the human participant's attitude. We used two kinds of agents; one had no SRT (no-SRT) and the other had a SRT of two seconds (2s-SRT). As a result, we found that the no-SRT agents could keep the participants' speech reaction times short even during the decision-making task in which the participants need time for careful consideration. In addition, from the analysis of the number of proposed categories and participant's behavior, we suggest that the participants had an active attitude toward interaction with no-SRT agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 8637
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 21740
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 25041
        }
      ],
      "sessionIds": [
        1288
      ],
      "eventIds": []
    },
    {
      "id": 4582,
      "typeId": 10158,
      "title": "Atlas: Local Graph Exploration in a Global Context",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Graphs are everywhere, growing increasingly complex, and still lack scalable, interactive tools to support sensemaking. To address this problem, we present Atlas, an interactive graph exploration system that adapts scalable edge decomposition to enable a new paradigm for large graph exploration, generating explorable multi-layered representations. Atlas simultaneously reveals peculiar subgraph structures, (e.g., quasi-cliques) and possible vertex roles in connecting such subgraph patterns. Atlas decomposes million-edge graphs in seconds, scaling to graphs with up to 117 million edges. We present the results from a think-aloud user study with three graph experts and highlight discoveries made possible by Atlas when applied to graphs from multiple domains, including suspicious yelp reviews, insider trading, and word embeddings. Atlas runs in-browser and is open-sourced. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 9538
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 13392
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 8855
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Tech",
              "dsl": "Computational Science and Engineering"
            }
          ],
          "personId": 14302
        }
      ],
      "sessionIds": [
        2102
      ],
      "eventIds": []
    },
    {
      "id": 7655,
      "typeId": 10158,
      "title": " Discovering Natural Language Commands in Multimodal Interfaces",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Discovering what to say and how to say it remains a challenge for users of multimodal interfaces supporting speech input. Users end up \"guessing\" commands that a system might support, often leading to interpretation errors and frustration. One solution to this problem is to display contextually relevant command examples as users interact with a system. The challenge, however, is deciding when, how, and which examples to recommend. In this work, we describe an approach for generating and ranking natural language command examples in multimodal interfaces. We demonstrate the approach using a prototype touch- and speech-based image editing tool. We experiment with augmentations of the UI to understand when and how to present command examples.  Through an online user study, we evaluate these alternatives and find that in-situ command suggestions promote discovery and encourage the use of speech input.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 18508
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 25031
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 14198
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 12265
        }
      ],
      "sessionIds": [
        1465
      ],
      "eventIds": []
    },
    {
      "id": 2792,
      "typeId": 10158,
      "title": "Explaining Models: An Empirical Study of  How Explanations Impact Fairness Judgment",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Ensuring fairness of machine learning systems is a human-in-the-loop process. It relies on developers, users, and the general public to identify fairness problems and make improvements. To facilitate the process we need effective, unbiased, and user-friendly explanations that people can confidently rely on.  Towards that end, we conducted an empirical study with four types of programmatically generated explanations to understand how they impact people's fairness judgments of ML systems. With an experiment involving more than 160 Mechanical Turk workers, we show that: 1) Certain explanations are considered inherently less fair, while others can enhance people's confidence in the fairness of the algorithm; 2) Different fairness problems--such as model-wide fairness issues versus case-specific fairness discrepancies--may be more effectively exposed through different styles of explanation; 3) Individual differences, including prior positions and judgment criteria of algorithmic fairness, impact how people react to different styles of explanation. We conclude with a discussion on providing personalized and adaptive explanations to support fairness judgments of ML systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 13794
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T.J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 25053
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T. J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 9881
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T.J. Watson Research",
              "dsl": ""
            }
          ],
          "personId": 15697
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM",
              "dsl": "IBM T.J.Watson Research Center"
            }
          ],
          "personId": 11567
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 25039
        }
      ],
      "sessionIds": [
        1407
      ],
      "eventIds": []
    },
    {
      "id": 4329,
      "title": "Innovating with AI",
      "trackId": 10075,
      "tags": [],
      "keywords": [],
      "abstract": "Google has created 8 products with over a billion users each. These products are powered by AI (artificial intelligence) at every level —from the core infrastructure and software platform to the applica-tion logic and the user interface. I’ll share a behind-the-scenes look at how Google AI works and how we use it to create innovative UX (user experience) at a planetary scale. I’ll end with our vision to democratize AI and how you can use Google AI in your own work.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Google Cloud"
            }
          ],
          "personId": 16053
        }
      ],
      "sessionIds": [
        1002
      ],
      "eventIds": []
    },
    {
      "id": 2794,
      "typeId": 10158,
      "title": "CoSummary: Adaptive Fast-Forwarding for Surgical Videos by Detecting Collaborative Scenes Using Hand Regions and Gaze Positions",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents CoSummary, an adaptive video fast-forwarding technique for browsing surgical videos recorded by wearable cameras. Current wearable technologies allow us to record complex surgical skills, however, an efficient browsing technique for these videos is not well established. In order to assist browsing surgical videos, our study focuses on adaptively changing playback speeds through the learning and detecting collaborative scenes based on surgeon hand placement and gaze information. Our evaluation shows that the proposed method is able to highlight important collaborative scenes and skip less important scenes during surgical procedures. We have also performed a subjective study with surgeons in order to have professional feedback. The results confirmed the effectiveness of the proposed method in comparison to uniform video fast-forwarding.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 20791
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 15318
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 13924
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 17205
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 20763
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 14328
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 20166
        }
      ],
      "sessionIds": [
        1365
      ],
      "eventIds": []
    },
    {
      "id": 5101,
      "typeId": 10158,
      "title": "Evaluating Narrative-Driven Movie Recommendations on Reddit",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems have become omni-present tools that are used by a wide variety of users in everyday life tasks, such as finding products in Web stores or online movie streaming portals. However, in situations where users already have an idea of what they are looking for (e.g., \"The Lord of the Rings, but in space with a dark vibe\"), most traditional recommender algorithms struggle to adequately address such a priori defined requirements. Therefore, users have built dedicated discussion boards to ask peers for suggestions, which ideally fulfill the stated requirements. In this paper, we set out to determine the utility of well-established recommender algorithms for calculating recommendations when provided with such a narrative. To that end, we first crowdsource a reference evaluation dataset from human movie suggestions. We use this dataset to evaluate the potential of five recommendation algorithms for incorporating such a narrative into their recommendations. Further, we make the dataset available for other researchers to advance the state of research in the field of narrative-driven recommendations. Finally, we use our evaluation dataset to improve not only our algorithmic recommendations, but also existing empirical recommendations of IMDb. Our findings suggest that the implemented recommender algorithms yield vastly different suggestions than humans when presented with the same a priori requirements. However, with carefully configured post-filtering techniques, we can outperform the baseline by up to 100%. This represents an important first step towards more refined algorithmic narrative-driven recommendations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 13148
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Detego",
              "dsl": ""
            }
          ],
          "personId": 15379
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "GESIS - Leibniz Institute for the Social Sciences",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 10240
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 25034
        }
      ],
      "sessionIds": [
        1417
      ],
      "eventIds": []
    },
    {
      "id": 7924,
      "typeId": 10158,
      "title": "Photo Sleuth: Combining Human Expertise and Face Recognition to Identify Historical Portraits",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Identifying people in historical photographs is important for preserving material culture, correcting the historical record, and creating economic value, but it is also a complex and challenging task. In this paper, we focus on identifying portraits of soldiers who participated in the American Civil War (1861-65), the first widely-photographed conflict. Many thousands of these portraits survive, but only 10--20% are identified. We created Photo Sleuth, a web-based platform that combines crowdsourced human expertise and automated face recognition to support Civil War portrait identification. Our mixed-methods evaluation of Photo Sleuth one month after its public launch showed that it helped users successfully identify unknown portraits and provided a sustainable model for volunteer contribution. We also discuss implications for crowd-AI interaction and person identification pipelines. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 16116
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23873
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Arlington",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 23717
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 25036
        }
      ],
      "sessionIds": [
        2477
      ],
      "eventIds": []
    },
    {
      "id": 4853,
      "typeId": 10158,
      "title": "Explaining Recommendations in an Interactive Hybrid Social Recommender",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Hybrid social recommender systems use social relevance from multiple sources to recommend relevant items or people to users. To make hybrid recommendations more transparent and controllable, several researchers have explored interactive hybrid recommender interfaces, which allow for a user-driven fusion of recommendation sources. In this field of work, the intelligent user interface has been investigated as an approach to increase transparency and improve the user experience. In this paper, we attempt to further promote the transparency of recommendations by augmenting an interactive hybrid recommender interface with several types of explanations. We evaluate user behavior patterns and subjective feedback by a within-subject study (N=33). Results from the evaluation show the effectiveness of the proposed explanation models. The result of post-treatment survey indicates a significant improvement in the perception of explainability, but such improvement comes with a lower degree of perceived controllability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": ""
            }
          ],
          "personId": 17790
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 15938
        }
      ],
      "sessionIds": [
        2043
      ],
      "eventIds": []
    },
    {
      "id": 7162,
      "typeId": 10158,
      "title": "Vajra: Step-by-step programming with natural language",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Building natural language programming systems that are geared towards end-users requires the abstraction of formalisms inherently introduced by programming languages, capturing the intent of natural language inputs and mapping it to existing programming language constructs.\r\n\r\nWe present a novel end-user programming paradigm for Python, which maps natural language commands into Python code. The proposed semantic parsing model aims to reduce the barriers for producing well-formed code (syntactic gap) and for exploring third-party APIs (lexico-semantic gap). The proposed method was imple-mented in a supporting system and evaluated in a usability study involving programmers as well as non-programmers.\r\nThe results show that both groups are able to produce code with or without prior programming experience.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "Univesity of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 19964
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Passau",
              "institution": "University of Passau",
              "dsl": "Department of Computer Science and Mathematics"
            }
          ],
          "personId": 11453
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Passau",
              "institution": "University of Passau",
              "dsl": "Department of Computer Science and Mathematics"
            }
          ],
          "personId": 11851
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 8691
        }
      ],
      "sessionIds": [
        2013
      ],
      "eventIds": []
    },
    {
      "id": 4862,
      "typeId": 10158,
      "title": "TiiS: The Effect of Culture on Trust in Automation: Reliability and Workload",
      "trackId": 10074,
      "tags": [],
      "keywords": [],
      "authors": [],
      "sessionIds": [
        1078
      ],
      "eventIds": []
    },
    {
      "id": 3838,
      "typeId": 10158,
      "title": "Analyzing User’s Task-Driven Interaction in Mixed Reality",
      "trackId": 10073,
      "tags": [],
      "keywords": [],
      "abstract": "Mixed reality (MR) provides exciting interaction approaches in several applications. The user experience of interacting in these visually rich environments depends highly on the way the user perceives, processes, and comprehends visual information. In this work we are investigating the differences between Field Dependent – Field Independent users towards their interaction behavior in a MR environment when they were asked to perform a specific task. A study was conducted using Microsoft HoloLens device in which participants interacted with a popular HoloLens application, modified by the authors to log user interaction data in real time. Analysis of the results demonstrates the differences in the visual processing of information, especially in visually complex environments and the impact on the user's interaction behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Cyprus",
              "state": "",
              "city": "Nicosia",
              "institution": "Open University of Cyprus",
              "dsl": "Cyprus Center for Algorithmic Transparency"
            },
            {
              "country": "Cyprus",
              "state": "",
              "city": "Nicosia",
              "institution": "Research Centre on Interactive Media Smart Systems and Emerging Technologies",
              "dsl": "Transparency in Algorithms Group"
            }
          ],
          "personId": 11181
        },
        {
          "affiliations": [
            {
              "country": "Cyprus",
              "state": "Aglantzia",
              "city": "Nicosia",
              "institution": "University of Cyprus",
              "dsl": "Computer Science"
            }
          ],
          "personId": 18388
        }
      ],
      "sessionIds": [
        2305
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 10240,
      "firstName": "Lisa",
      "lastName": "Posch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13315,
      "firstName": "Fernando",
      "lastName": "Paulovich",
      "middleInitial": "V.",
      "affiliations": []
    },
    {
      "id": 13317,
      "firstName": "Michael",
      "lastName": "Tasota",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17414,
      "firstName": "Ian",
      "lastName": "Simon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12295,
      "firstName": "Markus",
      "lastName": "Gross",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23047,
      "firstName": "Sven",
      "lastName": "Charleer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16392,
      "firstName": "John",
      "lastName": "O'Donovan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20492,
      "firstName": "Stephan",
      "lastName": "Lemmer",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 20495,
      "firstName": "Isuru",
      "lastName": "Jayarathne",
      "middleInitial": "Nihathamana",
      "affiliations": []
    },
    {
      "id": 18450,
      "firstName": "Tristan",
      "lastName": "Braud",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24083,
      "firstName": "Isha",
      "lastName": "Chaturvedi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15379,
      "firstName": "Simon",
      "lastName": "Walk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14869,
      "firstName": "Ajay",
      "lastName": "Chander",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9237,
      "firstName": "Dave",
      "lastName": "Gunning",
      "affiliations": []
    },
    {
      "id": 15899,
      "firstName": "Shikhar",
      "lastName": "Gupta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23068,
      "firstName": "Cristina",
      "lastName": "Conati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17952,
      "firstName": "Jian",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15912,
      "firstName": "Axel",
      "lastName": "Soto",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 23084,
      "firstName": "Huizi",
      "lastName": "Hu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23600,
      "firstName": "Jay",
      "lastName": "Pujara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16945,
      "firstName": "Arnab",
      "lastName": "Nandi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23601,
      "firstName": "Matthias",
      "lastName": "Gottlieb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11828,
      "firstName": "James",
      "lastName": "Michaelis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10294,
      "firstName": "Marko",
      "lastName": "Tkalcic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12854,
      "firstName": "Aaron",
      "lastName": "Springer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13367,
      "firstName": "Emanuela",
      "lastName": "Maggioni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14394,
      "firstName": "Helmut",
      "lastName": "Krcmar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17980,
      "firstName": "Sasha",
      "lastName": "Schriber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17983,
      "firstName": "Nyi Nyi",
      "lastName": "Htun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15938,
      "firstName": "Peter",
      "lastName": "Brusilovsky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11332,
      "firstName": "Homanga",
      "lastName": "Bharadhwaj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11334,
      "firstName": "Ryo",
      "lastName": "Yonetani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9288,
      "firstName": "Markus",
      "lastName": "Zanker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20042,
      "firstName": "Vito",
      "lastName": "D'Orazio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11851,
      "firstName": "Siegfried",
      "lastName": "Handschuh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18508,
      "firstName": "Arjun",
      "lastName": "Srinivasan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23628,
      "firstName": "Brent",
      "lastName": "Harrison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11853,
      "firstName": "Kazushi",
      "lastName": "Ikeda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17486,
      "firstName": "Pradipta",
      "lastName": "De",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14928,
      "firstName": "Lise",
      "lastName": "Getoor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13392,
      "firstName": "Fred",
      "lastName": "Hohman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21075,
      "firstName": "Hong",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13397,
      "firstName": "Kaustubh",
      "lastName": "Hiware",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16470,
      "firstName": "Ron",
      "lastName": "Artstein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19031,
      "firstName": "Protiva",
      "lastName": "Rahman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12376,
      "firstName": "Chelsea M.",
      "lastName": "Myers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24156,
      "firstName": "Kanji",
      "lastName": "Uchino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12893,
      "firstName": "Cong",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9310,
      "firstName": "Yinggang",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16990,
      "firstName": "Hanna",
      "lastName": "Schaefer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17502,
      "firstName": "Usman",
      "lastName": "Sohail",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13924,
      "firstName": "Keita",
      "lastName": "Higuchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11365,
      "firstName": "Alexandre",
      "lastName": "Bernadino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20073,
      "firstName": "Chi-Hsien",
      "lastName": "Yen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17514,
      "firstName": "Mirko",
      "lastName": "Gelsomini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21611,
      "firstName": "Chirag",
      "lastName": "Merchant",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12398,
      "firstName": "Samira",
      "lastName": "Shaikh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9329,
      "firstName": "Yufeng",
      "lastName": "Deng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21105,
      "firstName": "Oznur",
      "lastName": "ALKAN",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20082,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14971,
      "firstName": "Surya",
      "lastName": "Dwivedi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9340,
      "firstName": "Katrien",
      "lastName": "Verbert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16509,
      "firstName": "Sander",
      "lastName": "Dieleman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14462,
      "firstName": "Kun",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12415,
      "firstName": "Tahir",
      "lastName": "Mahmood",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19071,
      "firstName": "Carla",
      "lastName": "Gordon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19585,
      "firstName": "Alireza",
      "lastName": "Karduni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11906,
      "firstName": "Neda",
      "lastName": "Jahanshad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20098,
      "firstName": "Dmitrijs",
      "lastName": "Dmitrenko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10884,
      "firstName": "Bruce",
      "lastName": "Ferwerda",
      "affiliations": []
    },
    {
      "id": 22662,
      "firstName": "Benjamin",
      "lastName": "Watson",
      "affiliations": []
    },
    {
      "id": 9353,
      "firstName": "Jordan",
      "lastName": "Boyd-Graber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21650,
      "firstName": "Keum San",
      "lastName": "Chun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17554,
      "firstName": "Qizhang",
      "lastName": "Sun",
      "middleInitial": "(Kevin)",
      "affiliations": []
    },
    {
      "id": 14995,
      "firstName": "Matevz",
      "lastName": "Pesek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20115,
      "firstName": "Emily",
      "lastName": "Saldanha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21140,
      "firstName": "Wenwen",
      "lastName": "Dou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8853,
      "firstName": "Osnat",
      "lastName": "Mokryn",
      "middleInitial": "(Ossi) ",
      "affiliations": []
    },
    {
      "id": 23702,
      "firstName": "Peng",
      "lastName": "Ren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19094,
      "firstName": "Ludovik",
      "lastName": "Coba",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12438,
      "firstName": "Shlomo",
      "lastName": "Berkovsky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8855,
      "firstName": "Varun",
      "lastName": "Bezzam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22168,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9881,
      "firstName": "Yunfeng",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14493,
      "firstName": "Camille",
      "lastName": "Gobert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8349,
      "firstName": "Pradyumna",
      "lastName": "Tambwekar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16547,
      "firstName": "Julia",
      "lastName": "Campbell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23717,
      "firstName": "Sneha",
      "lastName": "Mehta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22693,
      "firstName": "Granit",
      "lastName": "Luzhnica",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13989,
      "firstName": "Shaoting",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18085,
      "firstName": "Brian",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 8358,
      "firstName": "Fang",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16041,
      "firstName": "Mohammad Rafayet",
      "lastName": "Ali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21674,
      "firstName": "Upol",
      "lastName": "Ehsan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12459,
      "firstName": "Alan",
      "lastName": "Hartman",
      "affiliations": []
    },
    {
      "id": 11948,
      "firstName": "Chris",
      "lastName": "Donahue",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9900,
      "firstName": "Panagiotis",
      "lastName": "Germanakos",
      "affiliations": []
    },
    {
      "id": 14509,
      "firstName": "Sergi",
      "lastName": "Bermúdez i Badia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15533,
      "firstName": "Beatrice",
      "lastName": "Dias",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18607,
      "firstName": "Sarah",
      "lastName": "Völkel",
      "middleInitial": "Theres",
      "affiliations": []
    },
    {
      "id": 22192,
      "firstName": "Veronika ",
      "lastName": "Bogina",
      "affiliations": []
    },
    {
      "id": 16053,
      "firstName": "Ashwin",
      "lastName": "Ram",
      "affiliations": []
    },
    {
      "id": 9914,
      "firstName": "Elizabeth",
      "lastName": "Daly",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 14524,
      "firstName": "Vicente",
      "lastName": "Dominguez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19132,
      "firstName": "Nathawan",
      "lastName": "Charoenkulvanich",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11453,
      "firstName": "Benedikt",
      "lastName": "Lang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20166,
      "firstName": "Maki",
      "lastName": "Sugimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20681,
      "firstName": "Leo",
      "lastName": "Neat",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 19149,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19661,
      "firstName": "Pablo",
      "lastName": "Pedemonte",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 20174,
      "firstName": "Domina",
      "lastName": "Kiunsi",
      "middleInitial": "Robert",
      "affiliations": []
    },
    {
      "id": 14033,
      "firstName": "Mark",
      "lastName": "Riedl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22226,
      "firstName": "Tianjia",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19666,
      "firstName": "Catherine",
      "lastName": "Havasi",
      "affiliations": []
    },
    {
      "id": 16596,
      "firstName": "Enrique",
      "lastName": "Pelaez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15063,
      "firstName": "Illah",
      "lastName": "Nourbakhsh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13528,
      "firstName": "Francisco",
      "lastName": "Gutiérrez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10457,
      "firstName": "Johanne",
      "lastName": "Christensen",
      "affiliations": []
    },
    {
      "id": 15068,
      "firstName": "Yu-Chun",
      "lastName": "Yen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9951,
      "firstName": "Min Hun",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21728,
      "firstName": "Matthew",
      "lastName": "Trimmer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16608,
      "firstName": "Adam",
      "lastName": "Perer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15586,
      "firstName": "Mehdi",
      "lastName": "Elahi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14562,
      "firstName": "Laurens",
      "lastName": "Rook",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13541,
      "firstName": "Pablo",
      "lastName": "Messina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18149,
      "firstName": "Christopher",
      "lastName": "Engen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20199,
      "firstName": "Ranjeet Kumar",
      "lastName": "Tayi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19690,
      "firstName": "Mohit",
      "lastName": "Jain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11499,
      "firstName": "Necole",
      "lastName": "Streeper",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13548,
      "firstName": "Edison",
      "lastName": "Thomaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21740,
      "firstName": "So",
      "lastName": "Kumano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14573,
      "firstName": "Michael",
      "lastName": "Schermann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19694,
      "firstName": "Matthew",
      "lastName": "Bonham",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10991,
      "firstName": "Ingrid",
      "lastName": "Lange",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14575,
      "firstName": "Sven",
      "lastName": "Mayer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22257,
      "firstName": "Katri",
      "lastName": "Leino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20210,
      "firstName": "James",
      "lastName": "Schaffer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16116,
      "firstName": "Vikram",
      "lastName": "Mohanty",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19700,
      "firstName": "Advait",
      "lastName": "Sarkar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13557,
      "firstName": "Shruti",
      "lastName": "Gadewar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13046,
      "firstName": "Prajwal",
      "lastName": "Paudyal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24311,
      "firstName": "Maria Jose",
      "lastName": "Valladares",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18681,
      "firstName": "Ashley",
      "lastName": "Sanders",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15611,
      "firstName": "Markus",
      "lastName": "Schedl",
      "affiliations": []
    },
    {
      "id": 11006,
      "firstName": "Ronnie",
      "lastName": "Taib",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22784,
      "firstName": "Sophia",
      "lastName": "Cook",
      "middleInitial": "Isadora",
      "affiliations": []
    },
    {
      "id": 13057,
      "firstName": "Doris Jung-Lin",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16644,
      "firstName": "An",
      "lastName": "Nguyen",
      "middleInitial": "Thanh",
      "affiliations": []
    },
    {
      "id": 19725,
      "firstName": "Styliani",
      "lastName": "Kleanthous",
      "affiliations": []
    },
    {
      "id": 11024,
      "firstName": "Chien-Ming",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13077,
      "firstName": "Matthew",
      "lastName": "Lease",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20247,
      "firstName": "Sashank",
      "lastName": "Santhanam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21271,
      "firstName": "Danny",
      "lastName": "Lange",
      "affiliations": []
    },
    {
      "id": 18713,
      "firstName": "An",
      "lastName": "Nguyen",
      "middleInitial": "Thanh",
      "affiliations": []
    },
    {
      "id": 20763,
      "firstName": "Yoichi",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 48932,
      "firstName": "Vidya",
      "lastName": "Setlur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21796,
      "firstName": "Narendra Nath",
      "lastName": "Joshi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 48933,
      "firstName": "Marianna",
      "lastName": "Obrist",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10536,
      "firstName": "Gregorio",
      "lastName": "Convertino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22825,
      "firstName": "Steven",
      "lastName": "Sutherland",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8490,
      "firstName": "Rebecca",
      "lastName": "Adaimi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19754,
      "firstName": "Haixun",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18731,
      "firstName": "Qifan",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11567,
      "firstName": "Bhanukiran",
      "lastName": "Vinzamuri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23344,
      "firstName": "Zhenglu",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 23346,
      "firstName": "Adi",
      "lastName": "Botea",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18739,
      "firstName": "James",
      "lastName": "Simpson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14643,
      "firstName": "Lewis",
      "lastName": "Johnson",
      "middleInitial": "W",
      "affiliations": []
    },
    {
      "id": 17205,
      "firstName": "Yuta",
      "lastName": "Itoh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17719,
      "firstName": "Behnam",
      "lastName": "Rahdari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11063,
      "firstName": "Surjya",
      "lastName": "Ghosh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20791,
      "firstName": "Irshad",
      "lastName": "Abibouraguimane",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17210,
      "firstName": "Daniel P.",
      "lastName": "Siewiorek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19259,
      "firstName": "Ioannis",
      "lastName": "Politis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23355,
      "firstName": "Nancy",
      "lastName": "Kamel",
      "affiliations": []
    },
    {
      "id": 16703,
      "firstName": "Rohan",
      "lastName": "Kumar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23873,
      "firstName": "David",
      "lastName": "Thames",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9538,
      "firstName": "James",
      "lastName": "Abello",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12098,
      "firstName": "Jonas",
      "lastName": "Jongejan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23362,
      "firstName": "Thomas",
      "lastName": "Köhn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15171,
      "firstName": "Mikko",
      "lastName": "Kurimo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14660,
      "firstName": "Nitish",
      "lastName": "Bansal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20808,
      "firstName": "Sebastien",
      "lastName": "Lalle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18248,
      "firstName": "James",
      "lastName": "Honaker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12618,
      "firstName": "Ivania",
      "lastName": "Donoso-Guzmán",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22346,
      "firstName": "Kartik",
      "lastName": "Talamadupula",
      "affiliations": []
    },
    {
      "id": 9040,
      "firstName": "Zhou",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 15697,
      "firstName": "Rachel",
      "lastName": "Bellamy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14162,
      "firstName": "Matija",
      "lastName": "Marolt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14164,
      "firstName": "Bivas",
      "lastName": "Mitra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17238,
      "firstName": "Nima",
      "lastName": "Maleki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23385,
      "firstName": "Martijn",
      "lastName": "Millecamp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20313,
      "firstName": "Roberto",
      "lastName": "Manduchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17241,
      "firstName": "Melanie",
      "lastName": "Tory",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21338,
      "firstName": "Ricardo ",
      "lastName": "Campos",
      "affiliations": []
    },
    {
      "id": 10075,
      "firstName": "Shunichi",
      "lastName": "Tahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14171,
      "firstName": " Martijn",
      "lastName": "Williemsen",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 13148,
      "firstName": "Lukas",
      "lastName": "Eberhard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13661,
      "firstName": "Fabio",
      "lastName": "Catania",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11103,
      "firstName": "Jason",
      "lastName": "Corso",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 12130,
      "firstName": "Tooba",
      "lastName": "Ahsen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18276,
      "firstName": "Josef ",
      "lastName": "Spjut",
      "affiliations": []
    },
    {
      "id": 19814,
      "firstName": "Jess",
      "lastName": "Holbrook",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23398,
      "firstName": "Aidong",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16231,
      "firstName": "Abel",
      "lastName": "Valente",
      "middleInitial": "N.",
      "affiliations": []
    },
    {
      "id": 13160,
      "firstName": "Theocharis",
      "lastName": "Amanatidis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20840,
      "firstName": "Daniel",
      "lastName": "Garijo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22890,
      "firstName": "Dustin",
      "lastName": "Arendt",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 21358,
      "firstName": "Jeffrey",
      "lastName": "Bevington",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14198,
      "firstName": "Eytan",
      "lastName": "Adar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19318,
      "firstName": "Dorota",
      "lastName": "Glowacka",
      "affiliations": []
    },
    {
      "id": 13179,
      "firstName": "Toyokazu",
      "lastName": "Akiyama",
      "affiliations": []
    },
    {
      "id": 16765,
      "firstName": "Byron",
      "lastName": "Wallace",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 17790,
      "firstName": "Chun-Hua",
      "lastName": "Tsai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21887,
      "firstName": "Aditya",
      "lastName": "Parameswaran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12161,
      "firstName": "Meraj Ahmed",
      "lastName": "Khan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17286,
      "firstName": "Jennifer",
      "lastName": "Cross",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 13702,
      "firstName": "Yolanda",
      "lastName": "Gil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13195,
      "firstName": "Blake",
      "lastName": "Williford",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9614,
      "firstName": "Pigi",
      "lastName": "Kouki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22414,
      "firstName": "Svitlana",
      "lastName": "Volkova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16785,
      "firstName": "Samuel",
      "lastName": "Sohn",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 23954,
      "firstName": "Mubbasir",
      "lastName": "Kapadia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19858,
      "firstName": "Tugba",
      "lastName": "Kulahcioglu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10136,
      "firstName": "Martijn",
      "lastName": "Willemsen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16280,
      "firstName": "Larry",
      "lastName": "Chan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13211,
      "firstName": "Pranav",
      "lastName": "Maneriker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15262,
      "firstName": "Zhongyu",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15775,
      "firstName": "Steve",
      "lastName": "Whittaker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11169,
      "firstName": "Yangyang",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13729,
      "firstName": "Isaac",
      "lastName": "Cho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23461,
      "firstName": "Huy Viet",
      "lastName": "Le",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13222,
      "firstName": "Casey",
      "lastName": "Dougan",
      "affiliations": []
    },
    {
      "id": 23975,
      "firstName": "Jahna",
      "lastName": "Otterbacher",
      "affiliations": []
    },
    {
      "id": 23977,
      "firstName": "Alison",
      "lastName": "Smith-Renner",
      "affiliations": []
    },
    {
      "id": 21418,
      "firstName": "Shi",
      "lastName": "Feng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15276,
      "firstName": "Carlos",
      "lastName": "Muniz",
      "middleInitial": "Manuel",
      "affiliations": []
    },
    {
      "id": 21933,
      "firstName": "Robin",
      "lastName": "Richter",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 11181,
      "firstName": "Styliani Kleanthous",
      "lastName": "Loizou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19885,
      "firstName": "Niloy",
      "lastName": "Ganguly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18862,
      "firstName": "Farshid Hassani",
      "lastName": "Bijarbooneh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13230,
      "firstName": "Ting-Hao",
      "lastName": "Huang",
      "middleInitial": "Kenneth",
      "affiliations": []
    },
    {
      "id": 17838,
      "firstName": "Shiyan",
      "lastName": "Yan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12719,
      "firstName": "Katie",
      "lastName": "Watson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9650,
      "firstName": "David E",
      "lastName": "Conroy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15795,
      "firstName": "Maria",
      "lastName": "Valladares",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12724,
      "firstName": "Pan",
      "lastName": "Hui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25020,
      "firstName": "Jean",
      "lastName": "Song",
      "middleInitial": "Y",
      "affiliations": []
    },
    {
      "id": 18876,
      "firstName": " Alfred",
      "lastName": "Kobsa",
      "affiliations": []
    },
    {
      "id": 25021,
      "firstName": "Wayne",
      "lastName": "Fu",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 15293,
      "firstName": "Paul",
      "lastName": "Dille",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8637,
      "firstName": "Yoshimasa",
      "lastName": "Ohmoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23485,
      "firstName": "Stacy",
      "lastName": "Joines",
      "affiliations": []
    },
    {
      "id": 25022,
      "firstName": "Niels",
      "lastName": "Henze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8639,
      "firstName": "Siyang",
      "lastName": "Qin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25023,
      "firstName": "Dong",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20928,
      "firstName": "Balaji Vasan",
      "lastName": "Srinivasan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25024,
      "firstName": "Aniket",
      "lastName": "Kittur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9153,
      "firstName": "Malin",
      "lastName": "Eiband",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25025,
      "firstName": "Run",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25026,
      "firstName": "Michelle",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25027,
      "firstName": "Carrie",
      "lastName": "Cai",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 25028,
      "firstName": "Justin",
      "lastName": "Weisz",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 9668,
      "firstName": "Yibo",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19909,
      "firstName": "Shudan",
      "lastName": "Zhong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25029,
      "firstName": "Michael Xieyang",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10693,
      "firstName": "Himel",
      "lastName": "Dev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25030,
      "firstName": "Tianyi",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25031,
      "firstName": "Mira",
      "lastName": "Dontcheva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23496,
      "firstName": "Asim",
      "lastName": "Smailagic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12744,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25032,
      "firstName": "Christine",
      "lastName": "Wolf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25033,
      "firstName": "Toby",
      "lastName": "Li",
      "middleInitial": "Jia-Jun",
      "affiliations": []
    },
    {
      "id": 13257,
      "firstName": "Joseph Chee",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25034,
      "firstName": "Denis",
      "lastName": "Helic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25035,
      "firstName": "Charlie",
      "lastName": "Hewitt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25036,
      "firstName": "Kurt",
      "lastName": "Luther",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18380,
      "firstName": "Andrew",
      "lastName": "Jones",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25037,
      "firstName": "Kai",
      "lastName": "Holländer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12749,
      "firstName": "Hazem",
      "lastName": "Elmeleegy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25038,
      "firstName": "Qian",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25039,
      "firstName": "Casey",
      "lastName": "Dugan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25040,
      "firstName": "David",
      "lastName": "Traum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25041,
      "firstName": "Toyoaki",
      "lastName": "Nishida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25042,
      "firstName": "Tobias",
      "lastName": "Höllerer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25043,
      "firstName": "Nathan",
      "lastName": "Hahn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20947,
      "firstName": "Mark",
      "lastName": "Graus",
      "middleInitial": "P.",
      "affiliations": []
    },
    {
      "id": 18388,
      "firstName": "Elena",
      "lastName": "Matsi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25044,
      "firstName": "Wallace",
      "lastName": "Lages",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 25045,
      "firstName": "Juho",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15318,
      "firstName": "Kakeru",
      "lastName": "Hagihara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25046,
      "firstName": "Clemens",
      "lastName": "Schartmüller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22999,
      "firstName": "Rie",
      "lastName": "Kamikubo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25047,
      "firstName": "Anna-Katharina",
      "lastName": "Frison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25048,
      "firstName": "Heinrich",
      "lastName": "Hussmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10200,
      "firstName": "Francesco",
      "lastName": "Ricci",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25049,
      "firstName": "Ziang",
      "lastName": "Xiao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13786,
      "firstName": "Gerard",
      "lastName": "de Melo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25050,
      "firstName": "Philipp",
      "lastName": "Wintersberger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25051,
      "firstName": "Andreas",
      "lastName": "Riener",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25052,
      "firstName": "Doug",
      "lastName": "Bowman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21468,
      "firstName": "Adrienne",
      "lastName": "Raglin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25053,
      "firstName": "Q. Vera",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15326,
      "firstName": "Michelle",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 14302,
      "firstName": "Duen Horng",
      "lastName": "Chau",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25054,
      "firstName": "Gilles",
      "lastName": "Bailly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 25055,
      "firstName": "Walter",
      "lastName": "Lasecki",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 9696,
      "firstName": "Panagiotis",
      "lastName": "Symeonidis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13794,
      "firstName": "Jonathan",
      "lastName": "Dodge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11746,
      "firstName": "Yuxiang",
      "lastName": "Gao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12259,
      "firstName": "Juhee",
      "lastName": "Bae",
      "affiliations": []
    },
    {
      "id": 19428,
      "firstName": "Alex",
      "lastName": "Djalali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24036,
      "firstName": "Denis",
      "lastName": "Parra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 18404,
      "firstName": "Willis",
      "lastName": "Fulmer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22501,
      "firstName": "Johnny",
      "lastName": "Torres",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16869,
      "firstName": "Jianlong",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10727,
      "firstName": "Keiichiro",
      "lastName": "Hoashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12265,
      "firstName": "Seth",
      "lastName": "Walker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11756,
      "firstName": "James",
      "lastName": "Johnson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15853,
      "firstName": "Ryan",
      "lastName": "Wesslen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23024,
      "firstName": "Rebecca",
      "lastName": "Fiebrink",
      "affiliations": []
    },
    {
      "id": 8691,
      "firstName": "André",
      "lastName": "Freitas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 16883,
      "firstName": "Paridhi",
      "lastName": "Maheshwari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23539,
      "firstName": "Tsung-Hsien",
      "lastName": "Wen",
      "middleInitial": "(Shawn)",
      "affiliations": []
    },
    {
      "id": 23540,
      "firstName": "Randy",
      "lastName": "Sargent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 20471,
      "firstName": "Sixto",
      "lastName": "Garcia",
      "middleInitial": "E",
      "affiliations": []
    },
    {
      "id": 14328,
      "firstName": "Tetsu",
      "lastName": "Hayashida",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19964,
      "firstName": "Viktor",
      "lastName": "Schlegel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 14333,
      "firstName": "Yen-Chia",
      "lastName": "Hsu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 23037,
      "firstName": "Shima",
      "lastName": "Kazerooni",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 18,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 07:46:57+00"
  }
}