{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10043,
    "startDate": 1584921600000,
    "endDate": 1585180800000,
    "shortName": "HRI",
    "name": "HRI 2020",
    "year": 2020,
    "fullName": "15th Annual ACM/IEEE International Conference on Human Robot Interaction",
    "url": "https://humanrobotinteraction.org/2020/",
    "location": "Cambridge, UK",
    "timeZoneOffset": 0,
    "logoUrl": "https://files.sigchi.org/conference/logo/86a1f7b8-9246-8e31-3d13-11cffc50ca8c.png",
    "timeZoneName": "Europe/London"
  },
  "sponsors": [
    {
      "id": 10095,
      "name": "Cambridge Consultants",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/f3f8b1d1-6f07-016d-a4a6-29244f8b1d0a.png",
      "levelId": 10075,
      "order": 2
    },
    {
      "id": 10096,
      "name": "arm",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/20f03292-f59d-36a2-301e-d42c6cbbe9df.png",
      "levelId": 10075,
      "order": 1
    },
    {
      "id": 10102,
      "name": "Robotics",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/cef887e0-387c-1c0f-7162-a79bcac4aaed.png",
      "levelId": 10076,
      "order": 7
    },
    {
      "id": 10104,
      "name": "EXG",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/7e24b9cf-03d7-78f2-a527-c119a70a3bad.png",
      "levelId": 10076,
      "order": 3
    },
    {
      "id": 10107,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/6186583b-db6d-c0e9-8fb8-2d7784708057.png",
      "levelId": 10076,
      "order": 4
    },
    {
      "id": 10100,
      "name": "Honda Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0067e265-168b-614c-9cf0-456ee4ede943.png",
      "levelId": 10076,
      "order": 5
    },
    {
      "id": 10108,
      "name": "ID Lab",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/69410b98-61f9-696f-1141-f942399d77b0.png",
      "levelId": 10076,
      "order": 6
    },
    {
      "id": 10109,
      "name": "Microsoft",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/546ee0db-91bd-dade-595d-bd7ed31c0237.png",
      "levelId": 10077,
      "order": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10077,
      "name": "Pioneers Workshop Sponsor",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10074,
      "name": "Platinum Supporter",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10075,
      "name": "Silver Supporter",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10076,
      "name": "Bronze Supporter",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10064,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [],
  "tracks": [
    {
      "id": 10965,
      "typeId": 11519
    },
    {
      "id": 10966,
      "typeId": 11522
    },
    {
      "id": 10951,
      "name": "HRI 2020 Full Papers",
      "typeId": 11521
    },
    {
      "id": 10949,
      "name": "HRI 2020 Workshop and Tutorial",
      "typeId": 11523
    },
    {
      "id": 10950,
      "name": "HRI 2020 Pioneers Workshop",
      "typeId": 11523
    },
    {
      "id": 10947,
      "name": "HRI 2020 Demonstrations",
      "typeId": 11596
    },
    {
      "id": 10946,
      "name": "HRI 2020 Student Design Competition",
      "typeId": 11523
    },
    {
      "id": 10952,
      "name": "HRI 2020 Late-Breaking Reports",
      "typeId": 11597
    },
    {
      "id": 10948,
      "name": "HRI 2020 Videos",
      "typeId": 11598
    },
    {
      "id": 10945,
      "name": "HRI 2020 alt.HRI",
      "typeId": 11599
    }
  ],
  "contentTypes": [
    {
      "id": 11517,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11514,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 0
    },
    {
      "id": 11515,
      "name": "Case Study",
      "color": "#993404",
      "duration": 0,
      "displayName": "Case Studies"
    },
    {
      "id": 11516,
      "name": "Course",
      "color": "#e6550d",
      "duration": 0,
      "displayName": "Courses"
    },
    {
      "id": 11518,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 0,
      "displayName": "Invited Talks"
    },
    {
      "id": 11519,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 0
    },
    {
      "id": 11520,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 0,
      "displayName": "Panels"
    },
    {
      "id": 11521,
      "name": "Paper",
      "color": "#08519c",
      "duration": 0,
      "displayName": "Papers"
    },
    {
      "id": 11522,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 0
    },
    {
      "id": 11523,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 0,
      "displayName": "Workshops"
    },
    {
      "id": 11596,
      "name": "Demos",
      "color": "#969696",
      "duration": 0
    },
    {
      "id": 11597,
      "name": "Late Breaking",
      "color": "#969696",
      "duration": 0
    },
    {
      "id": 11598,
      "name": "Videos",
      "color": "#969696",
      "duration": 0
    },
    {
      "id": 11599,
      "name": "alt.HRI",
      "color": "#969696",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 11396,
      "type": "SESSION",
      "startDate": 1585040400000,
      "endDate": 1585044000000
    },
    {
      "id": 11397,
      "type": "SESSION",
      "startDate": 1585044000000,
      "endDate": 1585047600000
    },
    {
      "id": 11398,
      "type": "SESSION",
      "startDate": 1585047600000,
      "endDate": 1585051200000
    },
    {
      "id": 11399,
      "type": "SESSION",
      "startDate": 1585051200000,
      "endDate": 1585054800000
    },
    {
      "id": 11400,
      "type": "SESSION",
      "startDate": 1585036800000,
      "endDate": 1585040400000
    }
  ],
  "sessions": [
    {
      "id": 38358,
      "name": "Demonstrations",
      "typeId": 11596,
      "chairIds": [],
      "contentIds": [
        38197,
        38200,
        38209,
        38208,
        38210,
        38213,
        38196,
        38202,
        38211,
        38193,
        38212,
        38199
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38349,
      "name": "Session 4: Human Behavior Analysis",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38143,
        38235,
        38293,
        38206
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38356,
      "name": "Session 4: Perceiving Robots",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38166,
        38121,
        38095,
        38292
      ],
      "timeSlotId": 11398
    },
    {
      "id": 38352,
      "name": "Session 5: Child Robot Interaction",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38185,
        38262,
        38165,
        38288,
        38214,
        38275
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38353,
      "name": "Session 3: Social Norms",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38303,
        38246,
        38132,
        38236,
        38090,
        38228
      ],
      "timeSlotId": 11398
    },
    {
      "id": 38360,
      "name": "Pioneers Workshop",
      "typeId": 11523,
      "chairIds": [],
      "contentIds": [
        38116,
        38285,
        38241,
        38308,
        38277,
        38270,
        38040,
        38248,
        38296,
        38110,
        38106,
        38029,
        38113,
        38051,
        38036,
        38307,
        38069,
        38295,
        38027,
        38068,
        38217,
        38306
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38341,
      "name": "Keynote 1",
      "typeId": 11522,
      "chairIds": [],
      "contentIds": [
        38387
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38372,
      "name": "Student Design Competition",
      "typeId": 11523,
      "chairIds": [],
      "contentIds": [
        38076,
        38141,
        38077,
        38183,
        38162,
        38119,
        38140,
        38118,
        38182,
        38117,
        38397,
        38401
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38362,
      "name": "Late Breaking Reports",
      "typeId": 11597,
      "chairIds": [],
      "contentIds": [
        38024,
        38047,
        38137,
        38290,
        38136,
        38300,
        38066,
        38170,
        38157,
        38098,
        38044,
        38265,
        38256,
        38108,
        38266,
        38050,
        38026,
        38271,
        38041,
        38251,
        38302,
        38276,
        38058,
        38088,
        38151,
        38274,
        38278,
        38071,
        38175,
        38061,
        38130,
        38109,
        38138,
        38054,
        38056,
        38060,
        38087,
        38257,
        38172,
        38218,
        38223,
        38124,
        38169,
        38045,
        38038,
        38112,
        38177,
        38062,
        38122,
        38021,
        38310,
        38238,
        38043,
        38153,
        38281,
        38039,
        38042,
        38037,
        38156,
        38291,
        38115,
        38025,
        38019,
        38171,
        38283,
        38258,
        38120,
        38147,
        38065,
        38167,
        38158,
        38179,
        38152,
        38294,
        38063,
        38032,
        38299,
        38089,
        38033,
        38148,
        38072,
        38159,
        38034,
        38178,
        38080,
        38028,
        38252,
        38253,
        38057,
        38114,
        38243,
        38070,
        38022,
        38304,
        38279,
        38078,
        38133,
        38126,
        38242,
        38222,
        38129,
        38173,
        38287,
        38035,
        38305,
        38049,
        38083,
        38079,
        38125,
        38268,
        38289,
        38155,
        38263,
        38059,
        38085,
        38107,
        38135,
        38168,
        38272,
        38103,
        38030,
        38219,
        38267,
        38093,
        38064,
        38301,
        38282,
        38146,
        38053,
        38309,
        38067,
        38097,
        38250,
        38101,
        38092,
        38096,
        38086,
        38123,
        38128,
        38255,
        38102,
        38284,
        38154,
        38105,
        38254,
        38220,
        38273,
        38134,
        38240,
        38127,
        38031,
        38081,
        38237,
        38150,
        38082,
        38023,
        38180,
        38084,
        38104,
        38249
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38342,
      "name": "Session 1: Trust",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38186,
        38131,
        38269,
        38176,
        38233,
        38091
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38373,
      "name": "Videos",
      "typeId": 11598,
      "chairIds": [],
      "contentIds": [
        38330,
        38331,
        38337,
        38332,
        38329,
        38336,
        38333,
        38335,
        38334
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38344,
      "name": "Session 3: Reproducibility and HRI",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38099,
        38205,
        38207,
        38230,
        38229
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38343,
      "name": "Session 2: Robots in the Real World & Longitudinal HRI",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38298,
        38074,
        38225,
        38261
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38359,
      "name": "alt.HRI",
      "typeId": 11599,
      "chairIds": [],
      "contentIds": [
        38055,
        38181,
        38174,
        38194,
        38189,
        38191,
        38161,
        38149
      ],
      "timeSlotId": 11399
    },
    {
      "id": 38345,
      "name": "Session 4: Health and Wellness",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38145,
        38163,
        38280,
        38142
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38347,
      "name": "Keynote 2",
      "typeId": 11522,
      "chairIds": [],
      "contentIds": [
        38388
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38350,
      "name": "Session 2: Morphology",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38046,
        38144,
        38215
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38346,
      "name": "Session 5: Prosocial and Antisocial Interaction",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38221,
        38160,
        38094
      ],
      "timeSlotId": 11396
    },
    {
      "id": 38361,
      "name": "Session 1: Robots and the Performing Arts",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38100,
        38231,
        38204,
        38048
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38354,
      "name": "Session 2: Telepresence",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38052,
        38075,
        38190
      ],
      "timeSlotId": 11398
    },
    {
      "id": 38357,
      "name": "Session 5: Learning and Inference",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38073,
        38020,
        38297,
        38234
      ],
      "timeSlotId": 11398
    },
    {
      "id": 38380,
      "name": "Welcome",
      "typeId": 11519,
      "chairIds": [],
      "contentIds": [
        38377,
        38392
      ],
      "timeSlotId": 11400
    },
    {
      "id": 38348,
      "name": "Keynote 3",
      "typeId": 11522,
      "chairIds": [],
      "contentIds": [
        38386
      ],
      "timeSlotId": 11398
    },
    {
      "id": 38351,
      "name": "Session 3: Groups and Teams",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38164,
        38239,
        38286,
        38264,
        38111,
        38232
      ],
      "timeSlotId": 11397
    },
    {
      "id": 38355,
      "name": "Session 1: Affect",
      "typeId": 11521,
      "chairIds": [],
      "contentIds": [
        38187,
        38188,
        38260,
        38216
      ],
      "timeSlotId": 11398
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 38401,
      "typeId": 11523,
      "title": "Physio Bot",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The Physio Bot is a robot that is specifically design to assist users recovering from physical strain or stroke as well as the elderly to train their muscles and brain-nerve signals by providing simple physiotherapy exercises on a daily basis. This robot help individuals to return to their prior level of functioning through assisted repetitive training which requires the user to achieve set target. By moving the muscles according to the brain stimuli that requires the muscle movement as response, the link between the nerves and the brains can be rehabilitated at a quicker pace whilst training the muscles to respond effectively. The Physio bot will contribute to quicken the rehabilitation process as well as allowing for a more versatile system to be used at any location.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Putrajaya",
              "institution": "Sekolah Sultan Alam Shah Putrajaya",
              "dsl": "Sekolah Sultan Alam Shah Putrajaya"
            },
            {
              "country": "Malaysia",
              "state": "",
              "city": "Putrajaya",
              "institution": "Sekolah Sultan Alam Shah Putrajaya",
              "dsl": "Sekolah Sultan Alam Shah Putrajaya"
            }
          ],
          "personId": 38399
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Putrajaya",
              "institution": "Sekolah Sultan Alam Shah",
              "dsl": "Sekolah Sultan Alam Shah"
            },
            {
              "country": "Malaysia",
              "state": "",
              "city": "Putrajaya",
              "institution": "Sekolah Sultan Alam Shah",
              "dsl": "Sekolah Sultan Alam Shah"
            }
          ],
          "personId": 38398
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Putrajaya",
              "institution": "Sekolak Sultan Alam Shah",
              "dsl": ""
            }
          ],
          "personId": 38400
        }
      ],
      "addons": {
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Y1RD4PpNcy8",
          "title": "Physio Bot: When Robot Meet Physiotherapy",
          "duration": 319,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38019,
      "typeId": 11597,
      "title": "Toward Robot Co-Labourers for Intelligent Farming",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents the results of preliminary experiments in human-robot collaboration for an agricultural task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 37387
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "University of Sheffield",
              "dsl": ""
            }
          ],
          "personId": 37391
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 37820
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 37586
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 37850
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Informatics"
            }
          ],
          "personId": 37353
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Informatics"
            }
          ],
          "personId": 37992
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": ""
            }
          ],
          "personId": 37393
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": ""
            }
          ],
          "personId": 37577
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378333"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=zfBDcD3EN0k",
          "title": "Toward Robot Co-Labourers for Intelligent Farming",
          "duration": 119,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38020,
      "typeId": 11521,
      "title": "See What I See: Enabling User-Centric Robotic Assistance Using First-Person Demonstrations",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We explore first-person demonstration as an intuitive way of producing task demonstrations to facilitate user-centric robotic assistance. First-person demonstration directly captures the human experience of task performance via head-mounted cameras and naturally includes productive viewpoints for task actions. We implemented a perception system that parses natural first-person demonstrations into task models consisting of sequential task procedures, spatial configurations, and unique task viewpoints. We also developed a robotic system capable of interacting autonomously with users as it follows previously acquired task demonstrations. To evaluate the effectiveness of our robotic assistance, we conducted a user study contextualized in an assembly scenario; we sought to determine how assistance based on a first-person demonstration (user-centric assistance) versus that informed only by the cover image of the official assembly instruction (standard assistance) may shape users’ behaviors and overall experience when working alongside a collaborative robot. Our results show that participants felt that their robot partner was more collaborative and considerate when it provided user-centric assistance than when it offered only standard assistance. Additionally, participants were more likely to exhibit unproductive behaviors, such as using their non-dominant hand, when performing the assembly task without user-centric assistance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 37959
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 38006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37732
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374820"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=IOwl3GTZI7k",
          "title": "See What I See: Enabling User-Centric Robotic Assistance Using First-Person Demonstrations",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38357
      ],
      "eventIds": []
    },
    {
      "id": 38021,
      "typeId": 11597,
      "title": "Using Image Boards to Analyze A Series of Cyborg Design Sketches",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present the main conclusions from an analysis of a cyborg design session in which sketches were created to brainstorm novel cyborg applications. We take these sketches and analyze them using image boards, which are a collage of pictures used to communicate a description of design aesthetics and intent. We used this technique to derive patterns and conclusions from the large set of sketches, which can inform the design of future cyborg applications. The sketches reveal that wearable arms are useful for reducing physical strain and multi-tasking, but should incorporate a ”rest” mode in full-time applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Interactions Lab"
            }
          ],
          "personId": 38007
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Victoria",
              "institution": "University of Victoria",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37637
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computational Media Design"
            }
          ],
          "personId": 37468
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37172
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378244"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=sGA7wtVQt1U",
          "title": "Using Image Boards to Analyze A Series of Cyborg Design Sketches",
          "duration": 133,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38022,
      "typeId": 11597,
      "title": "Cheating with a Socially Assistive Robot? A Matter of Personality",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Socially assistive robots might improve the quality of life of individuals by carrying out therapeutic interventions. However, when users try to cheat with robots by disregarding their recommendations, they might not be able to perform their supporting functions. In the present study, we aimed to evaluate how the robot behavior style could affect the users’ compliance and cheating behavior. Sixty volunteers underwent neuro-psychological testing administered by Pepper that was configured as neutral, friendly, or authoritarian. The results revealed that the robot characterized by neutral behavioral style seems to reduce individuals’ compliance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Caserta",
              "institution": "University of Campania \"Luigi Vanvitelli\" ",
              "dsl": ""
            }
          ],
          "personId": 37683
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": ""
            }
          ],
          "personId": 37614
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 37973
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 37258
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378334"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=2SN3YGh3Y_U",
          "title": "Cheating with a Socially Assistive Robot? A Matter of Personality",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38023,
      "typeId": 11597,
      "title": "A Software Framework to Create Behaviors for Androids and Its Implementation on the Mobile Android ``ibuki''",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Androids–humanoids with a human-like appearance are studied in limited communities but draw considerable attention. Considering an android integrates numbers of actuators, sensors and devices into a system, a standardized framework is beneficial to easy-move and extension of potential applications. In this report, we developed a ROS-based software framework for androids. With the framework,intuitively generating, blending and reusing the android motion is aimed to be realized. Some software applications are developed under this framework and they are implemented on a child-like android “ibuki”. Several android behaviors are created by implementing this framework on ibuki.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37479
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37876
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37252
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37302
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37414
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37392
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate school of engineering science"
            },
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate school of engineering science"
            }
          ],
          "personId": 37202
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 37995
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378245"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=WX2_vF4Gt9U",
          "title": "A Software Framework to Create Behaviors for Androids and Its Implementation on the Mobile Android ``ibuki''",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38024,
      "typeId": 11597,
      "title": "Crowd of Oz: A Crowd-powered Teleoperation System for Enhanced Human-Robot Conversations",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present Crowd of Oz (CoZ) -- a crowd powered system to enable real time dialogue between a Pepper robot and a user. CoZ includes media rich interfaces and advanced architecture to swiftly elicit crowd responses for the Pepper robot. We have empirically demonstrated that users can be engaged in a beneficial discussion with the crowd-powered robot, that acts as a coach to mitigate stress.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            },
            {
              "country": "Pakistan",
              "state": "-Select-",
              "city": "Mirpur AJK",
              "institution": "Mirpur University of Science & Technology",
              "dsl": "Software Engineering Dept"
            }
          ],
          "personId": 37326
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37891
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Lower Saxony",
              "city": "Hannover",
              "institution": "Leibniz University of Hannover",
              "dsl": "L3S Research Center"
            }
          ],
          "personId": 37377
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of technology",
              "dsl": "Social Robotics Lab, Industrial Design dept."
            }
          ],
          "personId": 37164
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37763
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378335"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4oWaIOU-i8o",
          "title": "Crowd of Oz: A Crowd-powered Teleoperation System for Enhanced Human-Robot Conversations",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38025,
      "typeId": 11597,
      "title": "Augmented Reality with Multi-view Merging for Robot Teleoperation",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper proposes a user-friendly teleoperation interface for manipulation. We provide the user with a view of the scene augmented with depth information from local cameras to provide visibility in occluded areas during manipulation tasks.\r\nThis gives an improved sense of the 3D environment which results in better task performance. Further, we monitor the pose of the robot's end effector in real-time so that we are able to superimpose a virtual representation into the scene when parts are occluded. The integration of these features enables the user to perform difficult manipulation tasks when the action environment is normally occluded in the main camera view. \r\n\r\nWe performed preliminary studies with this new setup and users provided positive feedback regarding the proposed augmented reality (AR) system",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tencent Robotics X",
              "dsl": ""
            }
          ],
          "personId": 37420
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Cambridge University",
              "dsl": ""
            }
          ],
          "personId": 38011
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Tencent Robotics X",
              "dsl": ""
            }
          ],
          "personId": 38369
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378336"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38026,
      "typeId": 11597,
      "title": "Becoming in touch with industrial robots through ethnography ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Touch is central to communication and social interaction. For both humans and robots touch is a mode through which they sense the world. A second wave of industrial robots is reshaping how touch operates within the labor process. Recent studies have turned their attention to the role of touch in Human-Robot Interaction (HRI). While these studies have produced useful knowledge in relation to the affective capacities of robotic touch, methods remain restrictive. This paper contributes to expanding research methods for the study of robotic touch. It reports on the design of an ongoing ethnography that forms part of the InTouch project. The interdisciplinary project takes forward a socially orientated stance and is concerned with how technologies shape the semiotic and sensory dimensions of touch in the ‘real world’. We contend that these dimensions are key factors in shaping how humans and robots interact, yet are currently overlooked in the HRI community. This multi-sited sensory ethnography research has been designed to explore the social implications of robotic touch within industrial settings (e.g. manufacturing and construction). ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London ",
              "dsl": ""
            }
          ],
          "personId": 37293
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "London",
              "city": "London",
              "institution": "UCL Institute of Education",
              "dsl": "UCL Knowledge Lab"
            }
          ],
          "personId": 37799
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL",
              "dsl": "UCL Knowledge Lab"
            }
          ],
          "personId": 37230
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378246"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38027,
      "typeId": 11523,
      "title": "Interactive Robot Training for Temporal Tasks",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 37475
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377437"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=ndPezio90Q8",
          "title": "Interactive Robot Training for Temporal Tasks",
          "duration": 198,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38028,
      "typeId": 11597,
      "title": "Joint Attention Estimator",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Joint attention has been identified as a critical component of successful human machine teams. Teaching robots to develop awareness of human cues is an important first step towards attaining and maintaining joint attention. We present a joint attention estimator that creates many possible candidates for joint attention and chooses the most likely object based on a human teammate’s hand cues. Our system works within natural human interaction time (< 3 seconds) and above 80% accuracy. Our joint attention estimator provides a meaningful step towards ensuring robots enable human social skills for successful human machine teaming.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "Naval Research Laboratory",
              "dsl": "AI Center"
            }
          ],
          "personId": 37862
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "US Naval Research Lab",
              "dsl": ""
            }
          ],
          "personId": 37407
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "US Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 38366
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37950
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378247"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=TQ-4zHRCJj4",
          "title": "Joint Attention Estimator",
          "duration": 116,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38029,
      "typeId": 11523,
      "title": "Ludic-HRI: Designing Playful Experiences with Robots",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "People are inherently playful, and playfulness matters not only when engaging in actual play but also in all other activities. Based on this, I propose using a ludic design approach as a means to broaden the design space for Human-Robot Interaction (HRI). In this paper, I discuss the application of ludic design in HRI and explore how ludic activities can act as a mechanism for achieving new understandings from people during their interactions with robots. Two projects, BubbleBot and Sketching Robot, are presented as cases of designing ludic activities with robots. In my work, I have investigated how people perceived robots by applying exploratory research through design methods in creating the ludic experiences.I am continuously learning from my design and exploring potential areas for designing robots while identifying new values and goals for having robots in our lives.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Robots in Groups Lab"
            }
          ],
          "personId": 37788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37663
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377429"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=P0oCU8L2J08",
          "title": "Ludic-HRI: Designing Playful Experiences with Robots",
          "duration": 186,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38030,
      "typeId": 11597,
      "title": "Emergence of Agent Gaze Behavior using Interactive Kinetics-Based Gaze Direction Model",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a model to control an agent's gaze behavior called Interactive Kinetics-Based Gaze Direction Model(iK-Gaze). iK-Gaze is a model used in one-to-one interaction between an agent and a human, where the agent's gaze direction is calculated using an energy function which uses the human's gaze direction as its input. iK-Gaze aims to generate the agent's gaze through interactions with a human rather than using predefined motions and predefined timings. Contrary to rule-based or statistics-based models, the agent's gaze motion changes dynamically according to the human's gaze motion. Moreover, by utilizing three parameters of the desire to look, the mutual gaze hesitation, and the mutual gaze stress in the iK-Gaze, the tendency of the agent's gaze behavior can be changed easily. From the case study, the fact that agent's gaze behavior changes according to the human gaze behavior is confirmed.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University",
              "dsl": "Dept. of Information & Computer Science, Faculty of Science & Technology"
            }
          ],
          "personId": 37607
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama-shi",
              "institution": "Keio University",
              "dsl": "Faculty of Science and Technology"
            }
          ],
          "personId": 37834
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Acroquest Technology Co., Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37868
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology/School of Science for Open and Environmental Systems/Michita Imai's Lab."
            }
          ],
          "personId": 37487
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "ATR",
              "dsl": ""
            }
          ],
          "personId": 37364
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Kouhoku-ku, Yokohama-shi",
              "institution": "3-14-1 Hiyoshi",
              "dsl": "Keio University"
            }
          ],
          "personId": 38014
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378248"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38031,
      "typeId": 11597,
      "title": "Couch to 5km Robot Coach: an Autonomous, Human-Trained Socially Assistive Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present a robot exercise coach, co-designed and then trained in real-time by a human fitness instructor, via interactive machine learning, to support the UK National Health Service (NHS) Couch to 5km (C25K) programme. The programme consists of undertaking 3x weekly exercise sessions for 9 weeks, with sessions building up from a combination of short runs and walks to a full 30 minutes running. The simplicity (and relatively boring nature) of the task places great importance on the ability of the robot, our ‘C25K coach’, to provide engaging and appropriate social supporting behaviour.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37520
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Lab"
            }
          ],
          "personId": 37852
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Faculty of Environment and Technology, UWE",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37410
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "Psychological Science"
            }
          ],
          "personId": 37737
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "HAS - Allied Health Professions"
            }
          ],
          "personId": 37696
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37754
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378337"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38032,
      "typeId": 11597,
      "title": "The Influence of Incremental Information Presentation on the Persuasiveness of a Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we investigate if there is a relationship between incremental information presentation and persuasion, by testing if a robot can persuade a person to perform more tasks than the required ones. We designed a between-subject experiment where the robot Nao would ask the participant to perform a series of tasks. Our results show no significant difference between incremental and non-incremental conditions in Nao's ability to persuade participants, nor in likeability. However, Nao was able to get participants to stay longer than they intended to.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37210
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37704
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37769
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37556
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37255
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "SDU",
              "dsl": ""
            }
          ],
          "personId": 37263
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sønderborg",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37839
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378338"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38033,
      "typeId": 11597,
      "title": "Towards Adaptive and Least-Collaborative-Effort Social Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In the future, assistive social robots will collaborate with humans in a variety of settings. Robots will not only follow human orders but will likely also instruct users during certain tasks. Such robots will inevitably encounter user uncertainty and hesitations. They will continuously need to repair mismatches in common ground in their interactions with humans. In this work, we argue that social robots should instruct humans following the principle of least-collaborative-effort. Like humans do when instructing each other, robots should minimise information efficiency over the benefits of collaborative interactive behaviour. In this paper, we first introduce the concept of least-collaborative-effort in human communication and then discuss implications for design of instructions in human-robot collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37419
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department for Culture and Communication"
            }
          ],
          "personId": 37964
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378249"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Y-mODw1V6vg",
          "title": "Towards Adaptive and Least-Collaborative-Effort Social Robots",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38034,
      "typeId": 11597,
      "title": "I’m Your Partner – I’m Your Boss:  Framing Human-Robot Collaboration with Conceptual Metaphors ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Using speech as an effective communication modality in human-robot interaction (HRI) allows designers to implement conceptual metaphors as a linguistic device. Metaphors appear to instantiate psychological framing effects and can influence the user´s reasoning significantly. The present paper presents two consecutive studies that explore how metaphors can be used in speech output for social robots to deliberately influence the user’s perception of their relationship with the robot. As a first step, metaphoric expressions for framing cooperation or delegation in HRI settings were identified in a workshop. Effects of those metaphors on people´s strategies for organizing tasks in human-robot collaboration were then evaluated in an online survey. We found that metaphors suitable for encouraging users to do a task together with a robot are metaphors like “partners on a journey” or “allies”. In contrast, metaphors that can be used to encourage handing over the task to a robot are metaphors like “boss” or “ruler”. Further research needs to test whether these metaphors also have an effect on users’ actions in HRI settings. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Fraunhofer Institute for Industrial Engineering IAO",
              "dsl": ""
            }
          ],
          "personId": 37166
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Fraunhofer Institute for Industrial Engineering IAO",
              "dsl": ""
            }
          ],
          "personId": 38005
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Stuttgart Media University",
              "dsl": ""
            }
          ],
          "personId": 37766
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378250"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38035,
      "typeId": 11597,
      "title": "A Drink-Serving Mobile Social Robot Selects who to Interact with Using Gaze",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Robots will soon deliver food and beverages in various environments. These robots will need to communicate their intention efficiently, for example, they should indicate who they are addressing. We conducted a real-world study of a water serving robot at a university cafeteria. The robot was operated in a Wizard-of-Oz manner. It approached and offered water to students having their lunch. Our analyses of the relationship between robot gaze direction and the likelihood that someone takes a drink show that if people do not already have a drink and the interaction is not dominated by an overly enthusiastic user, the robot’s gaze behavior is effective in selecting an interaction partner even “in the wild”. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37328
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37336
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37342
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37551
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37500
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378339"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38036,
      "typeId": 11523,
      "title": "Communicating Robot Goals via Haptic Feedback in Manipulation Tasks",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "In shared autonomy, human teleoperation blends with intelligent robot autonomy to create robot control. This combination enables assistive robot manipulators to help human operators by predicting and reaching the human’s desired target. However, this reduces the control authority of the user and the transparency of the interaction. This negatively affects their willingness to use the system. We propose haptic feedback as a seamless and natural way for the robot to communicate information to the user and assist them in completing the task. A proof-of-concept demonstration of our system illustrates the effectiveness of haptic feedback in communicating the robot’s goals to the user. We hypothesize that this can be an effective way to improve performance in teleoperated manipulation tasks, while retaining the control authority of the user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Computer Science"
            }
          ],
          "personId": 38015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37726
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377444"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38037,
      "typeId": 11597,
      "title": "Comparing Laboratory User Studies and Video-Enhanced Web Surveys for Eliciting User Gestures in Human-Robot Interactions",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Laboratory studies are time consuming and costly. We aimed to examine whether the gestures users naturally select to communicate various commands to a mobile robot during a laboratory user study can be comparable to those people selected during an online video-enhanced survey. 64 participants were divided into two experimental groups according to the interaction methodology. In both conditions, participants instructed the robot to perform eight different tasks using only upper body gestures. For 7 of the 8 tasks it was found that the physical gestures by which the participants chose to communicate with the robot did not depend on the type of interaction. Our investigation, while still preliminary, may suggest that video enhanced surveys can be used for human robot interaction design and evaluation, especially in the preliminary stages of defining users' existing mental models and expectations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37250
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37264
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378325"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38038,
      "typeId": 11597,
      "title": "Intuitive Programming with Remotely Instructed Robots inside Future Gloveboxes",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": " Our research aims at facilitating the design of 'Remotely Instructed Robots' for future glove-boxes in the nuclear industry. The two main features of such systems are: (1) They can automatically model the working environment and relay that information to the operator in virtual reality (VR). (2) They can receive instructions from the operator that are executed by the robot. However, the deficiency of these kind of systems is that they heavily rely on knowledge of expert programmers when the robot's capabilities or hardware are to be reconfigured, altered or upgraded. This late breaking report proposes to introduce a third important advancement on remotely instructed robots: (3) Intuitive programming modifications by operators who are non-programmers but have basic knowledge of hardware, and most importantly, have experience of the weaknesses in particular handling tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "south yorkshire",
              "city": "Sheffield",
              "institution": "The University of Sheffield",
              "dsl": "Autonomous and Control Systems Engineering"
            }
          ],
          "personId": 37623
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "The University of Sheffield",
              "dsl": "Autonomous and Control Systems Engineering"
            }
          ],
          "personId": 37547
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "The University of Sheffield",
              "dsl": "Autonomous and Control Systems Engineering"
            }
          ],
          "personId": 37516
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "The University of Sheffield",
              "dsl": "Autonomous and Control Systems Engineering"
            }
          ],
          "personId": 37319
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "The University of Sheffield",
              "dsl": "Autonomous and Control Systems Engineering"
            }
          ],
          "personId": 37729
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378326"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38039,
      "typeId": 11597,
      "title": "Pilot Study for Dynamic Trust Estimation in Human-Robot Collaboration",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper reports on a pilot study for investigating the relation between observable data from users and their trust estimation in Human Robot Collaboration. Two experiments have been set up that contain different situational risks (as one of the pre-requisites for trust investigation). Here we report on one of these. Preliminary results are promising and show a correlation between risk factors, observable behavior and subjective trust estimations. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 37692
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 37801
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            }
          ],
          "personId": 37389
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378327"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38040,
      "typeId": 11523,
      "title": "Design of Automatic Strawberry Harvest Robot Suitable in Complex Environments",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Strawberries are an important cash crop that are grown worldwide. They are also a labour-intensive crop, with harvesting a particularly labour-intensive task because the fruit needs careful handling. This project investigates collaborative human-robot strawberry harvesting, where interacting with a human potentially increases the adaptability of a robot to work in more complex environments. The project mainly concentrates on two aspects of the problem: the identification of the fruit and the picking of the fruit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 37387
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": ""
            }
          ],
          "personId": 37577
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": ""
            }
          ],
          "personId": 37393
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377443"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38041,
      "typeId": 11597,
      "title": "Augmenting Soft Robotics with Sound",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "During the past decade soft robotics has emerged as a growing field of research. In this paper we present exploratory research on sound design for soft robotics with potential applications within the human-robot interaction domain. We conducted an analysis of the sounds made by imaginary soft creatures in movies. Drawing inspiration from the analysis, we designed a soft robotic prototype that features real-time sound generation based on FM synthesis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "SDU Biorobotics"
            }
          ],
          "personId": 37277
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "SDU Biorobotics"
            }
          ],
          "personId": 37510
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378328"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MUgypo6XIQw",
          "title": "Augmenting Soft Robotics with Sound",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38042,
      "typeId": 11597,
      "title": "What a Pity, Pepper! How Warmth in Robots’ Language Impacts Reactions to Errors during a Collaborative Task",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the impact of warmth in robots’ language on the perception of errors in a shopping assistance task (N=81) and found that error-free behavior was favored over erroneous if the dialogue is machine-like, while errors do not negatively impact liking, trust and acceptance if the robot uses human-like language. Warmth in robots’ language thus seems to mitigate negative consequences and should be considered as a crucial design aspect.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "CITEC"
            }
          ],
          "personId": 37865
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Universität Bielefeld ",
              "dsl": ""
            }
          ],
          "personId": 37544
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Cluster of Excellence Cognitive Interaction Technology (CITEC)"
            }
          ],
          "personId": 37187
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378242"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Onduqbrx_G8",
          "title": "What a Pity, Pepper! How Warmth in Robots’ Language Impacts Reactions to Errors during a Collaborative Task",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38043,
      "typeId": 11597,
      "title": "Towards an Interaction-Centered and Dynamically Constructed Episodic Memory for Social Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper outlines an interaction-centered and dynamically constructed episodic memory for social robots, in order to enable naturalistic, social human-robot interaction. The proposed model includes a record of multi-timescale events stored in the event history, a record of multi-timescale interval definitions stored as interaction episodes, and a set of links associating specific elements of the two records. The event history is constructed dynamically, depending on the occurrence of internal and external events. The interaction episodes are defined on the basis of robot-initiated and user-initiated interactions. The episodic memory is realised within a social human-robot interaction architecture, whose components generate events pertaining to the context and state of interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": ""
            }
          ],
          "personId": 37221
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": ""
            }
          ],
          "personId": 37187
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378329"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QKBEfNhLdaM",
          "title": "Towards an Interaction-Centered and Dynamically Constructed Episodic Memory for Social Robots",
          "duration": 126,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38044,
      "typeId": 11597,
      "title": "Creating Robot Personality: Effects of Mixing Speech and Semantic Free Utterances",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Personality is a vital factor in understanding acceptance, trust or emotional attachment to a robot. From R2D2 to WALL-E, there is a rich history of robots in films using semantic free utterances (SFUs) sounds (such as squeaks, clicks and tones) as audio gestures to communicate and convey emotion and personality. However, unlike in a film where an actor can pretend to understand non-verbal noises, in a practical application synthesised speech is often used to communicate information, intention and status. Here we present a pilot study exploring the impact of mixing speech synthesis with SFUs on perceived personality. In a listening test, subjects were presented with synthesised short utterances with and without SFUs, together with a picture of the agent as a tabletop social robot or a young man. Both picture and SFUs had an impact on perceived personality. However, no interaction was seen between SFUs and picture suggesting the listeners failed to fuse the perceptions of the two audio elements, perceiving the SFUs as background noises rather than audio gestures generated by\r\nthe agent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37579
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37713
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37373
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378330"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38045,
      "typeId": 11597,
      "title": "Small Robots With Big Tasks: A Proof of Concept Implementation Using a MiRo for Fall Alert",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "A significant portion of the over 65 population experience a fall at least once a year in their home environment. This puts a faller under significant health risks and adds to the financial burden of the care services. This Late Breaking Report explores a proof of concept implementation of a fall alert system that uses MiRo (a small mobile social robot) in the home environment. We take advantage of MiRo’s pet-like characteristics, small size, mobility, and array of sensors to implement a system where a person who has fallen can interact with it and summon help if needed. The initial aim of this proof of concept system described here was to act as a demonstration tool for health professionals and housing association representatives, gauging their needs and requirements, driving this research forward.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37605
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37838
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37730
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37270
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378331"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38046,
      "typeId": 11521,
      "title": "The Uncanny Valley Effect in Zoomorphic Robots: The U-Shaped Relationship Between Animal-Likeness and Likeability",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The uncanny valley effect denotes a dip in the positive relation between a robot’s human likeness and likeability. This paper provides first evidence that this design-guiding effect is not limited to humanoids, but extends to zoomorphic robots. In a first online survey, a diverse group of 235 participants rated the animal likeness of 140 robots. Three predictors for high or low animal likeness emerged: surface properties, such as joint visibility; facial properties, such as presence of a pupil; animal-specific properties, such as presence of snout. In a second online survey, 187 participants rated the likeability of 53 robots of varying degrees of animal likeness drawn from the first study. The relation between animal likeness and likeability followed a U-shaped function and showed an uncanny valley effect: robots high and low in animal likeness were preferred over those mixing realistic and unrealistic features. Besides theoretical implications tentative guidelines for the design of zoomorphic robots are discussed. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Ubiquitous Design / Experience & Interaction"
            }
          ],
          "personId": 37818
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Ubiquitous Design / Experience & Interaction"
            }
          ],
          "personId": 37674
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Ubiquitous Design / Experience & Interaction"
            }
          ],
          "personId": 37178
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374788"
        }
      },
      "sessionIds": [
        38350
      ],
      "eventIds": []
    },
    {
      "id": 38047,
      "typeId": 11597,
      "title": "How do People Perceive Privacy and Interaction Quality while Chatting with a Crowd-operated Robot?",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present Crowd of Oz (CoZ) – a crowd powered system to enable real time dialogue between a Pepper robot and a user. CoZ includes media rich interfaces and advanced architecture to swiftly elicit crowd responses for the Pepper robot. We have empirically demonstrated that users can be engaged in a beneficial discussion with the crowd-powered robot, that acts as a coach to mitigate stress.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            },
            {
              "country": "Pakistan",
              "state": "-Select-",
              "city": "Mirpur AJK",
              "institution": "Mirpur University of Science & technology (MUST)",
              "dsl": "Software Engineering Dept"
            }
          ],
          "personId": 37326
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "-Select-",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37923
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37891
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Lower Saxony",
              "city": "Hannover",
              "institution": "Leibniz University of Hannover",
              "dsl": "L3S Research Center"
            }
          ],
          "personId": 37377
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of technology",
              "dsl": "Social Robotics Lab, Industrial Design dept."
            }
          ],
          "personId": 37164
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37763
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378332"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4Z7IhCfiTec",
          "title": "How do People Perceive Privacy and Interaction Quality while Chatting with a Crowd-operated Robot?",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38048,
      "typeId": 11521,
      "title": "Robot Role Design for Implementing Social Facilitation Theory in Musical Instruments Practicing",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The application of social robots has recently been explored in various types of educational settings including music learning. Earlier research presented evidence that simply the presence of a robot can influence a person’s task performance, confirming social facilitation theory and findings in human-robot interaction. Confirming the evaluation apprehension theory, earlier studies showed that next to a person’s presence, also that person’s social role could influence a user’s performance: the presence of a (non-) evaluative other can influence the user’s motivation and performance differently. To be able to investigate that, researchers need the roles for the robot which is missing now. In the current research, we describe the design of two social roles (i.e., evaluative role and non-evaluative role) of a robot that can have different appearances. For this, we used the SocibotMini: A robot with a projected face, allowing diversity and great flexibility of human-like social cue presentation. An empirical study at a real practice room including 20 participants confirmed that users (i.e., children) evaluated the robot roles as intended. Thereby, the current research provided the robot roles allowing to study whether the presence of social robots in certain social roles can stimulate practicing behavior and suggestions of how such roles can be designed and improved. Future studies can investigate how the presence of a social robot in a certain social role can stimulate children to practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "EINDHOVEN",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "EINDHOVEN",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37773
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Industrial Design",
              "dsl": "Eindhoven University of Technology"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Industrial Design",
              "dsl": "Eindhoven University of Technology"
            }
          ],
          "personId": 37184
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of technology",
              "dsl": "Social Robotics Lab, Industrial Design dept."
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of technology",
              "dsl": "Social Robotics Lab, Industrial Design dept."
            }
          ],
          "personId": 37164
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37553
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37763
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374787"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=GkPPnSwprKA",
          "title": "Robot Role Design for Implementing Social Facilitation Theory in Musical Instruments Practicing",
          "duration": 520,
          "type": "video"
        }
      },
      "sessionIds": [
        38361
      ],
      "eventIds": []
    },
    {
      "id": 38049,
      "typeId": 11597,
      "title": "Receptivity and Interaction of Social Robots in Hospitals",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "For social robots to be deployed in an interaction-centred environment such as a hospital, entities need to understand how to design human-robot interactions for the technology to be adopted successfully. For instance, if the robot was to be placed at a hospital concierge, it would need to engage visitors similar to a human concierge, either through verbal or non-verbal gestures (social cues). In this study, we investigate two hypotheses. Firstly, we hypothesized that various attention-drawing social cues significantly correlate to the receptivity of the robot. Secondly, we hypothesized that humans preferred medium of information transfer is through verbal interaction. We set up a humanoid concierge robot, Cruzr, in a hospital for 5 days as a trial. Our findings indicate an increase in receptivity when Cruzr performed a social cue as compared to neutral mode and that the preferred mode of communication was touch over voice. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "NCS Pte Ltd.",
              "dsl": ""
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Telecommunication",
              "dsl": ""
            }
          ],
          "personId": 37851
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "NCS Pte Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37972
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378243"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rv5S2XWUlXQ",
          "title": "Receptivity and Interaction of Social Robots in Hospitals",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38050,
      "typeId": 11597,
      "title": "Robot morphology and children’s perception of social robots:  An exploratory study ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The aim of this study was to investigate whether robot morphology (i.e., anthropomorphic, zoomorphic, or caricatured) influences children’s perceptions of animacy, anthropomorphism, social presence, and perceived similarity. Based on a sample of 35 children aged seven to fourteen, we found that, depending on the robot’s morphology, children’s perceptions of anthropomorphism, social presence and perceived similarity varied, with the anthropomorphic robot typically ranking higher than the zoomorphic robot. Our findings suggest that the morphology of social robots should be taken into account when planning, analyzing, and interpreting studies on child-robot interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": "Amsterdam School of Communication Research"
            }
          ],
          "personId": 37522
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": "Amsterdam School of Communication Research"
            }
          ],
          "personId": 37273
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": "Amsterdam School of Communication Research"
            }
          ],
          "personId": 37961
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": ""
            }
          ],
          "personId": 37602
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": "Amsterdam School of Communication Research"
            }
          ],
          "personId": 37571
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378348"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5N-su0yYTD8",
          "title": "Robot morphology and children’s perception of social robots:  An exploratory study ",
          "duration": 121,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38051,
      "typeId": 11523,
      "title": "Safe and Robust Robot Learning from Demonstration through Conceptual Constraints",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "This thesis summary presents research focused on incorporating high-level  abstract  behavioral  requirements,  called  ‘conceptual constraints’, into the modeling processes of robot Learning fromDemonstration (LfD) techniques. This idea is realized via an LfD algorithm calledConcept Constrained Learning from Demonstration.This algorithm encodes motion planning constraints as temporally associated logical formulae of Boolean operators that enforce high-level constraints over portions of the robot’s motion plan during learned skill execution. This results in more easily trained, more robust, and safer learned skills. Current work focuses on automat-ing constraint discovery, introducing conceptual constraints into human-aware motion planning algorithms, and expanding upon trajectory alignment techniques for LfD. Future work will focus on how concept constrained algorithms and models are best incorporated into effective interfaces for end-users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 37682
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 37452
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377428"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38052,
      "typeId": 11521,
      "title": "Closeness is Key over Long Distances: Effects of Interpersonal Closeness on Telepresence Experience",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Telepresence robots act as the remote embodiments of human operators, enabling people to stay connected to friends, family, and coworkers over lengthy physical separations. However, the factors affecting how humans can best make use of such systems are not yet well understood. This paper explores the effects of personalization and relationship closeness on telepresence via two studies. Study 1 was a between-participants experiment that investigated telepresence robot personalization. 32 pairs of friends (N = 64) participated in the study’s team-building-style activities and answered questions about robot operator presence. The results unexpectedly indicated that relationship closeness influenced the interaction experience more than any other considered predictor variable. To study closeness more rigorously as the central manipulation, we conducted Study 2, a between-participants experiment with 24 pairs (N = 48) and a similar procedure. Robot operators who reported a closer relationship with their teammate felt more present in this investigation. These findings can inform the design and application of telepresence robot systems to increase a remote operator’s feelings of presence via robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 37743
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Goshen",
              "institution": "Goshen College",
              "dsl": ""
            }
          ],
          "personId": 37546
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37647
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": "Psychology"
            }
          ],
          "personId": 37816
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374785"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=gxVhmWfJ4LY",
          "title": "Closeness is Key over Long Distances: Effects of Interpersonal Closeness on Telepresence Experience",
          "duration": 371,
          "type": "video"
        }
      },
      "sessionIds": [
        38354
      ],
      "eventIds": []
    },
    {
      "id": 38053,
      "typeId": 11597,
      "title": "Evaluation of the Handshake Turing Test for anthropomorphic Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Handshakes are fundamental and common greeting and parting gestures among humans. They are important in shaping first impressions as people tend to associate character traits with a person's handshake. To widen the social acceptability of robots and make a lasting first impression, a good handshaking ability is an important skill for social robots. Therefore, to test the human-likeness of a robot handshake, we propose an initial Turing-like test, primarily for the hardware interface to future AI agents. We evaluate the test on an android robot's hand to determine if it can pass for a human hand. This is an important aspect of Turing tests for motor intelligence where humans have to interact with a physical device rather than a virtual one. We also propose some modifications to the definition of a Turing test for such scenarios taking into account that a human needs to interact with a physical medium.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Marketing and Human Resource Management"
            }
          ],
          "personId": 37915
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Intelligent Autonomous Systems"
            }
          ],
          "personId": 37395
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Marketing and Human Resource Management"
            }
          ],
          "personId": 37501
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 37449
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt ",
              "dsl": ""
            }
          ],
          "personId": 37886
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378260"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=vh856OMivgY",
          "title": "Evaluation of the Handshake Turing Test for anthropomorphic Robots",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38054,
      "typeId": 11597,
      "title": "Carrot or stick: The effect of reward and punishment in robot assisted language learning",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Feedback plays an important role in language learning. However, limited research can be found on the influence of feedback in robot-assisted language learning. Therefore, this study aims to identify the effects of robot-feedback on learning gain, motivation, and anthropomorphism. In total, 60 students participated in a language learning task, with a robot using one of three feedback conditions: reward, punishment, and no feedback. The results showed that feedback only affected learning gain: students learned more with punishment, followed by reward, compared to no feedback. Thus, our results underscore the importance of feedback in RALL.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "-Select-",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Department of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37826
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "-Select-",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Department of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37485
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378349"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=eTUizPCGJkw",
          "title": "Carrot or stick: The effect of reward and punishment in robot assisted language learning",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38055,
      "typeId": 11599,
      "title": "Arts + Health: New Approaches to Arts and Robots in Health  Care",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "We describe the implementation and evaluation of a public interactive robotic art installation in a rehabilitation hospital. The project had two goals; to provide an enjoyable and novel artistic experience for the hospital community, and to better understand how human-centred robotics, particularly a receptive-focused intervention, might promote wellbeing and quality of life for members of hospital communities. By evaluating the experiences of the participants and stakeholders, the value of the installation for participants was assessed. This work contributes relevant insight towards the development of future art installations within the health jurisdiction and more broadly. The data also informs the ongoing discussion concerning the potential role of social and therapeutic robots in health care settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Bruce ",
              "institution": "University of Canberra",
              "dsl": "Human Centred Technology Research Centre"
            }
          ],
          "personId": 37447
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "Canberra Health Services",
              "dsl": "Curator, Arts in Health"
            }
          ],
          "personId": 37717
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "- OUTSIDE U.S.A. -",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Communication and Psychology"
            }
          ],
          "personId": 37450
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "University of Canberra",
              "dsl": "Faculty of Health"
            }
          ],
          "personId": 37498
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussels",
              "institution": "Artist",
              "dsl": ""
            }
          ],
          "personId": 37205
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380733"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5J_Vlexsk9c",
          "title": "Arts + Health: New Approaches to Arts and Robots in Health  Care",
          "duration": 536,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38056,
      "typeId": 11597,
      "title": "Effects of Pitch Gestures on Learning Chinese Orthography with a Social Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we investigate the effect of a social robot using head and arm gestures mimicking the lexical tones (i.e. pitch gestures) on learning pronunciations and translations of Chinese characters. Performance was compared between two within-subjects conditions: Gesture Observation condition, in which the robot used pitch gestures to teach six characters and the No Gesture condition in which the robot did not use gestures to teach six characters. Participants (N = 21) were tested on how well they pronounced and translated the learned characters. The study showed that a robot not using gestures was found to enhance learning, but only when participants could first familiarize with learning Chinese characters with the robot using pitch gestures. These results suggested that prior knowledge of learning Chinese attained from a robot using pitch gestures improved recall on learning the characters during a learning module with a robot not using gestures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 37826
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University ",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University ",
              "dsl": ""
            }
          ],
          "personId": 37331
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378350"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QXfYlvkpXbQ",
          "title": "Effects of Pitch Gestures on Learning Chinese Orthography with a Social Robot",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38057,
      "typeId": 11597,
      "title": "Attention-Based Multimodal Fusion for Estimating Human Emotion in Real-World HRI",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Toward empathetic and harmonious human-robot interaction (HRI), automatic estimation of human emotion has attracted increasing attention from multidisciplinary research fields. In this report, we propose an attention-based multimodal fusion approach that explores the space between traditional early and late fusion approaches, to deal with the problem of asynchronous multimodal inputs while considering their relatedness. The proposed approach enables the robot to align the human's visual and speech signals (more specifically, facial, acoustic, and lexical information) extracted by its cameras, microphones, and processing modules and is expected to achieve robust estimation performance in real-world HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Honda R&D Co., Ltd.",
              "dsl": ""
            }
          ],
          "personId": 38009
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37690
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The Graduate University for Advanced Studies",
              "dsl": ""
            }
          ],
          "personId": 37554
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378261"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MyD0vdAZd0c",
          "title": "Attention-Based Multimodal Fusion for Estimating Human Emotion in Real-World HRI",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38058,
      "typeId": 11597,
      "title": "Modeling the Interplay of Trust and Attention in HRI: an Autonomous Vehicle Study",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we study and model how two factors of human cognition, trust and attention, affect the way humans interact withautonomous vehicles. We develop a probabilistic model that succinctly captures how trust and attention evolve across time to drive behavior, and present results from a human-subjects experiment where participants interacted with a simulated autonomous vehicle while engaging with a secondary task. Our main findings suggest that trust affects attention, which in turn affects the human’s decision to intervene with the autonomous vehicle.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 37559
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37633
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37887
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37451
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378262"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_tvAUQyf5_8",
          "title": "Modeling the Interplay of Trust and Attention in HRI: an Autonomous Vehicle Study",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38059,
      "typeId": 11597,
      "title": "Informing the Design of a Robotic Coach through Systematic Observations",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Current physical rehabilitation techniques can be boring and frustrating for those that need them, especially when they are carried out alone over the long-term. Individual, repetitive exercises are also carried out by high performance athletes in sports such as squash. By observing the motivational behaviours used by professional squash coaches, we have analysed coaching styles which will help to inform the design of an autonomous robotic coach capable of increasing adherence to a long-term sports or rehabilitation exercise program.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            }
          ],
          "personId": 37908
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Mathematical and Computer Science"
            }
          ],
          "personId": 37270
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37730
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378351"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CJynaoYUaLA",
          "title": "Informing the Design of a Robotic Coach through Systematic Observations",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38060,
      "typeId": 11597,
      "title": "A Preparatory Study for Measuring Engagement in Pediatric Virtual and Robotics Rehabilitation Settings",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The rehabilitation intervention in children with motor or cognitive impairments requires to be effectively measured in terms of engagement to understand the effects of the rehabilitation process and to enable more targeted interventions leading to improved experiences for both children and rehabilitation therapists. Indeed, the rehabilitation practice may be enhanced by attending to the client’s signals of engagement in therapy. In this paper, we present the results of the validation of the PARE scale (Pediatric Assessment of Rehabilitation Engagement) that was designed to capture the dimensions of affective, cognitive, and behavioral engagement in the interaction of clients-rehabilitation providers. Results showed that the scale can be effectively used for the measurement of the different components of engagement in intervention settings to support therapeutic virtual scenarios or the application of robots to treat motor and cognitive impairments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": ""
            }
          ],
          "personId": 37614
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Caserta",
              "institution": "Universita' della Campania \"L. Vanvitelli\"",
              "dsl": ""
            }
          ],
          "personId": 37683
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "Sheffield Robotics"
            }
          ],
          "personId": 37875
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 37258
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378352"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=p4cSZ-6GZks",
          "title": "A Preparatory Study for Measuring Engagement in Pediatric Virtual and Robotics Rehabilitation Settings",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38061,
      "typeId": 11597,
      "title": "The Impact of Organizational Properties on the Design of  Robotic Support for Employees with Disabilities",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this position paper we report on a study of a Korean business that employs people with cognitive and developmental disabilities (DDs) across a variety of operations. The goal of the study was to contribute to the development of scenarios involving the use of a robotic platform to enhance the work-experience of the disabled employees. Based on our findings, we argue for the importance of understanding the broad organizational and bureaucratic properties of a business or workplace when devising HRI scenarios, and of bringing elements like business models, operating philosophy and organizational hierarchies directly into the design process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Naver Labs Europe",
              "dsl": ""
            }
          ],
          "personId": 37274
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Naver Labs Europe",
              "dsl": "UX and ethnography"
            }
          ],
          "personId": 37486
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Isere",
              "city": "GRENOBLE",
              "institution": "Naver Labs Europe",
              "dsl": "UX and Ethnography"
            }
          ],
          "personId": 37462
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378263"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38062,
      "typeId": 11597,
      "title": "Impression Evaluation of Presentation by a Communication Robot in an Actual Exhibition ",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In exhibitions by companies, there are many opportunities to conduct presentations to visitors using graphic slides. However, these presentations are hard work for the exhibitors because they must repeat the same presentation several times a day to inform as many visitors as possible. This paper introduces a presentation system that uses a robot rather than a human presenter. A case study is conducted in which the proposed system is experienced by 78 visitors in an actual exhibition; the goal is to consider whether the proposed system could be the possibility of an alternative to human presenters. The results indicate that presentations by real-world robots with appropriate non-verbal behaviors are more effective in terms of acceptability and understandability than alternatives.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Service Evolution Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37360
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Service Evolution Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37495
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Service Evolution Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37761
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378264"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JoCsNXm1-7Q",
          "title": "Impression Evaluation of Presentation by a Communication Robot in an Actual Exhibition ",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38063,
      "typeId": 11597,
      "title": "Robot-Theater Programs for Different Age Groups to Promote STEAM Education and Robotics Research ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Issues with learner engagement and interest in STEM and robotics fields may be resolved from an interdisciplinary education approach. We have developed a robot-theater framework using interactive robots as a way to integrate the arts into STEM and robotics learning. The present paper shows a breadth of our target populations, ranging from elementary school children to college students. Based on our experiences in conducting four different programs applied to different age groups, we discuss the characteristics, lessons,  design considerations, and implications for future work. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 37244
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 37664
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 37618
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab, Department of Industrial and Systems Engineering"
            }
          ],
          "personId": 37259
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab,Grado Department of Industrial and Systems Engineering"
            }
          ],
          "personId": 37552
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 37422
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378353"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38064,
      "typeId": 11597,
      "title": "Multimodal Representation Learning for Human Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present a neural network based system capable of learning a multimodal representation of images and words. This representation allows for bidirectional grounding of the meaning of words and the visual attributes that they represent, such as colour, size and object name.\r\nWe also present a new dataset captured specifically for this task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "EDINBURGH",
              "institution": "Edinburgh Centre for Robotics",
              "dsl": ""
            }
          ],
          "personId": 37760
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "EDINBURGH",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Buchs",
              "institution": "University of Applied Sciences in Technology",
              "dsl": "Institute for Development of Mechatronic Systems"
            }
          ],
          "personId": 37666
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378265"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38065,
      "typeId": 11597,
      "title": "End-User Robot Programming Case Study: Augmented Reality vs. Teach Pendant",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The work presents a preliminary experiment aimed on comparing a traditional method of programming an industrial collaborative robot using a teach pendant with a novel method based on augmented reality and interaction on a high-level of abstraction. In the experiment, three participants programmed a visual inspection task. Subjective and objective metrics are reported as well as selected usability-related issues of both interfaces. The main purpose of the experiment was to get initial insight into the problematic of comparing highly different user interfaces and to provide a basis for a more rigorous comparison that is going to be taken out.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Czech Republic",
              "state": "",
              "city": "Brno",
              "institution": "Brno University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37644
        },
        {
          "affiliations": [
            {
              "country": "Czech Republic",
              "state": "",
              "city": "Brno",
              "institution": "Brno University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37689
        },
        {
          "affiliations": [
            {
              "country": "Czech Republic",
              "state": "",
              "city": "Brno",
              "institution": "Brno University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37351
        },
        {
          "affiliations": [
            {
              "country": "Czech Republic",
              "state": "",
              "city": "Brno",
              "institution": "Brno University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37899
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378266"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=0ukFz0r0KRo",
          "title": "End-User Robot Programming Case Study: Augmented Reality vs. Teach Pendant",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38066,
      "typeId": 11597,
      "title": "Evaluating the Effectiveness of Nonverbal Communication in Human-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We evaluate the effectiveness of the type of directions given by a robot in a joint-activity task. The joint-activity task we focus on involves a humanoid robot giving directions to a human subject in exploring an unknown environment and finding objects hidden in certain locations inside the environment. The experiment is conducted in the form of a between-group study with 8 participants in which one group is provided directions by the robot verbally and the other group is given nonverbal directions in the form of hand gestures and gaze direction. Our findings show that the type of directions given by the robot do have a noticeable impact on the performance of the human subjects in the experiment. Specifically, the group which is given nonverbal directions by the robot is on average ∼70% quicker in finding all the hidden objects than the group which is given verbal directions. We evaluate the hypothesis from prior research that the reason for this discrepancy is correlated with the reduced perceived mental workload of the activity by the human subjects when the robot provides nonverbal directions, with the help of post-experiment survey responses provided by the participants of our study groups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "HEIR"
            }
          ],
          "personId": 37506
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37582
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378354"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38067,
      "typeId": 11597,
      "title": "The Effect of Robot’s Ice-breaking Humor on Likeability and Future Contact Intentions",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study explores the effect of social robot’s use of ice-breaking humor on likeability and future contact intentions. Result from a laboratory experiment showed that jokes used at greetings and topic transition were effective in increasing likeability and reducing awkwardness, while the intention to use the robot for a longer period was not affected by the use of humor. We suggest to include social jokes when designing the first encounter with robot, as long as the jokes do not interfere with task performance. This work may be helpful in deciding whether to insert humor and how to dispose humor in robot’s speech.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Market Kurly",
              "dsl": ""
            }
          ],
          "personId": 37481
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Human-Computer Interaction & Design Lab."
            }
          ],
          "personId": 37965
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378267"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=UKTQu8Nq49w",
          "title": "The Effect of Robot’s Ice-breaking Humor on Likeability and Future Contact Intentions",
          "duration": 88,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38068,
      "typeId": 11523,
      "title": "Body Language in Affective Human-Robot Interaction",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Social human–robot interaction is concerned with exploring the ways in which social interaction can be achieved between a human and a sociable robot. Affect has an important role in interaction as it helps interactants coordinate and indicate the success of the communication. Designing socially intelligent robots requires competence in communication, which includes exchanges of both verbal and non–verbal cues. This project will focus on non–verbal communication, more specifically body movements, postures and gestures as means of conveying socially affective information. Using the affective grounding perspective, which conceptualizes emotion asa coordination mechanism, together with honest signals as a measurement of the dynamics of the interaction, and the robot Pepper, we aim to develop a system that would be able to communicate affect, with the goal to enhance affective human–robot interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "-",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Institute of Visual Computing and Human-Centered Technology"
            }
          ],
          "personId": 37777
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Vienna University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37927
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377432"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=nF3RFkJZHA4",
          "title": "Body Language in Affective Human-Robot Interaction",
          "duration": 164,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38069,
      "typeId": 11523,
      "title": "Implicit Communication through Distributed Sound Design: Exploring a New Modality in Human-Robot Interaction",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "As robots find their way into homes, workplaces, and public spaces, rich and effective human-robot interaction will play an essential role in their success. While most sound-related research in the field of HRI focuses on speech and semantic-free utterances, the potential of sound as an implicit non-verbal channel of communication has only recently received attention and remains largely unexplored. This research will bring design approaches from the fields of sound design and spatial audio into the context of human-robot interaction to influence human perception of robot characteristics and refine non-verbal auditory communication. It will implement sound design systems into various physical robots and evaluate their effect through user studies. By developing design principles for the sonic augmentation of robots, we aim to provide the HRI community with new tools to enrich the way robots communicate with humans",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Paddington",
              "institution": "University of New South Wales",
              "dsl": "Creative Robotics Lab"
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "Interactive Media Lab"
            }
          ],
          "personId": 37347
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW Faculty of Art & Design, Interactive Media Lab",
              "dsl": ""
            }
          ],
          "personId": 37372
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "Creative Robotics Lab, School of Art & Design"
            }
          ],
          "personId": 37592
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377431"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=W8NMAAxh1mc",
          "title": "Implicit Communication through Distributed Sound Design: Exploring a New Modality in Human-Robot Interaction",
          "duration": 183,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38070,
      "typeId": 11597,
      "title": "The Effect of Virtual Reality Control of a Robotic Surrogate on Presence and Social Presence in Comparison to Telecommunications Software.",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social isolation can often be an issue in those with restricted mobility. Telecommunications software is popular, but lacks high-level interactions found in face-to-face conversation. This study investigated the use of an immersive control system for a robotic surrogate in comparison with Skype. It was found that there was no significant difference between the presence and social presence felt between the two systems; however, Skype was found to be significantly easier to use. Future work will focus on identifying user requirements and further developing the control system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37467
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37754
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37884
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378268"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cedD112qwfk",
          "title": "The Effect of Virtual Reality Control of a Robotic Surrogate on Presence and Social Presence in Comparison to Telecommunications Software.",
          "duration": 126,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38071,
      "typeId": 11597,
      "title": "Improving Engagement by Letting Social Robots Learn and Call Your Name",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper aims to improve the engagement in human-robot interactions by enabling social robots to call people by their name during a social interaction. To this end, we propose to integrate computer vision, online learning using a convolutional neural network, and speech technologies and concepts to learn names to facilitate and improve engagement. Our experiments show that human-robot engagement shows significant improvement through the proposed approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "University of Kansas",
              "dsl": ""
            }
          ],
          "personId": 37165
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "University of Kansas",
              "dsl": "Humanoid Engineering & Intelligent Robotics (HEIR) Lab"
            },
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37582
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378355"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=0IyV0Q4fxp8",
          "title": "Improving Engagement by Letting Social Robots Learn and Call Your Name",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38072,
      "typeId": 11597,
      "title": "Triadic Human-Robot Interaction. Distributed Agency and Memory in Robot Assisted Interactions",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We argue that the field of human-robot interaction needs a distributed and socially situated understanding of reminding and scheduling practices in the design of robots to meet the needs of people with cognitive disabilities. The results are based on an interaction analysis of video recorded workshop interactions during a co-creation process in which the participants tested a reminder-robot prototype that was designed for and with people with acquired brain injury.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Communication and Psychology"
            }
          ],
          "personId": 37406
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            }
          ],
          "personId": 37389
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            }
          ],
          "personId": 37955
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378269"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38073,
      "typeId": 11521,
      "title": "Interactive Tuning of Robot Program Parameters via Expected Divergence Maximization",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Enabling diverse users to program robots for different applications is critical for robots to be widely adopted. Most of the new collaborative robot manipulators come with intuitive programming interfaces that allow novice users to compose robot programs and tune their parameters. However, parameters like motion speeds or exerted forces cannot be easily demonstrated and often require manual tuning, resulting in a tedious trial-and-error process. To address this problem, we formulate tuning of one-dimensional parameters as an Active Learning problem where the learner iteratively refines its estimate of the feasible range of parameter values, by selecting informative queries. By executing the parametrized actions, the learner gathers the user’s feedback, in the form of directional answers (“higher,” “lower,” or “fine”), and integrates it in the estimate. We propose an Active Learning approach based on Expected Divergence Maximization for this setting and compare it against two baselines with synthetic data. We further compare the approaches on a real-robot dataset obtained from programs written with a simple Domain-Specific Language for a robot arm and manually tuned by expert users (N=8) to perform four manipulation tasks. We evaluate the effectiveness and usability of our interactive tuning approach against manual tuning with a user study where novice users (N=8) tuned parameters of a human-robot hand-over program.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "School of Electrical Engineering"
            }
          ],
          "personId": 37873
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "School of Electrical Engineering"
            }
          ],
          "personId": 37366
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 37944
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374784"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CIYdLbxa5Bc",
          "title": "Interactive Tuning of Robot Program Parameters via Expected Divergence Maximization",
          "duration": 593,
          "type": "video"
        }
      },
      "sessionIds": [
        38357
      ],
      "eventIds": []
    },
    {
      "id": 38074,
      "typeId": 11521,
      "title": "The Persistence of First Impressions: The Effect of Repeated Interactions on the Perception of a Social Robot",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Numerous studies in social psychology have shown that familiarization across repeated interactions improves people’s perception of the other. If and how these findings relate to human-robot interaction (HRI) is not well understood, even though such knowledge is crucial when pursuing long-term interactions. In our work, we investigate the persistence of first impressions by asking 49 participants to play a geography game with a robot. We measure how their perception of the robot changes over three sessions with three to ten days of zero exposure in between. Our results show that different perceptual dimensions stabilize within different time frames, with the robot’s competence being the fastest to stabilize and perceived threat the most fluctuating over time. We also found evidence that perceptual differences between robots with varying levels of humanlikeness persist across repeated interactions. This study has important implications for HRI design as it sheds new light on the influence of robots’ embodiment and interaction abilities. Moreover, it also impacts HRI theory as it presents novel findings contributing to research on the uncanny valley and robot perception in general.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 37609
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 37442
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Social Robotics Lab"
            }
          ],
          "personId": 37971
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374786"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JMPWnml-Lpg",
          "title": "The Persistence of First Impressions: The Effect of Repeated Interactions on the Perception of a Social Robot",
          "duration": 565,
          "type": "video"
        }
      },
      "sessionIds": [
        38343
      ],
      "eventIds": []
    },
    {
      "id": 38075,
      "typeId": 11521,
      "title": "Supporting Perception of Weight through Motion-induced Sensory Conflicts in Robot Teleoperation",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we design and evaluate a novel form of visually simulated haptic feedback cue for communicating weight in robot teleoperation. We propose that a visuo-proprioceptive cue results from inconsistencies created between the user’s visual and proprioceptive senses when the robot’s movement differs from the movement of the user’s input. In a user study where participants teleoperate a six-DoF robot arm, we demonstrate the feasibility of using such a cue for communicating weight in four telemanipulation tasks to enhance user experience and task performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 37770
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 37808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 37325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 37604
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374841"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=YO5jxtJ3eTQ",
          "title": "Supporting Perception of Weight through Motion-induced Sensory Conflicts in Robot Teleoperation",
          "duration": 585,
          "type": "video"
        }
      },
      "sessionIds": [
        38354
      ],
      "eventIds": []
    },
    {
      "id": 38076,
      "typeId": 11523,
      "title": "Robots Teaching Recycling: Towards Improving Environmental Literacy of Children",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The present pollution problem can be partially attributed to the lack of empathy for learning any ecological and environmental literacy skills. Although the use of robotics in education is increasing, there has been a surprisingly lack of interest towards developing devices designed to teach children how to be environmentally conscious, and in particular, how to recycle. This gap is the basis for our robot, which we call the Smart Trash Junior, a mechatronic trashcan that uses vision recognition to identify recyclable objects and enters into a dialogue that educates children, within elementary schools, how to recycle.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "ATR Lab"
            }
          ],
          "personId": 37528
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Dakota",
              "city": "Fargo",
              "institution": "North Dakota State University",
              "dsl": "Communication"
            }
          ],
          "personId": 38017
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "Advanced Telerobotics Research Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "DigitalMinds",
              "dsl": ""
            }
          ],
          "personId": 37620
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "Advanced Telerobotics Research Laboratory"
            }
          ],
          "personId": 37185
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "Advanced Telerobotics Research Lab"
            }
          ],
          "personId": 37555
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379462"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38077,
      "typeId": 11523,
      "title": "It’s Too Hot! A Robot that Sweats to Indicate High-temperature to the Elderly",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The elderly are more affected by higher environmental temperatures. If they misperceive the temperature, it can lead to a number of potentially dangerous health issues. To address this, we propose a robot that sweats to indicate the high environmental temperature to the elderly. In this paper, we present the development of our first prototype and the design considerations of the second prototype and pilot study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Department of Intelligent Interaction Technologies"
            }
          ],
          "personId": 37508
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Engineering Systems"
            }
          ],
          "personId": 37424
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379461"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Q37dOgdZb1c",
          "title": "It’s Too Hot! A Robot that Sweats to Indicate High-temperature to the Elderly",
          "duration": 236,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38078,
      "typeId": 11597,
      "title": "``Sorry to Disturb You'': Autism and Robot Interruptions",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Here, we present a novel experiment on the yet unstudied phenomenon of autism spectrum condition (ASC) adults' responses to interruption, using a robot role-play clerical task. Using an IQ, gender, and task parameter matched neuro-typical (NT) control sample we found that adults with an ASC experience marginally less task disruption from a robot interrupter comparatively to a human. We surmise that robot-assisted therapy for adults with the condition is a potential research avenue worth further exploration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Social Sciences"
            }
          ],
          "personId": 37563
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Robotics Lab",
              "dsl": "Heriot-Watt University"
            }
          ],
          "personId": 37168
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Edinburgh Centre for Robotics",
              "dsl": ""
            }
          ],
          "personId": 37603
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37530
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinbugh",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            }
          ],
          "personId": 37759
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Mathematical and Computer Science"
            }
          ],
          "personId": 37270
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Department of Psychology, School of Social Sciences"
            }
          ],
          "personId": 37557
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378340"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=P-Zz0csvcvA",
          "title": "``Sorry to Disturb You'': Autism and Robot Interruptions",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38079,
      "typeId": 11597,
      "title": "Robot vs. Voice Assistant: Is playing with Pepper More Fun than Playing with Alexa?",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Entertainment applications for robots are often based on voice interaction. In these scenarios, the robot is primarily used as a voice assistant with a physical body. We conducted a study to investigate whether the entertainment value of a robot during a game can be attributed to the experience of voice interaction alone, or whether it is related to the physical presence of a robot and its expressivity through gestures and movement. The study examined the user experience for three different set-ups of a quiz game (with voice assistant, robot without animation, and animated robot). The results indicate that the perceived hedonic quality increases with physical presence and movement of the robot. Pragmatic quality was not rated differently for the three game versions. Additional qualitative interviews suggest that it might be desirable to design not overly expressive, but meaningful robot behavior for entertainment applications, in order to promote both, hedonic and pragmatic experience. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Fraunhofer Institute for Industrial Engineering IAO",
              "dsl": ""
            }
          ],
          "personId": 38005
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Fraunhofer Institute for Industrial Engineering IAO",
              "dsl": ""
            }
          ],
          "personId": 37849
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Fraunhofer Institute for Industrial Engineering IAO",
              "dsl": ""
            }
          ],
          "personId": 37711
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Stuttgart Media University",
              "dsl": ""
            }
          ],
          "personId": 37477
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378251"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38080,
      "typeId": 11597,
      "title": "Let’s Talk About It! Subjective and Objective Disclosures to Social Robots ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study aims to test the viability of using social robots for eliciting rich disclosures from humans to identify their needs and emotional states. Self-disclosure has been studied in the psychological literature in many ways, addressing both peoples’ subjective perceptions of their disclosures, as well as objective disclosures evaluating these via direct observation and analysis of verbal and written output. Here we are interested in how people disclose (non-sensitive) personal information to robots, in an aim to further understand the differences between one’s subjective perceptions of disclosure compared to evidence of disclosure from the shared content. An experimental design is suggested for evaluating disclosure to social robots compared to humans and conversational agents. Initial results suggest that while people perceive they disclose more to humans than to humanoid social robots or conversational agents, no actual observed differences in the content of the disclosure emerges between the three agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "Institute of Neuroscience and Psychology"
            }
          ],
          "personId": 37639
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bangor",
              "institution": "Bangor University",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 37491
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "Macquarie University",
              "dsl": "the Department of Cognitive Science"
            }
          ],
          "personId": 37561
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378252"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cX1ScKQMxLU",
          "title": "Let’s Talk About It! Subjective and Objective Disclosures to Social Robots ",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38081,
      "typeId": 11597,
      "title": "Security Risks of Social Robots Used to Persuade and Manipulate: A Proof of Concept Study",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Earlier research has shown that robots can provoke social responses in people, and that robots often elicit compliance. In this paper we discuss three proof of concept studies in which we explore the possibility of robots being hacked and taken over by others with the explicit purpose of using the robot's social capabilities. Three scenarios are explored: gaining access to secured areas, extracting sensitive and personal information, and convincing people to take unsafe action. We find that people are willing to do these tasks, and that social robots tend to be trusted, even in situations that would normally cause suspicion. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 37731
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab, Department of Electronics and Information Systems"
            }
          ],
          "personId": 37909
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Nijmegen",
              "institution": "Radboud University",
              "dsl": ""
            }
          ],
          "personId": 37585
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Queensland",
              "city": "Brisbane",
              "institution": "Australian Centre for Robotic Vision",
              "dsl": ""
            }
          ],
          "personId": 37381
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "Plymouth University",
              "dsl": "Centre for Robotics and Neural Systems"
            }
          ],
          "personId": 37190
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378341"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=ds9BqKczcEs",
          "title": "Security Risks of Social Robots Used to Persuade and Manipulate: A Proof of Concept Study",
          "duration": 109,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38082,
      "typeId": 11597,
      "title": "Multimodal Emotion Recognition with Thermal and RGB-D Cameras for Human-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Human emotion detection is an important aspect in social robotics and HRI. In this paper, we propose a vision-based multimodal emotion recognition method based on gait data and facial thermal images designed for social robots. Our method can detect 4 human emotional states (i.e., neutral, happiness, anger, and sadness). We gathered data from 25 participants in order to build-up an emotion database for training and testing our classification models. We implemented and tested several approaches such as Convolutional Neural Network (CNN), Hidden Markov Model (HMM), Support Vector Machine (SVM), and Random Forest (RF). These were trained and tested in order to compare the emotion recognition ability and to find the best approach. We designed a hybrid model with both the gait and the thermal data and the accuracy of our system shows an improvement of 10% over the other models based on our emotion database. This will be explored in a real-time HRI scenario.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "Autonomous Systems and Robotics Lab, U2IS, ENSTA Paris"
            }
          ],
          "personId": 37411
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "Autonomous Systems and Robotics Lab, U2IS, ENSTA Paris"
            }
          ],
          "personId": 37775
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378342"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=P6l8bAPPLNU",
          "title": "Multimodal Emotion Recognition with Thermal and RGB-D Cameras for Human-Robot Interaction",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38083,
      "typeId": 11597,
      "title": "Your Eyes Never Lie: A Robot Magician Can Tell if You Are Lying",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Detecting lies in a real-world scenario is an important skill for a humanoid robot that aims to act as a teacher, a therapist, or a caregiver. In these contexts, it is essential to detect lies while preserving the pleasantness of the social interaction and the informality of the relation. This study investigates whether pupil dilation related to an increase in cognitive load can be used to swiftly identify a lie in an entertaining scenario. The iCub humanoid robot plays the role of a magician in a card game, telling which card the human partner is lying about. The results show a greater pupil dilation in presence of a false statement even if in front of a robot and without the need of a strictly controlled scenario. We developed a heuristic method (accuracy of 71.4% against 16.6% chance level) and a random forest classifier (precision and recall of 83.3%) to detect the false statement. Additionally, the current work suggests a potential method to assess the lying strategy of the partner.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Italian Institute of Technology (IIT)",
              "dsl": "Robotics, Brain and Cognitive Science (RBCS)"
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Italian Institute of Technology (IIT)",
              "dsl": " Information Tecnology Directorate (ICT)"
            }
          ],
          "personId": 37600
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "GE",
              "city": "Genova",
              "institution": "Italian Institute of Technology (IIT)",
              "dsl": "Cognitive Architecture for Collaborative Technologies (CONTACT)"
            }
          ],
          "personId": 37431
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Italian Institute of Technology (IIT)",
              "dsl": "Robotics, Brain and Cognitive Sciences"
            }
          ],
          "personId": 37772
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": ""
            }
          ],
          "personId": 37706
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": "Robotics, Brain and Cognitive Science"
            }
          ],
          "personId": 37283
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "GE",
              "city": "Genova",
              "institution": "Italian Institute of Technology (IIT)",
              "dsl": "Cognitive Architecture for Collaborative Technologies (CONTACT)"
            }
          ],
          "personId": 37466
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378253"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=RyQ3JTNjNts",
          "title": "Your Eyes Never Lie: A Robot Magician Can Tell if You Are Lying",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38084,
      "typeId": 11597,
      "title": "Quantitative Results of Robot-Play Therapy for Children with Autism, ADHD and Delayed Speech Development",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an ongoing work that aims to provide a quantitative analysis of a large clinical study that was conducted with 21 children aged 4-8 years old. Children were diagnosed with various forms of Autism Spectrum Disorder (ASD) or Delayed Speech Development (DSD) with autistic traits. Each child participated in four to six so-called Robot-Play Therapy (RPT) sessions that lasted for fifteen minutes each. We manually video-coded the videos from the sessions to find behavior patterns, engagement and valence scores, and we are now in the process of statistical data analysis to understand whether children's exposure to a robot had a significant effect on their ADOS-2 scores. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37463
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37405
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37747
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37295
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": "School of Science and Technology"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378254"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=liESFr_jX6k",
          "title": "Quantitative Results of Robot-Play Therapy for Children with Autism, ADHD and Delayed Speech Development",
          "duration": 111,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38085,
      "typeId": 11597,
      "title": "What Would You Like to Drink? Engagement and Interaction Styles in HRI",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we address the question of how specific barman/robot interaction styles can affect users’ engagement. To this extent, we implemented a barman-robot called “BRILLO” with neutral, entertaining, and emphatic behavioral style suggesting drinks and taking orders from customers. Results show that a robot’s interaction style may determine users’ level of engagement. Indeed, interacting with an emphatic robot that modulates its behavior according to the user’s one is more effective than a neutral robot in improving engagement and positive emotions in public-service contexts. Moreover, users experienced more positive emotions when they perceived BRILLO as safe and as more similar to a human.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 37258
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": ""
            }
          ],
          "personId": 37614
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Caserta",
              "institution": "University of Campania \"Luigi Vanvitelli\" ",
              "dsl": ""
            }
          ],
          "personId": 37683
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 37646
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378343"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Sd4TiADabO4",
          "title": "What Would You Like to Drink? Engagement and Interaction Styles in HRI",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38086,
      "typeId": 11597,
      "title": "Some Adults Fail the False-Belief Task When the Believer Is a Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "People's mental models of robots affect their predictions of robot behavior in interactions. The present study highlights some of the uncertainties that enter specifically into people's considerations about the minds and behavior of robots by exploring how people fare in the standard \"Sally–Anne\" false-belief task from developmental psychology when the protagonist is a robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37524
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37634
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37879
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378344"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4oaH8XVPtz4",
          "title": "Some Adults Fail the False-Belief Task When the Believer Is a Robot",
          "duration": 42,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38087,
      "typeId": 11597,
      "title": "Using Robots to Study the Perception of Feedback Cross-culturally ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "While feedback currently generates much interest among many scholars, how feedback is perceived in cross-cultural context has not been extensively studied yet, due to considerable methodological obstacles. In this study, we investigate how different ways of providing feedback are perceived by inhabitants of neighboring countries such as Denmark, Germany and Poland. Based on initial analyses of different feedback strategies in these countries, we used a robot to deliver both positive and negative feedback. Using a robot has the advantage that the feedback is provided by an embodied interactant, yet whose behavior can be completely controlled. We carried out a questionnaire study in which the EZ-bot presented feedback either using strategies identified as common in Denmark, Poland or Germany; participants were then asked to rate the robot. The results show highly significant differences in the perception of different feedback strategies even in countries in geographical proximity. Using robots for studying cross-cultural communication differences thus constitutes a promising methodology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37594
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37336
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378255"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4aMOtQ__eSw",
          "title": "Using Robots to Study the Perception of Feedback Cross-culturally ",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38088,
      "typeId": 11597,
      "title": "Longitudinal Diary Data: Six Months Real-world Implementation of Affordable Companion Robots for Older People in Supported Living",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Companion robots have potential for improving wellbeing within aged care, however literature focuses on shorter-term studies often using relatively expensive platforms, raising concerns around novelty effects and economic viability. Here, we report ecologically valid diary data from two supported living facilities for older people with dementia or learning difficulties. Both sites implemented Joy for All robot animals and maintained diaries for six months. Entries were analysed using thematic analysis. We found robot use increased over the six months, changing from short, structured sessions to mainly permanent availability. Thus previously reported concerns on novelty were not warranted. Both sites reported positive outcomes including reminiscence, improved communication and potential wellbeing benefits (reduced agitation/anxiety). Incidences of negative response included devices described as ‘creepy.’ Devices appeared sufficiently robust for prolonged daily use with multiple users. Overall, we provide insight into real-world implementation of affordable companion robots, and longitudinal development of use.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Devon",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": "Faculty of Health and Human Sciences"
            }
          ],
          "personId": 37425
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland University of Technology",
              "dsl": ""
            }
          ],
          "personId": 37787
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Nijmegen",
              "institution": "Radboud University",
              "dsl": "Donders Institute for Brain, Cognition and Behaviour"
            }
          ],
          "personId": 37476
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": "School of Nursing and Midwifery"
            }
          ],
          "personId": 37349
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378256"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=A3iUqdEcQpw",
          "title": "Longitudinal Diary Data: Six Months Real-world Implementation of Affordable Companion Robots for Older People in Supported Living",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38089,
      "typeId": 11597,
      "title": "Considerations in determining Human-Robot Ratio",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing level of system autonomy, the Human-Robot Ratio (HRR) can potentially be increased for more efficient operations. The HRR can be used to inform the plausible team structure in the real-world operations. However the computation of HRR can be inferred from various sources, such as human and mission performances.  A small-scale study was conducted to understand how these measurements could be interpreted and used to inform the optimal HRR. The result shows that the proposed HRR derived from the human and mission performance might not be the same, suggesting that various mission-centric and human-centric factors must be considered before an optimal HRR could be derived. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "DSO National Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37453
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "SINGAPORE",
              "institution": "DSO National Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37239
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "SINGAPORE",
              "institution": "DSO National Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37903
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "SINGAPORE",
              "institution": "DSO National Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37437
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "DSO National Laboratories",
              "dsl": ""
            }
          ],
          "personId": 37897
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378345"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=KqN944gfC0o",
          "title": "Considerations in determining Human-Robot Ratio",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38090,
      "typeId": 11521,
      "title": "Intonation in Robot Speech: Does it work the same as with people?",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Human-robot interaction (HRI) research aims to design natural interactions between humans and robots. Intonation, a social signaling function in human speech investigated thoroughly in linguistics, has not yet been studied in HRI. This study investigates the effect of robot speech intonation in four conditions (no intonation, focus intonation, end-of-utterance intonation, or combined intonation) on conversational naturalness, social engagement, and people’s humanlike perception of the robot collecting objective and subjective data of participant conversations (n = 120). Our results showed that humanlike intonation partially improved subjective naturalness but not observed fluency, and that intonation partially improved social engagement but did not affect humanlike perceptions of the robot. Given that our results mainly differed from our hypotheses based on human speech intonation, we discuss the implications and provide suggestions for future research to further investigate conversational naturalness in robot speech intonation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 37667
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "University of Amsterdam",
              "dsl": ""
            }
          ],
          "personId": 37805
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 37844
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374801"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=tJM5lb2WfH8",
          "title": "Intonation in Robot Speech: Does it work the same as with people?",
          "duration": 397,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38091,
      "typeId": 11521,
      "title": "Behavioural Responses to Robot Conversational Failures",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Humans and robots will increasingly collaborate in domestic environments which will cause users to encounter more failures in interactions. Robots should be able to infer conversational failures by detecting human users’ behavioural and social signals. In this paper, we study and analyse these behavioural cues in response to robot conversational failures. Using a guided task corpus, where robot embodiment and time pressure are manipulated, we ask human annotators to estimate whether user affective states differ during various types of robot failures. We also train a random forest classifier to detect whether a robot failure has occurred and compare results to human annotator benchmarks. Our findings show that human-like robots augment users’ reactions to failures, as shown in users’ visual attention, in comparison to non-humanlike smart-speaker embodiments. The results further suggest that speech behaviours are utilised more in responses to failures when non-human-like designs are present. This is particularly important to robot failure detection mechanisms that may need to consider the robot’s physical design in its failure detection model",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37419
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37531
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37311
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Department of Robotics, Perception and Learning"
            }
          ],
          "personId": 37469
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37218
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374782"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=HaqvPQ8Gl0E",
          "title": "Behavioural Responses to Robot Conversational Failures",
          "duration": 496,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38092,
      "typeId": 11597,
      "title": "An Implicit, Non-Verbal Measure of Belief Attribution to Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Studies of mental state attribution to robots usually rely on verbal measures. However, verbal measures are sensitive to people's rationalizations, and the outcomes of such measures are not always reflected in a person's behavior. In light of these limitations, we present the first steps toward developing an alternative, non-verbal measure of belief attribution to robots. We report preliminary findings from a comparative study indicating that the two types of measures (verbal vs. non-verbal) are not always consistent. Notably, the divergence between the two measures was larger when the task of inferring the robot's belief was more difficult.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37524
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37988
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37634
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37879
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378346"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=AcWEdTw9tXg",
          "title": "An Implicit, Non-Verbal Measure of Belief Attribution to Robots",
          "duration": 63,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38093,
      "typeId": 11597,
      "title": "Using Social Robots to Teach Language Skills to Immigrant Children in an Oslo City District",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots have been shown to help in language education for children. This can be good aid for immigrant children that need additional help to learn a second language their parents do not understand to attend school. We present the setup for a long-term study that is being carried out in blinded to aid immigrant children with poor skills in the Norwegian language to improve their vocabulary. This includes additional tools to help parents follow along and provide additional help at home.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "Norwegian Computing Center",
              "dsl": "Department of Applied Research in Information Technology"
            }
          ],
          "personId": 37199
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "Norwegian Computing Center",
              "dsl": ""
            }
          ],
          "personId": 37315
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "Norwegian Computing Center",
              "dsl": "Department of Applied Research in Information Technology"
            }
          ],
          "personId": 37985
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378257"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=6UbrhTSK2Kw",
          "title": "Using Social Robots to Teach Language Skills to Immigrant Children in an Oslo City District",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38094,
      "typeId": 11521,
      "title": "Prompting Prosocial Human Interventions in Response to Robot Mistreatment",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Inspired by the benefits of human prosocial behavior, we explore whether prosocial behavior can be extended to a Human-Robot Interaction (HRI) context. More specifically, we study whether robots can induce prosocial behavior in humans through a 1x2 betweensubjects user study (N = 30) in which a confederate abused a robot. Through this study, we investigated whether the emotional reactions of a group of bystander robots could motivate a human to intervene in response to robot abuse. Our results show that participants were more likely to prosocially intervene when the bystander robots expressed sadness in response to the abuse as opposed to when they ignored these events, despite participants reporting similar perception of robot mistreatment and levels of empathy for the abused robot. Our findings demonstrate possible effects of group social influence through emotional cues by robots in human-robot interaction. They reveal a need for further research regarding human prosocial behavior within HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37502
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37316
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37670
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 37416
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374781"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=76nX9wos75E",
          "title": "Prompting Prosocial Human Interventions in Response to Robot Mistreatment",
          "duration": 575,
          "type": "video"
        }
      },
      "sessionIds": [
        38346
      ],
      "eventIds": []
    },
    {
      "id": 38095,
      "typeId": 11521,
      "title": "A Robot by Any Other Frame: Framing and Behaviour Influence Mind Perception in Virtual but not Real-World Environments",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Mind perception in robots has been an understudied construct in human-robot interaction (HRI) compared to similar concepts such as anthropomorphism and the intentional stance. In a series of three experiments, we identify two factors that could potentially influence mind perception and moral concern in robots: how the robot is introduced (framing), and how the robot acts (social behaviour). In the first two online experiments, we show that both framing and behaviour independently influence participants’ mind perception. However, when we combined both variables in the following real-world experiment, these effects failed to replicate. We hence identify a third factor post-hoc: the online versus real-world nature of the interactions. After analysing potential confounds, we tentatively suggest that mind perception is harder to influence in real-world experiments, as manipulations are harder to isolate compared to virtual experiments, which only provide a slice of the interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Uppsala Social Robotics Lab"
            }
          ],
          "personId": 37685
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "Jacobs University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37474
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "Jacobs University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37375
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Social Robotics Lab"
            }
          ],
          "personId": 37971
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374800"
        }
      },
      "sessionIds": [
        38356
      ],
      "eventIds": []
    },
    {
      "id": 38096,
      "typeId": 11597,
      "title": "Anthropocentric Attribution Bias in Human Prediction of Robot Behavior",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In many types of human-robot interactions, people must track the beliefs of robots based on uncertain estimates of robots' perceptual and cognitive capabilities. Did the robot see what happened and did it understand what it saw? In this paper, we present preliminary experimental evidence that people estimating what a humanoid robot knows or believes about the environment anthropocentrically assume it to have human-like perceptual and cognitive capabilities. However, our results also suggest that people are able to adjust their incorrect assumptions based on observations of the robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37524
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37634
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37879
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378347"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=ZhUMuLKT4SU",
          "title": "Anthropocentric Attribution Bias in Human Prediction of Robot Behavior",
          "duration": 72,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38097,
      "typeId": 11597,
      "title": "Defining Transfers Between Multiple Service Robots",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In a future where many robot assistants support human endeavors, interactions with multiple robots either simultaneously or sequentially will occur. This paper highlights an initial exploration into one type of sequential interaction, which we call ``transfers'' between multiple service robots. We defined the act of transferring between service robots and further decomposed it into five stages. Our research was informed by a design workshop investigating usage of multiple service robots. We also identified open design and research questions on this topic.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute "
            }
          ],
          "personId": 37746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37492
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378258"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=miso-QODdAc",
          "title": "Defining Transfers Between Multiple Service Robots",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38098,
      "typeId": 11597,
      "title": "Collecting social signals in constructive and destructive events during human-robot collaborative tasks",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Adapting behaviors based on others' reactions is a fundamental skill for a social robot that must interact with people. In this work, we to develop a systematic method to collect ecologically plausible data of human reactions to robot behaviors and associated valence. We designed a dyadic interaction were 24 participants played a board game in a human-robot team for a chance to win a chocolate. The ''Grumpy robot'' is responsible for losing an easy-to-win game, while the ''Kind robot'' for winning a seemingly impossible-to-win game. Questionnaires show that participants recognize both robots' critical impact on the game's outcome, but show similar social attraction towards both. Videos' reactions are distinct: smiles and neutral faces to the ''Kind robot'', and laughter, confusion, or shock to the ''Grumpy robot''. Collected data will be used to teach the robot to understand human reactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Institute for Systems and Robotics",
              "dsl": "Instituto Superior Técnico"
            }
          ],
          "personId": 37589
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "Faculty of Psychology"
            }
          ],
          "personId": 37883
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisboa",
              "city": "Lisboa",
              "institution": "Universidade de Lisboa",
              "dsl": "Institute for Systems and Robotics, Instituto Superior Técnico"
            }
          ],
          "personId": 37948
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Psicologia, Universidade de Lisboa",
              "dsl": "CICPSI"
            }
          ],
          "personId": 37235
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "Instituto Superior Tecnico, University of Lisbon",
              "dsl": "Institute for Systems and Robotics"
            }
          ],
          "personId": 37636
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378259"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=tCTbYSMiWBw",
          "title": "Collecting social signals in constructive and destructive events during human-robot collaborative tasks",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38099,
      "typeId": 11521,
      "title": "A Three-Site Reproduction of the Joint Simon Effect with the NAO Robot",
      "award": "BEST_PAPER",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The generalizability of empirical research depends on the reproduction of findings across settings and populations. Consequently, generalizations demand resources beyond that which is typically available to any one laboratory. With collective interest in the joint Simon effect (JSE) – a phenomenon that suggests people work more effectively with humanlike (as opposed to mechanomorphic) robots – we pursued a multi-institutional research cooperation between robotics researchers, social scientists, and software engineers. To evaluate the robustness of the JSE in dyadic human-robot interactions, we constructed an experimental infrastructure for exact, lab-independent reproduction of robot behavior. Deployment of our infrastructure across three institutions with distinct research orientations (well-resourced versus resource-constrained) provides initial demonstration of the success of our approach and the degree to which it can alleviate technical barriers to HRI reproducibility. Moreover, with the three deployments situated in culturally distinct contexts (Germany, the U.S. Midwest, and the Mexico-U.S. Border), observation of a JSE at each site provides evidence its generalizability across settings and populations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "The University of Texas Rio Grande Valley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37493
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Cluster of Excellence Cognitive Interaction Technology"
            }
          ],
          "personId": 37279
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "CITEC, Bielefeld University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37322
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Cluster of Excellence Cognitive Interaction Technology"
            }
          ],
          "personId": 37910
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "BIelefeld",
              "institution": "Bielefeld University ",
              "dsl": "CITEC"
            }
          ],
          "personId": 37926
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "Psychological and Brain Sciences"
            }
          ],
          "personId": 37658
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 37344
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374783"
        }
      },
      "sessionIds": [
        38344
      ],
      "eventIds": []
    },
    {
      "id": 38100,
      "typeId": 11521,
      "title": "Comedians in Cafes Getting Data: Evaluating Timing and Adaptivity in Real-World Robot Comedy Performance",
      "award": "BEST_PAPER",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots and autonomous social agents are becoming more ingrained in our everyday lives. Interactive agents from Siri to Anki’s Cozmo robot include the ability to tell jokes to engage users. This ability will build in importance as in-home social agents take on more intimate roles, so it is important to gain a greater understanding of how robots can best use humor. Stand-up comedy provides a naturally-structured experimental context for initial studies of robot humor. In this preliminary work, we aimed to compare audience responses to a robotic stand-up comedian over multiple performances that varied robot timing and adaptivity. Our first study of 22 performances in the wild showed that a robot with good timing was significantly funnier. A second study of 10 performances found that an adaptive performance was not necessarily funnier, although adaptations almost always improved audience perception of individual jokes. The end result of this research provides key clues for how social robots can best engage people with humor",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "No affiliation",
              "dsl": ""
            }
          ],
          "personId": 37615
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 37743
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374780"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=to2FQRYrbXQ",
          "title": "Comedians in Cafes Getting Data: Evaluating Timing and Adaptivity in Real-World Robot Comedy Performance",
          "duration": 487,
          "type": "video"
        }
      },
      "sessionIds": [
        38361
      ],
      "eventIds": []
    },
    {
      "id": 38101,
      "typeId": 11597,
      "title": "Automatic Engagement Recognition of Children within Robot-Mediated Autism Therapy",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes a work in progress that aims to design automatic engagement recognition of Robot-Mediated Therapy (RMT) tailored for children with a diverse form of Autism Spectrum Disorder (ASD), Attention Deficit Hyperactivity Disorder (ADHD), and Delayed Speech Development (DSD). To this end, we utilized videos obtained from RMT sessions of 36 children aged 4-12 years old. The videos were pre-processed to be used for machine learning approaches. Also, by using the OpenPose tool the datapoints of the facial features and body joints were extracted. Based on these results, automatic engagement recognition model is going to be designed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            },
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37405
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            },
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37463
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            },
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37747
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Department of Robotics and Mechatronics"
            },
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Department of Robotics and Mechatronics"
            }
          ],
          "personId": 37483
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": "School of Science and Technology"
            },
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": "School of Science and Technology"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378390"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=sLiR41QedOA",
          "title": "Automatic Engagement Recognition of Children within Robot-Mediated Autism Therapy",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38102,
      "typeId": 11597,
      "title": "Child-centered Action Recognition in RGB and RGB-D Data",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The paper presents an ongoing work that aims for real-time \r\naction recognition specifically tailored for child-centered research. To this end, we collected and annotated a dataset of 200 primary school children aged 6 to 11 years old. Each child was asked to perform seven actions in total in front of Kinect depth camera and a standard RGB camera: walking from side to side, jogging, running, boxing, waving, clapping, and walking towards to the camera. Each action was repeated 3-4 times for each participant. By using the OpenPose tool the datapoints of the joints were extracted. The aim of this work is to build a prediction model which classifies the action, age and gender of the participant. This paper presents the ongoing work with this dataset and the results will be ready to be presented at the poster session in March 2020.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37747
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37463
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37405
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": ""
            }
          ],
          "personId": 37688
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": "School of Science and Technology"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378391"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=lOLIDKsrJpQ",
          "title": "Child-centered Action Recognition in RGB and RGB-D Data",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38103,
      "typeId": 11597,
      "title": "Does the Appearance of an Agent Affect How We Perceive his/her Voice? Audio-visual Predictive Processes in Human-robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Robots increasingly become part of our lives. How we perceive and predict their behavior has been an important issue in HRI. To address this issue, we adapted a well-established prediction paradigm from cognitive science for HRI. Participants listened a greeting phrase that sounds either human-like or robotic. They indicated whether the voice belongs to a human or a robot as fast as possible with a key press. Each voice was preceded with a human or robot image (a human-like robot or a mechanical robot) to cue the participant about the upcoming voice. The image was either congruent or incongruent with the sound stimulus. Our findings show that people reacted faster to robotic sounds in congruent trials than incongruent trials, suggesting the role of predictive processes in robot perception. In sum, our study provides insights about how robots should be designed, and suggests that designing robots that do not violate our expectations may result in a more efficient interaction between humans and robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Ankara University",
              "dsl": "Social Psychiatry"
            }
          ],
          "personId": 37527
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Interdisciplinary Neuroscience Program"
            }
          ],
          "personId": 37427
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37890
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology and Neuroscience"
            }
          ],
          "personId": 37912
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378302"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Yxcn4Kilo6g",
          "title": "Does the Appearance of an Agent Affect How We Perceive his/her Voice? Audio-visual Predictive Processes in Human-robot Interaction",
          "duration": 118,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38104,
      "typeId": 11597,
      "title": "Cancer Genetic Counseling by Humanoid Robot: Modeling Multimodal Communication of Health Risk",
      "award": "BEST_PAPER",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We describe the design and evaluation of a humanoid robot that explains inherited breast cancer genetics, and motivates women to obtain cancer genetic testing. The counseling dialogue is modeled after a human cancer genetic counselor, extended with data visualizations and nonverbal behavior. In a quasi-experimental pilot study, we demonstrated that interaction with the robot leads to significant increases in cancer genetics knowledge.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 37163
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 37355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Dana-Farber Cancer Institute",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Harvard Medical School",
              "dsl": ""
            }
          ],
          "personId": 37969
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 37649
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378303"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=RqDvFrAH--k",
          "title": "Cancer Genetic Counseling by Humanoid Robot: Modeling Multimodal Communication of Health Risk",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38105,
      "typeId": 11597,
      "title": "Ergonomic Adaptation of Robotic Movements in Human-Robot Collaboration",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Musculoskeletal Disorders (MSDs) are common occupational diseases. An interesting research question is whether collaborative robots actively can minimise the risk of MSDs during collaboration. In this work ergonomic adaptation of robotic movements during human-robot collaboration is explored in a first test case, namely, adjustment of work sureface height. Vision based markerless posture estimation is used as input in combination with ergonomic assessment methods to adapt robotic movements in order to facilitate better ergonomic conditions for the human worker. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 37569
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 37864
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378304"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=vhp9RGAhe6s",
          "title": "Ergonomic Adaptation of Robotic Movements in Human-Robot Collaboration",
          "duration": 118,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38106,
      "typeId": 11523,
      "title": "Measuring Relational Trust in Human-Robot Interactions",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "HRI Lab"
            }
          ],
          "personId": 37590
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377435"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38107,
      "typeId": 11597,
      "title": "Estimating and Influencing User Mental Models of a Robot's Perceptual Capabilities: Initial Development and Pilot Study",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "People develop mental models of robots to improve their interactions with them, but predictions from these models are not always accurate. Robots often fail to communicate their capabilities, especially perceptual capabilities---i.e., what they can sense and understand about the world. This paper describes ongoing preliminary work towards enabling robots to autonomously estimate and influence human beliefs about robot perceptual capabilities. A custom-designed, web-based game is being used to establish feasibility and guide ongoing work. Our approach is discussed along with results from a pilot study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": ""
            }
          ],
          "personId": 37768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": ""
            }
          ],
          "personId": 37267
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37741
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378392"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=kq1hlr7xt9Y",
          "title": "Estimating and Influencing User Mental Models of a Robot's Perceptual Capabilities: Initial Development and Pilot Study",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38108,
      "typeId": 11597,
      "title": "Legible Light Communications for Factory Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This work focuses on methods to improve mobile robot legibility in factories using lights. Implementation and evaluation were done at a robotics company that manufactures factory robots that work in human spaces. Three new sets of communicative lights were created and tested on the robots, integrated into the company's software stack and compared to the industry default lights that currently exist on the robots. All three newly designed light sets outperformed the industry default. Insights from this work have been integrated into software releases across North America.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Robotics"
            }
          ],
          "personId": 37584
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Kitchener",
              "institution": "OTTO Motors",
              "dsl": "Controls Autonomy Development"
            }
          ],
          "personId": 37470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 37275
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378305"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=URwIYnU4puM",
          "title": "Legible Light Communications for Factory Robots",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38109,
      "typeId": 11597,
      "title": "Uncertainty in Robot Assisted Second Language Conversation Practice",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Moments of uncertainty are common for learners when practicing a second language. The appropriate management of these events could help avoid the development of frustration and benefit the learner’s experience. Therefore, its detection is crucial in language practice conversations. In this study, an experimental conversation between an adult second language learner and a social robot is employed to visually characterize the learners’ uncertainty. The robot’s output is manipulated in prosody and lexical levels to provoke uncertainty during the conversation. These reactions are then processed to obtain Facial Action Units (AUs) and Gaze features. Preliminary results show distinctive behavioral patterns of uncertainty among the participants. Based on these results, a new annotation scheme is proposed, which will expand the data used to train sequential models to detect uncertainty. As future steps, the robotic conversational partner will use this information to adapt its behavior in dialogue generation and language complexity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Speech, Music and Hearing (TMH)"
            }
          ],
          "personId": 37837
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt University",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 37686
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Speech, Music and Hearing (TMH)"
            }
          ],
          "personId": 37847
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378306"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=A1czOVtw9ak",
          "title": "Uncertainty in Robot Assisted Second Language Conversation Practice",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38110,
      "typeId": 11523,
      "title": "Intuitive Autonomy for Real Users of Small Unmanned Aerial Vehicles (sUAV)",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Autonomous systems requiring user supervision or manual control of autonomy are becoming more prevalent in real world deployments. These systems will require transitions of control between autonomous and manual operations with users being required to both take control from and cede control to autonomy. Prior work inHuman-Drone Interaction (HDI) has observed or designed for user interaction with a perfectly functioning robot, but has not looked at interactions with a robot that is about to fail. In this paper we describe results from initial work on characterizing user responses to failures in aerial autonomous systems. Ongoing and future work involves evaluating user proficiency in system operation and its impact on HDI with semi-autonomous systems. This work is novel in the context of small Unmanned Aerial Vehicles (sUAVs) and will inform sUAV autonomy designers for systems with a range of user training from search and rescue to hobbyist users through recommendations for training, necessary timelines for information sharing, and failure planning or contingency options in HDI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Lincoln",
              "institution": "University of Nebraska-Lincoln",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 37382
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Lincoln",
              "institution": "University of Nebraska, Lincoln",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 37289
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377440"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=u_QN39wmz4k",
          "title": "Intuitive Autonomy for Real Users of Small Unmanned Aerial Vehicles (sUAV)",
          "duration": 161,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38111,
      "typeId": 11521,
      "title": "Decision-Making for Bidirectional Communication in Sequential Human-Robot Collaborative Tasks",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Communication is critical to collaboration; however, too much of it can degrade performance. Motivated by the need for effective use of a robot’s communication modalities, in this work, we present a computational framework that decides if, when, and what to communicate during human-robot collaboration. The framework, titled CommPlan, consists of a model specification process and an execution-time POMDP planner. To address the challenge of collecting interaction data, the model specification process is hybrid: where part of the model is learned from data, while the remainder is manually specified. Given the model, the robot’s decision-making is performed computationally during interaction and under partial observability of human’s mental states. We implement CommPlan for a shared workspace task, in which the robot has multiple communication options and needs to reason within a short time. Through experiments with human participants, we confirm that CommPlan results in the effective use of communication capabilities and improves human-robot collaboration. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37294
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "CAMBRIDGE",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Interactive Robotics Group"
            }
          ],
          "personId": 37261
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 37475
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374779"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=WU2Dh7QLG4Y",
          "title": "Decision-Making for Bidirectional Communication in Sequential Human-Robot Collaborative Tasks",
          "duration": 568,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38112,
      "typeId": 11597,
      "title": "A Robot Mediated Music Mixing Activity for Promoting Collaboration among Children",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Since children show favoritism of in-group members over out-group members from the age of five, children that newly arrive in a country or culture might have difficulties to be integrated into the already settled group. To address this problem, we developed a robot-mediated music mixing game for three players that aims to bring together children from the newly arrived and settled groups. We designed a game with the robot's goal in mind and allow the robot to observe the participation of the different players in real-time. With this information, the robot can encourage equal participation in the shared activity by prompting the least active child to act. Preliminary results show that the robot can potentially succeed in influencing participation behavior. These results encourage future work that not only studies the in-game effects but also effects on group dynamics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Department of Robotics, Perception and Learning"
            }
          ],
          "personId": 37867
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Department of Robotics Perception and Learning"
            }
          ],
          "personId": 37917
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378307"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Z5ZrvYFFNDM",
          "title": "A Robot Mediated Music Mixing Activity for Promoting Collaboration among Children",
          "duration": 127,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38113,
      "typeId": 11523,
      "title": "Immersive Control of a Robot Surrogate for Users in Palliative Care",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Quality of life (QoL) is especially important for palliative care patients, who can be at risk of mental health issues. This project aims to design and implement an immersive control system for a robotic surrogate, that allows users to interact as naturally as possible with their loved ones from a remote location, and navigate appropriately, in order to improve QoL for those living with a terminal illness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37467
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37754
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37884
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377445"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=02d-RrJszms",
          "title": "Immersive Control of a Robot Surrogate for Users in Palliative Care",
          "duration": 167,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38114,
      "typeId": 11597,
      "title": "Juiced and Ready to Predict Private Information in Deep Cooperative Reinforcement Learning",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In human-robot collaboration settings, each agent often has access to private information (PI) that is unavailable to others. Examples include task preferences, objectives, and beliefs. Here, we focus on the human-robot dyadic scenarios where the human has private information, but is unable to directly convey it to the robot. In this late breaking report, we present Q-Network with Private Information and Cooperation (Q-PICo), a method for training robots that can interactively assist humans with PI. In contrast to existing approaches, we explicitly model PI prediction, leading to a more interpretable network architecture. We also contribute Juiced, an environment inspired by the popular video game Overcooked, to test Q-PICo and other similar methods for human-robot collaboration. Our initial experiments in Juiced show that the agents trained with Q-PICo can accurately predict PI and exhibit collaborative behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37423
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37633
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37871
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37662
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37451
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378308"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=r9sO1Dedhr0",
          "title": "Juiced and Ready to Predict Private Information in Deep Cooperative Reinforcement Learning",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38115,
      "typeId": 11597,
      "title": "Human-Robot Cooperation in Prisoner Dilemma Games: People Behave More Reciprocally than Prosocially Toward Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study investigated human-robot cooperation in the context of prisoner’s dilemma games and examined the extent to which people’s willingness to cooperate with a robot would vary according to the incentives provided by a game context. We manipulated the payoff matrices of human-robot prisoner’s dilemma games and predicted that people would cooperate more often in the situation where cooperating with the robot was a relatively more rewarding option. Our results showed that, in the early rounds of the game, participants made significantly more cooperative decisions, when the game structure providing more incentives for cooperation. However, their subsequent game decisions were dominantly driven by Cozmo’s previous game choices and the incentive structure was no longer a predictive factor to their decisions. The findings suggest that people have a strong reciprocal tendency to social robots in economic games and this tendency might even surpass the influence of the reward value of their decisions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 37272
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 37854
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "Macquarie University",
              "dsl": ""
            }
          ],
          "personId": 37561
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378309"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38116,
      "typeId": 11523,
      "title": "Multimodal Training by Demonstration for Robot-Assisted Surgery",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Improving surgical training has the potential to reduce medical errors and consequently to save many lives. We briefly present our efforts to improve this training for robot-assisted surgery. In particular, we explore how data collected from expert demonstrations can enhance the training efficiency for novices. Thus far, our results show that combining hand-over-hand training based on experts’ motion data with trial and error training can improve the training outcomes in robotic and conventional laparoscopic surgery settings. We briefly describe our current efforts for exploring how gaze-based training methods, based on experts’ eye gaze data, can improve the training outcomes as well.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Electrical and Computer Engineering Department"
            }
          ],
          "personId": 37929
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 37171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 38368
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377448"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38117,
      "typeId": 11523,
      "title": "Meow: A Morning Companion Robot",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "Our robot is a very friendly, helpful and considerate morning-companion-clock, but also a very persuasive one! It uses multiple methods to effectively wake you up (light, sounds and movements), then it guides you through a simple stretching routine, accompanied by fresh aroma and soft music to help you make a perfect start of the day. Contrary to other alarm clocks, which wake you up by being annoying, the morning companion is so nice and pleasant, that you can form a bond with it and want to wake up to start the day in its company.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunication",
              "dsl": "International School"
            }
          ],
          "personId": 37996
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunication",
              "dsl": "International School"
            }
          ],
          "personId": 37299
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunication",
              "dsl": "International School"
            }
          ],
          "personId": 37564
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379457"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=hHj-RfdV-M0",
          "title": "Meow: A Morning Companion Robot",
          "duration": 236,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38118,
      "typeId": 11523,
      "title": "RemindLy, a personal note-bot assistant",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "In this project we present the design concept of RemindLy, a small egg-shaped robot with the purpose of reminding its user the daily to-do list. Users can interact with their RemindLy both using voice commands and physical interactions, designed as small to large tilting and rotations of the robot body. A small camera is placed on the robot to add a wake-up function when the face of the owner is recognized. To properly set-up the notes and customize the robot, a phone app will be developed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Brescia",
              "institution": "University of Brescia",
              "dsl": "Department of Mechanical and Industrial Engineering"
            }
          ],
          "personId": 37701
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "BS",
              "city": "Brescia",
              "institution": "University of Brescia",
              "dsl": "Department of Mechanical and Industrial Engineering"
            }
          ],
          "personId": 37526
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Brescia",
              "institution": "Università degli Studi di Brescia",
              "dsl": ""
            }
          ],
          "personId": 37916
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "BS",
              "city": "Brescia",
              "institution": "Università degli Studi di Brescia",
              "dsl": ""
            }
          ],
          "personId": 37895
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379456"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?lPq807txw7Q",
          "title": "RemindLy: A Personal Note-bot Assistant",
          "duration": 305,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38119,
      "typeId": 11523,
      "title": "The Sorting Hat, a Mediator Social Robot with a Fictional Character Appearance for Autistic Children",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The “Sorting Hat”, inspired from Harry Potter by J.K. Rowling was designed as a mediator social robot for autistic children. Since they are unable to interact with peers and show their emotions, the hat plays the role of a mediator to fill the communication gap. It is recommended to use the Sorting Hat in environments where children interact or in a domestic area. The hat was tested with 430 random set of volunteers to measure the attraction, the interactivity in use and the data demonstrated positive results. It also consists of an EEG sensor to record the raw brain wave data for future cognitive analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Provice",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37756
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Provice",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37496
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Provice",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37734
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Provice",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37628
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Province",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37936
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "state": "Western Province",
              "city": "Nugegoda",
              "institution": "University of Sri Jayewardenepura",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37960
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379460"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=tUKv53I4UAY",
          "title": "The Sorting Hat, a Mediator Social Robot with a Fictional Character Appearance for Autistic Children",
          "duration": 446,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38120,
      "typeId": 11597,
      "title": "Behavior-based Risk Detection of Autism Spectrum Disorder through Child-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This work presents a method to identify children at risk for Autism Spectrum Disorder (ASD) using behavioral data extracted from video analysis of child-robot interactions. Robots were used as a tool to elicit social engagement from the children in order to capture their social behaviors. A Convolutional Neural Network was used to classify the behavioral data as either at-risk ASD or Typical Development (TD). The network performance was compared to two machine learning classifiers and the utility of the proposed method as a way to streamline existing diagnostic procedures was discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Washington, DC",
              "institution": "The George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 37204
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 37855
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378382"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rK7WloUHaUg",
          "title": "Behavior-based Risk Detection of Autism Spectrum Disorder through Child-Robot Interaction",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38121,
      "typeId": 11521,
      "title": "What’s in a Name? : Effects of Category Labels on the Consumers’ Acceptance of Robotic Products ",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "A study was conducted to investigate the effects of category labels of domestic robots on their consumer acceptance. The authors posited that compared to the label robots, a pre-existent category label such as home appliances would increase the consumers’ evaluation of and purchase intention towards the products. It is suggested that the pre-existent category label helps consumers to perceive the functional values they stand to gain by consuming the product more than the label robots, which is often related to the concepts generated around cultural artifacts. The results of the study confirmed the hypotheses, and further discussions are provided in this paper. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KB Financial Group",
              "dsl": ""
            }
          ],
          "personId": 37621
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37856
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37574
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37550
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374799"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=1Oli5XhNKK4",
          "title": "What's in a Name?: Effects of Category Labels on the Consumers' Acceptance of Robotic Products",
          "duration": 597,
          "type": "video"
        }
      },
      "sessionIds": [
        38356
      ],
      "eventIds": []
    },
    {
      "id": 38122,
      "typeId": 11597,
      "title": "Robot that Sweats to Remind the Elderly of High-temperature",
      "award": "HONORABLE_MENTION",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The elderly are more affected by higher environmental temperatures. If they misperceive the temperature, it can lead to a number of potentially dangerous health issues. To address this, we propose a robot that sweats to indicate the high environmental temperature to the elderly. In this paper, we present the design of our first prototype for exploring the human perception of robot sweat status.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 37508
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 37949
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378383"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=AS8F5OY3pUQ",
          "title": "Robot that Sweats to Remind the Elderly of High-temperature",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38123,
      "typeId": 11597,
      "title": "Are People Ready for Social Robots in Public Spaces?",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Recent developments in robotics are changing the nature of service, and research in human-robot interaction has previously shown that humanoid robots could possibly work in public spaces. We conducted an ethnographic study with the humanoid Pepper robot at a central train station. The result show that people are not yet accustomed to talking to robots, and people seem to expect that the robot does not talk, that it is a queue ticket machine, and that, one should interact with it by using the tablet on the robot’s chest.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": ""
            }
          ],
          "personId": 37954
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 37879
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378294"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CQNC5gvsRNM",
          "title": "Are People Ready for Social Robots in Public Spaces?",
          "duration": 118,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38124,
      "typeId": 11597,
      "title": "Social Robots Don’t Do That: Exploring Robot-Typical Errors in Child-Robot Interaction ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Researchers have explored how robot errors affect people’s perceptions of and interactions with robots. However, the types of robot errors that have been studied often reflect errors that humans tend to make, instead of those typically made by robots. In this paper we explore robot-typical errors, as opposed to human-like errors, spearheading a discussion on the kinds of mistakes we may face from robots. We specifically focus on child-robot interaction, and how robot-typical errors may occur in the presence of children.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37869
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37461
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378295"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=PDFHIXrC8Ow",
          "title": "Social Robots Don’t Do That: Exploring Robot-Typical Errors in Child-Robot Interaction ",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38125,
      "typeId": 11597,
      "title": "Cognitive Performance Assessment based on Everyday Activities for Human-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Human-Robot Interaction often involves a robot assisting or providing feedback to a human partner's performance or cooperating to complete a task. In such an interaction scenario, the robotic system requires to perceive the human teammate's cognitive state that might affect task performance. In this pilot study, the focus is on developing a framework that assesses the human's cognitive performance for human-robot synergetic task, such as an assembly task. Specifically, we explore the correlation between a person's quality of sleep and performance metric through a standard task for cognitive assessment, the N-back task. To validate our hypothesis, we conducted a study with 25 participants, and our results indicate that there is a moderate correlation between some stages of sleep cycle and performance. Additionally, we present a possible Human-Robot Interaction setup that could benefit from our results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "The University of Texas, Arlington",
              "dsl": "Heracleia Lab, Department of Computer Science"
            }
          ],
          "personId": 37175
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "University of Texas at Arlington ",
              "dsl": "Computer Science and Engineering Department "
            }
          ],
          "personId": 37432
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "The University of Texas at Arlington",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 37188
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "The University of Texas at Arlington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 37305
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378384"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=sCSoQRtNjmA",
          "title": "Cognitive Performance Assessment based on Everyday Activities for Human-Robot Interaction",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38126,
      "typeId": 11597,
      "title": "AIDA: Using Social Scaffolding to Assist Workers with Intellectual and Developmental Disabilities",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": " In this paper, we present a social robotics framework to assist workers with intellectual and developmental disabilities (IDD). AIDA, which stands for artificially intelligent disability assistant, will help workers with IDD through social scaffolding techniques. For the experiments, we simulated disabilities with our participants and evaluated the impact of social scaffolding with the Pepper humanoid robot. The results show that stronger forms of scaffolding are required to provide more effective assistance workers with IDD.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37632
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37582
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378385"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=k8FiKF0BjuQ",
          "title": "AIDA: Using Social Scaffolding to Assist Workers with Intellectual and Developmental Disabilities",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38127,
      "typeId": 11597,
      "title": "“What Could Possibly Go Wrong?” Logging HRI Data for Robot Accident Investigation",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This abstract presents proposed experimental work to consider what might be required for an ‘ethical black box’, essentially a robot data recorder, to inform robot accident investigation processes and the implications for HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37520
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37465
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37842
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford ",
              "institution": "University of Oxford ",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37200
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottinhgham",
              "institution": "Nottingham University",
              "dsl": "Business School"
            }
          ],
          "personId": 37538
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "-",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37940
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378296"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38128,
      "typeId": 11597,
      "title": "Neural Speech Synthesis with Style Intensity Interpolation: a Perceptual Analysis",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "State of the art in speech synthesis considerably reduced the gap between synthetic and human speech on the perception level. However the impact of a speech style control on the perception is not well known. \r\nIn this paper, we propose a method to analyze the impact of controlling the TTS system parameters on the perception of the generated sentence. This is done through a visualization and analysis of listening test results.\r\nFor this, we train a speech synthesis system with different discrete categories of speech styles. Each style is encoded using a one-hot representation in the network. After training, we interpolate between the vectors representing each style. A perception test showed that despite being trained with only discrete categories of data, the network is capable of generating intermediate intensity levels between neutral and a given speech style.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 37617
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 37365
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 37677
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378297"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=8QdZ9pwhV-c",
          "title": "Neural Speech Synthesis with Style Intensity Interpolation: a Perceptual Analysis",
          "duration": 126,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38129,
      "typeId": 11597,
      "title": "Long-term motion generation for interactive humanoid robots using GAN with convolutional network",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this report, we propose a framework for generating long-term human-like motion based on a deep generative model.\r\nThanks to the network structure, the proposed method allows generating seem-less long-term motions while the model is trained by 4 seconds long short motion samples.\r\nThe CG of generated motions seems to be reproduced scenes where a pair of persons talking to each other.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate school of engineering science"
            }
          ],
          "personId": 37876
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate school of engineering science"
            }
          ],
          "personId": 37202
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 37995
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378386"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=60p4XRDfKVM",
          "title": "Long-term motion generation for interactive humanoid robots using GAN with convolutional network",
          "duration": 108,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38130,
      "typeId": 11597,
      "title": "\"But They’re My Avatar\": Examining Character Attachment to Android Avatars in Quantic Dream’s Detroit: Become Human",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study examines how players describe their rationale behind decisions made during gameplay of Detroit: Become Human and how responses may coincide with character attachment (CA). Semi-structured interviews were conducted to examine the presence of character attachment and how it may lead to the player’s understanding of their gameplay choices. Both the emotions (or lack of ) of the avatar and strategizing about gameplay emerged as two central themes. Results are discussed in light of human-robot interaction based on popular media.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Kalamazoo",
              "institution": "Western Michigan University",
              "dsl": ""
            }
          ],
          "personId": 37951
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Kalamazoo",
              "institution": "Western Michigan University ",
              "dsl": "Communication & Social Robotics Labs"
            }
          ],
          "personId": 37675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Kalamazoo",
              "institution": "Western Michigan University",
              "dsl": "Communication & Social Robotics Labs"
            }
          ],
          "personId": 37227
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378298"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38131,
      "typeId": 11521,
      "title": "Why Should We Gender? The Effect of Robot Gendering and Occupational Stereotypes on Human Trust and Perceived Competency",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The attribution of human-like characteristics onto humanoid robots has become a common practice in Human-Robot Interaction by designers and users alike. Robot gendering, the attribution of gender onto a robotic platform via voice, name, physique, or other features is a prevalent technique used to increase aspects of user acceptance of robots. One important factor relating to user acceptance is user trust. As robots continue to integrate themselves into common societal roles, it will be critical to evaluate user trust in the robot’s ability to do its job. This paper examines the relationship among occupational gender-roles, user trust and gendered design features of humanoid robots. Results from the study indicate that there was no significant difference in the perception of trust in the robot’s competency when considering the gender of the robot. This expands the findings found in prior efforts that suggest performance-based factors have larger influences on user trust than the robot’s gender characteristics. In fact, our study suggests that perceived occupational competency may be a better predictor for human trust than robot gender, occupational gender association, or participant gender. As such, gendering in robot design should be considered critically in the context of the application by designers. Such precautions would reduce the potential for robotic technologies to perpetuate societal gender stereotypes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 37953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Public Policy and Office of Graduate Studies"
            }
          ],
          "personId": 37333
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technolog",
              "dsl": ""
            }
          ],
          "personId": 37576
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374778"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=sak55q6oMGg",
          "title": "Why Should We Gender?: The Effect of Robot Gendering and Occupational Stereotypes on Human Trust and Perceived Competency",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38132,
      "typeId": 11521,
      "title": "Interacting with a Social Robot Affects Visual Perception of Space",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Human partners are very effective at coordinating in space and time. Such ability is particular remarkable considering that visual perception of space is a complex inferential process, which is affected by individual prior experience (e.g. the history of previous stimuli). As a result, two partners might perceive differently the same stimulus. Yet, they find a way to align their perception, as demonstrated by the high degree of coordination observed in sports or even in everyday gestures as shaking hands. Robots would need a similar ability to align with their partner's perception. However, to date there is no knowledge of how the inferential mechanism supporting visual perception operates during social interaction. In the current work, we use a humanoid robot to address this question. We replicate a standard protocol for the quantification of perceptual inference in a HRI setting. Participants estimated the length of a set of segments presented by the humanoid robot iCub. The robot behaved in one condition as a mechanical arm driven by a computer and in another condition as an interactive, social partner. Even if the stimuli presented were the same in the two conditions, length perception was different when the robot was judged as an interactive agent rather than a mechanical tool. When playing with the social robot, participants relied significantly less on stimulus history. This result suggests that the brain changes optimization strategies during interaction and lay the foundations to design humanaware robot visual perception. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": "CONTACT UNIT"
            }
          ],
          "personId": 37728
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "Istituto Italiano di Tecnologia ",
              "dsl": "Robotics, Brain and Cognitive Sciences"
            }
          ],
          "personId": 37431
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": ""
            }
          ],
          "personId": 37706
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Italian Institute of Technology",
              "dsl": "RBCS"
            }
          ],
          "personId": 37466
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374819"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=UpXYxXP3iX8",
          "title": "Interacting with a Social Robot Affects Visual Perception of Space",
          "duration": 499,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38133,
      "typeId": 11597,
      "title": "Understanding Robots’ Potential to Facilitate Piano Cognitive Training in Older Adults with Mild Cognitive Impairment",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Declining cognitive abilities can have a tremendous impact on one’s ability to age healthily and maintain a high quality of life. Cognitive training has been shown to improve neural plasticity and increase cognitive reserve, reducing the risk of dementia. Specifically, learning to play the piano has been shown to be an engaging, multimodal form of cognitive training. Socially assistive robots (SAR) present a unique opportunity to increase access to user-tailored piano learning cognitive training. In this report we present a four-week robot lead piano lesson feasibility intervention for older adults with mild cognitive impairment. Specifically, engaging with the SAR improved cognitive function across multiple domains, and participants found the SAR a highly competent instructor.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "University of Georgia",
              "dsl": "School of Social Work"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "University of Georgia",
              "dsl": "Institute of Gerontology"
            }
          ],
          "personId": 37877
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "University of Georgia",
              "dsl": "Institute of Gerontology"
            }
          ],
          "personId": 37591
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "The University of Georgia",
              "dsl": "Institute of Gerontology"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "The University of Georgia",
              "dsl": "Interdisciplinary Neuroscience Program"
            }
          ],
          "personId": 37445
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Columbia",
              "institution": "VAN Robotics",
              "dsl": ""
            }
          ],
          "personId": 37593
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Columbia",
              "institution": "Van Robotics",
              "dsl": ""
            }
          ],
          "personId": 37329
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Loretto",
              "institution": "Applied Universal Dynamics, Corp.",
              "dsl": ""
            }
          ],
          "personId": 37580
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "University of Georgia",
              "dsl": ""
            }
          ],
          "personId": 37225
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Athens",
              "institution": "University of Georgia",
              "dsl": "Institute of Gerontology"
            }
          ],
          "personId": 37693
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378299"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=p72Txe3tZQo",
          "title": "Understanding Robots’ Potential to Facilitate Piano Cognitive Training in Older Adults with Mild Cognitive Impairment",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38134,
      "typeId": 11597,
      "title": "Robotic Limb Repositioning with Supervised Autonomy",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents the initial efforts towards developing a robotic limb repositioning system. We aim to combine programming by demonstration and end-user programming in a tele-manipulation system that includes the user in the loop. We propose an approach based on a general-purpose mobile manipulator and a web-based interface where a user can select, edit, preview and execute different repositioning exercises based on the selected limb. This approach shows the potential to empower people who have mobility impairments to be more involved in an activity of daily living.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 38002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland, Baltimore County",
              "dsl": "Department of Computer Science and Electrical Engineering"
            }
          ],
          "personId": 37952
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 37521
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 37944
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378387"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=PH6KkGHn38A",
          "title": "Robotic Limb Repositioning with Supervised Autonomy",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38135,
      "typeId": 11597,
      "title": "Increasing Telepresence Robot Operator Awareness of Speaking Volume Appropriateness: Initial Model Development",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Telepresence robots could help homebound students to be physically embodied and socially connected in the classroom. However, most telepresence robots do not provide their operators with information about whether their speaking volume is appropriate in the remote context. We are investigating how operators could benefit from live feedback about speaking volume appropriateness as part of our ongoing research on using remote presence robots to improve education and social connectedness for students experiencing extended absences from school. This preliminary report describes (1) the development of a model of speaking volume appropriateness to provide this feedback, (2) implementation of a feedback element in the operator's user interface, and (3) plans for long-term deployment to assess impacts on the social and educational experience of homebound high school students. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37412
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Division of Engineering Education"
            }
          ],
          "personId": 37376
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378388"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cTvNR9hPoos",
          "title": "Increasing Telepresence Robot Operator Awareness of Speaking Volume Appropriateness: Initial Model Development",
          "duration": 103,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38136,
      "typeId": 11597,
      "title": "User Needs and Design Opportunities in End-User Robot Programming",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We report on a user study that sought to understand how users program robot tasks by direct demonstration and what problems they encounter when using a state-of-the-art robot programming interface to create and edit robot programs. We discuss how our findings translate to design opportunities in end-user robot programming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 38006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 37732
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378300"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=nlRoyGLH5ok",
          "title": "User Needs and Design Opportunities in End-User Robot Programming",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38137,
      "typeId": 11597,
      "title": "Distinguishing Robot Personality from Motion",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The central research question of this work is can robot motion effectively communicate distinct robot personalities? In this study, we implemented three distinct robot motion personalities inspired by a subset of the seven dwarfs: Happy, Sleepy, and Grumpy. We implemented autonomous motion generation systems that mapped each personality to path shape, timing, and seeking/avoidance of the participant features. A user study demonstrated that our 24 participants could distinguish these personalities. Robot motion style predicted robot personality features such as politeness, friendliness, and intelligence, which, for the most part, matched logically to the intended dwarf personality designs. These results indicate that robot motion style is sufficient to indicate a robot’s personality during its interactive behaviors with people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 37933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "wellesley",
              "institution": "wellesley college",
              "dsl": "Wellesley Engineering Laboratory"
            }
          ],
          "personId": 37408
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of Electrical Engineering & Computer Science"
            }
          ],
          "personId": 37268
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            },
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 37275
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378389"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38138,
      "typeId": 11597,
      "title": "Robots in Need: How Patterns of Emotional Behavior Influence Willingness to Help",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study explores how different patterns of robot emotional behavior influences people's intentions to help a robot requiring human assistance. In this online study, participants saw a set of videos of a robot receiving human help to get free of an obstruction, with different emotional behavior in each video. These initial results suggest that people would be more willing to aid a robot that showed either happy or sad behavior while stuck, compared to one that remained neutral. There was no influence of the behavior shown by the robot after being freed. This suggests that it is the behavior of the robot while stuck that is most influential in people's perceptions and intentions to interact with the robot. Furthermore, the finding that positive behavior increased intention to help suggests that behavior patterns that are unusual by human standards may be available to robots to gain assistance, but further work is required to identify whether this translates to actual helping.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37246
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "Experimental Psychology"
            }
          ],
          "personId": 37737
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37754
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378301"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=OkmmfOz5uV0",
          "title": "Robots in Need: How Patterns of Emotional Behavior Influence Willingness to Help",
          "duration": 130,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38140,
      "typeId": 11523,
      "title": "MoBi – An Interactive Classroom Robot Helping Children to Separate Waste",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "In order to live more sustainably, it is important to reduce, reuse and recycle the waste we produce. Therefore, those three principles of the waste hierarchy should be adopted from an early age to become a natural part of everyday behavior. Our robotic monster MoBi is an interactive robot designed to teach children how to handle familiar waste types produced in the classroom setting. It supports children in the decision-making process involved in correct waste separation and educates them about ways to avoid waste in the first place. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 37249
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen ",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 37280
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology "
            }
          ],
          "personId": 37794
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 37321
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 37417
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379459"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_DQmUiCstJY",
          "title": "MoBi – An Interactive Classroom Robot Helping Children to Separate Waste",
          "duration": 260,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38141,
      "typeId": 11523,
      "title": "Juno : An Interactive Storytelling Robot for Early Constructive Childhood Intervention",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "Early life adversity is a major risk factor for the development of psychological and behavioral problems in adult life. Traumatic experiences in childhood are linked to higher rates of depression, anxiety disorders and a range of mental health issues.[4] Additionally, stories form the basis of understanding in children and help develop empathy and cultivate imaginative and divergent thinking in them.\r\nIn this paper, we expound on the idea of leveraging the potential of storytelling through an interactive toy i.e. transitional object as a means of intentful intervention to help children understand and cope better with stressors in developmental ages.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujarat",
              "city": "Gandhinagar",
              "institution": "National Institute of Design",
              "dsl": ""
            }
          ],
          "personId": 37762
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujrat",
              "city": "Gandhinagar",
              "institution": "National Institute of Design",
              "dsl": "New Media Design"
            }
          ],
          "personId": 37802
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujarat",
              "city": "Gandhinagar",
              "institution": "National Institute of Design",
              "dsl": ""
            }
          ],
          "personId": 37209
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujrat",
              "city": "Gandhinagar ",
              "institution": "National institute of design",
              "dsl": "National institute of design "
            }
          ],
          "personId": 37872
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379458"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38142,
      "typeId": 11521,
      "title": "Is more autonomy always better? Exploring preferences of mobility-impaired users in robot-assisted feeding",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "A robot-assisted feeding system can potentially help a user with upper-body mobility impairments eat independently. However, autonomous assistance in the real world is challenging because of varying user preferences, impairment constraints, and possibility of errors in uncertain and unstructured environments. An autonomous robot-assisted feeding system needs to decide the appropriate strategy to acquire a bite of hard-to-model deformable food items, the right time to bring the bite close to the mouth, and the appropriate strategy to transfer the bite easily. Our key insight is that a system should be designed based on a user’s preference about these various challenging aspects of the task. In this work, we explore user preferences for different modes of autonomy given perceived error risks and also analyze the effect of input modalities on technology acceptance. We found that more autonomy is not always better, as participants did not have a preference to use a robot with partial autonomy over a robot with low autonomy. In addition, participants’ user interface preference changes from voice control during individual dining to web-based during social dining. Finally, we found differences on average ratings when grouping the participants based on their mobility limitations (lower vs. higher) that suggests that ratings from participants with lower mobility limitations are correlated with higher expectations of robot performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 37922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 37911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Personal Robotics Laboratory"
            }
          ],
          "personId": 37981
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 37521
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 37251
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 37944
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle ",
              "institution": "University of Washington ",
              "dsl": "School of Computer Science and Engineering "
            }
          ],
          "personId": 37656
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374818"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=M7Z2MAXbv9c",
          "title": "Is More Autonomy Always Better?: Exploring Preferences of Users with Mobility Impairments in Robot-assisted Feeding",
          "duration": 589,
          "type": "video"
        }
      },
      "sessionIds": [
        38345
      ],
      "eventIds": []
    },
    {
      "id": 38143,
      "typeId": 11521,
      "title": "Autonomously Learning One-To-Many Social Interaction Logic from Human-Human Interaction Data",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We envision a future where service robots autonomously learn how to interact with humans directly from human-human interaction data, without any manual intervention. In this paper, we present a data-driven pipeline that: (1) takes in low-level data of a\r\nhuman shopkeeper interacting with multiple customers (28 hours of collected data); (2) autonomously extracts high-level actions from that data; and (3) learns – without manual intervention – how a robotic shopkeeper should respond to customers’ actions online. Our proposed system for learning the interaction logic uses neural networks to first learn which customer actions are important to respond to and then learn how the shopkeeper should respond to those important customer actions. We present a novel technique for learning which customer actions are important by first learning the hidden causal relationship between customer and shopkeeper actions. In an offline evaluation, we show that our proposed technique significantly outperforms state-of-the-art baselines, in both which customer actions are important and how to respond to them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 37335
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Department of Social Informatics"
            }
          ],
          "personId": 37778
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Graduate School of Informatics"
            },
            {
              "country": "Japan",
              "state": "Kyouto",
              "city": "Souraku-gun",
              "institution": "ATR",
              "dsl": "IRC"
            }
          ],
          "personId": 37429
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kansai Science City",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": "Intelligent Robotics and Communication Laboratory"
            }
          ],
          "personId": 37611
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374798"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Ph6HcZlgXyw",
          "title": "Autonomously Learning One-To-Many Social Interaction Logic from Human-Human Interaction Data",
          "duration": 592,
          "type": "video"
        }
      },
      "sessionIds": [
        38349
      ],
      "eventIds": []
    },
    {
      "id": 38144,
      "typeId": 11521,
      "title": "Does Trait Loneliness Predict Rejection of Social Robots? The Role of Reduced Attributions of Unique Humanness ",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Since chronic loneliness is both a painful individual experience and an increasingly serious social problem, robot companions have emerged as a result of robotization of social work to confront this issue. We foresee that social robots will become pervasive in the near future. Thus, it is crucial to pinpoint the relationship between chronic experiences of loneliness (i.e., trait loneliness) and both anthropomorphism and acceptance of such artificial intelligent agents. Previous research demonstrated that experimentally induced state loneliness increases anthropomorphic inferences about nonhuman agents such as pets. However, in the present research we found that trait (vs. state) loneliness — a permanent personality disposition that is not easily relieved (vs. transitory experiences caused by circumstance, and easily relieved) — reduced participants’ anthropomorphic tendencies and acceptance of a social robot (regardless of the form: a picture of the robot, an on-site robot, or direct interaction with the robot). In particular, believing that the robot lacks good “unique humanness” traits (i.e., Humble, Thorough, Organized, Broadminded, and Polite) is one reason why dispositionally lonely participants are less likely to anthropomorphize a robot, which further prompts reduced acceptance of it. This finding suggests that unique humanness, exemplifying secondary emotions, is vital, not only in interpersonal contexts, but in establishing connections with social robots. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Tsinghua University",
              "dsl": "Tsinghua-Berkeley Shenzhen Institute"
            },
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37863
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanxi",
              "city": "Xi'an",
              "institution": "Xi'an Jiaotong University",
              "dsl": "Institution of Social Psychology"
            }
          ],
          "personId": 37829
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanxi",
              "city": "Xi'an",
              "institution": "Xi'an Jiaotong University",
              "dsl": "Institution of Social Psychology"
            }
          ],
          "personId": 37231
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37748
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374777"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JNCiHDMagTw",
          "title": "Does Trait Loneliness Predict Rejection of Social Robots?: The Role of Reduced Attributions of Unique Humanness",
          "duration": 431,
          "type": "video"
        }
      },
      "sessionIds": [
        38350
      ],
      "eventIds": []
    },
    {
      "id": 38145,
      "typeId": 11521,
      "title": "Social Robot for rehabilitation: Expert clinicians and post-stroke patients' evaluation following a long-term intervention",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We developed a novel gamified system for post-stroke long-term rehabilitation, using the humanoid robot Pepper (SoftBank, Aldebaran). In this paper, we present the results of a qualitative study with expert clinicians (n=12) on the compatibility of this system with the needs of post-stroke patients, and of a long-term intervention study with post-stroke participants (n=4) in a rehabilitation facility. Both the clinicians and the patients found the robot and the gamified system engaging, motivating and meet the needs of upper limb rehabilitation. The clinicians gave specific recommendations that may be applicable to a wide range of technologies for post-stroke rehabilitation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Recanati School for Community Health Professions, Department of Physical Therapy"
            }
          ],
          "personId": 37631
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Recanati School for Community Health Professions, Department of Physical Therapy"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Freiburg",
              "institution": "University of Freiburg",
              "dsl": "Freiburg Institute for Advanced Studies (FRIAS)"
            }
          ],
          "personId": 37635
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374797"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QhKV9DTmphU",
          "title": "Social Robot for rehabilitation: Expert clinicians and post-stroke patients' evaluation following a long-term intervention",
          "duration": 653,
          "type": "video"
        }
      },
      "sessionIds": [
        38345
      ],
      "eventIds": []
    },
    {
      "id": 38146,
      "typeId": 11597,
      "title": "Exploring Interactions between Rogue Autonomous Vehicles and People",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We present an early discussion on how people might interact with a rogue autonomous vehicle (AV). A rogue autonomous vehicle is one that may behave in unpredictable and dangerous ways because of malfunctioning sensors, vehicle tampering, or malicious hacking. Rogue AVs present a danger both to passengers’ and nearby pedestrians’ safety. To address this challenge we conducted a preliminary design study and gathered and analyzed design ideas that highlight how people envision they could interact with rogue AVs by both identifying and reacting to them. Our initial results highlight design concepts such as redundant sensors, tiered responses, audio and visual cues, and ways to obtain trusted confirmation. We conclude with a discussion on future steps for designing for interactions with rogue AVs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37601
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Victoria",
              "institution": "University of Victoria",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37637
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37795
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sendai",
              "institution": "Tohoku University",
              "dsl": "Research Institute of Electrical Communication"
            }
          ],
          "personId": 37645
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378316"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=0KmCZzPs5B8",
          "title": "Exploring Interactions between Rogue Autonomous Vehicles and People",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38147,
      "typeId": 11597,
      "title": "First Things First: A Survey Exploring the Key Services and Functions of a Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In order to find out the home robot’s services and functions that should be developed with priority, we created three scenarios for the robot services including cleaning, laundry, and cooking with detailed functions for each service. We investigated the effect of service types on service evaluation. Robot’s cleaning service was evaluated as the greatest positive compared to laundry and cooking service. Furthermore, we explored which robot attributes or task attributes affected robot service evaluation and purchase intention for each function. By service types, different attributes of the robot and the task affected service evaluation and purchase intention. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37574
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37856
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37226
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37550
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378317"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=faVdcVmok_o",
          "title": "First Things First: A Survey Exploring the Key Services and Functions of a Robot",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38148,
      "typeId": 11597,
      "title": "FRACTOS: Learning to be a Better Learner by Building Fractions",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Real-world learning interactions happening in classrooms, often involve children interacting simultaneously with teachers and peer learners. FRACTOS is a triadic learning interaction system which is structured to emphasize the learner on different phases of self- regulation such as planning, performance and reflection while engaging in a constructionist task of building fractions using virtual LEGO blocks with a virtual tutor and robot peer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne University",
              "dsl": "ISIR"
            }
          ],
          "personId": 37672
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS, ISIR",
              "dsl": "Sorbonne Universié"
            }
          ],
          "personId": 37993
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "Jacobs University",
              "dsl": "Psychology and Methods"
            }
          ],
          "personId": 37375
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378318"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=nqJq8eIW0nY",
          "title": "FRACTOS: Learning to be a Better Learner by Building Fractions",
          "duration": 116,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38149,
      "typeId": 11599,
      "title": "Would You Turn Off a Robot because It Confronts You with your Own Mortality?",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "A future world populated by robots is a projection that has long inspired and still inspires science-fiction stories. As their complexity increase people can help but compare to these artificial entities and why not question their own “human nature”. While it is easy to assess the superiority of humans on many dimensions, there is one, central to all humans that is not in favor of our kind: the mortality. In this study, we investigate how individuals react toward a robot that makes this comparison dimension stand out and how it may define attitudes towards robots and pro/antisocial behaviors towards them. Also, we evaluate the role of robot anthropomorphism and the spiritual thoughts activation as mediators of the above-mentioned process. Our study’s results demonstrate that facing a robot that confronts people with their own mortality results in less positive attitudes towards robots, and a higher likelihood of acting negatively toward them. Also, this particular robot tends to energize more spiritual thoughts among participants and less human traits attribution. Finally, we show that spirituality may be paradoxically associated with more anthropomorphism and more negative attitude showing the ambiguous multi-component stance of this dimension. These results are discussed in terms of attitudes towards robots, Terror Management Theory, social comparison process and implications for future human-robot interaction (HRI).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Clermont-Ferrand",
              "institution": "Clermont Auvergne University",
              "dsl": "LAPSCO CNRS UMR 6024"
            }
          ],
          "personId": 37301
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380736"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CxXrrxhF_Fc",
          "title": "Would You Turn Off a Robot because It Confronts You with your Own Mortality?",
          "duration": 560,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38150,
      "typeId": 11597,
      "title": "Perception of Emotional Gait-like Motion of Mobile Humanoid Robot Using Vertical Oscillation",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a subjective evaluation of the emotions of a wheeled mobile humanoid robot expressing emotions during movement by replicating human gait-induced upper body motion. For this purpose, we proposed the robot equipped with a vertical os- cillation mechanism that generates such motion by focusing on human center-of-mass trajectory. In the experiment, participants watched videos of the robot’s different emotional gait-induced upper body motions, and assess the type of emotion shown, and their confidence level in their answer. We calculated the emotion recognition rate and the average confidence level of their answers. As a result, we found that participants gave higher confidence levels in their assessment for the robot’s emotional movement with vertical oscillation than one without it.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37252
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37302
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37479
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37392
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": "Graduate school of engineering science"
            }
          ],
          "personId": 37202
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 37995
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378319"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Ewr48_-TXAA",
          "title": "Perception of Emotional Gait-like Motion of Mobile Humanoid Robot Using Vertical Oscillation",
          "duration": 121,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38151,
      "typeId": 11597,
      "title": "Evaluating the Usability of New Software for Medication Management on a Social Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "As the world’s population is rapidly ageing, social robots are being developed to help patients manage their chronic conditions at home. An important task for robots is to remind people of their medications. Although medication management systems aim to simplify the process of programming robots, these systems often suffer from usability issues that increase data entry errors. This study aimed to investigate whether cosmetic and validation changes in a social robot medication management system (Robogen) improved system usability and reduced medication errors. Forty participants underwent a 60-minute study during which they entered prescriptions using both the old and new systems in a random order. System usability and preference were assessed using questionnaires. Results showed that the new system (Robogen 2) had significantly higher usability scores and was preferred by the significant majority of participants (80%). This new system has been adopted for programming the robot in subsequent healthcare studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Psychological Medicine"
            }
          ],
          "personId": 37845
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 37629
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "School of Pharmacy"
            }
          ],
          "personId": 37439
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Psychological Medicine"
            }
          ],
          "personId": 38013
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Unversity of Auckland",
              "dsl": "Dep. of Electrical and Computer Eng."
            }
          ],
          "personId": 37220
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378320"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=3Pdvnv74tr0",
          "title": "Evaluating the Usability of New Software for Medication Management on a Social Robot",
          "duration": 115,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38152,
      "typeId": 11597,
      "title": "Vision-based Aided-Grasping in Teleoperation with Multiple Unknown Objects",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Grasping an object in the scene with multiple unknown objects requires both the knowledge of which object to be the target and the planning of grasping pose. However, teleoperating a robot hand to find a proper grasping pose in an unstructured environment is often too complicated and time-consuming to perform. In this study, we propose an aided-grasping algorithm which autonomously corrects the pose of a robot hand using an eye-in-hand camera. We used multiple cameras for a natural vision-based teleoperation interface with an aided-grasping algorithm. Experiments with objects placed in arbitrary positions and angles have shown a successful implementation of the algorithm. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Mechanical Engineering / BioRobotics"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Mechanical Engineering / BioRobotics"
            }
          ],
          "personId": 37515
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 37541
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 37861
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "DAEJEON",
              "institution": "KAIST",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "DAEJEON",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 38000
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "Department of Mechanical Engineering"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "Department of Mechanical Engineering"
            }
          ],
          "personId": 37581
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon ",
              "institution": "KAIST",
              "dsl": "Dept. of Mechanical eng."
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon ",
              "institution": "KAIST",
              "dsl": "Dept. of Mechanical eng."
            }
          ],
          "personId": 37229
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378321"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CegHc5AUWqo",
          "title": "Vision-based Aided-Grasping in Teleoperation with Multiple Unknown Objects",
          "duration": 121,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38153,
      "typeId": 11597,
      "title": "Experimental Investigation on the Influence of Prior Knowledge of a Decision-support Robot for Court Juries",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Previous studies in cognitive science have pointed out how the top-down processing of interpersonal cognition plays a role in human interactions, and the same mechanisms are observed in interactions with robots. This study investigates how mental models about the decision-supporting robots used in court will influence jury behavior. A laboratory experiment was conducted using a simple jury decision-making task, where participants play the role of a jury and make decisions regarding the length of the sentence for a particular crime. During the task, a robot with expert knowledge provides suggestions regarding the length of the sentence, based on other similar cases. In one scenario, participants receive a lecture about a case-based reasoning system and proceed to the experiment. Statistical analysis show that there were no significant differences between the conditions however, some participants engaging in the condition with prior knowledge performed with higher conformity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 37254
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 37473
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378238"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=v1GVzkJRS0k",
          "title": "Experimental Investigation on the Influence of Prior Knowledge of a Decision-support Robot for Court Juries",
          "duration": 133,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38154,
      "typeId": 11597,
      "title": "Programming a Robot or an Avatar: A Study on Learning Outcomes, Motivation, and Cooperation",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Robots have been found to be effective tools for programming instruction, although it is not yet clear why students learn more using robots as compared to receiving ‘traditional’ programming instruction. In this study, 121 nine- to twelve-year-old children received a programming training in pairs, in one of two conditions: using either a robot or a virtual avatar. The training was videotaped to study differences in children’s cooperation. Furthermore, children’s learning outcomes and motivation were assessed through questionnaires. Children were found to learn more from programming the robot than the avatar, although no differences in their cooperation during the training or self-reported motivation were found between the two conditions. Thus, future research is required to further understand how exactly robots lead to higher learning outcomes than ‘traditional’ tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 37982
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Almere",
              "institution": "Windesheim university of applied sciences",
              "dsl": ""
            }
          ],
          "personId": 37548
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 37368
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Almere",
              "institution": "Windesheim university of applied sciences",
              "dsl": ""
            }
          ],
          "personId": 37723
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378239"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38155,
      "typeId": 11597,
      "title": "Exploration of a Robot-based Adaptive Cognitive Stimulation System for the Elderly",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We explore the viability of developing a robot to assist cognitive stimulation activities for early-stage dementia users within a day-care centre. Working with healthcare professionals, we have designed interactive sessions where the robot Pepper plays stimulation games according to three levels of cognitive complexity. The initial design has been tested with an end-user to evaluate the system as well as to gain insight on the necessary interaction cues that the robot should perform to actively engage potential users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "La Salle-Ramon Llull University",
              "dsl": "Engineering"
            }
          ],
          "personId": 37367
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "La Salle-Ramon Llull University",
              "dsl": ""
            }
          ],
          "personId": 37700
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378322"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=IBhEqyrDqvA",
          "title": "Exploration of a Robot-based Adaptive Cognitive Stimulation System for the Elderly",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38156,
      "typeId": 11597,
      "title": "Using Customers' Online Reviews to Identify and Classify Human Robot Interaction Failures in Domestic Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Little information is available regarding which types of failures robots experience in domestic settings. To further the community's knowledge, we manually classified 3062 customer reviews of robotic vacuum cleaners on Amazon.com. The resulting database was analyzed and used to create a Natural Language Processing (NLP) model capable of predicting whether a review contains a description of a failure or not. The current work describes the initial analysis and model development process as well as preliminary findings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37250
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37984
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37264
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378323"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38157,
      "typeId": 11597,
      "title": "There's More than Meets the Eye: Enhancing Robot Control through Augmented Visual Cues  ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present the design of a visual feedback mechanism using Augmented Reality, which we call augmented visual cues, to assist pick-and-place tasks during robot control. We propose to augment the robot operator’s visual space in order to avoid attention splitting and increase situational awareness (SA). In particular, we aim to improve on the SA concepts of perception, comprehension, and projection as well as the overall task performance. For that, we built upon the interaction design paradigm proposed by Walker et al. On the one hand, our design augments the robot to support picking-tasks; and, on the other hand, we augment the environment to support placing-tasks. We evaluated our design in a first user study, and results point to specific design aspects that need improvement while showing promise for the overall approach, in particular regarding user satisfaction and certain SA concepts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 37380
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 37978
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 37454
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 37874
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378240"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5lHC4GXloN8",
          "title": "There's More than Meets the Eye: Enhancing Robot Control through Augmented Visual Cues  ",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38158,
      "typeId": 11597,
      "title": "How Early Task Success Affects Attitudes Toward Social Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "While social robots are designed to engage in socially interactive tasks, they may not always establish the intended social connection. We examined how people’s experiences of succeeding in completing these interactive tasks influence attitudes toward social robots. People developed more positive attitudes toward social robots when they completed more tasks successfully. These findings highlight potential constraints of complex interactive tasks increasingly implemented in commercially available social robots. A trade-off may exist between early task success and the sustained training of complex social robots by their human social partners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 37460
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": ""
            }
          ],
          "personId": 37543
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 37489
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 37806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 37341
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "U. S. Air Force Academy",
              "institution": "United States Air Force Academy",
              "dsl": "Warfighter Effectiveness Research Center"
            }
          ],
          "personId": 37823
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "AF Academy",
              "institution": "United States Air Force Academy",
              "dsl": "Department of Behavioral Sciences and Leadership"
            }
          ],
          "personId": 37509
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "United States Air Force Academy",
              "dsl": "Dept of Behavior Sciences and Leadership"
            }
          ],
          "personId": 37438
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378241"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=eR6TEvl0_f0",
          "title": "How Early Task Success Affects Attitudes Toward Social Robots",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38159,
      "typeId": 11597,
      "title": "Robot that Expresses Human Pains by Deformations",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "It is not always easy for humans to understand the pain of others.Therefore, having an effective method to express and explain their pain to others is valuable. In this study, we attempt to create a robot that is capable of expressing human pains by its deformations. The user of the robot is able to edit those deformations manually b his/her hand, and the robot is capable of recording the deformation processes for replays. In this late-breaking report, we describe the basic requirements of this robot and its early prototype.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Engineering Systems"
            }
          ],
          "personId": 37424
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 37949
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378324"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38160,
      "typeId": 11521,
      "title": "Defense Against the Dark Cars: Design Principles for Griefing of Autonomous Vehicles",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "As autonomous vehicles (AVs) become a reality on public roads, researchers and designers are beginning to see unexpected reactions from the public ranging from curiosity to vandalism. These behaviors are concerning, as AV platforms will need to know how to deal with people behaving unexpectedly or aggressively. We call this griefing of AVs, adopting the term from harassment in online gaming. We discuss several examples of griefing observed in onroad field studies using a Wizard-of-Oz driverless car. While Uber and Waymo have anecdotally mentioned vandalism towards AVs, we believe this to be the first public video available of AV griefing ranging from playful to aggressive. To stimulate discussion, we propose speculative design principles to address griefing",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center for Design Research"
            }
          ],
          "personId": 37774
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center For Design Research"
            }
          ],
          "personId": 37312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford Archaeology Center"
            }
          ],
          "personId": 37192
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 37827
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374796"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=X7Fkmf0XHiY",
          "title": "Defense Against the Dark Cars: Design Principles for Griefing of Autonomous Vehicles",
          "duration": 599,
          "type": "video"
        }
      },
      "sessionIds": [
        38346
      ],
      "eventIds": []
    },
    {
      "id": 38161,
      "typeId": 11599,
      "title": "Avatar Work: Telework for Disabled People Unable to Go Outside by Using Avatar Robots \"OriHime-D\" and Its Verification",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we propose a telework “avatar work” that enables people with disabilities to engage in physical works such as customer service in order to realize an inclusive society, where we can do anything if we have free mind, even though we are bedridden. In avatar work, disabled people can remotely engage in physical work by operating a proposed robot “OriHime-D” with a mouse or gaze input depending on their own disabilities. As a social implementation initiative of avatar work, we have opened a two-week limited avatar robot cafe and have evaluated remote employment by people with disabilities using OriHime-D. As the results by 10 people with disabilities, we have confirmed that the proposed avatar work leads to mental fulfillment for people with disparities, and can be designed with adaptable workload. In addition, we have confirmed that the work content of the experimental cafe is appropriate for people with a variety of disabilities seeking social participation. This study contributes to fulfillment all through life and lifetime working, and at the same time leads to a solution to the employment shortage problem.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Ory Laboratory",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Kanagawa Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37443
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Atsugi",
              "institution": "Kanagawa Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37830
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Minato Ward",
              "institution": "Ory Laboratory Inc.",
              "dsl": ""
            }
          ],
          "personId": 37771
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380737"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=OW43k-lLH0A",
          "title": "Avatar Work: Telework for Disabled People Unable to Go Outside by Using Avatar Robots \"OriHime-D\" and Its Verification",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38162,
      "typeId": 11523,
      "title": "The Role of Aesthetics in Robotics and the Rise of Polymorphic Robots",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The general topic of this inquiry looks at the methodology of design for new types of companions robots, in the context of a domestic setting. Personalization is essential, but most of Human Robot Interaction (HRI) research focus on adaptive behaviour for social interactions using commercially available devices. These robots represent finite projects, with very little room left for meaningful physical alterations. The goal of this research is to study the impact amongst users, of a robot offering by its conception, a high range of choice for personal customisation.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt",
              "dsl": "CDT Edinburgh Robotic lab"
            }
          ],
          "personId": 37572
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379452"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=VwOPf3J7z8U",
          "title": "The Role of Aesthetics in Robotics and the Rise of Polymorphic Robots",
          "duration": 303,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38163,
      "typeId": 11521,
      "title": "A Social Robot as Therapy Facilitator in Interventions to Deal with Dementia-related Behavioral Symptoms",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Several studies have been reported on the use of social robots for dementia care. These robots have been used for diverse tasks such as for companionship, as an exercise coach, and as daily life assistant. However, most of these studies have assessed impact on participants only at the time when the interaction takes place rather than their medium or long-term effects. In this work, we report on a nine-week study conducted in a nursing home in which a autonomous social robot, called Eva, acts as facilitator of a cognitive stimulation therapy (CST). During the study, eight persons with dementia interacted with the robot in a group session which included elements of music therapy, reminiscence, cognitive games, and relaxation. Using the Neuropsychiatric Inventory - Nursing Home version (NPI-NH), we analyzed the impact of the therapy guided by the robot. The results show a statistically significant decrease in the total score of NPI-NH. Also, three dementia-related symptoms: delusions, agitation/aggression, and euphoria/exaltation, show a statistically significant decrease after the intervention. In addition, a qualitative analysis on interviews conducted with caregivers shows that all participants exhibits positive short-term effects after the session and provides insights on why some changes in behavior prevailed beyond the therapy sessions. Our results provide evidence that a social robot could play a role in improving the quality of life of persons with dementia.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "Baja California",
              "city": "Ensenada",
              "institution": "CICESE",
              "dsl": ""
            }
          ],
          "personId": 37596
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Sonora",
              "institution": "ITSON",
              "dsl": ""
            }
          ],
          "personId": 38370
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "Baja Califronia",
              "city": "Ensenada",
              "institution": "CICESE",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 37363
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374840"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=p8tqRiMsqqc",
          "title": "A Social Robot as Therapy Facilitator in Interventions to Deal with Dementia-related Behavioral Symptoms",
          "duration": 607,
          "type": "video"
        }
      },
      "sessionIds": [
        38345
      ],
      "eventIds": []
    },
    {
      "id": 38164,
      "typeId": 11521,
      "title": "Not Some Random Agent: Multi-person interactions with a personalizing service robot",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Service robots often perform their main functions in public settings, interacting with more than one person at a time. How these robots should handle the affairs of individual users while also behaving appropriately when others are present is an open question. One option is to design for flexible agent embodiment: letting agents take control of different robots as people move between contexts. Through structured User Enactments, we explored how agents embodied within a single robot might interact with multiple people. Participants interacted with a robot embodied by a singular service agent, agents that re-embody in different robots and devices, and agents that co-embody within the same robot. Findings reveal key insights about the promise of re-embodiment and co-embodiment as design paradigms as well as what people value during interactions with service robots that use personalization.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 37513
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute "
            }
          ],
          "personId": 37746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Electrical Engineering Department"
            }
          ],
          "personId": 37444
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montréal",
              "institution": "McGill University",
              "dsl": "Faculty of Arts"
            }
          ],
          "personId": 37809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37751
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37492
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 37671
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "HCI Institute"
            }
          ],
          "personId": 37779
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374795"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Bw_xm2h5vvA",
          "title": "Not Some Random Agent: Multi-person Interaction with a Personalizing Service Robot",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38165,
      "typeId": 11521,
      "title": "Creativity Encounters between Children and Robots",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Creativity is an intrinsic human ability with multiple benefits across the lifespan. Despite its importance, societies not always are well equipped with contexts for creativity stimulation; as a consequence, a major decline in creative abilities occurs at the age of 7 years old. We investigated the effectiveness of using a robotic system named YOLO as an intervention tool to stimulate creativity in children. During the intervention, children used YOLO as a character for their stories and through the interaction with the robot, creative abilities were stimulated. Our study (n = 62) included 3 experimental conditions: i) YOLO displayed behaviors based on creativity techniques; ii) YOLO displayed behaviors based on creativity techniques plus social behaviors; iii) YOLO was turned off, not displaying any behaviors. We measured children’s creative abilities at pre- and\r\npost-testing and their creative process through behavior analysis. Results showed that the interaction with YOLO contributed to higher creativity levels in children, specifically contributing to the generation of more original ideas during story creation. This study shows the potential of using social robots as tools to empower intrinsic human abilities, such as the ability to be creative.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Universitário de Lisboa (ISCTE-IUL), CIS-IUL",
              "dsl": ""
            }
          ],
          "personId": 37962
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "ISCTE-IUL",
              "dsl": ""
            }
          ],
          "personId": 37894
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 37240
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "Instituto Superior Técnico"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "GAIPS"
            }
          ],
          "personId": 37534
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374817"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=GzXixz1LhA4",
          "title": "Creativity Encounters between Children and Robots",
          "duration": 513,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38166,
      "typeId": 11521,
      "title": "Death of a Robot: Social Media Reactions and Language Usage in Response to Robot Retirement",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "People take to social media to share their thoughts, joys, and sorrows. A recent popular trend has been to support and mourn people and pets that have died as well as other objects that have suffered catastrophic damage. As several popular robots have been discontinued, including the Opportunity Rover, Jibo, and Kuri, we are interested in how language used to mourn these robots compares to that to mourn people, animals, and other objects. We performed a study in which we asked participants to categorize deidentifed Twitter reactions as referencing the death of a person, an animal, a robot, or another object. Most reactions were labeled as being about humans, which suggests that people use similar language to describe feelings for animate and inanimate entities. We used a natural language toolkit to analyze language from a larger set of tweets. A majority of tweets about Opportunity included second-person (“you”) and gendered third-person pronouns (she/he versus it), but terms like “R.I.P” were reserved almost exclusively for humans and animals. Our findings suggest that people verbally mourn robots similarly to living things, but reserve some language for people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37751
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 37513
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 37536
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 37488
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37492
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374794"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=hNtboHz0MaU",
          "title": "Death of a Robot: Social Media Reactions and Language Usage when a Robot Stops Operating",
          "duration": 563,
          "type": "video"
        }
      },
      "sessionIds": [
        38356
      ],
      "eventIds": []
    },
    {
      "id": 38167,
      "typeId": 11597,
      "title": "The Sound Settler: Spontaneous HRI in an Art Setting",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The Sound Settler is a utopian project which combines engineering and the arts. The utopian vision is to deploy an interactive music system on Mars before the first humans arrive. Two real robotic arms are controlled in a gravity compensated master-slave mode resembling the human part on Earth, and the robotic art piece which is envisioned to be located on Mars. The movements generate electronic music by charging physical capacitors and on screen a simulated third arm is deployed, visualizing the movements on the red planet. This report presents the artistic design, the implemented infrastructure and first subjective insights about human interactions with the art piece in public spaces. It motivates a more formal analysis and opens several research avenues for the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Polytechnique Montreal",
              "dsl": "MIST lab, Department of Computer and Software engineering"
            }
          ],
          "personId": 37657
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "Perception Engineering"
            }
          ],
          "personId": 37716
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Concordia University",
              "dsl": "Fine Arts - Intermedia"
            }
          ],
          "personId": 37655
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Concorida University",
              "dsl": ""
            }
          ],
          "personId": 37979
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Polytechnique Montreal",
              "dsl": ""
            }
          ],
          "personId": 37968
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montréal",
              "institution": "École de Technologie Supérieure",
              "dsl": "INIT Robots Laboratory, Department of Mechanical Engineering"
            }
          ],
          "personId": 37278
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378310"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rqF_5s8rR6k",
          "title": "The Sound Settler: Spontaneous HRI in an Art Setting",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38168,
      "typeId": 11597,
      "title": "Orthographic Vision-based Interface with Motion-tracking System for Robot Arm Teleoperation: A Comparative Study",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Robot teleoperation is crucial for many hazardous situations such as handling radioactive materials, undersea exploration and firefighting. Visual feedback is essential to increase the operator's situation awareness and thus accurately teleoperate a robot. In addition, the control interface is equally important as the visual feedback for effective teleoperation. In this paper, we propose a simple and cost-effective orthographic visual interface for the teleoperation system by visualizing the remote environment to provide depth information using only a single inexpensive webcam. Further, we provide a simple modification to the control interface (Leap Motion) to achieve a wider workspace and make it more convenient for the user. To realize the merits of the proposed system, a comparison between the modified Leap Motion interface and traditional control modalities (i.e., joystick and keyboard) is conducted using both the proposed orthographic vision system and a traditional binocular vision system. We conduct a user study (N = 10) to evaluate the effectiveness of this approach to teleoperate a 6-DoF arm robot to carry out a pick and place task. The results show that the combination of Leap Motion with the orthographic visual system outperforms all other combinations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Mechanical Engineering"
            }
          ],
          "personId": 37893
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Mechanical Engineering"
            }
          ],
          "personId": 37203
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Mechanical Engineering"
            }
          ],
          "personId": 37824
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378311"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38169,
      "typeId": 11597,
      "title": "Applying the Participatory Design Workshop Method to Explore how Socially Assistive Robots Could Assist Stroke Survivors",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The field of human robot interaction (HRI) is still relatively new and often borrows methods and principles from the more established field of HCI. HRI researchers are adopting HCI methods, however, these methodologies may need slight modifications and adaptations in order to better investigate the unique challenges of working in HRI. In this paper, we present our findings which utilised one such method: Participatory Design (PD) workshop. We held the workshop in our assistive living lab with ten stroke survivors. This workshop aimed to explore the design of new socially assistive robotic technologies that could be used to support stroke survivors in the home environment. Some of our findings were unanticipated, which suggested that some adaptations to the existing framework for PD workshops need to be revised in order to be useful to HRI research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37605
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37730
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            }
          ],
          "personId": 37908
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Mathematical and Computer Science"
            }
          ],
          "personId": 37270
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378232"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38170,
      "typeId": 11597,
      "title": "The Uncanny Valley Manifests Even with Exposure to Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "To investigate the reproducibility of the uncanny valley and its robustness to exposure to robotic technologies, we carried out a direct reproduction of Strait et al. (2015, 2017) and conceptual repro- duction of Złotowski et al. (2017). Consistent with prior findings, we observed an uncanny valley in participants’ responses to agents ranging in appearance from mechanomorphic to highly humanlike to human. Specifically, participants exhibited particular aversion to and avoidance of highly humanlike robots relative to their less humanlike counterparts and humans. Preexposure to the NAO ro- bot (N = 52) was not found to significantly affect participants’ responding relative to that of participants without any exposure to robotic technologies (N = 34). The findings thus suggest that the uncanny valley reliably manifests, both with and without exposure.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "University of Texas Rio Grande Valley",
              "dsl": ""
            }
          ],
          "personId": 38010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "The University of Texas Rio Grande Valley",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 37718
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "University of Texas Rio Grande Valley",
              "dsl": ""
            }
          ],
          "personId": 37642
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "The University of Texas Rio Grande Valley",
              "dsl": "Computer Science "
            }
          ],
          "personId": 38365
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Edinburg",
              "institution": "The University of Texas Rio Grande Valley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37493
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378312"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38171,
      "typeId": 11597,
      "title": "Social Robots as Reinforcement in Applied Behavior Analysis",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Within the framework of Applied Behavior Analysis (ABA), a study at the Childen’s Hospital of Eastern Ontario determines whether children on the autism spectrum are motivated by social robots as positive reinforcement during skill acquisition compared to traditional toys, candy, escapement, and affection. Using five robots and nine subjects, the study suggests that social robots are viable candidates as reinforcement in real-world ABA practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Children’s Hospital of Eastern Ontario",
              "dsl": "Autism Program"
            }
          ],
          "personId": 37935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Manchester",
              "institution": "Southern New Hampshire University",
              "dsl": "Computer Information Systems"
            }
          ],
          "personId": 37937
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378233"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38172,
      "typeId": 11597,
      "title": "A Comparison of NAO and Jibo in Child-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "A study at the Canada Science and Technology Museum with 121 child/parent pairs compared attitudes toward NAO and Jibo in child-robot interaction (CRI) scenarios. The study suggests that subjects (i) favor the robots roughly equally but for different reasons, (ii) are motivated more by autonomous web smartness than domain-specific knowledge, and (iii) prefer human-like gesturing over screen animation for the expression of emotion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "Canada Science and Technology Museum",
              "dsl": "Education Office"
            }
          ],
          "personId": 37309
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Manchester",
              "institution": "Southern New Hampshire University",
              "dsl": "Computer Information Systems"
            }
          ],
          "personId": 37937
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "University of Ottawa",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 37714
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Ottawa",
              "institution": "University of Ottawa",
              "dsl": "Telfer School of Management"
            }
          ],
          "personId": 37999
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378234"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38173,
      "typeId": 11597,
      "title": "DroEye: Introducing a Social Eye Prototype for Drones",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "A drone agent can benefit from exhibiting social cues, as introducing behavioral cues in robotic agents can enhance interaction trust and comfort with users. In this work, we introduce the development and setup of a responsive eye prototype (DroEye) mounted on a drone to demonstrate prominent social cues in Human-Drone Interaction. We describe possible attributes associated with the DroEye prototype and our future research directions to enhance the overall experience with social drones in our environment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "Art and Design"
            }
          ],
          "personId": 37782
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "Western Sydney University",
              "dsl": "School of Computing, Engineering and Mathematics"
            }
          ],
          "personId": 37236
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "Art and Design"
            }
          ],
          "personId": 37222
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Select Region",
              "city": "Istanbul",
              "institution": "Koç University",
              "dsl": "KUAR, Research Center for Creative Industries"
            }
          ],
          "personId": 37324
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 37957
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 37223
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378313"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=OWNG9MM0sNg",
          "title": "DroEye: Introducing a Social Eye Prototype for Drones",
          "duration": 63,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38174,
      "typeId": 11599,
      "title": "Dog Sit! Domestic Dogs (Canis familiaris) Follow a Robot's Sit Commands",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "As personal social robots become more prevalent, the need for the designs of these systems to explicitly consider pets become more apparent. However, it is not known whether dogs would interact with a social robot. In two experiments, we investigate whether dogs respond to a social robot after the robot called their names, and whether dogs follow the 'sit' commands given by the robot. We conducted a between-subjects study (n = 34) to compare dogs' reactions to a social robot with a loudspeaker. Results indicate that dogs gazed at the robot more often after the robot called their names than after the loudspeaker called their names. Dogs followed the 'sit' commands more often given by the robot than given by the loudspeaker. The contribution of this study is that it is the first study to provide preliminary evidence that 1) dogs showed positive behaviors to social robots and that 2) social robots could influence dog's behaviors. This study enhance the understanding of the nature of the social interactions between humans and social robots from the evolutionary approach. Possible explanations for the observed behavior might point toward dogs perceiving robots as agents, the embodiment of the robot creating pressure for socialized responses, or the multimodal (i.e., verbal and visual) cues provided by the robot being more attractive than our control condition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Social Robotics Lab"
            }
          ],
          "personId": 37832
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Canine Cognition Center at Yale"
            }
          ],
          "personId": 37765
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Canine Cognition Center at Yale"
            }
          ],
          "personId": 37661
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37499
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37670
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380734"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cMCfQE0G6QQ",
          "title": "Dog Sit! Domestic Dogs (Canis familiaris) Follow a Robot's Sit Commands",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38175,
      "typeId": 11597,
      "title": "Using Expectancy Violations Theory to Understand Robot Touch Interpretation",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "As robots are increasingly placed in direct interaction and often times in physical contact with people, understanding how touch by a robot influences interactions has become an important topic in HRI. Although prior research in HRI has shown robotic touch to elicit both positive and negative reactions, it remains an open question when and why touch is perceived as positive or negative. Here we apply expectancy violations theory (EVT) to shed light onto this question. We present an online study with N=142 participants that investigates the impact of context (touch after error vs. touch after no error) and robot appearance (social/animated face vs. non-social/blank screen) in shaping the perception of a robot’s touch. We found that robot-initiated touch from a non-social robot was rated more positively after an error compared to ratings of the non-social robot touch after no error. Open-ended responses showed that people attach a wide array of meanings to the robot’s touch, highlighting the importance of additional cues that are needed to ensure that people understand the intention of the touch.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 37811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 37430
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 37176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37663
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378314"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=m7kZ5JF-u3Q",
          "title": "Using Expectancy Violations Theory to Understand Robot Touch Interpretation",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38176,
      "typeId": 11521,
      "title": "Effects of Anthropomorphism and Accountability on Trust in Human Robot Interaction",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper examines how people’s trust and dependence on robot teammates providing decision support varies as a function of different attributes of the robot, such as perceived anthropomorphism, type of support provided by the robot, and its physical presence. We conduct a mixed-design user study with multiple robots to investigate trust, inappropriate reliance, and compliance measures in the context of a time-constrained game. We also examine how the effect of human accountability addresses errors due to over-compliance in the context of human robot interaction (HRI). This study is novel as it involves examining multiple attributes at once, thus enabling us to perform multi-way comparisons between different attributes on trust and compliance with the agent. Results from the 4x4x2x2 study show that behavior and anthropomorphism of the agent are the most significant factors in predicting the trust and compliance with the robot. Furthermore, adding a coalition-building preface, where the agent provides context to why it might make errors while giving advice, leads to an increase in trust for specific behaviors of the agent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "CORE Robotics"
            }
          ],
          "personId": 37733
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 37504
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374839"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JW5RRBWJhF0",
          "title": "Effects of Anthropomorphism and Accountability on Trust in Human Robot Interaction",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38177,
      "typeId": 11597,
      "title": "Robots as Furniture, Integrating Human-Computer Interfaces into the Built Environment",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "What will the universal remote control of the near future look like? What form will the next generation of human-computer interfaces take? Will they be conspicuous interfaces within the built envi- ronment, like a computer screen or a smart speaker? Will they resemble the ubiquitous, portable rectangles that we all carry in our pockets? We propose a third paradigm: interfaces that hide in plain sight, inconspicuously integrated into the furniture always al- ready around us, but ready to be called upon when needed in order to establish a user interface. Our furniture-robot prototype - tbo, the TableBot - demonstrates the viability of this furniture-based human-computer paradigm.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "School of Engineering/Computer Science"
            }
          ],
          "personId": 37238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "School of Engineering"
            },
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37758
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378235"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38178,
      "typeId": 11597,
      "title": "Toy, Tutor, Peer, or Pet?: Preliminary Findings from Child-Robot Interactions in a Community School",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Research focused upon Child-Robot Interaction shows that robots in the classroom can support diverse learning goals amongst pre-school children. However, studies with children and robots in the Global South are currently limited. To address this gap, we conducted a study with children aged 4-8 years at a community school in New Delhi, India, to understand their interaction and experiences with a social robot. The children were asked to teach the English alphabet to a Cozmo robot using flash cards. Preliminary findings suggest that the children orient to the robot in a variety of ways including as a toy or pet. These orientations need to be explored further within the context of the Global South.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Delhi",
              "city": "New Delhi",
              "institution": "Indraprastha Institute of Information Technology Delhi",
              "dsl": ""
            }
          ],
          "personId": 37394
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": "Faculty of Information Technology and Communication Sciences"
            }
          ],
          "personId": 37828
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Delhi",
              "city": "New Delhi",
              "institution": "Indraprastha Institute of Information Technology Delhi",
              "dsl": ""
            }
          ],
          "personId": 37687
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "New Delhi",
              "institution": "Indraprastha Institute of Information Technology, Delhi (IIITD)",
              "dsl": "Department of Human-Centred Design"
            }
          ],
          "personId": 37880
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378315"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=hMyGznAFYMk",
          "title": "Toy, Tutor, Peer, or Pet?: Preliminary Findings from Child-Robot Interactions in a Community School",
          "duration": 113,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38179,
      "typeId": 11597,
      "title": "Petbe: Projecting a Real Being onto a Social Robot Using Contextual Data for a Pet Monitoring Method",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The demand for pet monitoring devices is growing due to the increasing number of one-person households raising pets. However, current monitoring methods using video camera entail various problems, which may lead to discontinued usage. To overcome this problem, we propose Petbe, a social robot that projects your own pet using a context-aware approach based on BLE beacons and Raspberry Pis. The corresponding smartphone application provides various robot status updates (robot head) and movements (robot body). With the development of Petbe, we conducted an exploratory study to verify the advancement of the above issues on monitoring user's own pets with the following factors: privacy concern, companionship, awareness, connectivity, and satisfaction. The outcomes indicate that Petbe helps to reduce privacy concerns and build companionship through empathetic interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Companoid Labs"
            }
          ],
          "personId": 37786
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Companoid Labs"
            }
          ],
          "personId": 37612
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Yonsei University",
              "dsl": "Companoid Labs"
            }
          ],
          "personId": 37798
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378236"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JqAZyF3pfpA",
          "title": "Petbe: Projecting a Real Being onto a Social Robot Using Contextual Data for a Pet Monitoring Method",
          "duration": 118,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38180,
      "typeId": 11597,
      "title": "Towards Enhancement of Unmanned Aerial Vehicle (UAV) Operators' Situation Awareness: How Often Should their Command and Control Map be Updated",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Operators of military Unmanned Aerial Vehicles (UAVs) work in dynamic environments, where they must use shared command and control (C2) maps to orient, plan and perform their work. The map is overloaded with information that is irrelevant to their immediate operational mission. This clutter may harm their situation awareness (SA) and increase workload. An intelligent and dynamic filter algorithm has been developed to reduce the clutter by filtering information items on the map based on the environmental context. Implementing it raises questions regarding the update rate of the map filter. Two update rates were tested and their effect on UAV operators’ workload and SA was examined empirically. Operators benefited from higher update rates in terms of SA and workload. This is an important step towards the development of the algorithm, which conceptualizes how intelligent algorithms can be used to improve human operators’ interaction with autonomous systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beersheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": ""
            }
          ],
          "personId": 37197
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beersheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": ""
            }
          ],
          "personId": 37990
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Industrial Engineering and Management"
            }
          ],
          "personId": 37264
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378237"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38181,
      "typeId": 11599,
      "title": "Robonomics: The Study of Robot-Human Peer-to-Peer Financial Transactions and Agreements",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "Can or should a robot ever engage in a financial transaction with a human? If so, how? How about an enforceable agreement? Blockchain technology has enabled the development of cryptocurrencies, smart contracts, and unlocked a plethora of other disruptive technologies. But, beyond its use case in cryptocurrencies, and in network coordination, blockchain technology may have serious sociotechnical implications in the future co-existence of robots and humans. Motivated by the recent explosion of interest around blockchains, and our extensive work on open-source blockchain technology and its integration into robotics - this paper addresses these questions and provides insights into how blockchains and other decentralized technologies can impact our interactions with robots and enable the social integration of robots into human society.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "Advanced Telerobotics Research Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "DigitalMinds",
              "dsl": ""
            }
          ],
          "personId": 37620
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Kent",
              "institution": "Kent State University",
              "dsl": "Advanced Telerobotics Research Lab"
            }
          ],
          "personId": 37555
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380735"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38182,
      "typeId": 11523,
      "title": "The Social Pendulum: An Oscillating Robot with Moods and Swings",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "The social pendulum is an oscillating robot, which exhibits certain personality traits based on its proximity to ‘known’ people through audile cues and changing patterns of its oscillations. The onboard camera(s) recognize faces of people in its surroundings and each new face is registered and a corresponding ‘behavior metric’ is prepared. Parameters such as time spent in its proximity, kind of words spoken etc. affects the individual’s behavior metric which determines the pendulums reaction. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": ""
            }
          ],
          "personId": 37814
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "West Midnapore",
              "institution": "IIT Kharagpur",
              "dsl": ""
            }
          ],
          "personId": 37212
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379455"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38183,
      "typeId": 11523,
      "title": "Flumzis: A DIY robot that improves student performance during study nights",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "Flumzis is a DIY social robot that optimizes the learning process of college students who spend the entirely night studying. It helps them by measuring their study time, monitoring break times, giving advices about how to eat healthy and tips to make the study night as optimized as possible. Flumzis also has a smart accessory, a intelligent base which allows to have a bunch of extra functions. Those work together in order to have better understanding of the functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontificia Universidad Católica del Perú",
              "dsl": "Sala VEO 3D"
            }
          ],
          "personId": 37695
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontificia Universidad Católica del Perú ",
              "dsl": "Sala VEO 3D"
            }
          ],
          "personId": 37174
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontificia Universidad Católica del Perú",
              "dsl": "Sala VEO 3D"
            }
          ],
          "personId": 37401
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontificia Universidad Católica del Perú",
              "dsl": "Sala VEO 3D"
            }
          ],
          "personId": 37570
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3379454"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=me2iYK25SyI",
          "title": "Flumzis: A DIY robot that improves student performance during study nights",
          "duration": 253,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    },
    {
      "id": 38185,
      "typeId": 11521,
      "title": "Varied Human-Like Gestures for Social Robots: Investigating the Effects on Children's Engagement and Language Learning",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "To investigate whether a humanoid robot’s use of gestures improves children’s learning of second language vocabulary, and if variation in gestures strengthens this effect, we conducted a field study where a total of 94 children (aged 4–6 years old) played a language learning game with a NAO robot. The robot either used no gestures at all, repeated the same gesture every time a target word was presented, or produced a different gesture for each occurrence of a target word. We found that, contrary to what the majority of existing research suggests, the robot’s use of gestures did not result in increased learning outcomes, compared to a robot that did not use gestures. However, engagement between child and robot was higher in both the repeated and varied gesture conditions, compared to the condition without gestures. An exploratory analysis showed that age played a role: the older children in the study learned more than the younger children when the robot used gestures. It is therefore important to carefully consider the design and application of robot gestures to support the learning process. The contribution of this work is twofold: it is a conceptual reproduction of a previous study, and we have taken first steps towards exploring the role of variation in gestures. The study was preregistered, and all materials are made publicly available.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Tilburg center for Cognition and Communication (TiCC)"
            }
          ],
          "personId": 37831
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Noord-Brabant",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Tilburg center for Cognition and Communication"
            }
          ],
          "personId": 37558
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Tilburg center for Cognition and Communication (TiCC)"
            }
          ],
          "personId": 37497
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37878
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374815"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=KPRw_ImV5eU",
          "title": "Varied Human-Like Gestures for Social Robots: Investigating the Effects on Children's Engagement and Language Learning",
          "duration": 597,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38186,
      "typeId": 11521,
      "title": "Taxonomy of Trust-Relevant Failures and Mitigation Strategies",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We develop a taxonomy that categorizes HRI failure types and their impact on trust to structure the broad range of knowledge contributions. We further identify research gaps in order to support fellow researchers in the development of trustworthy robots. Studying trust repair in HRI has only recently been given more interest and we propose a taxonomy of potential trust violations and suitable repair strategies to support researchers during the development of interaction scenarios. The taxonomy distinguishes four failure types: Design, System, Expectation, and User failures and outlines potential mitigation strategies. Based on these failures, strategies for autonomous failure detection and repair are presented, employing explanation, verification and validation techniques. Finally, a research agenda for HRI is outlined, discussing identified gaps related to the relation of failures and HR-trust.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Department of Informatics",
              "dsl": "Dynamic and Distributed Information Systems Group"
            }
          ],
          "personId": 37245
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 37330
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 38003
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Freiburg im Breisgau",
              "institution": "University of Freiburg",
              "dsl": ""
            }
          ],
          "personId": 37318
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Delaware",
              "city": "Newark",
              "institution": "Department of Philosophy",
              "dsl": "Center for Science, Ethics, and Public Policy"
            }
          ],
          "personId": 37183
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Liverpool",
              "institution": "University of Liverpool",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37484
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Interactive Intelligence"
            }
          ],
          "personId": 37817
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374793"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=9mg0lOy2XAI",
          "title": "Taxonomy of Trust-Relevant Failures and Mitigation Strategies",
          "duration": 586,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38187,
      "typeId": 11521,
      "title": "\"Are You Sad, Cozmo?\" How Humans Make Sense of a Home Robot’s Emotion Displays",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper explores how humans interpret displays of emotion produced by a social robot in real world situated interaction. Taking a multimodal conversation analytic approach, we analyze video data of families interacting with a Cozmo robot in their homes. Focusing on one happy and one sad robot animation, we study, on a turn-by-turn basis, how participants respond to audible and visible robot behavior designed to display emotion. We show how emotion animations are consequential for interactional progressivity: While displays of happiness typically move the interaction forward, displays of sadness regularly lead to a reconsideration of previous actions by humans. Furthermore, in making sense of the robot animations people may move beyond the designer’s reported intentions, actually broadening the opportunities for their subsequent engagement. We discuss how sadness functions as an interactional \"rewind button\" and how the inherent vagueness of emotion displays can be deployed in design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department for Culture and Communication"
            }
          ],
          "personId": 37964
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department for Culture and Communication"
            }
          ],
          "personId": 37404
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department for Culture and Communication"
            }
          ],
          "personId": 37857
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374814"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Hl-ZEZ2s9Nc",
          "title": "\"Are You Sad, Cozmo?\" How Humans Make Sense of a Home Robot’s Emotion Displays",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38355
      ],
      "eventIds": []
    },
    {
      "id": 38188,
      "typeId": 11521,
      "title": "Teachers’ Disappointment: Theoretical Perspective on the Inclusion of Ambivalent Emotions in Human-Robot Interactions in Education",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": " Following affective turn in cognitive science, recent decades have witnessed an increasing interest toward the role of emotions in education. Ample evidence suggests that learners and teachers experience a variety of emotions, ranging from joy and pride to anger and frustration. However, when it comes to the design of affective behavior in robotic systems for education purposes, the emphasis has been predominantly on communication of positive emotions. While we recognize that positive emotions are fundamental to successful learning, in this paper we wish to make the case for the consideration of ambivalent emotions for the design of social robots for tutoring. To ground this proposal, we focus on the emotion of teachers’ disappointment. First, we discuss under which conditions communicated teachers’ disappointment, while it may be experienced as emotionally ambivalent by teachers and students, functions as an affiliating pedagogical strategy. We proceed to sketch out the methodological suggestions we consider relevant for future studies of communicated disappointment in human-robot interactions within learning contexts. We conclude with critical reflections about the ethics of responsible designs of such studies",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "The Austrian Research Institute for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 37399
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Visual Computing and Human-Centred Technology"
            }
          ],
          "personId": 37796
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374816"
        }
      },
      "sessionIds": [
        38355
      ],
      "eventIds": []
    },
    {
      "id": 38189,
      "typeId": 11599,
      "title": "Adapting to a Robot: Adapting Gardening and the Garden to fit a Robot Lawn Mower",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "Introducing robots into the home changes the work that the homeowners carry out. This paper explores in depth how garden work and the garden changes when a robotic lawn mower is introduced. The methodology in this study is autoethnography, which gives access to personal experiences and thoughts. The paper describes how the usual work of manually mowing the lawn is automated and new tasks emerge as the gardeners adapt to the robot mower. A conceptual framework is presented and used to analyse these changes. Some gardening tasks become redundant and new tasks appear, and a new urgency is added to some of the old tasks. In addition, awareness about the robot mower’s movements is important to keep it active and avoid damage to the robot and things in the garden. The paper suggests that unwanted changes that become too demanding are important for user acceptance or rejection of robots in the home environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 37803
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380738"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38190,
      "typeId": 11521,
      "title": "Effects of Onset Latency and Robot Speed Delays on Mimicry-Control Teleoperation",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we study the effects of delays in a mimicry-control robot teleoperation interface which involves a user moving their arms to directly show the robot how to move and the robot follows in real time. Unlike prior work considering delays in other teleoperation systems, we consider delays due to robot slowness in addition to latency in the onset of movement commands. We present a human-subjects study that shows how different amounts and types of delays have different effects on task performance. We compare the movements under different delays to reveal the strategies that operators use to adapt to delay conditions and to explain performance differences. Our results show that users can quickly develop strategies to adapt to slowness delays but not onset latency delays. We discuss the implications of our results for the future development of methods designed to mitigate the effects of delays.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 37604
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 37808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 37325
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374838"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=M5O0UkPQsxE",
          "title": "Effects of Onset Latency and Robot Speed Delays on Mimicry-Control Teleoperation",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        38354
      ],
      "eventIds": []
    },
    {
      "id": 38191,
      "typeId": 11599,
      "title": "Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "As robots become more prevalent, the importance of the field of human-robot interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, most HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of four years of the International Conference on Human-Robot Interaction (2016 through 2019) and report on incorrect statistical practices and design of Likert scales. During these years, only 3 of the 110 papers applied proper statistical testing to correctly-designed Likert scales. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Lastly, we provide recommendations to improve the accuracy of conclusions drawn from Likert data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Institute for Robotics and Intelligent Machines"
            }
          ],
          "personId": 37169
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Institute for Robotics and Intelligent Machines"
            }
          ],
          "personId": 37170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 37745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 37504
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380739"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=mqhNxc0mXII",
          "title": "Four Years in Review: Statistical Practices of Likert Scales in Human-Robot Interaction Studies",
          "duration": 472,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38193,
      "typeId": 11596,
      "title": "CoWriting Kazakh: Learning a New Script with a Robot - Demonstration",
      "award": "BEST_PAPER",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "This interdisciplinary project aims to assess and manage the risks relating to the transition of Kazakh language from Cyrillic to Latin in Kazakhstan in order to address challenges of a) teaching and motivating children to learn a new script and its associated handwriting, and b) training and providing support for all demographic groups, in particular senior generation. We present the system demonstration that proposes to assist and motivate children to learn a new script with the help of a humanoid robot and a tablet with stylus.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37710
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37295
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37595
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37815
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37736
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378211"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=9hfhCk_pdCc",
          "title": "The CoWriting Kazakh Project: Demonstration",
          "duration": 272,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38194,
      "typeId": 11599,
      "title": "The Confucian Matador: Three Defenses Against the Mechanical Bull",
      "trackId": 10945,
      "tags": [],
      "keywords": [],
      "abstract": "It is critical for designers of language-capable robots to enable some degree of moral competence in those robots. This is especially critical at this point in history due to the current research climate, in which much natural language generation research focuses on language modeling techniques whose general approach may be categorized as “fabrication by imitation” (the titular mechanical “bull”), which is especially unsuitable in robotic contexts. Furthermore, it is critical for robot designers seeking to enable moral competence to consider previously under-explored moral frameworks that place greater emphasis than traditional Western frameworks on care, equality, and social justice, as the current sociopolitical climate has seen a rise of movements such as libertarian capitalism that have undermined those societal goals. In this paper we examine one alternate framework for the design of morally competent robots, Confucian ethics, and explore how designers may use this framework to enable morally sensitive human-robot communication through three distinct perspectives: (1) How should a robot reason? (2) What should a robot say? and (3) How should a robot act?",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 37233
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "Humanities, Arts & Social Sciences"
            }
          ],
          "personId": 37327
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 37421
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "US Air Force Academy",
              "dsl": "Department of Behavioral Sciences and Leadership"
            }
          ],
          "personId": 38363
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3380740"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=tmWgRTjvrlk",
          "title": "The Confucian Matador: Three Defenses Against the Mechanical Bull",
          "duration": 601,
          "type": "video"
        }
      },
      "sessionIds": [
        38359
      ],
      "eventIds": []
    },
    {
      "id": 38196,
      "typeId": 11596,
      "title": "PlantBot: A social robot prototype to help with behavioral activation in young people with minor depression",
      "award": "BEST_PAPER",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "The PlantBot is a home device that shows iconographic or simple lights to depict actions that it requests a young person (its user) to do as part of Behavioral Activation therapy. In this initial prototype, a separate conversational speech agent (i.e., Amazon Alexa) is wizarded to act as a second system the user can interact with.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Kista, Stockholm",
              "institution": "The Royal Institute of Technology - KTH",
              "dsl": "Mobile Service lab"
            }
          ],
          "personId": 37213
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Hengelo",
              "institution": "XABLU",
              "dsl": ""
            }
          ],
          "personId": 37934
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Denekamp",
              "institution": "LedLoket",
              "dsl": ""
            }
          ],
          "personId": 37624
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 37825
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 37822
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37282
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378210"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=LOAhsTLNalU",
          "title": "PlantBot: A social robot prototype to help with behavioral activation in young people with minor depression",
          "duration": 240,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38197,
      "typeId": 11596,
      "title": "Voice Puppetry: Towards conversational HRI WoZ experiments with synthesised voices",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "In order to research conversational factors in robot design the use of Wizard of Oz (WoZ) experiments, where an experimenter plays the part of the robot, are common. However, for conversational systems using a synthetic voice, it is extremely difficult for the experimenter to choose open domain content and enter it quickly enough to retain conversational flow. In this demonstration we show how voice puppetry can be used to control a neural TTS system in almost real time. The demo hopes to explore the limitations and possibilities of such a system for controlling a robot’s synthetic voice in conversational interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "-Select-",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37579
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "CereProc Ltd.",
              "dsl": ""
            }
          ],
          "personId": 37713
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378209"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38199,
      "typeId": 11596,
      "title": "Demonstration of A Social Robot for Control of Remote Autonomous Systems",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "There are many challenges when it comes to deploying robots remotely including lack of situation awareness for the operator, which can lead to decreased trust and lack of adoption. For this demonstration, delegates interact with a social robot who acts as a facilitator and mediator between them and the remote robots running a mission in a realistic simulator. We will demonstrate how such a robot can use spoken interaction and social cues to facilitate teaming between itself, the operator and the remote robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt University",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 37686
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37300
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "CS Dept, School of MACS"
            }
          ],
          "personId": 37941
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 37436
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378207"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=vETotX5jyOk",
          "title": "Demonstration of A Social Robot for Control of Remote Autonomous Systems",
          "duration": 292,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38200,
      "typeId": 11596,
      "title": "Teleport - Variable Autonomy across Platforms",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "-blank-",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "University Of Sheffield",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37975
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "University of Sheffield",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37471
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378208"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38202,
      "typeId": 11596,
      "title": "A Gesture Control System for Drones used with Special Operations Forces",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "Special Operations Forces (SOF) are facing extreme risks when prosecuting crimes in uncharted environments like buildings. Autonomous drones could potentially save officers’ lives by assisting in those exploration tasks, but an intuitive and reliable way of communicating with autonomous systems is yet to be established. This paper proposes a set of gestures that are designed to be used by SOF during operation for interaction with autonomous systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": "DAI-Labor"
            }
          ],
          "personId": 37180
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": "DAI-Labor"
            }
          ],
          "personId": 37983
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": "DAI-Labor"
            }
          ],
          "personId": 37503
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technical University of Berlin",
              "dsl": "DAI-Labor"
            }
          ],
          "personId": 37507
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378206"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=2QeCerCPIS4",
          "title": "A Gesture Control System for Drones used with Special Operations Forces",
          "duration": 61,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38204,
      "typeId": 11521,
      "title": "Live Dance Performance Investigating the Feminine Cyborg Metaphor with a Motion-activated Wearable Robot",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents artistic work that comments on the exploitation of feminine gender performance in technology, from the point of view of a particular artist, enabled by a wearable robotic device that creates an onstage cyborg character. The piece, entitled “Babyface” was created in residence in a robotics lab and has catalyzed the development of a motion-activated wearable robot. This paper reports on the creation of this piece, its accompanying device, along with initial responses that have been shared informally. Iterations of multiple flexible attachment structures, a clear plastic dress that references the hyperbolic representation of the female sex, as well as the electronic subsystems are presented alongside discussion of the somatic and choreographic investigations that accompanied prototype development. These fabricated elements alongside the actions of the performer and a soundscape that quotes statements made by real “female” robots create an otherwordly, sad cyborg character that causes viewers to question their assumptions about and pressures on the female ideal. This work is an important first step in the development of a wearable robot with embodied connection to the performer and, eventually, audience members.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering"
            }
          ],
          "personId": 37539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering"
            }
          ],
          "personId": 37303
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "The People Movers",
              "dsl": ""
            }
          ],
          "personId": 37583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering"
            }
          ],
          "personId": 37781
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374837"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=zS3ZI7FUHGg",
          "title": "Live Dance Performance Investigating the Feminine Cyborg Metaphor with a Motion-activated Wearable Robot",
          "duration": 581,
          "type": "video"
        }
      },
      "sessionIds": [
        38361
      ],
      "eventIds": []
    },
    {
      "id": 38205,
      "typeId": 11521,
      "title": "CoWriting Kazakh: Learning a New Script with a Robot ",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In the Republic of Kazakhstan, the transition from Cyrillic to Latin alphabet raises challenges to training an entire population in writing the new script. This paper presents a CoWriting Kazakh system, an extension of the existing CoWriter system, aiming to implement an autonomous social robot that would assist children in transition from the old Cyrillic alphabet to a new Latin alphabet. With the aim to investigate which learning strategy yields better learning gains, we conducted an experiment with 67 children, aged 8-11 years old, who interacted with a robot in a CoWriting Kazakh learning scenario. Participants were asked to teach a humanoid NAO robot how to write Kazakh words using one of the scripts, Latin or Cyrillic. We hypothesized that a scenario in which the child is asked to mentally convert the word to Latin would be more effective than having the robot perform conversion itself. Results show that the CoWriter was successfully applied to this new script-switching task. The findings also suggest interesting gender differences in the preferred method of learning with the robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Astana",
              "institution": "Nazarbayev University",
              "dsl": "School of Science and Technology"
            }
          ],
          "personId": 37882
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37815
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37295
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37710
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37463
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37747
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37405
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": ""
            }
          ],
          "personId": 37167
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37595
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37736
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374813"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CmRBnVcBluo",
          "title": "CoWriting Kazakh: Learning a New Script with a Robot ",
          "duration": 595,
          "type": "video"
        }
      },
      "sessionIds": [
        38344
      ],
      "eventIds": []
    },
    {
      "id": 38206,
      "typeId": 11521,
      "title": "Do humans imitate robots? An investigation of strategic social learning in Human-Robot Interaction.",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Theories on social learning indicate that imitative choices are usually performed whenever copying the others’ behaviour has no additional cost. Here, we extended such investigations of social learning to Human-Robot Interaction (HRI). Participants played the Economic Investment Game with a robot banker while observing\r\nanother robot player also investing in the robot banker. By manipulating the robot banker payoff, three conditions of unfairness were created: (1) unfair payoff for the participants, (2) unfair payoff for the robot player and (3) unfair payoff for both. Results showed that when the payoff was low for the participants and high for the robot player, participants invested more money in the robot banker than when both parties received a low return. Also, for this specific condition, participants’ investments increased further with a more interactive robot player (defined as demonstrating increased attention, congruent movements and speech) This suggests that social and cognitive human competencies can be used and transposed to non-human agents. Further, imitation can potentially be extended to HRI, with interactivity likely having a key role in increasing this effect.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "Plymouth University",
              "dsl": "school of computer science"
            }
          ],
          "personId": 37216
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": "School of Computing Electronics and Mathematics"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 37752
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 37441
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Nijmegen",
              "institution": "Radboud University",
              "dsl": "Donders Institute for Brain, Cognition and Behaviour"
            }
          ],
          "personId": 37476
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 37740
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374776"
        }
      },
      "sessionIds": [
        38349
      ],
      "eventIds": []
    },
    {
      "id": 38207,
      "typeId": 11521,
      "title": "\t JESSIE: Synthesizing Social Robot Behaviors for Personalized Neurorehabilitation and Beyond",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "JESSIE is a robotic system that enables novice programmers to program social robots by expressing high-level specifications. We employ control synthesis with a tangible front-end to allow users to define complex behavior for which we automatically generate control code. We demonstrate JESSIE in the context of enabling clinicians to create personalized treatments for people with mild cognitive impairment (MCI) on a Kuri robot, in little time and without error. We evaluated JESSIE with neuropsychologists who reported high usability and learnability. They gave suggestions for improvement, including increased support for personalization, multi-party programming, collaborative goal setting, and re-tasking robot role post-deployment, which each raise technical and sociotechnical issues in HRI. We exhibit JESSIE’s reproducibility by replicating a clinician-created program on a TurtleBot 2. As an open-source means of accessing control synthesis, JESSIE supports reproducibility, scalability, and accessibility of personalized robots for HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 37797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota-Twin Cities",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 37913
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 37725
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 37708
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374836"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=jO490NTsX3I",
          "title": "\t JESSIE: Synthesizing Social Robot Behaviors for Personalized Neurorehabilitation and Beyond",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38344
      ],
      "eventIds": []
    },
    {
      "id": 38208,
      "typeId": 11596,
      "title": "Towards Shoestring Solutions for UK Manufacturing SMEs",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "In the Digital Manufacturing on a Shoestring project we focus on low-cost digital solution requirements for UK manufacturing SMEs. This paper shows that many of these fall in the HRI domain while presenting the use of low-cost and off-the-shelf technologies in two demonstrators based on voice assisted production.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Institute for Manufacturing, Dept of Enginnering"
            }
          ],
          "personId": 37271
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Institute for Manufacturing, Dept of Engineering"
            }
          ],
          "personId": 37780
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37606
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 37562
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37343
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Institute for Manufacturing, Dept of Engineering"
            }
          ],
          "personId": 37946
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37338
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378205"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_d1DXyTqjcw",
          "title": "Towards Shoestring Solutions for UK Manufacturing SMEs",
          "duration": 217,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38209,
      "typeId": 11596,
      "title": "An Application of Low-Cost Digital Manufacturing to HRI",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "Digital Manufacturing (DM) broadly refers to applying digital information to enhance manufacturing processes, supply chains, products and services. In past work we proposed a low-cost DM architecture, supporting flexible integration of legacy robots. Here we discuss a demo of our architecture using an HRI scenario.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37562
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37271
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37343
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37789
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37338
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37606
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 37177
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378204"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=IOMt0ic6T3I",
          "title": "An Application of Low-Cost Digital Manufacturing to HRI",
          "duration": 133,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38210,
      "typeId": 11596,
      "title": "CardBot: Towards an affordable humanoid robot platform for Wizard of Oz Studies in HRI",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "CardBot is a cardboard based programmable humanoid robot platform designed for inexpensive and rapid prototyping of Wizard of Oz interactions in HRI incorporating technologies such as Arduino, Android and Unity3d. The table demonstration showcases the design of the CardBot and its wizard controls such as animating the movements, coordinating speech and gaze etc for orchestrating an interaction.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne University",
              "dsl": "ISIR"
            }
          ],
          "personId": 37672
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "CNRS, ISIR",
              "dsl": "Sorbonne Universié"
            }
          ],
          "personId": 37993
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378203"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Z3fq0UJL7Kw",
          "title": "CardBot: Towards an affordable humanoid robot platform for Wizard of Oz Studies in HRI",
          "duration": 275,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38211,
      "typeId": 11596,
      "title": "Demonstrating MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "We developed a method for modifying emotive robot movements with a reduced dependency on domain knowledge by using neural networks. We use hand-crafted movements for a Blossom robot and a classifying variational autoencoder to adjust affective movement features by using simple arithmetic in the network’s learned latent embedding space. We will demonstrate the workflow of using a graphical interface to modify the valence and arousal of movements. Participants will be able to use the interface themselves and watch Blossom perform the modified movements in real time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Human-Robot Collaboration and Companionship Lab"
            }
          ],
          "personId": 37860
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 37599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37987
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378202"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Ttq-2oQexuk",
          "title": "Demonstrating MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders",
          "duration": 65,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38212,
      "typeId": 11596,
      "title": "Comedy by Jon the Robot",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots might be more effective if they could adapt in playful, comedy-inspired ways based on heard social cues from users. Jon the Robot, a robotic stand-up comedian from the Oregon State University CoRIS Institute, showcases how this type of ability can lead to more enjoyable interactions with robots. We believe conference attendees will be both entertained and informed by this novel demonstration of social robotics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "No affiliation",
              "dsl": ""
            }
          ],
          "personId": 37615
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 37743
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378201"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=UZN6rpGvJXQ",
          "title": "Comedy by Jon the Robot",
          "duration": 136,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38213,
      "typeId": 11596,
      "title": "TapeBot: The Modular Robotic Kit for Creating the Environments ",
      "trackId": 10947,
      "tags": [],
      "keywords": [],
      "abstract": "Modular Robotic Kit for Creating an Interactive Environment",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37856
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37574
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37226
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 37550
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378200"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JyFbIBUp92c",
          "title": "TapeBot: The Modular Robotic Kit for Creating the Environments ",
          "duration": 304,
          "type": "video"
        }
      },
      "sessionIds": [
        38358
      ],
      "eventIds": []
    },
    {
      "id": 38214,
      "typeId": 11521,
      "title": "Patricc: A Platform for Triadic Interaction with Changeable Characters",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "While social robots for education are slowly being integrated in many scenarios, ranging from higher-education, through elementary school and kindergarten, the use case of robots for toddlers in their homes has not gained much attention. In this contribution, we introduce Patricc, a robotic platform that is specifically designed for toddler-parent-robot triadic interaction. It addresses the unique challenges of this age group, namely, desire for continuous physical interaction and novelty. Patricc’s unique design enables changing characters by using dress-able puppets over a 3D-printed skeleton and the use of physical props. A novel authoring tool enables robot behavior and content creation by non-programmers. We conducted an evaluation study with 18 parent-toddler pairs and compared Patricc to similar tablet-based interactions. Our quantitative and qualitative analyses show that Patricc promotes significantly more triadic interaction, measured by video-coded gaze, compared to the tablet and that parents indeed perceive the interaction as triadic. Furthermore, there was no novelty-induced significant change in task-oriented behaviors, when toddlers interacted with two different characters consecutively. Finally, parents pointed out the benefits of changeable puppet-like characters over tablets and the appropriateness of the platform for the target age-group. These results suggest that Patricc can serve as the first gateway of toddlers to the emerging world of social robots. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "Curiosity Lab, Department of Industrial Engineering"
            }
          ],
          "personId": 37648
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Tel Aviv University, Israel",
              "dsl": "Department of Sociology and Anthropology"
            }
          ],
          "personId": 37821
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "The program for cognitive studies of language use"
            }
          ],
          "personId": 37943
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "Curiosity Lab, Department of Industrial Engineering"
            },
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "Sagol School of Neuroscience"
            }
          ],
          "personId": 37415
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374792"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=9L42FNmFBRg",
          "title": "Patricc: A Platform for Triadic Interaction with Changeable Characters",
          "duration": 523,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38215,
      "typeId": 11521,
      "title": "What should robots feel like? Factors that Influence the Perception of Robot-Skin",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "It’s widely accepted that a robot’s embodiment plays an important role during human-robot interaction (HRI). While many studies have explored the effect of robot appearance, relatively little is known about how the texture and stiffness of the surface material, or what may be referred to as ‘robot-skin’, influences how the robot is perceived. Gaining improved understanding in this area may have direct and actionable consequences on robot design, since at present nearly all commercially available service robots have similar exterior surfaces composed of smooth, stiff materials, usually plastic. This study is framed around systematically investigating the type of textures that may be better suited for these robots. First, experiments were undertaken to classify the textural characteristics of 27 distinct materials which could potentially be used as a robotskin. A representative subset of these materials was then selected for a second experiment that explored how the stiffness and tactile properties of the material influenced its perceived suitability for use on a service robot. The research found that people strongly preferred surface textures that were soft, rather than stiff. The most suitable material stiffness was found to be context dependent; soft options were preferred in the blind test condition, but for cases where participants were presented with the 3D image of a service robot in an immersive virtual reality environment, medium stiffness materials were preferred. In the final part of the study, we identified a range of textural properties that seem to correlate with high and low suitability for use on service robots. It is hoped that these findings are useful to help inform the design of future HRI systems, and motivate further investigation into the social roles of robot-skin.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37348
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Please select",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37519
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374835"
        }
      },
      "sessionIds": [
        38350
      ],
      "eventIds": []
    },
    {
      "id": 38216,
      "typeId": 11521,
      "title": "Using the Geneva Emotion Wheel to Measure Perceived Affect in Human-Robot Interaction",
      "award": "BEST_PAPER",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to clearly communicate a wide range of emotional states is considered a desirable trait for social robots. This research proposes that the Geneva Emotion Wheel (GEW), a self-report instrument for measuring emotional reactions, has strong potential for use as a tool for evaluating the expression of affective content by robots. Factors that make the GEW advantageous over existing evaluation methods include: ease of administration, reduction in the importance of word labels, and coverage of ‘no emotion’ states. Statistical analyses of the GEW are proposed, isolating quantitative metrics of emotion distinctness. An experiment requiring participants to rate the perceived emotion of a social robot was conducted, employing the proposed methods. Analysis using the GEW revealed significant differences in the reliability of different expressions to clearly convey emotional states. The GEW provided a repeatable, systematic framework for estimating perceived affect of robot expression. Thus, the results suggest the GEW offers a powerful tool for design purposes as well as analysis. To support future research using the GEW, the software used for the analysis has been packaged and made available as an open-source resource to the community.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Please select",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37306
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Please select",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 38001
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Please select",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37348
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374834"
        }
      },
      "sessionIds": [
        38355
      ],
      "eventIds": []
    },
    {
      "id": 38217,
      "typeId": 11523,
      "title": "Adapting Mixed Reality Robot Communication to Mental Workload",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes early work in the intersection of Mixed Reality for Human-Robot Interaction and Brain-Computer Interface fields. Our research seeks to answer these two questions: (1) How do differ-ent types of mental workload impact the effectiveness of different robot communication modalities? (2) How can a robot select the effective communication modality given information regarding its human teammate’s level and type of mental workload?",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "Mines Interactive Robotics Research Lab"
            }
          ],
          "personId": 37545
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377438"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Saouc3oLKYo",
          "title": "Adapting Mixed Reality Robot Communication to Mental Workload",
          "duration": 185,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38218,
      "typeId": 11597,
      "title": "Studying Language Attitudes Using Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we describe how we use robots to study the effects of transferring features of a native language into one’s second language. Research on language attitudes concerns the identification of the beliefs people hold towards speakers of a particular variety (for instance, a dialect) or towards speakers with a foreign accent. While researchers have been very creative in finding methods for determining speaker attitudes towards their own and other linguistic productions, robots provide an excellent methodological tool to study language attitudes. We illustrate this methodology on the perception of transfer of speech melody from one’s mother tongue to a second language. Our results show effects of such transfer on the perception of the respective speaker; for instance, Danish native speakers may be perceived as dominant when transferring their intonation contours to German, whereas Germans may be perceived as too formal when transferring their speech melody into Danish.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design and Communication"
            }
          ],
          "personId": 37336
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Center for Industrial Electronics"
            }
          ],
          "personId": 37920
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378377"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=B4LkBsKnMO0",
          "title": "Studying Language Attitudes Using Robots",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38219,
      "typeId": 11597,
      "title": "Helping Educators Monitor Autistic Children’s Progress AcrossSessions: A Needfinding Study",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots may be beneficial to educators working with autistic children in helping to monitor the children’s progress. To identify needs for measuring and tracking progress of autistic children, we conducted interviews with nine experienced educators in Serbia and the Netherlands who work with autistic children. Responses revealed educators’ needs to identify antecedents of notable child behaviour, to have standardised measures of social skills, and to understand child behaviour across settings. We present initial design concepts how social robots could be utilised to meet these needs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37767
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37282
        },
        {
          "affiliations": [
            {
              "country": "Serbia and Montenegro",
              "state": "",
              "city": "Belgrade",
              "institution": "Serbian Society of Autism",
              "dsl": ""
            }
          ],
          "personId": 37791
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37549
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37755
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Nanyang Technological University",
              "dsl": ""
            }
          ],
          "personId": 37840
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378378"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=J7a0tmBSP4I",
          "title": "Helping Educators Monitor Autistic Children’s Progress AcrossSessions: A Needfinding Study",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38220,
      "typeId": 11597,
      "title": "Speech Related Accessibility Issues in Social Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This work describes an incidental finding from a longitudinal Human-Robot Interaction study that was investigating whether a robot showing emotions during interactions with older adults was perceived differently than to a robot that did not display emotions during the interaction. During this study we noted that some older adults found it hard to understand what the robot was saying, regardless of the volume of speech generated by the robot.  The fact that they did not have problems in understanding the researcher led us to investigating this accessibility-related issue in more depth. This paper describes the implications of this finding and recommendations on how to approach future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37836
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory, University of the West of England",
              "dsl": ""
            }
          ],
          "personId": 37511
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Department of Health and Social Sciences"
            }
          ],
          "personId": 37739
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37215
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37465
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Faculty of Environment and Technology, UWE",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37410
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378379"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38221,
      "typeId": 11521,
      "title": "An Escalating Model of Children’s Robot Abuse ",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We reveal the process of children engaging in such serious abuse as kicking and punching robots. In study 1, we established a process model of robot abuse and used a qualitative analysis method specialized for time-series data: the Trajectory Equifinality Model (TEM). With the TEM method, we analyzed interactions from nine children who committed serious robot abuse from which we developed a multi-stage model: the abuse escalation model. The model has four stages: approach, mild abuse, physical abuse, and escalation. For each stage, we identified social guides (SGs), which are influencing events that fuel the stage. In study 2, we conducted a quantitative analysis to examine the effect of these SGs. We analyzed 12 hours of data that included 522 children who visited the observed area nearby the robot, coded their behaviors, and statistically tested whether the presence of each SG promoted the stage. Our analysis confirmed the correlations of four SGs and children’s behaviors: the presence of other children related a new child to approach the robot (SG1); mild abuse by another child related a child to do mild abuse (SG2); physical abuse by another child related a child to conduct physical abuse (SG3); and encouragement from others related a child to escalate the abuse (SG5).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "Tokai University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "ATR",
              "dsl": "IRC"
            }
          ],
          "personId": 37247
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37611
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "ATR",
              "dsl": "Intelligent Robotics and Communication Laboratories"
            }
          ],
          "personId": 37281
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374833"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=qVdOmyiAiOM",
          "title": "An Escalating Model of Children's Robot Abuse",
          "duration": 602,
          "type": "video"
        }
      },
      "sessionIds": [
        38346
      ],
      "eventIds": []
    },
    {
      "id": 38222,
      "typeId": 11597,
      "title": "Humanoid Robots: Body Language and Charisma",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "The Darwin OP2 robot was used to play a cooperative game of concentration with a human. Guided calibration was used to teach the robot colors in different environments. Vision algorithms and servo movements were used to identify the elements of the game. The effect of additional body language, which was not necessary for the game, was studied in reference to the way the robot was perceived by human partners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "University of Kansas",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 37332
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378380"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38223,
      "typeId": 11597,
      "title": "Safety Blanket of Humanity: Thinking of Unfamiliar Humans or Robots Increases Conformity to Humans",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "As robots become prevalent, merely thinking of them existence may affect how people behave. When interacting with a robot, people conformed to the robot’s answers more than to their own initial response [1]. In this study, we examine how robot might affect conformity to other humans. We primed participants to think of different experiences: Humans (an experience with a human stranger), Robots (an experience with a robot), or Neutral (daily life). We then measured if participants conformed to other humans in survey answers. Results indicated that people conformed more when thinking of Humans or Robots than of Neutral events. This implies that robots have a similar effect on human conformity to other humans as human strangers do. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37573
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces ",
              "institution": "Department of psychology, New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 37464
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Marketing"
            }
          ],
          "personId": 38364
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378381"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5uslsDZaCqI",
          "title": "Safety Blanket of Humanity: Thinking of Unfamiliar Humans or Robots Increases Conformity to Humans",
          "duration": 98,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38225,
      "typeId": 11521,
      "title": "In Storage, Yet on Display: An Empirical Investigation of Robots’ Value as Social Signals",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper suggests that humans value robotic systems as signals – costly, visible commitments that can secure access to preferred resources. This contrasts with HRI research, design, engineering and deployment, which have focused on robots’ instrumental value – namely how designing and interacting with them may produce more or less productivity. Drawing on a multiyear ethnography of a “failed” robotic telepresence deployment in a teaching hospital, this paper shows that robots’ signaling value can significantly outweigh – and even contravene – any practical utility that they may provide through use. This analysis further suggests that – unlike nontechnological organizational signals – robots’ signaling value is highly contingent on the observability of their use. Considering robots’ signaling value in complex social systems such as organizations promises improved robotic systems development and deployment techniques and improved prediction regarding human-robot interaction. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "UC Santa Barbara",
              "dsl": "Technology Management"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Institute for the Digital Economy"
            }
          ],
          "personId": 37542
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374775"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=jvTUhrnfZ_4",
          "title": "In Storage, Yet on Display: An Empirical Investigation of Robots' Value as Social Signals",
          "duration": 565,
          "type": "video"
        }
      },
      "sessionIds": [
        38343
      ],
      "eventIds": []
    },
    {
      "id": 38228,
      "typeId": 11521,
      "title": "Can a Robot handle Customers with Unreasonable Complaints?",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "The service industry is facing an increase in the number of malicious customers (customers with unreasonable complaints). Employees have reported that handling unreasonable complaints is particularly stressful. Considering the recent push for workplace automation, we should have robots handling this task in place of humans. We propose a robot behavioral model designed for handling unreasonable complaints. The robot with this model has to “please the customer” without proposing a settlement. From a large survey of Japanese workers conducted by labor unions and an interview\r\nsurvey of experienced workers we conducted, we identified the conventional complaint handling flow as 1) listen to the complaint, 2) confirm the content of the complaint, 3) apologize, 4) give an explanation, and 5) conclude. The proposed behavioral model is a variation of this flow that takes into account the “state of mind” of the customer. In particular, the robot with this model does not leave the first step and keeps asking questions until the customer is “ready to listen”. We conducted a user study, using a Wizard-of-Oz approach, to compare the proposed behavioral model to a baseline one implementing the conventional flow. We replicated in our laboratory the situation of a customer in a mobile phone shop. The proposed behavioral model was significantly better at making the customers believe that the robot listened to them and tried to help.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37525
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37232
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37611
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374830"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=RS1Mzi5Dbt0",
          "title": "Can a Robot handle Customers with Unreasonable Complaints?",
          "duration": 509,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38229,
      "typeId": 11521,
      "title": "On-Road and Online Studies to Investigate Beliefs and Behaviors of Netherlands, US and Mexico Pedestrians Encountering Hidden-Driver Vehicles",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "A growing number of studies use a “ghost-driver” vehicle driven by a person in a car seat costume to simulate an autonomous vehicle. Using a hidden-driver vehicle in a field study in the Netherlands, Study 1 (N = 130) confirmed that the ghostdriver methodology is valid in Europe and confirmed that European pedestrians change their behavior when encountering a hidden-driver vehicle. As an important extension to past research, we find pedestrian group size is associated with their behavior: groups look longer than singletons when encountering an autonomous vehicle, but look for less time than singletons when encountering a normal vehicle. Study 2 (N = 101) adapted and extended the hidden-driver method to test whether it is believable as online video stimuli and whether car characteristics and participant feelings are related to the beliefs and behavior of pedestrians who see hidden-driver vehicles. As expected, belief rates were lower for hidden-driver vehicles seen in videos compared to in a field study. Importantly, we found noticing no driver was the only significant predictor of belief in car autonomy, which reinforces prior justification for the use of the ghostdriver method. Our contributions are a replication of the hidden-driver method in Europe and comparisons with past US and Mexico data; an extension and evaluation of the ghostdriver method in video form; evidence of the necessity of the hidden driver in creating the illusion of vehicle autonomy; and an extended analysis of how pedestrian group size and feelings relate to pedestrian behavior when encountering a hidden-driver vehicle.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37282
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center For Design Research"
            }
          ],
          "personId": 37312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 37827
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 37253
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 37176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 37785
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37840
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 37433
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374790"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=gcnDnL5T1zg",
          "title": "On-Road and Online Studies to Investigate Beliefs and Behaviors of Netherlands, US and Mexico Pedestrians Encountering Hidden-Driver Vehicles",
          "duration": 600,
          "type": "video"
        }
      },
      "sessionIds": [
        38344
      ],
      "eventIds": []
    },
    {
      "id": 38230,
      "typeId": 11521,
      "title": "Effects of Different Interaction Contexts when Evaluating Gaze Models in HRI",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We previously introduced a responsive joint attention system that uses multimodal information from users engaged in a spatial reasoning task with a robot and communicates joint attention via the robot’s gaze behavior. An initial evaluation of our system with adults showed it to improve users’ perceptions of the robot’s social presence. To investigate the repeatability of our prior findings across settings and populations, here we conducted two further studies employing the same gaze system with the same robot and task but in different contexts: evaluation of the system with external observers and evaluation with children. The external observer study suggests that third-person perspectives over videos of gaze manipulations can be used either as a manipulation check before committing to costly real-time experiments or to further establish previous findings. However, the replication of our original adults study with children in school did not confirm the effectiveness of our gaze manipulation, suggesting that different interaction contexts can affect the generalizability of results in human-robot interaction gaze studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37531
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Interactive Intelligence"
            }
          ],
          "personId": 37888
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Instituto Superior Tecnico",
              "dsl": ""
            }
          ],
          "personId": 37457
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Furhat Robotics",
              "dsl": ""
            }
          ],
          "personId": 37753
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37218
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374810"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=HGEdJc_k2SA",
          "title": "Effects of Different Interaction Contexts when Evaluating Gaze Models in HRI",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        38344
      ],
      "eventIds": []
    },
    {
      "id": 38231,
      "typeId": 11521,
      "title": "Designing Social Interactions with a Humorous Robot Photographer",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes our efforts to explore the design space of social interactions for a robot portrait photographer. Our human-centered design process involved professional and amateur photographers to better understand the social dimensions of subject-photographer interactions. This exploration then guided our design of a robot photographer, which employs humor to elicit spontaneous smiles during photography events. In a laboratory evaluation of our robot prototype, we found that the majority of the subjects considered the robot’s humor to be comical and appreciated it. More spontaneous smiles were elicited by the robot when it delivered humorous content to its subjects than when it was not humorous. Our findings provide insights for the design of future social robot photographers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": ""
            }
          ],
          "personId": 37712
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Hopkins School",
              "dsl": ""
            }
          ],
          "personId": 37697
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 38371
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": ""
            }
          ],
          "personId": 37416
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374809"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=l4WITdF6kkk",
          "title": "Designing Social Interactions with a Humorous Robot Photographer",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38361
      ],
      "eventIds": []
    },
    {
      "id": 38232,
      "typeId": 11521,
      "title": "Designing Social Cues for Collaborative Robots: The Role of Gaze and Breathing in Human-Robot Collaboration",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we investigate how collaborative robots, or cobots, typically composed of a robotic arm and a gripper carrying out manipulation tasks alongside human coworkers, can be enhanced with HRI capabilities by applying ideas and principles from character animation. To this end, we modified the appearance and behaviors of a cobot, with minimal impact on its functionality and performance, and studied the extent to which these modifications improved its communication with and perceptions by human collaborators. Specifically, we aimed to improve the Appeal of the robot by manipulating its physical appearance, posture, and gaze, creating an animal-like character with a head-on-neck morphology; to utilize Arcs by generating smooth trajectories for the robot arm; and to increase the lifelikeness of the robot through Secondary Action by adding breathing motions to the robot. In two user studies, we investigated the effects of these cues on collaborator perceptions of the robot. Findings from our first study showed breathing to have a positive effect on most measures of robot perception and reveal nuanced interactions among the other factors. Data from our second study showed that, using gaze cues alone, a robot arm can improve metrics such as likeability and perceived sociability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University Khoury College of Computer Sciences",
              "dsl": ""
            }
          ],
          "personId": 37241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 37808
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Kovan Research Laboratory"
            }
          ],
          "personId": 37161
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374829"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=-PuSBtjItgY",
          "title": "Designing Social Cues for Collaborative Robots: The Role of Gaze and Breathing in Human-Robot Collaboration",
          "duration": 459,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38233,
      "typeId": 11521,
      "title": "When Humans Aren’t Optimal: Robots that Collaborate with Risk-Aware Humans",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In order to collaborate safely and efficiently, robots need to anticipate how their human partners will behave. Some of today’s robots model humans as if they were also robots, and assume users are always optimal. Other robots account for human limitations, and relax this assumption so that the human is noisily rational. Both of these models make sense when the human receives deterministic rewards: i.e., gaining either $100 or $130 with certainty. But in realworld scenarios, rewards are rarely deterministic. Instead, we must make choices subject to risk and uncertainty—and in these settings, humans exhibit a cognitive bias towards suboptimal behavior. For example, when deciding between gaining $100 with certainty or $130 only 80% of the time, people tend to make the risk-averse choice—even though it leads to a lower expected gain! In this paper, we adopt a well-known Risk-Aware human model from behavioral economics called Cumulative Prospect Theory and enable robots to leverage this model during human-robot interaction (HRI). In our user studies, we offer supporting evidence that the Risk-Aware model more accurately predicts suboptimal human behavior. We find that this increased modeling accuracy results in safer and more efficient human-robot collaboration. Overall, we extend existing rational human models so that collaborative robots can anticipate and plan around suboptimal human behavior during HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 37905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Electrical Engineering"
            }
          ],
          "personId": 38004
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Fremont",
              "institution": "American High School",
              "dsl": ""
            }
          ],
          "personId": 37676
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "The Harker School",
              "dsl": ""
            }
          ],
          "personId": 37219
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37398
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science and Electrical Engineering"
            }
          ],
          "personId": 37587
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374832"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Viorzf-o8Nk",
          "title": "When Humans Aren't Optimal: Robots that Collaborate with Risk-Aware Humans",
          "duration": 577,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38234,
      "typeId": 11521,
      "title": "Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Reinforcement learning (RL) has achieved tremendous success as a general framework for learning how to make decisions. However, this success relies on the interactive hand-tuning of a reward function by RL experts. On the other hand, inverse reinforcement learning (IRL) seeks to learn a reward function from readily-obtained human demonstrations. Yet, IRL suffers from two major limitations: 1) reward ambiguity - there are an infinite number of possible reward functions that could explain an expert’s demonstration and 2) heterogeneity - human experts adopt varying strategies and preferences, which makes learning from multiple demonstrators difficult due to the common assumption that demonstrators seeks to maximize the same reward. In this work, we propose a method to jointly infer a task goal and humans’ strategic preferences via network distillation. This approach enables us to distill a robust task reward (addressing reward ambiguity) and to model each strategy’s objective (handling heterogeneity). We demonstrate our algorithm can better recover task reward and strategy rewards and imitate the strategies in two simulated tasks and a real-world table tennis task. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 37428
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Tech",
              "dsl": "Institute of Robotics and Intelligent Machines"
            }
          ],
          "personId": 37458
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 37745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 37504
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374791"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=BJMMip9hE-w",
          "title": "Joint Goal and Strategy Inference across Heterogeneous Demonstrators via Reward Network Distillation",
          "duration": 596,
          "type": "video"
        }
      },
      "sessionIds": [
        38357
      ],
      "eventIds": []
    },
    {
      "id": 38235,
      "typeId": 11521,
      "title": "LESS is More: Rethinking Probabilistic Models of Human Behavior",
      "award": "BEST_PAPER",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Robots need models of human behavior for both inferring human goals and preferences, and predicting what people will do. A common model is the Boltzmann noisily-rational decision model, which assumes people approximately optimize a reward function and choose trajectories in proportion to their exponentiated reward. While this model has been successful in a variety of robotics domains, its roots lie in econometrics, and in modeling decisions among different discrete options, each with its own utility or reward. In contrast, human trajectories lie in a continuous space, with continuous-valued features that influence the reward function. We propose that it is time to rethink the Boltzmann model, and design it from the ground up to operate over such trajectory spaces. We introduce a model that explicitly accounts for distances between trajectories, rather than only their rewards. Rather than each trajectory affecting the decision independently, similar trajectories now affect the decision together. We start by showing that our model better explains human behavior in a user study. We then analyze the implications this has for robot inference, first in toy environments where we have ground truth and find more accurate inference, and finally for a 7DOF robot arm learning from user demonstrations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37179
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37859
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37669
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37228
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374811"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=xa_l5HeyVgw",
          "title": "LESS is More: Rethinking Probabilistic Models of Human Behavior",
          "duration": 600,
          "type": "video"
        }
      },
      "sessionIds": [
        38349
      ],
      "eventIds": []
    },
    {
      "id": 38236,
      "typeId": 11521,
      "title": "Exploring the Role of Gender in Perceptions of Robotic Noncompliance",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "A key capability of morally competent robots is to reject or question potentially immoral human commands. However, robot rejections of inappropriate commands must be phrased with great care and tact. Previous research has shown that failure to calibrate the “face threat\" in a robot’s command rejection to the severity of the norm violation in the command can lead humans to perceive the robot as inappropriately harsh and can needlessly decrease robot likeability. However, it is well-established that gender plays a significant role in determining linguistic politeness norms and that people have a powerful natural tendency to gender robots. Yet, the effect of robotic gender presentation on these noncompliance interactions is not well understood. We present an experiment that explores the effects of robot and human gender on perceptions of robots in noncompliance interactions, and find evidence of a complicated interplay between these gendered factors. Our results suggest that (1) it may be more favorable for a male robot to reject commands than for a female robot to do so, (2) it may be more favorable to reject commands given by a male human than by a female human, and (3) that robots may be perceived more favorably when their gender matches that of human interactants and observers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 37480
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 37233
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "Mining Engineering"
            }
          ],
          "personId": 37906
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374831"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cv_u9hAmefk",
          "title": "Exploring the Role of Gender in Perceptions of Robotic Noncompliance",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38237,
      "typeId": 11597,
      "title": "Dance with a Robot: Encoder-Decoder Neural Network for Music-Dance Learning",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This late-breaking report presents a method for learning sequential and temporal mapping between music and dance via the Sequence-to-Sequence (Seq2Seq) architecture. In this study, the Seq2Seq model comprises two parts: the encoder for processing the music inputs and the decoder for generating the output motion vectors. This model has the ability to accept music features and motion inputs from the user for human-robot interactive learning sessions, which outputs the motion patterns that teach the corrective movements to follow the moves from the expert dancer. Three different types of Seq2Seq models are compared in the results and applied to a simulation platform. This model will be applied in social interaction scenarios with children with autism spectrum disorder (ASD).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 37518
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 37855
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378372"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=gV5mkpEhnVk",
          "title": "Dance with a Robot: Encoder-Decoder Neural Network for Music-Dance Learning",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38238,
      "typeId": 11597,
      "title": "Persuasive Social Robots using Reward/Coercion Strategies",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present a user-study designed to examine the effect of reward/coercion persuasive strategies inspired by social power. We ran the study with 90 participants in a persuasion scenario in which they were asked to make a real choice to select a less-desirable option. The preliminary results indicated that the robot succeeded in persuading the users to select a less desirable choice compared to a better one. However, no difference was found in the perception of the robot comparing the two strategies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "DEIC"
            }
          ],
          "personId": 37440
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37352
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37742
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37534
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37257
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37945
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378373"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_by3_NLrrVY",
          "title": "Persuasive Social Robots using Reward/Coercion Strategies",
          "duration": 135,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38239,
      "typeId": 11521,
      "title": "Strategies for the Inclusion of Human Members within Human-Robot Teams",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Team member inclusion is vital in collaborative teams. In this work, we explore two strategies to increase the inclusion of human team members in a human-robot team: 1) giving a person in the group a specialized role (the ‘robot liaison’) and 2) having the robot verbally support human team members. In a human subjects experiment (N = 26 teams, 78 participants), groups of three participants completed two rounds of a collaborative task. In round one, two participants (ingroup) completed a task with a robot in one room, and one participant (outgroup) completed the same task with a robot in a different room. In round two, all three participants and one robot completed a second task in the same room, where one participant was designated as the robot liaison. During round two, the robot verbally supported each participant 6 times on average. Results show that participants with the robot liaison role had a lower perceived group inclusion than the other group members. Additionally, when outgroup members were the robot liaison, the group was less likely to incorporate their ideas into the group’s final decision. In response to the robot’s supportive utterances, outgroup members, and not ingroup members, showed an increase in the proportion of time they spent talking to the group. Our results suggest that specialized roles may hinder human team member inclusion, whereas supportive robot utterances show promise in\r\nencouraging contributions from individuals who feel excluded.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37719
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37540
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37670
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374808"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=g88ypBQ1nUc",
          "title": "Strategies for the Inclusion of Human Members within Human-Robot Teams",
          "duration": 599,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38240,
      "typeId": 11597,
      "title": "Towards using Virtual Reality for replicating HRI studies",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "As a first step into utilizing Virtual Reality (VR) for Human-Robot interaction(HRI) studies we attempted to replicate a study from Kahn et al., which was interested in peoples' secret keeping behaviour with regards to robotic tour guides compared to human tour guides. Some changes had to be made to the original study but the essence of the experiment was maintained. Results suggest that there are many differences in how the participants in this study viewed the various robot behaviours when compared to how participants viewed the guides in the original study. As such no conclusive statements can be made about the overall suitability of VR as a platform for conducting HRI studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37346
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37754
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Lab"
            }
          ],
          "personId": 37852
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378374"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38241,
      "typeId": 11523,
      "title": "Detecting Hypothesis Space Misspecification in Robot Learning from Human Input",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Learning from human input has enabled autonomous agents to per-form increasingly more complex tasks that are otherwise difficult to carry out automatically. To this end, recent works have studied how robots can incorporate such input – like demonstrations or corrections – into objective functions describing the desired behaviors. While these methods have shown progress in a variety of settings, from semi-autonomous driving, to household robotics, to automated airplane control, they all suffer from the same crucial drawback: they implicitly assume that the person’s intentions can always be captured by the robot’s hypothesis space. We call attention to the fact that this assumption is often unrealistic, as no model can completely account for every single possible situation ahead of time. When the robot’s hypothesis space is misspecified, human input can be unhelpful – or even detrimental – to the way the robot is performing its tasks. Our work tackles this issue by proposing that the robot should first explicitly reason about how well its hypothesis space can explain human inputs, then use that situational confidence to inform how it should incorporate them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 37179
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377436"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Q0Zi7RVZb4g",
          "title": "Detecting Hypothesis Space Misspecification in Robot Learning from Human Input",
          "duration": 181,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38242,
      "typeId": 11597,
      "title": "Auditory and Haptic Feedback in a Socially Assistive Robot Memory Game",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Age-related cognitive impairment is becoming a more prevalent concern as the elderly population continues to increase. Technological systems created for cognitive rehabilitation need to be motivating to combat the personal and logistic factors that make it difficult for them to remain engaged [4]. In this paper, we present a pilot study with a socially assistive robot-facilitated memory game that employs sensory feedback (audio, haptic, and both) to explore the design considerations with adults. The ultimate aim is to inform the design of a cognitive rehabilitation system for individuals with age-related cognitive decline. The preliminary results suggest a preference for auditory feedback, and participants believed they performed best in this condition. Based on qualitative feedback, we have identified improvements that can be made to the system to enhance engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 37942
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37730
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Mathematical and Computer Science"
            }
          ],
          "personId": 37270
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378375"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=SCCBFwLPr9Y",
          "title": "Auditory and Haptic Feedback in a Socially Assistive Robot Memory Game",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38243,
      "typeId": 11597,
      "title": "Parental Expectations, Concerns, and Acceptance of Storytelling Robots for Children",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Robots that tell children stories are becoming common. Given that the practice of parent–child storytelling is part of family culture, it is critical to investigate parental acceptance of storytelling robots. Drawing on technology acceptance models, the theory of planned behavior, and Bowen family systems theory, we conducted a mixed-methods study involving an online survey of 115 respondents and 18 in-person interviews. We aimed to propose a model of parental acceptance of storytelling robots contextualized in potential use case scenarios. Preliminary findings indicate an overall positive attitude towards children’s storytelling robots and identify factors that can affect parental acceptance of these robots. This study may inform the design of storytelling robots tailored to the needs of parents and their children in the home.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 37400
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis ",
              "institution": "Indiana University ",
              "dsl": ""
            }
          ],
          "personId": 37208
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 37344
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis",
              "institution": "IUPUI (Indiana University Purdue University Indianapolis)",
              "dsl": "Department of Human Centered Computing, School of Informatics and Computing"
            }
          ],
          "personId": 37523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis",
              "institution": "Indiana University (IUPUI)",
              "dsl": "School of Informatics and Computing"
            }
          ],
          "personId": 37848
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378376"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38246,
      "typeId": 11521,
      "title": "Would You Mind Me if I Pass by You? Socially-Appropriate Behaviour for an Omni-based Social Robot in Narrow Environment",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Interacting physically with robots and sharing environment with them leads to situations where humans and robots have to cross each other in narrow corridors. In these cases, the robot has to make space for the human to pass. From observation of human-human\r\ncrossing behaviours, we isolated two main factors in this avoiding behaviour: body rotation and sliding motion. We implemented a robot controller able to vary these factors and explored how this variation impacted on people’s perception. Results from a within participants study involving 23 participants show that people prefer a robot rotating its body when crossing them. Additionally, a sliding motion is rated as being warmer. These results show the importance of social avoidance when interacting with humans.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Souraku-gun",
              "institution": "ATR",
              "dsl": "IRC"
            }
          ],
          "personId": 37810
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Souraku-gun,",
              "institution": "ATR",
              "dsl": "IRC"
            }
          ],
          "personId": 37490
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 37611
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374812"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=wPnGHM_edk4",
          "title": "Would You Mind Me if I Pass by You?: Socially-Appropriate Behaviour for an Omni-based Social Robot in Narrow Environment",
          "duration": 598,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38248,
      "typeId": 11523,
      "title": "Medical Interviewing with a Robot instead of a Doctor",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Patients  often  do  not  trust  their  physicians  with  confidential, private   information.   They   are   worried   about   judgment,   and ultimately this leads to poorer health outcomes. Physicians also do not listen to specific groups of people, biasing healthcare decisions. It may, therefore, be helpful to complement or delegate some of a physician’s tasks to a robot. People are more willing to disclose private  information  to robots,  which  they  find  unbiased  without negative  judgment [2].  Robots  can  ask  all  relevant  questions regardless of sex, gender, or sexual orientation [11]. This proposal explores the  use  of  robotics  within  medicine, evaluating patient trust and information  disclosure, to  supplement  and  promote unbiased   healthcare   provider   decisions.   The   experiment   will employ a physician to conduct 90 patient interviews between three groups(G)using  the  standardized  Brown  Interview  Checklist, either with (G1) or without (G2) a proxy robot. Patients interviewed by  the  robot  will  be  split  between  those  aware  (G2a) or unaware (G2b) that a physician will be controlling the robot. We hypothesize that using a physical robot will improve information disclosure with less stress, and perhaps even off-load physician workload for more targeted and appropriate healthcare decisions",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Drexel University",
              "dsl": "College of Medicine"
            }
          ],
          "personId": 37597
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "United States Air Force Academy",
              "dsl": "Warfighter Effectiveness Research Center"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "AF Academy",
              "institution": "United States Air Force Academy",
              "dsl": "Department of Behavioral Sciences and Leadership"
            }
          ],
          "personId": 37509
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            }
          ],
          "personId": 37308
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Drexel University",
              "dsl": "School of Biomedical Engineering, Science and Health Systems"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Children's Hospital of Philadelphia",
              "dsl": "Center for Injury Research and Prevention"
            }
          ],
          "personId": 37691
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377441"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=qlXG_eU8rMg",
          "title": "Medical Interviewing with a Robot instead of a Doctor",
          "duration": 187,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38249,
      "typeId": 11597,
      "title": "Introducing a Robotic Arm For Remedial Teaching: The impact of motion type on robot-student interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This project is an attempt to investigate the efficacy of a Robotic Arm as a paradigm-changing teaching aid in a graphomotorics therapy environment. The project investigates if a collaboration between human teacher and a robotic arm can significantly improve the quality of a teaching session. A series of experiments tested two modes of robotic arm movements: \"Human-like\" and \"Robotic\" with two different robots. The experiments featured exercises that improve graphomotorics contributing abilities such as fine motoric skills, serial and spatial memory. These exercises, based predominantly on pointing and gesturing by a teacher, were conducted by a robotic arm, and included feedback on successful completion of the task.  Results of preliminary experiments showed that in addition to positive interaction between the student and the robot, the basic relationship between student and teacher was also impacted by shifting the balance of power. Further research is required to conclude the optimal human-robot involvement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Jerusalem",
              "institution": "Bezalel Academy of Arts and Design",
              "dsl": "Master's Program in Industrial Design "
            }
          ],
          "personId": 37643
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378275"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38250,
      "typeId": 11597,
      "title": "Now, Over Here: Leveraging Extended Attentional Capabilities in Human-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Competent collaboration between robots and people in the open world requires sensing and reasoning about transitions of people's attention to the robots themselves, as well as to other people and objects in the environment. We present challenges and opportunities with designing extended attentional capabilities for interactive systems, including the need to track, reason about, and manage the attentional foci of all actors. We describe work in progress to leverage such attentional capabilities for interaction management with a prototype situated robotic system. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 37989
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 37243
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 37807
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 37610
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378363"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=UFLksLsWH5w",
          "title": "Now, Over Here: Leveraging Extended Attentional Capabilities in Human-Robot Interaction",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38251,
      "typeId": 11597,
      "title": "From Psychological Intention Recognition Theories to Adaptive Theory of Mind for Robots: Computational Models",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Progress in robots’ application to everyday scenarios has increased the interest in human-robot interaction (HRI) research. However, robots’ limited social skills are associated with decreased humans’ positive attitude during HRI. Here, we put forward the idea of developing adaptive Theory of Mind (ToM) model-based systems for social robotics, able to deal with new situations and interact with different users in new tasks. Therefore, we grouped current research from developmental psychology debating the computational processes underlying ToM for HRI strategy development. Defining a model describing adaptive ToM processes may in fact aid the development of adaptive robotic architectures for more flexible and successful HRI. Finally, we hope with this report to both further promote the cross-talk between the fields of developmental psychology and robotics and inspire future investigations in this direction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Colchester",
              "institution": "University of Essex",
              "dsl": "School of Computer Science and Electronic Engineering & Department of Psychology"
            }
          ],
          "personId": 37189
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Colchester",
              "institution": "University of Essex",
              "dsl": "School of Computer Science and Electonic Engineering"
            }
          ],
          "personId": 37616
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378364"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=3ZWQDcF4wlg",
          "title": "From Psychological Intention Recognition Theories to Adaptive Theory of Mind for Robots: Computational Models",
          "duration": 127,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38252,
      "typeId": 11597,
      "title": "Addressing Attention Difficulties in Autistic Children Using Multimodal Cues from a Humanoid Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We investigated how voice and motion from a small humanoid robot affect an autistic child's re-engagement of attention. Results suggest that a robot can use motion to re-engage the attention of an autistic child and that two adjoining multimodal cues are more effective than a single unimodal cue.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37282
        },
        {
          "affiliations": [
            {
              "country": "Serbia and Montenegro",
              "state": "",
              "city": "Belgrade",
              "institution": "Serbian Society of Autism",
              "dsl": ""
            }
          ],
          "personId": 37791
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Overijssel",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37379
        },
        {
          "affiliations": [
            {
              "country": "Serbia and Montenegro",
              "state": "",
              "city": "Belgrade",
              "institution": "Serbian Society of Autism",
              "dsl": ""
            }
          ],
          "personId": 37699
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "IIT",
              "dsl": ""
            }
          ],
          "personId": 37790
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37840
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378276"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=iaa7bDtevpU",
          "title": "Addressing Attention Difficulties in Autistic Children Using Multimodal Cues from a Humanoid Robot",
          "duration": 115,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38253,
      "typeId": 11597,
      "title": "How Culture and Presence of a Robot Affect Teachers' Use of Touch with Autistic Children",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We quantitatively analyze and compare how teachers in Serbia and the UK use physical contact to guide autistic children through an activity with and without a robot. We annotated 40 videos from the DE-ENIGMA dataset of autistic children interacting with or without a robot in the presence of an adult teacher in Serbia or the UK. Results show touch was widely used in both countries and more when the robot was not present. Culture affected where touch occurred, while the robot affected touch style.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37282
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37625
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378365"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MV_OS14_KXA",
          "title": "How Culture and Presence of a Robot Affect Teachers' Use of Touch with Autistic Children",
          "duration": 104,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38254,
      "typeId": 11597,
      "title": "Predicting Social Dynamics in Child-Robot Interactions with Facial Action Units",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "We examine the extent to which task engagement, social engagement, and social attitude in child-robot interaction can be predicted on the basis of Facial Action Unit (FAU) intensity. The analyses were based on child-robot and child-child interaction data from the PInSoRo dataset [1]. We applied Logistic Regression, Naive Bayes, and Probabilistic Neural Networks to these data. Results indicated that FAU intensities have potential to predict social dynamics in child-robot interactions (average balanced accuracy scores up to 84%), and illustrate a difference in behavior of children towards other children when compared to their interaction with robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 37812
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37835
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37878
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378366"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=a9nZLI0ZJpc",
          "title": "Predicting Social Dynamics in Child-Robot Interactions with Facial Action Units",
          "duration": 123,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38255,
      "typeId": 11597,
      "title": "Do Shy Children Behave Differently than Non-shy Children in a Long-term Child–robot Interaction? An Analysis of Positive and Negative Expressions of Shyness in Kindergarten Children",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots offer versatile possibilities to engage children in social interaction and are increasingly implemented in educational contexts. However, how children enter into social interaction with an interlocutor is substantially influenced by their temperament. In this paper, we present preliminary findings of a long-term study on child–robot interaction for language learning, in which we focused on children’s positive and negative expressions of shyness within the interaction over multiple sessions with the robot. We found that shy children initially displayed significantly fewer positive signals. However, in the long term, shy children seem to be able to overcome their restrained behavior in interaction with the robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities"
            }
          ],
          "personId": 37904
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities"
            }
          ],
          "personId": 37517
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderbon",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities"
            }
          ],
          "personId": 37514
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378367"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=FyFAa6Het-k",
          "title": "Do Shy Children Behave Differently than Non-shy Children in a Long-term Child–robot Interaction? An Analysis of Positive and Negative Expressions of Shyness in Kindergarten Children",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38256,
      "typeId": 11597,
      "title": "How do situation awareness affect people's physical engagement with a robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we investigate the relationship between situation awareness and compliance in human-robot interaction. We carried out a between-subject experiment (N=30) in which a robot interrupts people doing solitary work to do small physical exercises. In one condition the robot displays an awareness to the activity participants are currently engaged in. In the control condition the robot displays no such awareness. Results show that participants initially ignore the robot in both condition, over time participants interacting with the `aware' robot comply more often with the requests and as a result complete more exercises.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kolding",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37619
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kolding",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37839
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kolding",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37997
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kolding",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 37931
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378368"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38257,
      "typeId": 11597,
      "title": "An explorative study on robotics for supporting children with Autism Spectrum Disorder during clinical procedures",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This short report presents a small-scale explorative study about children with Autism Spectrum Disorder (ASD) interaction with robots during clinical interactions. This is part of an ongoing project, which aims at defining a robotic service for supporting children with developmental disabilities and increase the efficiency of routine procedures that may create distress, e.g. having blood taken or an orthopaedic plaster cast applied.\r\nFive children with confirmed diagnoses of ASD interacted with two social robots: the small humanoid NAO and the pet-like MiRo. The encounters mixed play activities with a simulated clinical procedure. We included parents/carers in the interaction to ensure the child was comfortable and at ease. The results of video analysis and parents' feedback confirm possible benefits of the physical presence of robots to reduce children’s anxiety and increase compliance with instructions. Parents/carers convincingly support the introduction of robots in hospital procedures to their help children.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "Advanced Wellbeing Research Centre"
            }
          ],
          "personId": 37357
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "Sheffield Robotics"
            }
          ],
          "personId": 37875
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "University of Sheffield",
              "dsl": "Department of Human Communication Sciences"
            }
          ],
          "personId": 37813
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "Centre for Automation and Robotics Research"
            }
          ],
          "personId": 37650
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "School of Health and Wellbeing"
            }
          ],
          "personId": 37478
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "Sheffield Hallam University",
              "dsl": "School of Health and Wellbeing"
            }
          ],
          "personId": 37560
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "SHEFFIELD CHILDREN'S NHS FOUNDATION TRUST",
              "dsl": ""
            }
          ],
          "personId": 37378
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Sheffield",
              "institution": "SHEFFIELD CHILDREN'S NHS FOUNDATION TRUST",
              "dsl": ""
            }
          ],
          "personId": 37738
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378277"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=l0cTOO5l6X4",
          "title": "An explorative study on robotics for supporting children with Autism Spectrum Disorder during clinical procedures",
          "duration": 121,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38258,
      "typeId": 11597,
      "title": "Challenges of a Real-World HRI Study with Non-Native English Speakers: Can Personalisation Save the Day?",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Real-world studies allow for testing the limits of HRI systems and observing how people react to failures. We developed a fully autonomous personalised barista robot and deployed the robot on an international student campus for five days. We experienced several challenges, the most important one being speech recognition failures due to foreign accents. Nonetheless, these failures showed a different perspective on HRI, and we demonstrate how personalisation can overcome a negative user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "Plymouth University",
              "dsl": "Centre for Robotics and Neural Systems"
            }
          ],
          "personId": 37902
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Polytech Sorbonne",
              "dsl": ""
            }
          ],
          "personId": 37535
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "SoftBank Robotics Europe",
              "dsl": ""
            }
          ],
          "personId": 38018
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "Plymouth University",
              "dsl": "Centre for Robotics and Neural Systems"
            }
          ],
          "personId": 37190
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378278"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_g2H1Dk83wQ",
          "title": "Challenges of a Real-World HRI Study with Non-Native English Speakers: Can Personalisation Save the Day?",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38260,
      "typeId": 11521,
      "title": "MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a method for modifying affective robot movements using neural networks. Social robots use gestures and other movements to express their internal states. However, a robot’s interactive capabilities are hindered by the predominant use of a limited set of preprogrammed or hand-animated behaviors, which can be repetitive and predictable, making sustained human-robot interactions difficult to maintain. To address this, we developed a method for modifying existing emotive robot movements by using neural networks. We use hand-crafted movement samples and a classifying variational autoencoder trained on these samples. Our method then allows for adjustment of affective movement features by using simple arithmetic in the network’s latent embedding space. We present the implementation and evaluation of this approach and show that editing in the latent space can modify the emotive quality of the movements while preserving recognizability and legibility in many cases. This supports neural networks as viable tools for creating and modifying expressive robot behaviors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Human-Robot Collaboration and Companionship Lab"
            }
          ],
          "personId": 37860
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 37599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37987
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374807"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=mP8qEnDWCH0",
          "title": "MoveAE: Modifying Affective Robot Movements Using Classifying Variational Autoencoders",
          "duration": 595,
          "type": "video"
        }
      },
      "sessionIds": [
        38355
      ],
      "eventIds": []
    },
    {
      "id": 38261,
      "typeId": 11521,
      "title": "Robots in the Danger Zone: Exploring Public Perception through Engagement",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Public perceptions of Robotics and Artificial Intelligence (RAI) are important in the acceptance, uptake, government regulation and research funding of this technology. Recent research has shown that the public’s understanding of RAI can be negative or inaccurate. We believe effective public engagement can help ensure that public opinion is better informed. In this paper, we describe our first iteration of a high throughput in-person public engagement activity. We describe the use of a light touch quiz-format survey instrument to integrate in-the-wild research participation into the engagement, allowing us to probe both the effectiveness of our engagement strategy, and public perceptions of the future roles of robots and humans working in dangerous settings, such as in the off-shore energy sector. We critique our methods and share interesting results into generational differences within the public’s view of the future of Robotics and AI in hazardous environments. These findings include that older peoples’ views about the future of robots in hazardous environments were not swayed by exposure to our exhibit, while the views of younger people were affected by our exhibit, leading us to consider carefully in future how to more effectively engage with and inform older people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37300
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37529
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh ",
              "institution": "The University of Edinburgh ",
              "dsl": ""
            }
          ],
          "personId": 37750
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "Institute for Integrated Micro and Nano Systems"
            }
          ],
          "personId": 37567
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "The University of Edinburgh",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37679
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Liverpool",
              "institution": "School of Engineering",
              "dsl": "University of Liverpool"
            }
          ],
          "personId": 37976
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "ECR",
              "dsl": ""
            }
          ],
          "personId": 37418
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 37386
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 37436
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Department of Aeronautics"
            }
          ],
          "personId": 37403
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "The University of Edinburgh",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37296
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374789"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=ldl4BtSIBow",
          "title": "Robots in the Danger Zone: Exploring Public Perception through Engagement",
          "duration": 336,
          "type": "video"
        }
      },
      "sessionIds": [
        38343
      ],
      "eventIds": []
    },
    {
      "id": 38262,
      "typeId": 11521,
      "title": "Using Self-Determination Theory in Social Robots to Increase Motivation in L2 Word Learning",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This study presents a second language word learning experiment using a social robot with motivational strategies. These strategies were implemented in a social robot tutor to stimulate preschool children’s intrinsic motivation. Subsequently, we investigated their effect on children’s task engagement and word learning performance. The strategies were derived from the Self-Determination Theory, a well-known psychological theory that assumes that intrinsic motivation is strongly related to the fulfillment of three basic human needs, namely the need for autonomy, competence, and relatedness. We found an increase in the strength and duration of task engagement when all three psychological needs were supported by the robot. However, no significant results for learning gains were observed. Our intervention appears a promising method for improving child-robot interactions in educational settings, especially to sustain in long-term interactions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept. of Communication and Cognition"
            }
          ],
          "personId": 37350
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept. of Communication and Cognition"
            }
          ],
          "personId": 37866
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept. of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37744
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg university",
              "dsl": "Dept. of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37286
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Tilburg center for Cognition and Communication (TiCC)"
            }
          ],
          "personId": 37831
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept. of Culture Studies"
            }
          ],
          "personId": 37598
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept. of Developmental Psychology"
            }
          ],
          "personId": 37665
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Dept of Cognitive Science and Artificial Intelligence"
            }
          ],
          "personId": 37878
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374828"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=mlD-XXwsUv0",
          "title": "Using Self-Determination Theory in Social Robots to Increase Motivation in L2 Word Learning",
          "duration": 596,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38263,
      "typeId": 11597,
      "title": "Context matters! Identifying Social Context Factors and Assessing Their Relevance for a Socially Assistive Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In three online studies we examine which social context factors determine whether a robot should (not) deliver a message/reminder to its user using three scenarios: a smart kitchen (n= 101), a smart living room (n= 96) and a smart office (n= 96). In addition, we varied the nature of the message (urgent/non-urgent; sensitive/non-sensitive). In this late braking report, we present some preliminary insights on the nature of the situations and message types for which participants would prefer the message to be delivered",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 37885
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-University Würzburg",
              "dsl": ""
            }
          ],
          "personId": 37681
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilian-University Würzburg",
              "dsl": ""
            }
          ],
          "personId": 37198
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Deutschland",
              "city": "Würzburg",
              "institution": "Julius-Maximilian-University",
              "dsl": "Medienkommunikation B.Sc."
            }
          ],
          "personId": 37292
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378370"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QHrPiKpgS8w",
          "title": "Context matters! Identifying Social Context Factors and Assessing Their Relevance for a Socially Assistive Robot",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38264,
      "typeId": 11521,
      "title": "Planning with Partner Uncertainty Modeling for Efficient Information Revealing in Teamwork",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Communication among team members is important for efficient teamwork, to coordinate behavior and ensure that all team members have the information they need to complete the task. To enable effective communication and thus efficient teamwork, we propose a multi-agent planning approach to revealing information based on its benefit to joint team performance. By explicitly modeling the partner’s knowledge and behavior, our approach allows a robot in a team to reason about when information is useful, how the communication is effective, and to communicate through efficient actions. That is, the robot provides only the necessary information for task completion, provides the information at the time that it is needed, and through the action(s) that optimizes team performance. We validated this approach in a human study in which participants walk together with a robot to a destination that is known only to the robot. We compared to a legible motion generation approach, and showed that users perceived our approach as more natural, socially appropriate, and fluent to team with, while being both more predictable and intent-clear. The ratings of our approach are equal or higher than legible motion across all 18 survey items.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 37932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37313
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 37374
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374827"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=06DgKuJIbNw",
          "title": "Planning with Partner Uncertainty Modeling for Efficient Information Revealing in Teamwork",
          "duration": 580,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38265,
      "typeId": 11597,
      "title": "Comparing the Effects of False Alarms and Misses on Humans’ Trust in (Semi)Autonomous Vehicles",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Trust in automated driving systems is crucial for effective driver-(semi)autonomous vehicles interaction. Drivers that do not trust the system appropriately are not able to leverage its benefits. This study presents a mixed design user experiment where participants conducted a non-driving task while traveling in a simulated semiautonomous vehicle with forward collision alarm and emergency braking functions. Occasionally, the system missed obstacles or provided false alarms. We varied these system error types as well as road shapes, and measured the effects of these variations on trust development. Results reveal that misses are more harmful to trust development than false alarms, and that these effects are strengthened by operation on risky roads. Our findings provide additional insight into the development of trust in automated driving systems, and are useful for the design of such technologies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "Robotics",
              "dsl": "University of Michigan"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan Robotics Institute",
              "dsl": "MAVRIC"
            }
          ],
          "personId": 37784
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Mechanical Engineering"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 37455
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan Robotics Institute",
              "dsl": "MAVRIC"
            }
          ],
          "personId": 37287
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 37970
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 37291
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            },
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 37532
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378371"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rOYLdaV5g2A",
          "title": "Comparing the Effects of False Alarms and Misses on Humans’ Trust in (Semi)Autonomous Vehicles",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38266,
      "typeId": 11597,
      "title": "Emotional Support Domestic Robots for Healthy Older Adults: Conversational Prototypes to Help With Loneliness",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "A burgeoning area of HRI explores how robots can be used to assist older adults who are dealing with health issues. However, much of this work focuses on aiding older adults that are living with dementia or other serious health-related problems. In this work, we focus on robots helping otherwise-healthy older adults living with social isolation and loneliness. We created an initial robot behavior design, which leverages techniques from therapy, to provide emotional support through basic conversational ability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "Lower Saxony",
              "city": "Osnabrück",
              "institution": "Osnabrück University",
              "dsl": ""
            }
          ],
          "personId": 37947
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37869
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37337
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37461
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378279"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38267,
      "typeId": 11597,
      "title": "Warning: This Robot is Not What it Seems! Exploring Expectation Discrepancy Resulting from Robot Design",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "People are starting to interact with robots in a range of everyday contexts including hospitals, shopping centers, and airports. When faced with a robot, people with little or no prior experience necessarily build expectations based on the robot’s superficial appearances and actions, mediated by any potential tangentially related experience (e.g., media depictions). However, the person’s constructed expectations (e.g., that a humanoid robot can hold a conversation) does not necessarily relate to actual robot capability, resulting in an expectation discrepancy. This can create disappointment, when the person notices the limited capability, or misplaced trust, if the person believes a robot is more capable than it is. In this paper we present an initial framework for describing and discussing expectation discrepancy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Karlsruhe",
              "institution": "Karlsruhe University of Applied Sciences",
              "dsl": "Department of Business Information Systems"
            }
          ],
          "personId": 37792
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37224
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 37461
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378280"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=lho3uPTe-GM",
          "title": "Warning: This Robot is Not What it Seems! Exploring Expectation Discrepancy Resulting from Robot Design",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38268,
      "typeId": 11597,
      "title": "A Long-term Study of Robot-Assisted Therapy for Children with Severe Autism and ADHD",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots are increasingly being used as a mediator between therapists and children with Autism Spectrum Disorder (ASD) and Attention Deficit Hyperactivity Disorder (ADHD). This paper describes an ongoing work that aims to target children with diverse forms of ASD that undergo long-term interventions with the robot. Additionally, this paper describes a novel behavior that was implemented to introduce and to practice a set of social behaviors used for greetings and non-verbal communications. We conducted a long-term study with a cohort of 15 children aged from 3 to 12 years old and this paper presents the preliminary results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Department of Robotics and Mechatronics"
            }
          ],
          "personId": 37483
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37196
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics/School of Engineering and Digital Sciences"
            },
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics/School of Engineering and Digital Sciences"
            }
          ],
          "personId": 37345
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics/School of Engineering and Digital Sciences"
            }
          ],
          "personId": 37694
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "Nur-Sultan",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics/School of Engineering and Digital Sciences"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378356"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=v5zs3jv__5s",
          "title": "A Long-term Study of Robot-Assisted Therapy for Children with Severe Autism and ADHD",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38269,
      "typeId": 11521,
      "title": "Examining Profiles for Robotic Risk Assessment: Does a Robot's Approach to Risk Affect User Trust?",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "As autonomous robots move towards ubiquity, the need for robots to make decisions under risk that are trustworthy becomes increasingly significant; both to aid acceptance and to fully utilise their autonomous capabilities. We propose that incorporating a human approach to risk assessment into a robot’s decision making process will increase user trust. This work investigates four robotic approaches to risk and, through a user study, explores the levels of trust placed in each. These approaches are: risk averse, risk seeking, risk neutral and a human approach to risk. Risk is artificially stimulated through performance-based compensation, in line with previous studies. The study was conducted in a virtual nuclear environment created using the Unity games engine. Forty participants were asked to complete a robot supervision task, in which they observed a robot making risk based decisions and were able to question the robot, question the robot further and ultimately accept or alter the robot’s decision. It is shown that a robot that is risk seeking is significantly less trusted than a risk averse robot, a risk neutral robot and a robot utilising human approach to risk. There was found to be no significant difference between the levels of trust placed in the risk averse, risk neutral and human approach to risk. It is also found that the level to which participants question a robot’s decisions does not form an accurate measure of trust. The results suggest that when designing a robot that must make risk based decisions during teleoperation in a hazardous environment, an engineer should avoid a risk seeking robot. However, that same engineer may choose whichever of the remaining risk profiles best suits the implementation, with knowledge that the trust in their system is unlikely to be significantly affected.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37703
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37884
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37836
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37640
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374804"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=lm_5MCSuJFY",
          "title": "Examining Profiles for Robotic Risk Assessment: Does a Robot's Approach to Risk Affect User Trust?",
          "duration": 565,
          "type": "video"
        }
      },
      "sessionIds": [
        38342
      ],
      "eventIds": []
    },
    {
      "id": 38270,
      "typeId": 11523,
      "title": "The Application of Social Power in Persuasive Social Robots",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "The  technology  of  the  future  will  bring  an  increasing  number of robots into our daily life. This has motivated a number of re-searchers to explore diverse factors to promote social interaction with robots. This PhD project aims at investigating social power dynamics in Human-Robot Interaction. Social power is defined as one’s ability to influence others to do something which they would not do otherwise. Different theories classify alternative ways to achieve social power, such as providing rewards, using coercion, or acting as an expert. After conceptualizing social power to allow implementation in social agents, we studied how those power strategies affect persuasion when using robots. Specifically, we attempted to design persuasive robots by creating persuasive strategies inspired from social power.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "DEIC"
            }
          ],
          "personId": 37440
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon - IST",
              "dsl": "GAIPS"
            }
          ],
          "personId": 37742
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": ""
            }
          ],
          "personId": 37352
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "Instituto Superior Técnico"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "GAIPS"
            }
          ],
          "personId": 37534
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37257
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 37945
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377447"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=jIYhnv7iN68",
          "title": "The Application of Social Power in Persuasive Social Robots",
          "duration": 183,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38271,
      "typeId": 11597,
      "title": "DeepTaxi: Teaching an Autonomous Car with Social Scaffolding",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present DeepTaxi, an extension to an existing autonomous RC car platform that allows for the dynamic learning of an environment. DeepTaxi employs a social scaffolding approach where a human user supervises and initially provides feedback to the car so that it can learn the names and order of various objects located around a track. Once it sufficiently learns about the environment, DeepTaxi can then autonomously navigate to any desired location without the need for human assistance. We test DeepTaxi with human participants on a custom made track with a varietyof objects/orders. We find that it can successfully learn about andnavigate the track with the participants expressing appreciation for the timeliness of the car’s communication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37194
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kansas",
              "city": "Lawrence",
              "institution": "The University of Kansas",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 37582
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378357"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38272,
      "typeId": 11597,
      "title": "The Unexpected Daily Situations (UDS) Dataset: A New Benchmark for Socially-Aware Assistive Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This article presents the progress in building a new dataset of 'unexpected daily situations' (like someone tripping on a box, while carrying a tray to the kitchen, or someone burning him/herself with hot water and dropping a mug). Each of the situations involve one or two humans in a familiar, structured environment (eg, a kitchen, a living room) with rich semantics. Correctly interpreting the situation (including recognising an error, undesired effect or incongruity when it occurs, as well as selecting the best repair action) requires beyond-state-of-art spatio-temporal, semantic and socio-cognitive modelling. As such, the aim of the dataset is to offer (i) a realistic source of data to train and test such novel algorithms and (ii) provide a new benchmark against which state-of-art algorithms can be demonstrated.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 37846
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37520
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37494
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Lab"
            }
          ],
          "personId": 37852
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS, Univ de Toulouse",
              "dsl": ""
            }
          ],
          "personId": 37181
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378270"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=FA0n2c24oCQ",
          "title": "The Unexpected Daily Situations (UDS) Dataset: A New Benchmark for Socially-Aware Assistive Robots",
          "duration": 126,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38273,
      "typeId": 11597,
      "title": "The Impact of Affective Verbal Expressions in Social Robots",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This research investigates whether there is an ethical concern in robots misrepresenting their internal state through speech. Participants were asked to discuss their food preferences with a robot, where the robot would either respond through facts or an implied personal stance. Results show that there are no significant differences in the way participants perceived the robot or accepted the interaction; nor that the interaction influenced their mood. This indicates that the use of personal opinion by a robot does not significantly impact participants' opinion of the robot and therefore may not necessarily be a concern in human-robot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37836
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": ""
            }
          ],
          "personId": 37721
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "SoftBank Robotics",
              "dsl": ""
            }
          ],
          "personId": 38018
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the Wesr of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37511
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": ""
            }
          ],
          "personId": 37739
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37215
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37465
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Faculty of Environment and Technology, UWE",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37410
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378358"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38274,
      "typeId": 11597,
      "title": "Abstract Visual Programming of Social Robots for Novice Users",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "To facilitate interaction of robots with people in public spaces it would be beneficial for them to use social behaviours: i.e. low-level behaviours that suggest the robot is a social agent. However, the implementation of such social behaviours would be difficult for novice users - i.e. non-roboticists. In this contribution, we present a high-level visual programming system that enables novices to design robot tasks which already incorporate social behavioural cues appropriate for the robot being programmed. A pilot study of this system in a museum involving members of the public designing guided tours demonstrated that the addition of the low-level social cues improve the perception of the robot and the effectiveness of the designed task behaviour. A number of areas of further exploration and development are highlighted.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lincolnshire",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37358
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37446
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37668
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 38003
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37889
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378271"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38275,
      "typeId": 11521,
      "title": "Design Patterns for an Interactive Storytelling Robot to Support Children's Engagement and Agency",
      "award": "BEST_PAPER",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we specify and validate three interaction design patterns for an interactive storytelling experience with an autonomous social robot. The patterns enable the child to make decisions about the story by talking with the robot, reenact parts of the story together with the robot, and recording self-made sound effects. The design patterns successfully support children’s engagement and agency.\r\n\r\nA user study (N = 27, 8-10 y.o.) showed that children paid more attention to the robot, enjoyed the storytelling experience more, and could recall more about the story, when the design patterns were employed by the robot during storytelling. All three aspects are\r\nimportant features of engagement. Children felt more autonomous during storytelling with the design patterns and highly appreciated that the design patterns allowed them to express themselves more freely. Both aspects are important features of children’s agency. Important lessons we have learned are that reducing points of confusion and giving the children more time to make themselves heard by the robot will improve the patterns efficiency to support engagement and agency. Allowing children to pick and choose from a diverse set of stories and interaction settings would make the storytelling experience more inclusive for a broader range of children.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": "Social AI"
            }
          ],
          "personId": 37354
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 37705
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU University",
              "dsl": ""
            }
          ],
          "personId": 37195
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374826"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=zh_07-9sCz4",
          "title": "Design Patterns for an Interactive Storytelling Robot to Support Children's Engagement and Agency",
          "duration": 612,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38276,
      "typeId": 11597,
      "title": "Improving Emotional Expression Recognition of Robots Using Regions of Interest from Human Data",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper is the first step of an attempt to equip social robots with emotion recognition capabilities comparable to those of humans. Most of the recent deep learning solutions for facial expression recognition under-perform when deployed in Human-Robot-Interaction scenarios, although they are capable of breaking records on the most varied benchmarks on facial expression recognition. The main reason for that we believe is that they are using techniques that are developed for recognition of static pictures, while in real-life scenarios, we infer emotions from intervals of expression. Utilizing on the feature of CNN to form regions of interests that are similar to human gaze patterns, we use recordings from human-gaze patterns to train such a network to infer facial emotions from 3 seconds video footage of humans expressing the 6 basic emotions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of technology",
              "dsl": "Social Robotics Lab, Industrial Design dept."
            }
          ],
          "personId": 37164
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Technical University of Eindhoven",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 37608
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378359"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38277,
      "typeId": 11523,
      "title": "Automatic Assessment and Learning of Robot Social Abilities",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37668
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 37889
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lincoln",
              "institution": "University of Lincoln",
              "dsl": "Lincoln Centre for Autonomous Systems"
            }
          ],
          "personId": 38003
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377430"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=ZjEO1s0leLQ",
          "title": "Automatic Assessment and Learning of Robot Social Abilities",
          "duration": 182,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38278,
      "typeId": 11597,
      "title": "Can a Social Robot Be Persuasive Without Losing Children’s Trust?",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots can be used to motivate children to engage in learning activities in education. In such contexts, they might need to persuade children to achieve specific learning goals. We conducted an exploratory study with 42 children in a museum setting. Children were asked to play an interactive storytelling game on a touchscreen. A Furhat robot guided them through the steps of creating the character of a story in two conditions. In one condition, the robot tried to influence children's choices using high-controlling language. In the other, the robot left children free to choose and used a low-controlling language. Participants in the persuasive condition generally followed the indications of the robot. Interestingly, the use of high-controlling language did not affect children's perceived trust towards the robot. We discuss the important implications that these results may have when designing children-robot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala Univesity",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 37709
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 37207
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 37442
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Embodied Social Agents Lab"
            }
          ],
          "personId": 37217
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Social Robotics Lab"
            }
          ],
          "personId": 37971
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378272"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CJA4Lo-YRdw",
          "title": "Can a Social Robot Be Persuasive Without Losing Children’s Trust?",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38279,
      "typeId": 11597,
      "title": "Generating Robotic Emotional Body Language of Targeted Valence and Arousal with Conditional Variational Autoencoders",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Non-verbal communication that encompasses emotional body language is a crucial aspect of social robotics applications. Deep learning models for the generation of robotic expressions of bodily affect gain more and more ground recently over the hand-coded methods. In this work, we present a Conditional Variational Autoencoder network that generates emotional body language animations of targeted valence and arousal for a Pepper robot, and we conduct a user study to evaluate the interpretability of the generated animations. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Plymouth",
              "institution": "University of Plymouth",
              "dsl": ""
            }
          ],
          "personId": 37533
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ANYbotics AG",
              "dsl": ""
            }
          ],
          "personId": 37397
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 37434
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378360"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=9ioncY-XxxE",
          "title": "Generating Robotic Emotional Body Language of Targeted Valence and Arousal with Conditional Variational Autoencoders",
          "duration": 126,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38280,
      "typeId": 11521,
      "title": "Vestibular Feedback on a Virtual Reality Wheelchair Driving Simulator: a Pilot Study",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Autonomy and ability to maintain social activities can be challenging for people with disabilities experiencing reduced mobility. \r\nIn the case of disabilities impacting mobility, Power Wheelchairs can help such people keeping or regain autonomy.\r\nNonetheless, driving a power wheelchair is a complex task that requires a combination of cognitive, visual and visuo-spatial abilities. In practice, people need to pass prior ability tests and driving training before being prescribed a power wheelchair by their therapist.\r\nStill, conventional training in occupational therapy can be insufficient for some people with severe cognitive and/or visuo-spatial functions. As such, these people are often prevented to obtain a power wheelchair prescription from their therapist for safety concerns.\r\nIn this context, driving simulators might be efficient and promising tools to provide alternative, adaptive, flexible, and safe training.\r\nIn previous work, we proposed a Virtual Reality (VR) driving simulator integrating a vestibular feedback to simulate wheelchair motion sensations.\r\nThe performance and acceptability of a VR simulator rely on satisfying user Quality of Experience (QoE). Therefore, our simulator is designed to provide high Sense of Presence (SoP) and low Cybersickness to the user.\r\nThis paper presents a pilot study assessing the impact of the provided vestibular feedback on user Quality of Experience (QoE). Participants were asked to perform a driving task while being in the simulator within 2 conditions: with vestibular feedback and without vestibular feedback. User QoE is assessed through subjective questionnaires measuring user SoP and cybersickness. Results show that vestibular feedback activation increases SoP and decreases cybersickness. This study constitutes a mandatory step before clinical trials and, as such, only enrolled people without disabilities",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37896
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37930
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37568
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37211
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37914
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA",
              "dsl": "Univ Rennes"
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Irisa-UMR6074",
              "dsl": "CNRS, Inria"
            }
          ],
          "personId": 37627
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374825"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=XY21JFHkLwY",
          "title": "Vestibular Feedback on a Virtual Reality Wheelchair Driving Simulator: a Pilot Study",
          "duration": 610,
          "type": "video"
        }
      },
      "sessionIds": [
        38345
      ],
      "eventIds": []
    },
    {
      "id": 38281,
      "typeId": 11597,
      "title": "Investigating the effectiveness of different interaction modalities for spatial human-robot interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing use of social robots in real environments, one of the areas of research requiring more attention is the study of human-robot interaction (HRI) when a person and robot are moving close to each other. Understanding effective ways to design how a robot should communicate its intention during dynamic movement is based on what people’s expectations are and how they interpret different cues from the robot. Building on the existing literature, we tested a range of non-verbal cues such as eye contact, gaze and head nodding as part of the robot’s behaviour during close proximate passing. The research aimed to investigate the effects of these cues, as well as their combination with body posture, on the efficiency of passing and the quality of HRI. Our results show that the combination of eye contact and the robot turning sideways is the most effective and appropriate compared to other modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37456
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 37836
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Faculty of Environment and Technology, UWE",
              "dsl": "Bristol Robotics Laboratory"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Faculty of Environment and Technology, UWE",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 37410
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378273"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38282,
      "typeId": 11597,
      "title": "Carrier-pigeon Robot: Promoting Interactions Among Older Adults in a Care Home",
      "award": "BEST_PAPER",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Communication among older adults in a care home is often reduced due to cognitive, communication, and mobility impairments. They tend to become isolated, which may lead to faster cognitive decline. We present an approach in which a robot is used as a communication vehicle between people located in adjacent rooms. To program the robot, we resorted to physical blocks with 3D icons. Older adults are able to create a sequence of messages that the robot delivers to another person or group. The blocks represented user-recorded voice messages, pre-recorded messages (e.g., proverbs), or actions (e.g., delivering cookies). A preliminary study with 22 older adults in a care home showed positive engagements between groups and an overall sense of excitement and fun. Carrier robots promise to extend the action range and operate as a communication tool, enabling interactions between people who may not be able to interact whenever they feel compelled to.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "Faculdade de Ciências",
              "dsl": "Lasige"
            }
          ],
          "personId": 37276
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências da Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 37727
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 37193
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": ""
            }
          ],
          "personId": 37921
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378361"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Zq4druvrvug",
          "title": "Carrier-pigeon Robot: Promoting Interactions Among Older Adults in a Care Home",
          "duration": 114,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38283,
      "typeId": 11597,
      "title": "The Naked Truth?: Clothing Does Not Influence the Robot Experience",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "When judging humans, (formal) clothes play a vital role for the attribution of trust, competence and sympathy. Most social robots, however, appear unclothed and not much is known whether and how clothes can influence how a robot is perceived. In an experiment, participants experienced either a formally dressed, an informally dressed or an undressed robot and rated their experience on different questionnaires. Inconsistent with our expectations, the data revealed no influence of robot clothing on the experience of the robot. Possible reasons and implications for further studies are discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-Universität",
              "dsl": ""
            }
          ],
          "personId": 37402
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-Universität",
              "dsl": ""
            }
          ],
          "personId": 37310
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378362"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=XK7mC9W5lOc",
          "title": "The Naked Truth?: Clothing Does Not Influence the Robot Experience",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38284,
      "typeId": 11597,
      "title": "Do Robots Distract us as much as Humans? The Effect of Human-like Appearance and Perceptual Load",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Attention is an important aspect for solving certain tasks, however our environment can distract us via irrelevant information. As robots increasingly become participants in our lives, one important question is whether they could distract us as much as humans, and if so to what extent. To address this question, we conducted a study in which subjects were engaged in a central letter detection task. The task irrelevant distractors were pictures of three agents; a mechanical robot, a human-like robot, and a real human. We also manipulated the perceptual load to investigate whether the demands of the task influence how much we are distracted by these agents. Our results show that people are distracted by robots as much as humans, as demonstrated by significant increase in reaction times and decrease in task accuracy in the presence of agent distractors as compared to the situation when there was no distractor. However, we found that the task difficulty interacted with the human-likeness of the distractor agent. When the task was less demanding, the agent that distracted most was the most human-like agent, whereas when the task was more demanding, the least human-like agent distracted the most. These results not only provide insights about how to design humanoid robots but also sets as a great example of a fruitful collaboration between human-robot interaction and cognitive sciences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology and Neuroscience"
            }
          ],
          "personId": 37912
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37173
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37371
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology"
            }
          ],
          "personId": 37757
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Userspots, Inc",
              "dsl": ""
            }
          ],
          "personId": 37966
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378274"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QFUoW0b6bWY",
          "title": "Do Robots Distract us as much as Humans? The Effect of Human-like Appearance and Perceptual Load",
          "duration": 109,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38285,
      "typeId": 11523,
      "title": "Eye Gaze for Assistive Manipulation",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "A key challenge of human-robot collaboration is to build systems that balance the usefulness of autonomous robot behaviors with the benefits of direct human control. This balance is especially relevant for assistive manipulation systems, which promise to help people with disabilities more easily control wheelchair-mounted robot arms to accomplish activities of daily living. To provide useful assistance, robots must understand the user’s goals and preferences for the task. Our insight is that systems can enhance this under-standing by monitoring the user’s natural eye gaze behavior, as psychology research has shown that eye gaze is responsive and relevant to the task. In this work, we show how using gaze enhances assistance algorithms. First, we analyze eye gaze behavior during teleoperated robot manipulation and compare it to literature results on by-hand manipulation. Then, we develop a pipeline for combining the raw eye gaze signal with the task context to build a rich signal for learning algorithms. Finally, we propose a novel use of eye gaze in which the robot avoids risky behavior by detecting when the user believes that the robot’s behavior has a problem.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 37314
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 38367
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377434"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=T2nN1mFTygM",
          "title": "Eye Gaze for Assistive Manipulation",
          "duration": 185,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38286,
      "typeId": 11521,
      "title": "Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "How should a robot that collaborates with multiple people decide upon the distribution of resources (e.g. social attention, or parts needed for an assembly)? People are uniquely attuned to how resources are distributed. A decision to distribute more resources to one team member than another might be perceived as unfair with potentially detrimental effects for trust. We introduce a multiarmed bandit algorithm with fairness constraints, where a robot distributes resources to human teammates of different skill levels. In this problem, the robot does not know the skill level of each human teammate, but learns it by observing their performance over time. We define fairness as a constraint on the minimum rate that each human teammate is selected throughout the task. We provide theoretical guarantees on performance and perform a large-scale user study, where we adjust the level of fairness in our algorithm. Results show that fairness in resource distribution has a significant effect on users’ trust in the system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 37811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37512
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37362
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37663
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 37726
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374806"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Rlq7esQGi10",
          "title": "Multi-Armed Bandits with Fairness Constraints for Distributing Resources to Human Teammates",
          "duration": 472,
          "type": "video"
        }
      },
      "sessionIds": [
        38351
      ],
      "eventIds": []
    },
    {
      "id": 38287,
      "typeId": 11597,
      "title": "Please Listen to Me: How to Make Passersby Stop by a Humanoid Robot in a Shopping Mall",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we investigated robot behaviors in a shopping mall that can make passersby stop in front of the robot. This is the first step to develop a social robot for advertising. The three types of robot motion: Greeting, Troubling, and Dancing were implemented into the robot. The result by 65000+ passersby shows that Troubling motion can make passersby stop more and stay longer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 37565
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "CyberAgent, Inc.",
              "dsl": ""
            }
          ],
          "personId": 37413
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "CyberAgent, Inc.",
              "dsl": ""
            }
          ],
          "personId": 37967
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 37722
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka Univ.",
              "dsl": ""
            }
          ],
          "personId": 37819
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Aichi",
              "institution": "Nagoya University",
              "dsl": ""
            }
          ],
          "personId": 37702
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 37800
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 37995
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378289"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=k7OQa5ux8e4",
          "title": "Please Listen to Me: How to Make Passersby Stop by a Humanoid Robot in a Shopping Mall",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38288,
      "typeId": 11521,
      "title": "Impact of Interaction Context on the Student Affect-Learning Relationship in Child-Robot Interaction",
      "award": "HONORABLE_MENTION",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Prior work in affect-aware educational robots has often relied on a common belief that the relationship between student affect and learning is independent of agent behaviors (child’s/robot’s) or unidirectional (positive/negative but not both) throughout the entire student-robot interaction. We argue that the student affect-learning relationship should be interpreted in two contexts: (1) social learning paradigm and (2) sub-events within child-robot interaction. In our paper, we examine two different social learning paradigms where children interact with a robot that acts either as a tutor or a tutee. Sub-events within child-robot interaction are defined as task-related events occurring in specific phases of an interaction (e.g., when the child/robot gets a wrong answer). We examine subevents at a macro level (entire interaction) and a micro level (within specific sub-events). In this paper, we provide an in-depth correlation analysis of children’s facial affect and vocabulary learning. We found that children’s affective displays became more predictive of their vocabulary learning when children interacted with a tutee robot who did not scaffold their learning. Additionally, children’s affect displayed during micro-level events was more predictive of their learning than during macro-level events. Last, we found that the affect-learning relationship is not unidirectional, but rather is modulated by context, i.e., several affective states facilitated student learning when displayed in some sub-events but inhibited learning when displayed in others. These findings indicate that both social learning paradigm and sub-events within interaction modulate student affect-learning relationship.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 37448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 37998
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 37660
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 37266
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374822"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=1V0d7gmL7fE",
          "title": "Impact of Interaction Context on the Student Affect-Learning Relationship in Child-Robot Interaction",
          "duration": 603,
          "type": "video"
        }
      },
      "sessionIds": [
        38352
      ],
      "eventIds": []
    },
    {
      "id": 38289,
      "typeId": 11597,
      "title": "Visuo-tactile mixed reality for offline cobot programming",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Manual guidance of a collaborative robot arm (i.e., moving the robot arm by hand) is a technique to program the robot without coding and in the context of the job. However, it requires the robot to be available and not in operation. As a workaround, we propose performing manual guidance on a hologram of that same robot, the feasibility of which we are investigating. A potential limitation of this approach is the lack of tangibility of the hologram, for which we are investigating the contribution that mid-air haptics (MAH) can make. Early results suggest beneficial effects from tactile feedback on AR pick-and-place.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Eibar",
              "institution": "Fundación Tekniker",
              "dsl": ""
            }
          ],
          "personId": 37290
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Eibar",
              "institution": "Fundación Tekniker",
              "dsl": ""
            }
          ],
          "personId": 37369
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378290"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=viQlLgeshAw",
          "title": "Visuo-tactile mixed reality for offline cobot programming",
          "duration": 97,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38290,
      "typeId": 11597,
      "title": "Social Robot for STEM Education",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This work explores the use of the Cozmo robot to deliver mathematics education. Recently, we see a lot of work presented on using robots for education but few works centred around the age group of children between 14-17 years of age in combination with a non-humanoid robot. Reflecting on this limitation, Cozmo autonomously delivered engaging material and exercises to learners. We studied the subjective ratings of young learners' knowledge gains with Cozmo on the topics of algebra, geometry and trigonometry. We found that participants' subjective rating on their knowledge changed significantly after the interaction. This implied a positive influence of employing Cozmo with this age group and also reflected on the need to do more research in schools with affordable non-humanoid, autonomous social robots.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37529
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 37359
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            }
          ],
          "personId": 37666
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378291"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38291,
      "typeId": 11597,
      "title": "When a Robot Violates Expectations: The Influence of Reward Valence and Expectancy Violation on People’s Evaluation of a Social Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In an experimental lab study with a 2x2 between-subjects-design (N = 162), the aim was to examine how a negative expectancy violation caused by a social robot and its reward valence, which represents how desirable it is to interact with this robot, affect the evaluation of the robot and the interaction with it. The negative expectancy violation led to less positive evaluations of the interaction with the robot as well as its sociability and competence. The robot with a high reward valence evoked a more positive evaluation of the interaction with it as well as its sociability. Furthermore, when the robot had a low reward valence, an expectancy violation led participants to increasingly rate the robot's behavior as deviating from what they expected. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Duisburg",
              "institution": "University of Duisburg-Essen",
              "dsl": "Social Psychology: Media and Communication"
            }
          ],
          "personId": 37870
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Duisburg",
              "institution": "Social Psychology - Media and Communication, Universität Duisburg-Essen",
              "dsl": ""
            }
          ],
          "personId": 37652
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378292"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=cTAK4QblfRg",
          "title": "When a Robot Violates Expectations: The Influence of Reward Valence and Expectancy Violation on People’s Evaluation of a Social Robot",
          "duration": 124,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38292,
      "typeId": 11521,
      "title": "Effects of a Social Robot’s Self-Explanations on How Humans Understand and Evaluate Its Behavior",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Social robots interacting with users in real-life environments will often show surprising or even undesirable behavior. In this paper we investigate whether a robot’s ability to self-explain its behavior affects the users’ perception and assessment of this behavior. We propose an explanation model based on humans’ folk-psychological concepts and test different explanation strategies in specifically designed HRI scenarios with robot behaviors perceived as intentional, but differently surprising or desirable. All types of explanation strategies increased the understandability and desirability of the behaviors. While merely stating an action had similar effects as giving a reason for it (an intention or need), combining both in a causal explanation helped the robot to better justify its behavior and to increase its understandability and desirability to a larger extent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Social Cognitive Systems Group"
            }
          ],
          "personId": 37958
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Social Cognitive Systems Group"
            }
          ],
          "personId": 37187
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374802"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4SGNsOFyMwg",
          "title": "Effects of a Social Robot's Self-Explanations on How Humans Understand and Evaluate Its Behavior",
          "duration": 617,
          "type": "video"
        }
      },
      "sessionIds": [
        38356
      ],
      "eventIds": []
    },
    {
      "id": 38293,
      "typeId": 11521,
      "title": "Plug-and-Play Gesture Control Using Muscle and Motion Sensors",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "As the capacity for machines to extend human capabilities continues to grow, the communication channels used must also expand.  Allowing machines to interpret nonverbal commands such as gestures can help make interactions more similar to interactions with another person.  Yet to be pervasive and effective in realistic scenarios, such interfaces should not require significant sensing infrastructure or per-user setup time.  The presented work takes a step towards these goals by using wearable muscle and motion sensors to detect gestures without dedicated calibration or training procedures.  An algorithm is presented for clustering unlabeled streaming data in real time, and it is applied to adaptively thresholding muscle and motion signals acquired via electromyography (EMG) and an inertial measurement unit (IMU).  This enables plug-and-play online detection of arm stiffening, fist clenching, rotation gestures, and forearm activation.  It also augments a neural network pipeline, trained only on strategically chosen training data from previous users, to detect left, right, up, and down gestures.  Together, these pipelines offer a plug-and-play gesture vocabulary suitable for remotely controlling a robot.  Experiments with 6 subjects evaluate classifier performance and interface efficacy. Classifiers correctly identified 97.6% of 1,200 cued gestures, and a drone correctly responded to 81.9% of 1,532 unstructured gestures as subjects remotely controlled it through target hoops during 123 minutes of total flight time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology (MIT)",
              "dsl": "Distributed Robotics Lab"
            }
          ],
          "personId": 37248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Distributed Robotics Lab"
            }
          ],
          "personId": 37265
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374823"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=2FaGsWoV2dA",
          "title": "Plug-and-Play Gesture Control Using Muscle and Motion Sensors",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        38349
      ],
      "eventIds": []
    },
    {
      "id": 38294,
      "typeId": 11597,
      "title": "Human Decisions for Robot Integration: Task Allocation in a Plan Based Building Assignment",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an experimental design to explore how inexperienced persons adapt their decision on task allocation in a construction task after their first experience with a collaborative robot. In the experiment, 12 participants had to decide, which part of the building plan should be carried out by the robot and which part should be carried out by themselves. Although they did not use a coherent strategy, the participants were able to adapt their decision to the robot. As a result, they have significantly reduced the total time of completion within four repetitions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Tu Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 37919
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Tu Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 37853
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378293"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38295,
      "typeId": 11523,
      "title": "Robots in Older People's Living Spaces: Designing for Trust in Situated Human-Robot Interaction",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "As robots are being designed to support older people in their living spaces, the use of robots in these contexts and particularly for AAL may come with issues of trust. While trust has gained increasing interest in HRI, we still have little understanding of trust when using robots in people’s living spaces and taking into account their social practices. Drawing on literature, a methodological contribution and various (long-term) studies with older people and technology, the aim of this thesis is to define design guidelines for trust in situated human-robot interaction in older people’s living spaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "HCI Group, Faculty of Informatics"
            }
          ],
          "personId": 37288
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377449"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=WsscrI9muaQ",
          "title": "Robots in Older People's Living Spaces: Designing for Trust in Situated Human-Robot Interaction",
          "duration": 145,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38296,
      "typeId": 11523,
      "title": "Improving Motor Coordination in HRI with Bio-Inspired Controllers",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Gestural communication is an important aspect of HRI in social, assistance and rehabilitation robotics. Indeed, social synchrony isa key component of interpersonal interactions which affects the interaction at a behavioral level, as well as at a social level. It is therefore paramount for the robot to be able to adapt to its interaction partner, at the risk of experiencing an awkward interaction.Bio-inspired controllers endowed with plasticity mechanisms can be employed in order to make these interactions as natural and enjoyable as possible. Integrating adaptive properties can lead to the emergence of motor coordination and hence to social synchrony. Anon-negligible aspect of the work consists in studying humans inHRI to understand human behavior better and design better inter-actions. On the long term, this could be quite useful for improved robot-assisted motor therapy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nancy",
              "institution": "Université de Lorraine",
              "dsl": "Loria"
            }
          ],
          "personId": 37298
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377439"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38297,
      "typeId": 11521,
      "title": "Teaching a robot tasks of arbitrary complexity via human feedback",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "This paper addresses the problem of training a robot to carry out temporal tasks of arbitrary complexity via evaluative human feedback that can be inaccurate. A key idea explored in our work is a kind of curriculum learning—training the robot to master simple tasks and then building up to more complex tasks. We show how a training procedure, using knowledge of the formal task representation, can decompose and train any task efficiently in the size of its representation. We further provide a set of experiments that support the claim that non-expert human trainers can decompose tasks in a way that is consistent with our theoretical results, with more than half of participants successfully training all of our experimental missions. We compared our algorithm with existing approaches and our experimental results suggest that our method outperforms alternatives, especially when feedback contains mistakes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 38012
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 37575
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "CoCoSci Lab"
            }
          ],
          "personId": 37843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37684
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374824"
        }
      },
      "sessionIds": [
        38357
      ],
      "eventIds": []
    },
    {
      "id": 38298,
      "typeId": 11521,
      "title": "Working with a social robot in school: A long-term real-world technology probe",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Interactive learning technologies, such as robots, increasingly find their way into schools. However, more research is needed to see how children might work with such systems in the future. This paper presents the unsupervised, four month deployment of a Robot-Extended Computer Assisted Learning (RECAL) system with 61 children working in their own classroom. Using automatically collected quantitative data we discuss how their usage patterns and self-regulated learning process developed throughout the study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Overijssel",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37379
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Faculty of Behavioural, Management and Social Sciences, ELAN department"
            }
          ],
          "personId": 37659
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussels",
              "institution": "European Commission",
              "dsl": "Centre for Advanced Studies"
            }
          ],
          "personId": 37858
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Faculty of Behavioural, Management and Social Sciences, ELAN department"
            }
          ],
          "personId": 37626
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37840
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human Media Interaction"
            }
          ],
          "personId": 37549
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374803"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=gXHJbzmOhNk",
          "title": "Working with a Social Robot in School: A Long-Term Real-World Unsupervised Deployment",
          "duration": 504,
          "type": "video"
        }
      },
      "sessionIds": [
        38343
      ],
      "eventIds": []
    },
    {
      "id": 38299,
      "typeId": 11597,
      "title": "Influence of Anxiety toward Robots on the Appearance Tendency of Uncanny Valley",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we analyzed the influence of an individual’s anxiety level toward robots on the appearance tendency of the uncanny valley. We conducted a series of questionnaire surveys via crowdsourcing to evaluate mechano-humanness (MH) score, likability, and uncanniness of 80 robot face images. Thereafter, we divided the participants into two groups according to their scores of the anxiety toward robot scale (RAS). The results of the t-test of the approximate curves using the MH scores, likability scores, and uncanniness scores showed that the appearance tendency of the uncanny valley is affected by users’ scores in the RAS. The individuals who are less anxious toward robots exhibited higher likability/lower uncanniness toward the 80 robot faces, whereas those with high anxiety exhibited lower likability/higher uncanniness toward the same faces, and the uncanny valley of the latter is deeper than that of the former.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka Institute of Technology",
              "dsl": "Department of Information Science and Technology"
            }
          ],
          "personId": 37566
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka Institute of Technology",
              "dsl": "Department of Information Science and Technology"
            }
          ],
          "personId": 37384
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378281"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=2shQTovc0OY",
          "title": "Influence of Anxiety toward Robots on the Appearance Tendency of Uncanny Valley",
          "duration": 112,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38300,
      "typeId": 11597,
      "title": "Generation and Evaluation of Audio-Visual Anger Emotional Expression for Android Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Recent studies in human-human interaction (HHI) have revealed the propensity of negative emotional expression to initiate affiliating functions that are beneficial to the expresser and also help to foster cordiality and closeness amongst interlocutors. However, efforts in human-robot interaction (HRI) have not attempted to investigate the consequences of expression of negative emotion by robots on HRI. Thus, the background of this study as a first step is to furnish humanoid robots with natural audio-visual anger expression for HRI. Based on the analysis results from a multimodal HHI corpus, we implemented different types of gestures related to anger expressions for humanoid robots and carried-out subjective evaluation of the generated anger expressions. Findings from this study revealed that the semantic context and functional content of anger-based utterances play a significant role in the choice of gesture to accompany such utterance. Our current result shows that the \"Pointing\" gesture is adjudged more appropriate for utterances with \"you\" and anger-based \"questioning\" utterances; while \"both arms spread\" and \"both arms swing\" gestures were evaluated more appropriated for \"declarative\" and `\"disagreement\" utterances respectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": " Toyonaka",
              "institution": "Osaka University",
              "dsl": "Department of Systems Innovation"
            },
            {
              "country": "Japan",
              "state": "KYOTO",
              "city": "SORAKU-GUN",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": "HIL"
            }
          ],
          "personId": 37269
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Keihanna",
              "institution": "ATR",
              "dsl": "Hiroshi Ishiguro Laboratories"
            }
          ],
          "personId": 37994
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Advanced Telecommunications Research Institute",
              "dsl": "HIL"
            }
          ],
          "personId": 37388
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nara",
              "institution": "Advanced Telecommunications Research Institute",
              "dsl": "HIL"
            }
          ],
          "personId": 37237
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 37995
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378282"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=DKIVzuoWzG8",
          "title": "Generation and Evaluation of Audio-Visual Anger Emotional Expression for Android Robot",
          "duration": 111,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38301,
      "typeId": 11597,
      "title": "SŌTO: An Android Platform with a Masculine Appearance for Social Touch Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we report an android platform with a masculine appearance. In the human-human interaction research field, several studies reported the effects of gender in the social touch context. However, in the human-robot interaction research field, gender effects are mainly focused on human genders, i.e., a robot’s perceived gender is less focused. The purpose of developing the android is to investigate gender effects in social touch in the context of the human-robot interaction, comparing to existing android platforms with feminine appearances. For this purpose, we prepared a nonexistent face design in order to avoid appearance effects and fabric-based capacitance type upper-body touch sensors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "ATR",
              "dsl": ""
            }
          ],
          "personId": 38016
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Soraku-gun",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": "Hiroshi Ishiguro Laboratories"
            }
          ],
          "personId": 37370
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Keihanna Science City",
              "institution": "ATR",
              "dsl": "Hiroshi Ishiguro Laboratory"
            }
          ],
          "personId": 37698
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Keihanna Science City",
              "institution": "ATR",
              "dsl": "Hiroshi Ishiguro Laboratories"
            }
          ],
          "personId": 37537
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Advnced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 37361
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378283"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=EovnmnwVH2A",
          "title": "SŌTO: An Android Platform with a Masculine Appearance for Social Touch Interaction",
          "duration": 97,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38302,
      "typeId": 11597,
      "title": "Exploring the difference between Solving and Teaching in Sensorimotor Tasks",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Enhancing robots with the capability to learn from humans in an intuitive manner is important to deploy them in everyday life. Considering a human is not only solving a task, but actively teaching how to solve it to a robot has not been extensively explored and is an important step to improve LfD algorithms. We explored the difference between solving and teaching in a sensorimotor task. In a first experiment participants first solved a continuous maze task and gave demonstrations for a robot afterwards. While teaching the participants could give negative demonstrations (how not to solve the task). In a second experiment we asked new participants to rate how informative they perceive the demonstrations from the first experiment. The results show that significantly more demonstrations from the Teaching-phase are perceived as informative than from the Solving-phase. Furthermore, significantly more negative than positive demonstrations were perceived as informative.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Île-de-France",
              "city": "Paris",
              "institution": "Sorbonne Université",
              "dsl": "Institut des Systèmes Intelligents et de Robotique"
            }
          ],
          "personId": 37715
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, Université Pierre et Marie Curie",
              "dsl": "Institut des Systèmes Intelligents et de Robotique (ISIR)"
            }
          ],
          "personId": 37907
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378284"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5mM87naPmrY",
          "title": "Exploring the difference between Solving and Teaching in Sensorimotor Tasks",
          "duration": 125,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38303,
      "typeId": 11521,
      "title": "Human Perceptions of a Curious Robot that Performs Off-Task Actions",
      "trackId": 10951,
      "tags": [],
      "keywords": [],
      "abstract": "Researchers have proposed models of curiosity as a means to drive robots to learn and adapt to their environments. While these models balance goal- and exploration-oriented actions in a mathematically principled manor, it is not understood how users perceive a robot that pursues off-task actions. Motivated by a model of curiosity based on intrinsic rewards, we conducted three online video-surveys with a total of 264 participants, evaluating a variety of curious behaviors. Our results indicate that a robot’s off-task actions are perceived as expressions of curiosity, but that these actions lead to a negative impact on perceptions of the robot’s competence. When the robot explains or acknowledges its deviation from the primary task, this can partially mitigate the negative effects of off-task actions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 37641
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "SANTA CRUZ",
              "institution": "University of California, Santa Cruz",
              "dsl": "Computational Media"
            }
          ],
          "personId": 37201
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Seattle Academy",
              "dsl": ""
            }
          ],
          "personId": 37356
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": "Computational Media"
            }
          ],
          "personId": 37816
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 37944
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374821"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=1j91ISstdH8",
          "title": "Human Perceptions of a Curious Robot that Performs Off-Task Actions",
          "duration": 555,
          "type": "video"
        }
      },
      "sessionIds": [
        38353
      ],
      "eventIds": []
    },
    {
      "id": 38304,
      "typeId": 11597,
      "title": "Hearing a nose? User Expectations of Robot Appearance Induced by Different Robot Voices",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "Congruence between the visual appearance of a robot and its behavioral and communicative characteristics has been shown to be a crucial determinant of user acceptance. Given the growing popularity of speech interfaces, a coherent design of a robot’s looks and its voice is becoming more important. Which robot voice fits which appearance, however, has hardly been investigated to date. This is where the present research comes in. A randomized lab experiment was conducted, in which 165 participants listened to one of five more or less humanlike robot voices and subsequently drew a sketch corresponding to their imagination of the robot. The sketches were analyzed regarding the presence of various body features. While some features appeared in almost all drawings regardless of the condition (e.g., head, eyes), other features were significantly more prevalent in voice conditions characterized by low human-likeness (wheels) or high human-likeness (e.g., nose). Our results give first hints on which embodiment users might expect from different robotic voices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University Linz",
              "dsl": "LIT Robopsychology Lab"
            }
          ],
          "personId": 37881
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": "LIT Robopsychology Lab"
            }
          ],
          "personId": 37833
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University Linz",
              "dsl": "LIT Robopsychology Lab"
            }
          ],
          "personId": 37578
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378285"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38305,
      "typeId": 11597,
      "title": "Learning, Generating and Adapting Wave Gestures for Expressive Human-Robot Interaction",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This study proposes a novel imitation learning approach for the stochastic generation of human-like rhythmic wave gestures and their modulation for effective non-verbal communication through a probabilistic formulation using joint angle data from human demonstrations. This is achieved by learning and modulating the overall expression characteristics of the gesture (e.g., arm posture, waving frequency and amplitude) in the frequency domain. The method was evaluated on simulated robot experiments involving a robot with a manipulator of 6 degrees of freedom. The results show that the method provides efficient encoding and modulation of rhythmic movements and ensures variability in their execution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "MINES ParisTech",
              "dsl": ""
            }
          ],
          "personId": 37653
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Offenbach am Main",
              "institution": "Honda Research Institute Europe",
              "dsl": ""
            }
          ],
          "personId": 37977
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institute",
              "dsl": ""
            }
          ],
          "personId": 38008
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378286"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=e07EmUHLbC4",
          "title": "Learning, Generating and Adapting Wave Gestures for Expressive Human-Robot Interaction",
          "duration": 120,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38306,
      "typeId": 11523,
      "title": "Changing Perspective as A Learning Mechanism",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "One of the numerous approaches that increases the interaction quality between two people is having a proper understanding of the other person’s perspective. In this doctoral thesis, we aim to under-stand children’s perspective taking behavior, create a perspective taking framework for social robots, and evaluate the framework in educational scenarios and real-life interactions. The research started by designing tasks that allow us to analyze and decompose children’s decision-making mechanisms in terms of their perspective taking choices. We collect data from series of studies that capture the dynamic between the child and the robot using different perspective taking tasks and develop a complementary adaptive model for the robot. This article summarizes the perspective taking tasks, experimental studies, and future works for developing a comprehensive model of perspective taking for social robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "IST",
              "dsl": "GAIPS, INESC-ID"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37749
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37736
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "GAIPS"
            }
          ],
          "personId": 37534
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377442"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Lgz9yHYhZs0",
          "title": "Changing Perspective as A Learning Mechanism",
          "duration": 186,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38307,
      "typeId": 11523,
      "title": "\"I'm Not in the Mode to Help\": Interface Design for Robots Operating at Varying Levels of Autonomy",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "Devices with multiple modes increase user stress, workload, and error rate. Robots that  operate at multiple levels of autonomy offer a wide range of functions, but may increase error.  In this work, we develop and test multimodal interfaces to communicate a robot’s current level of automation to a user who is engaged in a high-workload task.  Future studies will apply these findings to multiple-human, multiple-robot teams.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Intergroup Human Robot Interaction Lab"
            }
          ],
          "personId": 37186
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Intergroup Human Robot Interaction Lab"
            }
          ],
          "personId": 37678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 37304
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Human Robot Interaction Lab"
            }
          ],
          "personId": 37588
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 37573
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Moffet Field",
              "institution": "NASA Ames Research Center",
              "dsl": ""
            }
          ],
          "personId": 37262
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377450"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4WnvQt8Ns7I",
          "title": "\"I'm Not in the Mode to Help\": Interface Design for Robots Operating at Varying Levels of Autonomy",
          "duration": 186,
          "type": "video"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38308,
      "typeId": 11523,
      "title": "Preventing Robot Abuse through Emotional Robot Responses",
      "trackId": 10950,
      "tags": [],
      "keywords": [],
      "abstract": "My research concerns group influence and prosocial behavior in a Human-Robot Interaction (HRI) context. My collaborators and I created and ran an experiment (N=30) to measure if the emotional responses of a group of robots could induce participants to take prosocial action against robot abuse. Participants completed a collaborative block-building task with a confederate, during which the confederate abused one robot after it made mistakes. We mea-sured participants’ responses to these events. The results of the study indicate that humans are more likely to prosocially intervene when the bystander robots react in sadness as opposed to when they ignore the abuse. They motivate further research on social influence and group dynamics within HRI",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 37502
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3377433"
        }
      },
      "sessionIds": [
        38360
      ],
      "eventIds": []
    },
    {
      "id": 38309,
      "typeId": 11597,
      "title": "A Software System for Human-Robot Interaction To Collect Research Data: A HTML/Javascript Service on the Pepper Robot",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "A key area in human-robot interaction research is the use of a robot to collect participant research data. However, traditional website-based data collection methods do not intergrate with the robo providing the interaction. This leaves a clear disconnect between the static delivery of a digital questionnaire and a lack context-relevant behaviours from the robot. In this paper, we present a HTML/Javascript software system to create a direct link between digital data collection and the robot that delivers it. In doing so, this system can be used to create more dynamic data collection sessions using the robot’s speech, movement or presence to support the delivery of questionnaires. This system can also be used to create interactive sessions to support experimentation. We present two proposed use-case scenarios built with the system with the Pepper humanoid robot in mental health and well-being services. We present this software system to help speed up development time foruser studies, lower the entry barrier for non-technical researchers who want to use social robots for data collection, and to create more systematic data collection methods for robots. Future work of the software includes increasing the repertoire of questionnaire items available to allow for more sophisticated data collection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Queensland",
              "city": "Brisbane",
              "institution": "Australian Centre for Robotic Vision",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "Queensland",
              "city": "Brisbane",
              "institution": "Australian Centre for Robotic Vision",
              "dsl": ""
            }
          ],
          "personId": 37320
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Queensland",
              "city": "Brisbane",
              "institution": "Australian Centre for Robotic Vision",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "Queensland",
              "city": "Brisbane",
              "institution": "Australian Centre for Robotic Vision",
              "dsl": ""
            }
          ],
          "personId": 37381
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378287"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=HeMTNHzwUxU",
          "title": "A Software System for Human-Robot Interaction To Collect Research Data: A HTML/Javascript Service on the Pepper Robot",
          "duration": 122,
          "type": "video"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38310,
      "typeId": 11597,
      "title": "On the Importance of Posture and the Interaction Environment: Exploring Agency, Animacy and Presence in the Lab vs Wild using Mixed-Methods ",
      "trackId": 10952,
      "tags": [],
      "keywords": [],
      "abstract": "This work explores three concepts relevant to the study of human-robot interaction: posture, setting and evaluation methods. The first concept is the importance of a robot’s posture on its perceived interaction affordances. Early findings suggest that the same robot presented in different postural arrangements may significantly impact the way the interaction is perceived. Second, there is growing evidence to suggest the importance of situating interaction studies in-the-wild. We observed that the environment an interaction is situated in strongly affects the outcome, an indication that experiments constrained to the laboratory may not reveal useful social aspects relevant to understanding HRI fully. Finally, in order to conduct in-the-wild studies, we argue that current practice of using single-strand methods may not be sufficient; we instead explore a mixed-methods approach to study the complex social and environmental interplay between the robot, the participant and the bystanders.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 37901
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 37191
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "- OUTSIDE U.S.A. -",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Communication and Psychology"
            }
          ],
          "personId": 37450
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Bruce ",
              "institution": "University of Canberra",
              "dsl": "Human Centred Technology Research Centre"
            }
          ],
          "personId": 37447
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378288"
        }
      },
      "sessionIds": [
        38362
      ],
      "eventIds": []
    },
    {
      "id": 38329,
      "typeId": 11598,
      "title": "Kinetic AR: Robotic Motion Planning and Programming using Augmented Reality Interfaces",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "Using Augmented Reality (AR) interfaces, motion paths and tasks for co-bots in the factory can become visible and interactive. We present Kinetic AR, a system to control and manipulate motion of an MIR100 Automated Guided Vehicle (AGV) in Augmented Reality using the Reality Editor platform. The MIR100 robot performs a mapping of the environment using laser scanners. We synchro- nize the coordinate systems recognized by the smartphone and the AGV by performing spatial mapping. This allows for a seam- less interaction where the user can control the motion of the AGV in an intuitive and spatial manner without any further technical requirements than a mobile phone.\r\nThe user can perform path planning and visualize the motion of the AGV in real-time in AR. The synchronization of both environ- ments allows for a usable manipulation where the AGV is aware of the position of the phone at all times and can perform actions such as following the user or moving towards the position where the phone is pointing on the floor. Moreover, motion checkpoints can be actionable and visually connected to other equipment in order to program the coordinated behavior of multiple systems. The platform is spatially aware and allows for a co-located seamless interaction between machines. We envision this technology as a usable interface for the creation and visualization of manifold AGV operations while maintaining a low entry barrier to complex spatial hardware programming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "PTC",
              "dsl": "Reality Lab"
            }
          ],
          "personId": 38317
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "PTC",
              "dsl": "Reality Lab"
            }
          ],
          "personId": 38314
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "PTC",
              "dsl": "Reality Lab"
            }
          ],
          "personId": 38312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "PTC",
              "dsl": "Reality Lab"
            }
          ],
          "personId": 38320
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "PTC",
              "dsl": "Reality Lab"
            }
          ],
          "personId": 38327
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378394"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38330,
      "typeId": 11598,
      "title": "Learning Traffic Rules with a Social Robot in Pakistan",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "This work demonstrated shows the use of the Cozmo robot as a tool to engage and teach about traffic rules to younger children in a fun and interactive manner in Pakistan. This was the first effort of its kind to encourage traffic rules learning in children through using the social robot in the schools of Pakistan. Our ongoing work is towards understanding the social impact of this effort, mainly, finding whether children post-learning question parents about the traffic violation. We achieve this by creating a curriculum for social robots to teach about traffic rules at schools in Pakistan.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematics and Computer Science"
            }
          ],
          "personId": 37529
        },
        {
          "affiliations": [
            {
              "country": "Pakistan",
              "state": "",
              "city": "Lahore",
              "institution": "Lums",
              "dsl": ""
            }
          ],
          "personId": 38313
        },
        {
          "affiliations": [
            {
              "country": "Pakistan",
              "state": "",
              "city": "Lahore",
              "institution": "Lums",
              "dsl": ""
            }
          ],
          "personId": 38319
        },
        {
          "affiliations": [
            {
              "country": "Pakistan",
              "state": "",
              "city": "Lahore",
              "institution": "Lums",
              "dsl": ""
            }
          ],
          "personId": 38326
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378400"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38331,
      "typeId": 11598,
      "title": "YOLO - Your Own Living Object",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "Creativity is at the core of what it means to be human. It is an intrinsic ability that we all have and influences our well-being self-expression throughout life. However, a decline in creativity abilities occurs in children around the age of 7 years old. Our work aims to contribute to a re-balance of creative levels using social robots. In this video, we describe YOLO, an autonomous robotic toy for children that fosters their creativity during play. This robot is envisioned to be used as a character during storytelling, promoting creative story-lines that might not emerge otherwise.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Universitário de Lisboa (ISCTE-IUL), CIS-IUL",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": ""
            }
          ],
          "personId": 37962
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "ISCTE-IUL",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "CIS-IUL",
              "dsl": ""
            }
          ],
          "personId": 37894
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "Instituto Superior Técnico"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "INESC-ID",
              "dsl": "GAIPS"
            }
          ],
          "personId": 37534
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 37987
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378395"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38332,
      "typeId": 11598,
      "title": "Semi-Ethnographic Study on Human Responses to a Help-Seeker Robot",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "This video presents how people responded to a robot asking for help at six cafes at the Oregon State University campus. Each cafe was visited twice over eight weeks between August and September 2019, always around lunchtime for a two-hour period. Many participants expressed their delight at the presence of the robot, as seen in their help and care behaviors, and communications with each other. The wizarded mobile robot, called a ChairBot, had a whiteboard indicating its current ordering request, as well as a money clip for payment. We conducted fly-on-the-wall observations, participant interviews, and grounded coding to understand why and how people helped the robot. People helped the robot because: (1) they were curious, (2) they wanted to help the people behind the robot, and (3) they wanted to be perceived as ethical. The video shows these interactions in context, with diverse human-robot communication strategies and unexpected emergent behaviors that illustrate the value of in-the-wild studies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 38322
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of Language, Culture, and Society"
            }
          ],
          "personId": 38324
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvalis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 38325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 37275
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378401"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38333,
      "typeId": 11598,
      "title": "Guiding task through route description in the MuMMER project",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "The EU-funded MuMMER [1] project (http://mummer-project.eu/) has developed a socially intelligent robot to interact with the general public in open spaces. One of the core tasks for the robot is to guide the visitors to specific locations in the mall. The primary MuMMER deployment location is Ideapark, a large shopping mall in Lempäälä, Finland. The MuMMER robot system has been taken to the shopping mall several times for short-term co-design activities with the mall customers and retailers [2]; the full robot system has been deployed for short periods in the mall in September 2018, May 2019, and June 2019, and has been installed for a long-term, three-month deployment as of September 2019.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS",
              "dsl": ""
            }
          ],
          "personId": 38311
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 38321
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 38328
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 37846
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ENAC",
              "dsl": ""
            }
          ],
          "personId": 38315
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "CNRS",
              "dsl": "LAAS-CNRS"
            }
          ],
          "personId": 38316
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": "RIS"
            }
          ],
          "personId": 38323
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS, Univ de Toulouse",
              "dsl": ""
            }
          ],
          "personId": 38318
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378398"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38334,
      "typeId": 11598,
      "title": "CoWriting Kazakh: Learning a New Script with a Robot - Video",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "This is an explanatory video that gives a short overview of the CoWriting Kazakh project. The video demonstrates the CoWriting Kazakh activity as well as describes some of the results obtained in the experiment that will be presented at HRI 2020",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37295
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37710
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37463
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37747
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "NUR-SULTAN",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37405
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 37815
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37595
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": "CHILI"
            }
          ],
          "personId": 37736
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 37882
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378402"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38335,
      "typeId": 11598,
      "title": "Jon the Robot Goes Hollywood",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "Stand-up comedy performance is one interesting environment in which to evaluate social robot abilities; the natural format and expectations of the art form make it well suited for experimentation. We brought Jon the Robot, our autonomous robotic stand-up comedian, to over 20 performances in Los Angeles and recorded video footage from some of these performances. Our video is an entertaining compilation of this footage in a mock movie trailer format, including light details about the capabilities of the system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "No affiliation",
              "dsl": ""
            }
          ],
          "personId": 37615
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 37743
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378397"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38336,
      "typeId": 11598,
      "title": "Defense Against the Dark Cars: How People Grief Autonomous Vehicles",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "As autonomous vehicles (AVs) become a reality on public roads, researchers and designers are beginning to see unexpected behav- iors from the public. Ranging from curiosity to vandalism, these behaviors are concerning as AV platforms will need to know how to deal with people behaving unexpectedly or aggressively.\r\nWe call these antagonistic behaviors griefing of AVs, adopting the term from online gaming, which Warner and Raiter define as “Intentional harassment of other players...which utilizes aspects of the game structure or physics in unintended ways to cause distress” [4, p. 47]. We used the term griefing (rather than bullying), as not all behavior was intended to be violent or demeaning. However, any behavior that delays an AV’s journey could be problematic for\r\nAV developers and consumers.\r\nWe observed ten griefing instances over four years and five stud-\r\nies of pedestrian-AV behavior in three countries. For each study, we modified a conventional vehicle to appear autonomous through fake LiDAR and decals saying “Driverless Vehicle”. The driver hid beneath a costume that looked like a car seat, allowing them to remain in control of the vehicle at all times while the vehicle ap- peared fully autonomous from the outside. Pedestrians were gener- ally convinced of the illusion, as confirmed through interviews with consenting pedestrians and video recordings of all interactions. Full detail on the study, as well as proposed design principles to counter this behavior, will be published at HRI 2020 as a full paper [2].\r\nThese observations build on accounts of bullying towards robots that have been previously reported in the HRI community (e.g. [1, 3]). While AV developers such as Uber and Waymo have shared anecdotes of past vandalism, we believe this to be the first public video made available that captures the range of griefing from playful to aggressive.\r\nWe hope this video1 stimulates conversation regarding appro- priate design principles to counter griefing towards AVs. Several researchers study motivations behind this behavior, and it remains unclear how long it will take for it to naturally subside. In the meantime, AVs should be designed with this behavior in mind.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center for Design Research"
            }
          ],
          "personId": 37774
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Center For Design Research"
            }
          ],
          "personId": 37312
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford Archaeology Center"
            }
          ],
          "personId": 37192
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 37827
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378396"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38337,
      "typeId": 11598,
      "title": "SONŌ",
      "trackId": 10948,
      "tags": [],
      "keywords": [],
      "abstract": "In the course of the past ten years, soft robotics has become a growing field of research. This video presents SONŌ, a soft robot that features real-time sound generation based on FM synthesis in accordance with its movements. The system was constructed to explore how sound might augment soft robotics technology and potentially facilitate more engaging interactions with soft robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "SDU Biorobotics"
            }
          ],
          "personId": 37277
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "SDU Biorobotics"
            }
          ],
          "personId": 37510
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3371382.3378399"
        }
      },
      "sessionIds": [
        38373
      ],
      "eventIds": []
    },
    {
      "id": 38377,
      "typeId": 11519,
      "title": "General Chairs' Welcome",
      "trackId": 10965,
      "tags": [],
      "keywords": [],
      "abstract": "Welcome message from the general chairs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 38382
        },
        {
          "affiliations": [],
          "personId": 38383
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": ""
        },
        "Plenary": {
          "url": "https://www.youtube.com/watch?v=Fkg3YvA5n5o",
          "title": "General Chair's welcome",
          "duration": 223,
          "type": "video"
        }
      },
      "sessionIds": [
        38380
      ],
      "eventIds": []
    },
    {
      "id": 38386,
      "typeId": 11522,
      "title": "Embodied Affect for Real-World Human-Robot Interaction",
      "trackId": 10966,
      "tags": [],
      "keywords": [],
      "abstract": "The potential that robots offer to support humans in multiple aspects of our daily lives is increasingly acknowledged. Despite the clear progress in social robotics and human-robot interaction, the actual realization of this potential still faces numerous scientific and technical challenges, many of them linked to difficulties in dealing with the complexity of the real world. Achieving real-world human-robot interaction requires, on the one hand, taking into account and addressing real-world (e.g., stakeholder’s) needs and application areas and, on the other hand, making our robots operational in the real world. In this talk, I will address some of the contributions that Embodied Artificial Intelligence can make towards this goal, illustrating my arguments with examples of my and my group’s research on HRI using embodied autonomous affective robots in areas such as developmental robotics, healthcare, and computational psychiatry. So far little explored in HRI, Embodied AI, which started as an alternative to “symbolic AI” (a “paradigm change”) in the way to conceive and model the notion of “intelligence” and the interactions of embodied agents with the real world, is highly relevant towards achieving “real-world HRI”, with its emphasis on notions such as autonomy, adaptation, interaction with dynamic environments, sensorimotor loops and coordination, learning from interactions, and more generally, as Rodney Brooks put it, using and exploiting the real world as “its own best model”.",
      "authors": [
        {
          "affiliations": [],
          "personId": 38384
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374843"
        }
      },
      "sessionIds": [
        38348
      ],
      "eventIds": []
    },
    {
      "id": 38387,
      "typeId": 11522,
      "title": "Are We Trusting AI Too Much? Examining Human-Robot Interactions in the Real World",
      "trackId": 10966,
      "tags": [],
      "keywords": [],
      "abstract": "Intelligent systems, especially those with an embodied construct, are becoming pervasive in our society. From chatbots to rehabilitation robotics, from shopping agents to robot tutors, people are adopting these systems into their daily life activities. Alas, associated with this increased acceptance is a concern with the ethical ramifications as we start becoming more dependent on these devices [1]. Studies, including our own, suggest that people tend to trust, in some cases overtrusting, the decisionmaking capabilities of these systems [2]. For high-risk activities, such as in healthcare, when human judgment should still have priority at times, this propensity to overtrust becomes troubling [3]. Methods should thus be designed to examine when overtrust can occur, modelling the behavior for future scenarios and, if possible, introduce system behaviors in order to mitigate. In this talk, we will discuss a number of human-robot interaction studies conducted where we examined this phenomenon of overtrust, including healthcare-related scenarios with vulnerable populations, specifically children with disabilities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 38385
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374842"
        },
        "Plenary": {
          "url": "https://www.youtube.com/watch?v=SLkWSGJ93ug",
          "title": "Are We Trusting AI Too Much? Examining Human-Robot Interactions in the Real World",
          "duration": 1762,
          "type": "video"
        }
      },
      "sessionIds": [
        38341
      ],
      "eventIds": []
    },
    {
      "id": 38388,
      "typeId": 11522,
      "title": "Community, Art and the Vernacular in Technological Ecosystems",
      "trackId": 10966,
      "tags": [],
      "keywords": [],
      "abstract": "Community, craft, and the vernacular in artificially intelligent systems take the position that everyone participating in society is an expert in our experiences within the community infrastructures, which inform the makeup of robotic entities. Though we may not be familiar with the jargon used in specialized professional contexts, we share the vernacular of who we are as people and communities and the intimate sense that we are being learned. We understand that our data and collaboration is valuable, and our ability to successfully cooperate with the robotic systems proliferating around is well served by the creation of qualitatively informed systems that understand and perhaps even share the aims and values of the humans they work with. Using her art practice, which interrogates a humanoid robot and seeks to create culturally specific voice interactive entities as a case in point, Dinkins examines how interactions between humans and robots are reshaping human-robot and human-human relationships and interactions. She ponders these ideas through the lens of race, gender, and aging. She argues communities on the margins of tech production, code, and the institutions creating the future must work to upend, circumvent, or reinvent the algorithmic systems increasingly controlling the world, including robotics, that maintain us.",
      "authors": [
        {
          "affiliations": [],
          "personId": 38381
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3319502.3374844"
        },
        "Plenary": {
          "url": "https://www.youtube.com/watch?v=oVqL7FGMyTI",
          "title": "Community, Art and the Vernacular in Technological Ecosystems",
          "duration": 2467,
          "type": "video"
        }
      },
      "sessionIds": [
        38347
      ],
      "eventIds": []
    },
    {
      "id": 38392,
      "typeId": 11519,
      "title": "Program Chairs' Welcome",
      "trackId": 10965,
      "tags": [],
      "keywords": [],
      "abstract": "Welcome message from the program chairs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 38390
        },
        {
          "affiliations": [],
          "personId": 38391
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": ""
        },
        "Plenary": {
          "url": "https://www.youtube.com/watch?v=_74udxMmGJw&",
          "title": "Program Chair's welcome",
          "duration": 286,
          "type": "video"
        }
      },
      "sessionIds": [
        38380
      ],
      "eventIds": []
    },
    {
      "id": 38397,
      "typeId": 11523,
      "title": "MENOY: Mental and Emotional Nurturing Of Youth",
      "trackId": 10946,
      "tags": [],
      "keywords": [],
      "abstract": "MENOY is a social toy robot designed for children with autism. We designed a robot that focuses on developing healthy social habits in children by using a range of simple interactive motions such as handshakes, nodding and keeping eye contact. We aim to achieve this by having our robot being able to change its appearance and act like a toy, both entertaining and educating the child through practical interaction. MENOY can be used both autonomously, or controlled by professionals (Wizard of Oz).\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Rangitoto College",
              "dsl": ""
            }
          ],
          "personId": 38396
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Rangitoto College",
              "dsl": ""
            }
          ],
          "personId": 38393
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Kristin Highschool",
              "dsl": ""
            }
          ],
          "personId": 38394
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland ",
              "institution": "Kings college ",
              "dsl": ""
            }
          ],
          "personId": 38395
        }
      ],
      "addons": {
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MqL7NgAJAo8",
          "title": "MENOY: Mental and Emotional Nurturing of Youth",
          "duration": 169,
          "type": "video"
        }
      },
      "sessionIds": [
        38372
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 37161,
      "firstName": "Erol",
      "lastName": "Sahin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37162,
      "firstName": "Heather",
      "lastName": "Culbertson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37163,
      "firstName": "Shuo",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37164,
      "firstName": "Emilia",
      "lastName": "Barakova",
      "middleInitial": "I.",
      "affiliations": []
    },
    {
      "id": 37165,
      "firstName": "Xiangyu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37166,
      "firstName": "Max",
      "lastName": "Kuhn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37167,
      "firstName": "Anna",
      "lastName": "CohenMiller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37168,
      "firstName": "Ingo",
      "lastName": "Keller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37169,
      "firstName": "Mariah",
      "lastName": "Schrum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37170,
      "firstName": "Michael",
      "lastName": "Johnson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37171,
      "firstName": "Tim",
      "lastName": "Salcudean",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37172,
      "firstName": "Ehud",
      "lastName": "Sharlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37173,
      "firstName": "Ilayda",
      "lastName": "Guneysu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37174,
      "firstName": "Jose",
      "lastName": "Cortazar",
      "middleInitial": "Carlos",
      "affiliations": []
    },
    {
      "id": 37175,
      "firstName": "Akilesh",
      "lastName": "Rajavenkatanarayanan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37176,
      "firstName": "Hamish",
      "lastName": "Tennent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37177,
      "firstName": "Yedige",
      "lastName": "Tlegenov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37178,
      "firstName": "Marc",
      "lastName": "Hassenzahl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37179,
      "firstName": "Andreea",
      "lastName": "Bobu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37180,
      "firstName": "Marius",
      "lastName": "Montebaur",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37181,
      "firstName": "Rachid",
      "lastName": "Alami",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37183,
      "firstName": "Thomas",
      "lastName": "Powers",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 37184,
      "firstName": "Zhiping",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37185,
      "firstName": "Pradeep Kumar",
      "lastName": "Paladugula",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37186,
      "firstName": "Harrison",
      "lastName": "Preusse",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37187,
      "firstName": "Stefan",
      "lastName": "Kopp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37188,
      "firstName": "Maria",
      "lastName": "Kyrarini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37189,
      "firstName": "Francesca",
      "lastName": "Bianco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37190,
      "firstName": "Tony",
      "lastName": "Belpaeme",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37191,
      "firstName": "Juliane",
      "lastName": "Nilsson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37192,
      "firstName": "Michael",
      "lastName": "Shanks",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37193,
      "firstName": "David",
      "lastName": "Gonçalves",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37194,
      "firstName": "Michael",
      "lastName": "Bechtel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37195,
      "firstName": "Koen",
      "lastName": "Hindriks",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37196,
      "firstName": "Nurila",
      "lastName": "Seitkazina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37197,
      "firstName": "Yuval",
      "lastName": "Zak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37198,
      "firstName": "Sophia",
      "lastName": "Steinhaeusser",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 37199,
      "firstName": "Trenton",
      "lastName": "Schulz",
      "middleInitial": "W",
      "affiliations": []
    },
    {
      "id": 37200,
      "firstName": "Helena",
      "lastName": "Webb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37201,
      "firstName": "Kevin",
      "lastName": "Weatherwax",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37202,
      "firstName": "Yutaka",
      "lastName": "Nakamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37203,
      "firstName": "Waleed",
      "lastName": "Uddin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37204,
      "firstName": "Hifza",
      "lastName": "Javed",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37205,
      "firstName": "Patrick",
      "lastName": "Tresset",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37207,
      "firstName": "Maha",
      "lastName": "Elgarf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37208,
      "firstName": "Karl",
      "lastName": "MacDorman",
      "middleInitial": "F",
      "affiliations": []
    },
    {
      "id": 37209,
      "firstName": "Ajmal",
      "lastName": "Shah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37210,
      "firstName": "Lærke",
      "lastName": "Bøg",
      "middleInitial": "Work",
      "affiliations": []
    },
    {
      "id": 37211,
      "firstName": "Valérie",
      "lastName": "Gouranton",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37212,
      "firstName": "Aadithyaa",
      "lastName": "JS",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37213,
      "firstName": "Max",
      "lastName": "Meijer",
      "middleInitial": "Jan",
      "affiliations": []
    },
    {
      "id": 37215,
      "firstName": "Matthew",
      "lastName": "Studley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37216,
      "firstName": "Debora",
      "lastName": "Zanatto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37217,
      "firstName": "Christopher",
      "lastName": "Peters",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37218,
      "firstName": "Joakim",
      "lastName": "Gustafson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37219,
      "firstName": "Karan",
      "lastName": "Bhasin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37220,
      "firstName": "Craig",
      "lastName": "Sutherland",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37221,
      "firstName": "Teena",
      "lastName": "Hassan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37222,
      "firstName": "Scott",
      "lastName": "Brown",
      "middleInitial": "Andrew",
      "affiliations": []
    },
    {
      "id": 37223,
      "firstName": "Hideaki",
      "lastName": "Kuzuoka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37224,
      "firstName": "Derek",
      "lastName": "Dufault",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37225,
      "firstName": "Kerstin G.",
      "lastName": "Emerson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37226,
      "firstName": "Hanbyeol",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37227,
      "firstName": "Autumn",
      "lastName": "Edwards",
      "middleInitial": "P",
      "affiliations": []
    },
    {
      "id": 37228,
      "firstName": "Anca",
      "lastName": "Dragan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37229,
      "firstName": "Jung",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37230,
      "firstName": "Sara",
      "lastName": "Price",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37231,
      "firstName": "Feng",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37232,
      "firstName": "Jani",
      "lastName": "Even",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37233,
      "firstName": "Tom",
      "lastName": "Williams",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37235,
      "firstName": "Leonel",
      "lastName": "Garcia-Marques",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37236,
      "firstName": "Omar",
      "lastName": "Mubin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37237,
      "firstName": "Carlos Toshinori",
      "lastName": "Ishi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37238,
      "firstName": "Ian",
      "lastName": "Gonsher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37239,
      "firstName": "Hiok Hian",
      "lastName": "Ong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37240,
      "firstName": "Matthew A.",
      "lastName": "Cronin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37241,
      "firstName": "Yunus",
      "lastName": "Terzioglu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37243,
      "firstName": "Sean",
      "lastName": "Andrist",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37244,
      "firstName": "Sangjin",
      "lastName": "Ko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37245,
      "firstName": "Suzanne",
      "lastName": "Tolmeijer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37246,
      "firstName": "Joseph",
      "lastName": "Daly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37247,
      "firstName": "Sachie",
      "lastName": "Yamada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37248,
      "firstName": "Joseph",
      "lastName": "DelPreto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37249,
      "firstName": "Jayadev",
      "lastName": "Madyal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37250,
      "firstName": "Shanee",
      "lastName": "Honig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37251,
      "firstName": "Anat",
      "lastName": "Caspi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37252,
      "firstName": "Satoshi",
      "lastName": "Yagi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37253,
      "firstName": "David",
      "lastName": "Goedicke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37254,
      "firstName": "Yugo",
      "lastName": "Hayashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37255,
      "firstName": "Sumeya",
      "lastName": "Mohamed",
      "middleInitial": "Ahmed",
      "affiliations": []
    },
    {
      "id": 37257,
      "firstName": "Pedro",
      "lastName": "Santos",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 37258,
      "firstName": "Silvia",
      "lastName": "Rossi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37259,
      "firstName": "Chihab",
      "lastName": "Nadri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37261,
      "firstName": "Shen",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37262,
      "firstName": "Terry",
      "lastName": "Fong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37263,
      "firstName": "Kasper",
      "lastName": "Kobberholm",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37264,
      "firstName": "Tal",
      "lastName": "Oron-Gilad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37265,
      "firstName": "Daniela",
      "lastName": "Rus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37266,
      "firstName": "Cynthia",
      "lastName": "Breazeal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37267,
      "firstName": "Matthew",
      "lastName": "Tang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37268,
      "firstName": "Samarendra",
      "lastName": "Hedaoo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37269,
      "firstName": "Chinenye Augustine",
      "lastName": "Ajibo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37270,
      "firstName": "Frank",
      "lastName": "Broz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37271,
      "firstName": "Gregory",
      "lastName": "Hawkridge",
      "middleInitial": "T",
      "affiliations": []
    },
    {
      "id": 37272,
      "firstName": "Te-Yi",
      "lastName": "Hsieh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37273,
      "firstName": "Chiara",
      "lastName": "de Jong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37274,
      "firstName": "Tommaso",
      "lastName": "Colombino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37275,
      "firstName": "Heather",
      "lastName": "Knight",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37276,
      "firstName": "Hugo",
      "lastName": "Simão",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37277,
      "firstName": "Mads",
      "lastName": "Bering Christiansen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37278,
      "firstName": "David",
      "lastName": "St-Onge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37279,
      "firstName": "Florian",
      "lastName": "Lier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37280,
      "firstName": "Laura",
      "lastName": "Platte",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37281,
      "firstName": "Kanako",
      "lastName": "Tomita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37282,
      "firstName": "Jamy",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37283,
      "firstName": "Giulio",
      "lastName": "Sandini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37286,
      "firstName": "Mirle",
      "lastName": "Willems",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37287,
      "firstName": "Connor",
      "lastName": "Esterwood",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37288,
      "firstName": "Isabel",
      "lastName": "Schwaninger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37289,
      "firstName": "Brittany",
      "lastName": "Duncan",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 37290,
      "firstName": "Andoni",
      "lastName": "Rivera-Pinto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37291,
      "firstName": "Lionel",
      "lastName": "Robert",
      "middleInitial": "Peter",
      "affiliations": []
    },
    {
      "id": 37292,
      "firstName": "Lina",
      "lastName": "Klaas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37293,
      "firstName": "Ned",
      "lastName": "Barker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37294,
      "firstName": "Vaibhav",
      "lastName": "Unhelkar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37295,
      "firstName": "Zhanel",
      "lastName": "Zhexenova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37296,
      "firstName": "Tushar",
      "lastName": "Semwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37298,
      "firstName": "Melanie",
      "lastName": "Jouaiti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37299,
      "firstName": "Minghe",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37300,
      "firstName": "David",
      "lastName": "Robb",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 37301,
      "firstName": "Nicolas",
      "lastName": "SPATOLA",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37302,
      "firstName": "Naoki",
      "lastName": "Ise",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37303,
      "firstName": "Wali",
      "lastName": "Rizvi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37304,
      "firstName": "Teresa",
      "lastName": "Sletten",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37305,
      "firstName": "Fillia",
      "lastName": "Makedon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37306,
      "firstName": "Adam",
      "lastName": "Coyne",
      "middleInitial": "Kavanagh",
      "affiliations": []
    },
    {
      "id": 37308,
      "firstName": "Benjamin",
      "lastName": "Abramoff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37309,
      "firstName": "Catherine",
      "lastName": "Émond",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37310,
      "firstName": "Dominik",
      "lastName": "Arnold",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37311,
      "firstName": "Boran",
      "lastName": "Sahindal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37312,
      "firstName": "Rebecca",
      "lastName": "Currano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37313,
      "firstName": "Elaine",
      "lastName": "Short",
      "middleInitial": "Schaertl",
      "affiliations": []
    },
    {
      "id": 37314,
      "firstName": "Reuben",
      "lastName": "Aronson",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 37315,
      "firstName": "Till",
      "lastName": "Halbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37316,
      "firstName": "Nathan",
      "lastName": "Tsoi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37318,
      "firstName": "Felix",
      "lastName": "Lindner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37319,
      "firstName": "James",
      "lastName": "Clarke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37320,
      "firstName": "Gavin",
      "lastName": "Suddrey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37321,
      "firstName": "Marlon",
      "lastName": "Spangenberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37322,
      "firstName": "Jasmin",
      "lastName": "Bernotat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37324,
      "firstName": "Asim Evren",
      "lastName": "Yantac",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37325,
      "firstName": "Michael",
      "lastName": "Gleicher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37326,
      "firstName": "Tahir",
      "lastName": "Abbas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37327,
      "firstName": "Qin",
      "lastName": "Zhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37328,
      "firstName": "Oskar",
      "lastName": "Palinko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37329,
      "firstName": "Aditi",
      "lastName": "Ramachandran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37330,
      "firstName": "Astrid",
      "lastName": "Weiss",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37331,
      "firstName": "Priscilla",
      "lastName": "Zhang",
      "middleInitial": "Hui Shan",
      "affiliations": []
    },
    {
      "id": 37332,
      "firstName": "Michael",
      "lastName": "Neises",
      "middleInitial": "Christian",
      "affiliations": []
    },
    {
      "id": 37333,
      "firstName": "Jason",
      "lastName": "Borenstein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37335,
      "firstName": "Amal",
      "lastName": "Nanavati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37336,
      "firstName": "Kerstin",
      "lastName": "Fischer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37337,
      "firstName": "Adriana Lorena",
      "lastName": "González",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37338,
      "firstName": "Alan",
      "lastName": "Thorne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37339,
      "firstName": "Ankit",
      "lastName": "Shah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37341,
      "firstName": "Kaitlyn M.",
      "lastName": "Patterson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37342,
      "firstName": "Eduardo",
      "lastName": "Ruiz Ramirez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37343,
      "firstName": "German",
      "lastName": "Terrazas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37344,
      "firstName": "Selma",
      "lastName": "Sabanovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37345,
      "firstName": "Dauren",
      "lastName": "Turabayev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37346,
      "firstName": "Luc",
      "lastName": "Wijnen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37347,
      "firstName": "Frederic",
      "lastName": "Robinson",
      "middleInitial": "Anthony",
      "affiliations": []
    },
    {
      "id": 37348,
      "firstName": "Conor",
      "lastName": "McGinn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37349,
      "firstName": "Ray",
      "lastName": "Jones",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37350,
      "firstName": "Peggy",
      "lastName": "van Minkelen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37351,
      "firstName": "Daniel",
      "lastName": "Bambusek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37352,
      "firstName": "Marta",
      "lastName": "Couto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37353,
      "firstName": "Chipp",
      "lastName": "Jansen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37354,
      "firstName": "Mike",
      "lastName": "Ligthart",
      "middleInitial": "E.U.",
      "affiliations": []
    },
    {
      "id": 37355,
      "firstName": "Prasanth",
      "lastName": "Murali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37356,
      "firstName": "Julian",
      "lastName": "Allchin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37357,
      "firstName": "ALESSANDRO",
      "lastName": "DI NUOVO",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37358,
      "firstName": "Onis",
      "lastName": "Brown",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37359,
      "firstName": "Mark",
      "lastName": "Khordi-moodi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37360,
      "firstName": "Mitsuhiro",
      "lastName": "Goto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37361,
      "firstName": "Takashi",
      "lastName": "Minato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37362,
      "firstName": "Jignesh",
      "lastName": "Modi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37363,
      "firstName": "Jesus",
      "lastName": "Favela",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37364,
      "firstName": "Kazuhiko",
      "lastName": "Shinozawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37365,
      "firstName": "Kevin",
      "lastName": "El Haddad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37366,
      "firstName": "Ville",
      "lastName": "Kyrki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37367,
      "firstName": "Raquel",
      "lastName": "Ros",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37368,
      "firstName": "Annika",
      "lastName": "Hellendoorn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37369,
      "firstName": "Johan",
      "lastName": "Kildal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37370,
      "firstName": "Hidenobu",
      "lastName": "Sumioka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37371,
      "firstName": "Selin",
      "lastName": "Yılmaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37372,
      "firstName": "Oliver",
      "lastName": "Bown",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37373,
      "firstName": "Skaiste",
      "lastName": "Butkute",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37374,
      "firstName": "Andrea",
      "lastName": "Thomaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37375,
      "firstName": "Arvid",
      "lastName": "Kappas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37376,
      "firstName": "Gisele",
      "lastName": "Ragusa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37377,
      "firstName": "Ujwal",
      "lastName": "Gadiraju",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37378,
      "firstName": "Karen",
      "lastName": "Arnold",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37379,
      "firstName": "Daniel",
      "lastName": "Davison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37380,
      "firstName": "Stephanie",
      "lastName": "Arevalo Arboleda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37381,
      "firstName": "Nicole",
      "lastName": "Robinson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37382,
      "firstName": "Siya",
      "lastName": "Kunde",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37383,
      "firstName": "Carl",
      "lastName": "Trimbach",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37384,
      "firstName": "Kazuhiro",
      "lastName": "Ikeda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37386,
      "firstName": "Christian",
      "lastName": "Dondrup",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37387,
      "firstName": "Zhuoling",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37388,
      "firstName": "Chaoran",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37389,
      "firstName": "Matthias",
      "lastName": "Rehm",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37391,
      "firstName": "Genki",
      "lastName": "Miyauchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37392,
      "firstName": "Yoshihiro",
      "lastName": "Nakata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37393,
      "firstName": "Simon",
      "lastName": "Parsons",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37394,
      "firstName": "Divyanshu",
      "lastName": "Kumar Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37395,
      "firstName": "Jan",
      "lastName": "Peters",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37397,
      "firstName": "Fernando",
      "lastName": "Garcia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37398,
      "firstName": "Dylan",
      "lastName": "Losey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37399,
      "firstName": "Anna",
      "lastName": "Dobrosovestnova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37400,
      "firstName": "Chaolan",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37401,
      "firstName": "Roberto",
      "lastName": "Raez Pereyra",
      "middleInitial": "Alejandro",
      "affiliations": []
    },
    {
      "id": 37402,
      "firstName": "Jörn",
      "lastName": "Hurtienne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37403,
      "firstName": "Hai-Nguyen",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37404,
      "firstName": "Mathias",
      "lastName": "Broth",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37405,
      "firstName": "Zhansaule",
      "lastName": "Telisheva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37406,
      "firstName": "Antonia Lina",
      "lastName": "Krummheuer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37407,
      "firstName": "Anthony",
      "lastName": "Harrison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37408,
      "firstName": "Amy",
      "lastName": "Chan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37410,
      "firstName": "Praminda",
      "lastName": "Caleb-Solly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37411,
      "firstName": "Chuang",
      "lastName": "YU",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37412,
      "firstName": "Yulun",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37413,
      "firstName": "Yasunori",
      "lastName": "Ozaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37414,
      "firstName": "Yifei",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37415,
      "firstName": "Goren",
      "lastName": "Gordon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37416,
      "firstName": "Marynel",
      "lastName": "Vázquez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37417,
      "firstName": "Konstantin",
      "lastName": "Zähl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37418,
      "firstName": "Èric",
      "lastName": "Pairet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37419,
      "firstName": "Dimosthenis",
      "lastName": "Kontogiorgos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37420,
      "firstName": "Bidan",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37421,
      "firstName": "Ruchen",
      "lastName": "Wen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37422,
      "firstName": "Myounghoon",
      "lastName": "Jeon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37423,
      "firstName": "Eugene",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37424,
      "firstName": "Yuki",
      "lastName": "Kubota",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37425,
      "firstName": "Hannah",
      "lastName": "Bradwell",
      "middleInitial": "Louise",
      "affiliations": []
    },
    {
      "id": 37427,
      "firstName": "İmge",
      "lastName": "Saltik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37428,
      "firstName": "Letian",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37429,
      "firstName": "Drazen",
      "lastName": "Brscic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37430,
      "firstName": "Negar",
      "lastName": "Khojasteh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37431,
      "firstName": "Alexander",
      "lastName": "Aroyo",
      "middleInitial": "Mois",
      "affiliations": []
    },
    {
      "id": 37432,
      "firstName": "Varun",
      "lastName": "Kanal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37433,
      "firstName": "Wendy",
      "lastName": "Ju",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37434,
      "firstName": "Angelica",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37435,
      "firstName": "Joseph",
      "lastName": "Valdez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37436,
      "firstName": "Helen",
      "lastName": "Hastie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37437,
      "firstName": "Si Yong Andrew",
      "lastName": "Ho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37438,
      "firstName": "Chad",
      "lastName": "Tossell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37439,
      "firstName": "Nataly",
      "lastName": "Martini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37440,
      "firstName": "Mojgan",
      "lastName": "Hashemian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37441,
      "firstName": "Jeremy",
      "lastName": "Goslin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37442,
      "firstName": "Giulia",
      "lastName": "Perugia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37443,
      "firstName": "Kazuaki",
      "lastName": "Takeuchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37444,
      "firstName": "Janet",
      "lastName": "Wang",
      "middleInitial": "Z",
      "affiliations": []
    },
    {
      "id": 37445,
      "firstName": "Lisa",
      "lastName": "Renzi-Hammond",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 37446,
      "firstName": "Laurence",
      "lastName": "Roberts-Elliott",
      "middleInitial": "John Brian",
      "affiliations": []
    },
    {
      "id": 37447,
      "firstName": "Damith",
      "lastName": "Herath",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37448,
      "firstName": "Huili",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37449,
      "firstName": "Vignesh",
      "lastName": "Prasad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37450,
      "firstName": "Elizabeth",
      "lastName": "Jochum",
      "middleInitial": "Ann",
      "affiliations": []
    },
    {
      "id": 37451,
      "firstName": "Harold",
      "lastName": "Soh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37452,
      "firstName": "Bradley",
      "lastName": "Hayes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37453,
      "firstName": "Danny",
      "lastName": "Koh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37454,
      "firstName": "Franziska",
      "lastName": "Rücker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37455,
      "firstName": "Suresh Kumaar",
      "lastName": "Jayaraman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37456,
      "firstName": "Jinying",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37457,
      "firstName": "Leonor",
      "lastName": "Fermoselle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37458,
      "firstName": "Rohan",
      "lastName": "Paleja",
      "middleInitial": "R",
      "affiliations": []
    },
    {
      "id": 37459,
      "firstName": "Thomas",
      "lastName": "Groechel",
      "middleInitial": "Roy",
      "affiliations": []
    },
    {
      "id": 37460,
      "firstName": "Boyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37461,
      "firstName": "James E.",
      "lastName": "Young",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37462,
      "firstName": "Shreepriya",
      "lastName": "Shreepriya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37463,
      "firstName": "Aida",
      "lastName": "Zhanatkyzy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37464,
      "firstName": "Julian",
      "lastName": "Welsh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37465,
      "firstName": "Alan",
      "lastName": "Winfield",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37466,
      "firstName": "Alessandra",
      "lastName": "Sciutti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37467,
      "firstName": "Bethany",
      "lastName": "Mackey",
      "middleInitial": "Ann",
      "affiliations": []
    },
    {
      "id": 37468,
      "firstName": "Patrick",
      "lastName": "Finn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37469,
      "firstName": "Sanne",
      "lastName": "van Waveren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37470,
      "firstName": "Jason",
      "lastName": "Mercer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37471,
      "firstName": "Dr. Michael",
      "lastName": "Szollosy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37473,
      "firstName": "Kosuke",
      "lastName": "Wakabayashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37474,
      "firstName": "Rebecca",
      "lastName": "Stower",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37475,
      "firstName": "Julie",
      "lastName": "Shah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37476,
      "firstName": "Serge",
      "lastName": "Thill",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37477,
      "firstName": "Gottfried",
      "lastName": "Zimmermann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37478,
      "firstName": "Karen",
      "lastName": "Sage",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37479,
      "firstName": "Shiqi",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37480,
      "firstName": "Ryan",
      "lastName": "Jackson",
      "middleInitial": "Blake",
      "affiliations": []
    },
    {
      "id": 37481,
      "firstName": "Moonyoung",
      "lastName": "Tae",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37483,
      "firstName": "Nazerke",
      "lastName": "Rakhymbayeva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37484,
      "firstName": "Clare",
      "lastName": "Dixon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37485,
      "firstName": "Rianne",
      "lastName": "Conijn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37486,
      "firstName": "Danilo",
      "lastName": "Gallo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37487,
      "firstName": "Masahiko",
      "lastName": "Osawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37488,
      "firstName": "Stephanie",
      "lastName": "Rosenthal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37489,
      "firstName": "Heidi J.",
      "lastName": "Schellin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37490,
      "firstName": "Satoru",
      "lastName": "Satake",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37491,
      "firstName": "Val",
      "lastName": "Morrison",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37492,
      "firstName": "Aaron",
      "lastName": "Steinfeld",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37493,
      "firstName": "Megan",
      "lastName": "Strait",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37494,
      "firstName": "Nicola",
      "lastName": "Webb",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37495,
      "firstName": "Masanori",
      "lastName": "Yokoyama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37496,
      "firstName": "Asela",
      "lastName": "Wijesinghe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37497,
      "firstName": "Emiel",
      "lastName": "Krahmer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37498,
      "firstName": "Janie Busby",
      "lastName": "Grant",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37499,
      "firstName": "Laurie",
      "lastName": "Santos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37500,
      "firstName": "Rosalyn M.",
      "lastName": "Langedijk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37501,
      "firstName": "Katharina",
      "lastName": "Schneider",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37502,
      "firstName": "Joe",
      "lastName": "Connolly",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37503,
      "firstName": "Axel",
      "lastName": "Hessler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37504,
      "firstName": "Matthew",
      "lastName": "Gombolay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37506,
      "firstName": "Waqar",
      "lastName": "Ali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37507,
      "firstName": "Şahin",
      "lastName": "Albayrak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37508,
      "firstName": "Yijie",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37509,
      "firstName": "Ewart",
      "lastName": "de Visser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37510,
      "firstName": "Jonas",
      "lastName": "Jørgensen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37511,
      "firstName": "Sanja",
      "lastName": "Dogramadzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37512,
      "firstName": "Yifang",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37513,
      "firstName": "Samantha",
      "lastName": "Reig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37514,
      "firstName": "Katharina",
      "lastName": "Rohlfing",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37515,
      "firstName": "Yunjoo",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37516,
      "firstName": "Daniel",
      "lastName": "Paredes-soto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37517,
      "firstName": "Fanziska",
      "lastName": "Viertel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37518,
      "firstName": "Baijun",
      "lastName": "Xie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37519,
      "firstName": "Dylan",
      "lastName": "Dooley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37520,
      "firstName": "Katie",
      "lastName": "Winkle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37521,
      "firstName": "Maria",
      "lastName": "Cabrera",
      "middleInitial": "Eugenia",
      "affiliations": []
    },
    {
      "id": 37522,
      "firstName": "Alex",
      "lastName": "Barco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37523,
      "firstName": "Andrew",
      "lastName": "Miller",
      "middleInitial": "D",
      "affiliations": []
    },
    {
      "id": 37524,
      "firstName": "Sam",
      "lastName": "Thellman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37525,
      "firstName": "Daichi",
      "lastName": "Morimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37526,
      "firstName": "Stefano",
      "lastName": "Ghidini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37527,
      "firstName": "Büşra",
      "lastName": "Sarıgül",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37528,
      "firstName": "Marcus",
      "lastName": "Arnett",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37529,
      "firstName": "Muneeb",
      "lastName": "Ahmad",
      "middleInitial": "Imtiaz",
      "affiliations": []
    },
    {
      "id": 37530,
      "firstName": "Mei Yii",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37531,
      "firstName": "Andre",
      "lastName": "Pereira",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37532,
      "firstName": "Dawn",
      "lastName": "Tilbury",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37533,
      "firstName": "Mina",
      "lastName": "Marmpena",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37534,
      "firstName": "Ana",
      "lastName": "Paiva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37535,
      "firstName": "Mehdi",
      "lastName": "Hellou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37536,
      "firstName": "Gierad",
      "lastName": "Laput",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37537,
      "firstName": "Tomo",
      "lastName": "Funayama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37538,
      "firstName": "Carl",
      "lastName": "Macrae",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37539,
      "firstName": "Reika",
      "lastName": "McNish",
      "middleInitial": "Nicole",
      "affiliations": []
    },
    {
      "id": 37540,
      "firstName": "Ling",
      "lastName": "Dong",
      "middleInitial": "Liang",
      "affiliations": []
    },
    {
      "id": 37541,
      "firstName": "Woojong",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37542,
      "firstName": "Matthew",
      "lastName": "Beane",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37543,
      "firstName": "Kerstin",
      "lastName": "Haring",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 37544,
      "firstName": "Melanie",
      "lastName": "Derksen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37545,
      "firstName": "Nhan",
      "lastName": "Tran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37546,
      "firstName": "Luke",
      "lastName": "Rush",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 37547,
      "firstName": "Sandor",
      "lastName": "Veres",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 37548,
      "firstName": "Hans",
      "lastName": "Petersen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37549,
      "firstName": "Dennis",
      "lastName": "Reidsma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37550,
      "firstName": "JongSuk",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37551,
      "firstName": "Lotte",
      "lastName": "Damsgaard Nissen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37552,
      "firstName": "Harsh",
      "lastName": "Sanghavi",
      "middleInitial": "Kamalesh",
      "affiliations": []
    },
    {
      "id": 37553,
      "firstName": "Jaap",
      "lastName": "Ham",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37554,
      "firstName": "Xun",
      "lastName": "Shen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37555,
      "firstName": "Jong Hoon",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37556,
      "firstName": "Sisse",
      "lastName": "Ramskov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37557,
      "firstName": "Gnanathusharan",
      "lastName": "Rajendran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37558,
      "firstName": "Arold",
      "lastName": "Brandse",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37559,
      "firstName": "Indu",
      "lastName": "Bodala",
      "middleInitial": "Prasad",
      "affiliations": []
    },
    {
      "id": 37560,
      "firstName": "Rachel",
      "lastName": "Ibbotson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37561,
      "firstName": "Emily",
      "lastName": "Cross",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 37562,
      "firstName": "Lavindra",
      "lastName": "de Silva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37563,
      "firstName": "Peter",
      "lastName": "McKenna",
      "middleInitial": "Edward",
      "affiliations": []
    },
    {
      "id": 37564,
      "firstName": "Shuran",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37565,
      "firstName": "Yuki",
      "lastName": "Okafuji",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37566,
      "firstName": "Tomoko",
      "lastName": "Koda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37567,
      "firstName": "Simona",
      "lastName": "Aracri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37568,
      "firstName": "Louise",
      "lastName": "Devigne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37569,
      "firstName": "Marike",
      "lastName": "van den Broek",
      "middleInitial": "Koch",
      "affiliations": []
    },
    {
      "id": 37570,
      "firstName": "Midori",
      "lastName": "Sanchez Sifuentes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37571,
      "firstName": "Caroline",
      "lastName": "van Straten",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 37572,
      "firstName": "Alexandre",
      "lastName": "Colle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37573,
      "firstName": "Marlena",
      "lastName": "Fraune",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37574,
      "firstName": "Dahyun",
      "lastName": "Kang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37575,
      "firstName": "Jun Ki",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37576,
      "firstName": "Ayanna",
      "lastName": "Howard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37577,
      "firstName": "Elizabeth",
      "lastName": "Sklar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37578,
      "firstName": "Franz",
      "lastName": "Berger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37579,
      "firstName": "Matthew",
      "lastName": "Aylett",
      "middleInitial": "Peter",
      "affiliations": []
    },
    {
      "id": 37580,
      "firstName": "Paul",
      "lastName": "Gibson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37581,
      "firstName": "Jirou",
      "lastName": "Feng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37582,
      "firstName": "Andrew B.",
      "lastName": "Williams",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37583,
      "firstName": "Kate",
      "lastName": "Ladenheim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37584,
      "firstName": "Alexandra",
      "lastName": "Bacula",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37585,
      "firstName": "Djamari",
      "lastName": "Oetringer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37586,
      "firstName": "Richie",
      "lastName": "Bird",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37587,
      "firstName": "Dorsa",
      "lastName": "Sadigh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37588,
      "firstName": "Danielle",
      "lastName": "Langlois",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 37589,
      "firstName": "João",
      "lastName": "Avelino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37590,
      "firstName": "Theresa",
      "lastName": "Law",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37591,
      "firstName": "Bailey",
      "lastName": "Collete",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37592,
      "firstName": "Mari",
      "lastName": "Velonaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37593,
      "firstName": "Laura",
      "lastName": "Boccanfuso",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37594,
      "firstName": "Alicja",
      "lastName": "Depka Prondzinska",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37595,
      "firstName": "Thibault",
      "lastName": "Asselborn",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37596,
      "firstName": "Dagoberto",
      "lastName": "Cruz-Sandoval",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37597,
      "firstName": "Shawn",
      "lastName": "Joshi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37598,
      "firstName": "Rian",
      "lastName": "Aarts",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37599,
      "firstName": "Randy",
      "lastName": "Gomez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37600,
      "firstName": "Dario",
      "lastName": "Pasquali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37601,
      "firstName": "Christopher",
      "lastName": "Smith",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37602,
      "firstName": "Rinaldo",
      "lastName": "Kühne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37603,
      "firstName": "Jose L.",
      "lastName": "Part",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37604,
      "firstName": "Daniel",
      "lastName": "Rakita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37605,
      "firstName": "Theodoros",
      "lastName": "Georgiou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37606,
      "firstName": "Duncan",
      "lastName": "McFarlane",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37607,
      "firstName": "Riki",
      "lastName": "Satogata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37608,
      "firstName": "Anne",
      "lastName": "Bloem",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 37609,
      "firstName": "Maike",
      "lastName": "Paetzel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37610,
      "firstName": "Eric",
      "lastName": "Horvitz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37611,
      "firstName": "Takayuki",
      "lastName": "Kanda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37612,
      "firstName": "Kyungwon",
      "lastName": "Baek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37613,
      "firstName": "Matthew",
      "lastName": "Rueben",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37614,
      "firstName": "Elena",
      "lastName": "Dell’Aquila",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37615,
      "firstName": "John",
      "lastName": "Vilk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37616,
      "firstName": "Mr Dimitri",
      "lastName": "Ognibene",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37617,
      "firstName": "Noé",
      "lastName": "Tits",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37618,
      "firstName": "Haley",
      "lastName": "Swaim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37619,
      "firstName": "Mads",
      "lastName": "Bärenholdt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37620,
      "firstName": "Irvin Steve",
      "lastName": "Cardenas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37621,
      "firstName": "Jun San",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37623,
      "firstName": "Ayan",
      "lastName": "Ghosh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37624,
      "firstName": "Christiaan",
      "lastName": "Boersma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37625,
      "firstName": "Joeri",
      "lastName": "Planting",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37626,
      "firstName": "Jan",
      "lastName": "van der Meij",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37627,
      "firstName": "Marie",
      "lastName": "Babel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37628,
      "firstName": "Viraji",
      "lastName": "Amarajeewa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37629,
      "firstName": "Rhea",
      "lastName": "Montgomery Walsh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37631,
      "firstName": "ronit",
      "lastName": "feingold polak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37632,
      "firstName": "Ronald",
      "lastName": "Moore",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37633,
      "firstName": "Bing Cai",
      "lastName": "Kok",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37634,
      "firstName": "Annika",
      "lastName": "Silvervarg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37635,
      "firstName": "Shelly Levy",
      "lastName": "Tzedek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37636,
      "firstName": "Alexandre",
      "lastName": "Bernardino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37637,
      "firstName": "Sowmya",
      "lastName": "Somanath",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37639,
      "firstName": "Guy",
      "lastName": "Laban",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37640,
      "firstName": "Tony",
      "lastName": "Pipe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37641,
      "firstName": "Nick",
      "lastName": "Walker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37642,
      "firstName": "Ana Cecilia",
      "lastName": "Sánchez Ramos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37643,
      "firstName": "shaI",
      "lastName": "zucker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37644,
      "firstName": "Michal",
      "lastName": "Kapinus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37645,
      "firstName": "Yoshifumi",
      "lastName": "Kitamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37646,
      "firstName": "Davide",
      "lastName": "Russo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37647,
      "firstName": "Elizabeth",
      "lastName": "Cha",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37648,
      "firstName": "Omer",
      "lastName": "Gvirsman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37649,
      "firstName": "Timothy",
      "lastName": "Bickmore",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37650,
      "firstName": "Josh",
      "lastName": "Bamforth",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37652,
      "firstName": "Nicole",
      "lastName": "Krämer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37653,
      "firstName": "MICHAIL",
      "lastName": "PANTERIS",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37655,
      "firstName": "Jean-François",
      "lastName": "Robin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37656,
      "firstName": "Siddhartha",
      "lastName": "Srinivasa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37657,
      "firstName": "Marcel",
      "lastName": "Kaufmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37658,
      "firstName": "Robert",
      "lastName": "Goldstone",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37659,
      "firstName": "Frances",
      "lastName": "Wijnen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37660,
      "firstName": "Xiajie",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37661,
      "firstName": "Ellen",
      "lastName": "Stumph",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37662,
      "firstName": "Joshua",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37663,
      "firstName": "Malte",
      "lastName": "Jung",
      "middleInitial": "F",
      "affiliations": []
    },
    {
      "id": 37664,
      "firstName": "Jiayuan",
      "lastName": "Dong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37665,
      "firstName": "Jaap",
      "lastName": "Denissen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37666,
      "firstName": "Katrin",
      "lastName": "Lohan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37667,
      "firstName": "Ella",
      "lastName": "Velner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37668,
      "firstName": "Francesco",
      "lastName": "Del Duchetto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37669,
      "firstName": "Shankar",
      "lastName": "Sastry",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37670,
      "firstName": "Brian",
      "lastName": "Scassellati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37671,
      "firstName": "Jodi",
      "lastName": "Forlizzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37672,
      "firstName": "Sooraj",
      "lastName": "Krishna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37674,
      "firstName": "Judith",
      "lastName": "Dörrenbächer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37675,
      "firstName": "Chad",
      "lastName": "Edwards",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37676,
      "firstName": "Aditi",
      "lastName": "Talati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37677,
      "firstName": "Thierry",
      "lastName": "Dutoit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37678,
      "firstName": "Grace",
      "lastName": "Igwe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37679,
      "firstName": "Alistair",
      "lastName": "McConnell",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 37681,
      "firstName": "Birgit",
      "lastName": "Lugrin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37682,
      "firstName": "Carl",
      "lastName": "Mueller",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 37683,
      "firstName": "Gianpaolo",
      "lastName": "Maggi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37684,
      "firstName": "Michael L.",
      "lastName": "Littman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37685,
      "firstName": "Sebastian",
      "lastName": "Wallkotter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37686,
      "firstName": "José David",
      "lastName": "Lopes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37687,
      "firstName": "Jainendra",
      "lastName": "Shukla",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37688,
      "firstName": "ARMAN",
      "lastName": "SABYROV",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37689,
      "firstName": "Zdeněk",
      "lastName": "Materna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37690,
      "firstName": "Tianyu",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37691,
      "firstName": "Hasan",
      "lastName": "Ayaz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37692,
      "firstName": "Jesper Wædeled",
      "lastName": "Henriksen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37693,
      "firstName": "Jenay",
      "lastName": "Beer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37694,
      "firstName": "Alina",
      "lastName": "Pak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37695,
      "firstName": "Sebastian",
      "lastName": "Caballa Barrientos",
      "middleInitial": "Rony",
      "affiliations": []
    },
    {
      "id": 37696,
      "firstName": "Ailie",
      "lastName": "Turton",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37697,
      "firstName": "C.",
      "lastName": "Lyng-Olsen",
      "middleInitial": "Burton",
      "affiliations": []
    },
    {
      "id": 37698,
      "firstName": "Kurima",
      "lastName": "Sakai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37699,
      "firstName": "Snezana",
      "lastName": "Babovic Dimitrijevic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37700,
      "firstName": "Marc",
      "lastName": "Espona",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37701,
      "firstName": "Cristina",
      "lastName": "Nuzzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37702,
      "firstName": "Kohei",
      "lastName": "Ogawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37703,
      "firstName": "Tom",
      "lastName": "Bridgwater",
      "middleInitial": "J F",
      "affiliations": []
    },
    {
      "id": 37704,
      "firstName": "Katharina",
      "lastName": "Carstens",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37705,
      "firstName": "Mark",
      "lastName": "Neerincx",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37706,
      "firstName": "Rea",
      "lastName": "Francesco",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37708,
      "firstName": "Laurel D.",
      "lastName": "Riek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37709,
      "firstName": "Natalia",
      "lastName": "Calvo-Barajas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37710,
      "firstName": "Bolat",
      "lastName": "Tleubayev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37711,
      "firstName": "Kevin",
      "lastName": "Vetter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37712,
      "firstName": "Timothy",
      "lastName": "Adamson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37713,
      "firstName": "Yolanda",
      "lastName": "Vazquez-Alvarez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37714,
      "firstName": "Hajer",
      "lastName": "Chalghoumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37715,
      "firstName": "Manuel",
      "lastName": "Bied",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37716,
      "firstName": "Adrian",
      "lastName": "Zwiener",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37717,
      "firstName": "Jennifer",
      "lastName": "McFarlane",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37718,
      "firstName": "Hideki",
      "lastName": "Garcia Goo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37719,
      "firstName": "Sarah",
      "lastName": "Sebo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37720,
      "firstName": "Viola",
      "lastName": "Mocz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37721,
      "firstName": "Alexander",
      "lastName": "Sutherland",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37722,
      "firstName": "Asano",
      "lastName": "Kitahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37723,
      "firstName": "Hanno",
      "lastName": "van Keulen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37725,
      "firstName": "Vaishali",
      "lastName": "Rajendren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37726,
      "firstName": "Stefanos",
      "lastName": "Nikolaidis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37727,
      "firstName": "Ana",
      "lastName": "Pires",
      "middleInitial": "Cristina",
      "affiliations": []
    },
    {
      "id": 37728,
      "firstName": "Carlo",
      "lastName": "Mazzola",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37729,
      "firstName": "John Anthony",
      "lastName": "Rossiter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37730,
      "firstName": "Lynne",
      "lastName": "Baillie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37731,
      "firstName": "Pieter",
      "lastName": "Wolfert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37732,
      "firstName": "Chien-Ming",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37733,
      "firstName": "Manisha",
      "lastName": "Natarajan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37734,
      "firstName": "Tehani",
      "lastName": "Wanniarachchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37736,
      "firstName": "Pierre",
      "lastName": "Dillenbourg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37737,
      "firstName": "Ute",
      "lastName": "Leonards",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37738,
      "firstName": "Anna",
      "lastName": "Westaway",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37739,
      "firstName": "Nancy",
      "lastName": "Zook",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37740,
      "firstName": "Angelo",
      "lastName": "Cangelosi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37741,
      "firstName": "Maja",
      "lastName": "Mataric",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37742,
      "firstName": "Samuel",
      "lastName": "Mascarenhas",
      "middleInitial": "F.",
      "affiliations": []
    },
    {
      "id": 37743,
      "firstName": "Naomi",
      "lastName": "Fitter",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 37744,
      "firstName": "Pleun",
      "lastName": "van Hees",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37745,
      "firstName": "Muyleng",
      "lastName": "Ghuy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37746,
      "firstName": "Michal",
      "lastName": "Luria",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37747,
      "firstName": "Aizada",
      "lastName": "Turarova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37748,
      "firstName": "Kaiping",
      "lastName": "Peng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37749,
      "firstName": "Elmira",
      "lastName": "Yadollahi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37750,
      "firstName": "Carlo",
      "lastName": "Tiseo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37751,
      "firstName": "Elizabeth",
      "lastName": "Carter",
      "middleInitial": "Jeanne",
      "affiliations": []
    },
    {
      "id": 37752,
      "firstName": "Massimiliano",
      "lastName": "Patacchiola",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37753,
      "firstName": "Joe",
      "lastName": "Mendelson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37754,
      "firstName": "Paul",
      "lastName": "Bremner",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 37755,
      "firstName": "Dirk",
      "lastName": "Heylen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37756,
      "firstName": "Adhisha",
      "lastName": "Gammanpila",
      "middleInitial": "Chamikara",
      "affiliations": []
    },
    {
      "id": 37757,
      "firstName": "Begum",
      "lastName": "Cerrrahoglu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37758,
      "firstName": "Jung Yeop",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37759,
      "firstName": "Ruth",
      "lastName": "Aylett",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37760,
      "firstName": "Eli",
      "lastName": "Sheppard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37761,
      "firstName": "Yumiko",
      "lastName": "Matsuura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37762,
      "firstName": "Purav",
      "lastName": "Bhardwaj",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37763,
      "firstName": "Panos",
      "lastName": "Markopoulos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37765,
      "firstName": "Yiyun",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37766,
      "firstName": "Judith",
      "lastName": "Papadopoulos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37767,
      "firstName": "Bob",
      "lastName": "Schadenberg",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 37768,
      "firstName": "Eitan",
      "lastName": "Rothberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37769,
      "firstName": "Mafalda",
      "lastName": "Santos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37770,
      "firstName": "Pragathi",
      "lastName": "Praveena",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37771,
      "firstName": "Kentaro",
      "lastName": "Yoshifuji",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37772,
      "firstName": "Jonas",
      "lastName": "Gonzalez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37773,
      "firstName": "Heqiu",
      "lastName": "Song",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37774,
      "firstName": "Dylan",
      "lastName": "Moore",
      "middleInitial": "James",
      "affiliations": []
    },
    {
      "id": 37775,
      "firstName": "Adriana",
      "lastName": "Tapus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37777,
      "firstName": "Darja",
      "lastName": "Stoeva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37778,
      "firstName": "Malcolm",
      "lastName": "Doering",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37779,
      "firstName": "John",
      "lastName": "Zimmerman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37780,
      "firstName": "Benjamin",
      "lastName": "Schönfuß",
      "middleInitial": "Ingo",
      "affiliations": []
    },
    {
      "id": 37781,
      "firstName": "Amy",
      "lastName": "LaViers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37782,
      "firstName": "Mohammad",
      "lastName": "Obaid",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37784,
      "firstName": "Hebert",
      "lastName": "Azevedo Sa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37785,
      "firstName": "Aaron",
      "lastName": "Levine",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37786,
      "firstName": "Jinwook",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37787,
      "firstName": "Rhona",
      "lastName": "Winnington",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37788,
      "firstName": "Wen-Ying",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37789,
      "firstName": "Marco",
      "lastName": "Perez Hernandez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37790,
      "firstName": "Pauline",
      "lastName": "Chevalier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37791,
      "firstName": "Suncica",
      "lastName": "Petrovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37792,
      "firstName": "Lena",
      "lastName": "Schramm",
      "middleInitial": "Theodora",
      "affiliations": []
    },
    {
      "id": 37793,
      "firstName": "Hadas",
      "lastName": "Kress-Gazit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37794,
      "firstName": "Julia",
      "lastName": "Arndt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37795,
      "firstName": "Ehud",
      "lastName": "Sharlin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37796,
      "firstName": "Glenda",
      "lastName": "Hannibal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37797,
      "firstName": "Alyssa",
      "lastName": "Kubota",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37798,
      "firstName": "Jinkyu",
      "lastName": "Jang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37799,
      "firstName": "Carey",
      "lastName": "Jewitt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37800,
      "firstName": "Yuichiro",
      "lastName": "Yoshikawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37801,
      "firstName": "Anders Skaarup",
      "lastName": "Johansen",
      "middleInitial": "Skaarup",
      "affiliations": []
    },
    {
      "id": 37802,
      "firstName": "Cletus",
      "lastName": "Joseph",
      "middleInitial": "V",
      "affiliations": []
    },
    {
      "id": 37803,
      "firstName": "Guri",
      "lastName": "Verne",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37805,
      "firstName": "Paul",
      "lastName": "Boersma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37806,
      "firstName": "Tatiana N.",
      "lastName": "Oberley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37807,
      "firstName": "Dan",
      "lastName": "Bohus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37808,
      "firstName": "Bilge",
      "lastName": "Mutlu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37809,
      "firstName": "Danielle",
      "lastName": "Oltman",
      "middleInitial": "Jacqueline",
      "affiliations": []
    },
    {
      "id": 37810,
      "firstName": "Emmanuel",
      "lastName": "Senft",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37811,
      "firstName": "Houston",
      "lastName": "Claure",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37812,
      "firstName": "Kyana",
      "lastName": "van Eijndhoven",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37813,
      "firstName": "Dr Judy",
      "lastName": "Clegg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37814,
      "firstName": "Saptarshi",
      "lastName": "Sadhu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37815,
      "firstName": "Wafa",
      "lastName": "Johal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37816,
      "firstName": "Leila",
      "lastName": "Takayama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37817,
      "firstName": "Myrthe",
      "lastName": "Tielman",
      "middleInitial": "Lotte",
      "affiliations": []
    },
    {
      "id": 37818,
      "firstName": "Diana",
      "lastName": "Löffler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37819,
      "firstName": "Junya",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37820,
      "firstName": "Adrian",
      "lastName": "Salazar Gomez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37821,
      "firstName": "Yaacov",
      "lastName": "Koren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37822,
      "firstName": "Ernst",
      "lastName": "Bohlmeijer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37823,
      "firstName": "Elizabeth",
      "lastName": "Phillips",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37824,
      "firstName": "H. F. Machiel",
      "lastName": "Van der Loos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37825,
      "firstName": "Ashwin",
      "lastName": "Sadananda Bhat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37826,
      "firstName": "Mirjam",
      "lastName": "de Haas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37827,
      "firstName": "David",
      "lastName": "Sirkin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37828,
      "firstName": "Sumita",
      "lastName": "Sharma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37829,
      "firstName": "Liying",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37830,
      "firstName": "Yoichi",
      "lastName": "Yamazaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37831,
      "firstName": "Jan",
      "lastName": "de Wit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37832,
      "firstName": "Meiying",
      "lastName": "Qin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37833,
      "firstName": "Simon",
      "lastName": "Schreibelmayr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37834,
      "firstName": "Mitsuhiko",
      "lastName": "Kimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37835,
      "firstName": "Travis",
      "lastName": "Wiltshire",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 37836,
      "firstName": "Anouk",
      "lastName": "van Maris",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37837,
      "firstName": "Ronald",
      "lastName": "Cumbal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37838,
      "firstName": "Karan",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37839,
      "firstName": "Lars ",
      "lastName": "Jensen",
      "middleInitial": "Christian",
      "affiliations": []
    },
    {
      "id": 37840,
      "firstName": "Vanessa",
      "lastName": "Evers",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37842,
      "firstName": "Marina",
      "lastName": "Jirotka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37843,
      "firstName": "Mark",
      "lastName": "Ho",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 37844,
      "firstName": "Maartje",
      "lastName": "de Graaf",
      "middleInitial": "M.A.",
      "affiliations": []
    },
    {
      "id": 37845,
      "firstName": "Elizabeth",
      "lastName": "Broadbent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37846,
      "firstName": "Yoan",
      "lastName": "Sallami",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37847,
      "firstName": "Olov",
      "lastName": "Engwall",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37848,
      "firstName": "Erin",
      "lastName": "Brady",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37849,
      "firstName": "Christopher",
      "lastName": "Ruff",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37850,
      "firstName": "Amar",
      "lastName": "Kalsi",
      "middleInitial": "Singh",
      "affiliations": []
    },
    {
      "id": 37851,
      "firstName": "Shalv",
      "lastName": "Parekh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37852,
      "firstName": "Séverin",
      "lastName": "Lemaignan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37853,
      "firstName": "Bastian",
      "lastName": "Wibranek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37854,
      "firstName": "Bishakha",
      "lastName": "Chaudhury",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37855,
      "firstName": "Chung Hyuk",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37856,
      "firstName": "Sonya",
      "lastName": "Kwak",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 37857,
      "firstName": "Leelo",
      "lastName": "Keevallik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37858,
      "firstName": "Vicky",
      "lastName": "Charisi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37859,
      "firstName": "Dexter",
      "lastName": "Scobee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37860,
      "firstName": "Michael",
      "lastName": "Suguitan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37861,
      "firstName": "Seongwoong",
      "lastName": "Hong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37862,
      "firstName": "Ed",
      "lastName": "Lawson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37863,
      "firstName": "Sijia",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37864,
      "firstName": "Thomas B.",
      "lastName": "Moeslund",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37865,
      "firstName": "Laura",
      "lastName": "Hoffmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37866,
      "firstName": "Carmen",
      "lastName": "Gruson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37867,
      "firstName": "Sarah",
      "lastName": "Gillet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37868,
      "firstName": "Shun",
      "lastName": "Yoshioka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37869,
      "firstName": "Denise Y.",
      "lastName": "Geiskkovitch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37870,
      "firstName": "Aike",
      "lastName": "Horstmann",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 37871,
      "firstName": "Songli",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37872,
      "firstName": "Saurabh",
      "lastName": "Yadava",
      "middleInitial": "Kumar",
      "affiliations": []
    },
    {
      "id": 37873,
      "firstName": "Mattia",
      "lastName": "Racca",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37874,
      "firstName": "Jens",
      "lastName": "Gerken",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37875,
      "firstName": "Daniela",
      "lastName": "Conti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37876,
      "firstName": "Yusuke",
      "lastName": "Nishimura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37877,
      "firstName": "George",
      "lastName": "Mois",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37878,
      "firstName": "Paul",
      "lastName": "Vogt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37879,
      "firstName": "Tom",
      "lastName": "Ziemke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37880,
      "firstName": "Grace",
      "lastName": "Eden",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37881,
      "firstName": "Martina",
      "lastName": "Mara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37882,
      "firstName": "Anara",
      "lastName": "Sandygulova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37883,
      "firstName": "André",
      "lastName": "Gonçalves",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37884,
      "firstName": "Manuel",
      "lastName": "Giuliani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37885,
      "firstName": "Astrid",
      "lastName": "Rosenthal-von der Pütten",
      "middleInitial": "Marieke",
      "affiliations": []
    },
    {
      "id": 37886,
      "firstName": "Lejla",
      "lastName": "Nukovic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37887,
      "firstName": "Weicong",
      "lastName": "Sng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37888,
      "firstName": "Catharine",
      "lastName": "Oertel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37889,
      "firstName": "Paul",
      "lastName": "Baxter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37890,
      "firstName": "Batuhan",
      "lastName": "Hokelek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37891,
      "firstName": "Vassilis-Javed",
      "lastName": "Khan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37893,
      "firstName": "Maram",
      "lastName": "Sakr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37894,
      "firstName": "Patrícia",
      "lastName": "Arriaga",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37895,
      "firstName": "Federica",
      "lastName": "Ragni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37896,
      "firstName": "Guillaume",
      "lastName": "Vailland",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37897,
      "firstName": "Kenny",
      "lastName": "Chua",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37899,
      "firstName": "Vitezslav",
      "lastName": "Beran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37901,
      "firstName": "Andreas Kornmaaler",
      "lastName": "Hansen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37902,
      "firstName": "Bahar",
      "lastName": "Irfan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37903,
      "firstName": "Kok Hoo",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37904,
      "firstName": "Nils",
      "lastName": "Tolksdorf",
      "middleInitial": "Frederik",
      "affiliations": []
    },
    {
      "id": 37905,
      "firstName": "Minae",
      "lastName": "Kwon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37906,
      "firstName": "Nicole",
      "lastName": "Smith",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 37907,
      "firstName": "Mohamed",
      "lastName": "Chetouani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37908,
      "firstName": "Martin",
      "lastName": "Ross",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 37909,
      "firstName": "Jorre",
      "lastName": "Deschuyteneer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37910,
      "firstName": "Sven",
      "lastName": "Wachsmuth",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37911,
      "firstName": "Ethan",
      "lastName": "Gordon",
      "middleInitial": "Kroll",
      "affiliations": []
    },
    {
      "id": 37912,
      "firstName": "Burcu A.",
      "lastName": "Urgen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37913,
      "firstName": "Emma",
      "lastName": "Peterson",
      "middleInitial": "I. C.",
      "affiliations": []
    },
    {
      "id": 37914,
      "firstName": "Bruno",
      "lastName": "Arnaldi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37915,
      "firstName": "Ruth",
      "lastName": "Stock",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37916,
      "firstName": "Roberto",
      "lastName": "Pagani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37917,
      "firstName": "Iolanda",
      "lastName": "Leite",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37919,
      "firstName": "Verena",
      "lastName": "Klaer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37920,
      "firstName": "Oliver",
      "lastName": "Niebuhr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37921,
      "firstName": "Tiago",
      "lastName": "Guerreiro",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37922,
      "firstName": "Tapomayukh",
      "lastName": "Bhattacharjee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37923,
      "firstName": "Giovanni",
      "lastName": "Corpaccioli",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37924,
      "firstName": "Jaime",
      "lastName": "Fisac",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37925,
      "firstName": "Rey",
      "lastName": "Pocius",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37926,
      "firstName": "Friederike",
      "lastName": "Eyssel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37927,
      "firstName": "Margrit",
      "lastName": "Gelautz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37929,
      "firstName": "Alaa Eldin",
      "lastName": "Abdelaal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37930,
      "firstName": "Yoren",
      "lastName": "Gaffary",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37931,
      "firstName": "Emil",
      "lastName": "Petersen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37932,
      "firstName": "Shih-Yun",
      "lastName": "Lo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37933,
      "firstName": "Abhijeet",
      "lastName": "Agnihotri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37934,
      "firstName": "Maaike",
      "lastName": "Dokter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37935,
      "firstName": "Cailin",
      "lastName": "Hudson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37936,
      "firstName": "Dovini",
      "lastName": "Jayasinghe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37937,
      "firstName": "Lundy",
      "lastName": "Lewis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37940,
      "firstName": "Ulrik",
      "lastName": "Lyngs",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37941,
      "firstName": "Xingkun",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37942,
      "firstName": "Emilyann",
      "lastName": "Nault",
      "middleInitial": "Lacroix",
      "affiliations": []
    },
    {
      "id": 37943,
      "firstName": "Tal",
      "lastName": "Norman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37944,
      "firstName": "Maya",
      "lastName": "Cakmak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37945,
      "firstName": "Rui",
      "lastName": "Prada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37946,
      "firstName": "Liz",
      "lastName": "Salter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37947,
      "firstName": "Annalena",
      "lastName": "Baecker",
      "middleInitial": "Nora",
      "affiliations": []
    },
    {
      "id": 37948,
      "firstName": "Rodrigo",
      "lastName": "Ventura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37949,
      "firstName": "Fumihide",
      "lastName": "Tanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37950,
      "firstName": "Greg",
      "lastName": "Trafton",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37951,
      "firstName": "Matthew",
      "lastName": "Craig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37952,
      "firstName": "Kavita",
      "lastName": "Krishnaswamy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37953,
      "firstName": "De'Aira",
      "lastName": "Bryant",
      "middleInitial": "Gladys",
      "affiliations": []
    },
    {
      "id": 37954,
      "firstName": "Sofia",
      "lastName": "Thunberg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37955,
      "firstName": "Kasper",
      "lastName": "Rodil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37957,
      "firstName": "Mai",
      "lastName": "Otsuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37958,
      "firstName": "Sonja",
      "lastName": "Stange",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37959,
      "firstName": "Yeping",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37960,
      "firstName": "Ravindra",
      "lastName": "de Silva",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37961,
      "firstName": "Jochen",
      "lastName": "Peter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37962,
      "firstName": "Patrícia",
      "lastName": "Alves-Oliveira",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37964,
      "firstName": "Hannah",
      "lastName": "Pelikan",
      "middleInitial": "RM",
      "affiliations": []
    },
    {
      "id": 37965,
      "firstName": "Joonhwan",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37966,
      "firstName": "Ece",
      "lastName": "Dincer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37967,
      "firstName": "Jun",
      "lastName": "Baba",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37968,
      "firstName": "Giovanni",
      "lastName": "Beltrame",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37969,
      "firstName": "Meghan",
      "lastName": "Underhill-Blazey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37970,
      "firstName": "X. Jessie",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37971,
      "firstName": "Ginevra",
      "lastName": "Castellano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37972,
      "firstName": "Joo Cheng",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37973,
      "firstName": "Ilenia",
      "lastName": "Cucciniello",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37975,
      "firstName": "Daniel",
      "lastName": "Camilleri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37976,
      "firstName": "Vincent",
      "lastName": "Page",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37977,
      "firstName": "Simon",
      "lastName": "Manschitz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37978,
      "firstName": "Tim",
      "lastName": "Dierks",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37979,
      "firstName": "Jean-Pierre",
      "lastName": "Gauthier",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37980,
      "firstName": "Nicholas",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37981,
      "firstName": "Rosario",
      "lastName": "Scalise",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37982,
      "firstName": "Rianne",
      "lastName": "van den Berghe",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37983,
      "firstName": "Mathias",
      "lastName": "Wilhelm",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37984,
      "firstName": "Alon",
      "lastName": "Bartal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37985,
      "firstName": "Ivar",
      "lastName": "Solheim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37986,
      "firstName": "Nicole",
      "lastName": "Salomons",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37987,
      "firstName": "Guy",
      "lastName": "Hoffman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37988,
      "firstName": "Asenia",
      "lastName": "Giagtzidou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37989,
      "firstName": "Xiang Zhi",
      "lastName": "Tan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37990,
      "firstName": "Yisrael",
      "lastName": "Parmet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37992,
      "firstName": "Zeyang",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37993,
      "firstName": "Catherine",
      "lastName": "Pelachaud",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37994,
      "firstName": "Ryusuke",
      "lastName": "Mikata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37995,
      "firstName": "Hiroshi",
      "lastName": "Ishiguro",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37996,
      "firstName": "Yanning",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37997,
      "firstName": "Janne Elise",
      "lastName": "Pedersen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37998,
      "firstName": "Hae Won",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 37999,
      "firstName": "Muriel",
      "lastName": "Mignerat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38000,
      "firstName": "Seulki",
      "lastName": "Kyeong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38001,
      "firstName": "Andrew",
      "lastName": "Murtagh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38002,
      "firstName": "Xinyi",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38003,
      "firstName": "Marc",
      "lastName": "Hanheide",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38004,
      "firstName": "Erdem",
      "lastName": "Bıyık",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38005,
      "firstName": "Kathrin",
      "lastName": "Pollmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38006,
      "firstName": "Gopika",
      "lastName": "Ajaykumar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38007,
      "firstName": "Noor",
      "lastName": "Hammad",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38008,
      "firstName": "Sylvain",
      "lastName": "Calinon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38009,
      "firstName": "Yuanchao",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38010,
      "firstName": "Jaime",
      "lastName": "Alvarez Perez",
      "middleInitial": "Augusto",
      "affiliations": []
    },
    {
      "id": 38011,
      "firstName": "Nicholas",
      "lastName": "Timmons",
      "middleInitial": "Gerard",
      "affiliations": []
    },
    {
      "id": 38012,
      "firstName": "Guan",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38013,
      "firstName": "Kate",
      "lastName": "Loveys",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38014,
      "firstName": "Michita",
      "lastName": "Imai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38015,
      "firstName": "Naghmeh",
      "lastName": "Zamani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38016,
      "firstName": "Masahiro",
      "lastName": "Shiomi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38017,
      "firstName": "Zhenyang",
      "lastName": "Luo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38018,
      "firstName": "Alexandre",
      "lastName": "Mazel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38311,
      "firstName": "Phani Tejs",
      "lastName": "S",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38312,
      "firstName": "James",
      "lastName": "Hobin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38313,
      "firstName": "Maira",
      "lastName": "Abid",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38314,
      "firstName": "Benjamin",
      "lastName": "Reynolds",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38315,
      "firstName": "Guilhem",
      "lastName": "Buisan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38316,
      "firstName": "Kathleen",
      "lastName": "Belhassein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38317,
      "firstName": "Anna",
      "lastName": "Fuste",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38318,
      "firstName": "Aurelie",
      "lastName": "Clodic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38319,
      "firstName": "Muhammad",
      "lastName": "Shahrukh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38320,
      "firstName": "Valentin",
      "lastName": "Heun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38321,
      "firstName": "Amandine",
      "lastName": "Mayima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38322,
      "firstName": "Abrar",
      "lastName": "Fallatah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38323,
      "firstName": "Jules",
      "lastName": "Waldhart",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38324,
      "firstName": "Bohkyung",
      "lastName": "Chun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38325,
      "firstName": "Sogol",
      "lastName": "Balali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38326,
      "firstName": "Suleman",
      "lastName": "Shahid",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38327,
      "firstName": "Andrea",
      "lastName": "Braga",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38328,
      "firstName": "Guillaume",
      "lastName": "Sarthou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38363,
      "firstName": "Ewart",
      "lastName": "De Visser",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38364,
      "firstName": "Yanfen",
      "lastName": "You",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38365,
      "firstName": "Virginia",
      "lastName": "Contreras",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38366,
      "firstName": "Eric",
      "lastName": "Vorm",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38367,
      "firstName": "Henny",
      "lastName": "Admoni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38368,
      "firstName": "Gregory",
      "lastName": "Hager",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38369,
      "firstName": "Qiang",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38370,
      "firstName": "Arturo",
      "lastName": "Morales-Tellez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38371,
      "firstName": "Kendrick",
      "lastName": "Umstattd",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38381,
      "firstName": "Stephanie",
      "lastName": "Dinkins",
      "affiliations": []
    },
    {
      "id": 38382,
      "firstName": "Tony",
      "lastName": "Belpaeme",
      "affiliations": []
    },
    {
      "id": 38383,
      "firstName": "James",
      "lastName": "Young",
      "middleInitial": "E",
      "affiliations": []
    },
    {
      "id": 38384,
      "firstName": "Lola",
      "lastName": "Cañamero",
      "affiliations": []
    },
    {
      "id": 38385,
      "firstName": "Ayanna",
      "lastName": "Howard",
      "affiliations": []
    },
    {
      "id": 38390,
      "firstName": "Laurel",
      "lastName": "Riek",
      "affiliations": []
    },
    {
      "id": 38391,
      "firstName": "Hatice",
      "lastName": "Gunes",
      "affiliations": []
    },
    {
      "id": 38393,
      "firstName": "Joey",
      "lastName": "Back",
      "middleInitial": "Sehan",
      "affiliations": []
    },
    {
      "id": 38394,
      "firstName": "Tony",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38395,
      "firstName": "Peter",
      "lastName": "Cheong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38396,
      "firstName": "Seok Hyun",
      "lastName": "An",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38398,
      "firstName": "Abdul",
      "lastName": "Shafiq",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38399,
      "firstName": "Muhammad hakim",
      "lastName": "Irmie yuhazni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 38400,
      "firstName": "Muhammad Syahmi",
      "lastName": "Bin Khairul Anuar",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 84,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 07:46:29+00"
  }
}