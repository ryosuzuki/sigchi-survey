{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10117,
    "shortName": "MobileHCI",
    "displayShortName": "",
    "year": 2024,
    "startDate": 1727654400000,
    "endDate": 1727913600000,
    "fullName": "26th International Conference on Mobile Human-Computer Interaction",
    "url": "https://mobilehci.acm.org/2024",
    "location": "Melbourne, Australia",
    "timeZoneOffset": 600,
    "timeZoneName": "Australia/Melbourne",
    "logoUrl": "https://files.sigchi.org/conference/logo/10117/0b2f4fc4-80b4-e3d9-09a6-937cbbe20de8.png",
    "name": "MobileHCI 2024"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 23,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2024-10-03 08:14:08+00"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10345,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 11755,
      "name": "M01 & M02",
      "setup": "SPECIAL",
      "typeId": 13647,
      "note": ""
    },
    {
      "id": 11757,
      "name": "M03",
      "setup": "SPECIAL",
      "typeId": 13647,
      "note": ""
    },
    {
      "id": 11758,
      "name": "M13 & M14",
      "setup": "SPECIAL",
      "typeId": 13647,
      "note": ""
    },
    {
      "id": 11760,
      "name": "Science Gallery Melbourne / The Oculus",
      "setup": "SPECIAL",
      "typeId": 13642,
      "note": ""
    },
    {
      "id": 11761,
      "name": "Port Melbourne Yacht Club (Port Melb.)",
      "setup": "SPECIAL",
      "typeId": 13642,
      "note": ""
    },
    {
      "id": 11762,
      "name": "The Launchpad, Level M",
      "setup": "SPECIAL",
      "typeId": 13648,
      "note": ""
    },
    {
      "id": 11763,
      "name": "The Forum, Level M",
      "setup": "SPECIAL",
      "typeId": 13644,
      "note": ""
    },
    {
      "id": 11764,
      "name": "Level M",
      "setup": "SPECIAL",
      "typeId": 13642,
      "note": "Right outside The Forum"
    }
  ],
  "tracks": [
    {
      "id": 12992,
      "name": "MobileHCI 2024 Industrial Perspectives",
      "typeId": 13740
    },
    {
      "id": 12993,
      "name": "MobileHCI 2024 Student Design Competition",
      "typeId": 13742
    },
    {
      "id": 12994,
      "name": "MobileHCI 2024 Panels",
      "typeId": 13741
    },
    {
      "id": 12995,
      "name": "MobileHCI 2024 Late-Breaking Work",
      "typeId": 13643
    },
    {
      "id": 12996,
      "name": "MobileHCI 2024 Tutorials",
      "typeId": 13739
    },
    {
      "id": 12997,
      "name": "MobileHCI 2024 Workshops",
      "typeId": 13647
    },
    {
      "id": 12998,
      "name": "MobileHCI 2024 Papers",
      "typeId": 13644
    },
    {
      "id": 12999,
      "name": "MobileHCI 2024 Doctoral Consortium",
      "typeId": 13641
    },
    {
      "id": 13000,
      "name": "MobileHCI 2024 Demos",
      "typeId": 13640
    },
    {
      "id": 13066,
      "typeId": 13644
    }
  ],
  "contentTypes": [
    {
      "id": 13639,
      "name": "Course",
      "displayName": "Courses",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 13640,
      "name": "Demo",
      "displayName": "Demos",
      "color": "#006d2c",
      "duration": 60
    },
    {
      "id": 13641,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13642,
      "name": "Event",
      "displayName": "Events",
      "color": "#ffc034",
      "duration": 0
    },
    {
      "id": 13643,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 60
    },
    {
      "id": 13644,
      "name": "Paper",
      "displayName": "Papers",
      "color": "#0d42cc",
      "duration": 15
    },
    {
      "id": 13645,
      "name": "Poster",
      "displayName": "Posters",
      "color": "#ff7a00",
      "duration": 5
    },
    {
      "id": 13646,
      "name": "Work-in-Progress",
      "displayName": "Works-In-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 13647,
      "name": "Workshop",
      "displayName": "Workshops",
      "color": "#f60000",
      "duration": 240
    },
    {
      "id": 13648,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 13739,
      "name": "Tutorials",
      "color": "#acadb9",
      "duration": 5
    },
    {
      "id": 13740,
      "name": "Industrial Perspectives",
      "color": "#acadb9",
      "duration": 20
    },
    {
      "id": 13741,
      "name": "Panels",
      "color": "#acadb9",
      "duration": 0
    },
    {
      "id": 13742,
      "name": "Student Design Competition",
      "color": "#acadb9",
      "duration": 60
    }
  ],
  "timeSlots": [
    {
      "id": 14262,
      "type": "SESSION",
      "startDate": 1727683200000,
      "endDate": 1727686800000
    },
    {
      "id": 14263,
      "type": "SESSION",
      "startDate": 1727686800000,
      "endDate": 1727692200000
    },
    {
      "id": 14264,
      "type": "BREAK",
      "startDate": 1727692200000,
      "endDate": 1727694000000
    },
    {
      "id": 14265,
      "type": "SESSION",
      "startDate": 1727694000000,
      "endDate": 1727701200000
    },
    {
      "id": 14266,
      "type": "LUNCH",
      "startDate": 1727701200000,
      "endDate": 1727704800000
    },
    {
      "id": 14267,
      "type": "SESSION",
      "startDate": 1727704800000,
      "endDate": 1727710200000
    },
    {
      "id": 14268,
      "type": "BREAK",
      "startDate": 1727710200000,
      "endDate": 1727712000000
    },
    {
      "id": 14269,
      "type": "SESSION",
      "startDate": 1727712000000,
      "endDate": 1727717400000
    },
    {
      "id": 14270,
      "type": "SESSION",
      "startDate": 1727719200000,
      "endDate": 1727730000000
    },
    {
      "id": 14271,
      "type": "SESSION",
      "startDate": 1727769600000,
      "endDate": 1727773200000
    },
    {
      "id": 14272,
      "type": "SESSION",
      "startDate": 1727773200000,
      "endDate": 1727775000000
    },
    {
      "id": 14273,
      "type": "BREAK",
      "startDate": 1727778600000,
      "endDate": 1727780400000
    },
    {
      "id": 14274,
      "type": "SESSION",
      "startDate": 1727780400000,
      "endDate": 1727787600000
    },
    {
      "id": 14275,
      "type": "LUNCH",
      "startDate": 1727787600000,
      "endDate": 1727791200000
    },
    {
      "id": 14276,
      "type": "SESSION",
      "startDate": 1727791200000,
      "endDate": 1727797500000
    },
    {
      "id": 14277,
      "type": "BREAK",
      "startDate": 1727797500000,
      "endDate": 1727798400000
    },
    {
      "id": 14278,
      "type": "SESSION",
      "startDate": 1727798400000,
      "endDate": 1727802000000
    },
    {
      "id": 14279,
      "type": "SESSION",
      "startDate": 1727802000000,
      "endDate": 1727807400000
    },
    {
      "id": 14280,
      "type": "SESSION",
      "startDate": 1727775000000,
      "endDate": 1727778600000
    },
    {
      "id": 14281,
      "type": "SESSION",
      "startDate": 1727856000000,
      "endDate": 1727859600000
    },
    {
      "id": 14283,
      "type": "BREAK",
      "startDate": 1727863200000,
      "endDate": 1727865000000
    },
    {
      "id": 14284,
      "type": "SESSION",
      "startDate": 1727865000000,
      "endDate": 1727868600000
    },
    {
      "id": 14285,
      "type": "SESSION",
      "startDate": 1727868600000,
      "endDate": 1727872200000
    },
    {
      "id": 14286,
      "type": "LUNCH",
      "startDate": 1727872200000,
      "endDate": 1727875800000
    },
    {
      "id": 14287,
      "type": "SESSION",
      "startDate": 1727875800000,
      "endDate": 1727881200000
    },
    {
      "id": 14288,
      "type": "BREAK",
      "startDate": 1727881200000,
      "endDate": 1727882100000
    },
    {
      "id": 14289,
      "type": "SESSION",
      "startDate": 1727882100000,
      "endDate": 1727886600000
    },
    {
      "id": 14290,
      "type": "SESSION",
      "startDate": 1727859600000,
      "endDate": 1727863200000
    },
    {
      "id": 14291,
      "type": "SESSION",
      "startDate": 1727886600000,
      "endDate": 1727889300000
    },
    {
      "id": 14292,
      "type": "SESSION",
      "startDate": 1727892000000,
      "endDate": 1727902800000
    },
    {
      "id": 14293,
      "type": "SESSION",
      "startDate": 1727942400000,
      "endDate": 1727946000000
    },
    {
      "id": 14304,
      "type": "SESSION",
      "startDate": 1727946000000,
      "endDate": 1727951400000
    },
    {
      "id": 14305,
      "type": "BREAK",
      "startDate": 1727951400000,
      "endDate": 1727953200000
    },
    {
      "id": 14306,
      "type": "SESSION",
      "startDate": 1727953200000,
      "endDate": 1727960400000
    },
    {
      "id": 14307,
      "type": "LUNCH",
      "startDate": 1727960400000,
      "endDate": 1727964000000
    },
    {
      "id": 14308,
      "type": "SESSION",
      "startDate": 1727964000000,
      "endDate": 1727967600000
    },
    {
      "id": 14309,
      "type": "SESSION",
      "startDate": 1727967600000,
      "endDate": 1727969400000
    },
    {
      "id": 14310,
      "type": "BREAK",
      "startDate": 1727969400000,
      "endDate": 1727970300000
    },
    {
      "id": 14311,
      "type": "SESSION",
      "startDate": 1727970300000,
      "endDate": 1727978400000
    }
  ],
  "sessions": [
    {
      "id": 169666,
      "name": "Session 1: Privacy in the Virtual and Physical World",
      "isParallelPresentation": false,
      "importedId": "14637",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169510
      ],
      "contentIds": [
        169628,
        169584,
        169596,
        169641,
        169588,
        169614,
        169597,
        169571
      ],
      "source": "SYS",
      "timeSlotId": 14274
    },
    {
      "id": 169667,
      "name": "Session 2: Health and Exercise",
      "isParallelPresentation": false,
      "importedId": "14638",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169464
      ],
      "contentIds": [
        169578,
        169616,
        169640,
        169617,
        169573,
        169634,
        172246
      ],
      "source": "SYS",
      "timeSlotId": 14276
    },
    {
      "id": 169669,
      "name": "Session 3: Accessibility and Support",
      "isParallelPresentation": false,
      "importedId": "14640",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169556
      ],
      "contentIds": [
        169561,
        169562,
        169567,
        169559
      ],
      "source": "SYS",
      "timeSlotId": 14285
    },
    {
      "id": 169670,
      "name": "Session 4: Digital Well-Being",
      "isParallelPresentation": false,
      "importedId": "14641",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169524
      ],
      "contentIds": [
        169568,
        169563,
        169609,
        169589,
        169627,
        169622
      ],
      "source": "SYS",
      "timeSlotId": 14287
    },
    {
      "id": 169671,
      "name": "Session 5: User Perceptions and Attention",
      "isParallelPresentation": false,
      "importedId": "14642",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169301
      ],
      "contentIds": [
        169639,
        169574,
        169626,
        169591,
        169618
      ],
      "source": "SYS",
      "timeSlotId": 14289
    },
    {
      "id": 169672,
      "name": "Session 6: Supporting Navigation",
      "isParallelPresentation": false,
      "importedId": "14643",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169361
      ],
      "contentIds": [
        169599,
        169623,
        169570
      ],
      "source": "SYS",
      "timeSlotId": 14291
    },
    {
      "id": 169673,
      "name": "Session 7: Mobile Input Methods",
      "isParallelPresentation": false,
      "importedId": "14644",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169317
      ],
      "contentIds": [
        169612,
        169642,
        169631,
        169633,
        169613,
        169576
      ],
      "source": "SYS",
      "timeSlotId": 14304
    },
    {
      "id": 169674,
      "name": "Session 8: Augmented and Virtual Reality",
      "isParallelPresentation": false,
      "importedId": "14645",
      "typeId": 13644,
      "roomId": 11763,
      "chairIds": [
        169267
      ],
      "contentIds": [
        169611,
        169594,
        169602,
        169637,
        169638,
        169590,
        169582,
        169624
      ],
      "source": "SYS",
      "timeSlotId": 14306
    },
    {
      "id": 171149,
      "name": "Doctoral Consortium",
      "isParallelPresentation": false,
      "importedId": "14718",
      "typeId": 13641,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [
        169587,
        169629,
        169579,
        169604,
        169592,
        169586
      ],
      "source": "SYS",
      "timeSlotId": 14311
    }
  ],
  "events": [
    {
      "id": 169675,
      "name": "Crossing Cultures - Global Technology Perspectives",
      "isParallelPresentation": false,
      "importedId": "14646",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [
        169270,
        169485
      ],
      "contentIds": [],
      "startDate": 1727775000000,
      "endDate": 1727778600000,
      "link": {},
      "presenterIds": [
        169488,
        169425,
        172779
      ],
      "source": "SYS"
    },
    {
      "id": 169676,
      "name": "Keynote Day 2: Mobile Sign Language Recognition: Creating Useful and Usable Interfaces for the Deaf",
      "isParallelPresentation": false,
      "importedId": "14647",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727859600000,
      "endDate": 1727863200000,
      "link": {
        "href": "https://mobilehci.acm.org/2024/keynotes.php",
        "label": "Keynote website"
      },
      "presenterIds": [
        171238
      ],
      "source": "SYS"
    },
    {
      "id": 171116,
      "name": "Afternoon Break",
      "isParallelPresentation": false,
      "importedId": "14694",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727710200000,
      "endDate": 1727712000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171126,
      "name": "Morning Break",
      "isParallelPresentation": false,
      "importedId": "14695",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727692200000,
      "endDate": 1727694000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171127,
      "name": "Morning Break",
      "isParallelPresentation": false,
      "importedId": "14696",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727951400000,
      "endDate": 1727953200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171128,
      "name": "Morning Break",
      "isParallelPresentation": false,
      "importedId": "14697",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727778600000,
      "endDate": 1727780400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171129,
      "name": "Morning Break",
      "isParallelPresentation": false,
      "importedId": "14698",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727863200000,
      "endDate": 1727865000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171130,
      "name": "Afternoon Break",
      "isParallelPresentation": false,
      "importedId": "14699",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727797500000,
      "endDate": 1727798400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171131,
      "name": "Afternoon Break",
      "isParallelPresentation": false,
      "importedId": "14700",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727881200000,
      "endDate": 1727882100000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171132,
      "name": "Afternoon Break",
      "isParallelPresentation": false,
      "importedId": "14701",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727969400000,
      "endDate": 1727970300000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171133,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "14702",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727787600000,
      "endDate": 1727791200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171134,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "14703",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727872200000,
      "endDate": 1727875800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171135,
      "name": "Lunch and Town Hall",
      "isParallelPresentation": false,
      "importedId": "14704",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727960400000,
      "endDate": 1727964000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171136,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "14705",
      "typeId": 13642,
      "roomId": 11762,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727701200000,
      "endDate": 1727704800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171137,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14706",
      "typeId": 13642,
      "roomId": 11764,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727683200000,
      "endDate": 1727686800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171138,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14707",
      "typeId": 13642,
      "roomId": 11764,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727769600000,
      "endDate": 1727773200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171139,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14708",
      "typeId": 13642,
      "roomId": 11764,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727856000000,
      "endDate": 1727859600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171140,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14709",
      "typeId": 13642,
      "roomId": 11764,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727942400000,
      "endDate": 1727946000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171141,
      "name": "Welcome Reception",
      "isParallelPresentation": false,
      "importedId": "14710",
      "typeId": 13642,
      "roomId": 11760,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727719200000,
      "endDate": 1727730000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171142,
      "name": "Dedicated Poster and Demo Session",
      "isParallelPresentation": true,
      "importedId": "14711",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [
        169595,
        169615,
        169643,
        169593,
        169575,
        169636,
        172792,
        172793,
        172794,
        172795,
        172796,
        172797,
        172798,
        172799,
        172800,
        172801,
        172802,
        172803,
        172804,
        172805,
        172806,
        172807,
        172808,
        172809,
        172810,
        172811
      ],
      "startDate": 1727802000000,
      "endDate": 1727807400000,
      "description": "A longer poster & demo session to allow more time to interact with authors.",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171143,
      "name": "Banquet",
      "isParallelPresentation": false,
      "importedId": "14712",
      "typeId": 13642,
      "roomId": 11761,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727892000000,
      "endDate": 1727902800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171144,
      "name": "Conference Opening / Chair's welcome",
      "isParallelPresentation": false,
      "importedId": "14713",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727773200000,
      "endDate": 1727775000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171145,
      "name": "Minute Madness",
      "isParallelPresentation": true,
      "importedId": "14714",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [
        169663,
        169662,
        169664,
        169661,
        169566,
        169560,
        169625,
        169598,
        169607,
        169630,
        169619,
        169608,
        169580,
        169581,
        169601,
        169583,
        169585,
        169620,
        169565,
        169600,
        169577,
        169645,
        169610,
        169644,
        172819,
        172820,
        172821,
        172822,
        172823,
        172824
      ],
      "startDate": 1727798400000,
      "endDate": 1727802000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171146,
      "name": "Industry perspectives",
      "isParallelPresentation": false,
      "importedId": "14715",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [
        169603,
        169606,
        169621
      ],
      "startDate": 1727865000000,
      "endDate": 1727868600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171147,
      "name": "Keynote Key 3:  Generative AI for Seamless Any Time Any Place Wearable Interaction",
      "isParallelPresentation": false,
      "importedId": "14716",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727964000000,
      "endDate": 1727967600000,
      "link": {
        "href": "https://mobilehci.acm.org/2024/keynotes.php",
        "label": "Keynote website"
      },
      "presenterIds": [
        169410
      ],
      "source": "SYS"
    },
    {
      "id": 171148,
      "name": "Closing",
      "isParallelPresentation": false,
      "importedId": "14717",
      "typeId": 13642,
      "roomId": 11763,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1727967600000,
      "endDate": 1727969400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171232,
      "name": "(W) Affective Computing for Mobile Technologies",
      "isParallelPresentation": false,
      "importedId": "14732",
      "typeId": 13642,
      "roomId": 11755,
      "chairIds": [],
      "contentIds": [
        169569
      ],
      "startDate": 1727686800000,
      "endDate": 1727692200000,
      "link": {
        "href": "https://acimt.github.io/",
        "label": "Workshop website"
      },
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171233,
      "name": "(W) mobiCHAI",
      "isParallelPresentation": false,
      "importedId": "14733",
      "typeId": 13642,
      "roomId": 11758,
      "chairIds": [],
      "contentIds": [
        169564
      ],
      "startDate": 1727686800000,
      "endDate": 1727692200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: mobiCHAI - 1st International Workshop on Mobile Cognition-Altering Technologies (CAT) using Human-Centered AI",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 171234,
      "name": "(W) Designing Age-Inclusive Interfaces",
      "isParallelPresentation": false,
      "importedId": "14734",
      "typeId": 13642,
      "roomId": 11757,
      "chairIds": [],
      "contentIds": [
        169572
      ],
      "startDate": 1727686800000,
      "endDate": 1727692200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: Designing Age-Inclusive Interfaces: Emerging Mobile, Conversational, and Generative AI to Support Interactions across the Life Span",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172780,
      "name": "(W) Affective Computing for Mobile Technologies",
      "isParallelPresentation": false,
      "importedId": "14768",
      "typeId": 13642,
      "roomId": 11755,
      "chairIds": [],
      "contentIds": [
        169569
      ],
      "startDate": 1727704800000,
      "endDate": 1727710200000,
      "link": {
        "href": "https://acimt.github.io/",
        "label": "Workshop website"
      },
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172781,
      "name": "(W) Designing Age-Inclusive Interfaces",
      "isParallelPresentation": false,
      "importedId": "14769",
      "typeId": 13642,
      "roomId": 11757,
      "chairIds": [],
      "contentIds": [
        169572
      ],
      "startDate": 1727704800000,
      "endDate": 1727710200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: Designing Age-Inclusive Interfaces: Emerging Mobile, Conversational, and Generative AI to Support Interactions across the Life Span",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172782,
      "name": "(W) mobiCHAI",
      "isParallelPresentation": false,
      "importedId": "14770",
      "typeId": 13642,
      "roomId": 11758,
      "chairIds": [],
      "contentIds": [
        169564
      ],
      "startDate": 1727704800000,
      "endDate": 1727710200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: mobiCHAI - 1st International Workshop on Mobile Cognition-Altering Technologies (CAT) using Human-Centered AI",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172783,
      "name": "(W) Affective Computing for Mobile Technologies",
      "isParallelPresentation": false,
      "importedId": "14771",
      "typeId": 13642,
      "roomId": 11755,
      "chairIds": [],
      "contentIds": [
        169569
      ],
      "startDate": 1727712000000,
      "endDate": 1727717400000,
      "link": {
        "href": "https://acimt.github.io/",
        "label": "Workshop website"
      },
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172784,
      "name": "(W) Designing Age-Inclusive Interfaces",
      "isParallelPresentation": false,
      "importedId": "14772",
      "typeId": 13642,
      "roomId": 11757,
      "chairIds": [],
      "contentIds": [
        169572
      ],
      "startDate": 1727712000000,
      "endDate": 1727717400000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: Designing Age-Inclusive Interfaces: Emerging Mobile, Conversational, and Generative AI to Support Interactions across the Life Span",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172785,
      "name": "(W) mobiCHAI",
      "isParallelPresentation": false,
      "importedId": "14773",
      "typeId": 13642,
      "roomId": 11758,
      "chairIds": [],
      "contentIds": [
        169564
      ],
      "startDate": 1727712000000,
      "endDate": 1727717400000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: mobiCHAI - 1st International Workshop on Mobile Cognition-Altering Technologies (CAT) using Human-Centered AI",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172786,
      "name": "(W) Affective Computing for Mobile Technologies",
      "isParallelPresentation": false,
      "importedId": "14774",
      "typeId": 13642,
      "roomId": 11755,
      "chairIds": [],
      "contentIds": [
        169569
      ],
      "startDate": 1727694000000,
      "endDate": 1727701200000,
      "link": {
        "href": "https://acimt.github.io/",
        "label": "Workshop website"
      },
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172787,
      "name": "(W) Designing Age-Inclusive Interfaces",
      "isParallelPresentation": false,
      "importedId": "14775",
      "typeId": 13642,
      "roomId": 11757,
      "chairIds": [],
      "contentIds": [
        169572
      ],
      "startDate": 1727694000000,
      "endDate": 1727701200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: Designing Age-Inclusive Interfaces: Emerging Mobile, Conversational, and Generative AI to Support Interactions across the Life Span",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172788,
      "name": "(W) mobiCHAI",
      "isParallelPresentation": false,
      "importedId": "14776",
      "typeId": 13642,
      "roomId": 11758,
      "chairIds": [],
      "contentIds": [
        169564
      ],
      "startDate": 1727694000000,
      "endDate": 1727701200000,
      "link": {
        "href": "https://ai-enhanced-cognition.com/mobichai/",
        "label": "Workshop website"
      },
      "description": "Workshop: mobiCHAI - 1st International Workshop on Mobile Cognition-Altering Technologies (CAT) using Human-Centered AI",
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 169559,
      "typeId": 13644,
      "title": "Haptic2FA: Haptics-Based Accessible Two-Factor Authentication for Blind and Low Vision People",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6968",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169669
      ],
      "eventIds": [],
      "abstract": "Two-factor Authentication (also known as 2FA or two-step verification) is an authentication method that provides an extra layer of protection to ensure online account security. 2FA methods are used along with other primary authentication methods like PINs and Passwords to verify that the person trying to access any digital account is the person they are claiming to be. However, 2FA methods can be inaccessible for blind and low vision (BLV) users due to the requirement of multiple steps, apps, and/or devices for authentication. In addition, it can be a security risk as screen readers may read out the verification codes to bystanders. To address this, we present Haptic2FA, a haptic-based authentication method to improve 2FA accessibility for BLV users. Here, as a part of the 2FA process, the users are sent a `haptic pattern' (similar to a one-time passcode in traditional 2FA methods) that they are required to enter or select for verification. Through a usability study with 10 BLV participants, we evaluated haptic patterns and input methods for the haptic patterns in the Haptic2FA method. Through the findings, we discuss the accessibility and usability of the Haptic2FA method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Information"
            }
          ],
          "personId": 169405
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Information"
            }
          ],
          "personId": 169338
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 169274
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Information "
            }
          ],
          "personId": 169065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Information"
            }
          ],
          "personId": 169538
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": "School of Information"
            }
          ],
          "personId": 169306
        }
      ]
    },
    {
      "id": 169560,
      "typeId": 13643,
      "title": "Understanding Technological Considerations and needs of Nigerians towards community policing engagement An interview-based study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-9053",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Community policing initiatives are increasingly relying on technology to engage stakeholders and improve public safety efforts. Understanding the technological considerations and needs of citizens and police officers is crucial for the effective\r\nimplementation of such initiative in Nigeria. This study investigates perspectives of the Nigerian citizens and police officers on the technological requirements necessary for meaningful participation in community policing activities through qualitative interviews.\r\nFollowing purposive sampling approach and thematic analysis of interview data, this study identifies key technological concerns as well as desired features of mobile surveillance and communication technologies that can enhance community policing participation.\r\nOther findings reveal contrary options in the device visibility choices among the citizens and police. We propose iterative co-design as a better CP technology design approach for users with opposing preferences and detail some design implications that address the specific technology needs of both groups for an enhanced community policing initiative in Nigeria.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "WALES",
              "city": "SWANSEA",
              "institution": "SWANSEA UNIVERSITY",
              "dsl": "COMPUTER SCIENCE DEPARTMENT"
            },
            {
              "country": "Nigeria",
              "state": "ANAMBRA",
              "city": "OKO",
              "institution": "FEDERAL POLYTECHNIC OKO, NIGERIA",
              "dsl": "COMPUTER SCIENCE DEPARTMENT"
            }
          ],
          "personId": 169356
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169372
        }
      ]
    },
    {
      "id": 169561,
      "typeId": 13644,
      "title": "The Ability-Based Design Mobile Toolkit (ABD-MT): Developer Support for Runtime Interface Adaptation Based on Users' Abilities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-3737",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169669
      ],
      "eventIds": [],
      "abstract": "Despite significant progress in the capabilities of mobile devices and applications, most apps remain oblivious to their users' abilities. To enable apps to respond to users' situated abilities, we created the Ability-Based Design Mobile Toolkit (ABD-MT). ABD-MT integrates with an app's user input and sensors to observe a user's touches, gestures, physical activities, and attention at runtime, to measure and model these abilities, and to adapt interfaces accordingly. Conceptually, ABD-MT enables developers to engage with a user's \"ability profile,'' which is built up over time and inspectable through our API. As validation, we created example apps to demonstrate ABD-MT, enabling ability-aware functionality in 91.5% fewer lines of code compared to not using our toolkit. Further, in a study with 11 Android developers, we showed that ABD-MT is easy to learn and use, is welcomed for future use, and is applicable to a variety of end-user scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 169403
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 169318
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 169540
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 169488
        }
      ]
    },
    {
      "id": 169562,
      "typeId": 13644,
      "title": "Where's my TOR?: Evaluating the Effect of Take-Over Request Source on Older Driver's Control Transition in Level 3 Cars",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-5557",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169669
      ],
      "eventIds": [],
      "abstract": "Transitioning to manual control after a Take-Over Request (TOR) is issued by a Level 3 car presents challenges for older drivers. This study investigates if the presentation source of the TOR affects driver performance when resuming control. We measure take-over performance, hazard perception, and user acceptance when the TOR is presented on (1) a smartphone displaying a Non-Driving Related Task (NDRT) simultaneously with the In-Vehicle Information System (IVIS), or (2) presenting the TOR just on the IVIS. Two NDRTs that varied in cognitive demand  were tested with older drivers aged 60–69 and 70+. For the low cognitive demand NDRT, presenting the TOR on the smartphone+IVIS improved takeover performance, hazard perception, and user acceptance, with greater benefits observed in the 70+ group. For the cognitively demanding NDRT, the smartphone+IVIS presentation did not benefit either group of drivers. TOR designers can apply these findings to enhance TORs and assist older drivers in managing control transitions considering the NDRT cognitive demand. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "Glasgow University",
              "dsl": ""
            }
          ],
          "personId": 169547
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "Glasgow University",
              "dsl": ""
            }
          ],
          "personId": 169494
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": " University of Glasgow",
              "dsl": "School of Psychology and Neuroscience"
            }
          ],
          "personId": 169515
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 169535
        }
      ]
    },
    {
      "id": 169563,
      "typeId": 13644,
      "title": "Agent-based Mediation on Smartphone Usage among Co-located Couples",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6128",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Smartphone overuse around family and friends has been shown to be increasing over the past years and often leads to limited one-to-one interaction between co-located individuals. Smartphone-based virtual agents have been shown to be effective for behavior intervention and mediation, such as promoting physical activity. Little is known about leveraging smartphone-based agents to play a role in communication and facilitate conversation between co-located individuals. In this paper, we explore strengthening conversations between co-located couples on phone usage by introducing a smartphone-based agent that acts as a conversation facilitator between them. We contrast the results with a text-based alternative. Our findings suggest that virtual agents serve as a valuable social entity mediating support in couples' communication and relationship dynamics. Through this, we suggest design considerations for this context that leverage the unique qualities of virtual agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia - Okanagan",
              "dsl": ""
            }
          ],
          "personId": 169426
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science, Mathematics, Physics and Statistics"
            }
          ],
          "personId": 169498
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Information"
            }
          ],
          "personId": 169352
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia",
              "dsl": ""
            }
          ],
          "personId": 169324
        }
      ]
    },
    {
      "id": 169564,
      "typeId": 13647,
      "title": "mobiCHAI - 1st International Workshop on Mobile Cognition-Altering Technologies (CAT) using Human-Centered AI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24e-1002",
      "source": "PCS",
      "trackId": 12997,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172782,
        172785,
        172788,
        171233
      ],
      "abstract": "The quest for enhanced cognition has been a driving force behind human advancement, fostering innovation and personal fulfillment. Cognition Altering Technologies (CAT) holds immense promise in elevating the quality of life across diverse domains including education, decision-making, healthcare, and fitness. The current proliferation of Artificial Intelligence (AI), particularly the widespread adoption of Generative AI and foundational models, presents an unprecedented opportunity to prototype new CAT that can augment human capabilities. This workshop aims to unite interdisciplinary research communities to explore the potential of leveraging GenAI and human-centered AI to develop relevant CAT. Taking place at MobileHCI 2024, this one-day workshop invites researchers, practitioners, and designers from fields such as artificial intelligence, ubiquitous computing, human-computer interaction, and social sciences to collaborate and chart the future of cognitive enhancement through technology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Centre for Artificial Intelligence (DFKI)",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169427
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Embedded Intelligence",
              "dsl": "DKFI"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169423
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Graduate School of Media Design"
            }
          ],
          "personId": 169552
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU Kaiserslautern-Landau",
              "dsl": "Center for Cognitive Science"
            }
          ],
          "personId": 169499
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 169301
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU University of Kaiserslautern-Landau",
              "dsl": "Center for Cognitive Science"
            }
          ],
          "personId": 169479
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169410
        }
      ]
    },
    {
      "id": 169565,
      "typeId": 13643,
      "title": "Draw4CM: Detecting Cervical Myelopathy via Hand Drawings Captured by Mobile Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-3590",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Cervical myelopathy (CM), which causes numbness and pain in the hands, can affect manual dexterity and render daily life difficult. The characteristics of this disease are expected to be apparent owing to the necessity of fine motor control for drawing in daily life. Therefore, this study proposed a method to screen for CM by asking patients to draw a figure on a tablet screen using a stylus. The drawing was captured as an image and fed into our machine-learning model to predict CM. The proposed models exhibited sensitivity and specificity comparable to or better than conventional physical methods while avoiding examiner subjectivity and bias. Further, we developed a smartphone application that uses the camera to capture images of figures drawn on paper for CM screening. Moreover, the proposed methods can enable smartphones and tablets to capture disease characteristics and are ubiquitous, thus rendering it easier to reach potential patients.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169481
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169289
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169323
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169415
        }
      ]
    },
    {
      "id": 169566,
      "typeId": 13643,
      "title": "A Touch of Gold - Spraying and Electroplating 3D Prints to Create Biocompatible On-Skin Wearables",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-7396",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Iterative design cycles for tangible user interfaces and wearable devices require efficient prototyping techniques to optimize development and to elevate overall design efficacy. A challenge for rapid prototyping techniques such as cardboard prototyping, 3D printing or laser cutting is the integration of conductive surfaces. Additional wiring, applying electrical paint, or special materials like conductive filament often lack of the neccessary high conductivity and sufficient durability for designing on-skin wearables to measure muscle activity or to electrically stimulate the skin and muscles.We propose to combine spraying and electroplating to create surfaces that exhibit high conductivity, are solderable, corrosion-resistant, and skin-friendly, embodying both practical functionality and aesthetic value. In this paper, we describe an effective spraying and electroplating process for rapid prototyping procedures. We demonstrate its usefulness by several tangible examples, we discuss advantages and disadvantages and describe limitations of the process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169421
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Cooperative Autonomous Systems",
              "dsl": "Karlsruhe Institute of Technology"
            }
          ],
          "personId": 169471
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169313
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169527
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Human Computer Interaction (HCI)",
              "dsl": "Leibniz University Hannover"
            }
          ],
          "personId": 169348
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169514
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169442
        }
      ]
    },
    {
      "id": 169567,
      "typeId": 13644,
      "title": "ChitChatGuide: Conversational Interaction Using Large Language Models for Assisting People with Visual Impairments to Explore a Shopping Mall",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-7579",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169669
      ],
      "eventIds": [],
      "abstract": "To enable people with visual impairments (PVI) to explore shopping malls, it is important to provide information for selecting destinations and obtaining information based on the individual's interests. We achieved this through conversational interaction by integrating a large language model (LLM) with a navigation system. ChitChatGuide allows users to plan a tour through contextual conversations, receive personalized descriptions of surroundings based on transit time, and make inquiries during navigation. We conducted a study in a shopping mall with 11 PVI, and the results reveal that the system allowed them to explore the facility with increased enjoyment. The LLM-based conversational interaction, by understanding vague and context-based questions, enabled the participants to explore unfamiliar environments effectively. The personalized and in-situ information generated by the LLM was both useful and enjoyable. Considering the limitations we identified, we discuss the criteria for integrating LLMs into navigation systems to enhance the exploration experiences of PVI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169331
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169541
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "IBM Research - Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169364
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 169363
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "IBM Research - Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169477
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Miraikan - The National Museum of Emerging Science and Innovation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 169286
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda Research Institute for Science and Engineering",
              "dsl": ""
            }
          ],
          "personId": 169273
        }
      ]
    },
    {
      "id": 169568,
      "typeId": 13644,
      "title": "FacePsy: An Open-Source Affective Mobile Sensing System -- Analyzing Facial Behavior and Head Gesture for Depression Detection in Naturalistic Settings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-1193",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Depression, a prevalent and complex mental health issue affecting  millions worldwide, presents significant challenges for detection and monitoring. While facial expressions have shown promise in laboratory settings for identifying depression, their potential in real-world applications remains largely unexplored due to the difficulties in developing efficient mobile systems. In this study, we aim to introduce FacePsy, an open-source mobile sensing system designed to capture affective inferences by analyzing sophisticated features and generating real-time data on facial behavior landmarks, eye movements, and head gestures -- all within the naturalistic context of smartphone usage with 25 participants. Through rigorous development, testing, and optimization, we identified eye-open states, head gestures, smile expressions, and specific Action Units (2, 6, 7, 12, 15, and 17) as significant indicators of depressive episodes (AUROC=81%). Our regression model predicting PHQ-9 scores achieved moderate accuracy, with a Mean Absolute Error of 3.08. Our findings offer valuable insights and implications for enhancing deployable and usable mobile affective sensing systems, ultimately improving mental health monitoring, prediction, and just-in-time adaptive interventions for researchers and developers in healthcare.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Hoboken",
              "institution": "Stevens Institute of Technology",
              "dsl": "School of Systems and Enterprises"
            }
          ],
          "personId": 169519
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Hoboken",
              "institution": "Stevens Institute of Technology",
              "dsl": "Human-Computer Interaction Lab. AI for Healthcare Lab"
            }
          ],
          "personId": 169383
        }
      ]
    },
    {
      "id": 169569,
      "typeId": 13647,
      "title": "Affective Computing for Mobile Technologies",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24e-1006",
      "source": "PCS",
      "trackId": 12997,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172780,
        172783,
        171232,
        172786
      ],
      "abstract": "Mobile technologies have become integral to daily life, and understanding users' emotional states during interactions is crucial for enhancing user experience. However, integrating affective perception, behavior analysis, and affective computing for mobile technologies presents multifaceted challenges, ranging from technological limitations to ethical considerations. This workshop proposes a collaborative exploration of cutting-edge solutions for affective computing for mobile technologies. We aim to bring together experts to explore topics such as: user behavior analytics, user experience design, affective computing applications, cultural and contextual considerations, and the ethical implementation of affective computing. This workshop aims to bring together researchers and practitioners from both academia and industry to identify and explore: 1) innovative solutions, 2) novel applications, and 3) key challenges in this area to drive research in the coming decade. The long-term goal is to create a strong interdisciplinary research community that includes researchers and practitioners from HCI, HRI, Ubiquitous Computing, Cognitive Psychology, Mobile Technology, Interaction Techniques, User Privacy, and Design.  We envision ongoing research collaborations and accelerating innovations in affective computing for mobile technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Magic Lab, Industrial Engineering and Management"
            }
          ],
          "personId": 169425
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "The School of Electrical Engineering and Telecommunications"
            }
          ],
          "personId": 169467
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": " School of Computing and Information Systems"
            }
          ],
          "personId": 169390
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Rovaniemi",
              "institution": "University of Lapland",
              "dsl": ""
            }
          ],
          "personId": 169422
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Magic Lab, Department of Industrial Engineering and Management"
            }
          ],
          "personId": 169530
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma-shi",
              "institution": "Nara Institute of Science and Technolgy",
              "dsl": "Cybernetics and Reality Engineering Laboratory"
            }
          ],
          "personId": 169505
        }
      ]
    },
    {
      "id": 169570,
      "typeId": 13644,
      "title": "Shock Me The Way: Directional Electrotactile Feedback under the Smartwatch as a Navigation Aid for Cyclists",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4268",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169672
      ],
      "eventIds": [],
      "abstract": "Cycling navigation is a complex and stressful task as the cyclist needs to focus simultaneously on the navigation, the road, and other road users. We propose directional electrotactile feedback at the wrist to reduce the auditory and visual load during navigation-aided cycling. We designed a custom electrotactile grid with 9 electrodes that is clipped under a smartwatch. In a preliminary study we identified suitable calibration settings and gained first insights about a suitable electrode layout. In a subsequent laboratory study we showed that a direction can be encoded with a mean error of 19.28\\,° (\\textsigma~=~42.77\\,°) by combining 2 adjacent electrodes. Additionally, by interpolating with 3 electrodes a direction can be conveyed with a similar mean error of 22.54\\,° (\\textsigma~=~43.57\\,°). We evaluated our concept of directional electrotactile feedback for cyclists in an outdoor study, in which 98.8\\,\\% of all junctions were taken correctly by eight study participants. Only one participant deviated substantially from the optimal path, but was successfully navigated back to the original route by our system.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169421
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169282
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169482
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169457
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Human Computer Interaction (HCI)",
              "dsl": "Leibniz University Hannover"
            }
          ],
          "personId": 169348
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169442
        }
      ]
    },
    {
      "id": 169571,
      "typeId": 13644,
      "title": "Understanding Users' Perspectives on Location Privacy Management on Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6088",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "As the number of applications installed on smartphones continues to grow, the task of effectively managing location privacy has become increasingly complex. In this paper, we explore the factors that influence users' privacy-preserving intentions and contrast them with their actual behaviours. In addition, we compare location privacy concerns across different apps investigating the impact of app-specific features on the willingness to disclose location information. Our findings highlight significant challenges in privacy management due to privacy fatigue and perceived usability. Furthermore, participants raised the importance of more uniform standards regarding location privacy settings across various applications, calling for more detailed and interactive well-informed consent processes that highlight the risks instead of the benefits of disclosing location information. This research contributes important insights towards the development of more effective privacy settings that can foster increased user engagement in managing location privacy on smartphones.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "The University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "The University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 169302
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 169329
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Sydney",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 169377
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "QLD",
              "city": "St Lucia",
              "institution": "The University of Queensland",
              "dsl": "School of Electrical Engineering and Computer Science"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 169257
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": " School of Computing and Information Systems"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": " School of Computing and Information Systems"
            }
          ],
          "personId": 169390
        }
      ]
    },
    {
      "id": 169572,
      "typeId": 13647,
      "title": "Designing Age-Inclusive Interfaces: Emerging Mobile, Conversational, and Generative AI to Support Interactions across the Life Span",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24e-1007",
      "source": "PCS",
      "trackId": 12997,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172787,
        172781,
        172784,
        171234
      ],
      "abstract": "We are concurrently witnessing two significant shifts: voice and chat-based conversational user interfaces (CUIs) are becoming ubiquitous (especially more recently due to advances in generative AI and LLMs - large language models), and older people are becoming a very large demographic group (and increasingly adopting of mobile technology on which such interfaces are pre-sent). However, despite the recent increase in research activity, age-relevant and inter/cross-generational aspects continue to be underrepresented in both research and commercial product design. Therefore, the overarching aim of this workshop is to in-crease the momentum for research within the space of hands-free, mobile, and conversational interfaces that centers on age-relevant and inter- and cross-generational interaction. For this, we plan to create an interdisciplinary space that brings together researchers, designers, practitioners, and users, to discuss and share challenges, principles, and strategies for designing such interfaces across the life span. We thus welcome contributions of empirical studies, theories, design, and evaluation of hands-free, mobile, and conversational interfaces designed with aging in mind (e.g. older adults or inter/cross-generational). We particularly encourage contributions focused on leveraging recent advances in generative AI or LLMs. Through this, we aim to grow the com-munity of CUI researchers across disciplinary boundaries (human-computer interaction, voice and language technologies, geron-to-technologies, information studies, etc.) that are engaged in the shared goal of ensuring that the aging dimension is appropriately incorporated in mobile / conversational interaction design research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Technologies for Aging Gracefully Lab - TAGlab"
            }
          ],
          "personId": 169556
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "Birmingham City University",
              "dsl": "School of Computing and Digital Technology"
            }
          ],
          "personId": 169397
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia",
              "dsl": ""
            },
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Information"
            }
          ],
          "personId": 169498
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Faculty of Information"
            }
          ],
          "personId": 169453
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Lleida",
              "institution": "Universitat de Lleida",
              "dsl": "Computer Science and Digital Design Department"
            }
          ],
          "personId": 169398
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "The University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 169419
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "The University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 169524
        }
      ]
    },
    {
      "id": 169573,
      "typeId": 13644,
      "title": "Exploring Design Opportunities for Improved Self-motivation in Self-tracking and Health Goal Achievement",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4581",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Self-tracking has grown in popularity over recent years, leading to a rise in users who monitor data for improving their health. However, decreasing self-motivation due to different user circumstances such as priorities, lifestyles, and habits affects how users conduct self-tracking in apps and hamper their progress towards achieving health goals. We investigated factors that contribute to increasing and maintaining self-motivation while self-tracking for health goals, as well as the potential applications of these factors in health apps. We conducted semi-structured interviews with 15 participants to gain insights into their self-tracking habits and health app usage. Our thematic analysis suggests that users value convenient self-tracking methods and seeing notable progress changes toward a health goal. These findings reveal design opportunities for streamlining app features, reframing progress indicators in data visualizations, and accommodating user priorities. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Philippines",
              "state": "",
              "city": "Manila",
              "institution": "De La Salle University",
              "dsl": ""
            }
          ],
          "personId": 169341
        },
        {
          "affiliations": [
            {
              "country": "Philippines",
              "state": "",
              "city": "Manila",
              "institution": "De La Salle University",
              "dsl": "Center for Complexity and Emerging Technologies"
            }
          ],
          "personId": 169468
        }
      ]
    },
    {
      "id": 169574,
      "typeId": 13644,
      "title": "“Oh my Dave, your face says it all!”: Exploring the Multimodal Practices of Tasting in Live Streaming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9398",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169671
      ],
      "eventIds": [],
      "abstract": "Research on the activity of tasting, examined through the lens of conversation analysis, reveals that in face-to-face contexts, tasting is an interactive and sequential process that combines individual sensory experience with a public, witnessable, accountable, and intersubjective dimension. This perspective can be extended to the realm of live-streamed tasting, where streamers demonstrate and communicate the taste of food products to online audiences in real time. By adopting multimodal conversation analysis to scrutinize the unfolding sequence moment by moment, my study aims to demonstrate 1) Three practices to achieve the configuration of \"tasting heads,\" wherein the current taster's face is displayed on-screen. 2) Patterns of gaze withdrawal from the screen and subsequent gaze back to the screen during the tasting process. 3) The performative and animated facial expressions. 4) The structure of responses from both streamers and viewers after the tasting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "i3-SES, CNRS, Telecom Paris"
            }
          ],
          "personId": 169438
        }
      ]
    },
    {
      "id": 169575,
      "typeId": 13640,
      "title": "The Atlas of AI Incidents in Mobile Computing: Visualizing the Risks and Benefits of AI Gone Mobile",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-1812",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Today's visualization tools for conveying the risks and benefits of AI technologies are largely tailored for those with technical expertise. To bridge this gap, we have developed a visualization that employs narrative patterns and interactive elements, enabling the broader public to gradually grasp the diverse risks and benefits associated with AI. Using a dataset of 54 real-world incidents involving AI in mobile computing, we examined design choices that enhance public understanding and provoke reflection on how certain AI applications—even those deemed low-risk by law—can still lead to significant incidents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169458
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169173
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 169456
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169461
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169108
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 169459
        }
      ]
    },
    {
      "id": 169576,
      "typeId": 13644,
      "title": "Verifying Finger-Fitts Models for Normalizing Subjective Speed-Accuracy Biases",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6160",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "Previous studies on the Finger-Fitts law (FFitts law) are lacking in sufficient experiments to verify its inherent potential. Since the FFitts law is originally a modified version of the effective width method to normalize speed-accuracy biases, the model fit would improve if multiple biases were mixed together and the throughputs would be more stable than using the nominal target width. In this study, we conduct an experiment in which participants tap 1D-bar and 2D-circular targets under three subjective biases: balancing the speed and accuracy, emphasizing speed, and emphasizing accuracy when they perform the tasks. The results showed that applying the effective width to Ko et al.'s refined FFitts law, which represents the touch ambiguity with a free parameter, was the most successful in normalizing biases. Reanalyzing another dataset on ray-casting pointing also led to the same conclusion. We thus recommend using Ko et al.'s model with effective width when researchers compare several experimental conditions such as devices and user groups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "LY Corporation",
              "dsl": ""
            }
          ],
          "personId": 169299
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chiyoda-ku",
              "institution": "LY Corporation",
              "dsl": ""
            }
          ],
          "personId": 169293
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 169252
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": "Frontier Media Science Program, Graduate School of Advanced Mathematical Sciences, Miyahita Lab"
            }
          ],
          "personId": 169311
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 169478
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 169378
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 169349
        }
      ]
    },
    {
      "id": 169577,
      "typeId": 13643,
      "title": "Exploring the Feasibility of a Repeated Mobile One-Minute PVT In-the-Wild",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-4883",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Researchers have established that quality sleep is important for cognitive functions. Now that consumer-grade wearables can provide accurate measurements of sleep quality, we should be able to inform individuals about how their sleep impacts cognitive functions. However, as our internal clock follows cycles of alertness and sleepiness, it is still necessary to identify at what time of day the sleep quality impacts cognitive performance. In this study, we investigated the relationship between sleep stages and psychomotor vigilance using a 1-minute Psychomotor Vigilance Task (PVT) test. Participants wore a sleep-tracking device and performed the PVT test six times daily for three weeks. The results suggest that increased Rapid Eye Movement (REM) sleep duration led to better performance after lunch. Additionally, we developed a model to predict average response time based on the sleep data. The results show a promising step towards giving tangible meaning to sleep data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169429
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169431
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 169441
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Prefecture University",
              "dsl": ""
            }
          ],
          "personId": 169344
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169346
        }
      ]
    },
    {
      "id": 169578,
      "typeId": 13644,
      "title": "AudioMove: Applying the Spatial Audio to Multi-Directional Limb Exercise Guidance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-5393",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Guiding users with limb exercise can assist muscle training or physical recovery. However, traditional vision-based methods often require multiple camera angles to help users understand the motions and require them to be within the range of the screen. Therefore, we propose a non-visual system that can guide users with multiple-directional limb motions utilizing spatial audio, AudioMove, with the commercial-off-the-shelf (COTS) devices (i.e., smartphones and earphones). The proposed system addresses the challenge of conveying directional information encompassing multiple planes in real time. We conduct a mix-method user study to evaluate the effectiveness of the system with three methods combining motion data with spatial audio perception. Additionally, a user interface is built for collecting users' comments. The results conclude that spatial audio guidance could create a natural, pervasive, and non-visual exercise training solution in daily life.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Xidian University ",
              "dsl": "Guangzhou Institute of Technology"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 169386
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Lifestyle Computing Laboratory"
            }
          ],
          "personId": 169325
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169415
        }
      ]
    },
    {
      "id": 169579,
      "typeId": 13641,
      "title": "Meaningful Interaction with Digital Data in Motion",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-6602",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "The emergence of Augmented Reality (AR) technologies has revolutionized the way information can be presented, offering novel opportunities for data visualization and enhanced interactivity. However, investigating AR concepts outdoors, especially on-the-move, has proven to be a challenging task. Although previous work has examined use cases for using AR for everyday activities in urban environments, most of the contexts explored with people being on-the-move outdoors are in a conceptual stage and these contexts mainly regard AR from a utilitarian perspective. This thesis' goal is to design and evaluate meaningful AR concepts that intend to assist people while on-the-move in a context sensitive and personalized manner, hoping to empower them by leveraging new forms of interaction while merging the virtual and physical world. I aim to contribute an overview of the design space for AR, while people are on-the-move, provide insights for meaningful AR visualizations, and establish design guidelines for such concepts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Puch",
              "institution": "Salzburg University of Applied Sciences",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": ""
            }
          ],
          "personId": 169449
        }
      ]
    },
    {
      "id": 169580,
      "typeId": 13643,
      "title": "Bias-Aware Spoken Conversational Search: A Provocation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-3231",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Spoken Conversational Search (SCS) poses unique challenges in understanding user-system interactions due to the absence of visual cues, and the complexity of less structured dialogue. Tackling the impacts of cognitive bias in today’s information-rich online environment, especially when SCS becomes more prevalent, this paper integrates insights from information science, psychology, cognitive science, and wearable sensor technology to explore potential opportunities and challenges in studying cognitive biases in SCS. It then outlines a framework for experimental designs with various experiment setups to multimodal instruments. It also analyzes data from an existing dataset as a preliminary example to demonstrate the potential of this framework and discuss its implications for future research. In the end, it discusses the challenges and ethical considerations associated with implementing this approach. This work aims to provoke new directions and discussion in the community and enhance understanding of cognitive biases in Spoken Conversational Search.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC ",
              "city": "Melbourne ",
              "institution": "RMIT",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169503
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169360
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169260
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169531
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169485
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169409
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169339
        }
      ]
    },
    {
      "id": 169581,
      "typeId": 13643,
      "title": "DAP 𝄇 : Develop pair-Authentication Protocol with DAP",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-5452",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In today's interconnected world, secure authentication is crucial for both high-security environments and everyday interactions. Traditional authentication methods like passwords and biometrics are designed for individual use, but new challenges emerge in interactive gaming, theme parks, and collaborative virtual reality (VR) where multiple participants must authenticate collectively. This study introduces a multi-person authentication that leverages cooperative actions to enhance security. By analyzing synchronized sensor data from cooperative actions, the system ensures the presence and consent of all participants, making impersonation difficult. We propose a pair authentication using inertial sensors during a complex handshake known as Dignity And Pride (DAP). Our research evaluates the accuracy of pair authentication, the impact of behavioral degradation over time, and resistance to attacks. Experiments with university students demonstrate high authentication accuracy and robustness against time degradation, though vulnerabilities to spoofing attacks were identified, suggesting areas for improvement in secure cooperative authentication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka Univerisity",
              "dsl": ""
            }
          ],
          "personId": 169472
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169353
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169416
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN AIP",
              "dsl": ""
            }
          ],
          "personId": 169475
        }
      ]
    },
    {
      "id": 169582,
      "typeId": 13644,
      "title": "Toward Understanding the Impact of Visualized Focus Levels in Virtual Reality on User Presence and Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9390",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "Neurofeedback refers to the process of feeding a sensory representation of brain activity back to users in real time to improve a particular brain function, e.g., their focus and/or attention on a particular task. This study addressed the notable lack of research on methods used to visualize EEG data and their effects on the immersive quality of VR. We developed an algorithm to quantify focus, yielding a focus score. A pre-study with twenty participants confirmed its effectiveness in distinguishing between focused and relaxed mental states. Subsequently, we used this focus score to prototype a VR experience system visualizing the focus score in preconfigured manners, which was utilized in an exploratory study to assess the impact of different neurofeedback visualization methods on user engagement and focus in VR. Among all the visualization methods evaluated, the environmental scheme stood out due to its superior usability during task execution, its ability to evoke positive emotions through the visualization of objects or scenes, and its minimal deviation from user expectations. Additionally, we explored design guidelines based on collected results for future research to further refine the visualization scheme, ensuring effective integration of the focus score within the VR environment. These enhancements are crucial for designing neurofeedback visualization schemes that aim to boost participant focus in VR settings, offering significant insights into the optimization of such technologies.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": "Graduate Institute of Art and Technology"
            }
          ],
          "personId": 169278
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": ""
            }
          ],
          "personId": 169300
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 169516
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": ""
            }
          ],
          "personId": 169269
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Taiwan University",
              "dsl": "D-School"
            }
          ],
          "personId": 169319
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Department of Information Management"
            }
          ],
          "personId": 169504
        }
      ]
    },
    {
      "id": 169583,
      "typeId": 13643,
      "title": "Cultural influence on RE activities: An extended analysis of state of the art",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-1658",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Designing mobile software that aligns with cultural contexts is crucial for optimizing human-computer interaction. Considering cultural influences is essential not only for the actual set of functional/non-functional requirements, but also for the whole Requirement Engineering (RE) process. Without a clear understanding of cultural influences on RE activities, it's hardly possible to elaborate a correct and complete set of requirements. \r\n This research explores the impact of national culture on RE-related activities based on recent studies. We conducted a Systematic Literature Review (SLR) of studies published in 2019-2023 and compared them to an older SLR covering 2000-2018. We identified 17 relevant studies, extracted 33 cultural influences impacting RE activities, and mapped them to the Hofstede model, widely used for cultural analysis in software development research. Our work highlights the critical role of national culture in RE activities, summarizes current research trends, and helps practitioners consider cultural influences for mobile app/software development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169264
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169345
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169347
        }
      ]
    },
    {
      "id": 169584,
      "typeId": 13644,
      "title": "Delusio - Plausible Deniability For Face Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-1824",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "We developed an Android phone unlock mechanism utilizing facial recognition and specific mimics to access a specially secured portion of the device, designed for plausible deniability. The widespread adoption of biometric authentication methods, such as fingerprint and facial recognition, has revolutionized mobile device security, offering enhanced protection against shoulder-surfing attacks and improving user convenience compared to traditional passwords. However, a downside is the potential for third-party coercion to unlock the device.\r\nWhile text-based authentication allows users to reveal a hidden system by entering a special password, this is challenging with face authentication. We evaluated our approach in a role-playing user study involving 50 participants, with one participant acting as the attacker and the other as the suspect. Suspects successfully accessed the secured area, mostly without detection. They further expressed interest in this feature on their personal phones. We also discuss open challenges and opportunities in implementing such authentication mechanisms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "University of the Bundeswehr Munich",
              "dsl": ""
            }
          ],
          "personId": 169469
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "University of the Bundeswehr Munich",
              "dsl": ""
            }
          ],
          "personId": 169373
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "München",
              "institution": "Ludwig-Maximilans-Universität München",
              "dsl": ""
            }
          ],
          "personId": 169368
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "University of the Bundeswehr Munich",
              "dsl": ""
            }
          ],
          "personId": 169434
        }
      ]
    },
    {
      "id": 169585,
      "typeId": 13643,
      "title": "Exploiting Air Quality Monitors to Perform Indoor Surveillance: Academic Setting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-9658",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Changing public perceptions and government regulations have led to the widespread use of low-cost air quality monitors in modern indoor spaces. Typically, these monitors detect air pollutants to augment the end user's understanding of her indoor environment. Studies have shown that having access to one's air quality context reinforces the user's urge to take necessary actions to improve the air over time. Thus, user's activities significantly influence the indoor air quality. Such correlation can be exploited to get hold of sensitive indoor activities from the side-channel air quality fluctuations. This study explores the odds of identifying eight indoor activities (i.e., enter, exit, fan on, fan off, AC on, AC off, gathering, eating) in a research lab with an in-house low-cost air quality monitoring platform named DALTON. Our extensive data collection and analysis over three months shows 97.7\\% classification accuracy in our dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 169343
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "Cisco Meraki",
              "dsl": "Data Science"
            }
          ],
          "personId": 169297
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur, India",
              "dsl": "CSE"
            }
          ],
          "personId": 169267
        }
      ]
    },
    {
      "id": 169586,
      "typeId": 13641,
      "title": "Post Training Quantization Strategies for Diffusion Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-7853",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "This study addresses the critical need for quantizing audio diffusion models to enable efficient synthesis on low-resource-constrained devices. We propose to design objective audio quality metrics for realism and fidelity, which can be utilized to optimize the denoising process of diffusion models. The current state of our research includes the application of quantization strategies from the image domain to off-the-shelf audio diffusion models such as AudioLDM and Make-an-Audio. The proposed work explores the specific operations in the U-Net architecture that should be quantized, focusing on weights and activations and how to effectively calibrate them. We plan to evaluate our framework on mobile devices to cater to a range of hardware- off the shelf and customized.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "Dept of Computer Science"
            }
          ],
          "personId": 169492
        }
      ]
    },
    {
      "id": 169587,
      "typeId": 13641,
      "title": "Ethical Implications of Pervasive Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-2761",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "As a context-aware and omnipresent technology, Pervasive Augmented Reality (Pervasive AR) is poised to become a significant wearable in our daily lives. While extensive research has focused on the technical capabilities and social acceptance of Pervasive AR devices, there remains a notable gap in understanding their broader impacts. Given the continuous augmentation and ubiquitous nature of this technology, it is crucial to examine its effects on users, bystanders, and society as a whole. This work aims to qualitatively explore the potential ethical implications of Pervasive AR by exposing participants to predetermined scenarios using technology probes. Our findings will inform design recommendations to address identified ethical concerns. I believe the Doctoral Consortium would be an excellent platform to present my work, especially in the context of my upcoming projects, to expert researchers and receive constructive feedback on my PhD research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "North Dunedin",
              "institution": "University of Otago",
              "dsl": "School of Computing"
            }
          ],
          "personId": 169396
        }
      ]
    },
    {
      "id": 169588,
      "typeId": 13644,
      "title": "Unveiling User Perspectives: Exploring Themes in Femtech Mobile App Reviews for Enhanced Usability and Privacy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-5989",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "Femtech, a growing sector in mobile healthcare technology, caters to women's needs across various life stages with digital solutions like period tracking and pregnancy management apps. Maintaining robust data privacy is crucial due to the sensitive nature of the information involved, such as menstrual cycles and pregnancy status. Our research analyzes user feedback from platforms like the Apple App Store and Google Play Store to understand perceptions of Femtech apps, covering accessibility, interface, features, and privacy concerns. Understanding user perspectives helps developers enhance usability and trust, driving further adoption. Prioritizing privacy fosters industry advancement. This paper stresses the importance of dialogue among developers, users, and policymakers in Femtech. Our findings aim to facilitate positive change within the Femtech sector, leading to more inclusive, user-centric, and ethically driven advancements, benefiting both the industry and its users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Milwaukee",
              "institution": "Marquette University",
              "dsl": "Social and Computing Lab"
            }
          ],
          "personId": 169537
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Milwaukee",
              "institution": "Marquette University",
              "dsl": ""
            }
          ],
          "personId": 169263
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Milwaukee",
              "institution": "Marquette University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169528
        }
      ]
    },
    {
      "id": 169589,
      "typeId": 13644,
      "title": "Crafting for Emotion Appropriateness in Affective Robotics: Examining the Practicality of the OCC Model",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-7728",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Research in affective robotics has been using emotions to improve human-robot interaction. One important aspect has been to design recognizable and believable emotions in robotics. Recent work argued that externally displayed emotions on robots may or may not be appropriate for a given situation. However, the selection of emotions as appropriate/inappropriate is not trivial. We here examine the practicality of an established model to craft for emotion appropriateness based on situations of interaction. To do so, we explored the use of the Ortony, Clore, and Collins (OCC) model, which provides a psychological framework of appraisal in which the characteristics of situations are defined and connected to emotions, to identify emotion categories and create contrasting perceptions of emotion appropriateness. We then mapped these categories to four recognizable emotions on aerial robots and designed two video clips (3min35s each) of respectively appropriate and inappropriate emotions. The clips were evaluated in an online study (N=100) where significant differences were found in attitudes toward the robot's emotions. This paper contributes initial findings to designing for emotion appropriateness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Beer Sheva",
              "institution": "Ben-Gurion University of the Negev",
              "dsl": "Magic Lab, Department of Industrial Engineering and Management"
            }
          ],
          "personId": 169530
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Magic Lab, Industrial Engineering and Management"
            }
          ],
          "personId": 169425
        }
      ]
    },
    {
      "id": 169590,
      "typeId": 13644,
      "title": "An Examination of Ultrasound Mid-air Haptics for Enhanced Material and Temperature Perception in Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-1145",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "Rendering realistic tactile sensations of virtual objects remains a challenge in VR. While haptic interfaces have advanced, particularly with phased arrays, their ability to create realistic object properties like state and temperature remains unclear. This study investigates the potential of Ultrasound Mid-air Haptics (UMH) for enhancing the perceived congruency of virtual objects. In a user study with 30 participants, we assessed how UMH impacts the perceived material state and temperature of virtual objects. We also analyzed EEG data to understand how participants integrate UMH information physiologically. Our results reveal that UMH significantly enhances the perceived congruency of virtual objects, particularly for solid objects, reducing the feeling of mismatch between visual and tactile feedback. Additionally, UMH consistently increases the perceived temperature of virtual objects. These findings offer valuable insights for haptic designers, demonstrating UMH's potential for creating more immersive tactile experiences in VR by addressing key limitations in current haptic technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169336
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169315
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169534
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169350
        }
      ]
    },
    {
      "id": 169591,
      "typeId": 13644,
      "title": "Investigating User-perceived Impacts of Contextual Factors on Opportune Moments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9946",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169671
      ],
      "eventIds": [],
      "abstract": "In this exploratory experience sampling method (ESM) research, we examined the perceptions of 74 smartphone users regarding the opportuneness of moments for proceeding through a four-stage notification-response process: the phone generating an alert (Alert), the user roughly glancing at the notification (Glance), engaging with it (Engage), and acting on it (Act). We investigated how the moments perceived as opportune for each of the four stages related to users’ self-reported values of 20 contextual factors, and how these factors influenced users’ perceived opportuneness of the moments for each stage. Our results reveal that Alert and Glance stages were perceived as more distinct, with Alert being influenced by social-environmental related factors and Glance characterized by a lower threshold for what constitutes an opportune moment. The final two stages – Engage and Act – were the most similar to each other. The findings also indicated how the influence of contextual factors on perceived opportuneness of the moments varied across factors, notification types, stages, and how such variation was manifested in the likelihood, valence, and magnitude of their overall influence.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University ",
              "dsl": ""
            }
          ],
          "personId": 169332
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 169443
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 169432
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": " National Yang Ming Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 169333
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169506
        }
      ]
    },
    {
      "id": 169592,
      "typeId": 13641,
      "title": "Towards Enhanced Context Awareness with Vision-based Multimodal Interfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-9086",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "Vision-based Interfaces (VIs) are pivotal in advancing Human-Computer Interaction (HCI), particularly in enhancing context awareness. However, there are significant opportunities for these interfaces due to rapid advancements in multimodal Artificial Intelligence (AI), which promise a future of tight coupling between humans and intelligent systems. AI-driven VIs, when integrated with other modalities, offer a robust solution for effectively capturing and interpreting user intentions and complex environmental information, thereby facilitating seamless and efficient interactions. This PhD study explores three application cases of multimodal interfaces to augment context awareness, respectively focusing on three dimensions of visual modality: scale, depth, and time: a fine-grained analysis of physical surfaces via microscopic image, precise projection of the real world using depth data, and rendering haptic feedback from video background in virtual environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169328
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW",
              "dsl": ""
            }
          ],
          "personId": 169276
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO’s Data61 ",
              "dsl": "Science Director and Deputy Director"
            }
          ],
          "personId": 169270
        }
      ]
    },
    {
      "id": 169593,
      "typeId": 13640,
      "title": "GestureShirt: Exploring Gestures in Front of the Body for Truly Mobile Interaction while Running",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-4461",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Running is the most practiced sport worldwide. Modern technology significantly enhances the running experience. Runners frequently use audio devices, monitoring apps, and wearable sensors like heart rate monitors. Utilizing these technologies while in motion presents unique challenges, as continuous movement can obstruct user interaction. Common devices, such as smartphones, smartwatches, and fitness trackers, are widely employed, but their small interfaces and touch screens often necessitate slowing down for effective use. In this work we explore the space around the body for truly mobile interaction during the run. This demo paper introduces GestureShirt, a novel garment that employs removable distance sensors to explore gestural interaction around the body. Based on an exploratory study (N = 16) we distilled three strategies for utilizing gestures in front of the body for truly mobile interaction. In the demonstration, visitors can explore these strategies through functioning prototypes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Human Computer Interaction Division"
            }
          ],
          "personId": 169444
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": ""
            }
          ],
          "personId": 169501
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Human Computer Interaction Division"
            }
          ],
          "personId": 169271
        }
      ]
    },
    {
      "id": 169594,
      "typeId": 13644,
      "title": "Exploring Redirection and Shifting Techniques to Mask Hand Movements from Shoulder-Surfing Attacks during PIN Authentication in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4573",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "The proliferation of mobile Virtual Reality (VR) headsets shifts our interaction with virtual worlds beyond our living rooms into shared spaces. Consequently, we are entrusting more and more personal data to these devices, calling for strong security measures and authentication. However, the standard authentication method of such devices - entering PINs via virtual keyboards - is vulnerable to shoulder-surfing, as movements to enter keys can be monitored by an unnoticed observer. To address this, we evaluated masking techniques to obscure VR users' input during PIN authentication by diverting their hand movements. Through two experimental studies, we demonstrate that these methods increase users' security against shoulder-surfing attacks from observers without excessively impacting their experience and performance. With these discoveries, we aim to enhance the security of future VR authentication without disrupting the virtual experience or necessitating additional hardware or training of users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169315
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169336
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Center for Machine Learning (MCML)",
              "dsl": ""
            }
          ],
          "personId": 169262
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169251
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169414
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169272
        }
      ]
    },
    {
      "id": 169595,
      "typeId": 13640,
      "title": "AuditNet: Conversational AI Security Assistant",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-8947",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate compliance checking on the go, in diverse domains, including but not limited to network infrastructure, legal contracts, educational standards, environmental regulations, and government policies. By leveraging retrieval-augmented generation using large language models, our framework automates the review, indexing, and retrieval of relevant, context-aware information, streamlining the process of verifying adherence to established guidelines and requirements. This AI assistant not only reduces the manual effort involved in compliance checks but also enhances accuracy and efficiency, supporting professionals in maintaining high standards of practice and ensuring regulatory compliance in their respective fields. We propose and demonstrate AuditNet, the first conversational AI security assistant designed to assist IoT network security experts by providing instant access to security standards, policies, and regulations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Walses",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169424
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169484
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169557
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW Institute for Cyber Security",
              "dsl": "School of Computer Science and Engineering (CSE)"
            }
          ],
          "personId": 169316
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "Cisco",
              "dsl": ""
            }
          ],
          "personId": 169521
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169485
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169497
        }
      ]
    },
    {
      "id": 169596,
      "typeId": 13644,
      "title": "Ghost Readers of the Nile: Decrypting Password Sharing Habits in Chatting Applications among Egyptian Women",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-2992",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "Password sharing is a convenient means to access shared resources, save on subscription costs, provide emergency access, and avoid forgetting vital account details. However, it also raises significant privacy concerns, especially in digital communication contexts where content may be inadvertently exposed to unintended recipients. In this paper, we investigate this duality, using a survey of 86 Egyptian women to understand their sharing behavior and the design and evaluation of a chat application used by 60 participants. This application issues warnings based on content sensitivity, leading to increased user awareness about privacy risks. Our findings indicate that, while many participants initially shared passwords, they were surprised to discover others doing the same. Furthermore, our application effectively reduced password sharing, reflecting improved awareness of associated risks. This research acknowledges the cultural aspects of password sharing while striving to enhance the experience, enabling participants to make informed choices that enhance their information control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technical University inBerlin",
              "dsl": ""
            }
          ],
          "personId": 169256
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Centre for Artificial Intelligence (DFKI)",
              "dsl": ""
            }
          ],
          "personId": 169427
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 169410
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technical University of Ingolstadt",
              "dsl": "Computer Science"
            }
          ],
          "personId": 169310
        }
      ]
    },
    {
      "id": 169597,
      "typeId": 13644,
      "title": "The Impact of Data Privacy on Users' Smartphone App Adoption Decisions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9388",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "Mobile smartphone applications can fuel themselves with a large and diverse set of data. Apps thereby become aware of the user and their context, enabling intelligent and adaptive applications. However such data poses severe privacy risks. Although users are only partially aware of them, awareness increases with the proliferation of privacy-enhancing technologies. This leads to a lower adoption rate of data-heavy smartphone apps, as non-usage often is the user’s only option to protect themselves. How privacy concerns affect app adoption is unclear. Studies researched that privacy concerns are an issue and the lack of sufficient privacy-enhancing technologies lowers app adoption. However, it is unclear which privacy-relevant aspects are mainly responsible for this effect and to what extent it plays a role for users. We conducted a survey (N=100) to investigate the relationship between privacy-relevant app- and publisher characteristics with the users’ intention to install and use it. We found that users are especially critical about contentful datatypes and apps that have rights to perform actions on their behalves. On the other hand, the expectation of a productive benefit induced by the app can increase the app adoption intention. Our findings show which aspects designers of privacy-enhancing technologies should focus on, to meet the demand for more user-centered privacy.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169178
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169317
        }
      ]
    },
    {
      "id": 169598,
      "typeId": 13643,
      "title": "DriveStats: a Mobile Platform to Frame Effective Sustainable Driving Displays",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-2653",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Phone applications to track vehicle information have become more common place, providing insights into fuel consumption, vehicle status, and sustainable driving behaviors. However, to test what resonates with drivers without deep vehicle integration requires a proper research instrument. We built DriveStats: a reusable library (and encompassing a mobile app) to monitor driving trips and display related information. By providing estimated cost/emission reductions in a goal directed framework, we demonstrate how information utility can increase over the course of a 10 day diary study with a group of North American participants. Participants were initially interested in monetary savings reported increased utility for emissions-related information with increased app usage and resulted in self-reported sustainable behavior change. The DriveStats package can be used as a research probe for a plurality of mobility studies (driving, cycling, walking, etc.) for supporting mobile transportation research.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 169326
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169385
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169361
        }
      ]
    },
    {
      "id": 169599,
      "typeId": 13644,
      "title": "Snap&Nav: Smartphone-based Indoor Navigation System For Blind People via Floor Map Analysis and Intersection Detection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-8174",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169672
      ],
      "eventIds": [],
      "abstract": "We present Snap&Nav, a navigation system for blind people in unfamiliar buildings, without prebuilt digital maps. Instead, the system utilizes the floor map as its primary information source for route guidance. The system requires a sighted assistant to capture an image of the floor map, which is analyzed to create a node map containing intersections, destinations, and current positions on the floor. The system provides turn-by-turn navigation instructions while tracking users' positions on the node map by detecting intersections. Additionally, the system estimates the scale difference of the node map to provide distance information. Our system was validated through two user studies with 20 sighted and 12 blind participants. Results showed that sighted participants processed floor map images without being accustomed to the system, while blind participants navigated with increased confidence and lower cognitive load compared to the condition using only cane, appreciating the system's potential for use in various buildings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169502
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169541
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "IBM Research - Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169364
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "IBM Research - Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169477
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Miraikan - The National Museum of Emerging Science and Innovation",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM",
              "dsl": "IBM Research"
            }
          ],
          "personId": 169286
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda Research Institute for Science and Engineering",
              "dsl": ""
            }
          ],
          "personId": 169273
        }
      ]
    },
    {
      "id": 169600,
      "typeId": 13643,
      "title": "Usability Evaluation of a Mobile Application for Foot Health Monitoring of Smart Insoles: A Mixed Methods Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-1481",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Increasing research on intelligent wearable technologies, such as smart insoles for mobile health (mHealth) monitoring, brings new challenges for user-centered design, particularly in data visualization. Research shows various developments in mobile apps for monitoring foot health with smart insoles, while emerging trends like chatbot interactions and improved analytic visualizations offer new opportunities to enhance user experience. However, the usability of various health data visualizations remains unvalidated. A mixed-method experimental user study with 30 participants was conducted to assess the usability of three prototype mHealth applications for smart insoles: Analytical, Basic, and Chatbot visualizations. Quantitative results showed that Basic visualization achieved the highest usability, followed by Analytical, and finally Chatbot. Qualitative feedback supported these findings but also highlighted the potential of Chatbot interactions to enhance data understanding in mHealth apps. We discuss implications for future mHealth applications to monitor foot health and propose design recommendations to improve usability in chatbot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Cadiz",
              "institution": "University of Cadiz",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169391
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169489
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169550
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169454
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169279
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169493
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Cadiz",
              "institution": "University of Cadiz",
              "dsl": ""
            }
          ],
          "personId": 169303
        }
      ]
    },
    {
      "id": 169601,
      "typeId": 13643,
      "title": "MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale Language Model Agents for Multimodal Surface Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-5366",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "For ubiquitous and mobile computing, surface sensing from surroundings is essential for enhancing context-aware interactions between users and devices. However, traditional methods, often based on single-modality type algorithms, overlook potential synergies across various inputs and struggle with interpreting complex multimodal data in dynamic settings, which are crucial for fully understanding contextual factors such as location or behavior. Emerging multimodal large-scale language models offer new opportunities. We propose MultiSurf-GPT, which utilizes the advanced capabilities of GPT-4o to process and interpret diverse modalities (radar, microscope and multispectral data) uniformly based on prompting strategies (zero-shot and  prompting). We preliminarily validated our framework by using MultiSurf-GPT to identify low-level information, and to infer high-level context-aware analytics, demonstrating the capability of augmenting context-aware insights. This framework shows promise as a tool to expedite the development of more complex context-aware applications in the future, providing a faster, more cost-effective, and integrated solution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 169533
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology",
              "dsl": "School of Design"
            }
          ],
          "personId": 169522
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169253
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW",
              "dsl": ""
            }
          ],
          "personId": 169276
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO’s Data61 ",
              "dsl": "Science Director and Deputy Director"
            }
          ],
          "personId": 169270
        }
      ]
    },
    {
      "id": 169602,
      "typeId": 13644,
      "title": "ThermoGrasp: Enabling localized thermal feedback on fingers for precision grasps in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-8092",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "The increasing interest in thermal haptic feedback devices, particularly for virtual reality (VR) applications, highlights the need for more immersive user experiences. However, replicating precise thermal sensations on the fingers remains challenging due to the complexity of finger joints and movements. In this paper, we introduce ThermoGrasp, a novel thermal display designed to enhance VR experiences by providing realistic thermal feedback during precision object grasping. ThermoGrasp is a modular wearable device that targets controlled thermal feedback on the distal phalanges. The implications of designing its VR application were assessed through two experimental studies. The first study focused on the device's ability to accurately convey thermal sensations across different fingers during various precision grasps. The second study investigated the overall haptic experience in VR, examining the impact of thermal feedback on user immersion and realism during interactions with objects of varying temperatures. Participants' subjective responses were analyzed based on factors such as engagement, expressiveness, immersion, realism, and harmony. The findings indicate that precise, localized thermal feedback significantly enhances the VR experience, offering a marked improvement over traditional haptic feedback methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169392
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia",
              "dsl": ""
            }
          ],
          "personId": 169324
        }
      ]
    },
    {
      "id": 169603,
      "typeId": 13740,
      "title": "Stress Testing Gestures for Smartphone System Navigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24i-1004",
      "source": "PCS",
      "trackId": 12992,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171146
      ],
      "abstract": "The Android operating system (OS) started in 2008 with buttons (Home, Back, etc.) to enable users to navigate their smartphone. We describe a novel research methodology used during Android 10’s development of its first gesture based navigation system. Specifically, we describe a methodology that takes inspiration from using wear-and-tear as a measure of the quality of a product. For example, a shoe that can endure more steps is higher in quality than one that can endure fewer steps. We apply this logic to the research methodology to test gestures for smartphone system navigation - the more times users can gesture without discomfort, the higher the ergonomic quality of the gesture. We use this method in conjunction with surveying participants on their stated preferences. The combination of data affords us greater confidence in our conclusions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 169187
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 169436
        }
      ]
    },
    {
      "id": 169604,
      "typeId": 13641,
      "title": "Situated Instructions and Guidance For Self-training and Self-coaching in Sports",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-1722",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "Recent advancements in Virtual Reality (VR) and Augmented Reality (AR) have made remote education immersive and 3D, gaining traction in various fields like medicine, entertainment, education, and engineering. Remote sports training has also gained attention, leading to the development of applications for analyzing movements and improving skills through 3D visualization. However, automatic guidance systems are not well-researched and limitations of existing motion capture setups have been highlighted. In this thesis, we propose to use a combination of video analysis, AR, and activity recognition for remote sports training. Our approach includes two capture modes (egocentric and exocentric) for indoor and outdoor activities and utilizes computer vision-based methods for motion estimation instead of relying on wearable sensors that need expensive facilities. We explore different visualization modes and plan to develop a deep-learning model to provide automatic guidance during training sessions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169381
        }
      ]
    },
    {
      "id": 169605,
      "typeId": 13641,
      "title": "Meta-Origami: An Editable Rigid Tessellation Origami Canvas for User-Defined Shape-Changing Interfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-8475",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "We propose a meta-origami structure in the form of a tessellation canvas, which consist of basic 3D-printing triangle, leveraging to\r\nrelocate the Shape Memory Alloy (SMA) between different units to actuate different parts of the origami. By relocating the SMA fixed\r\ninside each unit, some parts of structure become circuit and triggering the fold motion of the SMA. Basically, our meta-origami structure\r\nbenefits the design in two domains: 1) the abstract shape-changing figure design, which can be achieved different configuration\r\ngeometries by simply cut off the meta-origami canvas. 2) the programmable motion design, people chose three different motion\r\npatterns in the different parts of the interface. In this paper, we provide a computational design tool for users to design their own\r\nshape-changing structure and set up the motion patterns in a specific parts. To the end, we demonstrate three application in the area\r\nof Storytelling tool, wearables, and Robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "School of creative media",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "School of creative media",
              "dsl": ""
            }
          ],
          "personId": 169430
        }
      ]
    },
    {
      "id": 169606,
      "typeId": 13740,
      "title": "Leveraging AI for Improved User Feedback in Mobile Survey Video Prototypes",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24i-1007",
      "source": "PCS",
      "trackId": 12992,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171146
      ],
      "abstract": "This paper explores the transformative impact of artificial intelligence (AI) on user feedback collection, particularly focusing on the transition from traditional textual surveys to video-based prototypes with AI-generated voiceovers. As digital landscapes evolve, the methods for capturing user feedback must adapt to meet changing user preferences and technological capabilities. We propose that integrating AI technology in survey design not only enhances the accessibility and engagement of surveys but also significantly reduces the cognitive load on respondents. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 169149
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 169388
        }
      ]
    },
    {
      "id": 169607,
      "typeId": 13643,
      "title": "MagSerea: Fingerprinting Magnetic Field of Specified Area with Wearable Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-7820",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In environments where GPS is impractical for indoor positioning, magnetic information has emerged as a promising alternative. This study proposes an authentication method called \"MagSerea,\" which utilizes time-series data of three-axis magnetic information captured by a wearable device attached to the arm when opening a door. Experimental results indicate that the door-opening motion restricts the deviations in the device's position and angle, thereby maintaining high identification accuracy despite temporal changes and variations in the user's belongings. These findings suggest that MagSerea demonstrates high identification accuracy in real-world conditions, offering lower installation costs and fewer constraints on reference points compared to existing methods, and thus holds potential for a wide range of applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169259
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169416
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169466
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN AIP",
              "dsl": ""
            }
          ],
          "personId": 169475
        }
      ]
    },
    {
      "id": 169608,
      "typeId": 13643,
      "title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-2973",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "The emergence of large vision models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there's a noticeable gap in structured research applying LVMs to human-robot interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical evaluation was implemented among 15 experts across five evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Robotics, Perception and Learning"
            }
          ],
          "personId": 169452
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": ""
            }
          ],
          "personId": 169529
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Techonology",
              "dsl": ""
            }
          ],
          "personId": 169407
        }
      ]
    },
    {
      "id": 169609,
      "typeId": 13644,
      "title": "WhisperCup: A Design Exploration for Improving the Remote Communication between Chinese Parents and Their Adult Children through Digitally-Augmented Everyday Objects",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6949",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Chinese families usually place a strong emphasis on maintaining a sense of integrality even after the children have grown up. However, this might pose challenges in remote communication. For instance, over-frequent remote communication may disturb each other's life; and because of Chinese people's conservative way of expression, pervasive communication tools, such as video or voice calls, cannot communicate each other's emotions conveniently. To enhance remote communication between Chinese parents and their adult children, we employed a three-stage design process to identify the target users' needs in remote communication and presented WhisperCup. Our study indicated that WhisperCup could unintentionally increase daily communication between Chinese parents and adult children. The awareness system embedded in the WhisperCup prototype could provide a glimpse of each other's daily life, helping virtually engage in each other's life. Additionally, we found that WhisperCup could facilitate a better understanding of each other's lives while addressing privacy concerns.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "School of creative media",
              "dsl": ""
            }
          ],
          "personId": 169430
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": ""
            }
          ],
          "personId": 169455
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University",
              "dsl": "School of Design"
            }
          ],
          "personId": 169525
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "City University, Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169440
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169283
        }
      ]
    },
    {
      "id": 169610,
      "typeId": 13643,
      "title": "DesignWatch: Analyzing Users’ Operations of Mobile Apps Based on Screen Recordings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-8780",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Screen recordings of users' operations to complete tasks in the mobile app are vital resources for designers to assess the app's usability. However, analyzing these recordings at a large scale could be mentally challenging. In this paper, we present DesignWatch, which assists designers in analyzing users’ operations of mobile apps based on collected screen recordings. DesignWatch supports interactive visual analyses of multiple users' operation paths in the app and prompts GPT-4 with vision to simulate users' thoughts during each operation. We conduct expert interviews with four designers, which highlight DesignWatch’s usefulness in helping them quickly understand users' operation patterns in the app, identify the potentially problematic UI design page, and get insights for improving the app design. We conclude with design implications for facilitating usability tests with interactive visualization and generative models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169490
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169354
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169401
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169335
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 169277
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Division of Integrative Systems and Design"
            }
          ],
          "personId": 169296
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169411
        }
      ]
    },
    {
      "id": 169611,
      "typeId": 13644,
      "title": "Augmented Reality on the Move: A Systematic Literature Review for Vulnerable Road Users",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4683",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "Due to the continuous improvement of Augmented Reality (AR) head-mounted displays (HMDs), these devices are bound to be increasingly integrated into our daily routines. So far, a major focus of AR research has been on indoor usage and deployment. However, since seamlessly supporting users in their activities while being on-the-move in various outdoor contexts becomes increasingly important, there is a need to investigate the current state-of-the-art of AR technologies while people are in motion outdoors. Therefore, we conducted a systematic literature review of pertinent HCI publications, specifically looking into applications concerning vulnerable road users. We identify the contexts in which such technologies have been researched, prevailing challenges in the field, and applied methodological approaches. Our findings show that most contributions address pedestrians, a shift towards HMDs, and a prevalence of lab studies due to technology limitations. Based on our findings, we discuss trends, existing gaps and opportunities for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Puch",
              "institution": "Salzburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169449
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Puch",
              "institution": "Salzburg University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169412
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Human Computer Interaction Division"
            }
          ],
          "personId": 169271
        }
      ]
    },
    {
      "id": 169612,
      "typeId": 13644,
      "title": "Hand Grips and Mobile Menus: Exploring Perceived Usability and User Preferences ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-5893",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "This paper investigates the relationship between menu design and hand positions in relation to the assessment of end users with main focus on usability, user preference, and potential adaptions to different hand positions. Sixteen (N = 16) participants first participated in a co-design workshop, in which they proposed menu designs for different hand grips. Based on the design proposals, a selection of menu designs were derived and implemented in a mobile app prototype, on which a menu selection study was conducted to investigate performance and perceived usability of the menus in one-handed and two-handed interaction. The results include user ratings and performance, which highlight the need for mobile menus to be adapted for different hand positions. Based on that, we derive design recommendations for more adaptive, user-centric and ergonomic mobile menu designs to match the natural interactions of users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 169465
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 169487
        }
      ]
    },
    {
      "id": 169613,
      "typeId": 13644,
      "title": "Head ’n Shoulder: Gesture-Driven Biking Through Capacitive Sensing Garments to Innovate Hands-Free Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-3397",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "Distractions caused by digital devices are increasingly causing dangerous situations on the road, particularly for more vulnerable road users like cyclists. While researchers have been exploring ways to enable richer interaction scenarios on the bike, safety concerns are frequently neglected and compromised. In this work, we propose Head ’n Shoulder, a gesture-driven approach to bike interaction without affecting bike control, based on a wearable garment that allows hands- and eyes-free interaction with digital devices through integrated capacitive sensors. It achieves an average accuracy of 97% in the final iteration, evaluated on 14 participants. Head ’n Shoulder does not rely on direct pressure sensing, allowing users to wear their everyday garments on top or underneath, not affecting recognition accuracy. Our work introduces a promising research direction: easily deployable smart garments with a minimal set of gestures suited for most bike interaction scenarios, sustaining the rider’s comfort and safety.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169357
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 169508
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": " Berlin,",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169255
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169258
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169413
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 169410
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169418
        }
      ]
    },
    {
      "id": 169614,
      "typeId": 13644,
      "title": "Exploring Users' Mental Models and Privacy Concerns During Interconnected Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-8569",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "Users frequently use their smartphones in combination with other smart devices, for example, when streaming music to smart speakers or controlling smart appliances. During these interconnected interactions, user data gets handled and processed by several entities that employ different data protection practices or are subject to different regulations. Users need to understand these processes to inform themselves in the right places and make informed privacy decisions. We conducted an online survey (N = 120) to investigate whether users have correct mental models about interconnected interactions. We found that users consider scenarios more privacy-concerning when multiple devices are involved. Yet, we also found that most users do not fully comprehend the privacy-relevant processes in interconnected interactions. Our results show that current privacy information methods are insufficient and that users must be better educated to make informed privacy decisions. Finally, we advocate for restricting data processing to the app layer and better encryption to reduce users’ data protection responsibilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Center for Machine Learning (MCML)",
              "dsl": ""
            }
          ],
          "personId": 169539
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169284
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Center for Machine Learning (MCML)",
              "dsl": ""
            }
          ],
          "personId": 169317
        }
      ]
    },
    {
      "id": 169615,
      "typeId": 13640,
      "title": "Demonstrating TOM: A Development Platform for Wearable Intelligent Assistants in Daily Activities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-4752",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Advanced wearable digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to improve users' abilities. However, developing these assistants presents several challenges. To address this, we introduce TOM (The Other Me), a conceptual architecture and open-source software platform (https://github.com/TOM-Platform) that supports the development of wearable intelligent assistants that are contextually aware of both the user and the environment. Collaboratively developed with researchers and developers, TOM meets their diverse requirements. TOM facilitates the creation of intelligent assistive AR applications for daily activities and supports the recording and analysis of user interactions, integration of new devices, and the provision of assistance for various activities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Smart Systems Institute, Synteraction lab"
            }
          ],
          "personId": 169433
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169369
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 169483
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "NUS-HCI Lab, Smart Systems Institute"
            }
          ],
          "personId": 169342
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "NUS",
              "dsl": ""
            }
          ],
          "personId": 169395
        }
      ]
    },
    {
      "id": 169616,
      "typeId": 13644,
      "title": "EarMonitor: Non-clinical Assesment of Ear Health Conditions Using a Low-cost Endoscope Camera on Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-8125",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Hearing loss affects 20% of people worldwide and is dramatically increasing with global aging. Early prevention and identification of ear disease can significantly reduce the risk of becoming disabled with hearing impairment. We propose EarMonitor, an interactive\r\nvision-based ear health monitoring system that enables users to examine their ear conditions with a low-cost hand-held endoscope. EarMonitor can detect six ear health conditions suitable for self-assessment, especially can recognize ear disease complications and support users to better understand the results. In the wild, our computer vision algorithm achieves a detection sensitivity of 0.949 for earwax buildup and blockage in 100 external auditory canal photos; our deep learning model achieves an average detection sensitivity of 0.861 for the other five conditions considering complications in 350 tympanic membrane photos. We validated EarMonitor’s effectiveness through a user study involving 17 participants and two experts, leading to valuable insights regarding the design and interpretation of non-clinical assessment devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "ASR",
              "institution": "The Hong Kong University Science and Technology",
              "dsl": "Academy of Interdisciplinary Studies"
            }
          ],
          "personId": 169304
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Division of Integrative Systems and Design"
            }
          ],
          "personId": 169402
        }
      ]
    },
    {
      "id": 169617,
      "typeId": 13644,
      "title": "“I Believe the Baby in the Picture is My Baby”: User Experiences with Commercial Pregnancy Apps",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4121",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Commercial pregnancy apps are becoming popular in mobile health and integral to health management ecosystems. For that, they can complement medical advice and be conveniently used for ubiquitous tracking of pregnancy. Besides their functional and medical purpose, they may elicit subjective, personal, and intimate experiences that are equally relevant to users. Yet, these qualitative aspects of experiencing pregnancy apps remain under-researched. An inquiry into those qualitative aspects may help advance the design of pregnancy apps for improved user embodiment, engagement, and experience. Here, we qualitatively inquire about experiences with six popular pregnancy apps through 4,000+ online reviews. Our findings reveal that pregnancy apps are more than mere trackers and can impact pregnancy experiences, either positively or negatively, based on their design features. Further, reviews pointed to a neglect of family, friends, and relatives in the apps' design, which users found often problematic. To counter these shortcomings, we outline avenues for improving the design of pregnancy apps beyond usability and medical outcomes and call for enhancing their design through more sensitive, user-centered, and inclusive design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 169532
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Somerville",
              "institution": "Northeastern University",
              "dsl": "Arts, Media, and Design"
            }
          ],
          "personId": 169275
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 169314
        }
      ]
    },
    {
      "id": 169618,
      "typeId": 13644,
      "title": "\"I Want Lower Tone for Work-Related Notifications\": Exploring the Effectiveness of User-Assigned Notification Alerts in Improving User Speculation of and Attendance to Mobile Notifications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6661",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169671
      ],
      "eventIds": [],
      "abstract": "Research indicates that smartphone users often speculate about notifications upon sensing their arrival, aiding their decision to attend to them. This speculation, however, relies on the presence of sufficient clues to associate with the notification, which are not always available. To address this challenge, through an experience sampling study, we investigated the effectiveness of delivering user-assigned alerts in influencing users' speculation accuracy, attendance effectiveness, and perceived disturbance. Our findings suggest that while user-assigned alerts enhanced the accuracy of speculation and improved participants’ decisions to attend to notifications, the increased notification awareness sometimes led participants to view their decision to ignore notifications as less favorable. Moreover, we found that sporadic alert delivery disrupted the association between the alert and the notification, leading to no reduction in perceived disturbance nor improvement in speculation accuracy. In assigning alerts to notifications, participants considered five strategies: familiarity, distinctiveness, disturbance, emotional resonance, and dimension representation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 169305
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": " Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Institute of Multimedia Engineering"
            }
          ],
          "personId": 169292
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169404
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 169450
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 169420
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Institute of Communication Studies"
            }
          ],
          "personId": 169506
        }
      ]
    },
    {
      "id": 169619,
      "typeId": 13643,
      "title": "Eyes-free Circular Gestures on Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-6965",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Smartphones are used in various situations, such as when users have limited visual focus, such as when walking or driving. Eyes-free gestures offer a way to interact with smartphones without requiring visual attention. This research delves into circular eyes-free gestures and elucidates the advantages they offer over other types of gestures in facilitating eyes-free interaction. We carried out two experiments to explore the ability of participants to accurately draw arcs with varying angles in a smartphone's eyes-free context. The results of the first experiment revealed that participants commonly tended to exceed the intended arc lengths, regardless of whether they were drawing arcs clockwise or counterclockwise. The results of the second experiment showed that there is a high variation in drawing eye-free circular gestures by the same user. However, this variation decreases if the second gesture is produced immediately after the first one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Lille University",
              "dsl": ""
            }
          ],
          "personId": 169408
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Valenciennes",
              "institution": "Université Polytechnique Hauts-de-France",
              "dsl": "LAMIH UMR CNRS 8201"
            }
          ],
          "personId": 169463
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussel",
              "institution": "Université catholique de Louvain",
              "dsl": ""
            }
          ],
          "personId": 169495
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Villeneuve d'Ascq",
              "institution": "university of lille",
              "dsl": ""
            }
          ],
          "personId": 169439
        }
      ]
    },
    {
      "id": 169620,
      "typeId": 13643,
      "title": "To Touch, or Not to Touch: Evaluating Manual Page Turning Modalities for Digital Sheet Music During Piano Play",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-8506",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "The widespread adoption of digital sheet music by performers in recent years appears promising, yet not without limitations. While enhancing interactivity and convenience, a persistent challenge remains: the act of turning pages while musicians' hands are engaged on the piano keys. To investigate perceived user experience of page turning in digital sheet music, we conducted a controlled laboratory experiment with fifteen participants (N~=~15), comparing five state-of-the-art page turning modalities. \r\nThe results revealed that hands-free modalities were generally preferred over touch-based modalities, with head gestures showing promise as hands-free alternatives to foot pedals, particularly in terms of perceived efficiency, attractiveness, stimulation, and novelty.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciendes Upper Austria",
              "dsl": "Digital Media Lab"
            }
          ],
          "personId": 169487
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 169406
        }
      ]
    },
    {
      "id": 169621,
      "typeId": 13740,
      "title": "Designing GPS: A User Engagement Module for Mobile Software Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24i-1012",
      "source": "PCS",
      "trackId": 12992,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171146
      ],
      "abstract": "In a collaboration between Rexuniversal S.L. in Marbella (Spain) and Universidad Carlos III de Madrid, we have designed a mobile application module called GPS that enhances user engagement by benefiting from gamification, personalization, and social networking. The module also considers other user engagement elements, such as interactivity, design quality, privacy, content quality, and utility. It also provides a personalization algorithm that adapts to user preferences. This paper presents the proposed module fundamentals and its design and prototyping process. We have also elaborated on how academic and industrial evaluation was used to evaluate the prototype design using key design and prototyping aspects of usability, aesthetics, functionality, satisfaction, and engagement. The initial findings of the evaluation of the GPS module showed strengths in visual design and engagement but indicated areas for improvement in navigation, consistency, and information presentation. An industrial evaluation with expert feedback confirmed the module’s potential and provided actionable insights for refinement. Software designers can benefit from the insights of this study for engineering and designing engaging mobile applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Leganés",
              "institution": "Universidad Carlos III de Madrid ",
              "dsl": "Computer Science Department "
            }
          ],
          "personId": 169387
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Marbella",
              "institution": "REXUNIVERSAL S.L.",
              "dsl": ""
            }
          ],
          "personId": 169294
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Marbella",
              "institution": "REXUNIVERSAL S.L.",
              "dsl": ""
            }
          ],
          "personId": 169555
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "University Carlos III of Madrid",
              "dsl": "Computer Science  and Engineering Department"
            }
          ],
          "personId": 169266
        }
      ]
    },
    {
      "id": 169622,
      "typeId": 13644,
      "title": "Good Intentions, Risky Inventions: A Method for Assessing the Risks and Benefits of AI in Mobile and Wearable Uses",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9770",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Integrating Artificial Intelligence (AI) into mobile and wearables offers numerous benefits at individual, societal, and environmental levels. Yet, it also spotlights concerns over emerging risks. Traditional assessments of risks and benefits have been sporadic, and often require costly expert analysis. We developed a semi-automatic method that leverages Large Language Models (LLMs) to identify AI uses in mobile and wearables, classify their risks based on the EU AI Act, and determine their benefits that align with globally recognized long-term sustainable development goals; a manual validation of our method by two experts in mobile and wearable technologies, a legal and compliance expert, and a cohort of nine individuals with legal backgrounds who were recruited from Prolific, confirmed its accuracy to be over 85%. We uncovered that specific applications of mobile computing hold significant potential in improving well-being, safety, and social equality. However, these promising uses are linked to risks involving sensitive data, vulnerable groups, and automated decision-making. To avoid rejecting these risky yet impactful mobile and wearable uses, we propose a risk assessment checklist for the Mobile HCI community.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169173
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169458
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169461
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169108
        }
      ]
    },
    {
      "id": 169623,
      "typeId": 13644,
      "title": "DriveR: Towards Generating a Dynamic Road Safety Map with Causal Contexts",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9693",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169672
      ],
      "eventIds": [],
      "abstract": "Road safety remains a critical global concern, with millions of crashes reported annually. Understanding the safety of individual road junctions is vital, especially in areas prone to road rage and reckless driving. However, current navigation systems lack detailed safety information, increasing risk for drivers and pedestrians. Recognizing this need, this paper introduces \\ourmethod{} that automatically annotates the road segments with a driving safety level to aid cautious maneuvering and safe driving practices. By leveraging onboard sensors, \\ourmethod{} identifies \\textit{causal chains} behind poor driving maneuvers, enabling the modeling of safety levels for various road segments. We perform a thorough evaluation of \\ourmethod{} over publicly available and collected datasets from multiple countries and observe $>80\\%$ accuracy (in terms of F1-score) in correctly annotating the safety concerns. In addition, a thorough user study indicates the generalizability and usability of the proposed approach for its practical deployment considerations. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "KHARAGPUR",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 169287
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur, India",
              "dsl": "CSE"
            }
          ],
          "personId": 169267
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 169312
        }
      ]
    },
    {
      "id": 169624,
      "typeId": 13644,
      "title": "Experience from Designing Augmented Reality Browsing Interfaces for Real-world Walking Scenarios",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9054",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "Mobile phones have enabled users to browse information in varying mobility contexts. For high-mobility settings such as walking, however, phones pose several usability challenges, particularly safety and limited screen sizes.  While Augmented Reality (AR) has been proposed to address these issues, prior work has yet to investigate AR interface design in real-world walking conditions beyond text readability and notification design. This paper presents the first exploration of AR browsing interface design and extended usage while walking in the wild. We first conducted design sessions with 12 UI designers while walking in varied environments to design the window size, distance, opacity, anchor type, and placement for three categories of apps: text, video, and mixed content. Results show that traffic level significantly affects the designed window size, whereas content type significantly affects window size, distance, opacity, and vertical placement. To gain further insights from real-world usage, we conducted a multi-day observational study with 5 participants and observed that participants on average switched among window layouts every 3.3 minutes, for reasons such as safety and the level of extended visual attention.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169523
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169321
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169320
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 169536
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169473
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169435
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 169437
        }
      ]
    },
    {
      "id": 169625,
      "typeId": 13643,
      "title": "\"What's this?\": Understanding User Interaction Behaviour with Multimodal Input Information Retrieval System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-2124",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Human communication relies on integrated multimodal channels to facilitate rich information exchange. Building on this foundation, researchers have long speculated about the potential benefits of incorporating multimodal input channels into conventional information retrieval (IR) systems to support users' complex daily IR tasks more effectively. However, the true benefits of such integration remain uncertain. This paper presents a series of exploratory pilot tests comparing Multimodal Input IR (MIIR) with Unimodal Input IR (UIIR) across various IR scenarios, concluding that MIIR offers distinct advantages over UIIR in terms of user experiences. Our preliminary results suggest that MIIR could reduce the cognitive load associated with IR query formulation by allowing users to formulate different query-component in a unified manner across different input modalities, particularly when conducting complex exploratory search tasks in unfamiliar, in-situ contexts. The discussions stemming from this finding draw scholarly attention and suggest new angles for designing and developing MIIR systems.\r\n\r\n ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Synteraction Lab"
            }
          ],
          "personId": 169544
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore ",
              "institution": "National University of Singapore",
              "dsl": "Synteraction Lab"
            }
          ],
          "personId": 169351
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Smart Systems Institute, Synteraction lab"
            }
          ],
          "personId": 169433
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 169291
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Ho Chi Minh City",
              "institution": "VNUHCM - University of Science",
              "dsl": ""
            }
          ],
          "personId": 169448
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169369
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "School of Electronics and Computer Science"
            }
          ],
          "personId": 169379
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Ho Chi Minh City",
              "institution": "University of Science, VNUHCM",
              "dsl": ""
            }
          ],
          "personId": 169382
        }
      ]
    },
    {
      "id": 169626,
      "typeId": 13644,
      "title": "Design and Perception of a Soft Shape Change Beneath a Smartwatch",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-6140",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169671
      ],
      "eventIds": [],
      "abstract": "In this paper, we explore the design of a watch that can deliver notifications through shape changes, specifically focusing on changes in curvature at the back of the watch face. We explain our design choices and challenges while creating such a watch. We conducted an experimental study to determine this novel form of feedback's absolute detection threshold (ADT). We compared the ADT of two watches with a back face that can change curvature and contact the wearer's wrist to notify them. These two watches exhibit different shapes when inflated with high air pressure. We conducted a standard two-down, one-up adaptive staircase procedure to determine the ADT. Our findings show that an ADT of 3.86 psi is required to inflate the back surface for detection by participants. Overall, our qualitative findings indicate that participants enjoyed this novel type of feedback and could feel different sensations with each watch.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Martin-d'Hères",
              "institution": "Laboratoire d'Informatique de Grenoble/CNRS",
              "dsl": "Université Grenoble-Alpes"
            }
          ],
          "personId": 169295
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "CNRS",
              "dsl": ""
            }
          ],
          "personId": 169554
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": ""
            }
          ],
          "personId": 169340
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "CNRS",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": ""
            }
          ],
          "personId": 169290
        }
      ]
    },
    {
      "id": 169627,
      "typeId": 13644,
      "title": "Rehab-Diary: Enhancing Recovery Identity with an Online Support Group for Middle Aged and Older Ovarian Cancer Patients",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-2060",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169670
      ],
      "eventIds": [],
      "abstract": "Ovarian cancer presents significant well-being challenges for middle-aged and older women. Recent research underscores the vital role of recovery identity in predicting wellbeing. However, a research gap exists regarding the influence of online support groups (OSPs) on identity synthesis for middle-aged and older cancer patients. This study introduces \"Rehab-Diary,\" a mobile age-friendly OSP grounded in The Social Identity Model of Identity Change, aimed at helping ovarian cancer patients foster recovery identity. A four-week randomized controlled trial involving 68 participants assessed the OSP's impact. The interface was tailored for ease of use by older individuals. The findings demonstrate the feasibility of utilizing Rehab-Diary among older individuals. The intervention effectively enhanced recovery identity. This study offers evidence-based insights for developing future age-friendly online support interventions, ultimately enhancing ovarian cancer patients' quality of care.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "--- Select One ---",
              "city": "Shanghai",
              "institution": "Donghua University",
              "dsl": "Schools of Fashion & Art Design"
            },
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 169512
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 169359
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "School of Design"
            }
          ],
          "personId": 169322
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "--- Select One ---",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "School of Media and Communication"
            }
          ],
          "personId": 169375
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": ""
            }
          ],
          "personId": 169308
        }
      ]
    },
    {
      "id": 169628,
      "typeId": 13644,
      "title": "Medusa3D: The Watchful Eye Freezing Illegitimate Users in Virtual Reality Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-1607",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "The remarkable growth of Virtual Reality (VR) in recent years has extended its applications beyond entertainment to sectors including education, e-commerce, and remote communication. Since VR devices contain user's private information, user authentication becomes increasingly important. Current authentication systems in VR, such as password-based or static biometric-based methods, are either cumbersome to use or vulnerable to attacks such as shoulder surfing. To address these limitations, we propose Medusa3D, a challenge-response authentication system for VR based on reflexive eye responses. Unlike existing methods, reflexive eye responses are involuntary and effortless, offering a secure and user-friendly credential for authentication. We implement Medusa3D on an off-the-shelf VR and conduct evaluations with 25 participants. The evaluation results show that Medusa3D achieves 0.21% FAR and 0.13% FRR, demonstrating high security under various ocular conditions and resilience against attacks such as zero-effort attack, replay attack, and mimicry attack. A user study indicates that Medusa3D is user-friendly and well-adopted among participants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169265
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169330
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169520
        }
      ]
    },
    {
      "id": 169629,
      "typeId": 13641,
      "title": "Exploring Visual Discomfort and Opportunities for Vision Augmentations: Visual Noise Cancellation and Head-worn LCD Light Actuators for Perception Modulation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24c-9497",
      "source": "PCS",
      "trackId": 12999,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        171149
      ],
      "eventIds": [],
      "abstract": "Recently, Head-Mounted Displays have shown great potential for augmenting vision beyond traditional visual aids, such as compensating for complex visual impairments or providing superhuman vision to the unimpaired eye. Example applications addressed colour vision deficiency or emphasised visual cues for people with low vision. However, Visual Discomfort or Visual Noise and how to address it in vision augmentations are less explored, which is surprising given that acoustic noise control or cancellation is widely applied in modern audio headphones. In my PhD, we explore the area of Visual Noise and Discomfort through a series of studies. Specifically, we explore the general problem of what represents Visual Discomfort and Visual Noise, prototypically explore the first prototypes to receive feedback on the concept of Visual Noise cancellation, including novel prototypes for vision augmentation using LCD light actuators, also review the literature for the current state of vision augmentation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169476
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169337
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "University of Copenhagen",
              "dsl": ""
            }
          ],
          "personId": 169394
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Otago",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 169268
        }
      ]
    },
    {
      "id": 169630,
      "typeId": 13643,
      "title": "Train Me: Exploring Mobile Sports Capture and Replay for Immersive Sports Coaching",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-5877",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In recent years, a wide variety of instructional materials and applications have been developed to enhance athletes’ learning in different sports. The amount of instructional videos, in particular, has increased significantly, though these often impose a high cognitive load as users must map displayed instructions to their actions and body movements. Mobile Augmented Reality (AR) interfaces can reduce this burden by presenting instructional information directly where it is needed. This paper explores a mobile sports capture and replay approach for immersive self-training, aiming to help users improve their skills without needing coaches on-site. We investigate different capturing methods, including an Exocentric capturing method, to enhance instructor mobility and flexibility. Using the captured data, we visualize sports training instructions in an immersive 3D environment on an AR headset. We propose three visualization methods and create a first prototype that allows us to explore the approach’s feasibility across different sports.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169381
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169510
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169337
        }
      ]
    },
    {
      "id": 169631,
      "typeId": 13644,
      "title": "Exploring User-Defined Gestures as Input for Hearables and Recognizing of Ear-Touch Gestures by IMUs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-2975",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "Hearables are highly functional earphone-type wearables; however, existing input methods using stand-alone hearables are limited in the number of commands, and there is a need to extend device operation through hand gestures. In previous research on hearables for hand input, user understanding and gesture recognition systems have been developed. However, in the realm of user understanding, exploration remains incomplete concerning hand input with hearables, and extant recognition systems have not demonstrated proficiency in discerning user-defined gestures.\r\nIn this study, we conducted a gesture elicitation study (GES) assuming hand input using hearables under six conditions (three interaction areas × two device shapes). Then, we extracted ear-touch gestures that the device's built-in IMU sensor could recognize from the user-defined gestures and investigated recognition performance. The results of the experiments in a sitting experiment showed that the gesture recognition rate for in-ear devices was 91.0%, and for ear-hook devices was 74.7%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169307
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Lifestyle Computing Lab"
            }
          ],
          "personId": 169546
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama City",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169366
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 169511
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169415
        }
      ]
    },
    {
      "id": 169633,
      "typeId": 13644,
      "title": "Studying the Simultaneous Visual Representation of Microgestures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-7704",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "Hand microgestures are promising for mobile interaction with wearable devices.\r\nHowever, they will not be adopted if practitioners cannot communicate to users the microgestures associated with the commands of their applications. \r\nThis requires unambiguous representations that simultaneously shows the multiple microgestures available to control an application. \r\nUsing a systematic approach, we evaluate how these representations should be designed and contrast 4 conditions depending on the microgestures (tap-swipe and tap-hold) and fingers considered (index and index-middle).\r\nBased on the results, we design a simultaneous representation of microgestures for a given set of 14 application commands. \r\nWe then evaluate the usability of the representation for novice users and the suitability of the representation for small screens compared with a baseline.\r\nFinally, we formulate 8 recommendations based on the results of all the experiments.\r\nIn particular, redundant graphical and textual representations of microgestures should only be displayed for novice users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": "LIG, Grenoble Computer Science Laboratory"
            }
          ],
          "personId": 169130
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": "LIG, Grenoble Computer Science Laboratory"
            }
          ],
          "personId": 169553
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Univ. Lille, Inria, CNRS, Centrale Lille, UMR 9189 CRIStAL",
              "dsl": ""
            }
          ],
          "personId": 169507
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Université Grenoble Alpes",
              "dsl": "LIG, Grenoble Computer Science Laboratory"
            }
          ],
          "personId": 169389
        }
      ]
    },
    {
      "id": 169634,
      "typeId": 13644,
      "title": "DIY Digital Interventions: Behaviour Change with Trigger-Action Programming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-5325",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": " Whether it is sleep, diet, or procrastination, changing behaviours can be challenging. Individuals could design and build their own personalised digital interventions to help them reach their goals, but little is known about this process. Building upon previous research we propose the Behaviour Change with Trigger-Action Programming (BC-TAP) model which describes how individuals could bridge the gap between their current and desired behaviour through the creation of `Do-It-Yourself' (DIY) digital interventions. We conducted a two-day participatory workshop based on the BC-TAP model with 28 participants. Participants articulated plans to change a behaviour of their choice and represented these plans in mobile device automations. After using their interventions for up to three weeks, participants reflected on their experience. Our findings report opportunities and challenges at each stage of the process. While formulating a digital proxy for certain behaviours was challenging, both failures and successes facilitated participants’ awareness of their behaviour, and their ability to change it.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "London",
              "city": "London",
              "institution": "UCL",
              "dsl": "UCLIC"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Institute of Cognitive Neuroscience"
            }
          ],
          "personId": 169464
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "UCL Interaction Centre"
            }
          ],
          "personId": 169358
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Nijmegen",
              "institution": "Radboud University",
              "dsl": "iHub and Digital Security, iCIS"
            }
          ],
          "personId": 169542
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL",
              "dsl": "UCLIC"
            }
          ],
          "personId": 169480
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 169491
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "HCI"
            }
          ],
          "personId": 169486
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": ""
            }
          ],
          "personId": 169451
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL ",
              "dsl": "UCLIC"
            }
          ],
          "personId": 169545
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Neuchâtel",
              "institution": "University of Neuchâtel",
              "dsl": ""
            }
          ],
          "personId": 169334
        }
      ]
    },
    {
      "id": 169636,
      "typeId": 13640,
      "title": "Head ’n Shoulder: Gesture-Driven Biking Through Capacitive Sensing Garments to Innovate Hands-Free Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-3037",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Distractions caused by digital devices are increasingly causing dangerous situations on the road, particularly for more vulnerable road users like cyclists. While researchers have been exploring ways to enable richer interaction scenarios on the bike, safety concerns are frequently neglected and compromised. In this work, we propose Head ’n Shoulder, a gesture-driven approach to bike interaction without affecting bike control, based on a wearable garment that allows hands- and eyes-free interaction with digital devices through integrated capacitive sensors. It achieves an average accuracy of 97% in the final iteration, evaluated on 14 participants. Head ’n Shoulder does not rely on direct pressure sensing, allowing users to wear their everyday garments on top or underneath, not affecting recognition accuracy. Our work introduces a promising research direction: easily deployable smart garments with a minimal set of gestures suited for most bike interaction scenarios, sustaining the rider’s comfort and safety.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169357
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 169508
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": " Berlin,",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169255
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169258
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169413
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 169410
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169418
        }
      ]
    },
    {
      "id": 169637,
      "typeId": 13644,
      "title": "Body-Based Augmented Reality Feedback During Conversations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-3660",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "Engaging with our devices as we engage with each other is problematic as it distracts us and diminishes our social interactions. Subtle interactions have been presented as an approach to reconcile personal and computing interactions, through less disrupting technology. Along those lines, we investigate showing information right on and next to the people we are engaging with. Body-based data visualization allows us to maintain our attention with others, but to also receive information at the same time. We explore potential designs of such body-based and especially on-face visualizations and create a set of five prototype visualizations in a Snapchat lens. We use these prototypes in a video call study with 16 participants to evaluate how body-based visualizations affect actual conversations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 169548
        }
      ]
    },
    {
      "id": 169638,
      "typeId": 13644,
      "title": "Understanding the Impact of the Reality-Virtuality Continuum on Visual Search using Physiological Measures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-7226",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169674
      ],
      "eventIds": [],
      "abstract": "While Mixed Reality allows the seamless blending of digital content in their surroundings, it is not clear if such a fusion of digital and physical information impacts users' perceptual and cognitive resources differently. While the fusion of real and virtual objects provides numerous opportunities to present additional information, it also introduces undesirable side effects, such as split attention and increased visual complexity. We conducted a visual search study in three manifestations of mixed reality to understand the effects of the environment on visual search behavior. We conducted a multimodal evaluation using EEG and eye-tracking correlates of search efficiency, distractor suppression, attention allocation, and behavioral measures. We found that, independently of the perceptual load, Augmented Reality environments reduce users' capacity to identify target information and suppress irrelevant stimuli. Participants reported AR as more demanding and distracting. We discuss design implications for MR interfaces based on physiological inputs for adaptive interactions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169069
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 169280
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Lancaster ",
              "institution": "Lancaster University ",
              "dsl": "Computing and Communications"
            }
          ],
          "personId": 169261
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Lancaster",
              "institution": "Lancaster University",
              "dsl": "Computing and Communications"
            }
          ],
          "personId": 169551
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169254
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169526
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 169309
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Center for Machine Learning (MCML)",
              "dsl": ""
            }
          ],
          "personId": 169317
        }
      ]
    },
    {
      "id": 169639,
      "typeId": 13644,
      "title": "Developing a Group-Based Literacy Screening for German Pre-Readers: A Digital, Game-Based Approach",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-8115",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169671
      ],
      "eventIds": [],
      "abstract": "Early prediction of children's literacy skills is crucial for successful literacy development. However, standardized screenings for pre-readers are mainly paper-based and designed for one-on-one sessions, demanding significant resources. \r\n  We present the development and feasibility evaluation of a digital, game-based literacy screening for German pre-readers that supports group sessions. The screening comprises five tasks that do not rely on written language skills. We detail critical design decisions and guidelines for the effective implementation of this group-based screening. \r\n  We evaluated the feasibility and user experience with 34 German second- and third-graders. Results revealed that the screening is suitable for use in group settings and that it was positively perceived by the children. Children found the tasks enganging and straightforward, often perceiving them as games. This study demonstrates that digital game-based screenings can be used effectively in group settings with young children with minimal adult guidance, offering a motivating and engaging assessment method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ludwigsburg",
              "institution": "Ludwigsburg University of Education",
              "dsl": "Institute of Computer Science"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "LEAD Graduate School & Research Network "
            }
          ],
          "personId": 169327
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ludwigsburg",
              "institution": "Ludwigsburg University of Education",
              "dsl": "Institute of Computer Science"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "LEAD Graduate School & Research Network"
            }
          ],
          "personId": 169447
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "Universität Tübingen",
              "dsl": "Department of Theoretical Computational Linguistics"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "LEAD Graduate School & Research Network"
            }
          ],
          "personId": 169362
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "University of Graz",
              "dsl": "Institute of Psychology"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Tuebingen",
              "institution": "University of Tuebingen",
              "dsl": "LEAD Graduate School and Research Network"
            }
          ],
          "personId": 169281
        }
      ]
    },
    {
      "id": 169640,
      "typeId": 13644,
      "title": "Informing the Design of Intervention Solutions for Body-Focused Repetitive Behaviors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-1044",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Body-Focused Repetitive Behaviors (BFRBs), such as nail biting, impact a wide demographic, and can negatively affect physical, psychological, and social well-being. Although pharmacological and behavioral therapies are common treatments, many avoid seeking help, and not everyone responds fully to treatment. Recent advances in wearable sensing enable new digital solutions that can detect BFRB episodes and intervene to mitigate them. While BFRBs have been extensively studied in medical research, translating this knowledge into effective digital intervention solutions may not be straightforward, and the end user’s perspective may be overlooked. We report a user study with 12 frequent nail biters, who shared their experiences about nail biting and expectations of intervention solutions in semi-structured qualitative interviews. We describe the progression of a nail biting episode from a nail biter's perspective and present a taxonomy of intervention strategies to mitigate nail biting. Our results inform the design of future digital BFRB intervention solutions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169500
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169399
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169285
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Nokia Technologies",
              "dsl": ""
            }
          ],
          "personId": 169371
        }
      ]
    },
    {
      "id": 169641,
      "typeId": 13644,
      "title": "Privacy Slider: Fine-Grain Privacy Control for Smartphones ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-4198",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169666
      ],
      "eventIds": [],
      "abstract": "Today, users are constrained by binary choices when configuring permissions. These binary choices contrast with the complex data collected, limiting user control and transparency. For instance, weather applications do not need exact user locations when merely inquiring about local weather conditions. We envision sliders to empower users to fine-tune permissions. First, we ran two online surveys (N=123 & N=109) and a workshop (N=5) to develop the initial design of Privacy Slider. After the implementation phase, we\r\nevaluated our functional prototype using a lab study (N=32). The results show that our slider design for permission control outperforms today’s system concerning all measures, including control and transparency.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169178
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169365
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169317
        }
      ]
    },
    {
      "id": 169642,
      "typeId": 13644,
      "title": "Using Pupil Dilation to Adaptively Select Speed-Reading Parameters in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24d-9441",
      "source": "PCS",
      "trackId": 12998,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169673
      ],
      "eventIds": [],
      "abstract": "Rapid Serial Visual Presentation (RSVP) improves the reading speed for optimizing the user's information processing capabilities on Virtual Reality (VR) devices. Yet, the user's RSVP reading performance changes over time while the reading speed remains static. In this paper, we evaluate pupil dilation as a physiological metric to assess the mental workload of readers in real-time. We assess mental workload under different background lighting and RSVP presentation speeds to estimate the optimal color that discriminates the pupil diameter varying RSVP presentation speeds. We discovered that a gray background provides the best contrast for reading at various presentation speeds. Then, we conducted a second study to evaluate the classification accuracy of mental workload for different presentation speeds. We find that pupil dilation relates to mental workload when reading with RSVP. We discuss how pupil dilation can be used to adapt the RSVP speed in future VR applications to optimize information intake.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Center for Machine Learning (MCML)",
              "dsl": ""
            }
          ],
          "personId": 169262
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 169445
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "HU Berlin",
              "dsl": ""
            }
          ],
          "personId": 169393
        }
      ]
    },
    {
      "id": 169643,
      "typeId": 13640,
      "title": "Geofence-to-Conversation: Hierarchical Geofencing for Augmenting City Walks with Large Language Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24b-3948",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "This study presents a geofence-based service architecture for city-wide audio augmented reality, tailored for the era of large language models. Traditional geofencing mechanisms, which monitor user entry to geofences, struggle to provide continuous storytelling in areas with few points of interest, degrading the audio tour experiences for pedestrians. Our proposed geofencing architecture consistently incorporates complex and multilayered city features, enabling seamless audio tour experiences. Furthermore, this paper introduces prompt engineering for generating entertaining guide scripts for large language models, that is, the geofence-to-conversation technique. The mobile application developed for the actual field demonstrates the feasibility of our proposed architecture and highlights future challenges in enhancing users’ interaction with a city.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169518
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169517
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169549
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169417
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169400
        }
      ]
    },
    {
      "id": 169644,
      "typeId": 13643,
      "title": "EmoFoot: Can Your Foot Tell How You Feel when Playing Virtual Reality Games?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-8930",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Understanding feelings in Virtual Reality (VR) games is vital for enhancing engagement in human-computer interaction. Traditional methods for assessing emotions and player experience, both subjective and objective, often fall short of capturing players' comprehensive and nuanced experiences. This preliminary study introduces EmoFoot, a novel approach leveraging foot pressure sensors to decode player feelings during VR gameplay. We show the diverse patterns of VR game experiences using subjective reports focused on immersion, competence, negative and positive affect, flow, tension, challenge, and engagement. By integrating smart insoles, our research investigates the potential of using foot pressure data to identify valence and arousal levels. We use Machine Learning models to discover how players' feet can reveal their emotions. EmoFoot aims to introduce a seamless and unobtrusive method for monitoring player experience, contributing to immersive technology by enhancing our understanding of the objective indicators of player emotions and improving the overall gaming experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "Data61, CSIRO",
              "dsl": ""
            }
          ],
          "personId": 169513
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169509
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "nsw",
              "city": "sydney",
              "institution": "unsw",
              "dsl": ""
            }
          ],
          "personId": 169384
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science & Engineering"
            }
          ],
          "personId": 169370
        }
      ]
    },
    {
      "id": 169645,
      "typeId": 13643,
      "title": "Enhancing Mobile Interaction: Practical Insights from Smartphone and Smartwatch Integration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24f-9107",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In the realm of mobile technology, smartwatches have emerged as valuable complements to smartphones, offering unique features such as enhanced activity tracking and in-situ data analysis. This study delves into the synergistic relationship between smartwatches and smartphones through practical applications developed by university students. We guided three groups of students from a computer-related program to design and develop cross-mobile device applications tailored to their daily contexts. Our evaluation of these student-developed applications revealed several key insights. Firstly, there was a strong preference for using smartwatches for straightforward notification purposes, indicating their convenience and immediacy in delivering critical information. However, their effectiveness as extended displays was found to depend heavily on a well-crafted approach to information delivery, necessitating careful design considerations. The study also highlighted the complexities inherent in designing intuitive and efficient cross-device interaction gestures, underlining the importance of thoughtful gesture selection and customization to enhance user experience. Furthermore, our findings indicated that users found it unnecessary to use smartwatches for voice input when paired with smartphones, as a single smartphone was deemed sufficient for this purpose.This research provides valuable insights into enhancing cross-device interaction, aiming to foster more seamless and user-focused integration of technology into daily life. These insights can guide developers in creating more effective and user-friendly applications, ultimately contributing to the broader field of human-computer interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169355
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169496
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169474
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169470
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169428
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169367
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi‘an jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169380
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169288
        }
      ]
    },
    {
      "id": 169661,
      "typeId": 13742,
      "title": "Express Yourself Simply: Mobile AI for Bridging Communication Gaps",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24h-1005",
      "source": "PCS",
      "trackId": 12993,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Doodle Me is an AI mobile application designed to bridge communication gaps and barriers. This tool is beneficial for individuals with articulation and phonological speech disorders. By using advanced machine learning algorithms, particularly a Convolutional Neural Network (CNN), the application interprets user-created drawings in real-time. This app provides a universal mode of expression that transcends traditional language barriers. The CNN model is trained on the QuickDraw dataset which achieves an accuracy of 92.6%.\r\n\r\nThe application is accessible and practical, functioning seamlessly across all smartphones. Doodle Me enhances communication by recognizing and categorizing doodles, and provides visual and auditory feedback to users. This tool provides a holistic approach by aiding at rehabilitation of numerous medical conditions. It deals with multilinguistics of different age groups and is also eco-friendly.\r\n\r\nThe application serves as an assistive tool in the healthcare system. It can support a wide range of medical conditions, including post-ischemic stroke aphasia, facial paralysis, stuttering/stammering, Autism Spectrum Disorder (ASD), post-operative laryngectomy or vocal cord paralysis, Alzheimer’s disease, and rehabilitation for post-cochlear implant patients (where children are taught about speech recognition by giving visual cues) and elderly with presbycusis (hearing loss at high frequencies).\r\n\r\nDoodle Me addresses communication barriers and promotes biosustainability by reducing the need for unnecessary stationary items. It is a practical application aimed at making a positive societal impact by facilitating seamless integration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Cork",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 169650
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": "SFI CRTAI"
            }
          ],
          "personId": 169656
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 169659
        }
      ]
    },
    {
      "id": 169662,
      "typeId": 13742,
      "title": "By the Fire: A Children’s Game for Promoting Hani Culture",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24h-1003",
      "source": "PCS",
      "trackId": 12993,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "The Hani ethnic group, which is mainly distributed in China, has produced many distinctive cultural traditions throughout its history. However, the lack of a writing system and the impact of modern society have caused some Hani cultural practices to face decline or even extinction. By the Fire: Hani's Epic is a game made for mobile tablets, such as an iPad. It integrates important events and culturally significant items in the history of the Hani people into the gameplay. The game combines card-based and minigame elements to capitalize on the curiosity of children aged 6-10, leading to increased appreciation and understanding of the Hani culture. The use of physical accessories also enhances the game immersion and strengthens the reflective experience outside the game, culminating in a greater understanding of Hani culture for both children and their parents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangzhou Academy of Fine Arts",
              "dsl": ""
            }
          ],
          "personId": 169646
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Guangzhou Academy of Fine Arts",
              "dsl": ""
            }
          ],
          "personId": 169647
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "Guangzhou Academy  of Fine Arts",
              "dsl": ""
            }
          ],
          "personId": 169653
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": ""
            }
          ],
          "personId": 169652
        }
      ]
    },
    {
      "id": 169663,
      "typeId": 13742,
      "title": "CN-T9: Optimization of T9 Chinese Input Layout",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24h-1011",
      "source": "PCS",
      "trackId": 12993,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "This paper presents a novel T9 keyboard layout designed specifically for Chinese users, considering the influence of the Pinyin system. The traditional T9 layout, originally developed for English typing, is inefficient for Chinese input, leading to a higher cognitive load and reduced typing speed. Our proposed layout optimizes key placement based on the frequency and distribution of Pinyin syllables, incorporating intuitive methods for inputting tone markers. By minimizing keystrokes and enhancing ergonomic design, the new layout aims to improve typing efficiency and comfort. We conducted comprehensive evaluations comparing the new layout with traditional methods, showing significant improvements in input speed and user satisfaction. This research highlights the importance of tailoring keyboard designs to specific language requirements, contributing to the broader field of human-computer interaction and providing a better typing experience for Chinese users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169655
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169660
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169657
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169649
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169648
        }
      ]
    },
    {
      "id": 169664,
      "typeId": 13742,
      "title": "Distancebit: Designing a Technology Probe to Envision AR Glasses Enhancing Embodied Cross-Cultural Social Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "mobilehci24h-1000",
      "source": "PCS",
      "trackId": 12993,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In cross-cultural social interactions, understanding non-verbal cues is as crucial as language. Distancebit is a critical design that reflects on how personal computing devices influence these interactions and explores the evolution of Augmented Reality (AR) in enhancing cross-cultural social experiences. Using a speculative design approach, we envision a future where everyone uses AR glasses. Our design focuses on differing interpersonal space (IPS) preferences in contact and non-contact cultures, helping users recognize culturally appropriate IPS and signal discomfort to the outside world when violated. Prototype evaluations indicated that Distancebit increased participant engagement and reflection on cross-cultural interactions. We also discuss cultural diversity, embodiment in cross-cultural settings, biosignal visualization, and human-technology relations, envisioning an future AR wearable device being more seamlessly embedded into the socio-cultural fabric, by communicating user's information to the outside environment, thereby enhancing understanding and social interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Informatics and Media"
            }
          ],
          "personId": 169654
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Informatics and Media"
            }
          ],
          "personId": 169658
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Informatics and Media"
            }
          ],
          "personId": 169651
        }
      ]
    },
    {
      "id": 172246,
      "typeId": 13644,
      "title": "Visual Noise Cancellation: Exploring Visual Discomfort and Opportunities for Vision Augmentations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 13066,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        169667
      ],
      "eventIds": [],
      "abstract": "Acoustic noise control or cancellation (ANC) is a commonplace component of modern audio headphones. ANC aims to actively mitigate disturbing environmental noise for a quieter and improved listening experience. ANC is digitally controlling frequency and amplitude characteristics of sound. Much less explored is visual noise and active visual noise control, which we address here. We first explore visual noise and scenarios in which visual noise arises based on findings from four workshops we conducted. We then introduce the concept of visual noise cancellation (VNC) and how it can be used to reduce identified effects of visual noise. In addition, we developed head-worn demonstration prototypes to practically explore the concept of active VNC with selected scenarios in a user study. Finally, we discuss the application of VNC, including vision augmentations that moderate the userÕs view of the environment to address perceptual needs and to provide augmented reality content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Otago",
              "institution": "University of Otago"
            }
          ],
          "personId": 172242
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Otago",
              "institution": "University of Otago"
            }
          ],
          "personId": 172243
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Otago",
              "institution": "University of Otago"
            }
          ],
          "personId": 172244
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "city": "Otago",
              "institution": "University of Otago"
            }
          ],
          "personId": 172245
        }
      ]
    },
    {
      "id": 172792,
      "typeId": 13643,
      "title": "A Touch of Gold - Spraying and Electroplating 3D Prints to Create Biocompatible On-Skin Wearables",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14780",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Iterative design cycles for tangible user interfaces and wearable devices require efficient prototyping techniques to optimize development and to elevate overall design efficacy. A challenge for rapid prototyping techniques such as cardboard prototyping, 3D printing or laser cutting is the integration of conductive surfaces. Additional wiring, applying electrical paint, or special materials like conductive filament often lack of the neccessary high conductivity and sufficient durability for designing on-skin wearables to measure muscle activity or to electrically stimulate the skin and muscles.We propose to combine spraying and electroplating to create surfaces that exhibit high conductivity, are solderable, corrosion-resistant, and skin-friendly, embodying both practical functionality and aesthetic value. In this paper, we describe an effective spraying and electroplating process for rapid prototyping procedures. We demonstrate its usefulness by several tangible examples, we discuss advantages and disadvantages and describe limitations of the process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169421
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Cooperative Autonomous Systems",
              "dsl": "Karlsruhe Institute of Technology"
            }
          ],
          "personId": 169471
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169313
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169527
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Human Computer Interaction (HCI)",
              "dsl": "Leibniz University Hannover"
            }
          ],
          "personId": 169348
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": ""
            }
          ],
          "personId": 169514
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Leibniz University Hannover",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 169442
        }
      ]
    },
    {
      "id": 172793,
      "typeId": 13643,
      "title": "Understanding Technological Considerations and needs of Nigerians towards community policing engagement An interview-based study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14781",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Community policing initiatives are increasingly relying on technology to engage stakeholders and improve public safety efforts. Understanding the technological considerations and needs of citizens and police officers is crucial for the effective\r\nimplementation of such initiative in Nigeria. This study investigates perspectives of the Nigerian citizens and police officers on the technological requirements necessary for meaningful participation in community policing activities through qualitative interviews.\r\nFollowing purposive sampling approach and thematic analysis of interview data, this study identifies key technological concerns as well as desired features of mobile surveillance and communication technologies that can enhance community policing participation.\r\nOther findings reveal contrary options in the device visibility choices among the citizens and police. We propose iterative co-design as a better CP technology design approach for users with opposing preferences and detail some design implications that address the specific technology needs of both groups for an enhanced community policing initiative in Nigeria.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "WALES",
              "city": "SWANSEA",
              "institution": "SWANSEA UNIVERSITY",
              "dsl": "COMPUTER SCIENCE DEPARTMENT"
            },
            {
              "country": "Nigeria",
              "state": "ANAMBRA",
              "city": "OKO",
              "institution": "FEDERAL POLYTECHNIC OKO, NIGERIA",
              "dsl": "COMPUTER SCIENCE DEPARTMENT"
            }
          ],
          "personId": 169356
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Swansea",
              "institution": "Swansea University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 169372
        }
      ]
    },
    {
      "id": 172794,
      "typeId": 13643,
      "title": "\"What's this?\": Understanding User Interaction Behaviour with Multimodal Input Information Retrieval System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14782",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Human communication relies on integrated multimodal channels to facilitate rich information exchange. Building on this foundation, researchers have long speculated about the potential benefits of incorporating multimodal input channels into conventional information retrieval (IR) systems to support users' complex daily IR tasks more effectively. However, the true benefits of such integration remain uncertain. This paper presents a series of exploratory pilot tests comparing Multimodal Input IR (MIIR) with Unimodal Input IR (UIIR) across various IR scenarios, concluding that MIIR offers distinct advantages over UIIR in terms of user experiences. Our preliminary results suggest that MIIR could reduce the cognitive load associated with IR query formulation by allowing users to formulate different query-component in a unified manner across different input modalities, particularly when conducting complex exploratory search tasks in unfamiliar, in-situ contexts. The discussions stemming from this finding draw scholarly attention and suggest new angles for designing and developing MIIR systems.\r\n\r\n ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Synteraction Lab"
            }
          ],
          "personId": 169544
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore ",
              "institution": "National University of Singapore",
              "dsl": "Synteraction Lab"
            }
          ],
          "personId": 169351
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Smart Systems Institute, Synteraction lab"
            }
          ],
          "personId": 169433
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 169291
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Ho Chi Minh City",
              "institution": "VNUHCM - University of Science",
              "dsl": ""
            }
          ],
          "personId": 169448
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169369
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "School of Electronics and Computer Science"
            }
          ],
          "personId": 169379
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Ho Chi Minh City",
              "institution": "University of Science, VNUHCM",
              "dsl": ""
            }
          ],
          "personId": 169382
        }
      ]
    },
    {
      "id": 172795,
      "typeId": 13643,
      "title": "DriveStats: a Mobile Platform to Frame Effective Sustainable Driving Displays",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14783",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Phone applications to track vehicle information have become more common place, providing insights into fuel consumption, vehicle status, and sustainable driving behaviors. However, to test what resonates with drivers without deep vehicle integration requires a proper research instrument. We built DriveStats: a reusable library (and encompassing a mobile app) to monitor driving trips and display related information. By providing estimated cost/emission reductions in a goal directed framework, we demonstrate how information utility can increase over the course of a 10 day diary study with a group of North American participants. Participants were initially interested in monetary savings reported increased utility for emissions-related information with increased app usage and resulted in self-reported sustainable behavior change. The DriveStats package can be used as a research probe for a plurality of mobility studies (driving, cycling, walking, etc.) for supporting mobile transportation research.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 169326
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169385
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 169361
        }
      ]
    },
    {
      "id": 172796,
      "typeId": 13643,
      "title": "MagSerea: Fingerprinting Magnetic Field of Specified Area with Wearable Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14784",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "In environments where GPS is impractical for indoor positioning, magnetic information has emerged as a promising alternative. This study proposes an authentication method called \"MagSerea,\" which utilizes time-series data of three-axis magnetic information captured by a wearable device attached to the arm when opening a door. Experimental results indicate that the door-opening motion restricts the deviations in the device's position and angle, thereby maintaining high identification accuracy despite temporal changes and variations in the user's belongings. These findings suggest that MagSerea demonstrates high identification accuracy in real-world conditions, offering lower installation costs and fewer constraints on reference points compared to existing methods, and thus holds potential for a wide range of applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169259
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169416
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku",
              "institution": "Waseda University",
              "dsl": ""
            }
          ],
          "personId": 169466
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN AIP",
              "dsl": ""
            }
          ],
          "personId": 169475
        }
      ]
    },
    {
      "id": 172797,
      "typeId": 13643,
      "title": "Train Me: Exploring Mobile Sports Capture and Replay for Immersive Sports Coaching",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14785",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "In recent years, a wide variety of instructional materials and applications have been developed to enhance athletes’ learning in different sports. The amount of instructional videos, in particular, has increased significantly, though these often impose a high cognitive load as users must map displayed instructions to their actions and body movements. Mobile Augmented Reality (AR) interfaces can reduce this burden by presenting instructional information directly where it is needed. This paper explores a mobile sports capture and replay approach for immersive self-training, aiming to help users improve their skills without needing coaches on-site. We investigate different capturing methods, including an Exocentric capturing method, to enhance instructor mobility and flexibility. Using the captured data, we visualize sports training instructions in an immersive 3D environment on an AR headset. We propose three visualization methods and create a first prototype that allows us to explore the approach’s feasibility across different sports.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169381
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169510
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Dunedin",
              "institution": "University of Otago",
              "dsl": ""
            }
          ],
          "personId": 169337
        }
      ]
    },
    {
      "id": 172798,
      "typeId": 13643,
      "title": "Eyes-free Circular Gestures on Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14786",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Smartphones are used in various situations, such as when users have limited visual focus, such as when walking or driving. Eyes-free gestures offer a way to interact with smartphones without requiring visual attention. This research delves into circular eyes-free gestures and elucidates the advantages they offer over other types of gestures in facilitating eyes-free interaction. We carried out two experiments to explore the ability of participants to accurately draw arcs with varying angles in a smartphone's eyes-free context. The results of the first experiment revealed that participants commonly tended to exceed the intended arc lengths, regardless of whether they were drawing arcs clockwise or counterclockwise. The results of the second experiment showed that there is a high variation in drawing eye-free circular gestures by the same user. However, this variation decreases if the second gesture is produced immediately after the first one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Lille University",
              "dsl": ""
            }
          ],
          "personId": 169408
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Valenciennes",
              "institution": "Université Polytechnique Hauts-de-France",
              "dsl": "LAMIH UMR CNRS 8201"
            }
          ],
          "personId": 169463
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussel",
              "institution": "Université catholique de Louvain",
              "dsl": ""
            }
          ],
          "personId": 169495
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Villeneuve d'Ascq",
              "institution": "university of lille",
              "dsl": ""
            }
          ],
          "personId": 169439
        }
      ]
    },
    {
      "id": 172799,
      "typeId": 13643,
      "title": "Vision Beyond Boundaries: An Initial Design Space of Domain-specific Large Vision Models in Human-robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14787",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "The emergence of large vision models (LVMs) is following in the footsteps of the recent prosperity of Large Language Models (LLMs) in following years. However, there's a noticeable gap in structured research applying LVMs to human-robot interaction (HRI), despite extensive evidence supporting the efficacy of vision models in enhancing interactions between humans and robots. Recognizing the vast and anticipated potential, we introduce an initial design space that incorporates domain-specific LVMs, chosen for their superior performance over normal models. We delve into three primary dimensions: HRI contexts, vision-based tasks, and specific domains. The empirical evaluation was implemented among 15 experts across five evaluated metrics, showcasing the primary efficacy in relevant decision-making scenarios. We explore the process of ideation and potential application scenarios, envisioning this design space as a foundational guideline for future HRI system design, emphasizing accurate domain alignment and model selection.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Robotics, Perception and Learning"
            }
          ],
          "personId": 169452
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": ""
            }
          ],
          "personId": 169529
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Techonology",
              "dsl": ""
            }
          ],
          "personId": 169407
        }
      ]
    },
    {
      "id": 172800,
      "typeId": 13643,
      "title": "Bias-Aware Spoken Conversational Search: A Provocation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14788",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Spoken Conversational Search (SCS) poses unique challenges in understanding user-system interactions due to the absence of visual cues, and the complexity of less structured dialogue. Tackling the impacts of cognitive bias in today’s information-rich online environment, especially when SCS becomes more prevalent, this paper integrates insights from information science, psychology, cognitive science, and wearable sensor technology to explore potential opportunities and challenges in studying cognitive biases in SCS. It then outlines a framework for experimental designs with various experiment setups to multimodal instruments. It also analyzes data from an existing dataset as a preliminary example to demonstrate the potential of this framework and discuss its implications for future research. In the end, it discusses the challenges and ethical considerations associated with implementing this approach. This work aims to provoke new directions and discussion in the community and enhance understanding of cognitive biases in Spoken Conversational Search.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC ",
              "city": "Melbourne ",
              "institution": "RMIT",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169503
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169360
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169260
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 169531
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169485
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169409
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169339
        }
      ]
    },
    {
      "id": 172801,
      "typeId": 13643,
      "title": "DAP 𝄇 : Develop pair-Authentication Protocol with DAP",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14789",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "In today's interconnected world, secure authentication is crucial for both high-security environments and everyday interactions. Traditional authentication methods like passwords and biometrics are designed for individual use, but new challenges emerge in interactive gaming, theme parks, and collaborative virtual reality (VR) where multiple participants must authenticate collectively. This study introduces a multi-person authentication that leverages cooperative actions to enhance security. By analyzing synchronized sensor data from cooperative actions, the system ensures the presence and consent of all participants, making impersonation difficult. We propose a pair authentication using inertial sensors during a complex handshake known as Dignity And Pride (DAP). Our research evaluates the accuracy of pair authentication, the impact of behavioral degradation over time, and resistance to attacks. Experiments with university students demonstrate high authentication accuracy and robustness against time degradation, though vulnerabilities to spoofing attacks were identified, suggesting areas for improvement in secure cooperative authentication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka Univerisity",
              "dsl": ""
            }
          ],
          "personId": 169472
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169353
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            }
          ],
          "personId": 169416
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hamamatsu",
              "institution": "Shizuoka University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN AIP",
              "dsl": ""
            }
          ],
          "personId": 169475
        }
      ]
    },
    {
      "id": 172802,
      "typeId": 13643,
      "title": "MultiSurf-GPT: Facilitating Context-Aware Reasoning with Large-Scale Language Model Agents for Multimodal Surface Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14790",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "For ubiquitous and mobile computing, surface sensing from surroundings is essential for enhancing context-aware interactions between users and devices. However, traditional methods, often based on single-modality type algorithms, overlook potential synergies across various inputs and struggle with interpreting complex multimodal data in dynamic settings, which are crucial for fully understanding contextual factors such as location or behavior. Emerging multimodal large-scale language models offer new opportunities. We propose MultiSurf-GPT, which utilizes the advanced capabilities of GPT-4o to process and interpret diverse modalities (radar, microscope and multispectral data) uniformly based on prompting strategies (zero-shot and  prompting). We preliminarily validated our framework by using MultiSurf-GPT to identify low-level information, and to infer high-level context-aware analytics, demonstrating the capability of augmenting context-aware insights. This framework shows promise as a tool to expedite the development of more complex context-aware applications in the future, providing a faster, more cost-effective, and integrated solution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 169533
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology",
              "dsl": "School of Design"
            }
          ],
          "personId": 169522
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169253
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW",
              "dsl": ""
            }
          ],
          "personId": 169276
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO’s Data61 ",
              "dsl": "Science Director and Deputy Director"
            }
          ],
          "personId": 169270
        }
      ]
    },
    {
      "id": 172803,
      "typeId": 13643,
      "title": "Cultural influence on RE activities: An extended analysis of state of the art",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14791",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Designing mobile software that aligns with cultural contexts is crucial for optimizing human-computer interaction. Considering cultural influences is essential not only for the actual set of functional/non-functional requirements, but also for the whole Requirement Engineering (RE) process. Without a clear understanding of cultural influences on RE activities, it's hardly possible to elaborate a correct and complete set of requirements. \r\n This research explores the impact of national culture on RE-related activities based on recent studies. We conducted a Systematic Literature Review (SLR) of studies published in 2019-2023 and compared them to an older SLR covering 2000-2018. We identified 17 relevant studies, extracted 33 cultural influences impacting RE activities, and mapped them to the Hofstede model, widely used for cultural analysis in software development research. Our work highlights the critical role of national culture in RE activities, summarizes current research trends, and helps practitioners consider cultural influences for mobile app/software development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169264
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169345
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 169347
        }
      ]
    },
    {
      "id": 172804,
      "typeId": 13643,
      "title": "Exploiting Air Quality Monitors to Perform Indoor Surveillance: Academic Setting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14792",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Changing public perceptions and government regulations have led to the widespread use of low-cost air quality monitors in modern indoor spaces. Typically, these monitors detect air pollutants to augment the end user's understanding of her indoor environment. Studies have shown that having access to one's air quality context reinforces the user's urge to take necessary actions to improve the air over time. Thus, user's activities significantly influence the indoor air quality. Such correlation can be exploited to get hold of sensitive indoor activities from the side-channel air quality fluctuations. This study explores the odds of identifying eight indoor activities (i.e., enter, exit, fan on, fan off, AC on, AC off, gathering, eating) in a research lab with an in-house low-cost air quality monitoring platform named DALTON. Our extensive data collection and analysis over three months shows 97.7\\% classification accuracy in our dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 169343
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "Cisco Meraki",
              "dsl": "Data Science"
            }
          ],
          "personId": 169297
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur, India",
              "dsl": "CSE"
            }
          ],
          "personId": 169267
        }
      ]
    },
    {
      "id": 172805,
      "typeId": 13643,
      "title": "To Touch, or Not to Touch: Evaluating Manual Page Turning Modalities for Digital Sheet Music During Piano Play",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14793",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "The widespread adoption of digital sheet music by performers in recent years appears promising, yet not without limitations. While enhancing interactivity and convenience, a persistent challenge remains: the act of turning pages while musicians' hands are engaged on the piano keys. To investigate perceived user experience of page turning in digital sheet music, we conducted a controlled laboratory experiment with fifteen participants (N~=~15), comparing five state-of-the-art page turning modalities. \r\nThe results revealed that hands-free modalities were generally preferred over touch-based modalities, with head gestures showing promise as hands-free alternatives to foot pedals, particularly in terms of perceived efficiency, attractiveness, stimulation, and novelty.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciendes Upper Austria",
              "dsl": "Digital Media Lab"
            }
          ],
          "personId": 169487
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 169406
        }
      ]
    },
    {
      "id": 172806,
      "typeId": 13643,
      "title": "Draw4CM: Detecting Cervical Myelopathy via Hand Drawings Captured by Mobile Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14794",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Cervical myelopathy (CM), which causes numbness and pain in the hands, can affect manual dexterity and render daily life difficult. The characteristics of this disease are expected to be apparent owing to the necessity of fine motor control for drawing in daily life. Therefore, this study proposed a method to screen for CM by asking patients to draw a figure on a tablet screen using a stylus. The drawing was captured as an image and fed into our machine-learning model to predict CM. The proposed models exhibited sensitivity and specificity comparable to or better than conventional physical methods while avoiding examiner subjectivity and bias. Further, we developed a smartphone application that uses the camera to capture images of figures drawn on paper for CM screening. Moreover, the proposed methods can enable smartphones and tablets to capture disease characteristics and are ubiquitous, thus rendering it easier to reach potential patients.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169481
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169289
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Institute of Science Tokyo",
              "dsl": ""
            }
          ],
          "personId": 169323
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 169415
        }
      ]
    },
    {
      "id": 172807,
      "typeId": 13643,
      "title": "Usability Evaluation of a Mobile Application for Foot Health Monitoring of Smart Insoles: A Mixed Methods Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14795",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Increasing research on intelligent wearable technologies, such as smart insoles for mobile health (mHealth) monitoring, brings new challenges for user-centered design, particularly in data visualization. Research shows various developments in mobile apps for monitoring foot health with smart insoles, while emerging trends like chatbot interactions and improved analytic visualizations offer new opportunities to enhance user experience. However, the usability of various health data visualizations remains unvalidated. A mixed-method experimental user study with 30 participants was conducted to assess the usability of three prototype mHealth applications for smart insoles: Analytical, Basic, and Chatbot visualizations. Quantitative results showed that Basic visualization achieved the highest usability, followed by Analytical, and finally Chatbot. Qualitative feedback supported these findings but also highlighted the potential of Chatbot interactions to enhance data understanding in mHealth apps. We discuss implications for future mHealth applications to monitor foot health and propose design recommendations to improve usability in chatbot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Cadiz",
              "institution": "University of Cadiz",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169391
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169489
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169550
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169454
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169279
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt am Main",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 169493
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Cadiz",
              "institution": "University of Cadiz",
              "dsl": ""
            }
          ],
          "personId": 169303
        }
      ]
    },
    {
      "id": 172808,
      "typeId": 13643,
      "title": "Exploring the Feasibility of a Repeated Mobile One-Minute PVT In-the-Wild",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14796",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Researchers have established that quality sleep is important for cognitive functions. Now that consumer-grade wearables can provide accurate measurements of sleep quality, we should be able to inform individuals about how their sleep impacts cognitive functions. However, as our internal clock follows cycles of alertness and sleepiness, it is still necessary to identify at what time of day the sleep quality impacts cognitive performance. In this study, we investigated the relationship between sleep stages and psychomotor vigilance using a 1-minute Psychomotor Vigilance Task (PVT) test. Participants wore a sleep-tracking device and performed the PVT test six times daily for three weeks. The results suggest that increased Rapid Eye Movement (REM) sleep duration led to better performance after lunch. Additionally, we developed a model to predict average response time based on the sleep data. The results show a promising step towards giving tangible meaning to sleep data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169429
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169431
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 169441
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Prefecture University",
              "dsl": ""
            }
          ],
          "personId": 169344
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sakai",
              "institution": "Osaka Metropolitan University",
              "dsl": ""
            }
          ],
          "personId": 169346
        }
      ]
    },
    {
      "id": 172809,
      "typeId": 13643,
      "title": "Enhancing Mobile Interaction: Practical Insights from Smartphone and Smartwatch Integration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14797",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "In the realm of mobile technology, smartwatches have emerged as valuable complements to smartphones, offering unique features such as enhanced activity tracking and in-situ data analysis. This study delves into the synergistic relationship between smartwatches and smartphones through practical applications developed by university students. We guided three groups of students from a computer-related program to design and develop cross-mobile device applications tailored to their daily contexts. Our evaluation of these student-developed applications revealed several key insights. Firstly, there was a strong preference for using smartwatches for straightforward notification purposes, indicating their convenience and immediacy in delivering critical information. However, their effectiveness as extended displays was found to depend heavily on a well-crafted approach to information delivery, necessitating careful design considerations. The study also highlighted the complexities inherent in designing intuitive and efficient cross-device interaction gestures, underlining the importance of thoughtful gesture selection and customization to enhance user experience. Furthermore, our findings indicated that users found it unnecessary to use smartwatches for voice input when paired with smartphones, as a single smartphone was deemed sufficient for this purpose.This research provides valuable insights into enhancing cross-device interaction, aiming to foster more seamless and user-focused integration of technology into daily life. These insights can guide developers in creating more effective and user-friendly applications, ultimately contributing to the broader field of human-computer interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169355
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169496
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169474
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169470
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169428
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Suzhou",
              "institution": "Xi’an Jiaotong-Liverpool University",
              "dsl": "School of Advanced Technology"
            }
          ],
          "personId": 169367
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi‘an jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169380
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Suzhou",
              "institution": "Xi'an Jiaotong-Liverpool University",
              "dsl": ""
            }
          ],
          "personId": 169288
        }
      ]
    },
    {
      "id": 172810,
      "typeId": 13643,
      "title": "DesignWatch: Analyzing Users’ Operations of Mobile Apps Based on Screen Recordings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14798",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Screen recordings of users' operations to complete tasks in the mobile app are vital resources for designers to assess the app's usability. However, analyzing these recordings at a large scale could be mentally challenging. In this paper, we present DesignWatch, which assists designers in analyzing users’ operations of mobile apps based on collected screen recordings. DesignWatch supports interactive visual analyses of multiple users' operation paths in the app and prompts GPT-4 with vision to simulate users' thoughts during each operation. We conduct expert interviews with four designers, which highlight DesignWatch’s usefulness in helping them quickly understand users' operation patterns in the app, identify the potentially problematic UI design page, and get insights for improving the app design. We conclude with design implications for facilitating usability tests with interactive visualization and generative models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169490
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169354
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169401
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169335
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 169277
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Division of Integrative Systems and Design"
            }
          ],
          "personId": 169296
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong Province",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University",
              "dsl": "School of Artificial Intelligence"
            }
          ],
          "personId": 169411
        }
      ]
    },
    {
      "id": 172811,
      "typeId": 13643,
      "title": "EmoFoot: Can Your Foot Tell How You Feel when Playing Virtual Reality Games?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14799",
      "source": "PCS",
      "trackId": 12995,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171142
      ],
      "abstract": "Understanding feelings in Virtual Reality (VR) games is vital for enhancing engagement in human-computer interaction. Traditional methods for assessing emotions and player experience, both subjective and objective, often fall short of capturing players' comprehensive and nuanced experiences. This preliminary study introduces EmoFoot, a novel approach leveraging foot pressure sensors to decode player feelings during VR gameplay. We show the diverse patterns of VR game experiences using subjective reports focused on immersion, competence, negative and positive affect, flow, tension, challenge, and engagement. By integrating smart insoles, our research investigates the potential of using foot pressure data to identify valence and arousal levels. We use Machine Learning models to discover how players' feet can reveal their emotions. EmoFoot aims to introduce a seamless and unobtrusive method for monitoring player experience, contributing to immersive technology by enhancing our understanding of the objective indicators of player emotions and improving the overall gaming experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "Data61, CSIRO",
              "dsl": ""
            }
          ],
          "personId": 169513
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169509
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "nsw",
              "city": "sydney",
              "institution": "unsw",
              "dsl": ""
            }
          ],
          "personId": 169384
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science & Engineering"
            }
          ],
          "personId": 169370
        }
      ]
    },
    {
      "id": 172819,
      "typeId": 13640,
      "title": "AuditNet: Conversational AI Security Assistant",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14807",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "In the age of information overload, professionals across various fields face the challenge of navigating vast amounts of documentation and ever-evolving standards. Ensuring compliance with standards, regulations, and contractual obligations is a critical yet complex task across various professional fields. We propose a versatile conversational AI assistant framework designed to facilitate compliance checking on the go, in diverse domains, including but not limited to network infrastructure, legal contracts, educational standards, environmental regulations, and government policies. By leveraging retrieval-augmented generation using large language models, our framework automates the review, indexing, and retrieval of relevant, context-aware information, streamlining the process of verifying adherence to established guidelines and requirements. This AI assistant not only reduces the manual effort involved in compliance checks but also enhances accuracy and efficiency, supporting professionals in maintaining high standards of practice and ensuring regulatory compliance in their respective fields. We propose and demonstrate AuditNet, the first conversational AI security assistant designed to assist IoT network security experts by providing instant access to security standards, policies, and regulations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Walses",
              "city": "Sydney",
              "institution": "UNSW",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169424
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169484
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169557
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "UNSW Institute for Cyber Security",
              "dsl": "School of Computer Science and Engineering (CSE)"
            }
          ],
          "personId": 169316
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "Cisco",
              "dsl": ""
            }
          ],
          "personId": 169521
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 169485
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": ""
            }
          ],
          "personId": 169497
        }
      ]
    },
    {
      "id": 172820,
      "typeId": 13640,
      "title": "Demonstrating TOM: A Development Platform for Wearable Intelligent Assistants in Daily Activities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14808",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Advanced wearable digital assistants can significantly enhance task performance, reduce user burden, and provide personalized guidance to improve users' abilities. However, developing these assistants presents several challenges. To address this, we introduce TOM (The Other Me), a conceptual architecture and open-source software platform (https://github.com/TOM-Platform) that supports the development of wearable intelligent assistants that are contextually aware of both the user and the environment. Collaboratively developed with researchers and developers, TOM meets their diverse requirements. TOM facilitates the creation of intelligent assistive AR applications for daily activities and supports the recording and analysis of user interactions, integration of new devices, and the provision of assistance for various activities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Smart Systems Institute, Synteraction lab"
            }
          ],
          "personId": 169433
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 169369
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Computer Science"
            }
          ],
          "personId": 169483
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "NUS-HCI Lab, Smart Systems Institute"
            }
          ],
          "personId": 169342
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "NUS",
              "dsl": ""
            }
          ],
          "personId": 169395
        }
      ]
    },
    {
      "id": 172821,
      "typeId": 13640,
      "title": "Geofence-to-Conversation: Hierarchical Geofencing for Augmenting City Walks with Large Language Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14809",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "This study presents a geofence-based service architecture for city-wide audio augmented reality, tailored for the era of large language models. Traditional geofencing mechanisms, which monitor user entry to geofences, struggle to provide continuous storytelling in areas with few points of interest, degrading the audio tour experiences for pedestrians. Our proposed geofencing architecture consistently incorporates complex and multilayered city features, enabling seamless audio tour experiences. Furthermore, this paper introduces prompt engineering for generating entertaining guide scripts for large language models, that is, the geofence-to-conversation technique. The mobile application developed for the actual field demonstrates the feasibility of our proposed architecture and highlights future challenges in enhancing users’ interaction with a city.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169518
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169517
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169549
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169417
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Akita",
              "institution": "Akita University",
              "dsl": ""
            }
          ],
          "personId": 169400
        }
      ]
    },
    {
      "id": 172822,
      "typeId": 13640,
      "title": "GestureShirt: Exploring Gestures in Front of the Body for Truly Mobile Interaction while Running",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14810",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Running is the most practiced sport worldwide. Modern technology significantly enhances the running experience. Runners frequently use audio devices, monitoring apps, and wearable sensors like heart rate monitors. Utilizing these technologies while in motion presents unique challenges, as continuous movement can obstruct user interaction. Common devices, such as smartphones, smartwatches, and fitness trackers, are widely employed, but their small interfaces and touch screens often necessitate slowing down for effective use. In this work we explore the space around the body for truly mobile interaction during the run. This demo paper introduces GestureShirt, a novel garment that employs removable distance sensors to explore gestural interaction around the body. Based on an exploratory study (N = 16) we distilled three strategies for utilizing gestures in front of the body for truly mobile interaction. In the demonstration, visitors can explore these strategies through functioning prototypes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Human Computer Interaction Division"
            }
          ],
          "personId": 169444
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": ""
            }
          ],
          "personId": 169501
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Salzburg",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Human Computer Interaction Division"
            }
          ],
          "personId": 169271
        }
      ]
    },
    {
      "id": 172823,
      "typeId": 13640,
      "title": "The Atlas of AI Incidents in Mobile Computing: Visualizing the Risks and Benefits of AI Gone Mobile",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14811",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Today's visualization tools for conveying the risks and benefits of AI technologies are largely tailored for those with technical expertise. To bridge this gap, we have developed a visualization that employs narrative patterns and interactive elements, enabling the broader public to gradually grasp the diverse risks and benefits associated with AI. Using a dataset of 54 real-world incidents involving AI in mobile computing, we examined design choices that enhance public understanding and provoke reflection on how certain AI applications—even those deemed low-risk by law—can still lead to significant incidents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169458
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169173
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 169456
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169461
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 169108
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": ""
            }
          ],
          "personId": 169459
        }
      ]
    },
    {
      "id": 172824,
      "typeId": 13640,
      "title": "Head ’n Shoulder: Gesture-Driven Biking Through Capacitive Sensing Garments to Innovate Hands-Free Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "14812",
      "source": "PCS",
      "trackId": 13000,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        171145
      ],
      "abstract": "Distractions caused by digital devices are increasingly causing dangerous situations on the road, particularly for more vulnerable road users like cyclists. While researchers have been exploring ways to enable richer interaction scenarios on the bike, safety concerns are frequently neglected and compromised. In this work, we propose Head ’n Shoulder, a gesture-driven approach to bike interaction without affecting bike control, based on a wearable garment that allows hands- and eyes-free interaction with digital devices through integrated capacitive sensors. It achieves an average accuracy of 97% in the final iteration, evaluated on 14 participants. Head ’n Shoulder does not rely on direct pressure sensing, allowing users to wear their everyday garments on top or underneath, not affecting recognition accuracy. Our work introduces a promising research direction: easily deployable smart garments with a minimal set of gestures suited for most bike interaction scenarios, sustaining the rider’s comfort and safety.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169357
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 169508
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": " Berlin,",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169255
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 169258
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 169413
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 169410
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 169418
        }
      ]
    }
  ],
  "people": [
    {
      "id": 169065,
      "firstName": "Tae",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "taVd_kEsAYrz0aYCPB5GXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169069,
      "firstName": "Francesco",
      "lastName": "Chiossi",
      "middleInitial": "",
      "importedId": "s1Xyc1j9ixsl9NOK4VT4lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169108,
      "firstName": "Daniele",
      "lastName": "Quercia",
      "middleInitial": "",
      "importedId": "WTGqrHzg4MtvxxZHYisuJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169130,
      "firstName": "Vincent",
      "lastName": "LAMBERT",
      "middleInitial": "",
      "importedId": "-kZ5PC5WJBlaSKm4xocVDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169149,
      "firstName": "Yue",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "3jHBrzwyRq6Gj4tTeQKjmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169173,
      "firstName": "Marios",
      "lastName": "Constantinides",
      "middleInitial": "",
      "importedId": "nkGVw2ca5PUqN-KjzkeoTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169178,
      "firstName": "Florian",
      "lastName": "Bemmann",
      "middleInitial": "",
      "importedId": "neHRwhECQA471ocQV6_kYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169187,
      "firstName": "Yuan Hang",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "NkX3enA6C-CHbrKDoKSjDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169251,
      "firstName": "Matthias",
      "lastName": "Hoppe",
      "middleInitial": "",
      "importedId": "Lkq0Iab9-No5KlhYyZuXNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169252,
      "firstName": "Yosuke",
      "lastName": "Oba",
      "middleInitial": "",
      "importedId": "6r6LfnRm50uOSvNrhgiCvg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169253,
      "firstName": "Zhuying",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "vx--wCIHC5ibFveZh31S-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169254,
      "firstName": "Changkun",
      "lastName": "Ou",
      "middleInitial": "",
      "importedId": "EyP8fiBm_iH2ZxNtjh8Y9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169255,
      "firstName": "Esther",
      "lastName": "Zahn",
      "middleInitial": "Friederike",
      "importedId": "b8fTNQKrGHLJIMLVEx4-MQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169256,
      "firstName": "Mennatallah",
      "lastName": "Saleh",
      "middleInitial": "",
      "importedId": "msus5zpWW5hq2hDIz_XX_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169257,
      "firstName": "Jarrod",
      "lastName": "Knibbe",
      "middleInitial": "",
      "importedId": "Yb6Bx6iANjcXzAoUplIIXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169258,
      "firstName": "Emil",
      "lastName": "Woop",
      "middleInitial": "",
      "importedId": "lyUWDlryGL5rqzGQ9t6VKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169259,
      "firstName": "Tomoya",
      "lastName": "Aiba",
      "middleInitial": "",
      "importedId": "j7ZHAFHik01pj0yD7ECYog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169260,
      "firstName": "Johanne",
      "lastName": "Trippas",
      "middleInitial": "R",
      "importedId": "_Y_dtTZY7lSL2JvV2ggWBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169261,
      "firstName": "Baosheng James",
      "lastName": "HOU",
      "middleInitial": "",
      "importedId": "BPt6sTsYZas6cMelaJMhtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169262,
      "firstName": "Jesse",
      "lastName": "Grootjen",
      "middleInitial": "W",
      "importedId": "T5P7d-YF5AQw5hdA15vWzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169263,
      "firstName": "Tania",
      "lastName": "Mishra",
      "middleInitial": "",
      "importedId": "fTK2MnQK1uSkZEwe0awMAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169264,
      "firstName": "Chowdhury Shahriar",
      "lastName": "Muzammel",
      "middleInitial": "",
      "importedId": "ucHkqRa-fwlBHe2AAJjdkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169265,
      "firstName": "Aochen",
      "lastName": "Jiao",
      "middleInitial": "",
      "importedId": "LsaC9Mdg_A2a-UY7lacZOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169266,
      "firstName": "Paloma",
      "lastName": "Diaz",
      "middleInitial": "",
      "importedId": "CW9G9VCx6DK7SDyhvIJSLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169267,
      "firstName": "Sandip",
      "lastName": "Chakraborty",
      "middleInitial": "",
      "importedId": "pWJ-wioEWooqt_RfR9zIsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169268,
      "firstName": "Holger",
      "lastName": "Regenbrecht",
      "middleInitial": "",
      "importedId": "DWY_L6ecUmwTUbCSy3EAiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169269,
      "firstName": "Wen-Ni",
      "lastName": "Lai",
      "middleInitial": "",
      "importedId": "c1acboYLGnVqpRC4UKVxrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169270,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "middleInitial": "J",
      "importedId": "MibYPMlrFfkZJFjG9MoptQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169271,
      "firstName": "Alexander",
      "lastName": "Meschtscherjakov",
      "middleInitial": "",
      "importedId": "U_zbCYI95FkvwIXAPi8ZtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169272,
      "firstName": "Florian",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "3gnZEiql5UVDZAWwJ8LD7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169273,
      "firstName": "Shigeo",
      "lastName": "Morishima",
      "middleInitial": "",
      "importedId": "wVfsCje18LEfkzwxd1E_Jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169274,
      "firstName": "Shivang",
      "lastName": "Bokolia",
      "middleInitial": "",
      "importedId": "IRUphmJC5RFVar-eZuarGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169275,
      "firstName": "Giovanni",
      "lastName": "Troiano",
      "middleInitial": "M",
      "importedId": "gSseNrZb4Iv_kHhptW9eaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169276,
      "firstName": "Wen",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "UPBx3yxYSNUobh2sKbf-wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169277,
      "firstName": "Qianyao",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "-jEPzSBOoWuKeTvwaDqOgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169278,
      "firstName": "Chuang-Wen",
      "lastName": "You",
      "middleInitial": "",
      "importedId": "Lhg9wJUskHC2kfRmfey4LA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169279,
      "firstName": "Valentin",
      "lastName": "Schwind",
      "middleInitial": "",
      "importedId": "RQzBxU_wVwvPHVcQsWWeXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169280,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "middleInitial": "",
      "importedId": "oLvfjZm0xHwl7reMcJOYcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169281,
      "firstName": "Manuel",
      "lastName": "Ninaus",
      "middleInitial": "",
      "importedId": "Dcq6gw9819FsBtcUZ9z3mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169282,
      "firstName": "Dennis",
      "lastName": "Stanke",
      "middleInitial": "",
      "importedId": "CdM7o5zza79-x7OPhvLsRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169283,
      "firstName": "Kening",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "ZulPc33c1AEwQgbRCriHPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169284,
      "firstName": "Magdalena",
      "lastName": "Schlegel",
      "middleInitial": "",
      "importedId": "9j_IGFgyEltST7xsyTBk6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169285,
      "firstName": "Akos",
      "lastName": "Vetek",
      "middleInitial": "",
      "importedId": "wc9wIwKPnrCqeQMAiyxpgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169286,
      "firstName": "Chieko",
      "lastName": "Asakawa",
      "middleInitial": "",
      "importedId": "n3ejsF3IZuCTTy0QuvMENA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169287,
      "firstName": "Debasree",
      "lastName": "Das",
      "middleInitial": "",
      "importedId": "p3upb_bDSrRBbummwVquAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169288,
      "firstName": "Yu",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "DT1svgUUG92tyHzKVvh8GQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169289,
      "firstName": "Eriku",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "0fr9HVeTLDvIwSHL7sFN-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169290,
      "firstName": "Céline",
      "lastName": "Coutrix",
      "middleInitial": "",
      "importedId": "_ERLCyenaLrR9ybrBTKMOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169291,
      "firstName": "Kun",
      "lastName": "Yue",
      "middleInitial": "",
      "importedId": "QXvMBYIMhSIzD-jD48yz3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169292,
      "firstName": "Li-Ting",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "TkW9fGy_XFCbNUhnobHmpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169293,
      "firstName": "Hiroki",
      "lastName": "Usuba",
      "middleInitial": "",
      "importedId": "f6BX4o3dD71ZdK5coIVinQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169294,
      "firstName": "Jaana",
      "lastName": "Külim",
      "middleInitial": "",
      "importedId": "3wjgn67OLyTt4QYKOXuXDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169295,
      "firstName": "ZHUZHI",
      "lastName": "FAN",
      "middleInitial": "",
      "importedId": "yV2_mUjNcJNqlSGZwxZllw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169296,
      "firstName": "Xiaozhu",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "5mxHX80ZflsQIgXTCOYs9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169297,
      "firstName": "Swadhin",
      "lastName": "Pradhan",
      "middleInitial": "",
      "importedId": "jhfjTVMq77ZyUHEWyn9VJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169299,
      "firstName": "Shota",
      "lastName": "Yamanaka",
      "middleInitial": "",
      "importedId": "1Qctf2puZsJX-j0cIylfDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169300,
      "firstName": "Hsin-Ai",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "g54AMfwAaBQ8j_O1zHcW3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169301,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "importedId": "JdMekqlspHzE2AN2wOoABg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169302,
      "firstName": "Ying",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "OntLndgr3-SVFioMI137CQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169303,
      "firstName": "Daniel",
      "lastName": "Sanchez-Morillo",
      "middleInitial": "",
      "importedId": "aesxsc7lEaB_mI_oBVgTEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169304,
      "firstName": "Xiaofu",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "RFGQNlNOqV5RejD6w1pKJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169305,
      "firstName": "Tang-Jie",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "1Mqm2AuRbxN5sNGEdn75ZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169306,
      "firstName": "Roshan",
      "lastName": "Peiris",
      "middleInitial": "L",
      "importedId": "CCCkdPuztOqMeppYS6o7WA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169307,
      "firstName": "Yukina",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "PPMNaHZvYmnSlmWNERUA8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169308,
      "firstName": "Ting",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "jeotXsYjkwTGO5eSw2BDzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169309,
      "firstName": "Robin",
      "lastName": "Welsch",
      "middleInitial": "",
      "importedId": "ihQfOi4bvjFKwaO1MNw8Xw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169310,
      "firstName": "Christian",
      "lastName": "Sturm",
      "middleInitial": "",
      "importedId": "tboZDiomWcHz7fhecbS1eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169311,
      "firstName": "Taiki",
      "lastName": "Kinoshita",
      "middleInitial": "",
      "importedId": "PxKDGWtrmDXuosCyRGieQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169312,
      "firstName": "Bivas",
      "lastName": "Mitra",
      "middleInitial": "",
      "importedId": "joveCyBbnszQRstfbW-Tsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169313,
      "firstName": "Justin",
      "lastName": "Schulte",
      "middleInitial": "",
      "importedId": "KWPImrn14T22rokUNUXGPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169314,
      "firstName": "Casper",
      "lastName": "Harteveld",
      "middleInitial": "",
      "importedId": "AdKhnmom2oLh__sU6QgtKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169315,
      "firstName": "Yannick",
      "lastName": "Weiss",
      "middleInitial": "",
      "importedId": "jWLdmyLXrs3NtXwj12QpCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169316,
      "firstName": "Arash",
      "lastName": "Shaghaghi",
      "middleInitial": "",
      "importedId": "EG9rnkv8AzFF_hS3gb5deQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169317,
      "firstName": "Sven",
      "lastName": "Mayer",
      "middleInitial": "",
      "importedId": "aGMdv_UqvRAFIxAEt3rNsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169318,
      "firstName": "Mingyuan",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "dQO5CKZoTOADE9_fxqfASw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169319,
      "firstName": "Chien Wen (Tina)",
      "lastName": "Yuan",
      "middleInitial": "",
      "importedId": "wGefP37c9haYHN4Pn4r8vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169320,
      "firstName": "Chiao-Ju",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "715n5friGvz1rWDTHZSRpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169321,
      "firstName": "Yen-Pu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "JqKRwi0vPAoF868kCPGfFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169322,
      "firstName": "Fangyuan",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "A--ELns0EkLR-83XeHqbnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169323,
      "firstName": "Koji",
      "lastName": "Fujita",
      "middleInitial": "",
      "importedId": "wxku2auJE2i5eL6SSOTEeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169324,
      "firstName": "Khalad",
      "lastName": "Hasan",
      "middleInitial": "",
      "importedId": "7CkZjwYqftGmEzOEveetqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169325,
      "firstName": "Tian",
      "lastName": "Min",
      "middleInitial": "",
      "importedId": "DdOgh1KoyMrydcoSrc6K0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169326,
      "firstName": "Song Mi",
      "lastName": "Lee-Kan",
      "middleInitial": "",
      "importedId": "BjAbsqKYrR-R0WbwFp6eBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169327,
      "firstName": "Heiko",
      "lastName": "Holz",
      "middleInitial": "",
      "importedId": "MubfGAEet7oZLRydCa0WCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169328,
      "firstName": "Yongquan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "CYzpMBOdLBgpgus4tfzmbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169329,
      "firstName": "Cherie",
      "lastName": "Sew",
      "middleInitial": "",
      "importedId": "NHX9ruq618_jnou8v_1pew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169330,
      "firstName": "Di",
      "lastName": "Duan",
      "middleInitial": "",
      "importedId": "Ne5ejy_GcJoyyxjt9CJZFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169331,
      "firstName": "Yuka",
      "lastName": "Kaniwa",
      "middleInitial": "",
      "importedId": "doUy9x0fKNe43eoJHYtfug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169332,
      "firstName": "Yu-Jen",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "K4OKybOW24_1hikREqO58g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169333,
      "firstName": "XiJing",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "YGJGDdj5WVA1_L8qwOEo8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169334,
      "firstName": "Adrian",
      "lastName": "Holzer",
      "middleInitial": "",
      "importedId": "tJ9nJNSlFofDEerWu1ncFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169335,
      "firstName": "Guanyi",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "qFv_JH7X21R5snsRMQ5E7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169336,
      "firstName": "Steeven",
      "lastName": "Villa",
      "middleInitial": "",
      "importedId": "H9eoI_KhiFXgd9zP2Qn9EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169337,
      "firstName": "Tobias",
      "lastName": "Langlotz",
      "middleInitial": "",
      "importedId": "mYrVKjYUdhZLQJBbqdqYGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169338,
      "firstName": "Ziming",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "c-D_VojwA0tkz9g-y2BJvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169339,
      "firstName": "Damiano",
      "lastName": "Spina",
      "middleInitial": "",
      "importedId": "ecXf-2v1LKoFYGLiDSmi8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169340,
      "firstName": "THOMAS",
      "lastName": "RAMES",
      "middleInitial": "",
      "importedId": "SZNMxeYr4yaCVMVt3tJN2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169341,
      "firstName": "Elyssia Barrie",
      "lastName": "Ong",
      "middleInitial": "",
      "importedId": "TEpnioMZO50TEUN7LUD79w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169342,
      "firstName": "Sherisse",
      "lastName": "Tan Jing Wen",
      "middleInitial": "",
      "importedId": "uhhM6YPl4f1MubVaeb2lCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169343,
      "firstName": "Prasenjit",
      "lastName": "Karmakar",
      "middleInitial": "",
      "importedId": "VugRWtTutB8Bmk9wt_yEMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169344,
      "firstName": "Ryunosuke",
      "lastName": "Nishitomi",
      "middleInitial": "",
      "importedId": "pCB0jtTqGl38bssRnw_y9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169345,
      "firstName": "Maria",
      "lastName": "Spichkova",
      "middleInitial": "",
      "importedId": "4nDPnf2gchArWuSbq1Ra3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169346,
      "firstName": "Koichi",
      "lastName": "Kise",
      "middleInitial": "",
      "importedId": "Hucq8SaNNbYRYCfeYNtvcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169347,
      "firstName": "James",
      "lastName": "Harland",
      "middleInitial": "",
      "importedId": "cYUoXTdbG_5PDgxD04ibPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169348,
      "firstName": "Ibraheem",
      "lastName": "Al-Azzawi",
      "middleInitial": "",
      "importedId": "1zec-9srZx-LWpK_cZEwuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169349,
      "firstName": "Homei",
      "lastName": "Miyashita",
      "middleInitial": "",
      "importedId": "8SUm8tZAhEI_3G3BIXTBiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169350,
      "firstName": "Alexander",
      "lastName": "Wiethoff",
      "middleInitial": "",
      "importedId": "E08FSQG07c39N8D3I0qAnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169351,
      "firstName": "Hyeongcheol",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "cc2XM09gK4zygBrqZB8qHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169352,
      "firstName": "Anastasia",
      "lastName": "Kuzminykh",
      "middleInitial": "",
      "importedId": "GkorhbP9GGlTsJ542N4FSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169353,
      "firstName": "Masora",
      "lastName": "Okano",
      "middleInitial": "",
      "importedId": "q726TG5gRhyyg6O4uJxd_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169354,
      "firstName": "Yixin",
      "lastName": "Zeng",
      "middleInitial": "",
      "importedId": "F2u26vS8EdiIf1Gju_mrOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169355,
      "firstName": "Qiuyao",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "0aDjSsNBz8iqXimNFbsxzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169356,
      "firstName": "OBINNA",
      "lastName": "OTUU",
      "middleInitial": "OGBONNIA",
      "importedId": "m3Fw0KuhjSARBrX6M_JEPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169357,
      "firstName": "Daniel",
      "lastName": "Geißler",
      "middleInitial": "",
      "importedId": "HKCOuXd8_wbcRs081VKV2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169358,
      "firstName": "Leon",
      "lastName": "Reicherts",
      "middleInitial": "",
      "importedId": "psnk66MAH-N-MNDPYyA94A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169359,
      "firstName": "dian",
      "lastName": "zhu",
      "middleInitial": "",
      "importedId": "UYBQKBTgGy6RcMyEWF2N6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169360,
      "firstName": "Sachin",
      "lastName": "Pathiyan Cherumanal",
      "middleInitial": "",
      "importedId": "D-r-MpcBmaM9fnwR7WZxjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169361,
      "firstName": "David",
      "lastName": "Shamma",
      "middleInitial": "A.",
      "importedId": "RABu3rA1LmUwtpC1C2r9qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169362,
      "firstName": "Denise",
      "lastName": "Löfflad",
      "middleInitial": "",
      "importedId": "0NJoTP5S9U2mY299LdzFAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169363,
      "firstName": "Daisuke",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "rF9uulCVepasiAOJhmljpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169364,
      "firstName": "Seita",
      "lastName": "Kayukawa",
      "middleInitial": "",
      "importedId": "lC2PWRN6B9ujxmIabs2_6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169365,
      "firstName": "Helena",
      "lastName": "Stoll",
      "middleInitial": "",
      "importedId": "q12m107BVKBaKtLWtgC9Bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169366,
      "firstName": "Takumi",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "importedId": "Q_lto2fPS0fLNeRr49IZ-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169367,
      "firstName": "Zixuan",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Lun0A6btI6kFK4AMXEdLIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169368,
      "firstName": "Daniel",
      "lastName": "Riesner",
      "middleInitial": "",
      "importedId": "CTkXKeWqCTq0WYDufwmHsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169369,
      "firstName": "Shengdong",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "rekgBuhTFAmu01G8SJEKbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169370,
      "firstName": "Gelareh",
      "lastName": "Mohammadi",
      "middleInitial": "",
      "importedId": "Dci-0EF6i1u98NbbRhnSQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169371,
      "firstName": "Marja",
      "lastName": "Salmimaa",
      "middleInitial": "",
      "importedId": "dCjoFY5Eb07xVZub8k5ipA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169372,
      "firstName": "Deepak",
      "lastName": "Sahoo",
      "middleInitial": "",
      "importedId": "kv1D4HpUdxoI5FtLOZWmNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169373,
      "firstName": "Lukas",
      "lastName": "Mecke",
      "middleInitial": "",
      "importedId": "1FcdyF5fykcJg1qBjXEtaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169375,
      "firstName": "Teng",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "ywcTvWUiMqaeDO5lMLUmlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169377,
      "firstName": "Zhanna",
      "lastName": "Sarsenbayeva",
      "middleInitial": "",
      "importedId": "P6fWyF6_130K28lYKq4FgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169378,
      "firstName": "Nobuhito",
      "lastName": "Kasahara",
      "middleInitial": "",
      "importedId": "pWIrTXDqrDkSwCKK_L5qjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169379,
      "firstName": "Haiming",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "6gNmS52wPrxilsyHJaODcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169380,
      "firstName": "YUNTONG",
      "lastName": "DONG",
      "middleInitial": "",
      "importedId": "1oKSQJnrZhh9Czan3iMJwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169381,
      "firstName": "Mehran",
      "lastName": "Rastegar Sani",
      "middleInitial": "",
      "importedId": "Vh9L5VDi0qLJ7yQm7gzY1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169382,
      "firstName": "Khanh-Duy",
      "lastName": "Le",
      "middleInitial": "",
      "importedId": "qhgX2QqR2KxGLWvllE-krw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169383,
      "firstName": "Sang Won",
      "lastName": "Bae",
      "middleInitial": "",
      "importedId": "U4qE1lMO0s-KlcEIds2lNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169384,
      "firstName": "Mahdi",
      "lastName": "Bamdad",
      "middleInitial": "",
      "importedId": "ItngvshelX24EluJmkv2yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169385,
      "firstName": "Alexandre",
      "lastName": "Filipowicz",
      "middleInitial": "L. S.",
      "importedId": "qmN5L725wq31rZdgh9_FvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169386,
      "firstName": "Chengshuo",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "Ln0bZ4TBaediEiNBlwHTpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169387,
      "firstName": "Mohammad",
      "lastName": "Hajarian",
      "middleInitial": "",
      "importedId": "354f2jfchIzAPQBQi9sN4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169388,
      "firstName": "Andrea",
      "lastName": "Miotto",
      "middleInitial": "",
      "importedId": "JxQgGCi3X6uhZ7-dDjhEBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169389,
      "firstName": "Laurence",
      "lastName": "Nigay",
      "middleInitial": "",
      "importedId": "xY6CiO-bfiM5lkRMtAOhaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169390,
      "firstName": "Jorge",
      "lastName": "Goncalves",
      "middleInitial": "",
      "importedId": "pkPdK0O9pXfoXIuAjmxdyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169391,
      "firstName": "Stefan",
      "lastName": "Resch",
      "middleInitial": "",
      "importedId": "MwoGttyJupe78AIlzZ4RYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169392,
      "firstName": "Arshad",
      "lastName": "Nasser",
      "middleInitial": "",
      "importedId": "4eYKbsK4MtO2lf6Le16wzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169393,
      "firstName": "Thomas",
      "lastName": "Kosch",
      "middleInitial": "",
      "importedId": "xNJhF8Um1cSTuL-8ebzsgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169394,
      "firstName": "Jonathan",
      "lastName": "Sutton",
      "middleInitial": "",
      "importedId": "NfzpalVWXthXBvjhG6DnTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169395,
      "firstName": "Chun Keat",
      "lastName": "Koh",
      "middleInitial": "",
      "importedId": "DBHnqA3t5LBB0Qlg3mFNCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169396,
      "firstName": "Kushani",
      "lastName": "Perera",
      "middleInitial": "Tharushika",
      "importedId": "oJu0z_7uW7VQ7LQnnkxN-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169397,
      "firstName": "Sayan",
      "lastName": "Sarcar",
      "middleInitial": "",
      "importedId": "OsqrQn04FEjYmh8m4cLKWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169398,
      "firstName": "Sergio",
      "lastName": "Sayago",
      "middleInitial": "",
      "importedId": "dgSsQ5Cj-5TLTBpS-HSK9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169399,
      "firstName": "Tero",
      "lastName": "Jokela",
      "middleInitial": "",
      "importedId": "U4e_NXXkmIhbyocBTfMysg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169400,
      "firstName": "Ryo",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "bxyLXt7ndortANBxLp7j2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169401,
      "firstName": "Qichang",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "UTya8SlAFNXnrCnmOYS_Vg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169402,
      "firstName": "Mingming",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "Sf6s1OuvcuM-r64kIOrFFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169403,
      "firstName": "Junhan",
      "lastName": "Kong",
      "middleInitial": "",
      "importedId": "_Vr6TuQoVWta4jCdlRMJug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169404,
      "firstName": "Yong-Han",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "xwjOtWOz0P_WhV3GAwqLkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169405,
      "firstName": "Palavi",
      "lastName": "Bhole",
      "middleInitial": "V.",
      "importedId": "tlRibeZF2d25Z8f4ejw-7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169406,
      "firstName": "Daniel",
      "lastName": "Ockiya",
      "middleInitial": "Beinmonyu",
      "importedId": "p6iT3HpolqmrlobAOVcRbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169407,
      "firstName": "Danica",
      "lastName": "Kragic",
      "middleInitial": "",
      "importedId": "h5Y5LPLMu6Kb3qWMKpnTxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169408,
      "firstName": "Milad",
      "lastName": "Jamalzadeh",
      "middleInitial": "",
      "importedId": "8slF18Bv_itw7-IWHNdOwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169409,
      "firstName": "Falk",
      "lastName": "Scholer",
      "middleInitial": "",
      "importedId": "SW5-ELz0EQloNqmza9n3og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169410,
      "firstName": "Paul",
      "lastName": "Lukowicz",
      "middleInitial": "",
      "importedId": "y8L6P8t-U44z-L7oNzzNkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169411,
      "firstName": "Zhenhui",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "QLB33PjBTuHKiEDUE3PJJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169412,
      "firstName": "Markus",
      "lastName": "Tatzgern",
      "middleInitial": "",
      "importedId": "1nW_YQE3rsBWA7M9Nv726A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169413,
      "firstName": "Bo",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "QLPz5ycPULDpm2i0tBppXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169414,
      "firstName": "Yasin",
      "lastName": "Kale",
      "middleInitial": "",
      "importedId": "CnNqkLFMvcFOvUQmcYExlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169415,
      "firstName": "Yuta",
      "lastName": "Sugiura",
      "middleInitial": "",
      "importedId": "TeKRGK2C776DV4tpr6ldBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169416,
      "firstName": "Masakatsu",
      "lastName": "Nishigaki",
      "middleInitial": "",
      "importedId": "7ZS3VbSsyUvca7GaDyd9BQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169417,
      "firstName": "Tomihiro",
      "lastName": "Utsumi",
      "middleInitial": "",
      "importedId": "mFu-hYN93nOfEtL7zlAavQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169418,
      "firstName": "Jakob",
      "lastName": "Karolus",
      "middleInitial": "",
      "importedId": "uS4ZLqmddQDCzjPrNQqp-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169419,
      "firstName": "Wei",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "NszTZXb0g230shGx_Oj9bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169420,
      "firstName": "ZiXun",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "Zid8BF9npKlF184t6suJNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169421,
      "firstName": "Tim",
      "lastName": "Duente",
      "middleInitial": "",
      "importedId": "qr_8QdifHNHEhRLqAdNFBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169422,
      "firstName": "Jonna",
      "lastName": "Häkkilä",
      "middleInitial": "",
      "importedId": "wTK4ZT01b7pmAztWA_X-cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169423,
      "firstName": "Agnes",
      "lastName": "Gruenerbl",
      "middleInitial": "",
      "importedId": "CDLkbWni3L1RHex0agOCHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169424,
      "firstName": "Shohreh",
      "lastName": "Deldari",
      "middleInitial": "",
      "importedId": "tpSrPEY1z1izChc0yE-5zQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169425,
      "firstName": "Jessica",
      "lastName": "Cauchard",
      "middleInitial": "R.",
      "importedId": "beYeovjkMGSBXCwSLbFomA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169426,
      "firstName": "Karanmeet",
      "lastName": "Khatra",
      "middleInitial": "",
      "importedId": "CRK31JYLgSNZKlZ_fLxCZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169427,
      "firstName": "Passant",
      "lastName": "ElAgroudy",
      "middleInitial": "",
      "importedId": "bjWBDSWVn_9w8NpwkmHu-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169428,
      "firstName": "Xinyao01",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "OnrZjc4F-ksRZM-_KYZJRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169429,
      "firstName": "Tasnim Irtifa",
      "lastName": "Chowdhury",
      "middleInitial": "",
      "importedId": "soVjL20hOU_CUDZt5nDm-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169430,
      "firstName": "Lina",
      "lastName": "ZHANG",
      "middleInitial": "",
      "importedId": "PaDi4Fx_jt7huHDC7TPg4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169431,
      "firstName": "Andrew",
      "lastName": "Vargo",
      "middleInitial": "",
      "importedId": "Fclo0LXKCGrOfIN_Y7amXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169432,
      "firstName": "Chung Chiao",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "FhzwrZVjN0UVKkEtWrQPWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169433,
      "firstName": "Nuwan",
      "lastName": "Janaka",
      "middleInitial": "",
      "importedId": "SDcyd3lM2dsCwfpKYPufEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169434,
      "firstName": "Florian",
      "lastName": "Alt",
      "middleInitial": "",
      "importedId": "Rch54wTSBXvObvR51LzYeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169435,
      "firstName": "Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "8gTbRL1gFvzOD0JXfltM7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169436,
      "firstName": "Mike",
      "lastName": "Digman",
      "middleInitial": "",
      "importedId": "Yrt8SDfVXh-RPQOmA_5uVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169437,
      "firstName": "Mike",
      "lastName": "Chen",
      "middleInitial": "Y.",
      "importedId": "lHUnsX0Tf6ZdyKH8WPjT9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169438,
      "firstName": "Le",
      "lastName": "SONG",
      "middleInitial": "",
      "importedId": "YxFJPOMeJDMn27ypyPxZQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169439,
      "firstName": "Laurent",
      "lastName": "Grisoni",
      "middleInitial": "",
      "importedId": "7rrJKdWwcSD8gpWBC5m0OA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169440,
      "firstName": "Alvaro",
      "lastName": "Cassinelli",
      "middleInitial": "",
      "importedId": "RHlTRLWWePDsBLox9GKl2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169441,
      "firstName": "Benjamin",
      "lastName": "Tag",
      "middleInitial": "",
      "importedId": "DEa3EvJq4ssEnGWw_FCDUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169442,
      "firstName": "Michael",
      "lastName": "Rohs",
      "middleInitial": "",
      "importedId": "WG5CH6vAl40qiw3lIXtxRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169443,
      "firstName": "Meng-Hsin",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "SldKIn0K5NW8k9byj6G6Kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169444,
      "firstName": "Vincent",
      "lastName": "van Rheden",
      "middleInitial": "",
      "importedId": "wn6ocEfh71Bh3YHkp7H3PQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169445,
      "firstName": "Philipp",
      "lastName": "Thalhammer",
      "middleInitial": "Tim",
      "importedId": "kwlCepjbEyus6joyw4QxQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169446,
      "firstName": "Candice",
      "lastName": "Hogan",
      "middleInitial": "",
      "importedId": "7xLpGwVOkID4O5TV2EB5CA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169447,
      "firstName": "Benedikt",
      "lastName": "Beuttler",
      "middleInitial": "",
      "importedId": "lNDGhW-NBBH_PTayLPKUMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169448,
      "firstName": "Hoang-Long",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "gzdcFGeptKImFwjmjCuTew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169449,
      "firstName": "Helen",
      "lastName": "Stefanidi",
      "middleInitial": "",
      "importedId": "xMkypd3ZA47hdAz8fXtuVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169450,
      "firstName": "Jie",
      "lastName": "Tsai",
      "middleInitial": "",
      "importedId": "_kTT6NLIYJvkQNXHruLiBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169451,
      "firstName": "Johannes",
      "lastName": "Schöning",
      "middleInitial": "",
      "importedId": "fc9sshp8L9i6K6CitF8edw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169452,
      "firstName": "Yuchong",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "XVkq6TE-KZEZn5zLi7MgFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169453,
      "firstName": "Christina",
      "lastName": "Wei",
      "middleInitial": "Ziying",
      "importedId": "7OS1VGlCYgZYRGYDMOKyjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169454,
      "firstName": "Dennis",
      "lastName": "Gerstung",
      "middleInitial": "",
      "importedId": "szE3t2rm9E102f9Q2FRjyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169455,
      "firstName": "Yichen",
      "lastName": "YUAN",
      "middleInitial": "",
      "importedId": "y13ccyln_DG7Sy88qkAcvg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169456,
      "firstName": "Julia",
      "lastName": "De Miguel Velazquez",
      "middleInitial": "",
      "importedId": "bXWDhxn2gbB__teRBzbERQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169457,
      "firstName": "Benjamin",
      "lastName": "Simon",
      "middleInitial": "",
      "importedId": "s-HUd4VSTR2xD43C7nTwnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169458,
      "firstName": "Edyta",
      "lastName": "Bogucka",
      "middleInitial": "Paulina",
      "importedId": "WQgEnU_AaGgKFhImwrDdmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169459,
      "firstName": "Andrés",
      "lastName": "Gvirtz",
      "middleInitial": "",
      "importedId": "11akqgM3M_at13zcZw7q8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169461,
      "firstName": "Sanja",
      "lastName": "Scepanovic",
      "middleInitial": "",
      "importedId": "y_tmhl6rWqxrgbS97OBNAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169462,
      "firstName": "Nayeli",
      "lastName": "Bravo",
      "middleInitial": "Suseth",
      "importedId": "XXu4s-QlThAo6XlHvqzzBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169463,
      "firstName": "Yosra",
      "lastName": "Rekik",
      "middleInitial": "",
      "importedId": "iiWMIHeA5-zeEPW08Z1-vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169464,
      "firstName": "Ava",
      "lastName": "Scott",
      "middleInitial": "Elizabeth",
      "importedId": "P1B_CmnavAd-fAGgj69YpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169465,
      "firstName": "Tamara",
      "lastName": "Zieher",
      "middleInitial": "",
      "importedId": "Gx1dy66UJ6mJookZLtX-IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169466,
      "firstName": "Tatsuya",
      "lastName": "Mori",
      "middleInitial": "",
      "importedId": "57w2tf3mxSR1YKgBt7biVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169467,
      "firstName": "Julien",
      "lastName": "Epps",
      "middleInitial": "",
      "importedId": "_xl3QtPs4r24joWY_idtmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169468,
      "firstName": "Briane Paul",
      "lastName": "Samson",
      "middleInitial": "V.",
      "importedId": "wGiHvzmT6WKBNtHVxD5eCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169469,
      "firstName": "Felix",
      "lastName": "Dietz",
      "middleInitial": "",
      "importedId": "I272kzU7g_n-ksVa5nc3Cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169470,
      "firstName": "Tan",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "YNX8c2K8QjzniGCTkpuAQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169471,
      "firstName": "Maximilian",
      "lastName": "Schrapel",
      "middleInitial": "",
      "importedId": "wOF1P7bXCAS84LjMHNbpqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169472,
      "firstName": "Soshi",
      "lastName": "Maeda",
      "middleInitial": "",
      "importedId": "siIzry-Rp1NSTNtmyKIGWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169473,
      "firstName": "Yu Lun",
      "lastName": "Hsu",
      "middleInitial": "",
      "importedId": "nkkuG7jY-zxC1tJnlWOCrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169474,
      "firstName": "Fengyuan",
      "lastName": "Liao",
      "middleInitial": "",
      "importedId": "VZiFoWTSBwKf8VTG9DcGXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169475,
      "firstName": "Tetsushi",
      "lastName": "Ohki",
      "middleInitial": "",
      "importedId": "oO56h0_HzjblN13_-Sphcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169476,
      "firstName": "Junlei",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "SynPDn3XJ7q7_QBPgQqeWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169477,
      "firstName": "Hironobu",
      "lastName": "Takagi",
      "middleInitial": "",
      "importedId": "m5HBmCllu2FoYcYxrc98Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169478,
      "firstName": "Ryuto",
      "lastName": "Tomihari",
      "middleInitial": "",
      "importedId": "qu7JHpex74AlRMGAANu5Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169479,
      "firstName": "Thomas",
      "lastName": "Lachmann",
      "middleInitial": "",
      "importedId": "UncFveMzwP3c25g4j2ROSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169480,
      "firstName": "Elahi",
      "lastName": "Hossain",
      "middleInitial": "",
      "importedId": "vywstClQ0PcHB6qhG5hVeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169481,
      "firstName": "Takuro",
      "lastName": "Watanabe",
      "middleInitial": "",
      "importedId": "iKoafBZLeMejxf-ojlaE-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169482,
      "firstName": "Moritz",
      "lastName": "Klose",
      "middleInitial": "",
      "importedId": "X9P3DGKq02ynJfXrHvRnFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169483,
      "firstName": "David",
      "lastName": "Hsu",
      "middleInitial": "",
      "importedId": "NnUBovQ1Yd05pubcjrUtLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169484,
      "firstName": "Mohammad",
      "lastName": "Goudarzi",
      "middleInitial": "",
      "importedId": "oOaAkJLhXY3iun3mzjbjpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169485,
      "firstName": "Flora",
      "lastName": "Salim",
      "middleInitial": "D.",
      "importedId": "DWrgPXRnHzslllOsqlYwqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169486,
      "firstName": "Nadine",
      "lastName": "Wagener",
      "middleInitial": "",
      "importedId": "crCvaUuTosnWCpqIPFRRLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169487,
      "firstName": "Kathrin",
      "lastName": "Probst",
      "middleInitial": "",
      "importedId": "_Vu8BrGjyG25DCx0ch89Fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169488,
      "firstName": "Jacob",
      "lastName": "Wobbrock",
      "middleInitial": "O.",
      "importedId": "u7VUTOERCrBjVBdNyeGk5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169489,
      "firstName": "Lucian",
      "lastName": "Zimmermann",
      "middleInitial": "",
      "importedId": "unDYBjFiJ4EWxdJfUp92wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169490,
      "firstName": "Xiucheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "-s9kj3z9693hYr1ixPs9mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169491,
      "firstName": "Evropi",
      "lastName": "Stefanidi",
      "middleInitial": "",
      "importedId": "Ad7-2v5GjnwqYpNAaCWH5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169492,
      "firstName": "Jayneel",
      "lastName": "Vora",
      "middleInitial": "",
      "importedId": "EEwZM05bJoQfU18RwTz6QQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169493,
      "firstName": "Diana",
      "lastName": "Völz",
      "middleInitial": "",
      "importedId": "hw6PjB2ca3ROWFG3tQwufA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169494,
      "firstName": "Thomas",
      "lastName": "Goodge",
      "middleInitial": "",
      "importedId": "IVw6yp8fW2NV_05Uv_M1AA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169495,
      "firstName": "Iqra",
      "lastName": "Shahzad",
      "middleInitial": "",
      "importedId": "NpuTLk710siuJrwBWvA7hQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169496,
      "firstName": "xuanyao",
      "lastName": "tian",
      "middleInitial": "",
      "importedId": "UF65HijQUZ7Gk9Ca7FLTdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169497,
      "firstName": "Sanjay",
      "lastName": "Jha",
      "middleInitial": "",
      "importedId": "bqrP13ZwSrwIl7nOEbYCXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169498,
      "firstName": "Jaisie",
      "lastName": "Sin",
      "middleInitial": "",
      "importedId": "MdxohVrovjEvt2EKywJj1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169499,
      "firstName": "Jan",
      "lastName": "Spilski",
      "middleInitial": "",
      "importedId": "5Gv8-2q43sfwGdWebwIG4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169500,
      "firstName": "Parisa",
      "lastName": "Pour Rezaei",
      "middleInitial": "",
      "importedId": "kzXSogPLcNFF-7MDYaE6Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169501,
      "firstName": "Lukas",
      "lastName": "Pöckl",
      "middleInitial": "",
      "importedId": "vwqohRnCARxo-JF8u_pQ8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169502,
      "firstName": "Masaya",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "OhphJGCm9rbz4dqruR-khg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169503,
      "firstName": "Kaixin",
      "lastName": "Ji",
      "middleInitial": "",
      "importedId": "vX69Qx60toNxWesMdQt4lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169504,
      "firstName": "Nanyi",
      "lastName": "Bi",
      "middleInitial": "",
      "importedId": "QwPAqTWTq_hBZzrpllTnLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169505,
      "firstName": "Monica",
      "lastName": "Perusquia-Hernandez",
      "middleInitial": "",
      "importedId": "GZ1r7eyqLjexGA4_p7AYfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169506,
      "firstName": "Yung-Ju",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "4fqA22UA7fOw_djjgasuxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169507,
      "firstName": "Sylvain",
      "lastName": "Malacria",
      "middleInitial": "",
      "importedId": "-XkZNXaCRSB1PSPFzDJFkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169508,
      "firstName": "Hymalai",
      "lastName": "Bello",
      "middleInitial": "",
      "importedId": "cc0EWcF1ABMuu__WbHrJKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169509,
      "firstName": "Xingjian",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "hNs--_lIKN2wwK5X98iwFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169510,
      "firstName": "Stefanie",
      "lastName": "Zollmann",
      "middleInitial": "",
      "importedId": "KqIZ_GgTpaXx8XNsZn-vtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169511,
      "firstName": "Hiroki",
      "lastName": "Watanabe",
      "middleInitial": "",
      "importedId": "j9uVhI0O6nKFZu_32F-wkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169512,
      "firstName": "Jianan",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "-Qb0oEOJlFd-XcW-zC7WmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169513,
      "firstName": "Rukshani",
      "lastName": "Somarathna",
      "middleInitial": "",
      "importedId": "FbRIsHkvis8cpWOZfAqMbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169514,
      "firstName": "Kerem Can",
      "lastName": "Demir",
      "middleInitial": "",
      "importedId": "EKi2F6rlpNgGYOxkj7EZtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169515,
      "firstName": "Monika",
      "lastName": "Harvey",
      "middleInitial": "",
      "importedId": "25uyO9oRO9GHmgDMMtjQwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169516,
      "firstName": "Pin-Chieh",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "Fe9ZeBBCkQX6ZZ1T8MBiGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169517,
      "firstName": "Masatoshi",
      "lastName": "Arikawa",
      "middleInitial": "",
      "importedId": "ORTyrmHf0ebOtgr9F58PHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169518,
      "firstName": "Iori",
      "lastName": "Sasaki",
      "middleInitial": "",
      "importedId": "WuGthwPd93bkgDGt5ZDqhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169519,
      "firstName": "Mohammad Rahul",
      "lastName": "Islam",
      "middleInitial": "",
      "importedId": "0bduyKrw-05Je4VEAc138Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169520,
      "firstName": "Weitao",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "_tG6lYPRm-Mi3dznJjCVyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169521,
      "firstName": "Simon",
      "lastName": "Finn",
      "middleInitial": "",
      "importedId": "MMT72gvm_OjE7rkeKl0Rjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169522,
      "firstName": "Pengcheng",
      "lastName": "An",
      "middleInitial": "",
      "importedId": "oEME8VYQZKQWVvPxTkuUPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169523,
      "firstName": "Yu-Cheng",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "pfkFpvD2cmftCk6Juwd97A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169524,
      "firstName": "Jenny",
      "lastName": "Waycott",
      "middleInitial": "",
      "importedId": "ukLlEmkAkkr_QrPs1wYd9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169525,
      "firstName": "Jiaan",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "zdjdkdkh67TBh5LUyGLZjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169526,
      "firstName": "Rulu",
      "lastName": "Liao",
      "middleInitial": "",
      "importedId": "l_XiJAEAHCjMS4S7waIVlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169527,
      "firstName": "Nick",
      "lastName": "Janßen",
      "middleInitial": "",
      "importedId": "M6KQ0CTPKeAZt9VOye3otw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169528,
      "firstName": "Michael",
      "lastName": "Zimmer",
      "middleInitial": "",
      "importedId": "VMh1THzIXR5aw1A-AjdzRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169529,
      "firstName": "Yong",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "VyWiWN3ckTzsZyF41x5WGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169530,
      "firstName": "Viviane",
      "lastName": "Herdel",
      "middleInitial": "",
      "importedId": "QPpGxXIVlK08NZj0N1JlJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169531,
      "firstName": "Danula",
      "lastName": "Hettiachchi",
      "middleInitial": "",
      "importedId": "SlayOPpkJgxHbffvhD9zrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169532,
      "firstName": "Ghada",
      "lastName": "Alsebayel",
      "middleInitial": "",
      "importedId": "yP0nHaa4qWZygzPAiO8VRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169533,
      "firstName": "Black",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "PbcllUkmkVbB4OUzsa1hpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169534,
      "firstName": "Niklas",
      "lastName": "Hirsch",
      "middleInitial": "",
      "importedId": "xcJ1VRN5Vcboqn_pNJOjiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169535,
      "firstName": "Stephen",
      "lastName": "Brewster",
      "middleInitial": "Anthony",
      "importedId": "5oR5tpEfy7_GSFNHxRxKQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169536,
      "firstName": "Wei Tian Mireille",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "g2usfsFDI9qqZZIZZsaONA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169537,
      "firstName": "Nidhi",
      "lastName": "Nellore",
      "middleInitial": "",
      "importedId": "XwUreVUyD9WuQC8KgFnj8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169538,
      "firstName": "Garreth",
      "lastName": "Tigwell",
      "middleInitial": "W.",
      "importedId": "cUdX3NzPSXqxc9BAbQb_ig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169539,
      "firstName": "Maximiliane",
      "lastName": "Windl",
      "middleInitial": "",
      "importedId": "QQBeKC9OzYuXI29Rb_jlOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169540,
      "firstName": "James",
      "lastName": "Fogarty",
      "middleInitial": "",
      "importedId": "16laD9XiwAkEWg2RfxVr1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169541,
      "firstName": "Masaki",
      "lastName": "Kuribayashi",
      "middleInitial": "",
      "importedId": "mk8jMb4nHlbnK7FI81mMqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169542,
      "firstName": "Aditya Kumar",
      "lastName": "Purohit",
      "middleInitial": "",
      "importedId": "yo-L4r2mf2H3hS_vjxDasg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169544,
      "firstName": "Silang",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "cTvTejVf-2My9RXzp-Zpnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169545,
      "firstName": "Yvonne",
      "lastName": "Rogers",
      "middleInitial": "",
      "importedId": "1XonrPa4CxxLvJn-6wJw7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169546,
      "firstName": "Takashi",
      "lastName": "Amesaka",
      "middleInitial": "",
      "importedId": "Ee3-4cSiiA6mPM8hL7KvRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169547,
      "firstName": "Rawan",
      "lastName": "Srour Zreik",
      "middleInitial": "",
      "importedId": "LJiBb4OpNDtQZNXtMnurWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169548,
      "firstName": "Henning",
      "lastName": "Pohl",
      "middleInitial": "",
      "importedId": "aZmLrFIR8HZWodGoIdNoYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169549,
      "firstName": "Min",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "YQQi0UHw73ZelvNNfkoDvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169550,
      "firstName": "Alexander",
      "lastName": "Kunkel",
      "middleInitial": "",
      "importedId": "xAtIVMLb2pynfNFxM9QXuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169551,
      "firstName": "Joshua",
      "lastName": "Newn",
      "middleInitial": "",
      "importedId": "_NriYeNxlJoriGThWQKfGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169552,
      "firstName": "Giulia",
      "lastName": "Barbareschi",
      "middleInitial": "",
      "importedId": "zQI_WMsL-y3to63Pg_mKfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169553,
      "firstName": "Alix",
      "lastName": "Goguey",
      "middleInitial": "",
      "importedId": "D--hWgGqn0i-gnRBtXJ9Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169554,
      "firstName": "Alexis",
      "lastName": "Sanson",
      "middleInitial": "",
      "importedId": "ZA6VHTuh2fMGsaKsmJZUSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169555,
      "firstName": "Andreas",
      "lastName": "Khoshnou",
      "middleInitial": "",
      "importedId": "Pa1unJRDYvcLBWIsHXFrMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169556,
      "firstName": "Cosmin",
      "lastName": "Munteanu",
      "middleInitial": "",
      "importedId": "2gfhj9dBSKZvSiyP8azncw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169557,
      "firstName": "Aditya",
      "lastName": "Joshi",
      "middleInitial": "",
      "importedId": "zi6i-A5QgwwLucCzxvt_QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169646,
      "firstName": "Lintong",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "CM1ZBjyGhh1_7qcTNOgVig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169647,
      "firstName": "Anqin",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "wUFFxPCtrzfFxjA5PmmgwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169648,
      "firstName": "Yuhang",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "D_Pn4gtTSr_WCJLKshE-tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169649,
      "firstName": "Zhengyang",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "EDEIZ3P-E3Wx9lAWxcCIbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169650,
      "firstName": "Anna",
      "lastName": "Kramar",
      "middleInitial": "",
      "importedId": "ZO0NEwyW-e_RZDuXXQkQ4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169651,
      "firstName": "Xin",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "LjPbYWWcn5cljV8wi7Iulw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169652,
      "firstName": "Zhijin",
      "lastName": "Lin",
      "middleInitial": "Jonathan",
      "importedId": "x5s3UnxubTy8GOJE1b1WgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169653,
      "firstName": "Jiayue",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "19BhdHr1O3BTxEG3KsyVhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169654,
      "firstName": "Shuo",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "5gjlmnHFfMHBZNSLPyur0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169655,
      "firstName": "Yikai",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "B6Asi04IRgAWvFDqoH_sqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169656,
      "firstName": "Kinza",
      "lastName": "Salim",
      "middleInitial": "",
      "importedId": "m7wTvuiEPYE8tg7NywAkKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169657,
      "firstName": "Rongtian",
      "lastName": "Bai",
      "middleInitial": "",
      "importedId": "wwWRDbZA9nXFXQF1rdpb2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169658,
      "firstName": "Ang",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "RsSA3Alyt7sXMlgrV7JujA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169659,
      "firstName": "Hoang D.",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "S3aEpLS2RGjRGwzxAmCKhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 169660,
      "firstName": "Jixiao",
      "lastName": "He",
      "middleInitial": "",
      "importedId": "BX96mCJWEdIuE33eiL4WEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 171238,
      "firstName": "Thad",
      "lastName": "Starner",
      "importedId": "14738",
      "source": "SYS",
      "affiliations": [
        {
          "country": "USA",
          "state": "Georgia",
          "city": "Atlanta",
          "institution": "Georgia Tech"
        }
      ]
    },
    {
      "id": 172242,
      "firstName": "Junlei",
      "lastName": "Hong",
      "importedId": "1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 172243,
      "firstName": "Tobias",
      "lastName": "Langlotz",
      "importedId": "2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 172244,
      "firstName": "Jonathan",
      "lastName": "Sutton",
      "importedId": "3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 172245,
      "firstName": "Holger",
      "lastName": "Regenbrecht",
      "importedId": "4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 172779,
      "firstName": "Chris",
      "lastName": "Speed",
      "importedId": "14767",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Australia",
          "state": "Victoria",
          "city": "Melbourne",
          "institution": "RMIT"
        }
      ]
    }
  ],
  "recognitions": []
}