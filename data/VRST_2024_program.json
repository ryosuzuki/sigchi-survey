{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10124,
    "shortName": "VRST",
    "displayShortName": "",
    "year": 2024,
    "startDate": 1728432000000,
    "endDate": 1728604800000,
    "fullName": "30th ACM Symposium on Virtual Reality Software and Technology (VRST)",
    "url": "https://vrst.acm.org/vrst2024/",
    "location": "Trier, Germany",
    "timeZoneOffset": 120,
    "timeZoneName": "Europe/Berlin",
    "logoUrl": "https://files.sigchi.org/conference/logo/10124/c14ee6da-41fb-3b1f-f8fa-3b3512ea4341.png",
    "name": "VRST 2024"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 2,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2024-09-29 17:36:27+00"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10377,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 11765,
      "name": "Kapelle",
      "setup": "NO_ROOM",
      "typeId": 13797
    },
    {
      "id": 11766,
      "name": "HS11+HS12",
      "setup": "NO_ROOM",
      "typeId": 13796
    }
  ],
  "tracks": [
    {
      "id": 13067,
      "name": "VRST 2024 Papers",
      "typeId": 13796
    },
    {
      "id": 13068,
      "name": "VRST 2024 Posters and Demos",
      "typeId": 13797
    },
    {
      "id": 13069,
      "name": "VRST 2024 Reproducibility"
    },
    {
      "id": 13078,
      "typeId": 13792
    },
    {
      "id": 13079,
      "typeId": 13792
    }
  ],
  "contentTypes": [
    {
      "id": 13791,
      "name": "Course",
      "displayName": "Courses",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 13792,
      "name": "Keynotes",
      "color": "#006d2c",
      "duration": 60
    },
    {
      "id": 13793,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13794,
      "name": "Event",
      "displayName": "Events",
      "color": "#ffc034",
      "duration": 0
    },
    {
      "id": 13795,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 13796,
      "name": "Paper",
      "displayName": "Papers",
      "color": "#0d42cc",
      "duration": 9
    },
    {
      "id": 13797,
      "name": "Posters And Demos",
      "color": "#ff7a00",
      "duration": 5
    },
    {
      "id": 13798,
      "name": "Work-in-Progress",
      "displayName": "Works-In-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 13799,
      "name": "Workshop",
      "displayName": "Workshops",
      "color": "#f60000",
      "duration": 240
    },
    {
      "id": 13800,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 30
    }
  ],
  "timeSlots": [
    {
      "id": 14352,
      "type": "SESSION",
      "startDate": 1728462600000,
      "endDate": 1728464400000
    },
    {
      "id": 14353,
      "type": "SESSION",
      "startDate": 1728464400000,
      "endDate": 1728468000000
    },
    {
      "id": 14354,
      "type": "SESSION",
      "startDate": 1728468000000,
      "endDate": 1728469800000
    },
    {
      "id": 14355,
      "type": "BREAK",
      "startDate": 1728469800000,
      "endDate": 1728471600000
    },
    {
      "id": 14356,
      "type": "SESSION",
      "startDate": 1728471600000,
      "endDate": 1728477000000
    },
    {
      "id": 14357,
      "type": "LUNCH",
      "startDate": 1728477000000,
      "endDate": 1728480600000
    },
    {
      "id": 14358,
      "type": "SESSION",
      "startDate": 1728480600000,
      "endDate": 1728486000000
    },
    {
      "id": 14359,
      "type": "BREAK",
      "startDate": 1728486000000,
      "endDate": 1728487800000
    },
    {
      "id": 14360,
      "type": "SESSION",
      "startDate": 1728487800000,
      "endDate": 1728493200000
    },
    {
      "id": 14362,
      "type": "SESSION",
      "startDate": 1728549000000,
      "endDate": 1728550800000
    },
    {
      "id": 14364,
      "type": "SESSION",
      "startDate": 1728550800000,
      "endDate": 1728556200000
    },
    {
      "id": 14365,
      "type": "BREAK",
      "startDate": 1728556200000,
      "endDate": 1728558000000
    },
    {
      "id": 14366,
      "type": "SESSION",
      "startDate": 1728558000000,
      "endDate": 1728563400000
    },
    {
      "id": 14368,
      "type": "LUNCH",
      "startDate": 1728563400000,
      "endDate": 1728567000000
    },
    {
      "id": 14369,
      "type": "SESSION",
      "startDate": 1728567000000,
      "endDate": 1728572400000
    },
    {
      "id": 14370,
      "type": "BREAK",
      "startDate": 1728572400000,
      "endDate": 1728574200000
    },
    {
      "id": 14371,
      "type": "SESSION",
      "startDate": 1728574200000,
      "endDate": 1728577800000
    },
    {
      "id": 14372,
      "type": "SESSION",
      "startDate": 1728577800000,
      "endDate": 1728581400000
    },
    {
      "id": 14373,
      "type": "SESSION",
      "startDate": 1728581400000,
      "endDate": 1728597600000
    },
    {
      "id": 14374,
      "type": "SESSION",
      "startDate": 1728635400000,
      "endDate": 1728637200000
    },
    {
      "id": 14375,
      "type": "SESSION",
      "startDate": 1728637200000,
      "endDate": 1728642600000
    },
    {
      "id": 14376,
      "type": "BREAK",
      "startDate": 1728642600000,
      "endDate": 1728644400000
    },
    {
      "id": 14377,
      "type": "SESSION",
      "startDate": 1728644400000,
      "endDate": 1728649800000
    },
    {
      "id": 14378,
      "type": "LUNCH",
      "startDate": 1728649800000,
      "endDate": 1728653400000
    },
    {
      "id": 14379,
      "type": "SESSION",
      "startDate": 1728653400000,
      "endDate": 1728658800000
    },
    {
      "id": 14380,
      "type": "BREAK",
      "startDate": 1728658800000,
      "endDate": 1728660600000
    },
    {
      "id": 14381,
      "type": "SESSION",
      "startDate": 1728660600000,
      "endDate": 1728664200000
    },
    {
      "id": 14382,
      "type": "SESSION",
      "startDate": 1728664200000,
      "endDate": 1728667800000
    }
  ],
  "sessions": [
    {
      "id": 172751,
      "name": "Session 1: Presence and Immersion",
      "isParallelPresentation": false,
      "importedId": "14743",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172483,
        172485,
        172468,
        172463,
        172474,
        172465
      ],
      "source": "SYS",
      "timeSlotId": 14356
    },
    {
      "id": 172752,
      "name": "Session 2: Navigation and Motion",
      "isParallelPresentation": false,
      "importedId": "14744",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172458,
        172478,
        172481,
        172475,
        172449,
        172469
      ],
      "source": "SYS",
      "timeSlotId": 14358
    },
    {
      "id": 172753,
      "name": "Session 3: Technologies",
      "isParallelPresentation": false,
      "importedId": "14745",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172460,
        172480,
        172464,
        172454,
        172489
      ],
      "source": "SYS",
      "timeSlotId": 14360
    },
    {
      "id": 172754,
      "name": "Session 4: Time",
      "isParallelPresentation": false,
      "importedId": "14746",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [
        172304
      ],
      "contentIds": [
        172450,
        172456,
        172490,
        172487,
        172491
      ],
      "source": "SYS",
      "timeSlotId": 14364
    },
    {
      "id": 172756,
      "name": "Session 5: Multimodality",
      "isParallelPresentation": false,
      "importedId": "14748",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172451,
        172471,
        172466,
        172488,
        172462
      ],
      "source": "SYS",
      "timeSlotId": 14366
    },
    {
      "id": 172757,
      "name": "Session 6: Collaboration and Games",
      "isParallelPresentation": false,
      "importedId": "14749",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172492,
        172457,
        172459,
        172455,
        172470,
        172467
      ],
      "source": "SYS",
      "timeSlotId": 14369
    },
    {
      "id": 172758,
      "name": "Session 7: Cognitive Aspects",
      "isParallelPresentation": false,
      "importedId": "14750",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172472,
        172479,
        172448,
        172486,
        172453,
        172484
      ],
      "source": "SYS",
      "timeSlotId": 14375
    },
    {
      "id": 172759,
      "name": "Session 8: Displays",
      "isParallelPresentation": false,
      "importedId": "14751",
      "typeId": 13796,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172452,
        172473,
        172482,
        172476,
        172461,
        172477
      ],
      "source": "SYS",
      "timeSlotId": 14377
    },
    {
      "id": 172769,
      "name": "Closing Keynote",
      "isParallelPresentation": false,
      "importedId": "14757",
      "typeId": 13792,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172768
      ],
      "source": "SYS",
      "timeSlotId": 14381
    },
    {
      "id": 172770,
      "name": "Joint Keynote VRST + SUI",
      "isParallelPresentation": false,
      "importedId": "14758",
      "typeId": 13792,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [
        172767
      ],
      "source": "SYS",
      "timeSlotId": 14353
    }
  ],
  "events": [
    {
      "id": 172241,
      "name": "Demos",
      "isParallelPresentation": false,
      "importedId": "14742",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [
        172721,
        172741,
        172716,
        172732,
        172746,
        172740,
        172737,
        172717,
        172701,
        172730,
        172697,
        172731,
        172695,
        172739,
        172718,
        172749
      ],
      "startDate": 1728637200000,
      "endDate": 1728658800000,
      "description": "Demos",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172763,
      "name": "Posters Day 1",
      "isParallelPresentation": false,
      "importedId": "14755",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [
        172723,
        172745,
        172742,
        172706,
        172710,
        172708,
        172728,
        172724,
        172748,
        172747,
        172743,
        172703,
        172700,
        172699,
        172715,
        172714,
        172713,
        172711,
        172736,
        172735,
        172734,
        172733,
        172727,
        172744,
        172750,
        172694,
        172707,
        172705,
        172704,
        172712,
        172709,
        172719,
        172726,
        172722,
        172720,
        172738,
        172725,
        172702,
        172698,
        172729,
        172696
      ],
      "startDate": 1728471600000,
      "endDate": 1728493200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172764,
      "name": "Posters Day 2",
      "isParallelPresentation": false,
      "importedId": "14756",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [
        172723,
        172745,
        172742,
        172706,
        172710,
        172708,
        172728,
        172724,
        172748,
        172747,
        172743,
        172703,
        172700,
        172699,
        172715,
        172714,
        172713,
        172711,
        172736,
        172735,
        172734,
        172733,
        172727,
        172744,
        172750,
        172694,
        172707,
        172705,
        172704,
        172712,
        172709,
        172719,
        172726,
        172722,
        172720,
        172738,
        172725,
        172702,
        172698,
        172729,
        172696
      ],
      "startDate": 1728550800000,
      "endDate": 1728572400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172771,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14759",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728462600000,
      "endDate": 1728464400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172772,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14760",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728549000000,
      "endDate": 1728550800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172773,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "14761",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728635400000,
      "endDate": 1728637200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172774,
      "name": "Opening",
      "isParallelPresentation": false,
      "importedId": "14762",
      "typeId": 13794,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728468000000,
      "endDate": 1728469800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172775,
      "name": "Transit to Banquet",
      "isParallelPresentation": false,
      "importedId": "14763",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728577800000,
      "endDate": 1728581400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172776,
      "name": "Banquet",
      "isParallelPresentation": false,
      "importedId": "14764",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728581400000,
      "endDate": 1728597600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172777,
      "name": "Interactive Session",
      "isParallelPresentation": false,
      "importedId": "14765",
      "typeId": 13794,
      "roomId": 11766,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728574200000,
      "endDate": 1728577800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172789,
      "name": "Joint Poster Reception (SUI and VRST) 18:00-20:30 on 08/10/24",
      "isParallelPresentation": false,
      "importedId": "14777",
      "typeId": 13794,
      "chairIds": [],
      "contentIds": [],
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172791,
      "name": "Session 1: Presence and Immersion - Discussion",
      "isParallelPresentation": false,
      "importedId": "14779",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728474840000,
      "endDate": 1728477000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172812,
      "name": "Session 2: Navigation and Motion - Discussion",
      "isParallelPresentation": false,
      "importedId": "14800",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728483840000,
      "endDate": 1728486000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172813,
      "name": "Session 3: Technologies - Discussion",
      "isParallelPresentation": false,
      "importedId": "14801",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728490500000,
      "endDate": 1728493200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172814,
      "name": "Session 4: Time - Discussion",
      "isParallelPresentation": false,
      "importedId": "14802",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728553500000,
      "endDate": 1728556200000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172815,
      "name": "Session 5: Multimodality -  Discussion",
      "isParallelPresentation": false,
      "importedId": "14803",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728560700000,
      "endDate": 1728563400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172816,
      "name": "Session 6: Collaboration and Games - Discussion",
      "isParallelPresentation": false,
      "importedId": "14804",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728570240000,
      "endDate": 1728572400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172817,
      "name": "Session 7: Cognitive Aspects -  Discussion",
      "isParallelPresentation": false,
      "importedId": "14805",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728640440000,
      "endDate": 1728642600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 172818,
      "name": "Session 8: Displays - Discussion",
      "isParallelPresentation": false,
      "importedId": "14806",
      "typeId": 13794,
      "roomId": 11765,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1728647640000,
      "endDate": 1728649800000,
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 172448,
      "typeId": 13796,
      "title": "Evaluating Gaze Interactions within AR for Nonspeaking Autistic Users",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-7606",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "Nonspeaking autistic individuals often face significant inclusion barriers in various aspects of life, mainly due to a lack of effective communication means. Specialized computer software, particularly delivered via Augmented Reality (AR), offers a promising and accessible way to improve their ability to engage with the world. While research has explored near-hand interactions within AR for this population, gaze-based interactions remain unexamined. Given the fine motor skill requirements and potential for fatigue associated with near-hand interactions, there is a pressing need to investigate the potential of gaze interactions as a more accessible option. This paper presents a study investigating the feasibility of eye gaze interactions within an AR environment for nonspeaking autistic individuals. We utilized the HoloLens 2 to create an eye gaze-based interactive system, enabling users to select targets either by fixating their gaze for a fixed period or by gazing at a target and triggering selection with a physical button (referred to as a 'clicker'). We developed a system called HoloGaze that allows a caregiver to join an AR session to train an autistic individual in gaze-based interactions as appropriate. Using HoloGaze, we conducted a study involving 14 nonspeaking autistic participants. The study had several phases, including tolerance testing, calibration, gaze training, and interacting with a complex interface: a virtual letterboard. All but one participant were able to wear the device and complete the system's default eye calibration; 10 participants completed all training phases that required them to select targets using gaze only or gaze-click. Interestingly, the 7 users who chose to continue to the testing phase with gaze-click were much more successful than those who chose to continue with gaze alone. We also report on challenges and improvements needed for future gaze-based interactive AR systems for this population. Our findings pave the way for new opportunities for specialized AR solutions tailored to the needs of this under-served and under-researched population.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Electrical and Software Engineering"
            }
          ],
          "personId": 172319
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Electrical and Software Engineering"
            }
          ],
          "personId": 172305
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Psychology"
            }
          ],
          "personId": 172352
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Psychology"
            }
          ],
          "personId": 172445
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 172416
        }
      ]
    },
    {
      "id": 172449,
      "typeId": 13796,
      "title": "Semi-Automated Guided Teleportation through Immersive Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9142",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "Immersive knowledge spaces like museums or cultural sites are often explored by traversing pre-defined paths that are curated to unfold a specific educational narrative. To support this type of guided exploration in VR, we present a semi-automated, hands-free path traversal technique based on teleportation that features a slow-paced interaction workflow targeted at fostering knowledge acquisition and maintaining spatial awareness. In an empirical user study with 34 participants, we evaluated two variations of our technique, differing in the presence or absence of intermediate teleportation points between the main points of interest along the route. While visiting additional intermediate points was objectively less efficient, our results indicate significant benefits of this approach regarding the user's spatial awareness and perception of interface dependability. However, the user's perception of flow, presence, attractiveness, perspicuity, and stimulation did not differ significantly. The overall positive reception of our approach encourages further research into semi-automated locomotion based on teleportation and provides initial insights into the design space of successful techniques in this domain.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Visual Computing Institute"
            }
          ],
          "personId": 172304
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Visual Computing Institute"
            }
          ],
          "personId": 172400
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172392
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Christchurch",
              "institution": "University of Canterbury",
              "dsl": "HIT Lab NZ"
            }
          ],
          "personId": 172434
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172254
        }
      ]
    },
    {
      "id": 172450,
      "typeId": 13796,
      "durationOverride": 15,
      "title": "Development and Validation of a 3D Pose Tracking System towards XR Home Training to Relieve Back Pain",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-3006",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172754
      ],
      "eventIds": [],
      "abstract": "Back pain significantly impacts society, leading to substantial economic costs and reducing individuals’ quality of life. A digital XR physiotherapist could support adherence to home-based training programs, thereby potentially enhancing treatment effectiveness. For the system to provide accurate biofeedback, which is crucial to the success if the system, it must be capable of tracking exercise execution reliably and accurately. \r\nIn this paper we present the design of a robust 4-Kinect system, capable of tracking human 3D pose, to be used in an autonomous, home-based XR rehabilitation program. The system is evaluated against OpenPose and validated using a marker-based Vicon system, considered the gold-standard, in a study involving 20 healthy participants. \r\nThe results show that the Kinects overall have a lower absolute positional error, with a median of 1.2 cm, than OpenPose, with a median of 2.0 cm and a lower median angular error of 5.2° over all keypoints (OpenPose: 5.9°). Furthermore, the time courses of the Kinect joint positions exhibit a higher correlation to the gold standard compared to the OpenPose system, as confirmed by the results of a Bland-Altman analysis. Generally, the joints of the lower body could be tracked with a higher level of accuracy than that of the upper body. \r\nThe study reveals that the multi-Kinect-system is overall more robust and tracks exercises with higher accuracy than the multi-OpenPose system, which makes it better suited for a quantitative XR training program for home use.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Applied Sciences Trier",
              "dsl": "Computer Sciences"
            }
          ],
          "personId": 172339
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172407
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "54208",
              "city": "Trier",
              "institution": "University of Applied Sciences Trier",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172337
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied science",
              "dsl": ""
            }
          ],
          "personId": 172343
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 172318
        }
      ]
    },
    {
      "id": 172451,
      "typeId": 13796,
      "title": "Simulating Object Weight in Virtual Reality: The Role of Absolute Mass and Weight Distributions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-1103",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172756
      ],
      "eventIds": [],
      "abstract": "Weight interfaces enable users of Virtual Reality (VR) to perceive the weight of virtual objects, significantly enhancing realism and enjoyment. While research on these systems primarily focused on their implementation, little attention has been given to determining the weight to be rendered by them: As the perceived weight of objects is influenced not only by their absolute mass, but also by their weight distribution and prior expectations, it is currently unknown which simulated mass provides the most realistic representation of a given object. We conducted a study, in which 30 participants chose the best fitting weight for 54 virtual object configurations, whereby we systematically varied the virtual objects' visual mass, their weight distribution, and the position of the physical mass on the grip. Our Bayesian analysis suggests that the visual weight distribution of objects does not affect which absolute physical mass best represents them, whereas the position of the provided physical mass does. Additionally, participants overweighted virtual objects with lower visual mass while underweighting objects with higher visual mass. We discuss how these findings can be leveraged by designers of weight interfaces and VR experiences to optimize realism.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 172348
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 172444
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 172405
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 172257
        }
      ]
    },
    {
      "id": 172452,
      "typeId": 13796,
      "title": "Evaluating the Effects of Situated and Embedded Visualisation in Augmented Reality Guidance for Isolated Medical Assistance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-1422",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "One huge advantage of Augmented Reality (AR) is its numerous possibilities of displaying information in the physical world, especially when applying Situated Analytics (SitA). AR devices and their respective interaction techniques allow for supplementary guidance to assist an operator carrying out complex procedures such as medical diagnosis and surgery or industrial maintenance and assembly. Their usage promotes user autonomy by presenting relevant information when the operator may not necessarily possess expert knowledge of every procedure and may also not have access to external help such as in a remote or isolated situation (e.g., International Space Station, middle of an ocean, desert).\r\n\r\nIn this paper, we propose a comparison of two different forms of AR visualisation : An Embedded Visualisation and a Situated Projected Visualisation, with the aim to assist operators with the most appropriate visualisation format when carrying out procedures (medical in our case). To evaluate these forms of visualisation, we carried out an experiment involving 23 participants possessing latent/novice medical knowledge of similar medical certification (Encompassing workplace safety risk management under EN ISO 14971). These participant profiles were representative of operators who are medically trained yet do not apply their knowledge every day (e.g., an astronaut in orbit, a sailor out at sea or perhaps a soldier in hostile territory).\r\n\r\nWe discuss our findings which include the advantages of Embedded Visualised Information in terms of precision compared to Situated/Projected Information with the accompanying limitations in addition to future improvements to our proposition. We conclude with the prospects of our work, notably the continuation and possibility of evaluating our proposition in a less controlled and real context in collaboration with our national space agency.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC UMR 6285 / ENIB",
              "dsl": ""
            }
          ],
          "personId": 172375
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC, CNRS, ENIB",
              "dsl": ""
            }
          ],
          "personId": 172396
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC UMR 6285",
              "dsl": ""
            }
          ],
          "personId": 172346
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC UMR 6285 / ENIB",
              "dsl": ""
            }
          ],
          "personId": 172279
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC UMR 6285 / ENIB",
              "dsl": ""
            }
          ],
          "personId": 172383
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lorient",
              "institution": "Groupe Hospitalier Bretagne Sud",
              "dsl": ""
            }
          ],
          "personId": 172366
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "ENIB",
              "dsl": "Lab-STICC"
            }
          ],
          "personId": 172273
        }
      ]
    },
    {
      "id": 172453,
      "typeId": 13796,
      "title": "A Critical Review of Virtual and Extended Reality Immersive Police Training: Application Areas, Benefits & Vulnerabilities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9023",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "Training is integral to a Police officer's ability to respond swiftly and effectively to both commonly occurring and emergency situations. Virtual and Extended Reality headsets have promised to enhance police training through the delivery of immersive simulations able to be conducted anywhere, anytime, with increasing degrees of perceptual realism and plausibility. However, whilst we know much about the benefits of XR training through individual examples, little consideration has been given to holistically reviewing the available evidence regarding not just those benefits, but crucially also the potential issues posed by XR police training. In this paper, we summarise the evidenced benefits and types of XR police training through a formative targeted literature review (n=41 publications). Informed by this review, we then reflect on the prospective technical, security, social and legal issues posed by XR police training, identifying four areas where issues or vulnerabilities exist: training content, trainees / trainers, systems and devices, and state and institutional stakeholders. Across these, we highlight significant concerns around e.g. the validity and efficacy of training; the short- and long-term psychological impact and risks of trauma; the safety and privacy risks posed to trainees and trainers, in particular through data leakage, breaches and immersive attacks; and the risks to the institutions leveraging said training, potentially undermining public trust through unvalidated training. We aim to provoke further consideration of the risks posed by immersive training, encouraging  end-user communities (e.g. police forces) to more openly reflect on the risks posed, so we can ultimately move towards transparent, validated, trusted training that is evidenced to improve policing outcomes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": ""
            }
          ],
          "personId": 172303
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 172256
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Coventry",
              "institution": "Coventry University",
              "dsl": ""
            }
          ],
          "personId": 172357
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal Holloway, University of London",
              "dsl": ""
            }
          ],
          "personId": 172261
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": ""
            }
          ],
          "personId": 172446
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Science, Technology, Engineering and Public Policy"
            }
          ],
          "personId": 172263
        }
      ]
    },
    {
      "id": 172454,
      "typeId": 13796,
      "title": "Exploring Visual Conditions in Virtual Reality for the Teleoperation of Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6613",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172753
      ],
      "eventIds": [],
      "abstract": "In the teleoperation of robots, the absence of proprioception means that visual information plays a crucial role. Previous research has investigated methods to offer optimal vantage points to operators during teleoperation, with virtual reality (VR) being proposed as a mechanism to give the operator intuitive control over the viewpoint for improved visibility and interaction. However, the most effective perspective for robot operation and the optimal portrayal of the robot within the virtual environment remain unclear. This paper examines the impact of various visual conditions on users’ efficiency and preference in controlling a simulated robot via VR. We present a user study that compares two operating perspectives and three robot appearances. The findings indicate mixed user preferences and highlight distinct advantages associated with each perspective and appearance combination. We conclude with recommendations on selecting the most beneficial perspective and appearance based on specific application requirements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 172267
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 172342
        }
      ]
    },
    {
      "id": 172455,
      "typeId": 13796,
      "title": "Hands or Controllers? How Input Devices and Audio Impact Collaborative Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-7922",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Advancing virtual reality technologies are enabling real-time virtual-face to virtual-face communication. Hand tracking systems that are integrated into Head-Mounted Displays (HMD) enable users to directly interact with their environments and with each other using their hands as opposed to using controllers. Due to the novelties of these technologies our understanding of how they impact our interactions is limited. In this paper, we investigate the consequences of using different interaction control systems, hand tracking or controllers, when interacting with others in a virtual environment. We design and implement NASA's Survival on the Moon teamwork evaluation exercise in virtual reality (VR) and test for effects with and without allowing verbal communication. We evaluate social presence, perceived comprehension, team cohesion, group synergy, task workload, as well as task performance and duration. \r\n    Our findings reveal that audio communication significantly enhances social presence, perceived comprehension, and team cohesion, but it also increases effort workload and negatively impacts group synergy. The choice of interaction control systems has limited impact on various aspects of virtual collaboration in this scenario, although participants using hand tracking reported lower effort workload, while participants using controllers reported lower mental workload in the absence of audio.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 172323
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 172435
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bamberg",
              "institution": "University of Bamberg",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 172441
        }
      ]
    },
    {
      "id": 172456,
      "typeId": 13796,
      "durationOverride": 15,
      "title": "Motion Passwords",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-3469",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172754
      ],
      "eventIds": [],
      "abstract": "This paper introduces \"Motion Passwords\", a novel biometric authentication approach where virtual reality users verify their identity by physically writing a chosen word in the air with their hand controller. This method allows combining three layers of verification: knowledge-based password input, handwriting style analysis, and motion profile recognition. As a first step towards realizing this potential, we focus on verifying users based on their motion profiles. We conducted a data collection study with 48 participants, who performed over 3800 Motion Password signatures across two sessions. We assessed the effectiveness of feature-distance and similarity-learning methods for motion-based verification using the Motion Passwords as well as specific and uniform ball-throwing signatures used in previous works. In our results, the similarity-learning model was able to verify users with the same accuracy for both signature types. This demonstrates that Motion Passwords, even when applying only the motion-based verification layer, achieve reliability comparable to previous methods. This highlights the potential for Motion Passwords to become even more reliable with the addition of knowledge-based and handwriting style verification layers. Furthermore, we present a proof-of-concept Unity application demonstrating the registration and verification process with our pretrained similarity-learning model. We publish our code, the Motion Password dataset, the pretrained model, and our Unity prototype on https://drive.google.com/drive/folders/1Zy2avoab6EZchMcxJszhvybleUBGk75N?usp=share_link .",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 172272
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 172275
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Würzburg",
              "institution": "HCI Group",
              "dsl": "Julius-Maximilians-Universität Würzburg"
            }
          ],
          "personId": 172280
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 172393
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction (HCI) Group"
            }
          ],
          "personId": 172376
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172362
        }
      ]
    },
    {
      "id": 172457,
      "typeId": 13796,
      "title": "HistoLab VR: A User Elicitation Study Exploring the Potential of Virtual Reality Game-based Learning for Hazard Awareness",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-2897",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Occupational medicine is a vital field for workplace safety and health but often encounters challenges in engaging students and effectively communicating subtle yet critical workplace hazards. To tackle these issues, we developed HistoLab VR, a virtual reality (VR) game that immerses participants in a histology lab environment based on real-world practice. Our comprehensive user study with 17 students and experts assessed the game's impact on hazard awareness, interest in occupational medicine, and user experience through quantitative and qualitative measures. Our findings show that HistoLab VR not just immersed participants in a relatable histology lab worker experience but that it effectively raised awareness about subtle hazards and conveyed the inherent stress of the job. We discuss our results and highlight the potential of VR as a valuable educational tool for occupational medicine training.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172250
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Zurich",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172372
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172420
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "University Zurich",
              "dsl": ""
            }
          ],
          "personId": 172402
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University of Zurich",
              "dsl": "Division of Occupational and Environmental Medicine"
            }
          ],
          "personId": 172358
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zürich",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172359
        }
      ]
    },
    {
      "id": 172458,
      "typeId": 13796,
      "title": "Effects of Different Tracker-driven Direction Sources on Continuous Artificial Locomotion in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-2452",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "Continuous artificial locomotion in VR typically involves users selecting their direction using controller input, with the forward direction determined by the Head, Hands, or less commonly, the Hip. The effects of these different sources on user experience are under-explored, and Feet have not been used as a direction source. To address these gaps, we compared these direction sources, including a novel Feet-based technique. A user study with 22 participants assessed these methods in terms of performance, preference, motion sickness, and sense of presence. Our findings indicate high levels of presence and minimal motion sickness across all methods. Performance differences were noted in one task, where the Head outperformed the Hand. The Hand method was the least preferred, feeling less natural and realistic. The Feet method was found to be more natural than the Head and more realistic than the Hip. This study enhances understanding of direction sources in VR locomotion and introduces Feet-based direction as a viable alternative.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "National and Kapodistrian University of Athens",
              "dsl": "Department of Informatics and Telecommunications/School of Science"
            }
          ],
          "personId": 172277
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "National Kapodistrian University of Athens",
              "dsl": ""
            }
          ],
          "personId": 172384
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "Athena Research Center",
              "dsl": ""
            }
          ],
          "personId": 172429
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "National and Kapodistrian University of Athens",
              "dsl": "Department of Informatics and Telecommunications"
            }
          ],
          "personId": 172336
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "National and Kapodistrian University of Athens",
              "dsl": ""
            }
          ],
          "personId": 172378
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Athens",
              "institution": "National and Kapodistrian University of Athens",
              "dsl": "Department of Informatics and Telecommunications"
            }
          ],
          "personId": 172364
        }
      ]
    },
    {
      "id": 172459,
      "typeId": 13796,
      "title": "Game-Based Motivation: Enhancing Learning with Achievements in a Customizable Virtual Reality Environment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4497",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Digital learning experiences that promote interactive learning and engagement are becoming increasingly relevant. Educational games can be used to generate an engaging learning atmosphere that allows knowledge acquisition through hands-on activities. Combining these games with virtual reality (VR) technology allows users to interact with virtual environments, leading to a highly immersive learning experience. In this study, we explore how game achievements impact students' motivation and learning in a customizable VR learning environment. Using an AB study involving 50 students, we utilized an interactive wave simulation to assess motivation, engagement, and the overall learning experience. Data collection involved standardized questionnaires, along with tracking interaction time and different interactions within the virtual environment. The findings revealed that users who earned game achievements to unlock customization features felt significantly more accomplished when they successfully mastered challenges and obtained all achievements. However, it was observed that adding achievements could also create pressure on students, leading to feelings of embarrassment when facing task failures. While achievements have the potential to enhance engagement and motivation, their excessive use may lead to distractions, anxiety, and reduced overall engagement. It shows that is crucial to find a good balance in employing game achievements within educational environments to ensure they contribute positively to the learning experience without causing undue stress or deterring learners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172317
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172443
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": ""
            }
          ],
          "personId": 172431
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": ""
            }
          ],
          "personId": 172329
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU",
              "dsl": ""
            }
          ],
          "personId": 172291
        }
      ]
    },
    {
      "id": 172460,
      "typeId": 13796,
      "title": "Neural Motion Tracking: Formative Evaluation of Zero Latency Rendering",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4354",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172753
      ],
      "eventIds": [],
      "abstract": "Low motion-to-photon latencies between physical movement and rendering updates are crucial for an immersive VR experience avoiding users' discomfort and sickness. Current methods aim to minimize the delay between the motion measurement and rendering at the cost of increasing technical complexity and possibly decreasing accuracy. By relying on capturing physical motion, these strategies will, by nature, not result in zero latency rendering or will be based on prediction and resulting uncertainty. In this paper, we present and evaluate an alternative concept and proof of principle for VR motion tracking that enables motion-to-photon latencies of zero and below zero in time. In contrast to measuring physical activity, we termed our concept neural motion tracking, which we define as the sensing and assessment of motion through human neural activation of the somatic nervous system. The key principle is that we aim to utilize the physiological timeframe between a user's intention and physical execution of motion and thus aim to foresee upcoming motion ahead of the physical movement. This is achieved by sampling preceding electromyographic signals before the muscle activation. The EMD between potential change in the muscle activation and actual physical movement opens a gap in which measurement can be taken and evaluated before the physical motion. In a first proof of principle, we evaluated two activities, arm bending and head rotation, and compared them for the first time to professional optical tracking. The measured latencies quantify that it is possible to predict muscle movement and update the rendering up to 2 ms before its physical execution, which is assessed by optical tracking after approximately 4 ms. However, to make the best use of this advantage, EMG sensor data should be as high quality as possible (i.e., low noise and from muscle-near electrodes). Our results empirically quantify this characteristic for the first time when compared to state-of-the-art optical tracking systems for VR. We discuss our results and potential pathways to motivate further work toward marker- and latency-less motion tracking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "School of Medicine and Health"
            }
          ],
          "personId": 172363
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen",
              "institution": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
              "dsl": "Human-Centered Computing and Extended Reality, Department Artificial Intelligence in Biomedical Engineering"
            }
          ],
          "personId": 172313
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen",
              "institution": "Friedrich-Alexander-Universität Erlangen-Nürnberg",
              "dsl": "Human-Centered Computing and Extended Reality"
            }
          ],
          "personId": 172374
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Human-Centered Computing and Extended Reality, TUM School of Medicine"
            }
          ],
          "personId": 172262
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "München",
              "institution": "Technical University of Munich (TUM)",
              "dsl": "Human-Centered Computing and Extended Reality"
            }
          ],
          "personId": 172292
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "School of Medicine and Health"
            }
          ],
          "personId": 172406
        }
      ]
    },
    {
      "id": 172461,
      "typeId": 13796,
      "title": "Aerial Imaging System to Reproduce Reflections in Specular Surfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6796",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "A table-top reflective aerial imaging system can display digital information as if it existed on a specular horizontal surface, such as a marble or acrylic table-top. This system allows a user sitting in a chair to naturally look down at an aerial image displayed on a table, thus integrating the aerial image into daily life. However, although it displays an aerial image on a specular surface, it does not reproduce the reflected image, thus failing to achieve optical consistency. To improve the presence of aerial images, we propose a new table-top reflective aerial imaging system that not only displays aerial images on a specular surface but also reproduces the reflected images in that surface. The proposed system consists of an aerial imaging optical system that can display independent aerial images on and in a specular surface, and a reflected image reproduction method that designs the luminance of the aerial image inside the specular surface according to the actual measured material reflectance. We implemented a prototype to verify the principle of aerial imaging optics and evaluated the naturalness of the reflected image reproduced by our method through user experimentation. The results show that the difference between the material of the specular surface and the shape of the aerial image affects whether the reflected image produced by the proposed method is perceived as natural or not.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 172298
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 172368
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 172326
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 172430
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 172413
        }
      ]
    },
    {
      "id": 172462,
      "typeId": 13796,
      "title": "How Different Is the Perception of Vibrotactile Texture Roughness in Augmented versus Virtual Reality?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9743",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172756
      ],
      "eventIds": [],
      "abstract": "Wearable haptic devices can modify the haptic perception of an object touched directly by the finger in a portable and unobtrusive way. In this paper, we investigate whether such wearable haptic augmentations are perceived differently in Augmented Reality (AR) vs. Virtual Reality (VR), as well as the effect of touching the augmented environment with a virtual hand instead of one's own hand. We first designed a system for real-time rendering of vibrotactile virtual textures without constraints on hand movements, integrated with an immersive visual AR/VR headset. We then conducted a psychophysical study with 20 participants to evaluate the haptic perception of virtual roughness textures on a real surface touched directly with the finger (1) without visual augmentation, (2) with a realistic virtual hand rendered in AR, and (3) with the same virtual hand in VR. On average, participants overestimated the roughness of haptic textures when touching with their real hand alone (without visual augmentation) and underestimated the roughness of haptic textures when touching with a virtual hand in AR, with VR in between. Their exploration behaviour was also slower in VR than with their real hand alone, although their subjective evaluation of the texture was not affected. This suggests an effect as if the same augmentation touched with a virtual hand in AR was perceived as less rough than when touched with the real hand alone. Finally, we discuss how the perceived visual delay of the virtual hand might produce this effect.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Univ Rennes, Inria, CNRS, IRISA",
              "dsl": ""
            }
          ],
          "personId": 172389
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "CNRS, Univ Rennes, Inria, IRISA",
              "dsl": ""
            }
          ],
          "personId": 172345
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Univ Rennes, Inria, CNRS, IRISA",
              "dsl": ""
            }
          ],
          "personId": 172353
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Univ. Rennes, INSA, IRISA, Inria",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Institut Universitaire de France",
              "dsl": ""
            }
          ],
          "personId": 172437
        }
      ]
    },
    {
      "id": 172463,
      "typeId": 13796,
      "title": "MeetingBenji: Tackling Cynophobia with Virtual Reality, Gamification, and Biofeedback",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-5565",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "Phobias, particularly animal phobias like cynophobia (fear of dogs), disrupt the lives of those affected by, for instance, limiting outdoor activities. While virtual reality exposure therapy (VRET) has emerged as a potential treatment for this phobia, these efforts have been limited by high dropout rates and lack of control of anxiety in people who suffer from cynophobia. Inspired by these challenges, we present MeetingBenji, a VRET system for cynophobia that uses (i) gamification to enhance motivation and engagement, and (ii) biofeedback to facilitate self-control and reduce physiological responses. In a study (N=10) that compared the effects of displaying dogs in 3D scenes and 360º videos using the Behavioral Approach Test (BAT) - in which participants are increasingly exposed to the source of phobia - participants reported a high level of immersion to the exposure sequence. Further, they reported feeling more anxiety with 3D content than 360º video (60%), lower heart rates in the presence of biofeedback (between 1.71% and 7.46%), and improved self-control across the three exposure levels. They appreciated our gamified elements -- completing all exposure levels. This study suggests that VRET with gamification and biofeedback is an effective approach to stimulate the habituation of people with cynophobia.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, University of Lisbon",
              "dsl": ""
            }
          ],
          "personId": 172281
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, University of Lisbon",
              "dsl": "ITI / LARSyS"
            }
          ],
          "personId": 172276
        }
      ]
    },
    {
      "id": 172464,
      "typeId": 13796,
      "title": "Interactive Multi-GPU Light Field Path Tracing Using Multi-Source Spatial Reprojection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-5686",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172753
      ],
      "eventIds": [],
      "abstract": "Path tracing combined with multi-view displays enables progress to-\r\nwards achieving ultrarealistic virtual reality. However, multi-view\r\ndisplays based on light field technology impose a heavy work-\r\nload for real-time graphics due to the large number of views to\r\nbe rendered. In order to achieve low latency performance, com-\r\nputational effort can be reduced by path tracing only some views\r\n(source views), and synthesizing the remaining views (target views)\r\nthrough spatial reprojection, which reuses path traced pixels from\r\nsource views to target views. Deciding the number of source views\r\nwith respect to the computational resources is not trivial, since spa-\r\ntial reprojection introduces dependencies in the otherwise trivially\r\nparallel rendering pipeline and path tracing multiple source views\r\nincreases the computation time.\r\nIn this paper, we demonstrate how to reach near-perfect linear\r\nmulti-GPU scalability through a coarse-grained distribution of the\r\nlight field path tracing workload. Our multi-source method path\r\ntraces a single source view per GPU, which helps decreasing the\r\nnumber of dependencies. Reducing dependencies reduces the over-\r\nhead of image transfers and G-Buffer rasterization used for spatial\r\nreprojection. In a node of 4× RTX A6000 GPUs, given 4 source\r\nviews, we reach a light field rendering frequency of 3–19 Hz, which\r\ncorresponds to interactive rate. On four test scenes, we outperform\r\nstate-of-the-art multi-GPU light field path tracing pipelines, achiev-\r\ning a speedup of 1.65× up to 4.63× for 1D light fields of dimension\r\n100 × 1, each view having a resolution of 768 × 432, and 1.51× up\r\nto 3.39× for 2D stereo near-eye light fields of size 12 × 6 (left eye:\r\n6 × 6 views and right eye: 6 × 6 views), 1024 × 1024 per view.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": ""
            }
          ],
          "personId": 172265
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": ""
            }
          ],
          "personId": 172415
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": ""
            }
          ],
          "personId": 172379
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Sundsvall",
              "institution": "Mid Sweden University",
              "dsl": ""
            }
          ],
          "personId": 172399
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Sundsvall",
              "institution": "Mid Sweden University",
              "dsl": ""
            }
          ],
          "personId": 172442
        }
      ]
    },
    {
      "id": 172465,
      "typeId": 13796,
      "title": "Exploring Presence in Interactions with LLM-Driven NPCs: A Comparative Study of Speech Recognition and Dialogue Options",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-7643",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "Head-mounted displays (HMDs) such as virtual reality (VR) headsets, are an effective way of immersing the user in a virtual environment (VE). HMDs, like other mediated experiences, seek to make the user forget the mediated nature of the experience. We break down the concept of presence, and how to make the user feel present within the VE. We combine theories of how immersive systems create a sense of place illusion (PI) and plausibility illusion (Psi), and how presence can be achieved when engaging with social actors within a medium. By conducting a literature review of the utilization of large-language models (LLMs) in narrative content, we find that research into this topic is scarce, but interacting with LLM NPCs might improve user experience. A state-of-the-art analysis of VR games was done to map out current user-NPC interaction implementations, and how this can be improved using LLMs. UI interfaces (dialogue options) were the most prevalent option for interacting with NPCs, and only one game allowed the player to speak to NPCs using voice commands. Previously, voice commands have been limited in their ability, but advancements in LLMs allow for user-NPC interactions to approach the levels of abstraction in human-to-human interactions. This paper covers the development of a murder mystery game used to test the difference in levels of presence and game experience between two input modalities used in user-NPC interactions: `Dialogue options' and `speech recognition'. Only the challenge component of the game-experience questionnaire showed a significant difference between modalities, but interaction with NPCs through the use of speech recognition tended to score higher in all three social presence components.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 172432
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 172411
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 172249
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design, and Media Technology"
            }
          ],
          "personId": 172259
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design, and Media Technology"
            }
          ],
          "personId": 172369
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 172426
        }
      ]
    },
    {
      "id": 172466,
      "typeId": 13796,
      "title": "Investigating the Impact of Odors and Visual Congruence on Motion Sickness in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4916",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172756
      ],
      "eventIds": [],
      "abstract": "Motion sickness is a prevalent side effect of exposure to virtual reality (VR). Previous work found that pleasant odors can be effective in alleviating symptoms of motion sickness such as nausea. However, it is unknown whether pleasant odors that do not match the anticipated scent of the virtual environment are also effective as they could, in turn, amplify symptoms such as disorientation. Therefore, we conducted a study with 24 participants experiencing a pleasant odor (rose) and an unpleasant odor (garlic) while being immersed in a virtual environment involving either virtual roses or garlic. We found that participants had lower motion sickness when experiencing the rose odor, however, only in the rose environment. Accordingly, we also showed that the sense of disorientation was lower for the rose odor, however, only while being immersed in the rose environment. Results indicate that whether pleasant odors are effective in alleviating motion sickness symptoms depends on the visual appearance of the virtual environment. We discuss possible explanations for such effects to occur. Our work contributes to the goal of mitigating visually induced motion sickness in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 172251
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 172306
        }
      ]
    },
    {
      "id": 172467,
      "typeId": 13796,
      "title": "Stand Alone or Stay Together: An In-situ Experiment of Mixed-Reality Applications in Embryonic Anatomy Education",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4738",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Understanding the location and function of anatomical structures is essential in medical education. Where traditional educational media and methods reach their limits, mixed-reality (MR) environments can provide effective learning support because of their high interactivity and spatial visualization capabilities. However, the underlying design and pedagogical requirements are as diverse as the technologies themselves. This paper examines the effectiveness of individual- and collaborative learning environments for anatomy education, using embryonic heart development as an example. Both applications deliver the same content using identical visualizations and hardware but differ in interactivity and pedagogical approach. The environments were evaluated in a user study with medical students (n = 90) during their examination phase, assessing usability, user experience, social interaction/co-presence, cognitive load, and personal preference. Additionally, we conducted a knowledge test before and after an MR learning session to determine educational effects compared to a conventional anatomy seminar. Results indicate that the individual learning environment was generally preferred. However, no significant difference in learning effectiveness could be shown between the conventional approach and the two MR applications. This suggests that both can effectively complement traditional seminars despite their different natures. Moreover, the novelty of MR applications seemed to overshadow usability and user experience measures. This study contributes to understanding how different MR settings could be tailored for anatomical education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 172294
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Medical Faculty"
            }
          ],
          "personId": 172439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 172404
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 172324
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Medical Faculty"
            }
          ],
          "personId": 172356
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Medical Faculty"
            }
          ],
          "personId": 172271
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 172327
        }
      ]
    },
    {
      "id": 172468,
      "typeId": 13796,
      "title": "The Influence of a Low-Resolution Peripheral Display Extension on the Perceived Plausibility and Presence",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4959",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "The Field of View (FoV) is a central technical display characteristic of Head-Mounted Displays (HMDs), which has been shown to have a notable impact on important aspects of the user experience. For example, an increased FoV has been shown to foster a sense of presence and improve peripheral information processing, but it also increases the risk of VR sickness. This article investigates the impact of a wider, but inhomogenous FoV on the perceived plausibility, also measuring its impact on presence, spatial presence, and VR sickness as a comparison to and replication of effects from prior work. We developed a low-resolution peripheral display extension to pragmatically increase the FoV, taking into account the lower peripheral acuity of the human eye. While this design results in inhomogenous resolutions of HMDs at the display edges, it also is a low complexity and low-cost extension. However, its effects on important VR qualities have to be identified. We conducted a pre-study (30 participants) and a user study (27 participants). In a randomized 2x3 within-subject design, participants played three rounds of bowling in VR, both with and without the display extension. Two rounds contained incongruencies to induce breaks in plausibility. In the user study, we enhanced one incongruency to make it more noticeable and improved the shortcomings of the display extension that had previously been identified. However, no effect of the low-resolution FoV extension was measured in terms of perceived plausibility, presence, spatial presence, or VR sickness in both studies. We did find that one of the incongruencies was able to cause a break in plausibility without the extension, confirming the results of a previous study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Psychology of Intelligent Interactive Systems"
            }
          ],
          "personId": 172301
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-Universität Würzburg",
              "dsl": ""
            }
          ],
          "personId": 172403
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Human-Computer Interaction Group, University of Würzburg",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Psychology of Intelligent Interactive Systems, University of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 172308
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Psychology of Intelligent Interactive Systems"
            }
          ],
          "personId": 172354
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172362
        }
      ]
    },
    {
      "id": 172469,
      "typeId": 13796,
      "title": "The Effects of Electrical Stimulation of Ankle Tendons on Redirected Walking with the Gradient Gain",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9272",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "As a redirected walking technique, a method has been proposed to enable users to walk in an undulating virtual space even in a flat physical environment by setting the slope of the floor in the virtual environment to be different from that in the physical environment without causing discomfort. However, the slope range in which discrepancies between visual and proprioceptive sensations are not perceived is limited, restricting the slopes that can be presented. In this study, we proposed redirected walking using electrical stimulation of the Achilles and tibialis anterior muscle tendons, extending the applicable slope range of redirected walking without compromising the natural gait sensation. Electrical stimulation of the ankle tendons affects the proprioceptive sensation and gives the illusion of tilting in the standing posture, expanding the applicable slope range. Two experiments showed that the proposed method improved the experience of uphill and downhill walking in terms of the range of the virtual slope where a high naturalness of gait and a high congruency of visual and proprioceptive sensations are maintained. Notably, electrical stimulation of the Achilles tendons significantly improved the naturalness of the walking experience during virtual downhill walking, which has been considered more challenging in previous studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 172300
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 172330
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Bunkyo-ku",
              "institution": "Virtual Reality Educational Research Center",
              "dsl": ""
            }
          ],
          "personId": 172335
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 172270
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "the University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 172395
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 172295
        }
      ]
    },
    {
      "id": 172470,
      "typeId": 13796,
      "title": "Exploring User Placement for VR Remote Collaboration in a Constrained Passenger Space",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9096",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Extended Reality (XR) offers the potential to transform the passenger experience by allowing users to inhabit varied virtual spaces for entertainment, work or social interaction, whilst escaping the constrained transit environment. XR allows remote collaborators to feel like they are together and enables them to perform complex 3D tasks. However, the social and physical constraints of the passenger space pose unique challenges to productive and socially acceptable collaboration. Using a collaborative VR puzzle task, we examined the effects of five different f-formations of collaborator placement and orientation in an interactive workspace on social presence, task workload, and implications for social acceptability. Our quantitative and qualitative results showed that face-to-face formations were preferred for tasks with a high need for verbal communication but may lead to social collisions, such as inadvertently staring at a neighbouring passenger, or physical intrusions, such as gesturing in another passenger’s personal space. More restrictive f-formations, however, were preferred for passenger use as they caused fewer intrusions on other passengers’ visual and physical space",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Telecom Paris/Institut Polytechnique de Paris",
              "dsl": "Informatique et Reseaux Dept"
            }
          ],
          "personId": 172417
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 172397
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172341
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington ",
              "institution": "Victoria University of Wellington",
              "dsl": "School of Design Innovation"
            }
          ],
          "personId": 172331
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 172256
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 172428
        }
      ]
    },
    {
      "id": 172471,
      "typeId": 13796,
      "title": "Enriching Industrial Training Experience in Virtual Reality with Pseudo-Haptics and Vibrotactile Stimulation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4637",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172756
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) technology facilitates effective, flexible, and safe industrial training for novice technicians when on-site training is not feasible. However, researchers have found that training in VR may be less successful than traditional learning approaches in real-world settings and enriching haptic interactions may be the key to improve virtual training. In this study, we integrated pseudo-haptic feedback from motion delay with vibrotactile stimulation to enhance the sense of presence, enjoyment, and the perception of physical properties in VR, which may be crucial factors when rendering faithful simulations. The impact of combined haptic support was assessed in a complex industrial training procedure completing a variety of tasks such as vacuum cleaning. The results indicate that vibrotactile cues are beneficial for presence and enjoyment, whereas pseudo-haptic illusions effectively enable kinesthetic sensations. Furthermore, multimodal haptic feedback that mixed the two yielded the most advantageous outcomes complementing each other. Our findings highlight the potential of the pseudo-haptic and vibrotactile fusion in industrial training scenarios presenting practical implications of the state-of-the-art haptic technologies for virtual learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172297
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": "HCI Group"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172419
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Electrical Engineering"
            }
          ],
          "personId": 172274
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172351
        }
      ]
    },
    {
      "id": 172472,
      "typeId": 13796,
      "title": "Contextual Matching Between Learning and Testing Within VR Does Not Always Enhance Memory Retrieval",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6060",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "Episodic memory is influenced by environmental contexts, such as location and auditory stimuli. The most well-known effect is the reinstatement effect, which refers to the phenomenon where contextual matching between learning and testing enhances memory retrieval. Previous studies have investigated whether the reinstatement effect can be observed within immersive virtual environments. However, only a limited number of studies have reported a significant reinstatement effect using virtual reality, while most have failed to detect it. In this study, we re-examined the reinstatement effect using 360-degree video-based virtual environments. Specifically, we carefully selected virtual environments to elicit different emotional responses, which has been suggested as a key factor in inducing a robust reinstatement effect in the physical world. Surprisingly, we found a significant reversed reinstatement effect with a large effect size. This counter-intuitive result suggests that contextual congruence does not necessarily enhance memory and may even interfere with it. This outcome may be explained by the retrieval-induced forgetting phenomenon, but further exploration is needed. This finding is particularly important for virtual reality-based educational applications and highlights the need for a deeper understanding of the complex interactions between memory and contextual cues within virtual environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 172386
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "the University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 172395
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 172295
        }
      ]
    },
    {
      "id": 172473,
      "typeId": 13796,
      "title": "Optimizing spatial resolution in head-mounted displays: evaluating characteristics of peripheral visual field",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4265",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "An ideal head-mounted display (HMD) is defined as a device that provides a visual experience indistinguishable from that given by the naked eyes. Such an HMD must display images with spatial resolution surpassing that of the human visual system. However, excessively high spatial resolution is resource-wasting and inefficient. To optimize this balance, we evaluated the spatial resolution characteristics of the human visual system in the peripheral visual area. Our experiment was performed based on head-centered coordinate system, acknowledging that users can move their eyes freely within the HMD housing fixed on users' head. We measured threshold eccentricities where low-pass filtered noise patterns could be distinguished from intact noise patterns, manipulating cut-off spatial frequencies between one to eight cycles per degree. The results revealed clear asymmetries between the temporal and nasal visual fields, as well as between the upper and lower visual fields. In the temporal and lower fields, lower cut-off spatial frequencies resulted in higher eccentricity thresholds. Notably, the smaller impact of spatial frequencies in the nasal and upper visual fields is likely due to visual obstruction by facial structures such as the nose. Our findings can play a role of a standard for pixel arrangement design in an ideal HMD.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Japan Broadcasting Corporation",
              "dsl": ""
            }
          ],
          "personId": 172414
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Japan Broadcasting Corporation",
              "dsl": ""
            }
          ],
          "personId": 172258
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Japan Broadcasting Corporation",
              "dsl": ""
            }
          ],
          "personId": 172373
        }
      ]
    },
    {
      "id": 172474,
      "typeId": 13796,
      "title": "iStrayPaws: Immersing in a Stray Animal's World through First-Person VR to Bridge Human-Animal Empathy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-7079",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "While Virtual Reality Perspective-Taking (VRPT) demonstrates its efficiency in inducing empathy, its application primarily focuses on vulnerable humans, not animals. Existing animal-related works mainly targets farm animals and wildlife. In this work, we focus on stray animals and introduce iStrayPaws, a VRPT system that simulates stray animals’ challenging lives. The system offers users an immersive first-person journey into the world of stray animals encountering different difficulties like inclement weather, hunger, and illnesses. Enriched with audio-visual and kinesthetic design, the system seeks to deepen users’ understanding of stray animals’ life and foster profound emotional connections. To evaluate the system, a user study was conducted, which showed that VRPT recipients exhibited significant improvement in both state and trait empathy compared to traditional method. Our research not only delivers a novel, accessible, and interactive animal empathy experience but also provides innovative solutions for addressing stray animal issues and advancing broader animal welfare work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 172381
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 172387
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 172418
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 172248
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Nanjing",
              "institution": "Southeast University",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 172284
        }
      ]
    },
    {
      "id": 172475,
      "typeId": 13796,
      "title": "Wheelchair Proxemics: interpersonal behaviour between pedestrians and power wheelchair drivers in real and virtual environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6881",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "Immersive environments provide opportunities to learn and transfer skills to real life. This opens up new areas of application, such as rehabilitation, where people with neurological disabilities can learn to drive a power wheelchair (PWC) through the development of immersive simulators. To expose these specific users to daily-life study interaction situations, it is important to ensure realistic interactions with the virtual humans that populate the simulated environment, as PWC users should learn to drive and navigate under everyday conditions. While non-verbal pedestrian-pedestrian interactions have been extensively studied, understanding pedestrian-PWC user interactions during locomotion is still an open research area. Our study aimed to investigate the regulation of interpersonal distance (i.e., proxemics) between a pedestrian and a PWC user in real and virtual situations. We designed 2 experiments in which 1) participants had to reach a goal by walking (respectively driving a PWC) and avoid a static PWC confederate (respectively a standing confederate) and 2) participants had to walk to a goal and avoid a static confederate seated on a PWC in real and virtual conditions. Our results showed that interpersonal distances were significantly different whether the pedestrian avoided the PWC user or vice versa, \r\nbut were not affected by the environment (VR vs. Real). We also showed an influence of the orientation of the person to be avoided. We discuss these findings with respect to pedestrian-pedestrian interactions, as well as their implications for the design of virtual humans interacting with PWC users for rehabilitation applications. In particular, we proposed a proof of concept by adapting existing microscopic crowd simulation algorithms to consider the specificity of pedestrian-PWC user interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Pôle Saint Hélier, rehabilitation center",
              "dsl": ""
            }
          ],
          "personId": 172427
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA Rennes, University of Rennes, IRISA, Inria, CNRS",
              "dsl": ""
            }
          ],
          "personId": 172288
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA Rennes, University of Rennes, IRISA, Inria, CNRS",
              "dsl": ""
            }
          ],
          "personId": 172385
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria, Univ Rennes, CNRS, IRISA - Rennes, France",
              "dsl": ""
            }
          ],
          "personId": 172433
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "INSA Rennes, University of Rennes, IRISA, Inria, CNRS",
              "dsl": ""
            }
          ],
          "personId": 172290
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "University of Rennes 2",
              "dsl": "M2S Lab"
            }
          ],
          "personId": 172401
        }
      ]
    },
    {
      "id": 172476,
      "typeId": 13796,
      "title": "Evaluation of AR Pattern Guidance Methods for a Surface Cleaning Task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-5890",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "In this study, we investigate the efficacy of augmented reality (AR) in enhancing a cleanroom cleaning task by implementing various pattern guidance designs. Cleanroom cleaning is an example of a surface coverage task that is hard to execute where the pattern should be followed correctly and the entire surface should be covered. We developed an AR guidance system for the cleaning procedure and evaluate four distinct pattern guidance methods: (1) breadcrumbs, (2) examples, (3) middle lines, and (4) outlines. We also vary the scale of the instructions, where information is present either on the entire surface or show the instructions as a single step when they are necessary. To measure the performance, accuracy, and user satisfaction associated with each guidance method, we conducted a large-scale (n=864) between-subjects study. Our findings indicate that the single step instructions proved to be more intuitive and efficient than the full instructions, especially for the breadcrumbs. We also discussed the implications of our results for the development of AR applications for surface coverage and pattern optimization, providing a multitude of observations related to instruction behaviors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172447
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172382
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172425
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Diepenbeek",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172289
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172268
        }
      ]
    },
    {
      "id": 172477,
      "typeId": 13796,
      "title": "Superpixel-guided Sampling for Compact 3D Gaussian Splatting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-8503",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose to integrate superpixel-guided sampling into the framework of 3D Gaussian Splatting for novel view synthesis. Given a sequence of frames, where each frame is a pair of RGB-D image and the camera pose that captures the image, we first select the keyframes. Then, we decompose each keyframe image into superpixels, back-project each superpixel’s center into 3D space, and initialize a 3D Gaussian at the back-projected position. This superpixel-guided sampling produces a set of sparse but well-distributed Gaussians, which enables the optimization process to converge quickly. The experimental results show that with a significantly reduced computing cost, we can synthesize a novel view the quality of which is comparable to that generated by the state-of-the-art methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 172252
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 172334
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 172266
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 172408
        }
      ]
    },
    {
      "id": 172478,
      "typeId": 13796,
      "title": "Influence of Rotation Gains on Unintended Positional Drift during Virtual Steering Navigation in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-3252",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "Unintended Positional Drift (UPD) is a phenomenon that occurs during navigation in Virtual Reality (VR). It is characterized by unconscious or unintentional physical movements of the user in the workspace while using a locomotion technique (LT) that does not require physical displacement (e.g., steering, teleportation). Recent work showed that some factors, such as the LT used and the type of trajectory, can influence UPD. However, little is known about the influence of rotation gains (that are commonly used in redirection-based LTs) on UPD during navigation in VR. In this paper, we conducted two user studies to assess the influence of rotation gains on UPD. In the first study, participants had to perform consecutive turns in a corridor virutal environment. In the second study, participants had to freely explore a large office floor and collect spheres. We compared the conditions between rotation gains and without gains, and we also varied the turning angle to perform the turns while considering factors such as sensitivity to cybersickness and the learning effect. We found that rotation gains and lower turning angle decreased UPD during the first study but the presence of rotation gains increased UPD in the second study. This work contributes to the understanding of UPD, which tends to be an overlooked topic and discusses the design implications of these results to improve navigation in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": ""
            }
          ],
          "personId": 172388
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 172296
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 172340
        }
      ]
    },
    {
      "id": 172479,
      "typeId": 13796,
      "title": "Toward Facilitating Search in VR With the Assistance of Vision Large Language Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-7033",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "While search is a common need in Virtual reality (VR) applications, current approaches are cumbersome and tend to require users to type on a mid-air keyboard using controllers in VR or remove VR equipment to search on a computer. We first performed the literature review and a formative study, from which we identified six common searching needs: knowing about one object, knowing about the object’s partial details, knowing objects with environmental context, knowing about interactions with objects, and finding objects within field of view (FOV) and out of  FOV in the VR scene. Informed by the needs, we designed technology probes that leveraged recent advances in Vision Large Language Models and conducted a probe-based study with users to elicit feedback. Based on the findings, we derived design principles for VR designers and developers to consider when designing a user-friendly search interface in VR. While prior work about VR search tended to address specific aspects of search, our work contributes design considerations toward making search easy in VR and potential future directions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": ""
            }
          ],
          "personId": 172350
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Academy of Interdisciplinary Studies"
            }
          ],
          "personId": 172316
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": ""
            }
          ],
          "personId": 172253
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": "Computational Media and Arts Thrust"
            }
          ],
          "personId": 172264
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Tsinghua Shenzhen International Graduate School",
              "dsl": ""
            }
          ],
          "personId": 172355
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong ",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 172332
        }
      ]
    },
    {
      "id": 172480,
      "typeId": 13796,
      "title": "Investigation of Redirection Algorithms in Small Tracking Spaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-5492",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172753
      ],
      "eventIds": [],
      "abstract": "In virtual reality, redirected walking lets users walk in larger virtual spaces than the physical tracking space set aside for their movements. This benefits immersion and spatial navigation compared to virtual locomotion techniques such as teleportation or joystick control. Different algorithms have tried to optimise redirected walking. These algorithms have been tested in simulation in large spaces and with small user studies. However few studies have looked at the user experience of these algorithms in small tracking spaces.\r\n\r\nWe conducted a user study to compare the performance of different redirected walking algorithms in a small tracking space of 3.5m x 3.5m. Three algorithms were chosen based on their approaches to redirection – Reset Only, Steer to Centre and Alignment Based Redirection Control. 36 people participated in the study. \r\n\r\nIt was found users preferred Reset Only in the tracking space. Reset Only redirects users less and is easier to implement than Steer to Centre or Alignment Based Redirection Control. Additionally Reset Only had similar performance to Steer to Centre and better task performance than Alignment Based Redirection Control despite resetting users more often. Based on these findings, we provide guidelines for developers working in small tracking spaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Kildare",
              "city": "Maynooth",
              "institution": "Maynooth University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 172370
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Kildare",
              "city": "Maynooth",
              "institution": "Maynooth University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 172338
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Kildare",
              "city": "Maynooth",
              "institution": "Maynooth University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 172311
        }
      ]
    },
    {
      "id": 172481,
      "typeId": 13796,
      "title": "Exploring the Impact of Visual Scene Characteristics and Adaptation Effects on Rotation Gain Perception in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6020",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172752
      ],
      "eventIds": [],
      "abstract": "Rotation gain is a subtle manipulation technique commonly employed in Redirected Walking (RDW) methods due to its superior capability to alter a user’s virtual trajectory. Previous studies have reported that the imperceptible ranges of rotation gains are influenced by various factors, resulting in different detection threshold values, which may alter RDW performance. In this study, we focus on the effects of scene visual characteristics on the rotation gain and rotation gain thresholds (RGTs), which has been less explored in this area. In our experiments, we focus on three visual characteristics: visual density, spatial size, and realism. Each characteristic is tested at two different levels, resulting in a design of eight distinct VR scenes. Through extensive statistical analysis, we show that spatial size is a meaningful factor that influences user perception of rotation gain in different virtual environments (VEs). No significant results of sensitivity differences were found for visual density and realism. We show that the short-term temporal effect is another predominant factor influencing user perception of rotation gain, even when users experience different visual stimuli in VEs, such as different scene visual characteristic settings in our study. This result indicates that users' learning effects on rotation gain can occur in as short a time as overnight intervals, rather than over weeks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172269
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 172314
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellingtong",
              "dsl": ""
            }
          ],
          "personId": 172328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 172315
        }
      ]
    },
    {
      "id": 172482,
      "typeId": 13796,
      "title": " An Evaluation of Targeting Methods in Spatial Computing Interfaces with Visual Distractions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-5196",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172759
      ],
      "eventIds": [],
      "abstract": "With the advancements in spatial computing technologies, the question of the most suitable object targeting and selection methods in virtual environments is becoming increasingly relevant for end users. Although there are a plethora of empirical studies on this question, there is just as much variation in the methods users are confronted with in current consumer devices. For example, the recently introduced Apple Vision Pro utilizes eye gaze as the primary technique for targeting objects and interface elements. This makes it the latest addition to a line-up of mixed reality headsets that already feature diverse default interaction mechanisms, such as hand gestures (cf. Microsoft HoloLens 2), touch gestures (cf. Google Glass Enterprise Edition 2), and external controllers (cf. Magic Leap 2). A limiting factor of several previous studies on object selection (such as classical Fitts’ law studies) is given by the partly artificial setups, which have not adequately accounted for influences in practical settings.\r\nIn this paper, we present a user study comparing four (i.e., two hand-based and two gaze-based) state-of-the-art selection methods, particularly differing in their targeting approach, using the HoloLens 2. We utilized a Fitts' law-inspired task and extended the traditional study design by incorporating a visual task that simulates changes in the user interface after a successful selection. An analysis of the results revealed a significant decrease in movement times for all methods when the visual task was present. While without a visual task, gaze-based techniques were on average faster than hand-based techniques, this performance gain was eliminated (for head gaze) or even reversed (for eye gaze) when the visual task was active. These findings underscore the value of continued practice-oriented research to obtain more realistic results on the performance of targeting methods in real-world scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Viewlicity GmbH",
              "dsl": ""
            }
          ],
          "personId": 172299
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": ""
            }
          ],
          "personId": 172349
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172391
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Viewlicity GmbH",
              "dsl": ""
            }
          ],
          "personId": 172310
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 172360
        }
      ]
    },
    {
      "id": 172483,
      "typeId": 13796,
      "title": "Context Relevant Locations as an Alternative to the Place Illusion in Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-1070",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "Presence is a powerful aspect of Virtual Reality (VR). However, there has been no consensus on how to achieve presence in Augmented Reality (AR) or whether it exists at all. It can reasonably be argued that the plausibility illusion, a key component in presence, exists in AR. The place illusion, however, cannot be obtained in AR as there is no way to make the user feel as though they are transported somewhere else when they are limited to what they can physically see in front of them. However, recently it has been argued that coherence or congruence are important parts of the place and plausibility illusions. The implication for AR is that the AR content might invoke a higher plausibility illusion if it is consistent with the physical place the content is situated in. In this study, we define the concept of a Context-Relevant Location (CRL), a physical place that is congruent with the experience. We present a study that allowed users to interact with AR objects in a CRL and in a generic environment. The results indicate that presence was higher in the CRL setting than the generic environment, contribute to the debate about providing a concrete description of presence-like phenomena in AR, and posit that CRLs play a similar role to the place illusion in an AR setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172282
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172410
        }
      ]
    },
    {
      "id": 172484,
      "typeId": 13796,
      "title": "The Impact of Task-Responsibility on User Experience and Behaviour under Asymmetric Knowledge Conditions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9253",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "Virtual Reality presents a promising tool for knowledge transfer, allowing users to learn in different environments and with the help of three-dimensional visualizations. At the same time, having to learn new ways of interacting with their environment can present a significant hurdle for novice users. When users enter a virtual space to receive knowledge from a more experienced person, the question arises as to whether they benefit from learning VR-specific interaction techniques instead of letting the expert take over some or all interactions.\r\nBased on related work about expert-novice interaction in virtual spaces, this paper presents a user study comparing three different distributions of interaction responsibilities between participants and an expert user.\r\nThe Role-Based interaction mode gives the expert the full interaction responsibility. The Shared interaction mode gives both users the same interaction capabilities, allowing them to share the responsibility of interacting with the virtual space. Finally, the Parallel interaction mode gives participants full interaction responsibility, while the expert can provide guidance through oral communication and visual demonstration.\r\nOur results indicate that assuming interaction responsibility led to higher task loads but also increased the participant's engagement and feeling of presence. For most participants, sharing interaction responsibilities with the expert represented the best trade-off between engagement and challenge. While we did not measure a significant increase in learning success, participant comments indicated that they also paid more attention to details when assuming more interaction responsibility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172392
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172424
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 172318
        }
      ]
    },
    {
      "id": 172485,
      "typeId": 13796,
      "title": "Lifter for VR Headset: Enhancing Immersion, Presence, Flow, and Alleviating Mental and Physical Fatigue during Prolonged Use",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-3459",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172751
      ],
      "eventIds": [],
      "abstract": "The virtual reality (VR) headset is still relatively heavy, causing a significant physical and mental burden and negatively affecting the VR user experience, particularly during extended time of use. In this paper, we present a prototype design of the ``Lifter,'' which utilizes a counterbalanced wire-pulley mechanism to partially relieve the weight of the VR headset (between 50% to 85%).  \r\nThe human subject study has confirmed that the Lifter relieved not only physical fatigue (as expected) but also significantly improved mental burden, sense of immersion, presence, and flow (perception of time passing) during prolonged usage (30 minutes or more). ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Sungbook-Gu",
              "institution": "Korea University",
              "dsl": "Digital Experience Lab"
            }
          ],
          "personId": 172325
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 172247
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Seoul",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 172436
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University, Medical Center",
              "dsl": "Neurotology and Neuroophthalmology Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University Medical Center",
              "dsl": "Department of Neurology"
            }
          ],
          "personId": 172283
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 172422
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 172286
        }
      ]
    },
    {
      "id": 172486,
      "typeId": 13796,
      "title": "Exploring Immersive Debriefing in Virtual Reality Training: A Comparative Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-8386",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172758
      ],
      "eventIds": [],
      "abstract": "Simulation and debriefing are two essential and inseparable phases of virtual reality training. With the widespread adoption of these training tools, it is crucial to define the best pedagogical approaches for trainers and learners to maximize their effectiveness. However, despite their educational benefits, virtual reality-specific debriefing methods remain underexplored in research.This article proposes an architecture and interface for an all-in-one immersive debriefing module that is adaptable to different types of training, including a complete system for recording, replaying, and redoing actions—a study with 36 participants compared this immersive debriefing system with traditional discussion-based and video-supported debriefing. Participants were divided into three groups to evaluate the effectiveness of each method. The results showed no significant differences between these debriefing methods across several criteria, such as satisfaction, motivation, or information retention. Immersive debriefing is as usable and retentive as traditional or video debriefing in this context. The next step will be to evaluate the Redo system in other training courses involving more dynamic scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Evry",
              "institution": "Univ Evry, Université Paris-Saclay",
              "dsl": "IBISC"
            }
          ],
          "personId": 172412
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Evry",
              "institution": "Univ Evry, Université Paris-Saclay",
              "dsl": "IBISC"
            }
          ],
          "personId": 172421
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Evry",
              "institution": "Univ Evry, Université Paris-Saclay",
              "dsl": "IBISC"
            }
          ],
          "personId": 172278
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Evry",
              "institution": "Univ Evry, Université Paris-Saclay",
              "dsl": "IBISC"
            }
          ],
          "personId": 172380
        }
      ]
    },
    {
      "id": 172487,
      "typeId": 13796,
      "durationOverride": 15,
      "title": "Some Times Fly: The Effects of Engagement and Environmental Dynamics on Time Perception in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4521",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172754
      ],
      "eventIds": [],
      "abstract": "An hour spent with friends seems shorter than an hour waiting for a medical appointment. Many physiological and psychological factors, such as body temperature and emotions, have been shown to correlate with our subjective perception of time. Experiencing virtual reality (VR) has been observed to make users significantly underestimate the duration. This paper explores the effect of virtual environment characteristics on time perception, focusing on two key parameters: user engagement and environmental dynamics. We found that increased presence and interaction with the environment significantly decreased the users' estimation of the VR experience duration. Furthermore, while a dynamic environment lacks significance in shifting perception toward one specific direction, that is, underestimation or overestimation of the durations, it significantly distorts perceived temporal length. Exploiting these two factors' influence smartly constitutes a powerful tool in designing intelligent and adaptive virtual environments that can reduce stress, alleviate boredom, and improve well-being by adjusting the pace at which we experience the passage of time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": "VR/AR Lab"
            }
          ],
          "personId": 172409
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 172440
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": "VR/AR Lab"
            }
          ],
          "personId": 172293
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": "VR/AR Lab"
            }
          ],
          "personId": 172322
        }
      ]
    },
    {
      "id": 172488,
      "typeId": 13796,
      "title": "Generative Terrain Authoring with Mid-air Hand Sketching in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6843",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172756
      ],
      "eventIds": [],
      "abstract": "Terrain generation and authoring in Virtual Reality (VR) offers unique benefits, including 360-degree views, improved spatial perception, immersive and intuitive design experience and natural input modalities. Yet even in VR it can be challenging to integrate natural input modalities, preserve artistic controls and lower the effort of landscape prototyping. To tackle these challenges, we present our VR-based terrain generation and authoring system, which utilizes hand tracking and a generative model to allow users to quickly prototype natural landscapes, such as mountains, mesas, canyons and volcanoes. Via positional hand tracking and hand gesture detection, users can use their hands to draw mid-air strokes to indicate desired shapes for the landscapes. A Conditional Generative Adversarial Network trained by using real-world terrains and their height maps then helps to generate a realistic landscape which combines features of the training data and the mid-air strokes. In addition, users can use their hands to further manipulate their mid-air strokes to edit the landscapes. In this paper, we explore this design space and present various scenarios of terrain generation. Additionally, we evaluate our system across a diverse user base that vary in VR experience and professional background. The study results indicate that our system is feasible, user-friendly and capable of fast prototyping.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "New York University",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 172398
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 172347
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "CREATE Lab"
            }
          ],
          "personId": 172285
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "CREATE Lab"
            }
          ],
          "personId": 172367
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Courant, NYU",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 172260
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Future Reality Lab"
            }
          ],
          "personId": 172371
        }
      ]
    },
    {
      "id": 172489,
      "typeId": 13796,
      "title": "Choose Your Reference Frame Right: An Immersive Authoring Technique for Creating Reactive Behavior",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-6743",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172753
      ],
      "eventIds": [],
      "abstract": "Immersive authoring enables content creation for virtual environments without a break of immersion. To enable immersive authoring of reactive behavior for a broad audience, we present modulation mapping, a simplified visual programming technique. To evaluate the applicability of our technique, we investigate the role of reference frames in which the programming elements are positioned, as this can affect the user experience. Thus, we developed two interface layouts: \"surround-referenced\" and \"object-referenced\". The former positions the programming elements relative to the physical tracking space, and the latter relative to the virtual scene objects. We compared the layouts in an empirical user study (𝑛 = 34) and found the surround-referenced layout faster, lower in task load, less cluttered, easier to learn and use, and preferred by users. Qualitative feedback, however, revealed the object-referenced layout as more intuitive, engaging, and valuable for visual debugging. Based on the results, we propose initial design implications for immersive authoring of reactive behavior by visual programming. Overall, modulation mapping was found to be an effective means for creating reactive behavior by the participants. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172344
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172287
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172255
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172307
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 172254
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 172318
        }
      ]
    },
    {
      "id": 172490,
      "typeId": 13796,
      "durationOverride": 15,
      "title": "Out-Of-Virtual-Body Experiences: Virtual Disembodiment Effects on Time Perception in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-4322",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172754
      ],
      "eventIds": [],
      "abstract": "This paper presents a novel experiment investigating the relationship between virtual disembodiment and time perception in Virtual Reality (VR). Recent work demonstrated that the absence of a virtual body in a VR application changes their perception of time. However, the effects of simulating an out-of-body experience (OBE) in VR on time perception are still unclear. We designed an experiment with two types of virtual disembodiment techniques based on viewpoint gradual transition: a virtual body’s behind view and facing view transitions. We investigated their effects on forty-four participants in an interactive scenario where a lamp was repeatedly activated and time intervals were estimated. Our results show that, while both techniques elicited a significant virtual disembodiment perception, time duration estimations in the minute range were only shorter in the facing view compared to the eye view condition. We believe that reducing agency in the facing view is a key factor in the time perception alteration. This provides a novel approach to manipulating time perception in VR, with potential applications for mental health treatments such as schizophrenia or depression and for improving our understanding of the relation between body, virtual body, and time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "HCI Group",
              "dsl": " University of Würzburg, Department of Computer Science"
            }
          ],
          "personId": 172320
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 172309
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172362
        }
      ]
    },
    {
      "id": 172491,
      "typeId": 13796,
      "durationOverride": 15,
      "title": "Enhancing VR Sketching with a Dynamic Shape Display",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-9798",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172754
      ],
      "eventIds": [],
      "abstract": "Sketching on virtual objects in Virtual Reality (VR) can be challenging due to the lack of a physical surface that constrains the movement and provides haptic feedback for contact and movement.\r\nWhile using a flat physical drawing surface has been proposed, it creates a significant discrepancy between the physical and virtual surfaces when sketching on non-planar virtual objects. \r\nWe propose using a dynamic shape display that physically mimics the shape of a virtual surface, allowing users to sketch on a virtual surface as if they are sketching on a physical object's surface. We demonstrate this using VRScroll, a shape-changing device that features seven independently-controlled flaps to automatically imitate the shape of a virtual surface. \r\nOur sketching study showed that participants exhibited higher precision when tracing simple shapes with the dynamic shape display and produced clearer sketches.  We also provided several design implications for dynamic shape displays aimed at enabling precise sketching in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172333
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172312
        }
      ]
    },
    {
      "id": 172492,
      "typeId": 13796,
      "title": "TeenWorlds: Supporting Emotional Expression for Teenagers with their Parents and Peers through a Collaborative VR Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24b-2626",
      "source": "PCS",
      "trackId": 13067,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172757
      ],
      "eventIds": [],
      "abstract": "Adolescence is a period of growth and exploration, marked by influential relationships with peers and parents. These relationships are essential for teenagers' well-being, highlighting the need to support their interpersonal interactions. Emotional expression is key in resolving conflicts that can frequently arise. This paper investigates the potential of TeenWorlds, a Virtual Reality (VR) application, to facilitate emotional expression and shared understanding among teenagers and their peers and parents.\r\nIn our study, teenagers, accompanied by either a peer or a parent (total n=42), used TeenWorlds to visually represent their emotions during a shared conflict, discuss them, and collaborate on a joint VR drawing. Our findings indicate that TeenWorlds can foster communication, reflection, and strengthen interpersonal relationships. However, notable differences were observed in interactions with peers versus parents.\r\nWe contribute insights into designing VR systems that support reflective experiences and meaningful family interactions, ultimately enhancing the well-being of adolescents, parents, and families.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": ""
            }
          ],
          "personId": 172394
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "HCI"
            }
          ],
          "personId": 172390
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 172321
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": ""
            }
          ],
          "personId": 172423
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "UCL Interaction Centre"
            }
          ],
          "personId": 172302
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": ""
            }
          ],
          "personId": 172377
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": ""
            }
          ],
          "personId": 172365
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL ",
              "dsl": "UCLIC"
            }
          ],
          "personId": 172438
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": ""
            }
          ],
          "personId": 172361
        }
      ]
    },
    {
      "id": 172694,
      "typeId": 13797,
      "title": "UXR-kit: An Ideation Kit and Method for Collaborative and User-Centered Design about Extended Reality systems.",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1113",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Emerging kits and methods about Extended Reality (XR) systems are mainly centered on the prototyping phase. The ideation phase, which comes before prototyping, is currently still under-explored. In this work, we propose UXR-kit: a toolkit and a method for the co-design of ideas for XR systems. UXR-kit is based on an approach inspired by \\textit{design studios} and \\textit{generative techniques} and highlights the specificities of XR systems. Results from an experimental study suggest that UXR-kit allows the emergence of ideas for XR designs through both World-In-Miniature representations and first-person representations at scale 1:1.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Immersion",
              "dsl": ""
            }
          ],
          "personId": 172502
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Immersion",
              "dsl": ""
            }
          ],
          "personId": 172654
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Immersion",
              "dsl": ""
            }
          ],
          "personId": 172556
        }
      ]
    },
    {
      "id": 172695,
      "typeId": 13797,
      "title": "Supporting Wildfire Evacuation Preparedness through a Virtual Reality Simulation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1034",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This demo presents a virtual reality simulation of a wildfire evacuation. Players are tasked with going through a home environment and collecting items they believe they would need and want to take if they were under an evacuation notice. The experience is playable on the Meta Quest 2 headset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "University of California, Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 172552
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "Universityof California Santa Cruz",
              "dsl": "Computational Media"
            }
          ],
          "personId": 172511
        }
      ]
    },
    {
      "id": 172696,
      "typeId": 13797,
      "title": "Generative Multi-Modal Artificial Intelligence for Dynamic Real-Time Context-Aware Content Creation in Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1110",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "We introduce a framework that uses generative Artificial Intel-\r\nligence (AI) for dynamic and context-aware content creation in\r\nAugmented Reality (AR). By integrating Vision Language Mod-\r\nels (VLMs), our system detects and understands the physical space\r\naround the user, recommending contextually relevant objects. These\r\nobjects are transformed into 3D models using a text-to-3D genera-\r\ntive AI techniques, allowing for real-time content inclusion within\r\nthe AR space. This approach enhances user experience by enabling\r\nintuitive customization through spoken commands, while reducing\r\ncosts and improving accessibility to advanced AR interactions. The\r\nframework’s vision and language capabilities support the genera-\r\ntion of comprehensive and context-specific 3D objects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": ""
            }
          ],
          "personId": 172638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172633
        }
      ]
    },
    {
      "id": 172697,
      "typeId": 13797,
      "title": "Rendering diffraction Phenomena on rough surfaces in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1032",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Wave-optical phenomena, such as diffraction, significantly impact the visual appearance of surfaces. Despite their importance, wave-optical reflection models are rare and computationally expensive. Recently, we presented a real-time model that accounts for diffraction-induced color shifts and speckle. Given that diffraction phenomena are highly dependent on illumination and viewing directions, as well as stereoscopic vision, we developed a VR demo to evaluate the new model. This demo shows the substantial impact of diffraction on the appearance of rough surfaces, particularly in stereoscopic viewing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Köln",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 172501
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Köln",
              "institution": "TH Köln",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 172561
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172594
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172362
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "EEMCS"
            }
          ],
          "personId": 172651
        }
      ]
    },
    {
      "id": 172698,
      "typeId": 13797,
      "title": "Walking > Walking-in-Place > Flying/Steering > Teleportation? Designing Locomotion Research for Replication and Extension",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1076",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In this abstract, we discuss the demand for replication and extension efforts related to two seminal studies focused on virtual reality (VR) locomotion interfaces, initially centered around a VR implementation of the Visual Cliff, often referred to as Virtual Pit. The original\r\nexperiments by Slater et al. (1995) and Usoh et al. (1999) compared different locomotion methods, including Real Walking, Walking-in-Place, and Flying/Steering, with a focus on presence and ease of use. We discuss the importance of these studies for the field, motivate replication efforts focused on these studies, discuss potential confounding factors, and present considerations for a concerted effort to reproduce the findings with state-of-the-art VR systems and measures, extensions to locomotion methods like Teleportation, and means to support future replications and extensions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172424
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172507
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "SREAL"
            }
          ],
          "personId": 172646
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "La Trobe University",
              "dsl": "Department of Computer Science & IT"
            }
          ],
          "personId": 172525
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "UNC Chapel Hill",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172602
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172410
        }
      ]
    },
    {
      "id": 172699,
      "typeId": 13797,
      "title": "From Ground to Sky: Flying-motion Generation via Motion Dataset Adaptation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1075",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "We conducted a study utilizing a lightweight generative network to create flying motions. The existing datasets used for training did not include any data on flying motions. Therefore, we selected certain classes from the existing motion datasets and transformed these motions to resemble flying actions. By training the existing generative network with the modified dataset, we were able to generate motions that closely resemble flying. The results of this study demonstrate the potential for generating flying motions. The generation of flying motions for human avatars is expected to be a critical technology not only in 3D animation or game industry but also in virtual environments, enabling users to experience various activities through their avatars. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "The Graduate school of Advanced Imaging Science, Multimedia & Film",
              "dsl": "Chung-Ang Univ."
            }
          ],
          "personId": 172691
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 172668
        }
      ]
    },
    {
      "id": 172700,
      "typeId": 13797,
      "title": "Haptic and Auditory Feedback on Immersive Media\\newline in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1074",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In Virtual Reality (VR), visual and auditory sensations are effectively leveraged to create immersive experiences. However, touch is significantly underutilized in immersive media. We enhance the VR image viewing experience by integrating haptic and auditory feedback into 3D environments constructed from immersive media. We address the challenges of utilizing depth maps from various image formats to create intractable environments. The VR experience is enhanced using vibrohaptic feedback and audio cues triggered by controller collisions with haptic materials.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": ""
            }
          ],
          "personId": 172679
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Games Engineering"
            }
          ],
          "personId": 172577
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hessdorf",
              "institution": "immerVR GmbH",
              "dsl": ""
            }
          ],
          "personId": 172643
        }
      ]
    },
    {
      "id": 172701,
      "typeId": 13797,
      "title": "Off-The-Shelf: Exploring 3D Arrangements of See-Through Masks to Switch between Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1072",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This demo explores prioritization techniques to arrange see-through masks in virtual reality. The oval masks show live previews of different virtual environments (VEs) and allow for seamless teleportation into a corresponding VE by putting the mask on. Each environment includes a mini-game (e.g., basketball and archery) in which the user has to perform a small task. The arrangement of the masks changes depending on a calculated rating, which considers the time since the game was last played. We envision this system to help users to multitask in VR. For example, to control multiple characters in VR games, to experience multi-strand (nonlinear) narratives, and to supervise semi-autonomous agents in different VEs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Giessen",
              "institution": "Technische Hochschule Mittelhessen",
              "dsl": ""
            }
          ],
          "personId": 172597
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Gießen",
              "institution": "Technische Hochschule Mittelhessen",
              "dsl": ""
            }
          ],
          "personId": 172588
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Giessen",
              "institution": "Technische Hochschule Mittelhessen",
              "dsl": ""
            }
          ],
          "personId": 172584
        }
      ]
    },
    {
      "id": 172702,
      "typeId": 13797,
      "title": "Travel Speed, Spatial Awareness, And Implications for Egocentric Target-Selection-Based Teleportation - A Replication Design",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1071",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Virtual travel in Virtual Reality experiences is common, offering users the ability to explore expansive virtual spaces. Various interfaces exist for virtual travel, with speed playing a crucial role in user experience and spatial awareness. Teleportation-based interfaces provide instantaneous transitions, while continuous and semi-continuous methods vary in speed and control. Prior research by Bowman et al. highlighted the impact of travel speed on spatial awareness, with instantaneous travel can lead to user disorientation. However, additional cues, such as visual target selection, can aid in reorientation. This study replicates and extends Bowman's experiment, investigating the influence of travel speed and visual target cues on spatial orientation. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172424
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Visual Computing Institute"
            }
          ],
          "personId": 172304
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human-Computer Interaction"
            }
          ],
          "personId": 172687
        }
      ]
    },
    {
      "id": 172703,
      "typeId": 13797,
      "title": "3D Human Pose Estimation Using Egocentric Depth Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1070",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In this paper, we present a novel approach for 3D human pose estimation using depth data in egocentric viewpoints. Depth data has the advantage that it is less sensitive to color and lighting changes. We acquired depth data streamed from multiple depth cameras attached to a user's head and calibrated them into a depth map. For joint detection, a ResNet neural network was optimized with skeletal joints of the Kinect camera. Unlike previous approaches, our approach can track 3D human poses in an egocentric setup without requiring a large dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 172622
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 172620
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Sejong",
              "institution": "Hongik Univeristy",
              "dsl": "School of Games"
            }
          ],
          "personId": 172644
        }
      ]
    },
    {
      "id": 172704,
      "typeId": 13797,
      "title": "Single Distance Clipping in Vertex Shader for VR Portal",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1119",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "VR portal is a powerful tool connecting multiple VR spaces and providing impressive visual effects. The implementation of VR portals requires the VR compositor to perform a clipping operation. For performance reason, the clipping needs to be performed in vertex shader, which prevents most of existing algorithm to be used here. This paper proposes a novel algorithm that performs the clipping operation with a single distance value. The proposed algorithm is well suited for the vertex shader, and can handle both normal clipping and inverse clipping. This enables users to view a VR portal from both sides. The proposed algorithm also saves the number of clip planes for other purposes, and its performance is superior to the traditional algorithm with multiple clip planes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 172657
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 172568
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Menlo Park",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 172590
        }
      ]
    },
    {
      "id": 172705,
      "typeId": 13797,
      "title": "Single Vs Dual: Influence of the Number of Displays on User Experience within Virtually Embodied Conversational Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1117",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "The current research evaluates user experience and preference when engaging with a Patient-Reported Outcome Measures (PROMs) healthcare application displayed on a single tablet in comparison to interaction with the same application distributed across two tablets. We conducted a within-subject user study with 43 participants who interacted with and rated the usability of our system and took part in a post-experiment interview for collecting subjective data. Our findings showed significantly higher usability and higher pragmatic quality ratings for the single tablet condition. While most participants attribute a lower social presence to the avatar and prefer it to be placed on the same tablet, some users attribute a higher level of presence to the avatar and prefer it to be placed on a second tablet. The findings of this study, provide valuable insights for designing avatar-assisted healthcare systems concerning the user experience and placement of the virtual assistants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "North Rhine-Westphalia",
              "city": "Lippstadt",
              "institution": "University of Applied Sciences Hamm-Lippstadt",
              "dsl": "Immersive Reality Lab"
            },
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technical University of Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172614
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamm",
              "institution": "University of Applied Sciences Hamm-Lippstadt",
              "dsl": "Immersive Reality Lab"
            }
          ],
          "personId": 172684
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Lippstadt",
              "institution": "University of Applied Sciences Hamm-Lippstadt",
              "dsl": "Immersive Reality Lab"
            }
          ],
          "personId": 172541
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172520
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technical University of Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172682
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Nordrhein-Westfalen",
              "city": "Hamm",
              "institution": "Hamm-Lippstadt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172551
        }
      ]
    },
    {
      "id": 172706,
      "typeId": 13797,
      "title": "Exploring Alternative Text Input Modalities in Virtual Reality: A Comparative Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1039",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Text input in Virtual Reality (VR) is crucial for various applications, including communication, search, and productivity. We compare different keyboard designs for text entry in VR, taking advantage of the flexibility and the tracking options that are available for a 3D environment. To assess the differences between the input modalities and the spatial keyboard layouts void of the user experience with specific keyboard layouts, the Dvorak keyboard layout was used. Four different settings were included in the comparison: (a) a floating keyboard with finger pointing input, (b) a keyboard attached on the back of the hand with finger pointing input, (c) a floating keyboard with eye tracking and finger pinch input, and (d) a keyboard laid out over a rolling shape with finger pointing input. Keyboards (b), (c), and (d) can move in 3D space, while keyboard design (a) is fixed. (a) and (d) showed similar typing efficiencies, however users reported an increase in perceived usability and lower physical demand for keyboard design (d). Users also reported a higher physical demand, effort, and annoyance for keyboard design (b), and a lower physical demand for keyboard design (c), with higher mental demand, effort and the highest error rate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172382
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172447
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172289
        }
      ]
    },
    {
      "id": 172707,
      "typeId": 13797,
      "title": "Cultural Windows: Towards Immersive Journeys into Global Living Spaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1116",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "“Cultural Windows” is a research initiative aimed at enhancing cross-cultural understanding through immersive extended reality (XR) experiences. The project deploys AR and VR platforms to allow users to explore diverse living spaces, bridging the gap between preconceived notions and the actual appearance of these spaces. By using 3D scanning to create accurate models of culturally significant objects and integrating them into immersive systems, the project provides insights into the use of immersive technologies in cultural education, promoting engagement with global living designs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Université de Bordeaux",
              "dsl": "Inria"
            }
          ],
          "personId": 172631
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Inria, Université de Bordeaux",
              "dsl": ""
            }
          ],
          "personId": 172534
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Bordeaux",
              "institution": "Univ. Bordeaux, CNRS, Inria, LaBRI",
              "dsl": ""
            }
          ],
          "personId": 172663
        }
      ]
    },
    {
      "id": 172708,
      "typeId": 13797,
      "title": "A Study on the Effectiveness of Augmented Reality Signal-Integrated Camera Monitor Systems for Safe Lane Changing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1047",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This study investigates the effectiveness of augmented reality (AR) signal in camera monitor systems (CMS) for enhancing safety during lane changes. Seventy participants used seven side mirror conditions, including traditional side mirrors and six CMS configurations with and without AR signal. Results showed that CMS with AR signal significantly reduced the number of collisions and reaction time compared to side mirror conditions without AR signal.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Kyung Hee University",
              "dsl": "Dept. of Information Display/HXRLAB"
            }
          ],
          "personId": 172522
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Information Display",
              "dsl": "Kyung Hee University"
            }
          ],
          "personId": 172618
        }
      ]
    },
    {
      "id": 172709,
      "typeId": 13797,
      "title": "A Volumetric Video Application to Enhance Museum Experiences",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1123",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Volumetric video (VV) is an emerging 3D format that allows the integration of real people into XR (extended reality) applications. Recent cost-effective AI-based methods have enabled VV capture using single handheld cameras or mobile phones. This study addresses the quality, integration, and acceptance of AI-based VV content creation in an augmented reality (AR) application designed to enhance museum experiences. The main result reveals that, although the current VV quality is lower than professional standards, users still find significant added value and enjoy its immersive experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": ""
            }
          ],
          "personId": 172569
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": ""
            }
          ],
          "personId": 172670
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": ""
            }
          ],
          "personId": 172546
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": ""
            }
          ],
          "personId": 172574
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Studio Freisicht",
              "dsl": ""
            }
          ],
          "personId": 172671
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Studio Freisicht",
              "dsl": ""
            }
          ],
          "personId": 172589
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Aarau",
              "institution": "Aarauer Kunsthaus",
              "dsl": ""
            }
          ],
          "personId": 172526
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 172685
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172570
        }
      ]
    },
    {
      "id": 172710,
      "typeId": 13797,
      "title": "A Study of Haptic Interaction Techniques Utilizing Body Possession by Virtual Characters",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1045",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In order to realize touch communication with virtual characters, we propose a new tactile presentation method which uses the user's own body part for a sense of touch along with a sense that the body part is possessed by virtual character. In this paper, as an initial study, we created a system in which the user and a virtual character's hands are visually fused, and a pseudo-high-five is performed. The applicability of the proposed method and points for improvement are discussed based on the results of user experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT",
              "dsl": "Human Informatics Laboratories"
            }
          ],
          "personId": 172567
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT",
              "dsl": "Human Informatics Laboratories"
            }
          ],
          "personId": 172619
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT",
              "dsl": "Human Informatics Laboratories"
            }
          ],
          "personId": 172580
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT",
              "dsl": "Human Informatics Laboratories"
            }
          ],
          "personId": 172669
        }
      ]
    },
    {
      "id": 172711,
      "typeId": 13797,
      "title": "Study of inpainting based on generative AI for noise-canceling HMDs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1089",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": " Entering a small space such as an elevator or a crowded train with a stranger can cause discomfort and suffocation. This is because the stranger is invading the individual's personal space. However, it is difficult to maintain an appropriate interpersonal distance from others at all times in various situations. Therefore, a noise-canceling HMD that uses AR to change the size of the person in the field of vision has been proposed as a means of reducing noise such as discomfort caused by inappropriate interpersonal distance. In this paper, we propose an improvement method using generative AI for background completion in noise-canceling HMDs. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Shiga",
              "city": "Otsu",
              "institution": "Ryukoku University",
              "dsl": " Faculty of Advanced Science and Technology"
            }
          ],
          "personId": 172680
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Otsu City",
              "institution": "Faculty of Advanced Science and Technology",
              "dsl": "Ryukoku University"
            }
          ],
          "personId": 172535
        }
      ]
    },
    {
      "id": 172712,
      "typeId": 13797,
      "title": "VReflect: Designing VR-Based Movement Training with Perspectives, Mirrors and Avatars",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1122",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Physical training in virtual environments, such as VR, has gained popularity, especially due to the coronavirus pandemic. VR training offers new opportunities compared to traditional methods, including the use of different perspectives, mirrors, and avatars to enhance the understanding of personal movements. However, the interaction of these elements has been less studied. To address this, we developed VReflect, a VR environment that uses mirrors and avatars as virtual self-visualization techniques (VSVT) to improve self-awareness during movement training. In a preliminary study on learning beginner Karate movements, we tested four combinations of perspectives and VSVTs. The results indicate that neither first-person nor third-person perspectives can be universally recommended, which is in alignment with previous work. Interviews revealed a preference for the traditional combination of mirrors and first-person perspective.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 172676
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 172530
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 172496
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 172505
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Eichstätt",
              "institution": "Catholic University of Eichstätt-Ingolstadt",
              "dsl": ""
            }
          ],
          "personId": 172533
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 172689
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo/Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 172493
        }
      ]
    },
    {
      "id": 172713,
      "typeId": 13797,
      "title": "Pipelining Processors for Decomposing Character Animation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1088",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This paper presents an openly available modular pipeline architecture for character animation. It effectively decomposes frequently necessary processing steps, such as copying data from various motion sources, applying inverse kinematics, or scaling the character into dedicated character processors. Processors can easily be parameterized, extended (e.g., with Artificial Intelligence (AI)), and freely arranged or even duplicated in any order necessary, greatly reducing side effects and fostering fine-tuning, maintenance, and reusability of the complex interplay of real-time animation steps.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Psychology of Intelligent Interactive Systems"
            }
          ],
          "personId": 172498
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Psychology of Intelligent Interactive Systems"
            }
          ],
          "personId": 172579
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 172557
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 172309
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Psychology of Intelligent Interactive Systems"
            }
          ],
          "personId": 172354
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172362
        }
      ]
    },
    {
      "id": 172714,
      "typeId": 13797,
      "title": "Enhanced Wayfinding Insights Through VR and Eye-Tracking Analysis",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1086",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This paper presents a novel method for evaluating wayfinding within a public building to provide meaningful insights for stakeholders. Our approach features unique methods for both data collection and evaluation, with a holistic digital capture of the entire virtual environment experienced by participants, maintained in an interactive format for in-depth analysis. We also captured and output data in point cloud formats, raw data text files, and task-specific metrics, which support interactive replays of participants' experiences. We developed algorithms to extract meaningful insights from the raw data based on assumptions about wayfinding characteristics. The contribution is a flexible framework that can be easily adapted for future projects with adjustable variables to suit specific applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Geelong",
              "institution": "Deakin University ",
              "dsl": "Arts and Education/School of Communication and Creative Arts/Deakin Motion Lab"
            }
          ],
          "personId": 172542
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Geelong",
              "institution": "Deakin University",
              "dsl": ""
            }
          ],
          "personId": 172613
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Geelong",
              "institution": "Deakin University",
              "dsl": "Engineering and Built Environment/School of Architecture and Built Environment"
            }
          ],
          "personId": 172517
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Burwood",
              "institution": "Deakin University",
              "dsl": "Arts and Education/School of Communication and Creative Arts"
            }
          ],
          "personId": 172596
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Geelong",
              "institution": "Deakin University",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 172573
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Geelong",
              "institution": "Deakin University",
              "dsl": "Deakin Motion Lab / School of Communication and Creative Arts"
            }
          ],
          "personId": 172529
        }
      ]
    },
    {
      "id": 172715,
      "typeId": 13797,
      "title": "Walking of uphill slopes in immersive virtual environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1085",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "We explore three visual manipulation techniques aiming to create a realistic feeling of walking an uphill slope while in reality being on flat ground. The techniques are based on real physical visual perception and consist of modification of height and display of virtual shoes, modification of speed, and modification of view pitch. Quantitative and qualitative evaluation indicated that modification of speed, and pitch contributed to user discomfort, as well as a general increase in discomfort correlating with the slope's increasing inclination. However, height manipulation was well received and can be used in future projects for more realistic landscape.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": ""
            }
          ],
          "personId": 172649
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Rotkreuz",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "Lucerne School of Information Technology"
            }
          ],
          "personId": 172497
        }
      ]
    },
    {
      "id": 172716,
      "typeId": 13797,
      "title": "EcoDive: Enhancing Presence and Ambient Environmental Awareness in a Virtual Reality Experience for Underwater Marine Debris Collection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1080",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This paper presents a VR-based serious game.  The game aims to raise awareness about ocean pollution by immersing players in a virtual underwater world where they collect trash to prevent coral bleaching and save marine life. Despite their efforts, players inevitably face game over, highlighting the futility of merely collecting trash and underscoring the need to prevent waste from entering oceans. The game uses various diegetic feedback mechanisms and enhanced user presence features to deepen emotional engagement and promote pro-environmental behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 172593
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 172506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 172504
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172424
        }
      ]
    },
    {
      "id": 172717,
      "typeId": 13797,
      "title": " Make America Great Again and Again: How to Adapt Interactive Installation Art for Virtual Reality ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1129",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Make America Great Again and Again is an interactive art project that uses visitors' YouTube selfie videos to explore diverse interpretations of \"Make America Great Again.\" The project features a large, fluttering American flag accompanied by the Star-Spangled Banner, with sixty small screens displaying one-minute video clips in sequence. This one-hour loop continues until participants upload their own videos titled MAGAA_Project01 to MAGAA_Project60, transforming the flag into a collage of visitor selfies. By providing a public sphere for local visitors, the project encourages them to share their opinions on this controversial issue. To capture global perspectives on the topic, the project was adapted into a virtual reality environment using the metaverse platform Styly. This paper outlines the process of converting the installation into virtual reality artwork.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Columbia",
              "institution": "University of South Carolina",
              "dsl": "Media Arts/School of Visual Art & Design"
            }
          ],
          "personId": 172660
        }
      ]
    },
    {
      "id": 172718,
      "typeId": 13797,
      "title": "Us Xtended - Tracking and Sensing through Embedded and Embodied Design in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1128",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This demonstration presents an embodied and embedded design method via biometric data tracking on the example of the virtual reality prototype Us Xtended. Users are taken through different immersive worlds and their task is to manipulate the environments via a certain type of physiological interaction (i.e. heart rate, gaze, voice, cognitive load). By employing biofeedback, the system tailors the immersive environment via audiovisual and haptic stimuli to user's psycho-physiological responses and reflects them on its scale which is part of the virtual environment. By recording their voice, users can self-assess their own affects. In the finale, users stand in a pastiche-like world filled with different artifacts of psycho-physiological evaluations they co-created with the biofeedback system throughout their journey. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "York University",
              "dsl": "Cinema and Media Studies"
            }
          ],
          "personId": 172621
        }
      ]
    },
    {
      "id": 172719,
      "typeId": 13797,
      "title": "Effectiveness of Adaptive Difficulty Settings on Self-efficacy in VR Exercise",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1127",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "The difficulty is a fundamental factor of the user's motivation and engagement in some tasks. Dynamic difficulty adjustment (DDA) systems provide users with an optimal level of challenge. Previously, some studies developed a DDA system that can set the task's difficulty to any level. However, these studies lack the investigation of the influence of the difficulty levels on the psychological aspect. For this purpose, we consider a difficulty setting that consists of stepwise difficulty levels (e.g., hard, normal, and easy) set to adapt to each user's skill and evaluate it using self-efficacy. In the experiment, we employ a Kendama task in a VR space where the difficulty level can be easily adjusted. The result shows that the difficulty levels in our method can be set according to the user's skill. Moreover, we experimentally clarify a correlation between successful experiences and self-efficacy, which means that adapting difficulty levels to the user's skill has the potential to improve their motivation and engagement effectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tamagawa University",
              "dsl": "Brain Science Institute"
            }
          ],
          "personId": 172641
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tamagawa University",
              "dsl": "Brain Science Institute"
            }
          ],
          "personId": 172592
        }
      ]
    },
    {
      "id": 172720,
      "typeId": 13797,
      "title": "Wheel-Based Attachable Footwear for VR: Challenges and Opportunities in Seated Walking-in-Place Locomotion",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1137",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This poster explores the potential of Cybershoes, a foot-based consumer input device, used with a swivel chair to enable seated walking-in-place (WIP) locomotion in virtual reality (VR). Through a qualitative study with 12 participants, we investigated the effects of Cybershoes on user comfort, presence, motion sickness, and overall experience during various sightseeing tasks. Our findings reveal both opportunities and challenges for Cybershoes as a seated-WIP solution. Participants perceived Cybershoes as more natural for navigation compared to handheld controllers, with most reporting reduced motion sickness. However, challenges included perceived slower movement speed, ergonomic issues, and limited action detection. Our work also highlights Cybershoes' potential beyond gaming, including applications in exercise, professional training, remote work, and accessibility.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "State College",
              "institution": "Penn State University",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 172647
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 172635
        }
      ]
    },
    {
      "id": 172721,
      "typeId": 13797,
      "title": "ChronoShore: Diagetic Temporal Exploration in a Simulated Virtual Coast Environment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1059",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This paper introduces ChronoShore, an immersive virtual reality (VR) experience designed to explore diegetic time manipulation mechanics within a semi-realistic coastal environment. \r\nTraditional 2D video scrubbing methods fall short in immersive settings, particularly for understanding time-bound processes such as simulations of geology or biology. \r\nChronoShore addresses this by allowing users to interact with celestial bodies to dynamically control and experience the passage of time, currently showcasing different weather events and atmospheric phenomena.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Costa Rica",
              "state": "",
              "city": "Cartago",
              "institution": "Costa Rica Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 172609
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172587
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172424
        }
      ]
    },
    {
      "id": 172722,
      "typeId": 13797,
      "title": "Exploring an XR Indoor Navigation System for Remote Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1136",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "While collaboration in shared extended reality spaces has been extensively explored, larger environments like entire floors or buildings have garnered less attention. To address this gap, spatial navigation and collaboration across realities must be possible to find each other and foster shared spatial understanding independent from reality. Current developments target either navigation or collaboration but lack the combination. In this poster, we present an extended reality remote collaboration system using an augmented reality (AR) based indoor navigation for on-site and a Building Information Model (BIM) of the physical environment Virtual Reality (VR) system for remote users. We conducted a user study with ten participants (five pairs) to gather initial insights into the system's usability and preferences for collaborative tools. The results offer initial insights into creating shared spatial understanding across realities. Our work contributes to a collaborative XR navigation system for extensive shared spaces. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172582
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "UC Santa Cruz",
              "dsl": ""
            }
          ],
          "personId": 172571
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Sankt Augustin",
              "institution": "Fraunhofer FIT",
              "dsl": ""
            }
          ],
          "personId": 172538
        }
      ]
    },
    {
      "id": 172723,
      "typeId": 13797,
      "title": "Editing of Immersive Recordings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1012",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Immersive recordings capture virtual reality interactions and are\r\nused in various contexts such as education and entertainment. However,\r\nthere has been only limited research on requirements and techniques\r\nfor editing such recordings.We interviewed expert editors of\r\nvideo recordings to understand their workflows, familiarised them\r\nwith immersive recordings, and asked them about what editing\r\nchallenges and capabilities they can envision for immersive recordings.\r\nThe experts identified several functionalities they considered\r\nrelevant for editing, including viewer placement, control over the\r\nviewer’s size, support for live and asynchronous collaboration, and\r\ndifferent transition types.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "VR and Visualisation Research Group"
            }
          ],
          "personId": 172637
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "VR and Visualisation Research Group"
            }
          ],
          "personId": 172512
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "VR and Visualisation Research Group"
            }
          ],
          "personId": 172640
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172547
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172659
        }
      ]
    },
    {
      "id": 172724,
      "typeId": 13797,
      "title": "Digital Eyes: Social Implications of XR EyeSight",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1055",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "The EyeSight feature, introduced with the new Apple Vision Pro XR headset, promises to revolutionize user interaction by simulating real human eye expressions on a digital display. This feature could enhance XR devices’ social acceptability and social presence when communicating with others outside the XR experience. In this pilot study, we explore the implications of the EyeSight feature by examining social acceptability, social presence, emotional responses, and technology acceptance. Eight participants engaged in conversational tasks in three conditions to contrast experiencing the Apple Vision Pro with EyeSight, the Meta Quest 3 as a reference XR headset, and a face-to-face setting. Our findings indicate that while the EyeSight feature improves perceptions of social presence and acceptability compared to the reference headsets, it does not match the social connectivity of direct human interactions",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172692
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172545
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172544
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172575
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172667
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Nordrhein-Westfalen",
              "city": "Hamm",
              "institution": "Hamm-Lippstadt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172551
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Quality and Usability Lab"
            }
          ],
          "personId": 172518
        }
      ]
    },
    {
      "id": 172725,
      "typeId": 13797,
      "title": "Dynamic Difficulty Adjustment in Virtual Reality Exergaming to Regulate Exertion Levels via Heart Rate Monitoring",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1011",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "By regulating exertion levels, Dynamic difficulty adjustment (DDA) has the potential to enhance user experience and optimize exercise in Virtual Reality (VR) exergames. This pilot study assesses the effectiveness of adjusting the difficulty of gameplay challenges based on heart rate (HR) data to control the intensity of physical activity in VR exergaming. Results from 13 participants indicate that the HR-based DDA more effectively maintained target heart rate zones compared to randomized difficulty adjustments. Improved perceived exertion, and increased enjoyment shows the potential of this approach for VR-based exercise and rehabilitation programs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172499
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172407
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172562
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Rhineland-Palatinate",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172653
        }
      ]
    },
    {
      "id": 172726,
      "typeId": 13797,
      "title": "Investigation of Simulator Sickness in Walking with Multiple Locomotion Technologies in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1131",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "With the increasing development of Virtual Reality, locomotion has become an essential component of interaction in VR. Currently, various locomotion technologies have been developed to provide users with a natural walking experience in virtual environments. However, the multiple walking techniques impact users' walking experience in different ways. Simulator sickness is a common issue in VR experiences. Since different walking methods may influence simulator sickness differently, we conducted a user study to evaluate simulator sickness in walking with three relevant walking methods: real walking, arm-swing, and omnidirectional treadmill, and the results indicated that these three walking methods caused different levels of simulator sickness, and people perceived stronger sickness when they walked on the omnidirectional treadmill.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 172519
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 172683
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 172306
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Hagenberg",
              "institution": "University of Applied Sciences Upper Austria",
              "dsl": ""
            }
          ],
          "personId": 172612
        }
      ]
    },
    {
      "id": 172727,
      "typeId": 13797,
      "title": "Object-Specific and Generic Difference Detection for Non-Destructive Testing Methods",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1098",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In this arcticle, we present a concept of a tool suite that combines a generic and object-specific difference detection between 3D measurment and 3D planning data. In a Generic Difference Detection (GDD) between measurement and planning data, the objects of interest to be compared are represented by the acquired 3D data (often the solid of a 3D reconstruction) and the 3D planning data in their entirety. An example of this is a shape analysis used as a non-destructive testing method for products from an additive (e.g. 3D printing) or subtractive (e.g. CNC milling) manufacturing process, in which a 3D reconstruction of the produced object is compared with its orignial 3D planning data. The range of applications in this area is very broad and includes quality controls, plagiarism checks and measuring wear and tear.On the other hand, in an Object-Specific Difference Detection (OSDD) the objects of interests are located within the measurement and planning data and are represented by specific components that have to be compared. Here, the sector of digital construction monitoring can be mentioned as an example, in which the construction process and associated specific construction components, such as walls, passages and window openings, are checked against the planning data based on acquired measurement data. Both difference detection techniques work, regardless of whether the object of interest is present in both the measurement and planning data or only in the measurement or planning data (e.g. after a building refurbishment). Besides the benefit of quality assurance following a production or construction process that is covered by both difference detection concepts, an early detection of errors is of great advantage. For example, follow-up costs after a construction process (e.g. building) can be reduced or avoided by identifying errors during the construction process. In addition, this can also help to ensure that the schedule is adhered to. Depending on the application, there is also the option of feeding identified deviations or errors back into the planning data in order to achieve synchronization between the planning data and the actual state of the object, in case the differences are intentional. While our approach for GDD is limited to a shape similarity analysis of two 3D objects and a subsequent real-time visualization of detected differences, our method for OSDD can already feed detected deviations and errors back into the 3D planning data and allows a collaborative result visualization based on a multi-codal presentation (e.g. tabular data or 3D rendering). ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Darmstadt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172626
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Darmstadt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172625
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Darmstadt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 172627
        }
      ]
    },
    {
      "id": 172728,
      "typeId": 13797,
      "title": "Exploring Influencers' and Users' Experiences in Douyin's Virtual Reality Live-Streaming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1053",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "VR live-streaming has become a significant content presentation mode on the Douyin. This study aims to explore the technical modes, content strategies, user experiences, and feedback effects of influencers in VR live-streaming. We employed interviews and focus groups, designing a three-phase study. The results indicate that VR technology is recognized by social media influencers and has become an essential part of their creative practice. For some influencers, VR technology is a key factor in enhancing audience engagement and immersive experiences, although technical literacy barriers may arise when setting up VR scenes. Additionally, we provide dimensions for improving and developing user adoption and experience of VR technology in social media environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Changsha",
              "institution": "Central South University",
              "dsl": ""
            }
          ],
          "personId": 172636
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": ""
            }
          ],
          "personId": 172572
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King’s College London",
              "dsl": ""
            }
          ],
          "personId": 172616
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Minzu University of China",
              "dsl": ""
            }
          ],
          "personId": 172564
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 172532
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Communication University of China",
              "dsl": ""
            }
          ],
          "personId": 172591
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 172559
        }
      ]
    },
    {
      "id": 172729,
      "typeId": 13797,
      "title": "Fade-to-Black Duration in Egocentric Target-Selection-Based Teleport - A Replication Design",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1097",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Fade-to-black animations are a commonly used technique to visualize transitions during teleportation. However, their duration varies across different implementations and has not been extensively researched. This abstract details a study design to understand how the level of environmental detail affects the preferred duration of fade-to-black animations. We propose a within-subject study, comparing participants preferred duration across three virtual environments with varying levels of detail. We discuss improvements to the task design of an existing study. Other than the level of environmental detail, we motivate research into the effects of different tasks (i.e. hurried or calm) on the preferred duration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172639
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172424
        }
      ]
    },
    {
      "id": 172730,
      "typeId": 13797,
      "title": "Real-Time Scent Prediction and Release for Video Games",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1130",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "This demo explores the use of computer vision technologies for the integration of scent in video games and interactive applications. \r\nWe present an extendable system that is domain-independent and allows for customization and debugging based on the targeted game.\r\nUsing Minecraft as a case study, we optimized the system configuration and evaluated its performance. \r\nOur aim is to advance the exploration of scent integration in gaming and inspire future designs for olfactory experiences.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "University of Zurich",
              "dsl": ""
            }
          ],
          "personId": 172672
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zurich",
              "dsl": "ETH Game Technology Center"
            }
          ],
          "personId": 172578
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zurich",
              "dsl": "Media Technology Center"
            }
          ],
          "personId": 172608
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zurich",
              "dsl": "ETH Game Technology Center"
            }
          ],
          "personId": 172540
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zurich",
              "dsl": "ETH Game Technology Center"
            }
          ],
          "personId": 172617
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            }
          ],
          "personId": 172586
        }
      ]
    },
    {
      "id": 172731,
      "typeId": 13797,
      "title": "Row your boat in VR and solve thinking exercises on the way: The Brain-Row Challenge",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1051",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "In this demo, we showcase Brain-Row Challenge. Brain-Row Challenge is a research prototype for dual-task training. Dual-task training combines a mental and a physical task. This training is relevant in neurodegenerative illnesses, especially in Parkinson's disease. The user is rowing with a Concept 2 ergometer over a Nordic lake and answers multiple-choice questions by rowing through gates.  Steering is done with an inertial measurement unit that is attached to the handlebar. The scenario can either be displayed with a TV screen or a VR headset. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Rheinland-Pfalz",
              "city": "Trier",
              "institution": "University of Applied Science Trier",
              "dsl": ""
            }
          ],
          "personId": 172558
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172562
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Science",
              "dsl": "Computer Science"
            }
          ],
          "personId": 172536
        }
      ]
    },
    {
      "id": 172732,
      "typeId": 13797,
      "title": "GazeLock: Gaze- and Lock Pattern-Based Authentication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1095",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Password entry is common authentication approach in Extended Reality (XR) applications for its simplicity and familiarity, but it faces challenges in public and dynamic environments due to its cumbersome nature and susceptibility to observation attacks. Manual password input can be disruptive and prone to theft through shoulder surfing or surveillance. While alternative knowledge-based approaches exist, they often require complex physical gestures and are impractical for frequent public use. We present GazeLock, an eye-tracking and lock pattern-based authentication method. This method aims to provide an easy-to-learn and efficient alternative by leveraging familiar lock patterns operated through gaze. It ensures resilience to external observation, as physical interaction is unnecessary and eyes are obscured by the headset. Its hands-free, discreet nature makes it suitable for secure public use. We demonstrate this method by simulating the unlocking of a smart lock via an XR headset, showcasing its potential applications and benefits in real-world scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            }
          ],
          "personId": 172645
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": ""
            }
          ],
          "personId": 172565
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": ""
            }
          ],
          "personId": 172509
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Oldenburg University",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 172665
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 172510
        }
      ]
    },
    {
      "id": 172733,
      "typeId": 13797,
      "title": "The MASTER XR Platform for Robotics Training in Manufacturing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1094",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "The MASTER project introduces an open Extended Reality (XR) platform designed to enhance human-robot collaboration and train workers in robotics within manufacturing settings. It includes modules for creating safe workspaces, intuitive robot programming, and user-friendly human-robot interactions (HRI), including eye-tracking technologies. The development of the platform is supported by two open calls targeting technical SMEs and educational institutes to enhance and test its functionalities. By employing the learning-by-doing methodology and integrating effective teaching principles, the MASTER platform aims to provide a comprehensive learning environment, preparing students and professionals for the complexities of flexible and collaborative manufacturing settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            }
          ],
          "personId": 172645
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Patras",
              "institution": "University of Patras",
              "dsl": "Laboratory for Manufacturing Systems and Automation, Department of Mechanical Engineering and Aeronautics"
            }
          ],
          "personId": 172513
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Patras",
              "institution": "University of Patras",
              "dsl": "Laboratory for Manufacturing Systems and Automation, Department of Mechanical Engineering and Aeronautics"
            }
          ],
          "personId": 172610
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Gipuzkoa",
              "institution": "Fundación Tekniker",
              "dsl": ""
            }
          ],
          "personId": 172675
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Gipuzkoa",
              "institution": "Fundación Tekniker",
              "dsl": ""
            }
          ],
          "personId": 172615
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Arrasate",
              "institution": "Mondragon Lingua-Alecop S.Coop",
              "dsl": ""
            }
          ],
          "personId": 172655
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Arrasate",
              "institution": "Mondragon Lingua-Alecop S.Coop",
              "dsl": ""
            }
          ],
          "personId": 172634
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Bizkaia",
              "city": "Basauri",
              "institution": "VIRTUALWARE 2007 SA",
              "dsl": ""
            }
          ],
          "personId": 172585
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Patras",
              "institution": "Teaching Factory Competence Center",
              "dsl": ""
            }
          ],
          "personId": 172539
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Patras",
              "institution": "Teaching Factory Competence Center",
              "dsl": ""
            }
          ],
          "personId": 172495
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Patras",
              "institution": "Teaching Factory Competence Center",
              "dsl": ""
            }
          ],
          "personId": 172566
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Oldenburg University",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 172665
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Interactive Machine Learning"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": "Applied Artificial Intelligence"
            }
          ],
          "personId": 172510
        }
      ]
    },
    {
      "id": 172734,
      "typeId": 13797,
      "title": "A Transfer Learning Approach for Music-driven 3D Conducting Motion Generation with Limited Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1092",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Studies on generating motions based on audio have been steadily conducted. However, previous studies were confined to speech-driven 3D gestures and music-driven 3D dance motion generation. We aim to generate 3D motions for specific scenarios such as conducting. To overcome the challenge of no existing training dataset, we constructed a multi-modal 3D conducting dataset containing 1.43 hours, which to our knowledge is a small-scale dataset. Furthermore, we designed a novel approach using transfer learning with a model pre-trained on a large-scale speech dataset for generating 3D conducting motions when trained with limited data. We compared visual results of generated motions with and without transfer learning by retargeting them to 3D human models and calculating the correlations between music beats. As a result, our proposed method shows improved performance compared to not using it.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-ang University",
              "dsl": ""
            }
          ],
          "personId": 172576
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 172691
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Chung-Ang University",
              "dsl": ""
            }
          ],
          "personId": 172668
        }
      ]
    },
    {
      "id": 172735,
      "typeId": 13797,
      "title": "Towards Effective Sensorimotor Skill Transfer: Initial Comparison between Haptic Guidance and Disturbance on VR Engraving Arts",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1091",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This paper compares force-feedback methods for sensorimotor skill transfer in a virtual environment. As the target skill in this study, we selected handling a carving knife for engraving arts in VR. Using a 3DoF force-feedback device, we implemented a VR system that provides users with an engraving art experience. For designing haptic training methods, we initially collected an artist's force-torque profile data and its corresponding position over time. As haptic guidance, the force and torque we gathered were presented as is, while the inverse direction of the profile was given as haptic disturbance.\r\nWe evaluated the user's task performance using two methods with a baseline of no haptic feedback. The results indicated that the haptic disturbance condition showed a slightly better performance than guidance, which seems to align with the previous motor learning efforts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan-si",
              "institution": "Hanyang University",
              "dsl": "Department of Applied Artificial Intelligence"
            }
          ],
          "personId": 172514
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": "Department of Applied Artificial Intelligence"
            }
          ],
          "personId": 172604
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan-si",
              "institution": "Hanyang University",
              "dsl": "Division of Computer Science"
            }
          ],
          "personId": 172508
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi-do",
              "city": "Ansan-si",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 172656
        }
      ]
    },
    {
      "id": 172736,
      "typeId": 13797,
      "title": "A Comparison between Vibrotactile Error-correction Feedback on Upper and Lower Body in the VR Snowboard Balancing Task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1090",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This study investigated the effect of vibrotactile stimulus location in the balancing task in virtual reality (VR). Using a virtual snowboarding system with wearable haptic devices, we conducted a between-subject user study comparing the effectiveness of two different body locations--upper body (UB; torso vibrations) and lower body (LB; ankle vibrations). The real-time vibrotactile balance-correction feedback was generated by the Center of Pressure (CoP) calculated from the sensor array on insoles. The initial results showed that UB feedback is better than LB to improve users’ balance ability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": "Department of Applied Artificial Intelligence"
            }
          ],
          "personId": 172604
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 172514
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 172650
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi-do",
              "city": "Ansan-si",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 172656
        }
      ]
    },
    {
      "id": 172737,
      "typeId": 13797,
      "title": "ICELab Demo: an industrial digital-twin and simulator in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1139",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "In this demo we present an application featuring the integration of Virtual Reality (VR) technologies with the demonstration laboratory (ICELab) built around Industry 4.0/5.0 concepts. In particular, we showcase a digital twin of the real laboratory that allows the user to explore its environment in VR and interact with the different machinery to obtain several data and information.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": ""
            }
          ],
          "personId": 172607
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": ""
            }
          ],
          "personId": 172599
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": "Department of Engineering for Innovation Medicine"
            }
          ],
          "personId": 172623
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": ""
            }
          ],
          "personId": 172662
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172600
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "University of Verona",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 172632
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Verona",
              "institution": "Università di Verona",
              "dsl": ""
            }
          ],
          "personId": 172516
        }
      ]
    },
    {
      "id": 172738,
      "typeId": 13797,
      "title": "White Lies in Virtual Reality: Impact on Enjoyment and Fatigue",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1138",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "In this paper, we investigated the effects of a \"white lie,\" which aims to enhance players' motivation while playing VR exercise, on mental elements of enjoyment and mental fatigue.  We utilized the ball throwing task as one example of a monotonous exercise and compared the group with a white lie and not. Results showed that participants found simple tasks such as ball throwing and target hitting enjoyable, with 17 out of 18 participants reporting that they had fun, regardless of the group.  Both groups experienced similar levels of enjoyment and mental fatigue, suggesting that the white lie did not significantly influence these factors. Introducing the \"White Lie\" system demonstrates the potential to mask effort perceptions without diminishing user satisfaction when its presence is disclosed. Although most participants preferred incorporating \"White Lie\" into their exercise routines, white lie did not enhance enjoyment or reduce fatigue in this context. However, the positive experience across all participants highlights the potential of VR for promoting exercise engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tamagawa University",
              "dsl": "Brain Science Institute"
            }
          ],
          "personId": 172673
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Turin",
              "institution": "Univeristy of Turin",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 172595
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Università degli Studi di Torino",
              "dsl": "Computer Science Dpt"
            }
          ],
          "personId": 172555
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tamagawa University",
              "dsl": "Brain Science Institute"
            }
          ],
          "personId": 172592
        }
      ]
    },
    {
      "id": 172739,
      "typeId": 13797,
      "title": "The Guide's Apprentices: Engaging Visitors of Virtual Museums through Appropriate Agent Embodiments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1017",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Embodied conversational agents are an established approach for immersively conveying the narratives and information of virtual museums.\r\nRecent advances in large language models and text-to-speech systems enable the active guidance of users by agents and raise the question of how such assistants should be visually represented to engage visitors and promote knowledge transfer.\r\nThis demo showcases a stylized humanoid guide, a novel animism-based approach with speaking objects, and a combination of both concepts to interactively guide users through a digital replica of the famous Gropius Room at the Bauhaus University in Weimar.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172628
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172686
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172547
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172642
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 172659
        }
      ]
    },
    {
      "id": 172740,
      "typeId": 13797,
      "title": "Hybrid Input Technique for VR Combining Head Mounted Keyboard with Head Pointing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1027",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "The Head Mounted Keyboard (HMK) is a text input method that attaches a physical split keyboard to the head mounted display (HMD).\r\nIt allows high-speed text input by utilizing the touch-typing skills that many PC users have.\r\nHowever, numeric input with HMK is challenging because the user cannot see the keyboard.\r\nOur solution is a hybrid technique that combines HMK with head pointing.\r\nThe technique, inputs letters via the HMK while numeric input uses head pointing to a virtual keyboard.\r\nIn an experiment on typing tasks, the proposed technique outperforms the conventional alternatives.\r\nSome application examples were implemented to explore the application space of the technique.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Texhnology",
              "dsl": ""
            }
          ],
          "personId": 172664
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO",
              "dsl": ""
            }
          ],
          "personId": 172543
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Shibaura Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 172603
        }
      ]
    },
    {
      "id": 172741,
      "typeId": 13797,
      "title": "Cybersicker: An Open Source VR Sickness Testbed - Do you still have fun, or are you already sick?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1069",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Cybersickness poses a significant barrier to the widespread adoption of VR applications, particularly when virtual movement occurs without corresponding physical movement. \r\nWhile current strategies to mitigate cybersickness exist, none are entirely effective, especially if virtual movement is essential. Further research is needed, and this requires controlled induction of cybersickness in empirical studies. In this demo, we introduce Cybersickner, a simulator designed to bridge the gap between high-intensity, proprietary simulators and less engaging alternatives. The simulator provides controlled translational and rotational vection while encouraging user participation. The simulator features multiple rotation axes, an adjustable speed increase, and additional fairground-style elements like a light show and loud music to enhance the experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": ""
            }
          ],
          "personId": 172424
        },
        {
          "affiliations": [
            {
              "country": "Costa Rica",
              "state": "",
              "city": "Cartago",
              "institution": "Costa Rica Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 172609
        }
      ]
    },
    {
      "id": 172742,
      "typeId": 13797,
      "title": "Comparing Tracking Accuracy in Standalone MR-HMDs: Apple Vision Pro, Hololens 2, Meta Quest 3, and Pico 4 Pro",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1025",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Modern Mixed Reality Head-Mounted Displays (MR-HMDs) can track user movements across large spaces without external markers. This study evaluates the tracking accuracy and the loop closure capabilities of four commercially available MR-HMDs across four distinct scenarios. We found consistent tracking performance in well-lit and expansive environments for all devices. Tracking accuracy remained stable even in outdoor nighttime conditions. Furthermore, most HMDs demonstrated effective error correction during loop closure, with errors in non-loop scenarios consistently exceeding those in loop scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "St. Gallen",
              "city": "Buchs",
              "institution": "RhySearch",
              "dsl": "Digital Innovation Lab"
            },
            {
              "country": "Switzerland",
              "state": "Zurich",
              "city": "Zurich",
              "institution": "Innovation Center Virtual Reality",
              "dsl": ""
            }
          ],
          "personId": 172524
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "St. Gallen",
              "city": "Buchs",
              "institution": "RhySearch",
              "dsl": "Digital Innovation Lab"
            }
          ],
          "personId": 172624
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Innovation Center Virtual Reality"
            }
          ],
          "personId": 172648
        }
      ]
    },
    {
      "id": 172743,
      "typeId": 13797,
      "title": "Testing 360-Degree Video Communication in a Debate Training between Teens: An Exploratory Field Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1067",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "This paper evaluates a virtual teleportation system based on 360-degree video communication in a field study at a secondary school and a guided debate training session. Socioemotional aspects, including social presence and avatar representation, were assessed by eight teens. Key findings include a higher \"wow\" effect for those joining the immersive environment, and challenges with avatar interaction such as raising their hand. The system shows promise as an educational tool for debate training, enhancing learning and student engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Nokia Spain",
              "dsl": "eXtended Reality Lab (XR Lab)"
            }
          ],
          "personId": 172527
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": "XR Lab"
            }
          ],
          "personId": 172528
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Department of Information and Communication Technologies"
            }
          ],
          "personId": 172661
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": "XR Lab"
            }
          ],
          "personId": 172688
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": "eXtended Reality Lab"
            }
          ],
          "personId": 172537
        }
      ]
    },
    {
      "id": 172744,
      "typeId": 13797,
      "title": "VR4UrbanDev: An Immersive Virtual Reality Experience for Energy Data Visualization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1100",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "The widespread adoption of virtual reality (VR) among consumers has paved the way for numerous industries to explore environments that are either costly, dangerous, or difficult to access. Among these industries, the architecture, engineering, and construction (AEC) sector can particularly reap the benefits of VR environments, as they necessitate simulation, visualization, and collaboration during projects. In this demonstration paper, we present our interactive VR experience, which has been designed to facilitate interaction with energy-related information in the AEC field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Institute of Interactive Systems and Data Science",
              "dsl": "Graz University of Technology"
            }
          ],
          "personId": 172550
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172549
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172605
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172666
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 172677
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "TU Graz",
              "dsl": ""
            }
          ],
          "personId": 172291
        }
      ]
    },
    {
      "id": 172745,
      "typeId": 13797,
      "title": "Towards an Avatar Customization System for Semi-realistic Ethnically-diverse Virtual Reality Avatars",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1022",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Due to the Proteus effect, in which people modify their behaviour based on their avatar, participant avatar representation is an important factor in virtual reality research studies. We develop an open source prototype avatar customization system that enables quick customization of semi-photorealistic, ethnically-diverse avatars, intended for use by participants in virtual reality studies. The prototype provides options for customizing body and face shape, hairstyle, glasses, religious clothing, and skin, eye, and hair colour. Our prototype generates avatar assets that are fully rigged and textured for incorporation into VR study code, and the prototype serves as a step towards designing more inclusive virtual reality research studies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 172563
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Cheriton School of Computer Science"
            }
          ],
          "personId": 172606
        }
      ]
    },
    {
      "id": 172746,
      "typeId": 13797,
      "title": "Hands-On Plant Root System Reconstruction in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1064",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "VRoot is an immersive extended reality reconstruction tool for root system architectures from 3D volumetric scans of soil columns.\r\nWe have conducted a laboratory user study to assess the performance of new users with our software in comparison to classical and established desktop software.\r\nWe utilize a functional-structural plant model to derive a synthetic root architecture that serves as objective quantification for the root system architecture reconstruction.\r\nThis demo showcases the processes and techniques that contribute to exact and efficient manual root architecture reconstruction in Virtual Reality.\r\nThe extraction task typically is the sparse graph-structure extraction from a 3D magnetic-resonance imaging (MRI) data set.\r\nWe visualize the RSA directly within the MRI and offer selection-set based methods of adapting and augmenting the root architecture.\r\nThis application is in productive use at the Institute of Bio- and Geosciences 3: Agrosphere, where it is used to analyze complex root images.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Jülich",
              "institution": "Forschungszentrum Jülich GmbH",
              "dsl": "Jülich Supercomputing Centre"
            },
            {
              "country": "Iceland",
              "state": "",
              "city": "Reykjavík",
              "institution": "University of Iceland",
              "dsl": "School of Engineering and Natural Sciences"
            }
          ],
          "personId": 172494
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Jülich",
              "institution": "Forschungszentrum Jülich",
              "dsl": ""
            }
          ],
          "personId": 172515
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Juelich",
              "institution": "Juelich Supercomputing Centre",
              "dsl": "Forschungszentrum Juelich GmbH"
            }
          ],
          "personId": 172630
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Jülich",
              "institution": "Forschungszentrum Jülich GmbH",
              "dsl": "Institute of Advanced Simulation 8: Data Analytics and Machine Learning"
            }
          ],
          "personId": 172581
        },
        {
          "affiliations": [
            {
              "country": "Iceland",
              "state": "",
              "city": "Reykjavík",
              "institution": "University of Iceland",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Jülich",
              "institution": "Forschungszentrum Jülich GmbH",
              "dsl": "Jülich Supercomputing Centre"
            }
          ],
          "personId": 172611
        },
        {
          "affiliations": [
            {
              "country": "Iceland",
              "state": "",
              "city": "Reykjavik",
              "institution": "University of Iceland",
              "dsl": "School of Engineering and Natural Sciences"
            }
          ],
          "personId": 172553
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Jülich",
              "institution": "Forschungszentrum Jülich GmbH",
              "dsl": "Insitute of Bio- and Geosciences 3: Agrosphere"
            }
          ],
          "personId": 172681
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "University of Trier",
              "dsl": "Human Computer Interaction"
            }
          ],
          "personId": 172424
        }
      ]
    },
    {
      "id": 172747,
      "typeId": 13797,
      "title": "Earscape: A VR Auditory Educational Escape Room",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1062",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "According to the World Health Organisation’s World Report on Hearing, there is a strong need to provide better education on hearing loss from a young age. This project aims to educate the Danish young population (13 to 17-year-olds) about the hearing sense through an educational multiplayer virtual reality-based escape room with the benefits of educational escape rooms. In collaboration with relevant audiologist stakeholders, this project follows an iterative process of design, implementation, and evaluation of the application. The developed solution will undergo several user studies in the following months.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Aalborg University",
              "dsl": ""
            }
          ],
          "personId": 172652
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Aalborg University Copenhagen",
              "dsl": "Multisensory Experience Lab"
            }
          ],
          "personId": 172583
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Copenhagen Hearing and Balance Center, Rigetshospital",
              "dsl": ""
            }
          ],
          "personId": 172521
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Aalborg University",
              "dsl": "Department of Architecture, Design and Media Technology"
            }
          ],
          "personId": 172554
        }
      ]
    },
    {
      "id": 172748,
      "typeId": 13797,
      "title": "SOLDAR: Supporting Low-Volume PCB Prototyping Using Collaborative Robots and Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1061",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "Printed circuit boards (PCBs) are fundamental to modern electronics, present in almost every electronic device. However, despite their ubiquity, current PCB assembly methods can be time-consuming and lack flexibility for one-off designs. This poster investigates how low-volume PCB prototyping can be enhanced by integrating collaborative robots (cobots) and Augmented Reality (AR). Specifically, we introduce SOLDAR, a system that facilitates the soldering of electronic through-hole components on PCBs. By using a cobot for optimal PCB positioning and AR glasses for step-by-step guidance, SOLDAR aims to streamline the assembly process. The expected outcomes are increased efficiency, reduced assembly time, and greater flexibility for low-volume PCB prototyping designs. To validate these hypotheses, user experiments are necessary.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "UHasselt",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172500
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "UHasselt - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 172601
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Limburg",
              "city": "Diepenbeek",
              "institution": "Hasselt University, Flanders Make - Expertise Centre for Digital Media",
              "dsl": "Hasselt University"
            }
          ],
          "personId": 172598
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Hasselt",
              "institution": "Flanders Make - Expertise Centre for Digital Media",
              "dsl": "Hasselt University "
            }
          ],
          "personId": 172674
        }
      ]
    },
    {
      "id": 172749,
      "typeId": 13797,
      "title": "Virtual Lab - A VR Showroom for Biosignals Research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1060",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172241
      ],
      "abstract": "Virtual Reality (VR) showrooms have emerged as powerful tools in\r\nthe realm of teaching and communication [Gironimo and Papa 2006;\r\nNowak and Flotyński 2018], offering immersive and interactive\r\nexperiences that help bring concepts and new developments to live\r\nfor visitors. In this context, we present a prototype for Virtual Lab,\r\na VR showroom designed to present biosignals research and devices\r\n(Figure 1). The primary purpose of Virtual Lab is to highlight and\r\nteach about scientific projects and the devices used within these in\r\nan interactive and hands-on manner. In Virtual Lab, participants\r\ncan engage directly with biosignals devices and observe detailed\r\nanimations of their application, including live changes induced by\r\ntheir own actions. Virtual Lab will be accessible from a Web-based\r\nplatform, which opens up the possibility for interested parties to\r\naccess the replica from any location and engage with the equipment\r\nthere. We hope that through these means, Virtual Lab can foster\r\na deeper understanding and appreciation of the science behind\r\nbiosignals research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 172548
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 172690
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 172560
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 172658
        }
      ]
    },
    {
      "id": 172750,
      "typeId": 13797,
      "title": "Learning From Yourself: Effects of Doppelgängers on Foreign Language Anxiety, Trust, and Learning Outcomes ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "vrst24a-1109",
      "source": "PCS",
      "trackId": 13068,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        172764,
        172763
      ],
      "abstract": "We investigate the role of doppelgängers on foreign language anxiety, trustworthiness, and Spanish vocabulary recall. Participants (n = 31) completed three Spanish language lessons presented by a doppelgänger, a generic virtual human, or a disembodied voice in immersive or desktop VR; completed the Foreign Language Anxiety Classroom Scale (FLACS), Ohanian inventory on trustworthiness; and were assessed on vocabulary recall. Our findings suggest potential avenues for leverage with the design of virtual humans for foreign language learning. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wyoming",
              "city": "Laramie",
              "institution": "University of Wyoming",
              "dsl": ""
            }
          ],
          "personId": 172629
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wyoming",
              "city": "Laramie",
              "institution": "University of Wyoming",
              "dsl": ""
            }
          ],
          "personId": 172503
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Fulton",
              "institution": "Itawamba County Schools",
              "dsl": ""
            }
          ],
          "personId": 172531
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wyoming",
              "city": "Laramie",
              "institution": "University of Wyoming",
              "dsl": ""
            }
          ],
          "personId": 172678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wyoming",
              "city": "Laramie",
              "institution": "University of Wyoming",
              "dsl": ""
            }
          ],
          "personId": 172523
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wyoming",
              "city": "Laramie",
              "institution": "University of Wyoming",
              "dsl": "Interactive Realities Lab"
            }
          ],
          "personId": 172693
        }
      ]
    },
    {
      "id": 172767,
      "typeId": 13792,
      "title": "Keynote: Playing with tangibles in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 13078,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172770
      ],
      "eventIds": [],
      "abstract": "The talk presents how tangible objects could be used in Virtual Reality to enhance the user’s perception and immersion. In Virtual Reality, the user wearing a Head Mounted Display is generally unable to directly see the tangible objects and ends up confronting the virtual objects he sees to the tangible ones he can feel. This situation often leads to breaks of immersion, thus degrading the user’s experience in virtual environments.\n\nThe talk presents some of our latest contributions to handle the use of tangible objects for improving our 3D interaction with virtual worlds. The talk will illustrate first how and to what extent a discrepancy between the tangible objects and the virtual objects can be introduced without breaking the user’s immersion through different algorithmic strategies. In a second part, the talk will present how we could improve the registration between the tangible and the virtual objects using new technological solutions combined with appropriate 3D interaction techniques. At the end, the talk aims at introducing some of the next challenges in Virtual Reality for handling haptic feedback through the use of tangible objects.",
      "authors": [
        {
          "affiliations": [],
          "personId": 172765
        }
      ]
    },
    {
      "id": 172768,
      "typeId": 13792,
      "title": "Keynote: Reimagining scholarly publishing to promote credible and trustworthy research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "2",
      "source": "CSV",
      "trackId": 13079,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        172769
      ],
      "eventIds": [],
      "abstract": "The purpose of scholarly publishing is to facilitate the communication and interrogation of evidence and claims to advance knowledge production. The business of scholarly publishing interferes with this purpose. Research is inhibited by a scholarly publishing system that is slow, incomplete, opaque, and static, treats the paper as the only meaningful scholarly output, offers dysfunctional, simplistic rewards based on publication and journal status, and is calcified in legacy, commercial business models, and infrastructure. The Lifecycle Journal is an alternative approach to scholarly publishing intended to address these weaknesses and align the practice of scholarly publishing with its purpose.",
      "authors": [
        {
          "affiliations": [],
          "personId": 172766
        }
      ]
    }
  ],
  "people": [
    {
      "id": 172247,
      "firstName": "DongYun",
      "lastName": "Joo",
      "middleInitial": "",
      "importedId": "ua9R5i23qDszk7sjjxOv7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172248,
      "firstName": "Zhuying",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "vx--wCIHC5ibFveZh31S-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172249,
      "firstName": "Niko Bach",
      "lastName": "Jensen",
      "middleInitial": "",
      "importedId": "v48C-KqW4MPVdpUwt3d-PA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172250,
      "firstName": "Robin",
      "lastName": "Hänni",
      "middleInitial": "Timon",
      "importedId": "04KlBREZdyyfQSV5ogOUZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172251,
      "firstName": "Lisa",
      "lastName": "Reichl",
      "middleInitial": "",
      "importedId": "1t8w_sw54QAzjkXXxI3QFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172252,
      "firstName": "Myoung Gon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "f7s_Rh9yt1JA7IQnlLtNJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172253,
      "firstName": "Mingqing",
      "lastName": "XU",
      "middleInitial": "",
      "importedId": "HhZJ61IOGTjjsyfFMqWkiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172254,
      "firstName": "Torsten",
      "lastName": "Kuhlen",
      "middleInitial": "Wolfgang",
      "importedId": "X04CYKYDVKKapfPiPeGdsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172255,
      "firstName": "Kilian",
      "lastName": "Sinke",
      "middleInitial": "",
      "importedId": "blzw8j8aRl52pAh-0xN3aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172256,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "importedId": "ob1mmWPesH21Yfh_UxGqsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172257,
      "firstName": "Niels",
      "lastName": "Henze",
      "middleInitial": "",
      "importedId": "HOEWm6ALiUarpg2dwCcZSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172258,
      "firstName": "Yamato",
      "lastName": "Miyashita",
      "middleInitial": "",
      "importedId": "LTW8davoL2iIgeKZFS9gFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172259,
      "firstName": "Kristian",
      "lastName": "Julsgaard",
      "middleInitial": "",
      "importedId": "fUKl0CU8EPVQRdE_34Airw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172260,
      "firstName": "Zhu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "uZDXU7gMOBeX0PJxUhfHjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172261,
      "firstName": "Jill",
      "lastName": "Marshall",
      "middleInitial": "",
      "importedId": "GSL2NrBzJI-JMnutOMsJbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172262,
      "firstName": "Constantin",
      "lastName": "Kleinbeck",
      "middleInitial": "",
      "importedId": "cbLMJx6ER65W7JOoW4m55w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172263,
      "firstName": "Leonie",
      "lastName": "Tanczer",
      "middleInitial": "Maria",
      "importedId": "jI_BN7JweYh8T-7-I_05jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172264,
      "firstName": "Zhongyue",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "Jz4uJMP8oHvxXXHl_m79Tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172265,
      "firstName": "Erwan",
      "lastName": "Leria",
      "middleInitial": "",
      "importedId": "FZk6eWf5g0HwF1Qg-gIOew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172266,
      "firstName": "Seohyeon",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "yYoLJBfeNy08hqsdKFo0lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172267,
      "firstName": "Paul",
      "lastName": "Gloumeau",
      "middleInitial": "Christopher",
      "importedId": "cT90dYi4_-JtT4w0EYtxrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172268,
      "firstName": "Fabian",
      "lastName": "Di Fiore",
      "middleInitial": "",
      "importedId": "no-4XVwC1EJKGXRD4cT4Dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172269,
      "firstName": "Qi Wen",
      "lastName": "Gan",
      "middleInitial": "",
      "importedId": "7o74ied_hryRP-jvIkxNNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172270,
      "firstName": "Tomohiro",
      "lastName": "Amemiya",
      "middleInitial": "",
      "importedId": "Chitd1KQixsidTz9yJ5fZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172271,
      "firstName": "Rüdiger",
      "lastName": "Braun-Dullaeus",
      "middleInitial": "",
      "importedId": "QX-xbBZjJqTjYNwZB60sqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172272,
      "firstName": "Christian",
      "lastName": "Rack",
      "middleInitial": "",
      "importedId": "VP_R5lL_9CnqfZ1c7AZNaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172273,
      "firstName": "Ronan",
      "lastName": "Querrec",
      "middleInitial": "",
      "importedId": "PP4V7wnsaYNNmCPTVaSOCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172274,
      "firstName": "Ian",
      "lastName": "Oakley",
      "middleInitial": "",
      "importedId": "b2tvt-MyhdyGByDL4JTqtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172275,
      "firstName": "Lukas",
      "lastName": "Schach",
      "middleInitial": "",
      "importedId": "qOSE5Io6M0Yj1u9NMO3uzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172276,
      "firstName": "Augusto",
      "lastName": "Esteves",
      "middleInitial": "",
      "importedId": "7f2iQ_8H8O1G87Aj2KDagw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172277,
      "firstName": "Christos",
      "lastName": "Lougiakis",
      "middleInitial": "",
      "importedId": "I1yHZQI-El6eCvfNONys6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172278,
      "firstName": "Thibault",
      "lastName": "Harquin",
      "middleInitial": "",
      "importedId": "tENCk4HjQkeSEsdqgxxqmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172279,
      "firstName": "Lucas",
      "lastName": "Brand",
      "middleInitial": "",
      "importedId": "UO0_LBkd8xH3xR0JTwhPwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172280,
      "firstName": "Felix",
      "lastName": "Achter",
      "middleInitial": "",
      "importedId": "RdH4g5K2mCr-pwy8GWwVrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172281,
      "firstName": "Inês",
      "lastName": "Alves",
      "middleInitial": "",
      "importedId": "9Mm5d18dU10OcOefGk6eJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172282,
      "firstName": "Kalila",
      "lastName": "Shapiro",
      "middleInitial": "",
      "importedId": "5V_Zjbz88UrpkM4tUaQt5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172283,
      "firstName": "Sun-Uk",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "jdl8aPy5QOZYBco-N0fMXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172284,
      "firstName": "Xiangyu",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "qD3JurHzHF_IyYkftqONKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172285,
      "firstName": "Yuli",
      "lastName": "Shao",
      "middleInitial": "",
      "importedId": "HEMrnoLeXp0nIppfyXHcYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172286,
      "firstName": "Hanseob",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "PKufY3vqjOhjNXkWfkqEyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172287,
      "firstName": "Patric",
      "lastName": "Schmitz",
      "middleInitial": "",
      "importedId": "PH1S-X-uabh7ETUsmdzpGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172288,
      "firstName": "Fabien",
      "lastName": "Grzeskowiak",
      "middleInitial": "",
      "importedId": "f1VVjW1m0UZapzquoRE_hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172289,
      "firstName": "Kris",
      "lastName": "Luyten",
      "middleInitial": "",
      "importedId": "uKnd7nN3OsOEy2epODWgQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172290,
      "firstName": "Marie",
      "lastName": "Babel",
      "middleInitial": "",
      "importedId": "3GLeQSBnylqSQWUz5A1Ldg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172291,
      "firstName": "Johanna",
      "lastName": "Pirker",
      "middleInitial": "",
      "importedId": "lwND6YgAz-JXMzgXRIl3jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172292,
      "firstName": "Hannah",
      "lastName": "Schieber",
      "middleInitial": "",
      "importedId": "YJpLAYJM7kVCWh7zIkNrnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172293,
      "firstName": "Valentina",
      "lastName": "Rondinelli",
      "middleInitial": "",
      "importedId": "KqUvAuakbwTfzWSpbK4X0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172294,
      "firstName": "Danny",
      "lastName": "Schott",
      "middleInitial": "",
      "importedId": "LYcPY3ECf_WaAKK9hYOybQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172295,
      "firstName": "Hideaki",
      "lastName": "Kuzuoka",
      "middleInitial": "",
      "importedId": "b7NCSK2IREioerbZhPPDhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172296,
      "firstName": "Arthur",
      "lastName": "Chaminade",
      "middleInitial": "",
      "importedId": "djnS1zHaVXRt8GV3XUvU_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172297,
      "firstName": "Chiwoong",
      "lastName": "Hwang",
      "middleInitial": "",
      "importedId": "h809nPh5BnVxTcbz6ypFPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172298,
      "firstName": "Ayaka",
      "lastName": "Sano",
      "middleInitial": "",
      "importedId": "XlvW7VIOQl7zpOdnMUcf5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172299,
      "firstName": "Fabian",
      "lastName": "Räthel",
      "middleInitial": "",
      "importedId": "4H5Juw-tQCIQMlp9gzJCQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172300,
      "firstName": "Takashi",
      "lastName": "Ota",
      "middleInitial": "",
      "importedId": "0OpNAWjtOlU9rWfUf8n4Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172301,
      "firstName": "Larissa",
      "lastName": "Brübach",
      "middleInitial": "",
      "importedId": "h4_TioDgFt9ChH3x5VikXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172302,
      "firstName": "Leon",
      "lastName": "Reicherts",
      "middleInitial": "",
      "importedId": "psnk66MAH-N-MNDPYyA94A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172303,
      "firstName": "Lena",
      "lastName": "Podoletz",
      "middleInitial": "",
      "importedId": "rBaxKBveWXBsoXFnariXJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172304,
      "firstName": "Tim",
      "lastName": "Weissker",
      "middleInitial": "",
      "importedId": "j20hHOW2RY54EBAd1fWzgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172305,
      "firstName": "Lorans",
      "lastName": "Alabood",
      "middleInitial": "",
      "importedId": "OWFsnh2af6XNMj804eKkGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172306,
      "firstName": "Martin",
      "lastName": "Kocur",
      "middleInitial": "",
      "importedId": "Y5ftVlnXsxwlzIEps73qXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172307,
      "firstName": "David",
      "lastName": "Anders",
      "middleInitial": "",
      "importedId": "O2g-AFUDDY3hG5_dH1R9uA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172308,
      "firstName": "Franziska",
      "lastName": "Westermeier",
      "middleInitial": "",
      "importedId": "ujahRJ8v_bDfoUTIHxJLMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172309,
      "firstName": "Jean-Luc",
      "lastName": "Lugrin",
      "middleInitial": "",
      "importedId": "r4rY9X740H62oLJErUf-nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172310,
      "firstName": "Lukas",
      "lastName": "Posniak",
      "middleInitial": "",
      "importedId": "IbuHBhjfxXLkd2iZ4LI60w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172311,
      "firstName": "Ralf",
      "lastName": "Bierig",
      "middleInitial": "",
      "importedId": "s_mGqQPGDZoVGiDSlerAvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172312,
      "firstName": "Seongkook",
      "lastName": "Heo",
      "middleInitial": "",
      "importedId": "8u6ENy5fbi7xGnVehduTFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172313,
      "firstName": "Valentin",
      "lastName": "Bräutigam",
      "middleInitial": "",
      "importedId": "_jJ_91cOLMVlIcceWB-D5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172314,
      "firstName": "Sen-Zhe",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "a74FJYDRISCJAvD_yegKWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172315,
      "firstName": "Song-Hai",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "e514cWA4xmQZnD-p0hHkRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172316,
      "firstName": "Clarence",
      "lastName": "Cheung",
      "middleInitial": "Chi San",
      "importedId": "V3xHciAkkiGh-RNuOPzlsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172317,
      "firstName": "Michael",
      "lastName": "Holly",
      "middleInitial": "",
      "importedId": "WtxnizOMLLWCKrgnaohH9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172318,
      "firstName": "Benjamin",
      "lastName": "Weyers",
      "middleInitial": "",
      "importedId": "Lp_V5AGjKnaENLx8IYyfpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172319,
      "firstName": "Ahmadreza",
      "lastName": "Nazari",
      "middleInitial": "",
      "importedId": "h9YjAQMA6l7uRXA9UnYuRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172320,
      "firstName": "Fabian",
      "lastName": "Unruh",
      "middleInitial": "",
      "importedId": "VcEU3KeQAZyagrjnUoxlNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172321,
      "firstName": "Dustin",
      "lastName": "Augsten",
      "middleInitial": "",
      "importedId": "BPpo7D-9ZA1CEzjPXqGSkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172322,
      "firstName": "Jean",
      "lastName": "Botev",
      "middleInitial": "",
      "importedId": "d9s6by8ZcT1lyddOFIPZiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172323,
      "firstName": "Alex",
      "lastName": "Adkins",
      "middleInitial": "",
      "importedId": "5Ot48TOQNk_mOpLALSsTrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172324,
      "firstName": "Jonas",
      "lastName": "Mandel",
      "middleInitial": "",
      "importedId": "ix7cpayl3Pnr7E1XMrVWSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172325,
      "firstName": "Jae Hoon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "05yOGMlP6JLRqCziJpohyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172326,
      "firstName": "Ayami",
      "lastName": "Hoshi",
      "middleInitial": "",
      "importedId": "U6aWkFYAX0_qCHL6ZijPYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172327,
      "firstName": "Christian",
      "lastName": "Hansen",
      "middleInitial": "",
      "importedId": "O6dpkaz2cyqpc0cLphO9rQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172328,
      "firstName": "Fang-Lue",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "m4KsOLvxxr-9idMGarsGTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172329,
      "firstName": "Kaushal Kumar",
      "lastName": "Bhagat",
      "middleInitial": "",
      "importedId": "hMtt8m7CScsK3LnS_bpEfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172330,
      "firstName": "Keigo",
      "lastName": "Matsumoto",
      "middleInitial": "",
      "importedId": "vm0-K7gO1OwZrUTFR1TcEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172331,
      "firstName": "Nadia",
      "lastName": "Pantidi",
      "middleInitial": "",
      "importedId": "cVye3CA0c3N6ptMCaRgmFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172332,
      "firstName": "Mingming",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "Sf6s1OuvcuM-r64kIOrFFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172333,
      "firstName": "Wen",
      "lastName": "Ying",
      "middleInitial": "",
      "importedId": "pksbxG9YNE7dgwZ2i8JCcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172334,
      "firstName": "SeungWon",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "Hz1ynEscJyKqAC_fVg6c1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172335,
      "firstName": "Kazuma",
      "lastName": "Aoyama",
      "middleInitial": "",
      "importedId": "QwJ960O3CG-lPafupxAyrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172336,
      "firstName": "Giorgos",
      "lastName": "Ganias",
      "middleInitial": "",
      "importedId": "Qf_gH27ScLVj4LNfKse5wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172337,
      "firstName": "Jörg",
      "lastName": "Lohscheller",
      "middleInitial": "",
      "importedId": "A0XFyWtuDpnii58pNhileg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172338,
      "firstName": "Charles",
      "lastName": "Markham",
      "middleInitial": "",
      "importedId": "-9qqAYPDoDAM9JrCoYZ27w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172339,
      "firstName": "Nikolai",
      "lastName": "Hepke",
      "middleInitial": "",
      "importedId": "irts82sLJLRaNAAeCb_ZuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172340,
      "firstName": "Ferran",
      "lastName": "Argelaguet Sanz",
      "middleInitial": "",
      "importedId": "hDnwHAn8UT3bSm9Ib36Hig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172341,
      "firstName": "Mauricio",
      "lastName": "Sousa",
      "middleInitial": "",
      "importedId": "4FX8NG9vjNpg0IflWPZcpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172342,
      "firstName": "Stephen",
      "lastName": "Pettifer",
      "middleInitial": "Robert",
      "importedId": "qwJV-YtOABON2U_TWGfq4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172343,
      "firstName": "Steffen",
      "lastName": "Mueller",
      "middleInitial": "",
      "importedId": "OUxYdokYEB35Vmo5ZkiUKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172344,
      "firstName": "Sevinc",
      "lastName": "Eroglu",
      "middleInitial": "",
      "importedId": "2v5Vr0w2sAFTj-qI1sORTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172345,
      "firstName": "Claudio",
      "lastName": "Pacchierotti",
      "middleInitial": "",
      "importedId": "JVJkDgsVFNUp9CpLclb0-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172346,
      "firstName": "Charlotte",
      "lastName": "Hoareau",
      "middleInitial": "",
      "importedId": "Q-fl0b-4m6Jt7Q2szT9Mzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172347,
      "firstName": "Keru",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "ZGP8h-nC7EmCLwB4ZbXqWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172348,
      "firstName": "Alexander",
      "lastName": "Kalus",
      "middleInitial": "",
      "importedId": "z34zA6WgKv1OBOv5nPw_Zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172349,
      "firstName": "Susanne",
      "lastName": "Schmidt",
      "middleInitial": "",
      "importedId": "nty9T-FExBWpAOmqzwYWjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172350,
      "firstName": "Chao",
      "lastName": "LIU",
      "middleInitial": "",
      "importedId": "d-gU-sCaiz_sFTmzUQN9-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172351,
      "firstName": "Kaj",
      "lastName": "Grønbæk",
      "middleInitial": "",
      "importedId": "7q6wCAlX85zIMCKTWiMQbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172352,
      "firstName": "Molly",
      "lastName": "Rathbun",
      "middleInitial": "Kay",
      "importedId": "iN4SoFtLpNKxZgVS9GmNAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172353,
      "firstName": "Eric",
      "lastName": "Marchand",
      "middleInitial": "",
      "importedId": "_kDDx-iNU21x5Xja6qLFfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172354,
      "firstName": "Carolin",
      "lastName": "Wienrich",
      "middleInitial": "",
      "importedId": "o8lLgPKdnkk7J2K0Hu3oxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172355,
      "firstName": "Mingyang",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "DFc5MFViU0TDQKzfitRDUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172356,
      "firstName": "Anne",
      "lastName": "Albrecht",
      "middleInitial": "",
      "importedId": "prlJ7U89lv8Ry40JdAhhjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172357,
      "firstName": "David",
      "lastName": "McIlhatton",
      "middleInitial": "",
      "importedId": "tdkOE67zLjEyct-DG6HVog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172358,
      "firstName": "Holger",
      "lastName": "Dressel",
      "middleInitial": "",
      "importedId": "1-6Skog5WkLp9Q0EWn-hrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172359,
      "firstName": "Christian",
      "lastName": "Holz",
      "middleInitial": "",
      "importedId": "iLPlU_z1dah9YY__uEeXsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172360,
      "firstName": "Frank",
      "lastName": "Steinicke",
      "middleInitial": "",
      "importedId": "w2YEfsGpoTqbpROww8WcOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172361,
      "firstName": "Jasmin",
      "lastName": "Niess",
      "middleInitial": "",
      "importedId": "7cCtnrBqZkw8ngUiXKa6tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172362,
      "firstName": "Marc",
      "lastName": "Latoschik",
      "middleInitial": "Erich",
      "importedId": "4yN36kev1hbUnLrcETD5iA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172363,
      "firstName": "Daniel",
      "lastName": "Roth",
      "middleInitial": "",
      "importedId": "lQjdWHa7mIxwI8CByoeigA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172364,
      "firstName": "Maria",
      "lastName": "Roussou",
      "middleInitial": "",
      "importedId": "XBKj6w51-g9JWpLG5pZONA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172365,
      "firstName": "Johannes",
      "lastName": "Schöning",
      "middleInitial": "",
      "importedId": "fc9sshp8L9i6K6CitF8edw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172366,
      "firstName": "Seamus",
      "lastName": "Thierry",
      "middleInitial": "",
      "importedId": "L9XRSBYT0bnQCXTIC6cNtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172367,
      "firstName": "Jan",
      "lastName": "Plass",
      "middleInitial": "",
      "importedId": "qn1ap7no2BXca6cJqqaPNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172368,
      "firstName": "Motohiro",
      "lastName": "Makiguchi",
      "middleInitial": "",
      "importedId": "bj6y0qnptvktgkgfug2gDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172369,
      "firstName": "Kristian",
      "lastName": "Jespersen",
      "middleInitial": "Nyborg",
      "importedId": "r_BYM_ADrkIANb_vPgnNMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172370,
      "firstName": "Linda",
      "lastName": "Krueger",
      "middleInitial": "",
      "importedId": "SoWnvfyTnUqpPqsEgt8ovQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172371,
      "firstName": "Ken",
      "lastName": "Perlin",
      "middleInitial": "",
      "importedId": "q_HAZ9FA6-4bxYBde_YIFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172372,
      "firstName": "Tiffany",
      "lastName": "Luong",
      "middleInitial": "",
      "importedId": "op7L1Kh7jn3NKLQnh8RnRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172373,
      "firstName": "Kazuteru",
      "lastName": "Komine",
      "middleInitial": "",
      "importedId": "kAyra9rVKCdL8zgxBlsOYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172374,
      "firstName": "Nidhi",
      "lastName": "Joshi",
      "middleInitial": "",
      "importedId": "FL_0RgeuvCeWQhtHEEBhTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172375,
      "firstName": "Frederick",
      "lastName": "VICKERY",
      "middleInitial": "George",
      "importedId": "ly8Zk-3cDA-leA3Mjt3AiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172376,
      "firstName": "Jinghuai",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "esxrs5KO4cfJFm6cqy1_eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172377,
      "firstName": "Paweł W.",
      "lastName": "Woźniak",
      "middleInitial": "",
      "importedId": "VJzNdW_xniDVJjfrB8dpcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172378,
      "firstName": "Ioannis",
      "lastName": "Ioannidis",
      "middleInitial": "Panagiotis",
      "importedId": "rr6d1VWbZyAi8O0CRf6Qzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172379,
      "firstName": "Pekka",
      "lastName": "Jääskeläinen",
      "middleInitial": "",
      "importedId": "HgNVOcpltLxFrfExFDVcmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172380,
      "firstName": "Samir",
      "lastName": "Otmane",
      "middleInitial": "",
      "importedId": "JeViQLrtKsYjs6TyrGfKOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172381,
      "firstName": "Yao",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "GVzDz3TkYUAWexLXe7_nvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172382,
      "firstName": "Mathias",
      "lastName": "Jans",
      "middleInitial": "",
      "importedId": "B3ec1zVEHGM0UJnwyH85Dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172383,
      "firstName": "Aurelien",
      "lastName": "Duval",
      "middleInitial": "",
      "importedId": "K8UPwy4px3TRCPIni6udpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172384,
      "firstName": "Theodoros",
      "lastName": "Mandilaras",
      "middleInitial": "",
      "importedId": "MXmQ_Uf26_PMqQuCXLallQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172385,
      "firstName": "Sebastien",
      "lastName": "Thomas",
      "middleInitial": "",
      "importedId": "9ZhImlFWv1cx6_RQtfT0fA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172386,
      "firstName": "Takato",
      "lastName": "Mizuho",
      "middleInitial": "",
      "importedId": "lr-Yp5NWAl7qKBc9HN5B_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172387,
      "firstName": "Ding",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "vdBSpQUERKppr3Pfdnh7cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172388,
      "firstName": "Hugo",
      "lastName": "Brument",
      "middleInitial": "",
      "importedId": "x42npIzUdK53MY-8n6q0lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172389,
      "firstName": "Erwan",
      "lastName": "Normand",
      "middleInitial": "",
      "importedId": "cM4KPw_2QFBeqU4w6OleWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172390,
      "firstName": "Nadine",
      "lastName": "Wagener",
      "middleInitial": "",
      "importedId": "crCvaUuTosnWCpqIPFRRLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172391,
      "firstName": "Jenny",
      "lastName": "Gabel",
      "middleInitial": "",
      "importedId": "dFoECjaoOFc5pkfaC3FQTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172392,
      "firstName": "Pauline",
      "lastName": "Bimberg",
      "middleInitial": "",
      "importedId": "3hIZtj1lmts_mlM86d4z0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172393,
      "firstName": "Yousof",
      "lastName": "Shehada",
      "middleInitial": "",
      "importedId": "k_ECoPd7UOYSbFAHr-E4Vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172394,
      "firstName": "Evropi",
      "lastName": "Stefanidi",
      "middleInitial": "",
      "importedId": "Ad7-2v5GjnwqYpNAaCWH5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172395,
      "firstName": "Takuji",
      "lastName": "Narumi",
      "middleInitial": "",
      "importedId": "6C7aUtZkLRJIYT0_TSY-SQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172396,
      "firstName": "Sébastien",
      "lastName": "Kubicki",
      "middleInitial": "",
      "importedId": "-SjDoE9ZqrKPdK0YDDiYKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172397,
      "firstName": "Graham",
      "lastName": "Wilson",
      "middleInitial": "",
      "importedId": "sGtFXnJgdG9Og0TCkppwNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172398,
      "firstName": "Yushen",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "dCfifHUa2M5GGqyYViOLzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172399,
      "firstName": "Mårten",
      "lastName": "Sjöström",
      "middleInitial": "",
      "importedId": "tS2nejm5vJ3ME77DFQb8wA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172400,
      "firstName": "Marius",
      "lastName": "Meier-Krueger",
      "middleInitial": "",
      "importedId": "sPT_NBC2F9T6yPlMRaZT4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172401,
      "firstName": "Anne-Hélène",
      "lastName": "Olivier",
      "middleInitial": "",
      "importedId": "miXNWd7xsVnNIxh31ql0vg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172402,
      "firstName": "Felix",
      "lastName": "Mangold",
      "middleInitial": "",
      "importedId": "zaU-OSWg7cs76REX2-e8Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172403,
      "firstName": "Marius",
      "lastName": "Röhm",
      "middleInitial": "",
      "importedId": "2wJFHzlP3-GVBhCLzHck7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172404,
      "firstName": "Florian",
      "lastName": "Heinrich",
      "middleInitial": "",
      "importedId": "eNMawyq4vXKXMoOzgi8cNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172405,
      "firstName": "Tien-Julian",
      "lastName": "Ho",
      "middleInitial": "",
      "importedId": "vbD8ZWYMMyIYvGDlfx5_Jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172406,
      "firstName": "Julian",
      "lastName": "Kreimeier",
      "middleInitial": "",
      "importedId": "81fDmlrDFdorLcllvuH7tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172407,
      "firstName": "Moritz",
      "lastName": "Scherer",
      "middleInitial": "",
      "importedId": "cYZvY-9h5SB3qL_trOi0Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172408,
      "firstName": "JungHyun",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "CAx9sFY-lgPi4-yQD9IsDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172409,
      "firstName": "Sahar",
      "lastName": "Niknam",
      "middleInitial": "",
      "importedId": "cnnDESY5O2SqjlueVFbOsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172410,
      "firstName": "Anthony",
      "lastName": "Steed",
      "middleInitial": "",
      "importedId": "L7vbmDhPr9tJmz7gPBK-nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172411,
      "firstName": "Linus Nørgaard",
      "lastName": "Hollensberg",
      "middleInitial": "",
      "importedId": "JEN1SHfR2ncUncuh5J9DnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172412,
      "firstName": "Kelly",
      "lastName": "Minotti",
      "middleInitial": "",
      "importedId": "CU6xZjzXUEPbC6EVFU04bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172413,
      "firstName": "Takayoshi",
      "lastName": "Mochiduki",
      "middleInitial": "",
      "importedId": "2HtWHV12xIVpJz9IbH39Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172414,
      "firstName": "Masamitsu",
      "lastName": "Harasawa",
      "middleInitial": "",
      "importedId": "r6SeK1CPdD3rSYb7MukvDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172415,
      "firstName": "Markku",
      "lastName": "Mäkitalo",
      "middleInitial": "",
      "importedId": "UBCsEA8uH9dkukOKctNu9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172416,
      "firstName": "Diwakar",
      "lastName": "Krishnamurthy",
      "middleInitial": "",
      "importedId": "0M17D6F9vaidpvlZlS3xyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172417,
      "firstName": "Daniel",
      "lastName": "Medeiros",
      "middleInitial": "",
      "importedId": "STkQ0KbP4yrwwsBQFvw5Tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172418,
      "firstName": "Yongxin",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "3UwTfgZb3sWsHCVxMJj3QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172419,
      "firstName": "Tiare",
      "lastName": "Feuchtner",
      "middleInitial": "",
      "importedId": "JihlC4aKWjzTUU-0xSTBPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172420,
      "firstName": "Julia",
      "lastName": "Chatain",
      "middleInitial": "",
      "importedId": "37xr-ei76LxipYDa4c0Fww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172421,
      "firstName": "Guillaume",
      "lastName": "Loup",
      "middleInitial": "",
      "importedId": "rDgEBtkWSG9X44WolKikkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172422,
      "firstName": "Gerard",
      "lastName": "Kim",
      "middleInitial": "Jounghyun",
      "importedId": "ZGhcicMSEzgS5hBiS0kmNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172423,
      "firstName": "Andy",
      "lastName": "Augsten",
      "middleInitial": "",
      "importedId": "YvuziEs51aSWgagpowFTzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172424,
      "firstName": "Daniel",
      "lastName": "Zielasko",
      "middleInitial": "",
      "importedId": "A3jicF6_24m1PAN3FScFrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172425,
      "firstName": "Gustavo",
      "lastName": "Rovelo Ruiz",
      "middleInitial": "Alberto",
      "importedId": "ruSDK92__LIXRdSnxf_pKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172426,
      "firstName": "Ivan",
      "lastName": "Nikolov",
      "middleInitial": "",
      "importedId": "tgDHl2TunLX-j3j4KJqeHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172427,
      "firstName": "Emilie",
      "lastName": "Leblong",
      "middleInitial": "",
      "importedId": "TcRS7wz3TapKcju1qSw0qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172428,
      "firstName": "Stephen",
      "lastName": "Brewster",
      "middleInitial": "Anthony",
      "importedId": "5oR5tpEfy7_GSFNHxRxKQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172429,
      "firstName": "Akrivi",
      "lastName": "Katifori",
      "middleInitial": "",
      "importedId": "-32bp6hR55t65ZqxCTr1Fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172430,
      "firstName": "Hiroshi",
      "lastName": "Chigira",
      "middleInitial": "",
      "importedId": "hB1dzjgdqjRSTgOT4JxCNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172431,
      "firstName": "Ajay Shankar",
      "lastName": "Tiwari",
      "middleInitial": "",
      "importedId": "tT85rcBt6n8V_9Ws_UZ_BA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172432,
      "firstName": "Frederik",
      "lastName": "Christiansen",
      "middleInitial": "Roland",
      "importedId": "gJcb8xwB6l7zgD9mJsS9FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172433,
      "firstName": "Louise",
      "lastName": "Devigne",
      "middleInitial": "",
      "importedId": "4V0cxeLRWSBRUC-m8WwxhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172434,
      "firstName": "Robert",
      "lastName": "Lindeman",
      "middleInitial": "W.",
      "importedId": "Tbcq5CY2MSefQ7PvqrZkmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172435,
      "firstName": "Ryan",
      "lastName": "Canales",
      "middleInitial": "",
      "importedId": "6fEDNWHHKPfvWtjfv0qz6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172436,
      "firstName": "Hyemin",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "YyoVJBt3pvn8SE9VTXYQYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172437,
      "firstName": "Maud",
      "lastName": "Marchal",
      "middleInitial": "",
      "importedId": "qMER2F2ivZu1wOxuZb-8og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172438,
      "firstName": "Yvonne",
      "lastName": "Rogers",
      "middleInitial": "",
      "importedId": "1XonrPa4CxxLvJn-6wJw7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172439,
      "firstName": "Matthias",
      "lastName": "Kunz",
      "middleInitial": "",
      "importedId": "6-aymhXefONkX37JL6fT-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172440,
      "firstName": "Stéven",
      "lastName": "Picard",
      "middleInitial": "",
      "importedId": "-cnRlavLtWZz0sJ2tGNp7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172441,
      "firstName": "Sophie",
      "lastName": "Joerg",
      "middleInitial": "",
      "importedId": "pt09YVzg8_YbN0S4LMlaUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172442,
      "firstName": "Tingting",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "jsl8b9RYlt91j_Hg0E2JqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172443,
      "firstName": "Sandra",
      "lastName": "Brettschuh",
      "middleInitial": "",
      "importedId": "_Wv6uAyg0wgIeLQwtniPdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172444,
      "firstName": "Johannes",
      "lastName": "Klein",
      "middleInitial": "",
      "importedId": "egTS8u8a_1Xbbw1gaj_BKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172445,
      "firstName": "Vikram",
      "lastName": "Jaswal",
      "middleInitial": "K.",
      "importedId": "z53YHoOVOQ7G7rB1FZHshQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172446,
      "firstName": "Niamh",
      "lastName": "Healy",
      "middleInitial": "",
      "importedId": "rUA65O2N2eQOXHy14sv5NA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172447,
      "firstName": "Jeroen",
      "lastName": "Ceyssens",
      "middleInitial": "",
      "importedId": "k7DO5ZsuDodoqRghlRCDYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172493,
      "firstName": "Matthias",
      "lastName": "Hoppe",
      "middleInitial": "",
      "importedId": "Lkq0Iab9-No5KlhYyZuXNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172494,
      "firstName": "Dirk",
      "lastName": "Baker",
      "middleInitial": "Norbert",
      "importedId": "D6vpaUh4RJ5JXRMg-e8kTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172495,
      "firstName": "Konstantina",
      "lastName": "Salagianni",
      "middleInitial": "",
      "importedId": "dVW4gGmNx2mRaEKguvQW_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172496,
      "firstName": "Changkun",
      "lastName": "Ou",
      "middleInitial": "",
      "importedId": "EyP8fiBm_iH2ZxNtjh8Y9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172497,
      "firstName": "Markus",
      "lastName": "Zank",
      "middleInitial": "",
      "importedId": "maorfpDhLMOPDN9pdF4hsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172498,
      "firstName": "Christian",
      "lastName": "Merz",
      "middleInitial": "",
      "importedId": "sD6FbM0GW8B_zLvXfiRN7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172499,
      "firstName": "Lucas",
      "lastName": "Küntzer",
      "middleInitial": "",
      "importedId": "wquHwTrwNnJQ9zQcDIPBKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172500,
      "firstName": "Xander",
      "lastName": "Vaes",
      "middleInitial": "",
      "importedId": "RfFZRg5MgYa-C7lF4YW9vg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172501,
      "firstName": "Olaf",
      "lastName": "Clausen",
      "middleInitial": "",
      "importedId": "FmD8RL4eXdxeIXA9eXoA0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172502,
      "firstName": "Juliette",
      "lastName": "Vauchez",
      "middleInitial": "",
      "importedId": "fQbdS9Jubk7V_xRKfMgLbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172503,
      "firstName": "Jennifer",
      "lastName": "LaVanchy",
      "middleInitial": "",
      "importedId": "pZyaf0oANE61LRO3-wZy0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172504,
      "firstName": "Jan Niclas",
      "lastName": "Ruppenthal",
      "middleInitial": "",
      "importedId": "TcNZRQn09li2xafpY29o1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172505,
      "firstName": "Francesco",
      "lastName": "Chiossi",
      "middleInitial": "",
      "importedId": "s1Xyc1j9ixsl9NOK4VT4lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172506,
      "firstName": "Philipp",
      "lastName": "Geier",
      "middleInitial": "",
      "importedId": "AVnpHxWhjx8v_CFioEE-yA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172507,
      "firstName": "Gregor",
      "lastName": "Domes",
      "middleInitial": "",
      "importedId": "ftX1HtfjUe3GCGkQorvvJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172508,
      "firstName": "Taeyoon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "mLnvomrAdsRDtReXN9Ageg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172509,
      "firstName": "Chiara",
      "lastName": "Karr",
      "middleInitial": "",
      "importedId": "WwKQnWd88m777jZzZMa_fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172510,
      "firstName": "Daniel",
      "lastName": "Sonntag",
      "middleInitial": "",
      "importedId": "diTOlC30UBfZj0a2hESESg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172511,
      "firstName": "MJ",
      "lastName": "Johns",
      "middleInitial": "",
      "importedId": "_U5vW76VLYiEyZXVIK_XOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172512,
      "firstName": "Jonas",
      "lastName": "Linß",
      "middleInitial": "",
      "importedId": "i-yVzvkN_Mxh3hoiCyW8hQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172513,
      "firstName": "Panagiotis",
      "lastName": "Karagiannis",
      "middleInitial": "",
      "importedId": "4Hh8qgmNUnw8INg3x1jPow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172514,
      "firstName": "Jiyoung",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "791DltivbwUbV7CoTkqLcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172515,
      "firstName": "Tobias",
      "lastName": "Selzner",
      "middleInitial": "",
      "importedId": "GDbcS_2GNDMGPwXevDRiEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172516,
      "firstName": "Andrea",
      "lastName": "Giachetti",
      "middleInitial": "",
      "importedId": "JQrALpPicD36DIZtU9CONw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172517,
      "firstName": "Tuba",
      "lastName": "Kocaturk",
      "middleInitial": "",
      "importedId": "kgIB2rJxvS3pd2VpDOx_iA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172518,
      "firstName": "Robert",
      "lastName": "Spang",
      "middleInitial": "",
      "importedId": "FfzAvcmYQizfXiXPiT2zuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172519,
      "firstName": "Yu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "YVrS0K450A6FZ1rZ703aBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172520,
      "firstName": "Philipp",
      "lastName": "Graf",
      "middleInitial": "",
      "importedId": "rq1DE_xO9SAgP2H_K_afSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172521,
      "firstName": "Lone Marianne",
      "lastName": "Percy-Smith",
      "middleInitial": "",
      "importedId": "GQhU74v2kcCbMaH1VvcszA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172522,
      "firstName": "Haechan",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "omhljBlmZCftySiM8WZepA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172523,
      "firstName": "Cade",
      "lastName": "Anderson",
      "middleInitial": "W",
      "importedId": "yvyzUjHBbp6OL1lbSobJAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172524,
      "firstName": "Long",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "C4WQmRzgwBM4GsdrJIJRag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172525,
      "firstName": "Richard",
      "lastName": "Skarbez",
      "middleInitial": "",
      "importedId": "dBUk9VZkGvw84LCCAJZ46g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172526,
      "firstName": "Jan",
      "lastName": "Lässig",
      "middleInitial": "",
      "importedId": "etcedqlFNMWFrdp-Hx6kag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172527,
      "firstName": "Marta",
      "lastName": "Orduna",
      "middleInitial": "",
      "importedId": "gyQ_1e2soNntTdC4PiDbew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172528,
      "firstName": "Pablo",
      "lastName": "Perez",
      "middleInitial": "",
      "importedId": "7iLj-tDUwFvYdJTfkDLosg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172529,
      "firstName": "Stefan",
      "lastName": "Greuter",
      "middleInitial": "",
      "importedId": "2_o9MV-W1LM-u7NaYPY_tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172530,
      "firstName": "Fabian",
      "lastName": "Berger",
      "middleInitial": "",
      "importedId": "OxuByBJ2nj7EeRiP8EnY9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172531,
      "firstName": "Yvonne",
      "lastName": "Swader",
      "middleInitial": "",
      "importedId": "jdnPf33JEx0eqht0ArBk7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172532,
      "firstName": "Xianzhe",
      "lastName": "Fan",
      "middleInitial": "",
      "importedId": "N4owW_9tna49BBKYkJh8Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172533,
      "firstName": "Giancarlo",
      "lastName": "Graeber",
      "middleInitial": "",
      "importedId": "iD_CLOXsdt8hwQL889kmlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172534,
      "firstName": "Pierre",
      "lastName": "Dragicevic",
      "middleInitial": "",
      "importedId": "c6Nw1u5_eQEBxy_t4Ji1Mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172535,
      "firstName": "Waki",
      "lastName": "Youei",
      "middleInitial": "",
      "importedId": "RBT3AlYkaZ6vaZrzMxplgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172536,
      "firstName": "Sven",
      "lastName": "Karstens",
      "middleInitial": "",
      "importedId": "NpI20zV9MaQxEMZj225CcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172537,
      "firstName": "Alvaro",
      "lastName": "Villegas",
      "middleInitial": "",
      "importedId": "Ld_Fq89JVXxPX06eE2dusQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172538,
      "firstName": "Leif",
      "lastName": "Oppermann",
      "middleInitial": "",
      "importedId": "9XLM_hZy2xOilou3-RQNEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172539,
      "firstName": "Nikolaos",
      "lastName": "Tseregkounis",
      "middleInitial": "",
      "importedId": "IxJ-Vb-zeqVEMl-1maKKDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172540,
      "firstName": "Börge",
      "lastName": "Scheel",
      "middleInitial": "",
      "importedId": "Y6TgCI9Ml5KdRa0OE7rJAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172541,
      "firstName": "Sina",
      "lastName": "Hinzmann",
      "middleInitial": "Lucia",
      "importedId": "jjR-I8GHJmbLM-S8H0_Pzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172542,
      "firstName": "Gerard",
      "lastName": "Mulvany",
      "middleInitial": "T",
      "importedId": "6OZJGLye6T6r1wju2y3Q9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172543,
      "firstName": "Hironori",
      "lastName": "Ishikawa",
      "middleInitial": "",
      "importedId": "yxjcvTimIPFMoLNyBrtNJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172544,
      "firstName": "Wafaa",
      "lastName": "Wardah",
      "middleInitial": "",
      "importedId": "XYcao_Vl3Ywhf_ZfzBXbbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172545,
      "firstName": "Tanja",
      "lastName": "Kojic",
      "middleInitial": "",
      "importedId": "1-WU7hiZe6eIX8-f_FXduA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172546,
      "firstName": "Philipp",
      "lastName": "Haslbauer",
      "middleInitial": "",
      "importedId": "gRtDqdAr7vSr9OFKROETUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172547,
      "firstName": "Tony",
      "lastName": "Zoeppig",
      "middleInitial": "Jan",
      "importedId": "Lz98IZZpb0p4jVKPZqRDVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172548,
      "firstName": "Asmus Eike",
      "lastName": "Eilks",
      "middleInitial": "",
      "importedId": "Tm_VjJpV0SaCfPPmz61jDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172549,
      "firstName": "Georg",
      "lastName": "Arbesser Rastburg",
      "middleInitial": "",
      "importedId": "SPX8-EcaHN7x-i4Eb2VfQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172550,
      "firstName": "Saeed",
      "lastName": "Safikhani",
      "middleInitial": "",
      "importedId": "s4OmeBWiyV9NlOjO_r800w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172551,
      "firstName": "Jan-Niklas",
      "lastName": "Voigt-Antons",
      "middleInitial": "",
      "importedId": "LGFky6ACc5CULFsuZ_MjvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172552,
      "firstName": "Alison",
      "lastName": "Crosby",
      "middleInitial": "",
      "importedId": "Hn6i2TSQ-2GBvh8ohuqalA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172553,
      "firstName": "Ebba Þóra",
      "lastName": "Hvannberg",
      "middleInitial": "",
      "importedId": "u_kFN553fJtebTgJyLarqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172554,
      "firstName": "Stefania",
      "lastName": "Serafin",
      "middleInitial": "",
      "importedId": "Fb6oWvQNHmPnJ42G2UwQ4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172555,
      "firstName": "Agata Marta",
      "lastName": "Soccini",
      "middleInitial": "",
      "importedId": "HOeY-09ark5WxsV1eRn50w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172556,
      "firstName": "Julien",
      "lastName": "CASTET",
      "middleInitial": "",
      "importedId": "wGLeADDg1U5RsaAWG-V7RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172557,
      "firstName": "Florian",
      "lastName": "Kern",
      "middleInitial": "",
      "importedId": "M_EwKTaWn3kn7K5aDjhRFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172558,
      "firstName": "Christoph",
      "lastName": "Lürig",
      "middleInitial": "",
      "importedId": "dox9Y9tyxp9GqPjS0z96kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172559,
      "firstName": "Qing",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "azZAAhTbYlZdpQKFWR9dYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172560,
      "firstName": "Felix",
      "lastName": "Putze",
      "middleInitial": "",
      "importedId": "SzOZ6r3ubGd3oP02ggywkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172561,
      "firstName": "Arnulph",
      "lastName": "Fuhrmann",
      "middleInitial": "",
      "importedId": "2oIlNid1cNt6gBdU93w64Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172562,
      "firstName": "Tilo",
      "lastName": "Mentler",
      "middleInitial": "",
      "importedId": "ibrv0c4ZqtThMEOZ7ogVZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172563,
      "firstName": "Nathanaël",
      "lastName": "Lambert",
      "middleInitial": "",
      "importedId": "bel3I_462VKGsJpl4PQoKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172564,
      "firstName": "Menghan",
      "lastName": "Yin",
      "middleInitial": "",
      "importedId": "LhFWBSRbrLPJBSkwS14XoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172565,
      "firstName": "Tobias",
      "lastName": "Schneider",
      "middleInitial": "Sebastian",
      "importedId": "uORPciM_C8X08j5JZ2SEbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172566,
      "firstName": "Panagiotis",
      "lastName": "Aivaliotis",
      "middleInitial": "",
      "importedId": "xhjKP0t5XkJrcmYBfnECzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172567,
      "firstName": "Wakana",
      "lastName": "Oshiro",
      "middleInitial": "",
      "importedId": "AXZiVBrAgnln87kWwJIsWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172568,
      "firstName": "Yichao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HFd6DY1ilELknsYTeVuB6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172569,
      "firstName": "Anh",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "nxDwx7Mddx8IMcGAqRbyWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172570,
      "firstName": "Aljosa",
      "lastName": "Smolic",
      "middleInitial": "",
      "importedId": "bIB4ur11lfcwckHPHQSHsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172571,
      "firstName": "Linda",
      "lastName": "Hirsch",
      "middleInitial": "",
      "importedId": "pmkxcL0rjAOrRdmQEI9giw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172572,
      "firstName": "Jingjia",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "mNyDFKbAxRJsyMU2bLKSug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172573,
      "firstName": "Thuong",
      "lastName": "Hoang",
      "middleInitial": "",
      "importedId": "VWjzecbdc9-qUQWt1hiUGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172574,
      "firstName": "Lam Kit",
      "lastName": "Yung",
      "middleInitial": "",
      "importedId": "lY9vzFduUH2p8SoODSA-hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172575,
      "firstName": "Maximilian",
      "lastName": "Warsinke",
      "middleInitial": "",
      "importedId": "A1cYqJohuWctAcxB_sDIMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172576,
      "firstName": "Jisoo",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "vSy_T6ocidj0Y7997vPGww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172577,
      "firstName": "Sebastian",
      "lastName": "von Mammen",
      "middleInitial": "",
      "importedId": "oAiXV2Qi8IVsnEZNsv2e5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172578,
      "firstName": "Henry",
      "lastName": "Raymond",
      "middleInitial": "",
      "importedId": "mvk9Zuu_M8iSTbCWL9qHJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172579,
      "firstName": "Jonathan",
      "lastName": "Tschanter",
      "middleInitial": "",
      "importedId": "fvBzabggDqOEsu-hu-URvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172580,
      "firstName": "Masanori",
      "lastName": "Yokoyama",
      "middleInitial": "",
      "importedId": "PwvJSxdgFQ3VfDZI_WlE_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172581,
      "firstName": "Hanno",
      "lastName": "Scharr",
      "middleInitial": "",
      "importedId": "7Q8Mt59USCa8NfqKf5o8FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172582,
      "firstName": "Rishab",
      "lastName": "Bhattacharyya",
      "middleInitial": "",
      "importedId": "lgWxt16EwQyUi_k-0sv7Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172583,
      "firstName": "Ali",
      "lastName": "Adjorlu",
      "middleInitial": "",
      "importedId": "SRW7o1Cm5ylhVNj449V8zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172584,
      "firstName": "Martin",
      "lastName": "Weigel",
      "middleInitial": "",
      "importedId": "b7OoAkw-1tCvVIv1wraijQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172585,
      "firstName": "Maria",
      "lastName": "Madarieta",
      "middleInitial": "",
      "importedId": "usPyMOmzXinpe9LnvANypA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172586,
      "firstName": "Fabio",
      "lastName": "Zund",
      "middleInitial": "",
      "importedId": "xcVTS0meUvqgfU20y5ZcSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172587,
      "firstName": "Lucca",
      "lastName": "Troll",
      "middleInitial": "",
      "importedId": "gD1333M25N9QFdHvbwwvgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172588,
      "firstName": "Sven",
      "lastName": "Thomas",
      "middleInitial": "",
      "importedId": "0LKwTRcCh5MILNDsC-77mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172589,
      "firstName": "Anna",
      "lastName": "Kälin",
      "middleInitial": "Flurina",
      "importedId": "QT0brU3h7RcsgEDAQEVbaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172590,
      "firstName": "Steven",
      "lastName": "Lansel",
      "middleInitial": "",
      "importedId": "jF-QcG5_cZmfOvSzsJAp0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172591,
      "firstName": "Zihe",
      "lastName": "Ran",
      "middleInitial": "",
      "importedId": "FOQD23DkxsldwpBJMexGpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172592,
      "firstName": "Tetsunari",
      "lastName": "Inamura",
      "middleInitial": "",
      "importedId": "89mhPs3NhK1u6ZVBtHD82Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172593,
      "firstName": "Michael",
      "lastName": "Feldmann",
      "middleInitial": "",
      "importedId": "OZt1yT7bwR5dW2b5JfircQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172594,
      "firstName": "Martin",
      "lastName": "Mišiak",
      "middleInitial": "",
      "importedId": "Hmk85UInj3EjqN_aWeRi0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172595,
      "firstName": "Vittorio",
      "lastName": "Fiscale",
      "middleInitial": "",
      "importedId": "d10rM1ghvWCllxFE91-Jkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172596,
      "firstName": "Victoria",
      "lastName": "Duckett",
      "middleInitial": "",
      "importedId": "8swNK52DQstYhkjPRyu8Rw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172597,
      "firstName": "Kevin",
      "lastName": "Linne",
      "middleInitial": "",
      "importedId": "07Aa45K86_IKxQ91s9YWCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172598,
      "firstName": "Mannu",
      "lastName": "Lambrichts",
      "middleInitial": "",
      "importedId": "9yzBEbnALKMvHTb2eZHQKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172599,
      "firstName": "Marco",
      "lastName": "Emporio",
      "middleInitial": "",
      "importedId": "yGtfI78DTA9ZPB7Ou6BnFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172600,
      "firstName": "Lorenzo",
      "lastName": "Genghini",
      "middleInitial": "",
      "importedId": "rx5vEas1ecl5n0LGz44tuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172601,
      "firstName": "Dries",
      "lastName": "Cardinaels",
      "middleInitial": "",
      "importedId": "KEIegCdJmfVEFuJRDB3M5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172602,
      "firstName": "Mary",
      "lastName": "Whitton",
      "middleInitial": "C.",
      "importedId": "AjaZVP5cQw5qorgwuaOMqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172603,
      "firstName": "Hiroyuki",
      "lastName": "Manabe",
      "middleInitial": "",
      "importedId": "GPbdshGJqVFsnL-tpVbWZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172604,
      "firstName": "Jaewan",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "ksu9MtFjcSZ-mPwDrShPng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172605,
      "firstName": "Anna",
      "lastName": "Schreuer",
      "middleInitial": "",
      "importedId": "CEOtkfHQU4fOjaSkyD3ngQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172606,
      "firstName": "Matthew",
      "lastName": "Lakier",
      "middleInitial": "",
      "importedId": "ax3DrnNbuHVWM_EcPCrwfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172607,
      "firstName": "Deborah",
      "lastName": "Pintani",
      "middleInitial": "",
      "importedId": "FxOeOgXWl3LNkoaRf9orLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172608,
      "firstName": "Ayca",
      "lastName": "Takmaz",
      "middleInitial": "",
      "importedId": "MJ0Tiq1cqO4Ud9pooI35Gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172609,
      "firstName": "Yuen",
      "lastName": "Law",
      "middleInitial": "C.",
      "importedId": "XmTjN-hNbqYO8F-NRUedOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172610,
      "firstName": "Sotiris",
      "lastName": "Makris",
      "middleInitial": "",
      "importedId": "-iB-RKZB1wLXhZfCDltuGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172611,
      "firstName": "Morris",
      "lastName": "Riedel",
      "middleInitial": "",
      "importedId": "d4sVRfqmXmgUJ7eLry2jDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172612,
      "firstName": "Philipp",
      "lastName": "Wintersberger",
      "middleInitial": "",
      "importedId": "ppY-s6wNfRIQCeIlfKoDCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172613,
      "firstName": "Christian",
      "lastName": "Hayes",
      "middleInitial": "John Dyson",
      "importedId": "0wyjKWghfZmNBtV95v_5DQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172614,
      "firstName": "Navid",
      "lastName": "Ashrafi",
      "middleInitial": "",
      "importedId": "2SGoXCAoMdsIwcxNj1Uf2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172615,
      "firstName": "Andoni",
      "lastName": "Rivera-Pinto",
      "middleInitial": "",
      "importedId": "idTfXCe8LULvrpuiO9gquA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172616,
      "firstName": "Zilu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "M894k3gZgDwroapvaXOcYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172617,
      "firstName": "Henning",
      "lastName": "Metzmacher",
      "middleInitial": "",
      "importedId": "1Iqb_ZZAzeyKsIGBN0RyBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172618,
      "firstName": "Uijong",
      "lastName": "Ju",
      "middleInitial": "",
      "importedId": "b5-Q0Cyai1LelDdDawhaYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172619,
      "firstName": "Haruno",
      "lastName": "Kataoka",
      "middleInitial": "",
      "importedId": "F1HhAIPpBqTh7I9unqx2Ug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172620,
      "firstName": "Youn-Hee",
      "lastName": "Gil",
      "middleInitial": "",
      "importedId": "fPhXPQXUq49Lv5pFGsDmyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172621,
      "firstName": "Michaela",
      "lastName": "Pnacekova",
      "middleInitial": "",
      "importedId": "2_q_AT9lnxmMOABMmPDAXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172622,
      "firstName": "Seongmin",
      "lastName": "Baek",
      "middleInitial": "",
      "importedId": "RCG1ajdxKvPMk4htzD6aUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172623,
      "firstName": "Ariel",
      "lastName": "Caputo",
      "middleInitial": "",
      "importedId": "bVR28WPW_4yXO2ng8UVXsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172624,
      "firstName": "Michael",
      "lastName": "Schreiner",
      "middleInitial": "",
      "importedId": "E3WZej98wbLl0yEEc_QX1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172625,
      "firstName": "Yvonne",
      "lastName": "Jung",
      "middleInitial": "A",
      "importedId": "uDt3DOjT8dX80fTfU8Parg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172626,
      "firstName": "Andreas",
      "lastName": "Dietze",
      "middleInitial": "",
      "importedId": "eIf9yqXMnyU_fWyUscD5Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172627,
      "firstName": "Paul",
      "lastName": "Grimm",
      "middleInitial": "",
      "importedId": "kLsd7tsOWNUYZfSnbgjwVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172628,
      "firstName": "Ephraim",
      "lastName": "Schott",
      "middleInitial": "",
      "importedId": "OHQjqHmnpbVTcn4aai6Kcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172629,
      "firstName": "Milana",
      "lastName": "Wolff",
      "middleInitial": "",
      "importedId": "Tv0FlGTZv3pBhu0zSWp5Iw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172630,
      "firstName": "Jens Henrik",
      "lastName": "Göbbert",
      "middleInitial": "",
      "importedId": "HHjA__blwBPgHbCFoE5Cyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172631,
      "firstName": "Hessam",
      "lastName": "Djavaherpour",
      "middleInitial": "",
      "importedId": "Bhz7VDNNeXOuBN24oaKFFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172632,
      "firstName": "Nicola",
      "lastName": "Tomasoni",
      "middleInitial": "",
      "importedId": "sD2BITYDd1o8Pr4uNQ7bYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172633,
      "firstName": "Denis",
      "lastName": "Gracanin",
      "middleInitial": "",
      "importedId": "NiSHY7RvpTRiwkCLlA-qlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172634,
      "firstName": "Jesús",
      "lastName": "Rosel",
      "middleInitial": "",
      "importedId": "90S1kuCFskgyYIeVnexmpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172635,
      "firstName": "Syed Masum",
      "lastName": "Billah",
      "middleInitial": "",
      "importedId": "fk_oHW4xdx90gCXhWb40Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172636,
      "firstName": "Rongyi",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "dC5LGwiEO5qAaOKRhi1hxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172637,
      "firstName": "Anton",
      "lastName": "Lammert",
      "middleInitial": "Benjamin",
      "importedId": "4L4-HGv2afqJmlVhpq63oQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172638,
      "firstName": "Majid",
      "lastName": "Behravan",
      "middleInitial": "",
      "importedId": "TdT4NKzFaie9HI3vW5pmww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172639,
      "firstName": "Matthias",
      "lastName": "Wölwer",
      "middleInitial": "",
      "importedId": "r7ch8Og_GK2d9SSYObxs5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172640,
      "firstName": "Juan",
      "lastName": "Garcia Cano",
      "middleInitial": "Camilo",
      "importedId": "2FU_C9S8V1wxgJnxtU_ZFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172641,
      "firstName": "Yusuke",
      "lastName": "Goutsu",
      "middleInitial": "",
      "importedId": "eH8lW2wQ5g-NUI6YeA8Qlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172642,
      "firstName": "Manuel",
      "lastName": "Hartmann",
      "middleInitial": "",
      "importedId": "AvP-fEJcg4WsBobgD3rewA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172643,
      "firstName": "Daniel",
      "lastName": "Pohl",
      "middleInitial": "",
      "importedId": "JTiOiAwcqXsWmMw-nY0AsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172644,
      "firstName": "Yejin",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "NZ0eugd5jRDyDVGyehp0MQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172645,
      "firstName": "László",
      "lastName": "Kopácsi",
      "middleInitial": "",
      "importedId": "CTXU2FnWF-xpeKk-TP0Q6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172646,
      "firstName": "Gerd",
      "lastName": "Bruder",
      "middleInitial": "",
      "importedId": "tZQ7gLt4C6wd0tuTsCuv8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172647,
      "firstName": "Zheyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "sxTuje_bPZd1ObARMjBkRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172648,
      "firstName": "Andreas",
      "lastName": "Kunz",
      "middleInitial": "",
      "importedId": "9hMGyKbo35yaW_xlm4rOmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172649,
      "firstName": "Michelle",
      "lastName": "Meyer",
      "middleInitial": "",
      "importedId": "OwCuUWaSbZvREvUltN1cew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172650,
      "firstName": "Dohoon",
      "lastName": "Kwak",
      "middleInitial": "",
      "importedId": "M1WvrsGkkeOU8b3UaLV7NA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172651,
      "firstName": "Ricardo",
      "lastName": "Marroquim",
      "middleInitial": "",
      "importedId": "0gm2LebRZPGL-x_fcQtP-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172652,
      "firstName": "Christian",
      "lastName": "Tsalidis",
      "middleInitial": "",
      "importedId": "IOwqKjC4ouu3NwZOOStihg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172653,
      "firstName": "Georg",
      "lastName": "Rock",
      "middleInitial": "",
      "importedId": "PdII3eUur-8ehVSYgZq-rQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172654,
      "firstName": "Charles",
      "lastName": "Bailly",
      "middleInitial": "",
      "importedId": "YQHw74CgXp2t9Eb1u9_jPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172655,
      "firstName": "Judit Ruiz",
      "lastName": "de Munain",
      "middleInitial": "",
      "importedId": "CD_DR7CbqiYDDjFDg4uKQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172656,
      "firstName": "Yongjae",
      "lastName": "Yoo",
      "middleInitial": "",
      "importedId": "WZ0U-W1x8-8OYVoY5Wy_3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172657,
      "firstName": "Guodong",
      "lastName": "Rong",
      "middleInitial": "",
      "importedId": "JL7DMOZpZ_7GibMnf4owLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172658,
      "firstName": "Tanja",
      "lastName": "Schultz",
      "middleInitial": "",
      "importedId": "9Sf23RtlxWlYSTA1PxgIKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172659,
      "firstName": "Bernd",
      "lastName": "Froehlich",
      "middleInitial": "",
      "importedId": "ZrSKlpvVHOHGINAPpsT0bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172660,
      "firstName": "Byeongwon",
      "lastName": "Ha",
      "middleInitial": "",
      "importedId": "JOcJfjgDaYkrmyc_A2R--A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172661,
      "firstName": "Kamil",
      "lastName": "Koniuch",
      "middleInitial": "",
      "importedId": "EyCnKUvD6ZhqCNaGqwH40g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172662,
      "firstName": "Dong Seon",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "vSHOSdiCH_kiVJ6aUA5Oeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172663,
      "firstName": "Yvonne",
      "lastName": "Jansen",
      "middleInitial": "",
      "importedId": "EPJ11CyJqjpb_sYzE_dS1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172664,
      "firstName": "Ryoto",
      "lastName": "Nohara",
      "middleInitial": "",
      "importedId": "szhs0Bfpivu-gD8QVVdVyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172665,
      "firstName": "Michael",
      "lastName": "Barz",
      "middleInitial": "",
      "importedId": "WsChlJFPL7ZlsvnGI05r6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172666,
      "firstName": "Jürgen",
      "lastName": "Suschek-Berger",
      "middleInitial": "",
      "importedId": "35HztnDIv9K_znBRG-n7EQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172667,
      "firstName": "Sebastian",
      "lastName": "Möller",
      "middleInitial": "",
      "importedId": "aVPcW2X1L25rJVthTvrOUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172668,
      "firstName": "youngho",
      "lastName": "chai",
      "middleInitial": "",
      "importedId": "jaK8O325tbNPlIDmK6Bevw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172669,
      "firstName": "Ryuji",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "importedId": "YDBJTX3sDphjrGpFDuzxVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172670,
      "firstName": "Ariana",
      "lastName": "Huwiler",
      "middleInitial": "",
      "importedId": "JxzPMiljHH6JYL6awUX1Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172671,
      "firstName": "Thomas",
      "lastName": "Stettler",
      "middleInitial": "",
      "importedId": "W9Sxk8nD1uEGKt7bUV4ZSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172672,
      "firstName": "Yuchen",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "lHmM7CfnWZK42sjNoC-jNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172673,
      "firstName": "Haruka",
      "lastName": "Murakami",
      "middleInitial": "",
      "importedId": "Quv31E7fOupqKlfsES8V_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172674,
      "firstName": "Raf",
      "lastName": "Ramakers",
      "middleInitial": "",
      "importedId": "D5UPmdoyrXN_yI0FHEzJgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172675,
      "firstName": "Johan",
      "lastName": "Kildal",
      "middleInitial": "",
      "importedId": "fGN-iKlPfaVI2bbDF-E6cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172676,
      "firstName": "Dennis",
      "lastName": "Dietz",
      "middleInitial": "",
      "importedId": "vgxLIoAibzGRpJdX-ltEUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172677,
      "firstName": "Hermann",
      "lastName": "Edtmayer",
      "middleInitial": "",
      "importedId": "Dj5MjJS9uscWYHQIKwEzlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172678,
      "firstName": "Stephanie",
      "lastName": "May",
      "middleInitial": "",
      "importedId": "ru3avjZS1Jy9kgm67f-Gpw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172679,
      "firstName": "Vanessa",
      "lastName": "Pfeiffer",
      "middleInitial": "",
      "importedId": "hHJxqZK7GU9pmJq9exPtZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172680,
      "firstName": "Nobuchika",
      "lastName": "Sakata",
      "middleInitial": "",
      "importedId": "fRWn6un6lsvJ2x0kUv_8HA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172681,
      "firstName": "Andrea",
      "lastName": "Schnepf",
      "middleInitial": "",
      "importedId": "tbc8mncfcnWnnLM9SHZgBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172682,
      "firstName": "Philipp",
      "lastName": "Harnisch",
      "middleInitial": "Lars",
      "importedId": "Bjai5VFjl1dZiEV_JS3bJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172683,
      "firstName": "Jakob",
      "lastName": "Eckkrammer",
      "middleInitial": "",
      "importedId": "JA3Wu7wERKio8fQom8IYYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172684,
      "firstName": "Francesco",
      "lastName": "Vona",
      "middleInitial": "",
      "importedId": "BYRHTYyFGr0VAE7az0Fx3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172685,
      "firstName": "Gareth",
      "lastName": "Young",
      "middleInitial": "W.",
      "importedId": "2dESPWlxLdKPLgHNVCbdmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172686,
      "firstName": "Irene",
      "lastName": "López García",
      "middleInitial": "",
      "importedId": "UC_hqHNNA8merf78uDbM6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172687,
      "firstName": "Doug",
      "lastName": "Bowman",
      "middleInitial": "",
      "importedId": "aXmKyN3zd9ZRPIPNSf6HHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172688,
      "firstName": "Ester",
      "lastName": "Gonzalez-Sosa",
      "middleInitial": "",
      "importedId": "iTMyUMqTqEybLGWBQYtLAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172689,
      "firstName": "Andreas",
      "lastName": "Butz",
      "middleInitial": "Martin",
      "importedId": "CWYpLecThpHr6-Hh2dxUZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172690,
      "firstName": "Ahmed Seyit",
      "lastName": "Kücük",
      "middleInitial": "",
      "importedId": "9AhWwhWKM9JTs48Er_bq3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172691,
      "firstName": "JINWOO",
      "lastName": "JEONG",
      "middleInitial": "",
      "importedId": "zZfPDOKkKALOFHY-D9VezA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172692,
      "firstName": "Maurizio",
      "lastName": "Vergari",
      "middleInitial": "",
      "importedId": "_9_JAuBo70txmtZUZpOzhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172693,
      "firstName": "Amy",
      "lastName": "Banic",
      "middleInitial": "",
      "importedId": "WfcTgU0rCsIl_NFxumj8xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 172765,
      "firstName": "Maude",
      "lastName": "Marchal",
      "importedId": "1",
      "source": "CSV",
      "affiliations": [
        {
          "country": "France",
          "institution": "University Rennes"
        },
        {
          "country": "France",
          "institution": "INSA/IRISA"
        }
      ]
    },
    {
      "id": 172766,
      "firstName": "Brian",
      "lastName": "Nosek",
      "importedId": "2",
      "source": "CSV",
      "affiliations": [
        {
          "country": "USA",
          "institution": "University of Virginia"
        }
      ]
    }
  ],
  "recognitions": []
}