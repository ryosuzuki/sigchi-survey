{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10087,
    "shortName": "IMX",
    "year": 2023,
    "startDate": 1686528000000,
    "endDate": 1686787200000,
    "fullName": "Interactive Media Experiences",
    "url": "https://imx.acm.org/2023/",
    "location": "Nantes, France",
    "timeZoneOffset": 120,
    "timeZoneName": "Europe/Paris",
    "logoUrl": "https://files.sigchi.org/conference/logo/10087/c8054544-f017-bfec-1e64-0d3c8e4bfd7f.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10087/abb6f917-a403-b077-fa1d-90329e51cdb6.html",
    "name": "IMX 2023"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 22,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": true,
    "publicationDate": "2023-06-14 09:52:06+00"
  },
  "sponsors": [
    {
      "id": 10365,
      "name": "InterDigital",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/cbbf76f8-cc96-7586-79b1-4789af075fa1.png",
      "levelId": 10222,
      "url": "https://www.interdigital.com/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10375,
      "name": "Meta",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/d5ad3bd0-6798-fb4c-b5fb-2b8d3b3e30f8.png",
      "levelId": 10220,
      "url": "https://about.meta.com/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10376,
      "name": "YouTube",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/f59cd4c5-9ca1-81f7-f51c-34985134d722.png",
      "levelId": 10220,
      "url": "https://www.youtube.com/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10377,
      "name": "HEMI+FAME",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/1753afd4-7f5e-56cd-de58-cc5716bb9d6d.png",
      "levelId": 10222,
      "url": "https://next-isite.fr/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10378,
      "name": "Jellysmack",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/ee12074e-f80f-a0b3-501d-8dd94de57045.png",
      "levelId": 10222,
      "url": "https://jellysmack.com/",
      "order": 2,
      "extraPadding": 0
    },
    {
      "id": 10379,
      "name": "Tobii",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10087/logo/b9c2cf6e-1513-77da-a392-16a8c11b3acf.png",
      "levelId": 10231,
      "url": "https://www.tobii.com/",
      "order": 0,
      "extraPadding": 0
    }
  ],
  "sponsorLevels": [
    {
      "id": 10195,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    },
    {
      "id": 10220,
      "name": "Platinum sponsor",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10222,
      "name": "Silver sponsor",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10223,
      "name": "Bronze sponsor",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10231,
      "name": "Gold sponsor",
      "rank": 2,
      "isDefault": false
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 11128,
      "name": "109",
      "setup": "Classroom",
      "typeId": 12522
    },
    {
      "id": 11129,
      "name": "208",
      "setup": "Classroom",
      "typeId": 12522
    },
    {
      "id": 11130,
      "name": "206",
      "setup": "Classroom",
      "typeId": 12522
    },
    {
      "id": 11131,
      "name": "207",
      "setup": "Classroom",
      "typeId": 12522
    },
    {
      "id": 11132,
      "name": "105",
      "setup": "Classroom",
      "typeId": 12522
    },
    {
      "id": 11133,
      "name": "108",
      "setup": "Classroom",
      "typeId": 12514
    },
    {
      "id": 11134,
      "name": "XL",
      "setup": "Special",
      "typeId": 12515
    },
    {
      "id": 11135,
      "name": "Agora",
      "setup": "Theatre",
      "typeId": 12519
    }
  ],
  "tracks": [
    {
      "id": 12236,
      "name": "IMX 2022 IMX-in-Industry",
      "typeId": 12884
    },
    {
      "id": 12237,
      "name": "IMX 2023 Demos",
      "typeId": 12515
    },
    {
      "id": 12238,
      "name": "IMX 2022 Doctoral Consortium",
      "typeId": 12884
    },
    {
      "id": 12239,
      "name": "IMX 2023 Technical Papers",
      "typeId": 12519
    },
    {
      "id": 12240,
      "name": "IMX 2023 Work-in-Progress",
      "typeId": 12521
    },
    {
      "id": 12241,
      "name": "IMX 2023 IMX-in-Industry",
      "typeId": 12864
    },
    {
      "id": 12242,
      "name": "IMX 2022 Work-in-Progress",
      "typeId": 12884
    },
    {
      "id": 12243,
      "name": "IMX 2022 Demos",
      "typeId": 12884
    },
    {
      "id": 12244,
      "name": "IMX 2023 Doctoral Symposium",
      "typeId": 12516
    }
  ],
  "contentTypes": [
    {
      "id": 12514,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12515,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 5,
      "displayName": "Demos"
    },
    {
      "id": 12516,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 12517,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12518,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 12519,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 12520,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 12521,
      "name": "Work-in-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 12522,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12523,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 12864,
      "name": "Industrial talk",
      "color": "#969696",
      "duration": 10
    },
    {
      "id": 12884,
      "name": "2022",
      "color": "#ff99ca",
      "duration": 0
    },
    {
      "id": 12885,
      "name": "Registration",
      "color": "#ff99ca",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 13083,
      "type": "SESSION",
      "startDate": 1686645000000,
      "endDate": 1686646800000
    },
    {
      "id": 13084,
      "type": "SESSION",
      "startDate": 1686646800000,
      "endDate": 1686647700000
    },
    {
      "id": 13085,
      "type": "SESSION",
      "startDate": 1686647700000,
      "endDate": 1686651300000
    },
    {
      "id": 13086,
      "type": "SESSION",
      "startDate": 1686651300000,
      "endDate": 1686652200000
    },
    {
      "id": 13087,
      "type": "BREAK",
      "startDate": 1686652200000,
      "endDate": 1686654000000
    },
    {
      "id": 13088,
      "type": "SESSION",
      "startDate": 1686654000000,
      "endDate": 1686654900000
    },
    {
      "id": 13089,
      "type": "SESSION",
      "startDate": 1686654900000,
      "endDate": 1686658500000
    },
    {
      "id": 13090,
      "type": "SESSION",
      "startDate": 1686658500000,
      "endDate": 1686659400000
    },
    {
      "id": 13091,
      "type": "LUNCH",
      "startDate": 1686659400000,
      "endDate": 1686663900000
    },
    {
      "id": 13092,
      "type": "SESSION",
      "startDate": 1686663900000,
      "endDate": 1686666600000
    },
    {
      "id": 13093,
      "type": "SESSION",
      "startDate": 1686666600000,
      "endDate": 1686667500000
    },
    {
      "id": 13094,
      "type": "SESSION",
      "startDate": 1686667500000,
      "endDate": 1686670200000
    },
    {
      "id": 13095,
      "type": "BREAK",
      "startDate": 1686670200000,
      "endDate": 1686672000000
    },
    {
      "id": 13096,
      "type": "SESSION",
      "startDate": 1686560400000,
      "endDate": 1686573000000
    },
    {
      "id": 13097,
      "type": "LUNCH",
      "startDate": 1686573000000,
      "endDate": 1686578400000
    },
    {
      "id": 13098,
      "type": "SESSION",
      "startDate": 1686578400000,
      "endDate": 1686592800000
    },
    {
      "id": 13099,
      "type": "SESSION",
      "startDate": 1686594600000,
      "endDate": 1686605400000
    },
    {
      "id": 13100,
      "type": "SESSION",
      "startDate": 1686672000000,
      "endDate": 1686672900000
    },
    {
      "id": 13101,
      "type": "SESSION",
      "startDate": 1686672900000,
      "endDate": 1686675600000
    },
    {
      "id": 13102,
      "type": "SESSION",
      "startDate": 1686678300000,
      "endDate": 1686682800000
    },
    {
      "id": 13103,
      "type": "SESSION",
      "startDate": 1686682800000,
      "endDate": 1686695400000
    },
    {
      "id": 13104,
      "type": "SESSION",
      "startDate": 1686731400000,
      "endDate": 1686733200000
    },
    {
      "id": 13105,
      "type": "SESSION",
      "startDate": 1686734100000,
      "endDate": 1686735000000
    },
    {
      "id": 13106,
      "type": "SESSION",
      "startDate": 1686735000000,
      "endDate": 1686738600000
    },
    {
      "id": 13107,
      "type": "SESSION",
      "startDate": 1686738600000,
      "endDate": 1686739500000
    },
    {
      "id": 13108,
      "type": "BREAK",
      "startDate": 1686739500000,
      "endDate": 1686741300000
    },
    {
      "id": 13109,
      "type": "SESSION",
      "startDate": 1686741300000,
      "endDate": 1686742200000
    },
    {
      "id": 13110,
      "type": "SESSION",
      "startDate": 1686742200000,
      "endDate": 1686745800000
    },
    {
      "id": 13111,
      "type": "SESSION",
      "startDate": 1686745800000,
      "endDate": 1686746700000
    },
    {
      "id": 13112,
      "type": "LUNCH",
      "startDate": 1686746700000,
      "endDate": 1686751200000
    },
    {
      "id": 13113,
      "type": "SESSION",
      "startDate": 1686751200000,
      "endDate": 1686755700000
    },
    {
      "id": 13114,
      "type": "SESSION",
      "startDate": 1686755700000,
      "endDate": 1686757500000
    },
    {
      "id": 13115,
      "type": "BREAK",
      "startDate": 1686757500000,
      "endDate": 1686759300000
    },
    {
      "id": 13116,
      "type": "SESSION",
      "startDate": 1686759300000,
      "endDate": 1686760200000
    },
    {
      "id": 13117,
      "type": "SESSION",
      "startDate": 1686760200000,
      "endDate": 1686762900000
    },
    {
      "id": 13118,
      "type": "SESSION",
      "startDate": 1686762900000,
      "endDate": 1686764700000
    },
    {
      "id": 13119,
      "type": "SESSION",
      "startDate": 1686825000000,
      "endDate": 1686830400000
    },
    {
      "id": 13120,
      "type": "SESSION",
      "startDate": 1686558600000,
      "endDate": 1686560400000
    },
    {
      "id": 13121,
      "type": "SESSION",
      "startDate": 1686643200000,
      "endDate": 1686646800000
    },
    {
      "id": 13122,
      "type": "SESSION",
      "startDate": 1686729600000,
      "endDate": 1686733200000
    },
    {
      "id": 13123,
      "type": "SESSION",
      "startDate": 1686764700000,
      "endDate": 1686766500000
    }
  ],
  "sessions": [
    {
      "id": 117010,
      "name": "Oral session 4",
      "isParallelPresentation": false,
      "importedId": "a14bc3fc-cd97-4a24-8c4f-5741d0e57134",
      "typeId": 12519,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [
        117075,
        117065,
        117070
      ],
      "source": "SYS",
      "timeSlotId": 13110
    },
    {
      "id": 117283,
      "name": "Oral session 1",
      "isParallelPresentation": false,
      "importedId": "f1b839e7-d01f-4b98-a008-048b33032aa7",
      "typeId": 12519,
      "roomId": 11135,
      "chairIds": [
        116924
      ],
      "contentIds": [
        117067,
        117074,
        117066
      ],
      "source": "SYS",
      "timeSlotId": 13085
    },
    {
      "id": 117284,
      "name": "Oral session 3",
      "isParallelPresentation": false,
      "importedId": "533370a6-50e2-480e-9d4b-5b858e55cf08",
      "typeId": 12519,
      "roomId": 11135,
      "chairIds": [
        117090
      ],
      "contentIds": [
        117076,
        117063,
        117071
      ],
      "source": "SYS",
      "timeSlotId": 13106
    },
    {
      "id": 117285,
      "name": "Oral session 2",
      "isParallelPresentation": false,
      "importedId": "c6b041e8-118c-470b-a93c-1f03ff7394fa",
      "typeId": 12519,
      "roomId": 11135,
      "chairIds": [
        116890
      ],
      "contentIds": [
        117068,
        117073,
        117069
      ],
      "source": "SYS",
      "timeSlotId": 13089
    },
    {
      "id": 117286,
      "name": "Remote session",
      "isParallelPresentation": false,
      "importedId": "f2ca0b01-3b7c-453a-b7b6-470473015e9e",
      "typeId": 12519,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [
        117072,
        117064,
        117232,
        117259,
        117248
      ],
      "source": "SYS",
      "timeSlotId": 13113
    },
    {
      "id": 117287,
      "name": "DC posters 1",
      "isParallelPresentation": false,
      "importedId": "50a7362f-ee35-4017-a860-48393f1e18a1",
      "typeId": 12516,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [
        117238,
        117247,
        117252,
        117255,
        117257,
        117246,
        117236
      ],
      "source": "SYS",
      "timeSlotId": 13087
    },
    {
      "id": 117288,
      "name": "Demo 1 (105)",
      "isParallelPresentation": false,
      "importedId": "4b6fc0f3-49c9-43ee-a361-9c94e3bb63ee",
      "typeId": 12515,
      "roomId": 11132,
      "chairIds": [
        117125
      ],
      "contentIds": [
        117258,
        117270,
        117263
      ],
      "source": "SYS",
      "timeSlotId": 13095
    },
    {
      "id": 117289,
      "name": "Demo 2 (105)",
      "isParallelPresentation": false,
      "importedId": "69985277-ea37-49bd-8917-962ab9b2d993",
      "typeId": 12515,
      "roomId": 11132,
      "chairIds": [
        117125
      ],
      "contentIds": [
        117282,
        117266
      ],
      "source": "SYS",
      "timeSlotId": 13115
    },
    {
      "id": 117290,
      "name": "DC posters 2",
      "isParallelPresentation": false,
      "importedId": "7e10c644-01c2-4487-b661-0a69d9713864",
      "typeId": 12516,
      "chairIds": [],
      "contentIds": [
        117264,
        117262,
        117256,
        117245,
        117253,
        117260,
        117249,
        117251
      ],
      "source": "SYS",
      "timeSlotId": 13108
    },
    {
      "id": 117291,
      "name": "WiP posters 1 (109)",
      "isParallelPresentation": false,
      "importedId": "82c4e873-49de-466d-9a02-f4a70a932bc0",
      "typeId": 12521,
      "roomId": 11128,
      "chairIds": [
        117084
      ],
      "contentIds": [
        117231,
        117227,
        117228,
        117277,
        117250,
        117276,
        117275
      ],
      "source": "SYS",
      "timeSlotId": 13094
    },
    {
      "id": 117292,
      "name": "WiP posters 2 (109)",
      "isParallelPresentation": false,
      "importedId": "be47ad01-fc4e-4356-ad67-2304f25ed2b1",
      "typeId": 12521,
      "roomId": 11128,
      "chairIds": [
        117084
      ],
      "contentIds": [
        117244,
        117271,
        117267,
        117226,
        117281,
        117234,
        117269,
        117278
      ],
      "source": "SYS",
      "timeSlotId": 13101
    },
    {
      "id": 117293,
      "name": "WiP posters 3 (109)",
      "isParallelPresentation": false,
      "importedId": "96404a6f-9695-48d4-a6e8-27cb1fe5ef21",
      "typeId": 12521,
      "roomId": 11133,
      "chairIds": [
        117084
      ],
      "contentIds": [
        117237,
        117265,
        117254,
        117279,
        117235,
        117230,
        117229,
        117240,
        117243
      ],
      "source": "SYS",
      "timeSlotId": 13117
    },
    {
      "id": 117294,
      "name": "Industrial talk 1",
      "isParallelPresentation": false,
      "importedId": "cd9ae997-ab4f-4d3c-bc09-fb5595d8e0c7",
      "typeId": 12864,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [
        117242
      ],
      "source": "SYS",
      "timeSlotId": 13088
    },
    {
      "id": 117295,
      "name": "Industrial talk 2",
      "isParallelPresentation": false,
      "importedId": "0af7c183-69dd-4ece-b2b0-01beecf5e8e0",
      "typeId": 12864,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [
        117241
      ],
      "source": "SYS",
      "timeSlotId": 13109
    },
    {
      "id": 117296,
      "name": "Keynote",
      "isParallelPresentation": false,
      "importedId": "38ec8d69-80b8-4ebb-b8ea-b315e63a69d1",
      "typeId": 12514,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13092
    },
    {
      "id": 117297,
      "name": "Demo pitch 2",
      "isParallelPresentation": false,
      "importedId": "388a9674-8c0b-4e25-a37e-664ee4485578",
      "typeId": 12515,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13111
    },
    {
      "id": 117298,
      "name": "Sponsor pitch: Meta",
      "isParallelPresentation": false,
      "importedId": "74238775-d7d6-4e2a-9eed-2aef4386f092",
      "typeId": 12514,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13114
    },
    {
      "id": 117299,
      "name": "Lunch 1",
      "isParallelPresentation": false,
      "importedId": "39826214-be2d-4382-995e-92210b00ca3a",
      "typeId": 12523,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13091
    },
    {
      "id": 117300,
      "name": "Lunch 2",
      "isParallelPresentation": false,
      "importedId": "daa33a71-d10b-4bde-892f-3fcb74feb372",
      "typeId": 12523,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13112
    },
    {
      "id": 117301,
      "name": "Awards + IMX 2024 announcement + closure",
      "isParallelPresentation": false,
      "importedId": "119c716c-24cf-4bf2-bec0-27aa21cc5b09",
      "typeId": 12517,
      "roomId": 11135,
      "chairIds": [
        117090,
        117105,
        116865,
        116890
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13118
    },
    {
      "id": 117302,
      "name": "Welcome reception",
      "isParallelPresentation": false,
      "importedId": "9e12af7b-74ad-4011-9113-84f467b17206",
      "typeId": 12517,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13099
    },
    {
      "id": 117303,
      "name": "Social event",
      "isParallelPresentation": false,
      "importedId": "fc1bc4c4-d740-4f99-b979-011fbdf8bc32",
      "typeId": 12517,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13103
    },
    {
      "id": 117304,
      "name": "Workshop - Dancing with Technology",
      "isParallelPresentation": true,
      "importedId": "a8980355-7c01-4145-93c6-82dd1733aaae",
      "typeId": 12522,
      "roomId": 11129,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13096
    },
    {
      "id": 117305,
      "name": "Workshop - SensoryX",
      "isParallelPresentation": true,
      "importedId": "6af6e0c6-de6b-49a8-9a8f-7f9568de3132",
      "typeId": 12522,
      "roomId": 11130,
      "chairIds": [
        116874,
        116865
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13096
    },
    {
      "id": 117306,
      "name": "Workshop - WICED x Cinemotions",
      "isParallelPresentation": true,
      "importedId": "ee8813b8-5f28-48c9-b4db-93fac6f1f1a3",
      "typeId": 12522,
      "roomId": 11131,
      "chairIds": [
        117046,
        117042,
        117039
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13096
    },
    {
      "id": 117307,
      "name": "Workshop - Care IMX",
      "isParallelPresentation": true,
      "importedId": "5ba8560c-8ce9-490d-8282-5696f911404b",
      "typeId": 12522,
      "roomId": 11128,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13098
    },
    {
      "id": 117308,
      "name": "Workshop - Dancing with Technology",
      "isParallelPresentation": true,
      "importedId": "524cf21f-9256-46f0-aaba-3b98b07ae3be",
      "typeId": 12522,
      "roomId": 11129,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13098
    },
    {
      "id": 117309,
      "name": "Workshop - LIQUE",
      "isParallelPresentation": true,
      "importedId": "c7fe779b-bb22-4f9e-98cc-efb410bf2a42",
      "typeId": 12522,
      "roomId": 11132,
      "chairIds": [
        116865
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13098
    },
    {
      "id": 117310,
      "name": "Workshop - SensoryX",
      "isParallelPresentation": true,
      "importedId": "c6776a93-b005-41ff-a622-3c61f2a82589",
      "typeId": 12522,
      "roomId": 11130,
      "chairIds": [
        116865,
        116874
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13098
    },
    {
      "id": 117311,
      "name": "Workshop - WICED x Cinemotions",
      "isParallelPresentation": true,
      "importedId": "b19ff4a8-4f61-4379-8474-d870ec4098a9",
      "typeId": 12522,
      "roomId": 11131,
      "chairIds": [
        117046,
        117042,
        117039
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13098
    },
    {
      "id": 117312,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "2ac386d5-a40e-4218-b3ee-3252e2aa99fb",
      "typeId": 12885,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13120
    },
    {
      "id": 117313,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "4b623070-d85a-4e2b-a6c2-28702fbe5eec",
      "typeId": 12885,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13083
    },
    {
      "id": 117314,
      "name": "Registration",
      "isParallelPresentation": false,
      "importedId": "d25d571b-56e6-4adf-a62b-0da3cc2e801b",
      "typeId": 12885,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13104
    },
    {
      "id": 117315,
      "name": "Opening",
      "isParallelPresentation": false,
      "importedId": "892192eb-09b1-42ac-b799-61e98318417f",
      "typeId": 12514,
      "roomId": 11135,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13084
    },
    {
      "id": 117316,
      "name": "Workshop - VAMEXP",
      "isParallelPresentation": true,
      "importedId": "5efc9d90-bf44-4554-b8e9-931d3c9a0823",
      "typeId": 12522,
      "roomId": 11132,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13119
    },
    {
      "id": 117317,
      "name": "Panel discussion",
      "isParallelPresentation": true,
      "importedId": "2ef79976-4c4d-447e-8bd7-d66d984c1df8",
      "typeId": 12518,
      "roomId": 11133,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 13119
    },
    {
      "id": 117319,
      "name": "Demo 1 (XL)",
      "isParallelPresentation": false,
      "importedId": "0008538f-a21d-4abe-bcbd-c21509292519",
      "typeId": 12515,
      "roomId": 11134,
      "chairIds": [
        117125
      ],
      "contentIds": [
        117268,
        117272
      ],
      "source": "SYS",
      "timeSlotId": 13095
    },
    {
      "id": 117320,
      "name": "Demo 2 (108)",
      "isParallelPresentation": false,
      "importedId": "9b20444b-c207-42da-88c8-75e43ca98bd4",
      "typeId": 12515,
      "roomId": 11133,
      "chairIds": [
        117125
      ],
      "contentIds": [
        117280,
        117261
      ],
      "source": "SYS",
      "timeSlotId": 13115
    },
    {
      "id": 117321,
      "name": "Demo 2 (XL)",
      "isParallelPresentation": false,
      "importedId": "adfba7ec-4960-4969-a256-1f1646f02164",
      "typeId": 12515,
      "roomId": 11134,
      "chairIds": [
        117125
      ],
      "contentIds": [
        117274
      ],
      "source": "SYS",
      "timeSlotId": 13115
    },
    {
      "id": 117323,
      "name": "WiP posters 1 (108)",
      "isParallelPresentation": false,
      "importedId": "339d2387-69a5-4aeb-b87f-9d27dfc027ec",
      "typeId": 12521,
      "roomId": 11133,
      "chairIds": [
        117084
      ],
      "contentIds": [
        117273,
        117233,
        117239
      ],
      "source": "SYS",
      "timeSlotId": 13094
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 116970,
      "typeId": 12884,
      "title": "User-Centered Broadcasting Service Utilizing Personal Data Store",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1018",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Recently, the data management model using personal data store (PDS), which is a mechanism for users to store and manage their personal data, has been discussed in response to stricter privacy protection worldwide. In this regard, we developed a data-driven personalization method for broadcasting services. Various personal data, such as program viewing history and Internet service usage history, are stored centrally in the PDS on the user’s side. This personal data can be utilized under user control when using various services allowing cross-industry service collaboration while maintaining a high level of transparency to the user. This enables users to use broadcasting service more widely and conveniently by linking it with various Internet services. In this study, we developed a prototype system that implements end-to-end components from acquisition to utilization of broadcast program viewing history. The system consists of a set of functions that acquires viewing history from broadcast and Internet streaming, stores it on PDS, and uses it in applications. The PDS implementation utilizes open-source software based on web standards to facilitate data linkage with a variety of Internet services. As an effective example for system evaluation, we designed and prototyped on-demand video viewing services in which separate applications of different service providers are linked via the PDS.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 116879
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 116904
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Setagaya, Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": "Science and Technology Research Labs"
            }
          ],
          "personId": 116855
        }
      ]
    },
    {
      "id": 116971,
      "typeId": 12884,
      "title": "Scenario-based Exploration of Integrating Radar Sensing into Everyday Objects for Free-Hand Television Control",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532982"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1035",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "We address gesture input for TV control, for which we examine mid-air free-hand interactions that can be detected via radar sensing. We adopt a scenario-based design approach to explore possible locations from the living room where to integrate radar sensors, e.g., in the TV set, the couch armrest, or the user's smartphone, and we contribute a four-level taxonomy of locations relative to the TV set, the user, personal robot assistants, and the living room environment, respectively. We also present preliminary results about an interactive system using a 15-antenna ultra-wideband 3D radar, for which we implemented a dictionary of six directional swipe gestures for the control of dichotomous TV system functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 116880
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ştefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 116862
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 116930
        }
      ]
    },
    {
      "id": 116972,
      "typeId": 12884,
      "title": "Learning Sustainable Locust Control Methods in Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1013",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Invasion of locust swarms has affected the crops in many countries in Africa and Asia, which is a significant threat to food security. Therefore, different approaches are adopted to monitor and control the locust swarms to save the crops. Furthermore, it has been proved in various studies that technology can help in agriculture through drones, real-time data monitoring, or teaching the farmers with the latest tools. \r\n\r\nFollowing the UN sustainability goals for food security, this research has presented a Virtual Reality(VR) based educational application to teach sustainable locust management strategies. Using hand tracking technology in the Oculus Quest lets users learn how farmers can deal with locusts without pesticides. Based on a storytelling approach, the methods presented are profitable for the farmers and free of any harm to crops regarding food security. This application can help motivate the adoption of these sustainable locust control strategies in broader interventions for environmental recovery.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 116881
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Applied Science BFI Vienna",
              "dsl": ""
            }
          ],
          "personId": 116944
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 116937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 116898
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": ""
            }
          ],
          "personId": 116908
        }
      ]
    },
    {
      "id": 116973,
      "typeId": 12884,
      "title": "The Augmented Museum: A Multimodal, Game-Based, Augmented Reality Narrative for Cultural Heritage",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532967"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1036",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "As digitization has transformed media and Augmented Reality (AR) is evolving from a research area to a commodity, museums are creating interactive AR experiences to digitally enhance their collection and increase audience engagement. Head-worn AR experiences, though, face interaction challenges as they are often employed in busy spaces and are in need of intuitive multimodal interfaces for users on the move. This paper presents an innovative, work-in-progress, multimodal AR experience integrating non-obtrusive dialogue, music, and sound as well as gesture and gaze-based interaction, while a user is wearing a head-worn AR display. Users are motivated to explore and interact with digital cultural artefacts superimposed onto the real-world museum setting and physical artefacts, while moving around in a museum setting. We initially analyze interactive AR experiences to identify specific user requirements related to head-worn AR experiences. We deploy these requirements for the design of interactive, multimodal AR in a museum setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 116920
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 116922
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 116959
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 116932
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 116918
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 116969
        }
      ]
    },
    {
      "id": 116974,
      "typeId": 12884,
      "title": "Classification of the Video Type and Device Used in 360-Degree Videos from the Trajectories of its Viewers' Orientations with LSTM Neural Network Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532975"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1015",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "360-degree videos are consumed in diverse devices: some based in immersive interfaces, such as viewed through Virtual Reality headsets and some based in non-immersive interfaces, as in a computer with a pointing device or mobile devices with touchscreens. We have found, in prior work, significant differences in user behavior between these devices. From a dataset of the trajectories of the users’ head orientation in 775 video reproductions, we classify which kind of video was played (two values) and which of the four possible devices was used to reproduce these videos. We found that recurrent neural network models based on LSTM layers are able to classify the video type and the device used to play the video with an average accuracy of over 90% with only four seconds of trajectory. We are convinced that this knowledge can improve techniques to predict future viewports used in viewport-adaptive streaming when diverse devices are used.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 116935
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 116893
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 116957
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 116827
        }
      ]
    },
    {
      "id": 116975,
      "typeId": 12884,
      "title": "CS:NO - an Extended Reality Experience for Cyber Security Education",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1037",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This work-in-progress presents the design of an XR prototype for the purpose of educating basic cybersecurity concepts. We have designed an experimental virtual reality cyberspace to visualise data traffic over network, enabling the user to interact with VR representations of data packets. Our objective was to help the user better conceptualise abstract cybersecurity topics such as encryption and decryption, firewall and malicious data. Additionally, to better stimuli the sense of immersion we have used Peltier thermoelectric modules and Arduino Uno to experiment with multisensory XR. Furthermore, we reflect on early evaluation of this experimental prototype and present potential paths for future improvements. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 116966
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 116927
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 116963
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 116933
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 116867
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 116949
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "RISE Research Institutes of Sweden",
              "dsl": "RISE  Cybersecurity"
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 116899
        }
      ]
    },
    {
      "id": 116976,
      "typeId": 12884,
      "title": "Deep Learning Augmented Realistic Avatars for Social VR Human Representation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532976"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1016",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Virtual reality (VR) has created a new and rich medium for people to meet each other digitally. In VR, people can choose from a broad range of representations. In several cases, it is important to provide users with avatars that are a lifelike representation of themselves, to increase the user experience and effectiveness of communication. In this work, we propose a pipeline for generating a realistic and expressive avatar from a single reference image. The pipeline consists of a blendshape-based avatar combined with two deep learning improvements. The first improvement module runs offline and improves the texture map of the base avatar. The second module runs inference in real-time at the rendering stage and performs a style transfer to the avatar's eyes. The deep learning modules effectively improve the visual representation of the avatar and show how AI techniques can be integrated with traditional animation methods to generate realistic human avatars for social VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 116845
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 116943
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 116905
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 116931
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 116912
        }
      ]
    },
    {
      "id": 116977,
      "typeId": 12884,
      "title": "Augmenting a Nature Documentary with a Lifelike Hologram in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532974"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1038",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "While augmented reality television (ARTV) is being investigated in research labs, the high cost of AR headsets makes it difficult for audiences to benefit from the research. However, the relative affordability of virtual reality (VR) headsets provides ARTV researchers with opportunities to test their prototypes in VR. Additionally, as VR becomes an acceptable medium for watching conventional TV, augmenting such viewing experiences in VR creates new opportunities.\r\nWe prototype a nature documentary ARTV experience in VR and conduct a remote user study (n=10) to investigate six points on the visual display design dimension of presenting a lifelike programme-related hologram. We manipulated the starting point and the movement behaviour of the hologram to gain insight into viewer preferences.\r\nOur findings highlight the importance of personal preferences and that of the perceived role of a hologram in relation to the underlying TV content; suggesting there may not be a single way to augment a TV programme. Instead, creators may need to provide the audiences with capabilities to customise ARTV content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 116903
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Salford",
              "institution": "The British Broadcasting Corporation",
              "dsl": "Research and Development"
            }
          ],
          "personId": 116940
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 116843
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 116916
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 116828
        }
      ]
    },
    {
      "id": 116978,
      "typeId": 12884,
      "title": "Neuroscience-based Intelligent Video Evaluation System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22d-1004",
      "source": "PCS",
      "trackId": 12236,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "BrainAnswer is a web-based platform for psychophysiological data acquisition and storage. It enables creating highly customizable protocols for neuroscience studies of media or advertising content and allows the simultaneous recording of content, physiological signals, and self-reporting from the participants using the compatible hardware. The industry has a lot to gain from integrating psychophysiological data in media studies as this data is not dependent on the consciously declared self-report of subjects to obtain measurable results and is able to depict unconscious processes occurring during the experience or the contact with content. As an example, a study of an advertising video with 28 subjects showed a connection between the drop in psychophysiological emotional engagement at its end and its low success. This shows that media and advertising companies can benefit from the BrainAnswer approach of studying psychophysiological data to adapt their content in order to reduce failure risk and improve success rates.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Castelo Branco",
              "city": "Castelo Branco",
              "institution": "BrainAnswer, LDA",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "IPCB",
              "dsl": "ESALD"
            }
          ],
          "personId": 116952
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "BrainAnswer",
              "dsl": ""
            }
          ],
          "personId": 116913
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "IPCB",
              "dsl": "ESALD"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "BrainAnswer Lda",
              "dsl": ""
            }
          ],
          "personId": 116925
        }
      ]
    },
    {
      "id": 116979,
      "typeId": 12884,
      "title": "HiruXR: a Web library for Collaborative and Interactive Data Visualizations in XR and 2D",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532981"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1020",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "HiruXR is a Javascript library aimed at creating collaborative and interactive data visualizations in eXtended Reality (XR). Developers can use it to create environments where multiple users can communicate and collaborate around one or many visualizations. We want to open these environments to users with 2D displaying and interaction capabilities, e.g. allowing them to collaborate from their laptops and phones with others using Head Mounted Displays (HMD). But displaying the same interface is not optimal, users should see an interface according to their device capabilities. Therefore, we are steering the design of HiruXR towards supporting responsiveness without putting all the burden in application developers. The library defines visualization, interaction and collaboration components that adapt to the user device capabilities, i.e. 2D or XR. In this paper, we share the design philosophy, initial implementation examples and lessons learned so far building some of these components for collaboration between VR and 2D devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 116946
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Donostia",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 116947
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 116900
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 116951
        }
      ]
    },
    {
      "id": 116980,
      "typeId": 12884,
      "title": "AR in the OR: exploring use of augmented reality to support endoscopic surgery",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532970"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1043",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Modern operating rooms (OR) are equipped with several ceiling- and wall-mounted screens that display surgical information. These physical displays are restricted in placement, limiting the surgeons' ability to freely position them in the environment. Our work addresses this issue by exploring the feasibility of using an augmented reality (AR) headset (Microsoft HoloLens 2) as an alternative to traditional surgical screens; leading to a reduced OR footprint and improved surgical ergonomics. We developed several prototypes using state-of-the-art hardware/software and conducted various neurosurgery-related exploratory studies. Initial feedback from users suggests that coloration and resolution of the holographic feed were adequate, however, surgeons frequently commented on tactile/visual asynchrony. This emphasizes the need for novel, more efficient hardware/software solutions to support fine motor tasks in the OR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 116938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116834
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116842
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116945
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 116884
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 116860
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 116829
        }
      ]
    },
    {
      "id": 116981,
      "typeId": 12884,
      "title": "\"I want to be independent. I want to make informed choices.\": An Exploratory Interview Study of the Effects of Personalisation of Digital Media Services on the Fulfilment of Human Values",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532977"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1022",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "From the landing page of a shopping website, to a tailored layout on a video streaming app, digital media experiences are becoming increasingly personalised, and none of us have the same experience as each other. We report on a series of in-depth interviews, with UK media users from 19 to 68 years old, exploring their awareness, feelings, expectations and concerns about the digital media being personalised ’for them’, and the language that they use when talking about it. Our repeatable, extensible methodology develops insights aligned to a framework of fundamental human values.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 116870
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Salford, Manchester",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 116857
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": ""
            }
          ],
          "personId": 116936
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 116854
        }
      ]
    },
    {
      "id": 116982,
      "typeId": 12884,
      "title": "The Promotion of Empathy in Intelligent Assistants for iTV through Proactive Behaviours",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1004",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "The integration of intelligent assistants, in devices belonging to the television ecosystem, has simplified the accomplishment of more demanding tasks (such as content searches). However, it can be seen that these assistants are restricted to purely reactive behaviours and show a reduced human and empathic dimension in relation to the users. However, in other application domains, there has been an increasing integration of proactive behaviours, which can counteract these barriers and, consequently, improve the respective User Experience (UX). It is precisely in this context of proactivity that this research is designed. The goal is to contribute to the advancement of intelligent assistants in the interactive TV (iTV) domain, studying which proactive behaviours can be integrated in an intelligent assistant for iTV to promote its empathy, the associated UX and, consequently, its adoption in a more fluid and massive way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia"
            }
          ],
          "personId": 116965
        }
      ]
    },
    {
      "id": 116983,
      "typeId": 12884,
      "title": "Usability Of Text-To-Speech Technology in Creating News Podcasts using Portuguese Of Portugal",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1040",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "The increase in the consumption of digital formats has, in many cases, been a penalty for traditional media companies. In the adaptation to digital, the transformation of written news into audio formats, that guarantee spatio-temporal flexibility in its consumption, is one of the differentiating options. Artificial intelligence tools can help accelerate and automate the digitalization processes. It is, therefore, the objective of this paper to evaluate the integration of Text-to-Speech (TTS) technology in the process of creating news podcasts. The study comprised two surveys. The first corresponding to the validation of TTS services in Portuguese from Portugal and, the second for the validation of three models of news podcasts containing human voice, synthesized voice via TTS, and a hybrid model with TTS voice and human voice. The results point to a general acceptance of the integration of voices generated by TTS in news podcasts without prejudice to the consumer experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            },
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116858
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116967
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 116914
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 116875
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116958
        }
      ]
    },
    {
      "id": 116984,
      "typeId": 12884,
      "title": "Enabling User-centric Assessment and Modelling of Immersiveness in Multimodal Multimedia Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1002",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Multimodal, immersive systems are the latest development within the field of multimedia. These systems emulate the senses by means of omnidirectional visuals, 360° sound, motion tracking and touch simulation to create a feeling of presence in the virtual environment. They have the potential to substitute physical interactions in application domains such as training (Industry 4.0), or e-health (tele-surgery). However, the COVID-19 pandemic has shown that they are not ready, as they still have room for improvement in terms network streaming quality, usability and the users’ feeling of presence. In addition, these systems can induce feelings of dizziness, nausea etc. (i.e. cybersickness). These factors therefore have an important impact on the user’s total immersion. In this work, we therefore propose that immersiveness can be devised from measuring four aspects, namely: presence (i.e. the feeling of being \"in\" the environment), cybersickness, network related Quality-of-Experience (QoE) and the usability of the application. Therefore, a two-dimensional user-centric approach on the assessment and modelling of immersiveness is proposed. These dimensions include (i) subjective and objective assessment of presence, cybersickness, usability and QoE and (ii) real-time modelling of immersiveness. Furthermore, a proof-of-concept is envisioned including two use cases. As such, we believe that this position paper will significantly advance the state of the art on immersive systems and multimedia in general.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Oost-Vlaanderen",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            }
          ],
          "personId": 116885
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": ""
            }
          ],
          "personId": 116955
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 116953
        }
      ]
    },
    {
      "id": 116985,
      "typeId": 12884,
      "title": "Human-Computer Interaction Patterns for Head-Mounted-Device-based Augmented Reality in the Exhibition Domain",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1007",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Head-mounted-device (HMD) based augmented reality (AR) applications bring new opportunities to the exhibition domain. However, first-time users such as exhibition visitors are not familiar with the interaction method of the HMD, which also brings challenges to exhibition authors while implementing HMD based AR technologies. This research project focuses on the accessibility of the HMD based AR application in the exhibition domain, which explores potential interaction patterns based on the technical feature of the HMD for exhibition-related use cases. Both information system research method and design research method are applied while exploring interaction solutions for human factor challenges. Several prototypes will be created and iteratively tested and evaluated at the exhibition for generalizing effective and accessible interaction patterns. As the result, it will provide a pattern-based interaction system as an artifact with design knowledge to developers so that they can convert the interaction pattern into an authoring tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wiesbaden",
              "institution": "RheinMain University of Applied Sciences",
              "dsl": "Design, Computer Science, Media"
            }
          ],
          "personId": 116872
        }
      ]
    },
    {
      "id": 116986,
      "typeId": 12884,
      "title": "Factors influencing video Quality of Experience measured with ecologically valid methods",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1005",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Over the last decade, user subjective perception became the subject of interest in video quality studies. Researchers conclude about this Quality of Experience (QoE) based on participants’ statements, behaviors, and psychophysiological reactions to distorted videos. The reason for that is the fact that direct, objective evaluation of QoE is impossible due to its subjective nature. Thus, clear operationalization of variables in QoE studies is crucial. For that purpose theoretical background is necessary. Current theoretical models of QoE consist of many strongly correlated variables and omit the role of content. In effect, most QoE studies related to compression use strict laboratory experiments with Absolute Category Ratings. The ecological validity of such studies is limited. In my Ph.D., I investigate factors influencing QoE in a more natural context. To be able to conclude about those multiple, complex variables I am working on a content-based video QoE model inspirited by Structural Causal Models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Department of Information and Communication Technologies"
            }
          ],
          "personId": 116928
        }
      ]
    },
    {
      "id": 116987,
      "typeId": 12884,
      "title": "Remote Presence: Live Holograms for a Social Classroom",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1011",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Existing communication technologies have displayed a lack of affordances in supporting social-emotional connections, which is of particular interest in educational settings. We are therefore developing a live sensory immersive 3D video technology, built on a prior developed platform. Pilot trials in a Finnish school have yielded promising findings. We continue to advance the state-of-the-art platform in parallel with regards to 3D capture quality and data compression algorithms. Current developments entail joint investigations and evaluations of affordances to support emotional, social, motivational, and achievement impacts with learners and teachers from a Namibian and Finnish school. Participants can experience \"remote presence\" wearing the hololens 2 while others are live streamed from another country captured by two cameras.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 116850
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 116869
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 116856
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 116894
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 116950
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": "Interaction Design Lab"
            }
          ],
          "personId": 116835
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 116838
        }
      ]
    },
    {
      "id": 116988,
      "typeId": 12884,
      "title": "A Podcast Creation Platform to Support News Corporations: Results from UX Evaluation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1046",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Podcasts as a media format have been increasing popular in recent years. The ease of access to this format have contributed for its success story. However, the creation of Podcasts requires specific hardware and software for recording and editing it. Some platforms have emerged with the proposal to ease this creation process, namely by introducing Text-to-Speech (TTS) technologies removing the need for capturing and editing voice, reducing the effort necessary for producing this format, yet no platform allows the use of TTS in Portuguese of Portugal while retaining the scope of “podcast creation platform”. Taking these limitations in mind we present the proposal of an all-in-one Podcast Creation platform with the availability of TTS Technology in Portuguese of Portugal. The paper describes the usability testing (UX) of the platform using 3 methodologies being: Self-Assessment Manikin (SAM), System Usability Scale (SUS) and Attrakdiff. with promising results regarding its usability and desirability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116967
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 116914
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 116875
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116858
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116958
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 116906
        }
      ]
    },
    {
      "id": 116989,
      "typeId": 12884,
      "title": "Acting emotions: physiological correlates of emotional valence and arousal dynamics in theatre",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1024",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Professional theatre actors are highly specialized in controlling their own expressive behaviour and non-verbal emotional expressiveness, so they are of particular interest in fields of study such as affective computing. We present Acting Emotions, an experimental protocol to investigate the physiological correlates of emotional valence and arousal within professional theatre actors. Ultimately, our protocol investigates the physiological agreement of valence and arousal amongst several actors. Our main contribution lies in the open selection of the emotional set by the participants, based on a set of four categorical emotions, which are self-assessed at the end of each experiment. The experiment protocol was validated by analyzing the inter-rater agreement (> 0.261 arousal, > 0.560 valence), the continuous annotation trajectories, and comparing the box plots for different emotion categories. Results show that the participants successfully induced the expected emotion set to a significant statistical level of distinct valence and arousal distributions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "Faculty of Engineering of Porto",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "INESC-TEC",
              "dsl": ""
            }
          ],
          "personId": 116868
        }
      ]
    },
    {
      "id": 116990,
      "typeId": 12884,
      "title": "Integrating 3D Objects in Multimodal Video Annotation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1047",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This paper presents and discusses the introduction of 3D functionalities for an existing web-based multimodal video annotation tool. Over the past years, we have developed a multimodal web video annotation tool that now combines 3D models and 360º content with more traditional annotation types (e.g., text, drawings, images), offering users the possibility of adding extra information in their annotation work. We show how 3D models augment the annotation work and add advantages like viewing or exploring objects in detail and from different angles. The paper reports detailed feedback from a pilot study in form of a workshop with traditional dance experts to whom these new features were presented. We conclude with an outlook of future iterations of the video annotator based on the experts’ feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "NOVA University of Lisboa",
              "dsl": "Faculdade de Ciências e Tecnologia "
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Setúbal",
              "institution": "Polytechnic Institute of Setúbal",
              "dsl": "Escola Superior de Tecnologia "
            }
          ],
          "personId": 116873
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Nova University of Lisboa",
              "dsl": "ICNOVA, FSCH"
            }
          ],
          "personId": 116849
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "FCSH - Universidade Nova de Lisboa",
              "dsl": "ICNOVA"
            }
          ],
          "personId": 116895
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "NOVA School of Science & Technology, NOVA University Lisbon,",
              "dsl": ""
            }
          ],
          "personId": 116902
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS"
            }
          ],
          "personId": 116859
        }
      ]
    },
    {
      "id": 116991,
      "typeId": 12884,
      "title": "Emotional Virtual Reality Stroop Task: an Immersive Cognitive Test",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532988"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1025",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Stroop Colour-Word Task has been widely used as a cognitive task. There are computerised and Virtual Reality versions of this task that are commonly used. The emotional version of the task, called the Emotional Stroop Colour-Word task is commonly used to induce certain emotions in a person. We are developing an application that brings the Emotional Stroop Colour-Word task into Virtual Reality. The aim of this application is to elicit different stress levels on the user and to record associated brain, heart and skin activity using wearable sensors. It is an immersive application that includes a tutorial, artificial intelligence generated audio instructions and a logging system for the user activity.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Munster",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 116896
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 116839
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 116926
        }
      ]
    },
    {
      "id": 116992,
      "typeId": 12884,
      "title": "Party Mascot: Experimental Prop Design for Streaming Actual Plays ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1026",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Party Mascot is an experimental design for a dynamic, interactive prop used in “actual play” streaming. Taking the form of a talking mechanical bird, the Party Mascot extends audience participation on the Twitch platform from its native chat interface to the physical playspace. Building on a critical review of frame analytical approaches to role-playing game studies and supported by an ethnographic study of actual play performers, the Party Mascot is designed to “flicker” between social, gameplay, and fictional frames of interaction. It can accommodate any number of participants and adapts to multiple roles within new mediated performance contexts. Shifting spectatorship from the screen to the physical world, the Party Mascot can reconfigure audience/performer relationships, open new avenues for game design, and engage the genre of actual play as a new site of experimentation and innovation between the producers and consumers of media.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media, School of Literature, Media and Communication"
            }
          ],
          "personId": 116948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology,",
              "dsl": "Digital Media, School of Literature, Media, and Communication"
            }
          ],
          "personId": 116863
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media/ School of Literature, Media, and Communication"
            }
          ],
          "personId": 116882
        }
      ]
    },
    {
      "id": 116993,
      "typeId": 12884,
      "title": "ScenaProd: creating interactive medias without programming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1013",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This demonstration aims at presenting ScenaProd, a tool that allows people to produce multisensory scenagrams (multisensory exercises or interactive media). All participants can create their own scenagrams with that tool or test a more complete one that is already created. A scenagram can be defined as an interaction between a human being and different devices. For example, a robot asks a question while displaying a visual clue on a screen. Then, the participant can respond by pressing a large and colored contactor. In case of a correct answer, the robot would play a short victory song and a light would light up green. If a wrong answer were given, the system would have a different reaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116874
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Vannes",
              "institution": "South Brittany University",
              "dsl": ""
            }
          ],
          "personId": 116968
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116901
        }
      ]
    },
    {
      "id": 116994,
      "typeId": 12884,
      "title": "Peripheral Light Cues as a Naturalistic Measure of Focus",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1027",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Deeply immersive experiences are intrinsically rewarding; evoking them for another is a cornerstone of success in artistic or design practice.  At the same time, modern interfaces have created a state of 'partial continuous attention', and frequent self-interruption is more common than ever.  In this paper, we propose a smart-glasses based interaction to quantify self-interruption dynamics in naturalistic settings, in which a slowly changing peripheral LED is monitored as a secondary task by the user.  We demonstrate that this interaction captures useful information about a user's state of engagement in real-world conditions.  These data can provide designers and artists novel, objective insight into the depth of immersive experience evoked in real-world settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments Group"
            }
          ],
          "personId": 116848
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments"
            }
          ],
          "personId": 116844
        }
      ]
    },
    {
      "id": 116995,
      "typeId": 12884,
      "title": "Towards Multimodal Search and Visualization of Movies Based on Emotions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1049",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Movies are one of the most important and impactful forms of entertainment and a powerful vehicle for culture and education, due to the cognitive and emotional impact on the viewers, and technology has been making them more accessible in pervasive services and devices. As such, the huge amount of movies we can access, and the important role emotions play in our lives, make more pertinent the ability to access, visualize and search movies based on their emotional impact. In this paper, we characterize the challenges and approaches in this scenario, and present interactive means to visualize and search movies based on their dominant and actual emotional impact along the movie, with different models and modalities. In particular through emotional highlights and trajectories, the user’s emotional state, or a music being played. Music contributes greatly to the emotional impact of movies and it can also be a trigger to get us into one of them in\r\nserendipitous moments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 116907
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 116954
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculty of Science at University of Lisbon",
              "dsl": ""
            }
          ],
          "personId": 116934
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Universidade de Lisboa",
              "dsl": "LASIGE, Faculdade de Ciências"
            }
          ],
          "personId": 116890
        }
      ]
    },
    {
      "id": 116996,
      "typeId": 12884,
      "title": "Modeling Cognitive Load and Affect in Interactive Game-based Learning Using Physiological Features",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1009",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Media use in educational environments has been rapidly developing with the increasing availability and diversity of interactive elements. By understanding how student cognitive load changes when interacting with learning technologies, we can make sense of their learning process and how to provide appropriate, personalized media design to enhance the learning experience. Recent developments in sensing technologies makes it possible to capture learner’s dynamic physiological reactions. In this thesis research, we will identify learner’s cognitive load when interacting with educational media. We will explore how affective reactions contribute to the modeling of cognitive load, and how real-time cognitive load changes alongside learning activities. We focus on modeling such information using physiological reactions that include pupillary, cardiovascular, and electrodermal responses. We are conducting this work in a game-based learning (GBL) environment for reading comprehension. We have implemented a sensing pipeline that will enable the modelling of learner affect and cognitive load. The modeling and analysis of this project can further support the design of interactive learning media that provides real-time adaptation in learning processes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 116892
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Dept. of Computing Science"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Language Sciences"
            }
          ],
          "personId": 116919
        }
      ]
    },
    {
      "id": 116997,
      "typeId": 12884,
      "title": "Sunflower: An Interactive Artistic Environment based on IoMusT Concepts",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1012",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "The Internet of Musical Things (IoMusT) is an interdisciplinary area that aims to improve the relationship between musicians and their peers, as well as between musicians and audience members, creating new forms of interaction in concerts, studio productions, and music learning. \r\n  Although emerging, this field already faces some challenges, such as lack of privacy and security, and mainly, lack of standardization and interoperability between its devices. Therefore, an environment design, called Sunflower, was proposed, which tries to contribute to solving the most recurrent problems in this area, specifying an architecture pattern, protocol, and sound features that aim to allow heterogeneity in these systems. \r\n  Its practical implementation resulted in an interoperable, multimedia, and interactive environment. This paper, therefore, shows a demonstration of how Sunflower works in the accomplishment of an artistic presentation, also emphasizing its approach, the technologies that support it, and the advances it can bring to the area of IoMusT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "Niterói",
              "institution": "Fluminense Federal University",
              "dsl": "MidiaCom Lab"
            }
          ],
          "personId": 116846
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Rio de Janeiro",
              "city": "Niterói",
              "institution": "Fluminense Federal University - UFF",
              "dsl": "Computer Science"
            }
          ],
          "personId": 116865
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Minas Gerais",
              "city": "São João del-Rei",
              "institution": "Federal University of São João del-Rei",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 116923
        }
      ]
    },
    {
      "id": 116998,
      "typeId": 12884,
      "title": "Olympics on the Google Assistant: Modular Conversation Design",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22d-1002",
      "source": "PCS",
      "trackId": 12236,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "While many voice assistants use natural language processing and understanding to determine a user’s intent, Conversation Designers (CxDs) create many responses by producing voice user interface specs. These are often bespoke solutions, optimized for a feature. However, this approach is difficult to scale. This industry talk describes the design process of the first Olympics experience on Google Assistant and how it uses modular design to scalably answer millions of questions. To do this effectively, the design process required grouping together frequently asked queries, creating modular design components based on available data, and emphasizing/de-emphasizing components of an answer to get the widest intent coverage. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": "Assistant/Search"
            }
          ],
          "personId": 116878
        }
      ]
    },
    {
      "id": 116999,
      "typeId": 12884,
      "title": "Immersive Tele-operation Driving thought 5G",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22d-1003",
      "source": "PCS",
      "trackId": 12236,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "In environments of autonomous AGVs operating at industrial areas, carrying material from place to place, and performing different tasks, some unexpected problems may arise. Troublesome objects in the middle of the AGV path can be problematic and sometimes involve a stop in the system. In such cases, a human intervention is necessary.\r\nThe Tele-operation Driving (ToD) with MR and haptics devices is an innovative solution to have AGVs under control, being able to control them remotely without the need to be exposed to the dangers of the industrial environment. \r\nIn order to achieve the immersive ToD an E2E system is designed.\r\n·      Immersive cockpit: an innovative virtual reality application is designed to emulate an ordinary driving experience. A virtual cockpit is visualized at head-mounted display and a real time streaming video from the AGV surroundings is projected around the virtual car. The immersive application allows the operator to feel inside of the AGV and enables an intuitive remote driving of the AGV.\r\n·      Connectivity requirements: to accomplish with necessities of the ToD data transmission some requirements must be fulfilled. The AGV connectivity is set through 5G with millimeter wave. This innovative connectivity enables low latencies at the highest throughput, especially focused on uplink. Those characteristics allow to send video streaming in real time and set the information flux required for the ToD. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Bell Labs",
              "dsl": "Nokia"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Bell Labs",
              "dsl": "Nokia"
            }
          ],
          "personId": 116840
        }
      ]
    },
    {
      "id": 117000,
      "typeId": 12884,
      "title": "Intelligent Fatigue Driving Detection & Management System based on Sensing Technology and AI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22d-1001",
      "source": "PCS",
      "trackId": 12236,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "In this presentation, we successfully developed the first intelligent driving fatigue detection system ( IDFDS) system based on GSR signals, and it has been released to car makers. The system has overcome the technical challenges of physiological signals and being transformed into a wearable ring satisfied with drivers’ experience. Thanks to the advanced physiological computing technology, material science and excellent support from industrial production chain, the commercial IDFDS is available to the market. In the future, IDFDS can be integrated with other sensors, or other detection technologies, and to provide more accurate service to users.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "WenZhou",
              "institution": "SenTech Wearable & AI",
              "dsl": "Intelligent Driving AI Lab"
            }
          ],
          "personId": 116939
        }
      ]
    },
    {
      "id": 117001,
      "typeId": 12884,
      "title": "Designing a VR Lobby for Remote Opera Social Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532980"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1031",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Several social VR platforms support virtual entertainment events, however their value for post-show activities remains unclear. Through a user-centered approach, we design a social VR lobby experience to enrich four motivations of theatre-goers: social, intellectual, emotional, and spiritual engagement. We ran a context-mapping focus group session with professionals (N=6) to conceptualize the social VR space for digital opera experiences. Based on our findings, we propose a social VR lobby consisting of four rooms: 1) a Bar for social engagement, 2) an Info Booth for intellectual engagement, 3) a Photo Zone for emotional engagement, and 4) an Interactive Stage for spiritual engagement. Based on this work, we plan to experimentally evaluate audience experiences in each room in order to create a social VR lobby template for theater experiences.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 116897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 116942
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 116924
        }
      ]
    },
    {
      "id": 117002,
      "typeId": 12884,
      "title": "The Co-Creation Space: An Online Safe Space for Community Opera Creation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1004",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This work presents the Co-Creation Space, a multilingual platform for professional and community artists to 1) generate raw artistic ideas, and 2) discuss and reflect on the shared meaning of those ideas. The paper describes the architecture and the technology behind the platform, and how it was used to facilitate the communication process during several user trials. By supporting ideation sessions around media items guided by a facilitator and allowing users to express themselves and be part of the creation of an artistic product, participants were enabled to access new cultural spaces and be part of the creative process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": ""
            }
          ],
          "personId": 116960
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Netherlands",
              "city": "Amsterdam",
              "institution": "CWI ",
              "dsl": "DIS "
            }
          ],
          "personId": 116942
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 116946
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 116924
        }
      ]
    },
    {
      "id": 117003,
      "typeId": 12884,
      "title": "Grasping Temperature: Thermal Feedback in VR Robot Teleoperation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532969"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1032",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This paper presents a proof-of-concept of a robotic teleoperation system, that provides the human operator a thermal sense in addition to the visual sense. With a sensor suite comprising a stereo camera, 360⁰ camera and long-wave infra-red camera, our demonstrator pushes the boundaries of virtual-reality situational awareness by bringing not only 3D visual content but also a 360⁰ thermal experience to the operator. The visual channel of our robotic teleoperation system is represented through a head-mounted-display and the thermal channel is displayed through directional heaters in the operator cockpit and a thermal glove. Initial tests showed that an operator successfully experienced a 360⁰ remote environment, correctly distinguished between and interacted with hot and cold objects, and could notice the presence of nearby people outside her direct field-of-view, based on their emitted heat.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 116943
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 116851
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 116852
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 116929
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 116941
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 116905
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 116826
        }
      ]
    },
    {
      "id": 117004,
      "typeId": 12884,
      "title": "Extended Reality Ulysses Demo",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1003",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "This demo paper proposes to exhibit the pilot episode of \\textit{XR Ulysses}, a creative project investigating the possibilities for live performance using three-dimensional volumetric video (VV) techniques via virtual reality (VR) technologies. \\textit{XR Ulysses} is part of a series of innovative performance experiments hybridizing theatre and extended reality (XR) technologies. Conference attendees are invited to don an HMD, embody the character of Stephen Dedalus, and engage Buck Mulligan in the famous opening scene of Joyce's book, situated on the top of the Martello Tower at Sandycove (Dublin). This scene enables individuals to experience a live-action re-enactment of James Joyce's \\textit{Ulysses} in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 116889
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 116961
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 116833
        }
      ]
    },
    {
      "id": 117005,
      "typeId": 12884,
      "title": "Video for Health (V4H) Platform.  A Secure Video Suite Platform for Online Care, Teleconsultation, Tele-orientation and Teleconsulting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1006",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "One of the major problems faced by public health managers around the globe is the lack of specialized professionals in remote locations to meet the health demands of society. To propose a solution for these problems, the authors developed the Video for Health (V4H) Platform for Brazil's National Education and Research Network (RNP), an R&D agency from the Brazilian Ministry of Science, Technology and Innovation (MCTI), which will be the topic of this proposed demo. The V4H is a system that aims to reduce the distance between health professionals and the population that needs primary care. The original proposal of the V4H was developed in two teams: Working group phase 1 (WG1) and Working Group phase 2 (WG2) at the Federal University of Paraíba (UFPB). The first working group grant went to develop the design of the system and it was tested at the Telehealth Center and the Federal University of São Paulo (Unifesp), at the São Paulo Area Military Hospital (HMASP) and at the TeleDentistry project at the University of São Paulo's (USP) Dentistry School (FOUSP). In the second phase of the project we developed more complex systems such as blockchain for video preservation, billing, time control and accessibility for the teleconsultations. The platform was tested and integrated with University of São Paulo's Hearth Institute (InCOR). The WG2 was coordinated by Prof. Guido Lemos (UFPB) and Prof. Marco Antonio Gutierrez (InCOR). The V4H Platform supports synchronous and confidential video streaming, with a scalable architecture to simplify integration with telehealth, teleconsulting and Electronic Health Record (HRE) platforms. The V4H system allows the authentication of the participants of the transmission, as well as the recording, retrieval and preservation of the transmitted content, using signature technologies with digital certificates and blockchain to ensure that the content remains immutable and providing the integrity and authenticity of the persisted data. The main focus of the solution is to offer a synchronous video service for platforms that support the electronic health record, where the recorded and preserved contents can be attached together with the patient's data, serving as legal evidence of the healthcare provided. There is also the possibility that these contents can be used as a data source for teleconsulting, tele-diagnosis and preceptorship activities for health professionals from all areas, initially focusing on basic and primary health care, in locations that lack specialized health professionals. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 116909
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "SP",
              "city": "São Paulo",
              "institution": "Federal University of São Paulo",
              "dsl": "Educational Design/TEDE"
            }
          ],
          "personId": 116910
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Paraíba Federal Institute of Technology",
              "dsl": "Graduate Studies in Information Technology"
            }
          ],
          "personId": 116866
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 116832
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 116921
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 116876
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "RJ",
              "city": "Rio de Janeiro",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 116831
        }
      ]
    },
    {
      "id": 117006,
      "typeId": 12884,
      "title": "A Children-Created Virtual Learning Space Station",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1005",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "We present a virtual learning space station, a design space, which was co-designed by 63 school children from three continents, namely Namibia, Malaysia and Finland. The design space station is developed on the Ohyay platform for children by children to plan, interact and conduct co-design activities. In our hands-on demonstration, attendees can organise/join a design session and test the online collaboration and facilitation tools provided by the space station. Our demo contributes directly to the IMX 2022 theme of\r\n“Interactive Media Brings us Together”.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "ASSET"
            }
          ],
          "personId": 116886
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": ""
            }
          ],
          "personId": 116830
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 116891
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 116894
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 116856
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Teaching and Learning Unit"
            }
          ],
          "personId": 116883
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "DRAC"
            }
          ],
          "personId": 116853
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "University of Namibia",
              "dsl": ""
            }
          ],
          "personId": 116877
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 116838
        }
      ]
    },
    {
      "id": 117007,
      "typeId": 12884,
      "title": "Augmenting Speech Agent with Gaze for Enhancing Interaction. By Drawing from human-human Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22e-1015",
      "source": "PCS",
      "trackId": 12238,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Speech technologies are increasing in popularity by offering new interaction modalities for users. Spoken interaction design centers around the use of a wake-word to initiate interaction and the transcription of the users' spoken instruction to complete the task. However, in human-to-human conversation, speech is initiated by and supplemented with a range of other modalities, such as gaze and gesture. My research focuses on the need to better understand how human-technology `conversations' can be improved by borrowing from human-human interaction. Therefore, Tama -- a gaze-activated smart speaker, was designed to explore the use of gaze in conversational interaction. Tama uses gaze to indicate attention and intent to interact on behalf of the user and as feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University ",
              "dsl": "Department of Computer and System Sciences"
            }
          ],
          "personId": 116836
        }
      ]
    },
    {
      "id": 117008,
      "typeId": 12884,
      "title": "Interactive Touch Kiosks Designed for the Elderly: A Compilation of Requisites Acknowledging Physical and Psycho-sociological Age-related Changes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532979"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22b-1030",
      "source": "PCS",
      "trackId": 12242,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "Aiming cognitive stimulation and physical exercise, interactive touch kiosks designed for the elderly seem to be promising options to promote active aging, ensuring e-health and well-being services. They need to be created and improved according to the elderly population's real needs; however, recommendations to develop these solutions are scattered in several guidelines and standards. In this study standards regarding physical and psycho- sociological age-related changes, such as vision, hearing, cognition, communication, gross and fine motor skills were gathered; physical and social factors were also considered. A total of 107 items were found, and the following categories were defined: Terminals, Interface, Content, and Other. The proposal can be used as: a) a list to guide the creation of services and systems; b) a grid to be color coded according to the level of problems found while usability evaluations are being conducted. This is a contribution to experts who can easily recognize the items that need to be improved in the services and systems, to better support the experience of the elderly user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 116887
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 116841
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 116837
        }
      ]
    },
    {
      "id": 117009,
      "typeId": 12884,
      "title": "The Fushimi Inari Experience: An Interactive Volumetric Film",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx22c-1009",
      "source": "PCS",
      "trackId": 12243,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [],
      "abstract": "As we move further into a spatial future, the demand for content creation tools is immense. Social media, gaming platforms and e-commerce have been converging into interactive spaces that involve spatial representations of the world the viewer is occupying, including digital humans. Creating digital representations of humans, or holograms, can be achieved through volumetric capture technologies. This method of bringing people to virtual three-dimensional environments has been rapidly increasing in popularity, but still lacks a key element: interactivity. In this paper we describe our work on producing interactive volumetric video that responds to viewers’ actions in real-time. We present the Fushimi Inari project: a commercial use case pushing the boundaries of what can be achieved with volumetric video and describe how our spatial content creation tools allow for interactive films to be created. Our contributions include blending volumetric clips, skeletonizing captures and applying multi-bone retargeting. We also provide means to integrate this in game engines for real-time photorealistic and interactive stories to be enjoyed by any viewer. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116962
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116871
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116915
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116861
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116864
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116888
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116964
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 116847
        }
      ]
    },
    {
      "id": 117063,
      "typeId": 12519,
      "title": "Influence of Multi-Modal Interactive Formats on Subjective Audio Quality and Exploration Behavior",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1020",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117284
      ],
      "eventIds": [],
      "abstract": "This study uses a mixed between- and within-subjects test design to evaluate the influence of interactive formats on the quality of binaurally rendered 360 degree spatial audio content. Focusing on ecological validity using real-world recordings of 60 s duration, three independent groups of subjects (N/3=18) were exposed to three formats: audio only (A), audio with 2D visuals (A2DV), and audio with head-mounted display (AHMD) visuals. Within each interactive format, two sessions were conducted to evaluate degraded audio conditions: bit-rate and Ambisonics order. Our results show a statistically significant effect (p < .05) of format only on spatial audio quality ratings for Ambisonics order. Exploration data analysis shows that format A yields little variability in exploration, while formats A2DV and AHMD yield broader viewing distribution of 360 degree content. The results imply audio quality factors can be optimized depending on the interactive format. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen",
              "institution": "Fraunhofer IIS",
              "dsl": "International Audio Laboratories Erlangen"
            }
          ],
          "personId": 117062
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Thueringen",
              "city": "Ilmenau",
              "institution": "TU Ilmenau",
              "dsl": ""
            }
          ],
          "personId": 117020
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "TU Ilmenau",
              "dsl": ""
            }
          ],
          "personId": 117021
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen",
              "institution": "Fraunhofer IIS",
              "dsl": "International Audio Laboratories Erlangen"
            }
          ],
          "personId": 117029
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen",
              "institution": "Fraunhofer IIS",
              "dsl": "International Audio Laboratories Erlangen"
            }
          ],
          "personId": 117045
        }
      ]
    },
    {
      "id": 117064,
      "typeId": 12519,
      "title": "What’s my future: a Multisensory and Multimodal Digital Human Agent Interactive Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1043",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117286
      ],
      "eventIds": [],
      "abstract": "This paper describes an interactive multimodal and multisensory fortune-telling experience for digital signage applications that combines digital human agents along with touchless haptic technology and gesture recognition. For the first time, human-to-digital human interaction is mediated through hand gesture input and mid-air haptic feedback, motivating further research into multimodal and multisensory location-based experiences using these and related technologies. We take a phenomenological approach and present our design process, the system architecture, and discuss our gained insights, along with some of the challenges and opportunities we have encountered during this exercise. Finally, we use our singular implementation as a paradigm as a proxy for discussing complex aspects such as privacy, consent, gender neutrality, and the use of digital non-fungible tokens at the phygital border of the metaverse.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Wroclaw",
              "institution": "SoftServe",
              "dsl": ""
            }
          ],
          "personId": 117019
        },
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "state": "",
              "city": "Lviv",
              "institution": "Softserve",
              "dsl": ""
            }
          ],
          "personId": 117049
        },
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "state": "",
              "city": "Lviv",
              "institution": "Softserve",
              "dsl": ""
            }
          ],
          "personId": 117011
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Wroclaw",
              "institution": "Softserve",
              "dsl": ""
            }
          ],
          "personId": 117036
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Ultraleap",
              "dsl": ""
            }
          ],
          "personId": 117058
        }
      ]
    },
    {
      "id": 117065,
      "typeId": 12519,
      "title": "Enhancing Engagement through Digital Cultural Heritage: A Case Study about Senior Citizens using a Virtual Reality Museum",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1028",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117010
      ],
      "eventIds": [],
      "abstract": "As the use of Virtual Reality (VR) increases, museums have been using it to create simulations of their artefact collections. However, the level of accessibility, inclusiveness, and engagement of these simulations with senior citizens has been understudied. To address the problem, this case study presents the design of the “Pop-up VR Museum”, a VR experience based on cultural heritage artefacts from the Design Museum in Helsinki that attempts to engage with audiences of wide age ranges. Users can interact with virtual artefacts and listen to stories contributed by different communities. The Pop-up VR Museum has been tested with 254 users at the museum and taken to several elderly care homes. Evaluation is based on users’ gameplay data and their responses to post-experience questionnaires. Results indicate some variation in types of engagement based on users’ age groups. Despite potential limitations, this study provides valuable insights for other museums to create inclusive VR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "Uusimaa",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Dept of Media"
            },
            {
              "country": "Finland",
              "state": "Uusimaa",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Dept of Media"
            }
          ],
          "personId": 117056
        }
      ]
    },
    {
      "id": 117066,
      "typeId": 12519,
      "title": "Virtual Rehearsal Suite: An Environment and Framework for Virtual Performance Practice",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1048",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117283
      ],
      "eventIds": [],
      "abstract": "Contemporary performance artists use Virtual Reality (VR) tools to create immersive narratives and extend the boundaries of traditional performance mediums. As the medium evolves, performance practice is changing with it. Our work explores ways to leverage VR to support the creative process by introducing the Virtual Rehearsal Suite (VRS) that provides users with the experience of a large-scale rehearsal or performance environment while occupying limited physical space and minimal real-world obstructions. In this paper, we discuss findings from scene study experiments conducted within the VRS. In addition, we contribute our thresholding protocols, a framework designed to support user transitions into and out of VR experiences. Our integrated approach to digital performance practice and creative collaboration combines traditional and contemporary acting techniques with HCI research to harness the innovative capabilities of virtual reality technologies creating accessible, immersive experiences for actors while facilitating user presence through state change protocols.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computational Media Design"
            }
          ],
          "personId": 117041
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "Computer Science"
            }
          ],
          "personId": 117012
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "School of Creative and Performing Art"
            },
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": "School of Creative and Performing Art"
            }
          ],
          "personId": 117057
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 117053
        }
      ]
    },
    {
      "id": 117067,
      "typeId": 12519,
      "title": "Detecting Human Attitudes through Interactions with Responsive Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1015",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117283
      ],
      "eventIds": [],
      "abstract": "The paper is based on the developments of the research project “Paradigms of Ubiquitous Computing”, funded by the Swiss National Science Foundation, 2019-23. It investigates the impact of environmentally embedded sensor-actuator systems on humans. With a critical stance, we examine human-machine interfaces to make quantitative statements about behavior patterns and attitudes, only based on their physical interactions with a responsive environment. By staging different paradigms of Ubiquitous Computing in an experimental setup and evaluating them with test persons, we aim to gain insights into the human experience and appropriation of immersive and sometimes challenging situations. The artistic approach is based on strategies of New Media Art and Speculative Design and is not aligned with processes commonly used in applied research and development. The evaluation design is based on mixed methods with a strong emphasis on semantic differentials to quantify user interactions with electronically enhanced devices and furnishings. The focus is on interaction design strategies and evaluation design methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Basel",
              "city": "Basel",
              "institution": "University of Applied Sciences and Arts Northwestern Switzerland FHNW",
              "dsl": ""
            }
          ],
          "personId": 117032
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Basel",
              "institution": "Institute of Experimental Design and Media",
              "dsl": "University of Applied Sciences and Arts Northwestern Switzerland"
            }
          ],
          "personId": 117024
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Basel",
              "city": "Basel",
              "institution": "University of Applied Sciences and Arts Northwestern Switzerland FHNW",
              "dsl": "Institute Experimental Design and Media Cultures IXDM"
            }
          ],
          "personId": 117054
        }
      ]
    },
    {
      "id": 117068,
      "typeId": 12519,
      "title": "Supporting Video Authoring for Communication of Research Results",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1005",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117285
      ],
      "eventIds": [],
      "abstract": "Video summaries of scientific publications have gained more and more popularity over the last years, requiring many researchers to familiarize themselves with the tools and techniques of video production which can be an overwhelming task. This paper introduces a video structuring framework embedded into the authoring tool Pub2Vid. The tool supports users with the creation of their video outline and script, providing real video examples and recommendations based on the analysis of 40 publication summarization videos which were rated in a user study with 68 participants. Following a four-tier evaluation methodology, the application's usability is assessed and improved via amateur and expert interviews, two rounds of usability tests and two case studies. It is shown that the tool and its recommendations are particularly useful for beginners due to the simple design and intuitive components as well as suggestions based on real video examples.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Vienna",
              "dsl": "Faculty of Computer Science"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Austrian Academy of Sciences",
              "dsl": "Austrian Centre for Digital Humanities and Cultural Heritage"
            }
          ],
          "personId": 117025
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Univ. of Vienna",
              "dsl": "Faculty of Computer Science "
            }
          ],
          "personId": 117028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "The Ohio State University",
              "dsl": "Faculty of Computer Science and Engineering at OSU"
            }
          ],
          "personId": 117026
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Vienna",
              "dsl": "Faculty of Computer Science and Data Science @ Uni Vienna"
            }
          ],
          "personId": 117050
        }
      ]
    },
    {
      "id": 117069,
      "typeId": 12519,
      "title": "Producing Personalised Object-Based Audio-Visual Experiences: an Ethnographic Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1049",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117285
      ],
      "eventIds": [],
      "abstract": "Developments in object-based media and IP-based delivery offer an opportunity to create superior audience experiences through personalisation. Towards the aim of making personalised experiences regularly available across the breadth of audio-visual media, we conducted a study to understand how personalised experiences are being created. This consisted of interviews with producers of six representative case studies, followed by a thematic analysis. We describe the workflows and report on the producers' experiences and obstacles faced. We found that the metadata models, enabling personalisation, were developed independently for each experience, restricting interoperability of personalisation affordances provided to users. Furthermore, the available tools were not effectively integrated into preferred workflows, substantially increasing role responsibilities and production time. To ameliorate these issues, we propose the development of a unifying metadata framework and novel production tools. These tools should be integrated into existing workflows; improve efficiency using AI; and enable producers to serve more diverse audiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Surrey",
              "city": "Guildford",
              "institution": "University of Surrey",
              "dsl": "CVSSP"
            }
          ],
          "personId": 117055
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Media City UK, Manchester",
              "institution": "BBC Research & Development",
              "dsl": ""
            }
          ],
          "personId": 117017
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Guildford",
              "institution": "University of Surrey",
              "dsl": "CVSSP"
            }
          ],
          "personId": 117022
        }
      ]
    },
    {
      "id": 117070,
      "typeId": 12519,
      "title": "A Dataset of Gaze and Mouse Patterns in the Context of Facial Expression Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1046",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117010
      ],
      "eventIds": [],
      "abstract": "Facial expression recognition is an important and challenging task for both the computer vision and affective computing communities, and even more specifically in the context of multimedia applications, where audience understanding is of particular interest. Recent data-oriented approaches have created the need for large-scale annotated datasets. However, most existing datasets present some weaknesses, because of the collecting methods used. In order to further highlight these issues, we investigate in this work how human visual attention is deployed when performing a facial expression recognition task. To do so, we carried out several complementary experiments, using the eye-tracking technology, as well as the BubbleView metaphor, both under laboratory and crowdsourcing settings. We show significant variations in gaze patterns depending on the emotion represented, but also on the difficulty of the task, i.e., whether the emotion is correctly recognised or not. Moreover, we use these results to propose recommendations on the ways to collect label data for facial expression recognition datasets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117042
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117046
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117051
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117034
        }
      ]
    },
    {
      "id": 117071,
      "typeId": 12519,
      "title": "Immersion or Disruption? Readers’ Evaluation of and Requirements for 3D-audio as a Tool to Support Immersion in Digital Reading Practices. ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1036",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117284
      ],
      "eventIds": [],
      "abstract": "In this paper, we aim to contribute to the understanding of how readers experience immersion in digital reading experiences, more specifically with digital reading supported by (3D-)audio tracks. We formulate user and content requirements for implementing\r\n(3D-)audio soundtracks for readers in a digital reading application. The main research question addressed in this paper is: (how) can audio aid the immersion of readers in digital fiction stories? To answer this question, three online focus group discussions were organised in Belgium and Germany. As part of the set-up of the Horizon Europe project Möbius, 18 participants tested different 3D-audio tracks while reading via the Thorium Reader application. The results first address how participants define immersion, and\r\nhow the role of audio in immersion can become paradoxical. Then, the paper presents a detailed evaluation of the factors en- or disabling immersion for the specific 3D-audio tracks, and how these insights can be implemented in reading apps via user and content\r\nrequirements.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Etterbeek",
              "institution": "Vrije Universiteit Brussel",
              "dsl": "imec-smit"
            }
          ],
          "personId": 117015
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Etterbeek",
              "institution": "imec-smit, Vrije Universiteit Brussel",
              "dsl": ""
            }
          ],
          "personId": 117031
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussels",
              "institution": "Vrije Universiteit Brussel",
              "dsl": "imec-smit"
            }
          ],
          "personId": 117038
        }
      ]
    },
    {
      "id": 117072,
      "typeId": 12519,
      "title": "LIFT - A System to Create Mixed 360° Video and 3D Content for Live Immersive Virtual Field Trip",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1047",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117286
      ],
      "eventIds": [],
      "abstract": "Our paper presents LIFT, a system that enables educators to create immersive virtual field trip experiences for their students. LIFT overcomes the challenges of enabling non-technical educators to create their own content and allowing educators to act as guides during the immersive experience. The system combines live-streamed 360° video, 3D models, and live instruction to create collaborative virtual field trips. To evaluate LIFT, we developed a field trip with biology educators from the University of Central Florida and showcased it at a science festival. Our results suggest that LIFT can help educators create immersive educational content while out in the field. However, our pilot observational study at the museum highlighted the need for further research to explore the instructional design of mixed immersive content created with LIFT. Overall, our work provides an application development framework for educators to create immersive, hands-on field trip experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 117035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Virtual and Augmented Reality Lab"
            }
          ],
          "personId": 117033
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Coastal and Estuarine Ecology Lab."
            }
          ],
          "personId": 117048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Biology"
            }
          ],
          "personId": 117037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arkansas",
              "city": "Little Rock",
              "institution": "University of Arkansas at Little Rock",
              "dsl": "Emerging Analytics Center"
            }
          ],
          "personId": 117052
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 117014
        }
      ]
    },
    {
      "id": 117073,
      "typeId": 12519,
      "title": "Referencing in YouTube Knowledge Communication Videos",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1014",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117285
      ],
      "eventIds": [],
      "abstract": "In recent years, there has been widespread concern about misinformation and hateful content on social media that are damaging societies. Being one of the most influential social media that practically serves as a new search engine, YouTube has accepted criticisms of being a major conduit of misinformation. However, it is often neglected that there exist communities on YouTube that aim to produce credible and informative content - usually falling under the educational category. One way to characterize this valuable content is to find references entailed to each video. While such citation practices function as a voluntary gatekeeping culture within the community, how they are actually done varies and remains unquestioned. Our study aims to investigate common citation practices in major knowledge communication channels on YouTube. After investigating 44 videos manually sampled from YouTube, we characterized two common referencing methods, namely bibliographies and in-video citations. We then selected 129 referenced resources, assessed and categorized their availability as being immediate, conditional, and absent. After relating the observed referencing methods to the characteristics of the knowledge communication community, we show that the usability of references could vary depending on viewers’ user profiles. Furthermore, we witnessed the use of rich-text technologies that can enrich the usability of online video resources. Finally, we discuss design implications for the platform to have a standardized referencing convention that can promote information credibility and improve user experience, especially valuable for the young audiences who tend to watch this content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne and Martigny",
              "institution": "EPFL and Idiap Research Institute",
              "dsl": ""
            }
          ],
          "personId": 117027
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny and Lausanne",
              "institution": "Idiap Research Institute and EPFL",
              "dsl": ""
            }
          ],
          "personId": 117060
        }
      ]
    },
    {
      "id": 117074,
      "typeId": 12519,
      "title": "An Integrated Framework for Understanding Multimodal Embodied Experiences in Interactive Virtual Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1022",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117283
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) technology enables ``embodied interactions'' in realistic environments where users can freely move and interact, with deep physical and emotional states. However, a comprehensive understanding of the embodied user experience is currently limited by the extent to which one can make relevant observations, and the accuracy at which observations can be interpreted. \r\n\r\nPaul Dourish proposed a way forward through the characterisation of embodied interactions in three senses: ontology, intersubjectivity, and intentionality. In a joint effort between computer and neuro-scientists, we built a framework to design studies that investigate multimodal embodied experiences in VR, and apply it to study the impact of simulated low-vision on user navigation. Our methodology involves the design of 3D scenarios annotated with an ontology, modelling intersubjective tasks, and correlating multimodal metrics such as gaze and physiology to derive intentions. We show how this framework enables a more fine-grained understanding of embodied interactions in behavioural research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Biot",
              "institution": "Centre Inria d'Université Côte d'Azur",
              "dsl": "Biovision"
            },
            {
              "country": "France",
              "state": "",
              "city": "Biot",
              "institution": "Université Côte d'Azur",
              "dsl": "I3S, SPARKS"
            }
          ],
          "personId": 117044
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Biot",
              "institution": "Centre Inria d'Université Côte d'Azur",
              "dsl": "Biovision"
            }
          ],
          "personId": 117039
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Sophia Antipolis",
              "institution": "Université Côte d'Azur",
              "dsl": "I3S"
            }
          ],
          "personId": 117059
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nice",
              "institution": "Université Côte d'Azur",
              "dsl": "LAHMESS"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université",
              "dsl": "INSERM, CNRS, Institut de la Vision"
            }
          ],
          "personId": 117016
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nice",
              "institution": "Université Côte d'Azur, CHU de Nice",
              "dsl": "CoBTeK"
            }
          ],
          "personId": 117061
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Sophia Antipolis",
              "institution": "Université Nice Sophia Antipolis",
              "dsl": "Polytech/Equipe SPARKS/I3S"
            }
          ],
          "personId": 117023
        }
      ]
    },
    {
      "id": 117075,
      "typeId": 12519,
      "title": "More Immersed but Less Present: Unpacking Factors of Presence Across Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1012",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117010
      ],
      "eventIds": [],
      "abstract": "The production of immersive media often involves 360-degree viewing on mobile or immersive VR devices, particularly in the field of immersive journalism. However, it is unclear how the different technologies used to present such media affect the experience of presence. To investigate this, a laboratory experiment was conducted with 87 participants who were assigned to one of three conditions: HMD-360, Monitor-360, or Monitor-article, representing three distinct levels of technological immersion. All three conditions represented the same base content, with high and mid-immersion featuring a panoramic 360-video and low-immersion presenting an article composed of a transcript and video stills.\r\n\r\nThe study found that presence could be considered a composite of Involvement, Naturalness, Location, and Distraction. Mid- and high-immersion conditions elicited both higher Involvement and higher Distraction compared to low immersion. Furthermore, the participants’ propensity for psychological immersion maximized the effects of technological immersion, but only through the aspect of Involvement. In conclusion, the study sheds light on how different technologies used to present immersive media affect the experience of presence and suggests that higher technological immersiveness does not necessarily result in a higher reported presence.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": "Gamification Group"
            }
          ],
          "personId": 117043
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "National Defence University",
              "dsl": ""
            }
          ],
          "personId": 117047
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Tampere",
              "institution": "Tampere University",
              "dsl": ""
            }
          ],
          "personId": 117030
        }
      ]
    },
    {
      "id": 117076,
      "typeId": 12519,
      "title": "Accessibility Research in Digital Audiovisual Media: What Has Been Achieved and What Should Be Done Next?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23a-1009",
      "source": "PCS",
      "trackId": 12239,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117284
      ],
      "eventIds": [],
      "abstract": "The consumption of digital audiovisual media is a mainstay of many people's lives. However, people with accessibility needs often have issues accessing this content. With a view to addressing this inequality, there exists a wide range of interventions that researchers have explored to bridge this accessibility gap. Despite this work, our understanding of the capability of these interventions is poor. In this paper, we address this through a systematic review of the literature, creating a dataset of and analysing N=181 scientific papers. We have found that certain areas have accrued a disproportionate amount of attention from the research community -- for example, blind and visually impaired and d/Deaf and hard of hearing people account for 93.9% of papers (N=170). We describe challenges researchers have addressed, end-user communities of focus, and interventions examined. We conclude by evaluating gaps in the literature and areas that could use more focus on in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117018
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117013
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117040
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 116930
        }
      ]
    },
    {
      "id": 117226,
      "typeId": 12521,
      "title": "Validating Objective Evaluation Metric: Is Fréchet Motion Distance able to Capture Foot Skating Artifacts ?",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596460"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1009",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "Automatically generating character motion is one of the technologies required for virtual reality, graphics, and robotics. Motion synthesis with deep learning is an emerging research topic. A key component of the development of such an algorithm involves the design of a proper objective metric to evaluate the quality and diversity of the synthesized motion dataset, two key factors of the performance of generative models. The Fréchet distance is nowadays a common method to assess this performance. In the motion generation field, the validation of such evaluation methods relies on the computation of the Fréchet distance between embeddings of the ground truth dataset and motion samples polluted by synthetic noise to mimic the artifacts produced by generative algorithms. However, the synthetic noise degradation does not fully represent motion perturbations that are commonly perceived. One of these artifacts is foot skating: the unnatural foot slides on the ground during locomotion. In this work-in-progress paper, we tested how well the Fréchet Motion Distance (FMD), which was proposed in previous works, is able to measure foot skating artifacts, and we found that FMD is not able to measure efficiently the intensity of the skating degradation. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": "ISIA Lab"
            }
          ],
          "personId": 117141
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": ""
            }
          ],
          "personId": 117083
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117211
        }
      ]
    },
    {
      "id": 117227,
      "typeId": 12521,
      "title": "A Social Awareness Interface for Helping Immigrants Maintain Connections to Their Families and Cultural Roots: The Case of Venezuelan Immigrants",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596461"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1048",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "International migration forces people into an unfamiliar reality in which their customs and values lose relevance. Moreover, former relationships are left behind, which makes immigrants more likely to experience loneliness. This study focuses particularly on Venezuelan immigrants by incorporating cultural aspects into a solution aimed at reducing loneliness and increasing social connectedness. Among Venezuelans, coffee is a staple of their daily routine and their favorite social beverage. We propose KEPEIN, a coffee maker-shaped interface to transfer a sense of presence and share coffee over distance. Through an experimental study, we evaluated the user’s perception and reaction when communicating through the interface. The results show potential added value to communication by including KEPEIN in a traditional remote interaction scenario. We discuss the benefits and limitations of this type of tangible communication interface and the importance of incorporating culture into the design of solutions for immigrants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "La Sapienza University Of Rome",
              "dsl": "Faculty of Architecture, School of Product design"
            }
          ],
          "personId": 117097
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 117176
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 117137
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "La Sapienza University Of Rome",
              "dsl": "Department of Planning, Design and Architecture technology"
            }
          ],
          "personId": 117167
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "RM",
              "city": "Rome",
              "institution": "Sapienza University of Rome",
              "dsl": ""
            }
          ],
          "personId": 117115
        }
      ]
    },
    {
      "id": 117228,
      "typeId": 12521,
      "title": "Enhancing VR Gaming Experience using Computational Attention Models and Eye-Tracking",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1047",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "This study explores the potential of enhancing interaction experiences, such as virtual reality (VR) games, through the use of computational attention models. Our proposed approach utilizes a saliency map generated by attention models to dynamically adjust game difficulty levels and to help in the game level design, resulting in a more immersive and engaging experience for users. To inform the development of this approach, we present an experimental setup that is able to collect data in a VR environment and intends to be able to validate the adaptation of attention models to this domain. Through this work, we aim to create a framework for VR game design that leverages attention models to offer a new level of immersion and engagement for users. We believe our contributions have significant potential to enhance VR experiences and advance the field of game design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "ISIA Lab, Faculty of Engineering, University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117225
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "Faculty of Engineering, University of Mons",
              "dsl": "ISIA LAB"
            }
          ],
          "personId": 117199
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117084
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "Faculty of Engineering, University of Mons",
              "dsl": "ISIA LAB"
            }
          ],
          "personId": 117147
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117107
        }
      ]
    },
    {
      "id": 117229,
      "typeId": 12521,
      "title": "Enabling and Understanding Interactive Social VR360 Video Viewing",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3597216"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1046",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "This paper reports on the research being done towards enabling and understanding interactive social VR360 video viewing scenarios, by exclusively relying on web-based technologies, and using different types of consumption devices. After motivating the relevance of the research topic and associated impact, the paper elaborates on key requirements, features, and system components to effectively enable such scenarios, such as: adaptive and low-latency streaming, media synchronization, social presence, interaction channels, and assistive methods. For each of these features and components, different alternatives are assessed and proof of concept implementations are being provided. With an effective combination and integration of all these contributions, an end-to-end platform can be built and used as a research framework to explore the applicability and potential benefits of social VR360viewing in a variety of use cases, like education, culture or surveillance, by tailoring the technological components based on lessons learned from experimental studies. These use case studies can also provide relevant insights into activity patterns, behaviors, and preferences in Social Viewing scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT Foundation",
              "dsl": ""
            }
          ],
          "personId": 117133
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": " i2cat Foundation",
              "dsl": "Media & Internet Area"
            }
          ],
          "personId": 117124
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT",
              "dsl": ""
            }
          ],
          "personId": 117212
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT Foundation",
              "dsl": ""
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Politècnica de Catalunya (UPC)",
              "dsl": ""
            }
          ],
          "personId": 117080
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT",
              "dsl": ""
            }
          ],
          "personId": 117119
        }
      ]
    },
    {
      "id": 117230,
      "typeId": 12521,
      "title": "Navigating Full-Motion Video: Emerging Design Patterns for Parameterized Replay Stories",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1045",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "The 1980s saw full-motion video (FMV) titles inaugurate design patterns for a new genre of interactive digital narrative (IDN). Recently, this genre made a resurgence in popularity. Despite the intervening years, FMV’s design conventions remain tightly coupled to the affordances of laserdisc technology. This paper employs IDN affordances and aesthetics as a lens to examine modern FMV games, namely the recent works of Wales Interactive. Also, this paper leverages research on the emerging conventions of Timeline--an authoring platform for tightly parallel, parameterized stories--to address the challenges of FMV design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 117109
        }
      ]
    },
    {
      "id": 117231,
      "typeId": 12521,
      "title": "Design of an Assistance Tool for Analyzing and Modeling the Activity of Trainers in Professional Training Situations through Simulation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596475"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1007",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "Human audience analysis is crucial in numerous domains to understand people's behavior based on their knowledge and environment. In this paper we focus on simulation-based training which has become a popular teaching approach that requires a trainer able to manage a lot of data at the same time. We present tools that are currently being developed to help trainers from two different fields: training for practical teaching gestures and training in civil defense. In this sense, three technological blocks are built to collect and analyze data about trainers’ and learners’ gestures, gaze, speech and movements. The paper also discusses the future work planned for this project, including the integration of the framework into the Noldus system and its use in civil security training. Overall, the article highlights the potential of technology to improve simulation-based training and provides a roadmap for future development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117112
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117163
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117188
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117118
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117110
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117168
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117084
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117107
        }
      ]
    },
    {
      "id": 117232,
      "typeId": 12521,
      "title": "Enhancing Arabic Content Generation with Prompt Augmentation Using Integrated GPT and Text-to-Image Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1006",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117286
      ],
      "eventIds": [],
      "abstract": "With the advances in text-to-image synthesis it has become critical to design prompts that make the best guides them to generate the most desirable images, hence, emerged the concept of prompt engineering. Here, we propose a framework for enhancing text-to-image models for the Arabic culture by integrating them with the generative pre-trained transformer to generate multiple prompts for the Arabic culture. In the proposed method, a simple initial prompt is used to generate multiple, more detailed prompts from multiple categories through in-context learning. The augmented prompts are then used as input to a text-to-image model to generate images enhanced for the Arabic culture. We perform multiple experiments with a number of participants to evaluate the performance of the proposed method, which shows promising results, specially for generating prompts that are more inclusive of the different Arabic countries and with a wider variety in terms of image subjects, where we find that our proposed method generates image with more variety 85 %  of the time and are more inclusive of the Arabic countries more than 72.66 %  of the time, compared to the direct approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "Hamad Bin Khalifa University",
              "dsl": "college of science and engineering"
            }
          ],
          "personId": 117209
        },
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "Hamad Bin Khalifa University",
              "dsl": "college of science and engineering"
            }
          ],
          "personId": 117164
        },
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "HBKU",
              "dsl": "Qatar Computing Research Institute"
            }
          ],
          "personId": 117150
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Department of Electronic & Computer Engineering"
            }
          ],
          "personId": 117160
        }
      ]
    },
    {
      "id": 117233,
      "typeId": 12521,
      "title": "A Quality of Experience Evaluation of an Interactive Multisensory 2.5D Virtual Reality Art Exhibit",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3597214"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1049",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117323
      ],
      "eventIds": [],
      "abstract": "In recent years, museums have become more interactive and immersive through the adaptation of technology within large scale art exhibitions. Due to these changes, new types of cultural experiences are more appealing to a younger audience. Despite these positive changes, some museum experiences are still primarily focused on visual art experiences, which remain out of reach to those with visual impairments. Such unimodal and visual dominated experiences restrict these users who depend on sensory feedback to experience the world around them. \r\nIn this paper, the authors propose a novel VR experience which incorporates multisensory technologies. It allows individuals to engage and interact with a visual artwork museum experience presented as a fully immersive VR environment. Users can interact with virtual paintings and trigger sensory zones which deliver multisensory feedback to the user. These sensory zones are unique to each painting, presenting thematic audio and smells, custom haptic feedback to feel the artwork, and lastly air, light and thermal changes in an effort to engage those with visual impairments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon: Midlands Midwest",
              "dsl": "Department of Engineering and Informatics"
            }
          ],
          "personId": 117206
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon: Midlands Midwest",
              "dsl": "Department of Engineering and Informatics"
            },
            {
              "country": "Ireland",
              "state": "Westmeath",
              "city": "Athlone",
              "institution": "Athlone Institute of Technology",
              "dsl": "Electronics & Informatics"
            }
          ],
          "personId": 117090
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon: Midlands Midwest",
              "dsl": "Department of Engineering and Informatics"
            },
            {
              "country": "Ireland",
              "state": "Co. Westmeath",
              "city": "Athlone",
              "institution": "Athlone Institute of Technology",
              "dsl": "Department of Engineering and Informatics"
            }
          ],
          "personId": 117185
        }
      ]
    },
    {
      "id": 117234,
      "typeId": 12521,
      "title": "Generating Utterances for Companion Robots using Television Program Subtitles",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596463"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1005",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "This study presents a method for generating utterances for companion robots that watch TV with people, using TV program subtitles. To enable the robot to automatically generate relevant utterances while watching TV, we created a dataset of approximately 12,000 utterances that were manually added to the collected TV subtitles. Using this dataset, we fine-tuned a large-scale language model to construct an utterance generation model. The proposed model generates utterances based on multiple keywords extracted from the subtitles as topics, while also taking into account the context of the subtitles by inputting them. The evaluation of the generated utterances revealed that approximately 88% of the sentences were natural Japanese, and approximately 75% were relevant and natural in the context of the TV program. Moreover, approximately 99% of the sentences contained the extracted keywords, indicating that our proposed method can generate diverse and contextually appropriate utterances containing the targeted topics. These findings provide evidence of the effectiveness of our approach in generating natural utterances for companion robots that watch TV with people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117096
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117195
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK（Japan Broadcasting Corporation）",
              "dsl": ""
            }
          ],
          "personId": 117208
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK（Japan Broadcasting Corporation）",
              "dsl": ""
            }
          ],
          "personId": 117114
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117152
        }
      ]
    },
    {
      "id": 117235,
      "typeId": 12521,
      "title": "Video Consumption in Context: Influence of Data Plan Consumption on QoE",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596474"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1055",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "User expectations are one of the main factors on providing satisfactory QoE for streaming service providers. Measuring acceptability and annoyance of video content, therefore, provide a valuable insight when measured under a given context. In this ongoing work, we measure video QoE in terms of acceptability and annoyance for the remaining data in a mobile data plan context.. We show that simple logos can be used during the experiment to prompt the context to subjects and the different context levels may impact the user expectations and consequently their satisfactions. Finally, we show that objective metrics can be used to determine the acceptability and annoyance thresholds for a given context.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes University",
              "dsl": ""
            }
          ],
          "personId": 117193
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes University",
              "dsl": ""
            }
          ],
          "personId": 117219
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "California",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 117223
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "California",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 117138
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes University",
              "dsl": ""
            }
          ],
          "personId": 117134
        }
      ]
    },
    {
      "id": 117236,
      "typeId": 12516,
      "title": "Towards the Creation of Tools for Automatic Quality of Experience Evaluation with Focus on Interactive Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1014",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "This paper contains the research proposal of Juan Antonio De Rus presented at the IMX 23 Doctoral Symposium. Virtual Reality (VR) applications are already used to support diverse tasks such as online meetings, education, or training, and the usages grow every year.  To enrich the experience VR scenarios, include multimodal content (video, audio, text, synthetic content) and multi-sensory stimuli are typically included. Tools to evaluate the Quality of Experience (QoE) of such scenarios are needed. Traditional tools used to evaluate the QoE of users performing any kind of task typically involves surveys, user testing or analytics. However, these methods provide limited insights for our tasks with VR and have shortcomings and a limited scalability. In this doctoral study we have formulated a set of open research questions and objectives on which we plan to generate contributions and knowledge in the field of Affective Computing (AC) and Multimodal Interactive Virtual Environments. Hence, in this paper we present a set of tools we are developing to automatically evaluate QoE in different use cases. They include dashboards to monitor in real time reactions to different events in the form of emotions and affections predicted by different models based on physiological data, as well as the creation of a dataset for AC and its associated methodology. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Valencia",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117131
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": " i2cat Foundation",
              "dsl": "Media & Internet Area"
            }
          ],
          "personId": 117124
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117210
        }
      ]
    },
    {
      "id": 117237,
      "typeId": 12521,
      "title": "Subjective Test Environments: A Multifaceted Examination of Their Impact on Test Results",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596470"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1054",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "Quality of Experience (QoE) in video streaming scenarios is significantly affected by the viewing environment and display device. Understanding and measuring the impact of these settings on QoE can help develop viewing environment-aware metrics and improve the efficiency of video streaming services. In this ongoing work, we conducted a subjective study in both laboratory and home settings using the same content and design to measure QoE in Degradation Category Rating (DCR). We first analyzed subject inconsistency and confidence intervals of the Mean Opinion Scores (MOS) between the two settings. We then used statistical models such as ANOVA and t-test to analyze the differences in subjective tests on video quality between the two viewing environments. Additionally, we employed the Eliminated-By-Aspects (EBA) model to quantify the influence of different settings on the measured QoE. We conclude with several research questions that could be further explored to better understand the impact of the viewing environment on QoE.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "Loire-Atlantique",
              "city": "Nantes",
              "institution": "Nantes university",
              "dsl": ""
            }
          ],
          "personId": 117116
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes University",
              "dsl": ""
            }
          ],
          "personId": 117193
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université, CAPACITÉS SAS",
              "dsl": ""
            }
          ],
          "personId": 117078
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "LS2N/université de Nantes",
              "dsl": ""
            }
          ],
          "personId": 117034
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon Prime Video",
              "dsl": ""
            }
          ],
          "personId": 117093
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Amazon Prime Video",
              "dsl": ""
            }
          ],
          "personId": 117123
        }
      ]
    },
    {
      "id": 117238,
      "typeId": 12516,
      "title": "Analysing and Understanding Embodied Interactions in Virtual Reality Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1015",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) offers opportunities in human-computer interaction research, to embody users in immersive environments and observe how they interact with 3D scenarios under well-controlled environments. VR content has stronger influences on users physical and emotional states as compared to traditional 2D media, however, a fuller understanding of this kind of embodied interaction is currently limited by the extent to which attention and behavior can be observed in a VR environment, and the accuracy at which these observations can be interpreted as, and mapped to, real-world interactions and intentions. This thesis aims at the creation of a system to help designers in the understanding of the embodied user experience in VR environment: how they feel, what is their intentions when interacting with a certain object, provide them guidance based on their needs and attention. Controlled environment guiding the user will help to establish a better intersubjectivity between the designer building an experience, and the user living it, leading to more efficient behaviour analysis in VR systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Biot",
              "institution": "Centre Inria d'Université Côte d'Azur",
              "dsl": "Biovision"
            },
            {
              "country": "France",
              "state": "",
              "city": "Biot",
              "institution": "Université Côte d'Azur",
              "dsl": "I3S, SPARKS"
            }
          ],
          "personId": 117044
        }
      ]
    },
    {
      "id": 117239,
      "typeId": 12521,
      "title": "ICAMUS: Evaluation Criteria of an Interactive Multisensory Authoring Tool",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1053",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117323
      ],
      "eventIds": [],
      "abstract": "This paper presents the advancement of the PRIM project that aims at giving the power to non-computer experts to create digital and interactive scenarios. For this purpose, we explored the strengths and limitations of the visual programming languages and the mulsemedia editors chosen for their ease of use. Results led to a criteria list that our final solution should meet, named the ICAMUS criteria for Interface, Combinatorics, Affordance, Modularity, Ubiquity, and Synoptic. This paper proposes a scale based on the ICAMUS criteria that may assess the Interactive and Multisensory Authoring Tools (IMAT scale). Last, this paper discusses how to compute a score based on three metrics (presence/absence of elements, number of clicks to do an action, and time needed to do this action) and the visual representation of this score that has to give a complete profile of the tool. We hypothesize that this scale will be able to highlight the complementarities of visual programming languages and mulsemedia editors as well as the challenges to face.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116874
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "ENIB",
              "dsl": "Lab-STICC laboratory"
            }
          ],
          "personId": 117086
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116901
        }
      ]
    },
    {
      "id": 117240,
      "typeId": 12521,
      "title": "Enhancing Emotional Awareness and Regulation in Movies and Music Based on Personality ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596462"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1052",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "Music and movies are powerful art forms that elicit deep feelings and emotions and help us reflect on our and other people’s lives on subjects such as: dreams, mental states, routines, society and culture. The evolution of technology has been easing access to these forms of entertainment and education for everyone, everywhere in the world. Given the easy and frequent interaction with a huge amount of movies and music daily, and the impact of these in our emotions, it becomes more and more relevant to address and think of ways to augment people’s emotional perception and awareness of multimedia content in and through movies and music. In this paper, we present the motivation and describe the background for these challenges, and propose an approach for the design, development and expansion of interactive features that allow users to visualize and access emotions felt while engaging with movies and music. A special focus is put on the content in these forms of entertainment that have in some way meant something or can be associated with a significant memory, providing insights and helping to manage and regulate emotions, allowing to revisit content with an increased awareness or even recommend new content, taking into account users’ personality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 117174
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 116890
        }
      ]
    },
    {
      "id": 117241,
      "typeId": 12864,
      "title": "Challenges of ML methods for video enhancement at scale",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23c-1002",
      "source": "PCS",
      "trackId": 12241,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117295
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nevada",
              "city": "Las Vegas",
              "institution": "Meta",
              "dsl": ""
            }
          ],
          "personId": 117117
        }
      ]
    },
    {
      "id": 117242,
      "typeId": 12864,
      "title": "Artificial intelligence for content creators",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23c-1000",
      "source": "PCS",
      "trackId": 12241,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117294
      ],
      "eventIds": [],
      "abstract": "This presentation is about Jellysmack, a company that helps creators with the technical aspects of video creation, editing and publishing. It will focus on the artificial-powered solutions developed within the AI Lab of the company. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Levallois-Perret",
              "institution": "Jellysmack",
              "dsl": "AI Lab"
            }
          ],
          "personId": 117213
        }
      ]
    },
    {
      "id": 117243,
      "typeId": 12521,
      "title": "Identifying The Developmental Challenges Of Creating Virtual Reality Theatre ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1059",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "Virtual Reality Theatre is continue to grow as a form of digital creative output for theatre practitioners. However, an understanding of the\r\ncommon challenges being faced during the development of productions by practitioners and the barriers to entry produced by the\r\ncomplexity of platforms is undocumented. This paper provides an in depth analysis of several challenges identified through semi structured\r\ninterviews of practitioners and a thematic review.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Theatre, Film, Television and Interactive Media"
            }
          ],
          "personId": 117121
        }
      ]
    },
    {
      "id": 117244,
      "typeId": 12521,
      "title": "On Legal and Ethical Challenges of Automatic Facial Expression Recognition: An Exploratory Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1015",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "Automatic facial expression recognition (FER) has a lot of potential applications. However, even if it can be beneficial for some areas, e.g. security and healthcare, several legal and ethical challenges arise. In this article, we first present such challenges related to the deployment of FER. Then, we introduce the conduct of a focus group which allowed to highlight interesting points regarding the use of FER in a medical context. Particularly, transparency, data management, diagnoses, liability, best endeavours obligation, and non-discrimination principle are debated. We finally discuss on our study's limitations and directions for future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117145
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": ""
            }
          ],
          "personId": 117100
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117046
        }
      ]
    },
    {
      "id": 117245,
      "typeId": 12516,
      "title": "Ia-Human Collaboration for in Situ Interactive Exploration of Behaviors From Immersive Environment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1010",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "Experiments in immersive environments enables the capture of massive amount of data that is closely related to individual behaviour. The recording of such experiments enables the complex study for under-constrained tasks, ie. tasks that allow high contingency in their resolution. Whilst this contingency should allow for better discrimination between individual behaviors, the high complexity of the tasks implies difficulties in the analysis.\r\n\r\nMy thesis aims to discuss the benefits of Immersive Analytics (Visual Analytics in immersive environment) for the analysis of hybrid sequential data (trajectory and events) generated in immersive environment. The high contingency for the completion of tasks requires the analysis to take place in a very high level of abstraction. The massive amount of data that could be generated stresses the need for the construction of a model that would be able to extract features on a high level of abstraction. \r\nSince the scheme of exploration is unknown before-hand, both visual representations of the data and the model should provide great interactivity and adaptability in order to be able to follow the analyst queries in their seek for new insights within the data. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N"
            }
          ],
          "personId": 117207
        }
      ]
    },
    {
      "id": 117246,
      "typeId": 12516,
      "title": "Towards Distributed and Interactive Multi-cam and Multi-device VR360 Video Experiences",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1011",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "The production and consumption of multimedia content is continuously increasing, and this particularly affects to immersive formats, like VR360 video. Even though significant advances have been witnessed with regard to the processing, delivery and consumption of interactive VR360 video, key challenges and research questions still need to be addressed to efficiently provide interactive multi-camera and multi-user VR360 video services over distributed and heterogeneous environments. This research work aims at providing novel and efficient contributions to overcome existing limitations in this topic. First, it will develop an end-to-end modular web-based VR360 video platform, including the measurement of Quality of Service (QoS) and activity metrics, to be used as a research testbed. Second, it will provide lightweight yet efficient viewport-aware video processing and delivery strategies to dynamically concentrate the video resolution on the user’s viewport, with a single stream and decoding process through the web browser. Third, it will propose innovative encoding, signaling and synchronization solutions to enable an effective support for multi-camera and multi-device VR360 services, in a synchronized manner and with the lowest latency possible. Fourth, it will explore how to effectively provide social viewing scenarios between remote users while watching the same or related VR360 videos, assisted with interactive and guiding techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT Foundation",
              "dsl": ""
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Politècnica de Catalunya",
              "dsl": "Department of Network Engineering"
            }
          ],
          "personId": 117133
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": " i2cat Foundation",
              "dsl": "Media & Internet Area"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117124
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "i2CAT Foundation",
              "dsl": ""
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Politècnica de Catalunya (UPC)",
              "dsl": ""
            }
          ],
          "personId": 117080
        }
      ]
    },
    {
      "id": 117247,
      "typeId": 12516,
      "title": "Artificial Intelligence Techniques for Quality Assessments of Immersive Multimedia",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1012",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "Artificial Intelligence techniques are gaining traction in quality assessments of immersive multimedia, such as virtual and augmented reality applications. The immersive nature of these applications poses a unique challenge to traditional quality assessment methods. In fact, estimating the user acceptance of these immersive technologies is complex because of multiple aspects, such as usability, enjoyment, and cybersickness. AI-based approaches offer a promising solution to this scope by enabling objective evaluations of immersive multimedia such as spatial audios, point clouds, and light field images. This work details an overview of various AI techniques that have been used for quality assessments of immersive multimedia, including machine learning algorithms, deep learning, and computer vision. Advantages of these techniques are provided with some examples of how they have been applied in practice. Possible future works are presented in this under-researched problem, underlining the possible outcomes of a Ph.D. in this field. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "Roma Tre University",
              "dsl": "Department of Industrial, Electronics, and Mechanical Engineering"
            }
          ],
          "personId": 117126
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "University of Roma Tre",
              "dsl": ""
            }
          ],
          "personId": 117155
        }
      ]
    },
    {
      "id": 117248,
      "typeId": 12521,
      "title": "Survey on the Impact of Listening to Audio for Adaptive Japanese Subtitles and Captions Ruby",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596456"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1012",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117286
      ],
      "eventIds": [],
      "abstract": "Subtitles and closed captions, which are prepared for hearing-impaired users, are now widely used by users without hearing concerns. In this paper, we focus on the adaptation of subtitles and captions for non-hearing-impaired users, particularly the adaptation of the kanji ruby. From our experiments on non-hearing-impaired adults, Welch's t-test was used to clarify whether listening to audio with the same content affects the necessity of kanji ruby. In addition, we proposed and evaluated an adaptive model to predict whether ruby should be added to kanji captions based on the experimental results. The experimental results suggest that not only the difficulty of the kanji and the user's kanji ability, but also the content of the audio is important for the optimization of kanji ruby.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117216
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117159
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117157
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya",
              "institution": "NHK",
              "dsl": ""
            }
          ],
          "personId": 117194
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117129
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": "Internet Service Systems Research Division / Science & Technology Research Laboratories"
            }
          ],
          "personId": 117205
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Setagaya, Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": "Science and Technology Research Labs"
            }
          ],
          "personId": 116855
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Setagaya-ku",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 117204
        }
      ]
    },
    {
      "id": 117249,
      "typeId": 12516,
      "title": "Objective Metrics Definition for QoE Assessment for Extended Reality Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1013",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "The increasing development of Virtual Reality and Augmented Reality technologies opens new research perspectives in a wide variety of fields. While for traditional multimedia content, quality assessment has been extensively investigated, for these innovative technologies different issues need to be addressed. More specifically, Quality of Experience assessment needs to take into account different aspects, which concern, on one hand, the quality of the displayed multimedia content, and, on the other hand, human factors. Due to its inherent subjectivity, the definition of objective metrics for Quality of Experience is definitely not a trivial task. This paper aims at framing the problem of objective Quality of Experience assessment and proposing different research paths that should be followed in this field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "Roma Tre University",
              "dsl": "Department of Industrial, Electronic and Mechanical Engineering"
            }
          ],
          "personId": 117220
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "University of Roma Tre",
              "dsl": ""
            }
          ],
          "personId": 117155
        }
      ]
    },
    {
      "id": 117250,
      "typeId": 12521,
      "title": "A VR Intervention Based on Social Story™ ‌to Develop Social Skills in Children with ASD",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596459"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1017",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "Social interactions and communication play a crucial role in people’s lives. Those with autism spectrum disorder (ASD), especially children, may have difficulties participating in social interactions. Such challenges can be characterised by displaying atypical behaviours and limited sharing intention in social settings. Sharing is an important part of social interaction, and a lack of awareness or limited willingness to share undermines the development of social skills. These characteristics may be related to the impaired theory of mind (ToM). This means that it is difficult to understand people’s wishes and feelings. A range of interventions have been created to help develop social communication skills. The Social Story™ intervention is one such example, and it provides clear visual narratives to explain social situations and concepts to help children with ASD. The narratives provide a mechanism to visually communicate typical communication behaviours. The social story intervention approach is book-based. As such, it is dependent on a reader to communicate well the concepts and demands a certain level with respect to the listener’s imagination capacity. With the limitation of the paper-based medium in mind, this work-in-progress paper outlines the steps, approach, and end application to translate the Social Story™ into a virtual reality (VR) experience. The Social Story™ experience in VR potentially offers a more interactive, immersive and flexible intervention. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon",
              "dsl": "Computer & Software Engineering"
            }
          ],
          "personId": 117087
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon",
              "dsl": "Computer & Software Engineering"
            }
          ],
          "personId": 117185
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Westmeath",
              "city": "Athlone",
              "institution": "Technological University of the Shannon",
              "dsl": "Computer & Software Engineering"
            }
          ],
          "personId": 117090
        }
      ]
    },
    {
      "id": 117251,
      "typeId": 12516,
      "title": "Nothing Beside Remains",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1008",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "An existing body of Human-Computer Interaction (HCI) work has had a focus on cultural heritage settings such as historic sites and museums where digital technologies are used to augment user experience through a variety of methods such as interactive displays, museum guides and mixed/virtual reality experiences. However, very little of this work has been used to explore some of the more contemporary and pressing issues surrounding museums and heritage today. This project, Nothing Beside Remains explores the nature of narratives and stories in the context of museums and heritage through the creation of tangible interactive digital artefacts. Specifically, it looks at ‘Contested Histories’ - a broad spectrum of issues relating to heritage sites and museum artefacts that in recent years have become more urgent to address. These can include issues such as heritage loss (e.g., through decay, destruction or theft either through nature or human conflict), unsustainable preservation and conservation practice (the continuing accumulation of historic artefacts, lack of financial and specialist resources in maintaining sites and artefacts) and repatriation (the disputed claims by nations in reclaiming artefacts that were looted often due to colonial activity). Design (and additionally Speculative Design) as an approach to these issues allows individuals or groups to imagine radically different futures or pose questions relating to phenomena through the convergence of creativity and critical theory – therefore this work aims to extend how this kind of critical enquiry can pose questions to museum and heritage sectors as well as other stakeholders, surrounding contentious narratives and propose possible futures. It involves the creation of physical digital artefacts and tests their effectiveness in creating debate and discussion surrounding these issues. In addition, it will look at the agency of museum and heritage organisations to address these problems and explore experimental design work as a vehicle for actionable change.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "School of Arts and Creative Technologies"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "School of Arts and Creative Technologies"
            }
          ],
          "personId": 117172
        }
      ]
    },
    {
      "id": 117252,
      "typeId": 12516,
      "title": "Construction of immersive and interactive methodology based on physiological indicators to subjectively and objectively assess comfort and performances in work offices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1009",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "The building sector and the indoor environment conception is undergoing major changes. There is a need to reconsider the way offices are built from a user's centric point of view. Research has shown the influence of perceived comfort and satisfaction on performances in the workplace. By understanding how multi sensory information is integrated into the nervous system and which environmental parameters influence the most perception, it could be possible to improve work environments. With the emergence of new virtual reality (VR) and augmented reality (AR) technologies, the collection and processing of sensory information is rapidly advancing, moving forward more dynamic aspects of sensory perception. Through simulated environments, environmental parameters can be easily manipulated at reasonable costs, allowing to control and guide the user’s sensory experience. Moreover, effects of contextual and surrounding stimuli on users can be easily collected throughout the test, in the form of physiological and behavioral data. Through the use of indoor simulations, this doctoral research goal is to develop a multi-criteria comfort scale based on physiological indicators under performance constraints. In doing this, it would be possible to define new quality indicators combining the different physical factors adapted to the uses and space. In order to achieve the objectives of this project, a first step is to develop and validate an immersive and interactive methodology for the assessment of multisensory information on comfort and performances in work environments.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Centre Scientifique et Technique du Bâtiment",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117190
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Centre Scientifique et Technique du Bâtiment",
              "dsl": ""
            }
          ],
          "personId": 117222
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N"
            }
          ],
          "personId": 117105
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N/Polytech Nantes"
            }
          ],
          "personId": 117134
        }
      ]
    },
    {
      "id": 117253,
      "typeId": 12516,
      "title": "Quality Assessment of Video Services in the Long Term",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1003",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "In traditional subjective video quality experiments, the presented sequences are short and quality ratings are based on a single interaction with a service (i.e., one session). However, in real-life scenarios, users interact with a video service for a longer period of time. If decisions are made, such as to abandon a service, they are formulated based on longitudinal multi-episodic interaction. Therefore, it is important to better understand how quality is perceived in a longer interaction and how quality perception is linked to behavioral implications. My PhD work encompasses a longitudinal study of users' interactions with a video service using a mobile device. In our study, which consists of six phases, we use different study designs to investigate how users perceive quality in a more ecologically valid setting. The study is carried out using a previously validated setup, which consists of compression software and a mobile application.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "malopolskie",
              "city": "Kraków",
              "institution": "AGH",
              "dsl": ""
            }
          ],
          "personId": 117189
        }
      ]
    },
    {
      "id": 117254,
      "typeId": 12521,
      "title": "Interconnecting Personal assistants and TVs a friendly approach to connect generations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1021",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "Technology has been a link between people, and the use of cell phones, tablets and other devices contribute to reduce distances. However, while these devices can unite, they can also result in exclusion. That's what happens to a lot of seniors. This condition makes social isolation in this age group one of the biggest world's problems. Due to this reason, thinking of solutions aimed at the social integration of this public is extremely important to reduce the indicators of loneliness. In this study, we used a system that combines an Intelligent Personal Assistant (IPA) and interactive television (iTV) to verify if this type of approach can facilitate and promote the realization of audio calls to friends, family and caregivers. The prototype was used for seven days in a real context with people between 64 and 90 years old. In general, the acceptance of the system was quite positive and changed the routine of the participants in some way during the testing period. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Arts"
            }
          ],
          "personId": 117142
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 116875
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Dept. Communication and Arts"
            }
          ],
          "personId": 117182
        }
      ]
    },
    {
      "id": 117255,
      "typeId": 12516,
      "title": "Human-Centered and AI-driven Generation of 6-DoF Extended Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1004",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "In order to unlock the full potential of Extended Reality (XR) and its application to societal sectors such as health (e.g., training) or Industry 5.0 (e.g., remote control of infrastructure) there is a need for very realistic environments to enhance the presence of the user. However, current photo-realistic content generation methods (such as Light Fields) require a massive amount of data transmission (i.e., ultra-high bandwidths) and extreme computational power for displaying. Thus, they are not suited for interactive immersive and realistic applications. In this research, we hypothesize that is possible to generate realistic dynamic 3D environments by means of Deep Generative Networks. The work will consist of two parts: (1) a computer vision system that generates the 3D environment based on 2D images, and (2) a Human-Computer Interaction system (HCI) that predicts Region of Interest (RoI) for efficient 3D rendering, subjective and objective assessment of user perception (by means of presence) to enhance the 3D scene quality. This work aims to gain insights into how well deep generative methods can create realistic and immersive environments. This will significantly help future developments in realistic and immersive XR content creation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Electrical Engineering (ESAT)"
            }
          ],
          "personId": 117140
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Electrical Engineering (ESAT)"
            }
          ],
          "personId": 116955
        }
      ]
    },
    {
      "id": 117256,
      "typeId": 12516,
      "title": "Physically-based Lighting of 3D Point Clouds for Quality Assessment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1005",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "Point clouds are acknowledged as an essential data structure to represent 3D objects in many use cases, notably immersive experience settings such as Virtual, Augmented or Mixed Reality. This work is the first part of a research project on immersive Quality Assessment of point clouds in different lighting. In this report, I focus mainly on the physically-based rendering of such data in Unity 3D, and the impact of point cloud compression when considering various lighting conditions on the objects. These first observations and results will serve in the implementation of a 6DoF immersive experiment setting for subjective quality assessment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117089
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N"
            }
          ],
          "personId": 117105
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117178
        }
      ]
    },
    {
      "id": 117257,
      "typeId": 12516,
      "title": "Object-Based Access: Enhancing Accessibility with Data-Driven Media",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1006",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117287
      ],
      "eventIds": [],
      "abstract": "Audiovisual media is an integral part of many people's everyday lives. People with accessibility needs, especially people with complex accessibility needs, however, may face challenges accessing this content. This doctoral work addresses this problem by investigating how complex accessibility needs can be met by content personalisation by leveraging data-driven methods. To this end, I will collaborate with people with aphasia, a complex language impairment, as an exemplar community of people with complex accessibility needs. To better understand the needs of people with aphasia, I will use collaborative design techniques to meet the needs of end users. This will involve them in the design, development and evaluation of systems that demonstrate the benefits of content personalisation as an accessibility intervention. This paper outlines the background and motivation to this PhD, the work that has already been completed, and current planned future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117018
        }
      ]
    },
    {
      "id": 117258,
      "typeId": 12515,
      "title": "Sensorial Immersive Experiences using MPEG Haptic and Scene Description Standards",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1005",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117288
      ],
      "eventIds": [],
      "abstract": "This paper presents a demonstration showcasing the capabilities of the upcoming MPEG-I standards on Haptics and Scene Description. The demonstration is a sensorial immersive experience using a virtual reality headset, haptic-enabled controllers and a haptic vest. The proposed experience is an interactive go-kart race-like game where interactions with the environment will trigger spatialized haptic feedback to provide an enhanced sense of immersion. \r\n  By implementing the MPEG-I standards on Haptics and Scene Description, and by designing this demonstration using exclusively these standards, this work showcases part of the research and standardization efforts produced through the MPEG ecosystem.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117173
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117191
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117218
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117221
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117146
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117153
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117088
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Cesson-Sevigne",
              "institution": "Interdigital",
              "dsl": ""
            }
          ],
          "personId": 117125
        }
      ]
    },
    {
      "id": 117259,
      "typeId": 12521,
      "title": "Zero-shot virtual product placement in videos",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3597213"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1025",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117286
      ],
      "eventIds": [],
      "abstract": "Virtual Product Placement (VPP) is an advertising technique that digitally places branded objects into movie or TV show scenes. Despite being a billion-dollar industry, current ad rendering techniques are time-consuming, costly, and executed manually with the help of visual effects (VFX) artists. In this paper, we present a fully automated and generalized framework for placing 2D ads in any linear TV cooking show captured using a single-view camera with minimal camera movements. The framework detects empty spaces, understands the kitchen scene, handles occlusion, renders ambient lighting, and tracks ads. Our framework without requiring access to full video or production camera configuration reduces the time and cost associated with manual post-production ad rendering techniques, enabling brands to reach consumers seamlessly while preserving the continuity of their viewing experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 117170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "Amazon",
              "dsl": "Amazon Web Services"
            }
          ],
          "personId": 117200
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Amazon ",
              "dsl": ""
            }
          ],
          "personId": 117186
        }
      ]
    },
    {
      "id": 117260,
      "typeId": 12516,
      "title": "Optimization and Evaluation of Emerging Codecs ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1000",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "Video streaming is growing exponentially. High-resolution videos require high bandwidth to transport the videos over the network. There is a great demand for compression technologies to compress video and maintain quality. Video codecs are used to encode and decode video streams. These codecs have been developed by MPEG, Google, Microsoft, and Apple Inc. The goal of this research is to develop a technology that will realize contribution transmission through connecting the latest methods generation of single and multi-way video encoding with the new protocols that will provide transmission reliability and keep low latency. A literature review will be carried out. The literature covers different video codecs and transmission techniques and the methods used to evaluate the quality of those techniques and codecs. Based on the literature review, the theoretical framework will be formed, and a video encoding method prototype will be developed. The developed method will be for one-way and multi-way software that will automatically optimize the settings of the video codec to set its operating conditions at optimal. The new transmission software will use newer codecs, such as H265/HEVC, VP9, and AV1, MPEG5, which will allow additional reduction of the bit stream and deliver secure, reliable, and quality video with low latency",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "AGH",
              "dsl": ""
            }
          ],
          "personId": 117181
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "AGH",
              "dsl": ""
            }
          ],
          "personId": 117095
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "AGH",
              "dsl": ""
            }
          ],
          "personId": 117196
        }
      ]
    },
    {
      "id": 117261,
      "typeId": 12515,
      "title": "MiroAR: Ubiquitous AR Teleconferencing Through The Mirror",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1004",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117320
      ],
      "eventIds": [],
      "abstract": "Video call systems rely on being able to capture and transmit a self view, while at the same time rendering the view of the other party. Due to the lack of inwards facing cameras in XR devices (AR Glasses, HMD etc.) this is not a straightforward process. As a solution, recent XR teleconferencing platforms are trying to create a more \"immersive'' experience by replacing the self view with avatars, placing 3D models in space, creating shared spaces and other engaging features; approaches that are quite demanding and even then do not create a \"traditional\" teleconferencing experience. In this work, we are using an XR device (AR Glasses, or smartphone) to create a seamless and natural video calling experience. By using the AR Glasses to record an existing self-view from a reflective surface, like a mirror, the user is able to easily conduct a video call with a party, even if they are using a different setup. To demonstrate this concept, we present the MiroAR application. We conclude this paper by discussing the roadmap, shortcomings and possible extensions of our work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "Xiaomi Technology",
              "dsl": ""
            }
          ],
          "personId": 117162
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "Xiaomi Technology",
              "dsl": ""
            }
          ],
          "personId": 117098
        }
      ]
    },
    {
      "id": 117262,
      "typeId": 12516,
      "title": "Evaluation of Media-Based Social Interactions in Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1001",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "The evaluation of users’ experiences in virtual environments is an important task for researchers in the fields of human-computer interaction and extended reality. It can be used to understand and enhance the quality of users’ mediated interactions and communications. In a constantly evolving world, where people are growing with technology, it is important to understand, evaluate and enhance the use of immersive media. In the research agenda of this Ph.D. thesis, the challenge of developing multi-user experiences in virtual environments and setting evaluation metrics for researchers are considered. This Ph.D. thesis showcases an interest in how to enhance trust formation in media-based social environments. The findings of this Ph.D. are expected to help create new open-source tools to facilitate the understanding of individuals and groups in extended reality applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 117154
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 117180
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "UCL",
              "dsl": ""
            }
          ],
          "personId": 117079
        }
      ]
    },
    {
      "id": 117263,
      "typeId": 12515,
      "title": "An AR Game for Primary Learners to Safeguard Intangible Cultural Heritage of the Ovahimba Tribe",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1007",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117288
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR) is a new technology that enhances the actual world by superimposing computer-generated or extracted real-world sensory data such as images or sound onto it. Incorporating Augmented Reality into Cultural Heritage has a slew of benefits. Safeguarding of Cultural Heritage is vital because it serves as a link between the past and the present. Museums do not engage their audiences, particularly primary school-aged children, and as a result, youngsters show little interest in their cultural history, and museums cannot compete with more technologically advanced and modern kinds of entertainment. The purpose of this study was to spark children's interest in Cultural Heritage as they engage with it. Under the project, a Cultural Heritage game using eight Ovahimba items was developed and tested at a local primary school. Research-by-Design was the primary methodology of this research project.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Faculty of Computing and Informatics"
            }
          ],
          "personId": 117165
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "Khomas",
              "city": "Windhoek",
              "institution": "Namibia Uniiversity of Science and Technology",
              "dsl": "School of Computing and Informatics"
            }
          ],
          "personId": 117179
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Sceince and Technology",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 116838
        }
      ]
    },
    {
      "id": 117264,
      "typeId": 12516,
      "title": "Behavior as a Function of Video Quality in an Ecologically Valid Experiment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23d-1002",
      "source": "PCS",
      "trackId": 12244,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117290
      ],
      "eventIds": [],
      "abstract": "Most user studies in the QoE multimedia domain are done by asking users about quality. This approach has advantages: it obtains many answers and reduces variance by repeated measurements. However, the results obtained in this context may be different from those obtained in the real application, since quality is not asked about this often in everyday life. It is more natural is to focus on user behavior. The proposed PhD focuses on a method for performing experiments based on observations of a participant's behavior. We address two main challenges that exist in any new experiment design: how to calculate the interval validity of the proposed method and how to analyze the obtained data. The data analysis we propose is based on psychometric functions. We propose two different experiments, one of which is already ongoing. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Institute of Telecommunications"
            }
          ],
          "personId": 117135
        }
      ]
    },
    {
      "id": 117265,
      "typeId": 12521,
      "title": "Tap or Swipe? Effects of Interaction Gestures for Retrieval of Match Statistics via Second Screen on Watching Soccer on TV",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596473"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1023",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "Accessing match statistics through second screen while watching soccer matches on TV has grown into a popular practice. Although early works have shown how gestures on touch screens performed under distracting environments, little is known regarding how specific gestures (swiping and tapping) to retrieve information on second screen affect the viewing experience of soccer games on TV. For this, a mixed-method user study, which included prototype tests of watching short clips of a soccer match, questionnaires and short interviews, was conducted with 28 participants. The results revealed that the number of people who preferred tapping was more than the number of people who favored swiping under two different second screen activity time scenarios i.e. On-Play or Off-Play. However, neither swiping nor tapping yield better performance of recalling verbatim match stats and exact comparisons in both On-Play and Off-Play. Participant evaluations in On-Play and interviews give us clues regarding such difference.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Dundee",
              "institution": "Abertay University",
              "dsl": "Division of Games and Arts"
            }
          ],
          "personId": 117215
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Lancaster",
              "institution": "Lancaster University",
              "dsl": "LICA"
            }
          ],
          "personId": 117104
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Koblenz",
              "institution": "University of Koblenz-Landau",
              "dsl": "Computer Science/Institute for IS Research"
            }
          ],
          "personId": 117149
        }
      ]
    },
    {
      "id": 117266,
      "typeId": 12515,
      "title": "DatAR: Comparing Relationships between Brain Regions and Diseases: An Immersive AR Tool for Neuroscientists",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1006",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117289
      ],
      "eventIds": [],
      "abstract": "Different brain diseases are associated with distinct patterns of affected brain regions. Neuroscientists aim to understand the variability of these patterns across different brain diseases. We used a user-centered design approach to develop support for neuroscientists in exploring and comparing which regions are affected by which diseases based on neuroscience literature. Our study involved six neuroscientists and nine visualization experts who evaluated the usability and explainability of our visualization tool for comparing disease patterns in brain regions. Our results show that our tool can help neuroscientists explore and visualize relationships between brain regions and diseases, and compare patterns of affected regions in an immersive AR environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": "Information and Computing Sciences"
            }
          ],
          "personId": 117108
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "CWI",
              "dsl": ""
            }
          ],
          "personId": 117203
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": "Department of Information and Computing Sciences"
            }
          ],
          "personId": 117085
        }
      ]
    },
    {
      "id": 117267,
      "typeId": 12521,
      "title": "LLM-Based Interaction for Content Generation: A Case Study on the Perception of Employees in an IT Department",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596457"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1029",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "In the past years, AI has seen many advances in the field of NLP. This has led to the emergence of LLMs, such as the now famous GPT-3.5, which revolutionise the way humans can access or generate content. Current studies on LLM-based generative tools are mainly interested in the performance of such tools in generating relevant content (code, text or image). However, ethical concerns related to the design and use of generative tools seem to be growing, impacting the public acceptability for specific tasks. This paper presents a questionnaire survey to identify the intention to use generative tools by employees of an IT company in the context of their work. This survey is based on empirical models measuring intention to use (TAM by Davis, 1989, and UTAUT2 by Venkatesh and al., 2008). Our results indicate a rather average acceptability of generative tools, although the more useful the tool is perceived to be, the higher the intention to use seems to be. Furthermore, our analyses suggest that the frequency of use of generative tools is likely to be a key factor in understanding how employees perceive these tools in the context of their work. Following on from this work, we plan to investigate the nature of the requests that may be made to these tools by specific audiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            },
            {
              "country": "France",
              "state": "-Select-",
              "city": "NANTES",
              "institution": "L'Ecole de Design Nantes Altantique",
              "dsl": "Digital Design Lab"
            }
          ],
          "personId": 117139
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "-Select-",
              "city": "NANTES",
              "institution": "L'Ecole de Design Nantes Altantique",
              "dsl": "Digital Design Lab"
            }
          ],
          "personId": 117161
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "University of Nantes",
              "dsl": "LS2N"
            }
          ],
          "personId": 117051
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N/Polytech Nantes"
            }
          ],
          "personId": 117134
        }
      ]
    },
    {
      "id": 117268,
      "typeId": 12515,
      "title": "Use of immersive and interactive systems to  objectively and subjectively characterize user experience in work places",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1008",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117319
      ],
      "eventIds": [],
      "abstract": "This demo's objective is to display how simulated environments can be used in the evaluation of work-related indoor environments compared to physical environments. In fact, in indoor environment evaluations, Virtual Reality (VR) offers new possibilities for experimental design as well as a functional rapprochement between the laboratory and real life. With VR, environmental parameters (e.g light, color, furniture. . . ) can be easily manipulated at reasonable costs, allowing to control and guide the user’s sensorial experience. One main challenge is to acknowledge to which extent simulated environments are ecologically valid and which functions would be more solicited in different environmental-simulation display formats. User-centric evaluation and sensory analysis in the building sector is in its beginning; this new tool could be of benefit for the building sector, on one hand for methodological facilitation purposes and on the other for cost reductions. In order to achieve the objectives of this project, a first step is to develop and validate the indoor simulations. In environmental simulations, one of the most used formats, for its visual realism and ease of use are 360° panoramic photos and videos, which permits capturing physical-world images. In an objective of validation of the format, 360° photos of workplaces were taken in the building of Halle 6 Ouest of Nantes University and an immersive and interactive test based on physiological indicators to subjectively and objectively assess comfort and performances in work offices was developed. The demo will comprise a head-mounted display with integrated eye-tracking and the measure of electrodermal activity, heart rate and galvanic skin response.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Centre Scientifique et Technique du Bâtiment",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université",
              "dsl": "LS2N"
            }
          ],
          "personId": 117190
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Centre Scientifique et Technique du Bâtiment",
              "dsl": ""
            }
          ],
          "personId": 117222
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N"
            }
          ],
          "personId": 117105
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N/Polytech Nantes"
            }
          ],
          "personId": 117134
        }
      ]
    },
    {
      "id": 117269,
      "typeId": 12521,
      "title": "The Green Notebook - A Co-Creativity Partner for Facilitating Sustainablity Reflection in AI Art Practises",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3596465"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1028",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "AI is becoming increasingly popular in artistic work. Yet tools for calculating environmental impact of AI are more adapted for other contexts than creative practices, making them sometimes hard to comprehend for the non-expert. In this study, based on interviews with AI artists, a design artifact called The Green Notebook was developed: a physical notebook where the AI artist could discuss ideas and receive feedback of their expected environmental impact. The conversational experience between the artist and the interface was informed by online content analysis of artistic work processes. The Notebook was explored and assessed with five artists in Wizard-of-Oz and focus group studies. Generally, the participants found a co-creation process with the enhanced ability to reflect on sustainability an accessible way to engage with sustainability considerations of their AI artistic practices. We provide insights of the Notebook's perceived role and the conversational strategies used by the artists. Furthermore, we discuss trade-offs between politeness vs. efficiency and focus vs. integration to inform future research. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "EECS/HCT/MID"
            }
          ],
          "personId": 117077
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "EECS/HCT/MID"
            }
          ],
          "personId": 117201
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm ",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 117101
        }
      ]
    },
    {
      "id": 117270,
      "typeId": 12515,
      "title": "ScenaConnect: an original device to enhance experiences with multisensoriality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1012",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117288
      ],
      "eventIds": [],
      "abstract": "This demonstration aims at presenting ScenaConnect, a multisensory device which allows people to live various several multisensory experiences. ScenaConnect is inexpensive, compact, easy to install and allows to improve experiences in added new interactions. The demonstration will present two cases of use. The first one is an interactive math exercise and the second one is a multisensory experience that will take the visitor on a journey through history. Moreover, ScenaConnect could be used in museums for immersive and interactive experiences or by a teacher who can use it to make the learning of his students more interactive and adapted. The perspectives are to allows non-expert in computer science to quickly integrate ScenaConnect in several and various experiences thanks to the software ScenaProd, which is, like ScenaConnect, a goal of the PRIM project presented in more detail on this paper.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 116874
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 117197
        }
      ]
    },
    {
      "id": 117271,
      "typeId": 12521,
      "title": "Feedback Driven Multi Stereo Vision System for Real-Time Event Analysis",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3597220"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1032",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "2D cameras are often used in interactive systems. Other systems like gaming consoles provide more powerful 3D cameras for short range depth sensing. Overall, these cameras are not reliable in large, complex environments. In this work, we propose a 3D stereo vision based pipeline for interactive systems, that is able to handle both ordinary and sensitive applications, through robust scene understanding. We explore the fusion of multiple 3D cameras to do full scene reconstruction, which allows for preforming a wide range of tasks, like event recognition, subject tracking, and notification. Using possible feedback approaches, the system can receive data from the subjects present in the environment, to learn to make better decisions, or to adapt to completely new environments. Throughout the paper, we introduce the pipeline and explain our preliminary experimentation and results. Finally, we draw the roadmap for the next steps that need to be taken, in order to get this pipeline into production.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117094
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117084
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "University of Mons",
              "dsl": ""
            }
          ],
          "personId": 117177
        }
      ]
    },
    {
      "id": 117272,
      "typeId": 12515,
      "title": "GoNature AR: Air Quality & Noise Visualization Through a Multimodal and Interactive Augmented Reality Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1011",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117319
      ],
      "eventIds": [],
      "abstract": "As Extended Reality (XR) media experiences become a commodity, there is a unique opportunity to deploy XR for environmental awareness. Interaction challenges of Augmented Reality (AR) still exist, focused on limited gesture and head tracking affordances. AR technologies should also be seamlessly integrated with sensor data, analytics and ultimately status prediction, to be visualized in an AR experience, rather than merely superimposing visuals onto the real world. This paper presents an innovative, work-in-progress, multimodal AR experience integrating interactive narration, gestures, hands recognition and voice commands, while a citizen is wearing a head-worn AR display, promoting environmental awareness, health, and wellness. By combining AR technologies and a sensor network, GoNature AR provides citizen awareness of real-time, multimodal, air and noise pollution data. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 117175
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 117130
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 117198
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 117169
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 117148
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "Crete",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 117103
        }
      ]
    },
    {
      "id": 117273,
      "typeId": 12521,
      "title": "Zenctuary VR. Simulating Nature in an Interactive Virtual Reality Application ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3597215"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1030",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117323
      ],
      "eventIds": [],
      "abstract": "In this paper we present the design process of a virtual reality experience the aim of which is to have a restorative effect on users. In the simulated natural site, the user can interact with some elements of the environment and can also explore the view.We describe how we tried to create a more realistic sense of nature by relying on high quality graphics, the use of free-roaming space, and naturalistic interactions. During the design process we avoided gameful interactions and instead created playful interactions, while also relying on the multimodal aspect of the virtual reality technology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Innovation Center",
              "dsl": "Moholy-Nagy University of Art and Design"
            }
          ],
          "personId": 117158
        },
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Moholy-Nagy University of Art and Design",
              "dsl": "Innovation Center"
            }
          ],
          "personId": 117127
        },
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Moholy-Nagy University of Art and Design",
              "dsl": "Innovation Center"
            }
          ],
          "personId": 117183
        },
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Code and Soda",
              "dsl": ""
            }
          ],
          "personId": 117132
        },
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Code and Soda",
              "dsl": ""
            }
          ],
          "personId": 117144
        },
        {
          "affiliations": [
            {
              "country": "Hungary",
              "state": "",
              "city": "Budapest",
              "institution": "Code and Soda",
              "dsl": ""
            }
          ],
          "personId": 117136
        },
        {
          "affiliations": [
            {
              "country": "Bulgaria",
              "state": "",
              "city": "Sofia",
              "institution": "Institute of Philosophy and Sociology Bulgarian Academy of Sciences",
              "dsl": "Philosophy of Science Department"
            }
          ],
          "personId": 117113
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "HKU University of the Arts",
              "dsl": "Professorship Interactive Narrative Design"
            }
          ],
          "personId": 117187
        }
      ]
    },
    {
      "id": 117274,
      "typeId": 12515,
      "title": "Kinetic particles : from human pose estimation to an immersive and interactive piece of art questionning thought-movement relationships.",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1013",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117321
      ],
      "eventIds": [],
      "abstract": "Digital tools offer extensive solutions to explore novel interactive- art paradigms, by relying on various sensors to create installations and performances where the human activity can be captured, anal- ysed and used to generate visual and sound universes in real-time. Deep learning approaches, including human detection and human pose estimation, constitute ideal human-art interaction mediums, as they allow automatic human gesture analysis, which can be directly used to produce the interactive piece of art. In this con- text, this paper presents an interactive work of art that explores the relationship between thought and movement by combining dance, philosophy, numerical arts, and deep learning. We present a novel system that combines a multi-camera setup to capture hu- man movement, state-of-the-art human pose estimation models to automatically analyze this movement, and an immersive 180° projection system that projects a dynamic textual content that intu- itively responds to the users’ behaviors. The demonstration being proposed consists of two parts. Firstly, a professional dancer will uti- lize the proposed setup to deliver a conference-show. Secondly, the audience will be given the opportunity to experiment and discover the potential of the proposed setup, which has been transformed into an interactive installation. This allows multiple spectators to engage simultaneously with clusters of words and letters extracted from the conference text.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "New Media Artist",
              "dsl": ""
            }
          ],
          "personId": 117151
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Intermittent Worker",
              "dsl": ""
            }
          ],
          "personId": 117102
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "F-44000 Nantes",
              "institution": "Nantes Université, École Centrale Nantes, CNRS, LS2N, UMR 6004",
              "dsl": ""
            }
          ],
          "personId": 117120
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Nantes Université, CAPACITÉS SAS",
              "dsl": ""
            }
          ],
          "personId": 117099
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xuzhou",
              "institution": "China University of Mining and Technology",
              "dsl": ""
            }
          ],
          "personId": 117224
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Université de Nantes",
              "dsl": "LS2N/Polytech Nantes"
            }
          ],
          "personId": 117134
        }
      ]
    },
    {
      "id": 117275,
      "typeId": 12521,
      "title": "A Preliminary Study of the Eye Tracker in the Meta Quest Pro",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596467"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1034",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "This paper presents the preliminary results of an accuracy testing of the Meta Quest Pro's eye tracker. We conducted user testing to evaluate the spatial accuracy, spatial precision and subjective performance under head-free and head-restrained conditions. Our measurements indicated an average accuracy of 1.652° with a precision of 0.699° (standard deviation) and 0.849° (root mean square) for a visual field spanning 15° during head-free. The signal quality of Quest Pro's eye-tracker is comparable to existing AR/VR eye-tracking headsets. Notably, careful considerations are required when designing the size of scene objects, mapping areas of interest, and determining the interaction flow. Researchers should also be cautious about interpreting the fixation results when multiple targets are within close proximity. Further investigation and better specification information transparency are needed to establish its capabilities and limitations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Oxfordshire",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Psychiatry"
            }
          ],
          "personId": 117184
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "Independent",
              "dsl": ""
            }
          ],
          "personId": 117082
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "Oxford Health NHS Foundation Trust",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": ""
            }
          ],
          "personId": 117122
        }
      ]
    },
    {
      "id": 117276,
      "typeId": 12521,
      "title": "Proof-of-Concept Study to Evaluate the Impact of Spatial Audio on Social Presence and User Behavior in Multi-Modal VR Communication",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596458"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1039",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "This paper presents a proof-of-concept study conducted to analyze the effect of simple diotic vs. spatial, position-dynamic binaural synthesis on social presence in VR, in comparison with face-to-face communication in the real world, for a sample two-party scenario. A conversational task with shared visual reference was realized. The collected data includes questionnaires for direct assessment, tracking data, and audio and video recordings of the individual participants' sessions for indirect evaluation. While tendencies for improvements with binaural over diotic presentation can be observed, no significant difference in social presence was found for the considered scenario. The gestural analysis revealed that participants used the same amount and type of gestures in face-to-face as in VR, highlighting the importance of non-verbal behavior in communication. As part of the research, an end-to-end framework for conducting communication studies and analysis has been developed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 117081
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 117143
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Electronic Media Technology Group"
            }
          ],
          "personId": 117092
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 117202
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 117128
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Ilmenau University of Technology",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 117217
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Weimar",
              "institution": "Bauhaus-Universität Weimar",
              "dsl": "Virtual Reality and Visualization Research Group"
            }
          ],
          "personId": 117192
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 117021
        }
      ]
    },
    {
      "id": 117277,
      "typeId": 12521,
      "title": "Audio Augmented Reality Outdoors",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3573381.3597028"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1040",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117291
      ],
      "eventIds": [],
      "abstract": "Audio Augmented Reality (AAR) is a novel and unexplored area of AR representing the augmentation of reality with auditory as well as visual content. AAR's interaction affordances as well as accurate real-world registration of visual as well as sound elements are challenging issues, especially in noisy, bright and busy environments outdoors. This paper presents a novel, mobile AAR experience that is deployed in a city environment while walking past six archaeological excavation sites in the city of Chania, Crete, Greece. The proposed AAR experience utilizes cutting-edge gamification techniques, non-linear storytelling, precise AR visualization and spatial audio, offering innovative AAR interaction while exploring the city’s archaeological sites and history, outdoors. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 116922
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 116920
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities",
              "dsl": ""
            }
          ],
          "personId": 116959
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities",
              "dsl": ""
            }
          ],
          "personId": 116932
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities",
              "dsl": ""
            }
          ],
          "personId": 116918
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 117103
        }
      ]
    },
    {
      "id": 117278,
      "typeId": 12521,
      "title": "Towards the Creation of Scalable Tools for automatic Quality of Experience Evaluation and a Multi-Purpose Dataset for Affective Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1044",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "Traditional tools used to evaluate the Quality of Experience (QoE) of users after browsing an ad, using a product, or performing any kind of task typically involves surveys, user testing, and analytics. However, these methods provide limited insights and have limitations due to the need of users’ active cooperation and sincerity, the long testing time, the high cost, and the limited scalability. On this work we present the tools we are developing to automatically evaluate QoE in different use cases such as dashboards that show on real time reactions to different events in the form of emotions and affections predicted by different models based on physiological data. To develop these tools, we require datasets on affective computing.  We highlight some limitations of the available ones, the difficulties during the creation of such data, and our current work in the confection of a new one with automatic annotation of ground truth.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Valencia",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117131
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": " i2cat Foundation",
              "dsl": "Media & Internet Area"
            }
          ],
          "personId": 117124
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Valencia",
              "institution": "University of Valencia",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 117210
        }
      ]
    },
    {
      "id": 117279,
      "typeId": 12521,
      "title": "Proactivity in the TV Context: Understanding the Relevance and Characteristics of Proactive Behaviours in Voice Assistants",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1043",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117293
      ],
      "eventIds": [],
      "abstract": "In the context of Television (TV), intelligent voice assistants are still mainly based on reactive behaviours, only responding to users' requests. However, the addition of proactivity in TV assistants could have the potential to reduce the user-assistant interaction effort and bring an additional \"human\" layer, leading to a more organic adoption of these systems. Within this framework, this paper aims to contribute to the understanding of the relevance of a set of proactive scenarios for TV and get clues on how to design these behaviours to be efficient and avoid intrusion feelings. To this end, focus groups were conducted to discuss the conceptualised scenarios. The results showed that most of the idealized scenarios were relevant for the participants and that they will be positive if some characteristics can be pre-configured regarding the type of communication and intrusion of proactive suggestions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia Research Centre, Department of Communication and Arts"
            }
          ],
          "personId": 117111
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia Research Centre, Department of Communication and Arts"
            }
          ],
          "personId": 117182
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia Research Centre, Águeda School of Technology and Management"
            }
          ],
          "personId": 117214
        }
      ]
    },
    {
      "id": 117280,
      "typeId": 12515,
      "title": "InnovART: à l'écoute des chantiers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1000",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117320
      ],
      "eventIds": [],
      "abstract": "À l’écoute des chantiers is an immersive self-guided walking tour, a sound and visual exploration of the industrial heritage of Nantes’ Parc des Chantiers - with ten sound capsules (storytelling, interviews with former shipyard workers, and sound design) and two site-specific large-scale augmented reality projections of the transborder ferry bridge (dismantled in 1958) and the construction of the Brissac car ferry (1955). Accessible along Nantes’ popular green-line tourist path through signage on the ground, this itinerary describes, explains, and puts into perspective a landscape familiar to the people of Nantes, newcomers, as well as tourists, by giving keys to understanding a former industrial site undegoing rapid transformation and in the process of disappearing. The itinerary is available on the Nantes Patrimonia website, downloadable as a smartphone application or activated via QR codes present in the park. Part of Campus France’s InnovART2 research project, À l’écoute des chantiers was part of the Voyage à Nantes 2022 and an ongoing part of the Parcours des Écoles, and it questions the limits of smart tourism applications in an era of increasing digital privacy concerns that reduce geo-location capabilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "l'école de design Nantes Atlantique",
              "dsl": "Digital Design Lab"
            }
          ],
          "personId": 117161
        }
      ]
    },
    {
      "id": 117281,
      "typeId": 12521,
      "title": "Developing an Interactive Agent for Blind and Visually Impaired People",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3573381.3596471"
        }
      },
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23b-1042",
      "source": "PCS",
      "trackId": 12240,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117292
      ],
      "eventIds": [],
      "abstract": "The aim of this project is to create an interactive assistant that incorporates different assistive features for blind and visually impaired people. The assistant might incorporate screen readers, magnifiers, voice synthesis, OCR, GPS, face recognition, and object recognition among other tools. Recently, the work done by OpenAI and Be My Eyes with the implementation of GPT-4 is comparable to the aim of this project. It shows the development of an interactive assistant has become simpler due to recent developments in large language models. However, older methods like named entity recognition and intent classification are still valuable to build lightweight assistants. A hybrid solution combining both methods seems possible, would help to reduce the computational cost of the assistant, and would facilitate the data collection process. Despite being more complex to implement in a multilingual and multimodal context, a hybrid solution has the potential to be used offline and to consume less resources.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "Faculty of Engineering, University of Mons",
              "dsl": "Numediart Institute, ISIA Lab"
            }
          ],
          "personId": 117156
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "Faculty of Engineering, University of Mons",
              "dsl": "Numediart Institute, ISIA Lab"
            }
          ],
          "personId": 117106
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Mons",
              "institution": "Faculty of Engineering, University of Mons",
              "dsl": "Numediart Institute, ISIA Lab"
            }
          ],
          "personId": 117211
        }
      ]
    },
    {
      "id": 117282,
      "typeId": 12515,
      "title": "HoloBrand: A Co-located XR Multiplayer Serious Game on an Economic Model",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "imx23e-1003",
      "source": "PCS",
      "trackId": 12237,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        117289
      ],
      "eventIds": [],
      "abstract": "Extended reality (XR) and serious games are considered promising approaches for modern teaching and learning concepts. However, they often do not exploit the advantages that co-located experiences could provide (e.g., more immediate exchange of ideas and a sense of community). One reason is the high technical, design, and didactic requirements for such solutions. In this work, we introduce HoloBrand, a co-located XR multiplayer serious game for HoloLens 2. The game can be played at the exhibit with one to three players in a time frame from three minutes on upwards. With the game, we enable students to experience the dynamics of an economic model on mass market products (Urban's perceptor model). Prior knowledge is not required. We describe the fundamental conceptual and implementation aspects of the HoloBrand system and game and interaction design. We include the implementation of the triadic game design concept and the four-factor fun model.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Science",
              "dsl": "Computer Science"
            }
          ],
          "personId": 117166
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Science",
              "dsl": ""
            }
          ],
          "personId": 117091
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University of Applied Sciences",
              "dsl": "Computer Science"
            }
          ],
          "personId": 117171
        }
      ]
    }
  ],
  "people": [
    {
      "id": 116826,
      "firstName": "Jan",
      "lastName": "Van Erp",
      "middleInitial": "",
      "importedId": "77Tl4LXKOC1y6TpLZ35GEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116827,
      "firstName": "Antoni",
      "lastName": "Bibiloni Coll",
      "middleInitial": "",
      "importedId": "_iW_kqDe_A0asZ8bTu6s0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116828,
      "firstName": "Robert",
      "lastName": "Stevens",
      "middleInitial": "",
      "importedId": "RWqicRw8reqU75q8JBbSyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116829,
      "firstName": "Jacob",
      "lastName": "Biehl",
      "middleInitial": "",
      "importedId": "FKU6pyTWBEuUY0srdo7RTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116830,
      "firstName": "David",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "kZ98m7yjyjb4grptqT6Wjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116831,
      "firstName": "Marcelo de Abreu Borges",
      "lastName": "Borges",
      "middleInitial": "A",
      "importedId": "Ry9EhCSXPAp6sx_3A3x6og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116832,
      "firstName": "Giuliano",
      "lastName": "Maia",
      "middleInitial": "",
      "importedId": "wWdLGkMT0pn85gMOx81UJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116833,
      "firstName": "Aljosa",
      "lastName": "Smolic",
      "middleInitial": "",
      "importedId": "dXGNiDMLRrNPfmxKM_vYzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116834,
      "firstName": "Paul A",
      "lastName": "Gardner",
      "middleInitial": "",
      "importedId": "HfJ8z1ovDp8kgzX-P5KBOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116835,
      "firstName": "Erkki",
      "lastName": "Sutinen",
      "middleInitial": "",
      "importedId": "DgfLSpRww2YS3uLWVEXUOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116836,
      "firstName": "Razan",
      "lastName": "Jaber",
      "middleInitial": "",
      "importedId": "NLhhSDuE-8e4ckLupD4KYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116837,
      "firstName": "Ana Margarida Pisco",
      "lastName": "Almeida",
      "middleInitial": "",
      "importedId": "AJu3XVGwXkTyFvmtMy0lQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116838,
      "firstName": "Heike",
      "lastName": "Winschiers-Theophilus",
      "middleInitial": "",
      "importedId": "i-vIgqYGhPAW_WH-dDQF7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116839,
      "firstName": "Sabin",
      "lastName": "Tabirca",
      "middleInitial": "",
      "importedId": "zHt-CjbCWZ5Nyhrux4Z0dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116840,
      "firstName": "Ignacio",
      "lastName": "Benito Frontelo",
      "middleInitial": "",
      "importedId": "fkctoyXo0_ivkp3LsKl41A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116841,
      "firstName": "Sofia",
      "lastName": "Nunes",
      "middleInitial": "",
      "importedId": "gwFx3ZSRCAfFQWdtk5LVLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116842,
      "firstName": "Jeffrey R",
      "lastName": "Head",
      "middleInitial": "",
      "importedId": "tMFakF2WDAIPlMQxr1JueQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116843,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "importedId": "ob1mmWPesH21Yfh_UxGqsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116844,
      "firstName": "Joe",
      "lastName": "Paradiso",
      "middleInitial": "",
      "importedId": "mPgn9zhAqjthCQ5b5psNcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116845,
      "firstName": "Matthijs",
      "lastName": "van der Boon",
      "middleInitial": "",
      "importedId": "UtRvKA6Uz0lFo4hioMSbag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116846,
      "firstName": "Rômulo",
      "lastName": "Vieira",
      "middleInitial": "",
      "importedId": "bzcHQyKyMbpJgETqpEUThA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116847,
      "firstName": "Devin",
      "lastName": "Horsman",
      "middleInitial": "",
      "importedId": "snmAPIVjeA3iQp9XKihPxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116848,
      "firstName": "David",
      "lastName": "Ramsay",
      "middleInitial": "",
      "importedId": "3wPvvQe3xQWyeqAYgjVnYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116849,
      "firstName": "Stephan",
      "lastName": "Jürgens",
      "middleInitial": "",
      "importedId": "d57P9hVQWW0DyNVmvwTgcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116850,
      "firstName": "Nicolas",
      "lastName": "Pope",
      "middleInitial": "",
      "importedId": "xzB2-dkpiSdi7p-trHWUPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116851,
      "firstName": "Alexander",
      "lastName": "Toet",
      "middleInitial": "",
      "importedId": "iPSQgnZVSffp-er0Srbp2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116852,
      "firstName": "Nirul",
      "lastName": "Hoeba",
      "middleInitial": "",
      "importedId": "J0y-Oth-8bh5MTL43VWLUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116853,
      "firstName": "Daniel Yong Wen",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "6Z8abvvrMJH76JLIKlQvww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116854,
      "firstName": "Todd",
      "lastName": "Burlington",
      "middleInitial": "",
      "importedId": "38PYcrfBNp2jVfQ6p08lmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116855,
      "firstName": "Kinji",
      "lastName": "Matsumura",
      "middleInitial": "",
      "importedId": "p3PcS1b1YpWKmd8EaGEwKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116856,
      "firstName": "Erkki",
      "lastName": "Rötkönen",
      "middleInitial": "",
      "importedId": "DtK3j3WNN_Y8saeasD8uIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116857,
      "firstName": "Lianne",
      "lastName": "Kerlin",
      "middleInitial": "",
      "importedId": "gYQ8vrm9qiKiUBm_kMUPIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116858,
      "firstName": "Marcelo",
      "lastName": "Afonso",
      "middleInitial": "",
      "importedId": "bL7qqWIREQVsQYUDkcGJGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116859,
      "firstName": "Nuno",
      "lastName": "Correia",
      "middleInitial": "",
      "importedId": "CgB0401kd5HQqQRbv5o1TA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116860,
      "firstName": "Dmitriy",
      "lastName": "Babichenko",
      "middleInitial": "",
      "importedId": "oob8HcMT6uzeDA9cDxozUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116861,
      "firstName": "Kyle",
      "lastName": "Bostelmann",
      "middleInitial": "",
      "importedId": "3S5CqbzK6AqM3CIctEXU0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116862,
      "firstName": "Cristian",
      "lastName": "Pamparău",
      "middleInitial": "",
      "importedId": "e_w6ZMaH1FTy8-PwB5uKVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116863,
      "firstName": "Xingyu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "lok3Cha4FDtwrG3pPrrFhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116864,
      "firstName": "Jordan",
      "lastName": "Marczak",
      "middleInitial": "",
      "importedId": "B3RVs6DFpDCQ0Z0glGnbkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116865,
      "firstName": "Debora",
      "lastName": "Muchaluat-Saade",
      "middleInitial": "Christina",
      "importedId": "HRJQZjcRgBYsf1N_2-b9jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116866,
      "firstName": "Denio",
      "lastName": "Mariz",
      "middleInitial": "",
      "importedId": "bnQv_GFRBDrKuXR8TkMxhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116867,
      "firstName": "Luis",
      "lastName": "Quintero",
      "middleInitial": "",
      "importedId": "hO__dxBgjSZsz6-i3AzASQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116868,
      "firstName": "Luis",
      "lastName": "Aly",
      "middleInitial": "",
      "importedId": "w47et0oAFCx1Przh2k76Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116869,
      "firstName": "Sebastian",
      "lastName": "Hahta",
      "middleInitial": "",
      "importedId": "mSrZmXQ4xLmJuis8sPhsCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116870,
      "firstName": "Michael",
      "lastName": "Evans",
      "middleInitial": "",
      "importedId": "XjvS_7GSUiG1Gxz-AKMyzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116871,
      "firstName": "Abhinav",
      "lastName": "Tyagi",
      "middleInitial": "",
      "importedId": "OCmd7ldyp282zrLqDTJu6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116872,
      "firstName": "Yu",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "54AOVZg1tAZW_SedGi7Bcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116873,
      "firstName": "Rui",
      "lastName": "Rodrigues",
      "middleInitial": "",
      "importedId": "HOl_opiJs3B3p2SrDr59cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116874,
      "firstName": "Céline",
      "lastName": "JOST",
      "middleInitial": "",
      "importedId": "JWlDS9y3zTqOeFPZ0XOjiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116875,
      "firstName": "Telmo",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "UDkEm4BSJ-8u-eyo4FgSiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116876,
      "firstName": "Leoberto",
      "lastName": "Soares",
      "middleInitial": "",
      "importedId": "cqrQFgbE3HeI_9sW6bAWkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116877,
      "firstName": "Sydney",
      "lastName": "Mutelo",
      "middleInitial": "",
      "importedId": "hvmYXiGk5xBRNZRrGauJ_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116878,
      "firstName": "Ryan",
      "lastName": "Pham",
      "middleInitial": "",
      "importedId": "4Wxx_e4HmnuPaZhmjgqAAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116879,
      "firstName": "Yuki",
      "lastName": "Yamakami",
      "middleInitial": "",
      "importedId": "r5Q8Dttr9kw6nLf-tOy3Ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116880,
      "firstName": "Alexandru-Ionuț",
      "lastName": "Șiean",
      "middleInitial": "",
      "importedId": "eI7dvi463f9iODt-6o-vwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116881,
      "firstName": "NATHAN",
      "lastName": "HAHN",
      "middleInitial": "",
      "importedId": "RGB-IVBcAYRCfdraFrEVQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116882,
      "firstName": "Michael",
      "lastName": "Nitsche",
      "middleInitial": "",
      "importedId": "h52EmbxgDYXcz-RGJpl_Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116883,
      "firstName": "Helvi",
      "lastName": "Itenge",
      "middleInitial": "",
      "importedId": "J2Ki22s3fJrI13NSx3R_VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116884,
      "firstName": "Georgios",
      "lastName": "A Zenonos",
      "middleInitial": "",
      "importedId": "SPZiF0h3Jx7sV_LOFR-KRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116885,
      "firstName": "Sam",
      "lastName": "Van Damme",
      "middleInitial": "",
      "importedId": "fqOLSGnFptbkmE71Lfyn2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116886,
      "firstName": "Tariq",
      "lastName": "Zaman",
      "middleInitial": "",
      "importedId": "GG2MgwcvLI-gm4lY9FqonA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116887,
      "firstName": "Carla V.",
      "lastName": "Leite",
      "middleInitial": "",
      "importedId": "R4cqxE1aYw-IuSu7SBK6Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116888,
      "firstName": "Amir",
      "lastName": "Abbasnejad",
      "middleInitial": "",
      "importedId": "tez3bfiyK8AO3WkrXDexQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116889,
      "firstName": "Néill",
      "lastName": "O'Dwyer",
      "middleInitial": "",
      "importedId": "Eg3iEnB5juPraDbjodXtXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116890,
      "firstName": "Teresa",
      "lastName": "Chambel",
      "middleInitial": "",
      "importedId": "soT7Ey7h0AgZEl1ihx4MGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116891,
      "firstName": "Isaac",
      "lastName": "Makosa",
      "middleInitial": "",
      "importedId": "JkmvKoZKIHBYRH-N_M-pdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116892,
      "firstName": "Minghao",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "q1sXxogr_iHwEAY0_L6M4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116893,
      "firstName": "Pere",
      "lastName": "Palmer",
      "middleInitial": "",
      "importedId": "ex-PXJcshhHGa78d30waeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116894,
      "firstName": "Naska",
      "lastName": "Goagoses",
      "middleInitial": "",
      "importedId": "b9Bhpe-oZlG-kDHNlOKteA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116895,
      "firstName": "Carla",
      "lastName": "Fernandes",
      "middleInitial": "",
      "importedId": "DqaHydhnFtpKjjGT3ospbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116896,
      "firstName": "Deniz",
      "lastName": "Mevlevioğlu",
      "middleInitial": "",
      "importedId": "A58SAdsEspNDXMG6Gtzrhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116897,
      "firstName": "Sueyoon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "XcI-WHi9ei1hj45xCZXBUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116898,
      "firstName": "Elijah",
      "lastName": "Cobb",
      "middleInitial": "",
      "importedId": "MK3BU8exKv_MSFe3OLRisg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116899,
      "firstName": "Asreen",
      "lastName": "Rostami",
      "middleInitial": "",
      "importedId": "9lDKGWRcXUhNRYjt7cheTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116900,
      "firstName": "Guillermo",
      "lastName": "Pacho",
      "middleInitial": "",
      "importedId": "HXmoEs72ZRmATRbp4OVRVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116901,
      "firstName": "Gérard",
      "lastName": "Uzan",
      "middleInitial": "",
      "importedId": "CYtuDdK3HpGPzMZyFCXfrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116902,
      "firstName": "João",
      "lastName": "Diogo",
      "middleInitial": "",
      "importedId": "l1FB0RU5m9E4yKEMwjVoNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116903,
      "firstName": "Pejman",
      "lastName": "Saeghe",
      "middleInitial": "",
      "importedId": "wgVYUdzEYo_GHL_g4JUgmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116904,
      "firstName": "Mikihiro",
      "lastName": "Ueno",
      "middleInitial": "",
      "importedId": "wInLyzlSadETsLIOAFdEkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116905,
      "firstName": "Frank",
      "lastName": "ter Haar",
      "middleInitial": "B.",
      "importedId": "PL35ntpy27j9AgxjVItC_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116906,
      "firstName": "Carolina",
      "lastName": "Nicolau",
      "middleInitial": "Duarte",
      "importedId": "d0stL_qXrl53KdTJtC1qmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116907,
      "firstName": "Francisco",
      "lastName": "Caldeira",
      "middleInitial": "",
      "importedId": "KpAIgOuZiMr72a1NrUXm5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116908,
      "firstName": "Muhammad Zahid",
      "lastName": "Iqbal",
      "middleInitial": "",
      "importedId": "kYsyNQpKaVS3hWj89-CnYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116909,
      "firstName": "Guido Lemos",
      "lastName": "de Souza Filho",
      "middleInitial": "",
      "importedId": "qozdhZNECjCtGenA2Mnz8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116910,
      "firstName": "Cicero Inacio",
      "lastName": "da Silva",
      "middleInitial": "",
      "importedId": "FUEDvceQJDgKLHAfhg0d0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116911,
      "firstName": "Edward",
      "lastName": "Andrews",
      "middleInitial": "",
      "importedId": "O8RoAwZknoj3tGOmCAGfHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116912,
      "firstName": "Omar",
      "lastName": "Niamut",
      "middleInitial": "",
      "importedId": "ISsiwK1Agw8ejGlnQiIBVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116913,
      "firstName": "Leonor",
      "lastName": "Godinho",
      "middleInitial": "",
      "importedId": "U_9Vu0FhT87KZXPePvoWvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116914,
      "firstName": "Pedro",
      "lastName": "Beça",
      "middleInitial": "",
      "importedId": "VvYX293Q9bJSyef8hTbAMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116915,
      "firstName": "Andrew",
      "lastName": "MacIntyre",
      "middleInitial": "",
      "importedId": "-wFwt-EWWsU7jRiSBUwYPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116916,
      "firstName": "Sarah",
      "lastName": "Clinch",
      "middleInitial": "",
      "importedId": "DQB5-X25-6OVjYs7WP9zpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116917,
      "firstName": "Arka N",
      "lastName": "Mallela",
      "middleInitial": "",
      "importedId": "W9Scs84J4GRXw3mz1FX-zA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116918,
      "firstName": "Eleni",
      "lastName": "Papadopoulou",
      "middleInitial": "",
      "importedId": "8eSoBwY_lZNzXR7F7-i1-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116919,
      "firstName": "Carrie",
      "lastName": "Demmans Epp",
      "middleInitial": "",
      "importedId": "RA6CLxYmkLhrPQu7Oku55A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116920,
      "firstName": "Fotis",
      "lastName": "Giariskanis",
      "middleInitial": "",
      "importedId": "LpnZ9kpS1nczJqeY4Siayg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116921,
      "firstName": "Lucas",
      "lastName": "Aversari",
      "middleInitial": "",
      "importedId": "pUyjqouxYNPINj5hkbDUkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116922,
      "firstName": "Yannis",
      "lastName": "Kritikos",
      "middleInitial": "",
      "importedId": "Lzu9ZY76-lNAmycWH4_eog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116923,
      "firstName": "Flávio Luiz",
      "lastName": "Schiavoni",
      "middleInitial": "",
      "importedId": "k5c87U1U2p9gbFCylMZc6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116924,
      "firstName": "Pablo",
      "lastName": "Cesar",
      "middleInitial": "",
      "importedId": "iHc0TJyBZeQDKJv2npPzWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116925,
      "firstName": "Veronika",
      "lastName": "Kozlova",
      "middleInitial": "",
      "importedId": "iAZ7_tbFIAOkQ886WPhnAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116926,
      "firstName": "David",
      "lastName": "Murphy",
      "middleInitial": "",
      "importedId": "2-fgdf7fKZkyfRAiilK38A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116927,
      "firstName": "Arvin",
      "lastName": "Moshfegh",
      "middleInitial": "",
      "importedId": "R7Cdlh2qMsJ2Kw_0Xjbp7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116928,
      "firstName": "Kamil",
      "lastName": "Koniuch",
      "middleInitial": "",
      "importedId": "EyCnKUvD6ZhqCNaGqwH40g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116929,
      "firstName": "Jeanine",
      "lastName": "van Bruggen",
      "middleInitial": "",
      "importedId": "PLDV_B4x5zPOSvtiJKropA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116930,
      "firstName": "Radu-Daniel",
      "lastName": "Vatavu",
      "middleInitial": "",
      "importedId": "8aqyMcEhyX2llYoyIaN2iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116931,
      "firstName": "Sylvie",
      "lastName": "Dijkstra-Soudarissanane",
      "middleInitial": "",
      "importedId": "nJqYUo48cCccoIiwimPNJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116932,
      "firstName": "Anthi",
      "lastName": "Papanastasiou",
      "middleInitial": "",
      "importedId": "1QiTYx--twY6cliKGjWmMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116933,
      "firstName": "Stefan",
      "lastName": "Bajin",
      "middleInitial": "",
      "importedId": "TFlAsLt0VWa49zOGHk8EDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116934,
      "firstName": "Nuno",
      "lastName": "Silva",
      "middleInitial": "Tavares",
      "importedId": "xWXA-CgoNELwAOd9CtvJlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116935,
      "firstName": "Antoni",
      "lastName": "Oliver",
      "middleInitial": "",
      "importedId": "57LojtMbmhY_Xr0vTwESXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116936,
      "firstName": "Joanne",
      "lastName": "Parkes",
      "middleInitial": "",
      "importedId": "yEdQ3GJIjJOJ48IwvsKXQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116937,
      "firstName": "Max",
      "lastName": "Fortna",
      "middleInitial": "",
      "importedId": "aaj5cj0BBU_xTSfIT4yZMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116938,
      "firstName": "Talha",
      "lastName": "Khan",
      "middleInitial": "",
      "importedId": "_eIhjWkEH-lsm9ifFU-pHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116939,
      "firstName": "Ellen",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "YJxnRbqcH-hsEDADkpXG1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116940,
      "firstName": "Bruce",
      "lastName": "Weir",
      "middleInitial": "",
      "importedId": "03rHInfq6BbGK4pbbCiMdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116941,
      "firstName": "Nanda",
      "lastName": "van der Stap",
      "middleInitial": "",
      "importedId": "MJAW98lERBbcC2gofatWEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116942,
      "firstName": "Alina",
      "lastName": "Striner",
      "middleInitial": "",
      "importedId": "VYr7S94tdKamy_ISzECL6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116943,
      "firstName": "Leonor",
      "lastName": "Fermoselle",
      "middleInitial": "",
      "importedId": "f0Tuf_nWtZI5yChDJKBTHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116944,
      "firstName": "Beatrice",
      "lastName": "Fuchs",
      "middleInitial": "",
      "importedId": "pb-kCvPVRXe0po-aGZR79Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116945,
      "firstName": "Joseph C",
      "lastName": "Maroon",
      "middleInitial": "",
      "importedId": "5T792mHmrsNsaO0-T5NnfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116946,
      "firstName": "Héctor",
      "lastName": "Rivas Pagador",
      "middleInitial": "",
      "importedId": "sXK17nxOe81U2KD56z4XoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116947,
      "firstName": "Sergio",
      "lastName": "Cabrero Barros",
      "middleInitial": "",
      "importedId": "agbIvqFO49Xc3N3ZVYhnUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116948,
      "firstName": "Colin",
      "lastName": "Stricklin",
      "middleInitial": "",
      "importedId": "45zx6Opsh7o-CpeBZd9iYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116949,
      "firstName": "Jordi",
      "lastName": "Solsona Belenguer",
      "middleInitial": "",
      "importedId": "O7JJXgxIpzb9WzeJQQMdDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116950,
      "firstName": "Jason",
      "lastName": "Mendes",
      "middleInitial": "",
      "importedId": "wNEoUzLP460sywnlWJ-qQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116951,
      "firstName": "Mikel",
      "lastName": "Zorrilla",
      "middleInitial": "",
      "importedId": "gj-pkllsbEdUf4CGKjkksA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116952,
      "firstName": "João",
      "lastName": "Valente",
      "middleInitial": "P.S.S.",
      "importedId": "S69qxxmcxE8KNKHYIdSiDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116953,
      "firstName": "Filip",
      "lastName": "De Turck",
      "middleInitial": "",
      "importedId": "NUh5OGYNHOUg4u6IINZCFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116954,
      "firstName": "João",
      "lastName": "Lourenço",
      "middleInitial": "",
      "importedId": "6jFPXFVLOapR9VtJ6n2xAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116955,
      "firstName": "Maria",
      "lastName": "Torres Vega",
      "middleInitial": "",
      "importedId": "57YcElOPtgHo1jTDHGGuUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116956,
      "firstName": "Justin",
      "lastName": "Debloos",
      "middleInitial": "",
      "importedId": "fA_ysADhZ7oZyfliDhDC-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116957,
      "firstName": "Javier",
      "lastName": "del Molino",
      "middleInitial": "",
      "importedId": "bbGyorH0OxyoJi868zD1Zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116958,
      "firstName": "Iulia",
      "lastName": "Covalenco",
      "middleInitial": "",
      "importedId": "oC9W7pV0HQcJVlH5oRkdpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116959,
      "firstName": "Eftychia",
      "lastName": "Protopapadaki",
      "middleInitial": "",
      "importedId": "WiEMxCYyseJZTbZVB49LJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116960,
      "firstName": "Thomas",
      "lastName": "Röggla",
      "middleInitial": "",
      "importedId": "oMts-FgVjiqSOtH5p9_mUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116961,
      "firstName": "Gareth",
      "lastName": "Young",
      "middleInitial": "W.",
      "importedId": "2dESPWlxLdKPLgHNVCbdmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116962,
      "firstName": "Filipe",
      "lastName": "Pires",
      "middleInitial": "",
      "importedId": "pi5b3Q6jY2P2TIgRDlTyYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116963,
      "firstName": "Kevin",
      "lastName": "Lindén",
      "middleInitial": "",
      "importedId": "W66LV7_82eDPMSUdwum9DQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116964,
      "firstName": "Ewan",
      "lastName": "Johnson",
      "middleInitial": "",
      "importedId": "n_E9D5GvmbJuVGeb71mp_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116965,
      "firstName": "Tiffany",
      "lastName": "Marques",
      "middleInitial": "",
      "importedId": "qLBIcv9tX9xzTr7Aa468_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116966,
      "firstName": "Melina",
      "lastName": "Bernsland",
      "middleInitial": "",
      "importedId": "BhXtdkwHKmz9Z4JpfvSp-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116967,
      "firstName": "Pedro",
      "lastName": "Almeida",
      "middleInitial": "",
      "importedId": "3opM6q2OjBUxsBRTKdZVRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116968,
      "firstName": "Brigitte",
      "lastName": "Le Pévédic",
      "middleInitial": "",
      "importedId": "menatvq7NL9BZqpRrwZrBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 116969,
      "firstName": "Katerina",
      "lastName": "Mania",
      "middleInitial": "",
      "importedId": "P0l049N7eDXT2Nu9dawNiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117011,
      "firstName": "Sam",
      "lastName": "Frish",
      "middleInitial": "",
      "importedId": "Lj_KFN4dFxQorKzCt0JI9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117012,
      "firstName": "Lior",
      "lastName": "Somin",
      "middleInitial": "",
      "importedId": "MZT-5PVZx4Db89iBd1IWkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117013,
      "firstName": "Timothy",
      "lastName": "Neate",
      "middleInitial": "",
      "importedId": "J-ot7lpDAkXjMVlBx368ow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117014,
      "firstName": "Carolina",
      "lastName": "Cruz-Neira",
      "middleInitial": "",
      "importedId": "jTU_a7iLn7h6iVGCgV8h0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117015,
      "firstName": "Iris",
      "lastName": "Jennes",
      "middleInitial": "",
      "importedId": "Kd7oSGw66yE1-Z5gBuE7Jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117016,
      "firstName": "Stephen",
      "lastName": "Ramanoël",
      "middleInitial": "",
      "importedId": "W2lQsvxu2ANZzMQy3ViEQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117017,
      "firstName": "Maxine",
      "lastName": "Glancy",
      "middleInitial": "",
      "importedId": "K6guNjoz88tPnDBHUVx4yA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117018,
      "firstName": "Alexandre",
      "lastName": "Nevsky",
      "middleInitial": "",
      "importedId": "L5m6wO_6ZXRwwGTtueP3RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117019,
      "firstName": "Anna",
      "lastName": "Sheremetieva",
      "middleInitial": "",
      "importedId": "mgalYTIzkDtokhSRY7r9CA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117020,
      "firstName": "Ashutosh",
      "lastName": "Singla",
      "middleInitial": "",
      "importedId": "bPc7EM4cJwK-DJ_jk0sR1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117021,
      "firstName": "Alexander",
      "lastName": "Raake",
      "middleInitial": "",
      "importedId": "169_SECnbge0qGbMUaaKAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117022,
      "firstName": "Philip J.B.",
      "lastName": "Jackson",
      "middleInitial": "",
      "importedId": "uacOqx0H24RJ0TvdXdMshQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117023,
      "firstName": "Marco",
      "lastName": "Winckler",
      "middleInitial": "",
      "importedId": "wjct9CEOlrNsOojhNFC-hQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117024,
      "firstName": "Cedric",
      "lastName": "Spindler",
      "middleInitial": "Julian",
      "importedId": "HmDHk_LiC4SjvSZTGA_3BQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117025,
      "firstName": "Katharina",
      "lastName": "Wünsche",
      "middleInitial": "",
      "importedId": "1Ro40kY25FSu8NwGEbYIrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117026,
      "firstName": "Jian",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "Nd8fgQWxdsEw06-g39dg4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117027,
      "firstName": "HaeEun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "Qnw7AOwUoinBUtcSjOG3xw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117028,
      "firstName": "Laura",
      "lastName": "Koesten",
      "middleInitial": "",
      "importedId": "0Z7ljM-1fEsKz2V5iT-9pw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117029,
      "firstName": "Olli",
      "lastName": "Rummukainen",
      "middleInitial": "S.",
      "importedId": "y6lIiPYVyeW-7s4JmdeGuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117030,
      "firstName": "Juho",
      "lastName": "Hamari",
      "middleInitial": "",
      "importedId": "TiS2zuDoXvBPfT0hov0tLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117031,
      "firstName": "Elias",
      "lastName": "Blanckaert",
      "middleInitial": "",
      "importedId": "j8iZQKN_ukM9_KY5f7ACnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117032,
      "firstName": "Jan",
      "lastName": "Torpus",
      "middleInitial": "",
      "importedId": "_FAZlc63QHRPV08dl-1-YA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117033,
      "firstName": "Stephanie",
      "lastName": "Carnell",
      "middleInitial": "",
      "importedId": "10zOfmepAyvY7yraZHAStg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117034,
      "firstName": "Patrick",
      "lastName": "Le Callet",
      "middleInitial": "",
      "importedId": "Gp25CAIMfxZ2N3L3mhbp9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117035,
      "firstName": "Liangding",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "j0DPzOLZm1OWQnoPBcN2Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117036,
      "firstName": "Mykola",
      "lastName": "Maksymenko",
      "middleInitial": "",
      "importedId": "3KLFSw4JDHPFyFUZpecWRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117037,
      "firstName": "Linda Walters",
      "lastName": "Walters",
      "middleInitial": "",
      "importedId": "iPvqP-S2yWzmEqGVUMAuJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117038,
      "firstName": "Wendy",
      "lastName": "Van den Broeck",
      "middleInitial": "",
      "importedId": "us5OJE7SsfpgAhbL7Abzcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117039,
      "firstName": "Hui-Yin",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "_Rerl9zhgod2fl9p0dEBGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117040,
      "firstName": "Elena",
      "lastName": "Simperl",
      "middleInitial": "",
      "importedId": "hTtkm0OgvCdQQmsurmgZmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117041,
      "firstName": "Zachary",
      "lastName": "McKendrick",
      "middleInitial": "",
      "importedId": "KK7ciEaki75uey8bsEfVWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117042,
      "firstName": "Alexandre",
      "lastName": "Bruckert",
      "middleInitial": "",
      "importedId": "6lQzyg-Cf8TWrW-4YAD-xQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117043,
      "firstName": "Mila",
      "lastName": "Bujić",
      "middleInitial": "",
      "importedId": "wxXFA59Uo5ZUwwFm1ZrTYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117044,
      "firstName": "Florent",
      "lastName": "Robert",
      "middleInitial": "",
      "importedId": "_J5Dp8QXuNH2aKzxX0LevQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117045,
      "firstName": "Emanuël A. P.",
      "lastName": "Habets",
      "middleInitial": "",
      "importedId": "-1kfzFhsujatO9oxTji4KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117046,
      "firstName": "Lucie",
      "lastName": "Lévêque",
      "middleInitial": "",
      "importedId": "toUB8GR0VC0QDz0InUxM_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117047,
      "firstName": "Mikko",
      "lastName": "Salminen",
      "middleInitial": "",
      "importedId": "YWmnzP7epSmg6WVYCrvJHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117048,
      "firstName": "Katherine",
      "lastName": "Harris",
      "middleInitial": "",
      "importedId": "6cR9YU7DNiDyxp2e4A528w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117049,
      "firstName": "Ihor",
      "lastName": "Romanovych",
      "middleInitial": "",
      "importedId": "GlA0AUEPbJFw0Axb-ROScQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117050,
      "firstName": "Torsten",
      "lastName": "Möller",
      "middleInitial": "",
      "importedId": "QVqbumoOn903JyXnF25Jjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117051,
      "firstName": "Matthieu",
      "lastName": "Perreira da Silva",
      "middleInitial": "",
      "importedId": "aIyU8dIRIw1NJ1wKp_zdrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117052,
      "firstName": "Dirk",
      "lastName": "Reiners",
      "middleInitial": "",
      "importedId": "g1bg0rScAZLgFqn2biQOug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117053,
      "firstName": "Ehud",
      "lastName": "Sharlin",
      "middleInitial": "",
      "importedId": "qWDVW9DZNkt9PRyucpJ8Fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117054,
      "firstName": "Jonas",
      "lastName": "Kellermeyer",
      "middleInitial": "",
      "importedId": "QdJsgwPYbsxvf-hKSvpUxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117055,
      "firstName": "Craig",
      "lastName": "Cieciura",
      "middleInitial": "R",
      "importedId": "sqBHAkEuCxiujNkm6Ccp9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117056,
      "firstName": "Gautam",
      "lastName": "Vishwanath",
      "middleInitial": "",
      "importedId": "nFaSUwrFyzUVTYgiACroRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117057,
      "firstName": "Patrick",
      "lastName": "Finn",
      "middleInitial": "",
      "importedId": "8oEtzk2jSa-Rq61t6N6hcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117058,
      "firstName": "Orestis",
      "lastName": "Georgiou",
      "middleInitial": "",
      "importedId": "o_wMyc65_ErHS6_e3OB6Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117059,
      "firstName": "Lucile",
      "lastName": "Sassatelli",
      "middleInitial": "",
      "importedId": "WX4KZxrkuJ3-0CNSJgLxcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117060,
      "firstName": "Daniel",
      "lastName": "Gatica-Perez",
      "middleInitial": "",
      "importedId": "UCqX3IuE6gYHmz5H6AZD6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117061,
      "firstName": "Auriane",
      "lastName": "Gros",
      "middleInitial": "",
      "importedId": "xfL3M44n-Qa91Ij9PLGIGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117062,
      "firstName": "Thomas",
      "lastName": "Robotham",
      "middleInitial": "",
      "importedId": "ex5fif4U51XiUzkbIlhx_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117077,
      "firstName": "Yawen",
      "lastName": "Deng",
      "middleInitial": "",
      "importedId": "d5tC0w9rvCR3BcHIMC7i7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117078,
      "firstName": "CHARLES",
      "lastName": "DORMEVAL",
      "middleInitial": "",
      "importedId": "gga6NFTrYR9J7WWzGGCqnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117079,
      "firstName": "Ifat",
      "lastName": "Yasin",
      "middleInitial": "",
      "importedId": "YQsFjk2jzdtdH4YVqcSu6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117080,
      "firstName": "Josep",
      "lastName": "Paradells",
      "middleInitial": "",
      "importedId": "PK74iOJKV7vAHWpIhWgPjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117081,
      "firstName": "Felix",
      "lastName": "Immohr",
      "middleInitial": "",
      "importedId": "up1PAR0RYZ43ZN0pPtmBsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117082,
      "firstName": "Desmond",
      "lastName": "Bloemers",
      "middleInitial": "",
      "importedId": "Fa9fJcIO895kZ6UmE8elUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117083,
      "firstName": "Youngwoo",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "X8xWZXfVJGFaO85crokYrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117084,
      "firstName": "Matei",
      "lastName": "Mancas",
      "middleInitial": "",
      "importedId": "A2ijntNRkIfscAlYKBwfVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117085,
      "firstName": "Wolfgang",
      "lastName": "Hürst",
      "middleInitial": "",
      "importedId": "xvUq-koas5snpM0S6QTA3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117086,
      "firstName": "Brigitte",
      "lastName": "Le Pévédic",
      "middleInitial": "",
      "importedId": "4DBTDGfhcK6GIEp5fH4RmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117087,
      "firstName": "Yujing",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "lsZt1TaMzV20g9bVFl13xQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117088,
      "firstName": "Patrick",
      "lastName": "Fontaine",
      "middleInitial": "",
      "importedId": "j_CGqRK9ed3Evd5GkQANXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117089,
      "firstName": "Amar",
      "lastName": "Tious",
      "middleInitial": "",
      "importedId": "qVnYOBnjQcSJ9NpmJlJsmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117090,
      "firstName": "Niall",
      "lastName": "Murray",
      "middleInitial": "",
      "importedId": "aGKt5t4WeLvCSS1JBsKc4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117091,
      "firstName": "Jörg",
      "lastName": "Gutsche",
      "middleInitial": "",
      "importedId": "Fis0q7wadM61ZK_A_WPeVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117092,
      "firstName": "Annika",
      "lastName": "Neidhardt",
      "middleInitial": "",
      "importedId": "nvdiQU4BOWoQUPtO1HiCdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117093,
      "firstName": "KUMAR",
      "lastName": "RAHUL",
      "middleInitial": "",
      "importedId": "nEJ3Uf1ORR8h3vdyKTsSqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117094,
      "firstName": "Mohamed",
      "lastName": "Benkedadra",
      "middleInitial": "",
      "importedId": "_mzzxdk3chZbMGg2ppbavw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117095,
      "firstName": "Mikolaj",
      "lastName": "Leszczuk",
      "middleInitial": "",
      "importedId": "lgGys_qGcVGSL_6N8EKzNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117096,
      "firstName": "Yuta",
      "lastName": "Hagio",
      "middleInitial": "",
      "importedId": "Vnny6cK3BIvtM1dSeRxUiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117097,
      "firstName": "Andreina",
      "lastName": "Nunez Morales",
      "middleInitial": "",
      "importedId": "L0tJBWAFgh-y4W54_XfIeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117098,
      "firstName": "Emmanuel",
      "lastName": "Thomas",
      "middleInitial": "",
      "importedId": "R2ynjzVmT93LWmYelE4z-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117099,
      "firstName": "Yujie",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "7AxveQfxsBUlx7BAr42LvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117100,
      "firstName": "Sonia",
      "lastName": "Desmoulin-Canselier",
      "middleInitial": "",
      "importedId": "wEr3ELEJ6tAJatjGo6oaaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117101,
      "firstName": "Victoria",
      "lastName": "Popova",
      "middleInitial": "",
      "importedId": "kb3MlOv2PD8M9XsndlSv1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117102,
      "firstName": "JULIE",
      "lastName": "CLOAREC-MICHAUD",
      "middleInitial": "",
      "importedId": "ocehC9-z2uMSGYQdalZtJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117103,
      "firstName": "Katerina",
      "lastName": "Mania",
      "middleInitial": "",
      "importedId": "mW_fD7j7mNy46DE36mTGbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117104,
      "firstName": "Emmanuel",
      "lastName": "Tsekleves",
      "middleInitial": "",
      "importedId": "S17cHk46dm26ZeVy1z-yqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117105,
      "firstName": "Toinon",
      "lastName": "Vigier",
      "middleInitial": "",
      "importedId": "mmM8oYeTBF78wr4YYYUrlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117106,
      "firstName": "Omar",
      "lastName": "SEDDATI",
      "middleInitial": "",
      "importedId": "wn3o2URgS6686vh1cKSS7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117107,
      "firstName": "Bernard",
      "lastName": "Gosselin",
      "middleInitial": "",
      "importedId": "0pctTkKD3j4_cPPcViHrHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117108,
      "firstName": "Ghazaleh",
      "lastName": "Tanhaei",
      "middleInitial": "",
      "importedId": "1wIRph_ly0ePM3CqKeVBvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117109,
      "firstName": "Pedro",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "Wdb9ohXUig4xwHPdkLWq8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117110,
      "firstName": "Marc",
      "lastName": "Demeuse",
      "middleInitial": "",
      "importedId": "lYcgaP9alGqSawEwc9UVNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117111,
      "firstName": "Tiffany",
      "lastName": "Marques",
      "middleInitial": "",
      "importedId": "pCuK0E198LWeWlvda6ctog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117112,
      "firstName": "Francois",
      "lastName": "Rocca",
      "middleInitial": "",
      "importedId": "EZ2Ht2D89pbzoylljJxBqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117113,
      "firstName": "Iva",
      "lastName": "Georgieva",
      "middleInitial": "",
      "importedId": "tsWn6c405KT1AtWTFsj5Rg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117114,
      "firstName": "Yutaka",
      "lastName": "Kaneko",
      "middleInitial": "",
      "importedId": "o8ZdlFglTE94LBCGAkqlWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117115,
      "firstName": "Ioannis",
      "lastName": "Chatzigiannakis",
      "middleInitial": "",
      "importedId": "id8KEo47yNScaKTRkJ348g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117116,
      "firstName": "Jingwen",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "UaRhnmUpvYRQaKAiLFoZjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117117,
      "firstName": "Cosmin",
      "lastName": "Stejerean",
      "middleInitial": "",
      "importedId": "WX9dDiMC-Ypxt6sg8WA9Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117118,
      "firstName": "Agnès",
      "lastName": "Van Daele",
      "middleInitial": "",
      "importedId": "xQpXD6iIrg1HZulZJBccAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117119,
      "firstName": "Sergi",
      "lastName": "Fernández",
      "middleInitial": "",
      "importedId": "ks_e_kqq0oUmlKDkIKXF0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117120,
      "firstName": "Kevin",
      "lastName": "Riou",
      "middleInitial": "",
      "importedId": "lsggPi1VJ9jbRkD3Gm40pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117121,
      "firstName": "Daniel",
      "lastName": "Lock",
      "middleInitial": "",
      "importedId": "oNv1A2q2_o-d2D8qv-m-FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117122,
      "firstName": "Aitor",
      "lastName": "Rovira",
      "middleInitial": "",
      "importedId": "LhIM-cDO8n0LqJyv021pew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117123,
      "firstName": "SRIRAM",
      "lastName": "SETHURAMAN",
      "middleInitial": "",
      "importedId": "Mx0gVo-M7M9pukqGDwpEqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117124,
      "firstName": "Mario",
      "lastName": "Montagud Climent",
      "middleInitial": "",
      "importedId": "cbccwVdEbQtD4gTFScDDKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117125,
      "firstName": "Philippe",
      "lastName": "Guillotel",
      "middleInitial": "",
      "importedId": "F-Yvu9GnW45Ai2pANTRadQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117126,
      "firstName": "Michael",
      "lastName": "Neri",
      "middleInitial": "",
      "importedId": "MTLDrMr7rtRIVh5lDcn4rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117127,
      "firstName": "Borbála",
      "lastName": "Tölgyesi",
      "middleInitial": "",
      "importedId": "DrL4goezyPhh9YZwIOcNsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117128,
      "firstName": "Rakesh Rao",
      "lastName": "Ramachandra Rao",
      "middleInitial": "",
      "importedId": "DEt73IzeD_IDSQVfLKHscg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117129,
      "firstName": "Go",
      "lastName": "Ohtake",
      "middleInitial": "",
      "importedId": "f8NIMlOAVgWR0SM-m4Cypw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117130,
      "firstName": "Elisavet",
      "lastName": "Tsekeri",
      "middleInitial": "",
      "importedId": "i1_B_HHHk1zochVn5kzfOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117131,
      "firstName": "Juan Antonio",
      "lastName": "De Rus Arance",
      "middleInitial": "",
      "importedId": "HTtR7KdSx7IvhP00E1RZyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117132,
      "firstName": "Balázs",
      "lastName": "Buri",
      "middleInitial": "",
      "importedId": "mW1M3B1VPPTxVabUVEXVdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117133,
      "firstName": "Miguel",
      "lastName": "Fernández-Dasí",
      "middleInitial": "",
      "importedId": "-Z6h_Dj3nfC8GHs7GgZqGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117134,
      "firstName": "Patrick",
      "lastName": "LE CALLET",
      "middleInitial": "",
      "importedId": "e8N7rF2BmzIL6GvXZov_NA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117135,
      "firstName": "Dominika",
      "lastName": "Wanat",
      "middleInitial": "K",
      "importedId": "I1_bOwgTHyvNgSsuxyt95w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117136,
      "firstName": "Botond",
      "lastName": "Tobai",
      "middleInitial": "",
      "importedId": "4-gj78UJnPErF3rK-cWPrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117137,
      "firstName": "Masakazu",
      "lastName": "Hirokawa",
      "middleInitial": "",
      "importedId": "KRC2ByMI04ACqOoaBCZ5gA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117138,
      "firstName": "IOANNIS",
      "lastName": "KATSAVOUNIDIS",
      "middleInitial": "",
      "importedId": "_1SYTQVcSRNM4xrG2AIliA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117139,
      "firstName": "Alexandre",
      "lastName": "Agossah",
      "middleInitial": "",
      "importedId": "-K6S-_GyzKEkuB4HxSjzFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117140,
      "firstName": "Jit",
      "lastName": "Chatterjee",
      "middleInitial": "",
      "importedId": "XGN2WpvulpqDfmLjg6v01Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117141,
      "firstName": "Antoine",
      "lastName": "Maiorca",
      "middleInitial": "",
      "importedId": "vS0lJsiX0ZENf1yto7oxlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117142,
      "firstName": "Juliana",
      "lastName": "Camargo",
      "middleInitial": "Duarte",
      "importedId": "fENQeq_3eFOJaJKVqnHfPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117143,
      "firstName": "Gareth",
      "lastName": "Rendle",
      "middleInitial": "",
      "importedId": "mYAFdFgex5gHS6s4LJTDvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117144,
      "firstName": "András",
      "lastName": "Szabó",
      "middleInitial": "",
      "importedId": "J8Fk3A-zirHsDt1UP4Hojg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117145,
      "firstName": "Alex",
      "lastName": "Boutin",
      "middleInitial": "",
      "importedId": "qay8R4eX3SGgsOggBF19pg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117146,
      "firstName": "Patrice",
      "lastName": "Hirtzlin",
      "middleInitial": "",
      "importedId": "FqTSnsa_ajPIaWZzTbOoLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117147,
      "firstName": "Mohammed",
      "lastName": "El Amine MOKHTARI",
      "middleInitial": "",
      "importedId": "atLU-YAQr1olP6_i1XhmKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117148,
      "firstName": "Dionysia",
      "lastName": "Kolokotsa",
      "middleInitial": "",
      "importedId": "EvOODy-cVjuC2-dnAFLAxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117149,
      "firstName": "Andreas",
      "lastName": "Mauthe",
      "middleInitial": "U.",
      "importedId": "Swsr_sQ1NJ6ZH-VQlZxing",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117150,
      "firstName": "Preslav",
      "lastName": "Nakov",
      "middleInitial": "",
      "importedId": "T_hgLOgeuqrOkxgjtfwBbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117151,
      "firstName": "MICKAEL",
      "lastName": "LAFONTAINE",
      "middleInitial": "",
      "importedId": "1nqvc82jpXXHFVPl0GE4VQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117152,
      "firstName": "Hisayuki",
      "lastName": "Ohmata",
      "middleInitial": "",
      "importedId": "63-turOgUrCQk9luWzx0hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117153,
      "firstName": "pierrick",
      "lastName": "Jouet",
      "middleInitial": "",
      "importedId": "b4xMqr1_7D07VB4NBBQNhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117154,
      "firstName": "Lisa",
      "lastName": "Izzouzi",
      "middleInitial": "",
      "importedId": "xxGYOLRxzFB8A1ZegIfHHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117155,
      "firstName": "Marco",
      "lastName": "Carli",
      "middleInitial": "",
      "importedId": "7AbHvxRiiF6ewwfZuEAJ1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117156,
      "firstName": "Vincent",
      "lastName": "STRAGIER",
      "middleInitial": "",
      "importedId": "plMDfRQ8OFA8WHTWvI0wwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117157,
      "firstName": "Hideya",
      "lastName": "Mino",
      "middleInitial": "",
      "importedId": "u40iAFLwkCc0mBesvLw1Aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117158,
      "firstName": "Ágnes Karolina",
      "lastName": "Bakk",
      "middleInitial": "",
      "importedId": "MkoonGQIhp7h_3-8qbSEbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117159,
      "firstName": "Shoko",
      "lastName": "Fujii",
      "middleInitial": "",
      "importedId": "mf0yvCWAoLu-0YDsYsp2Ig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117160,
      "firstName": "Simon",
      "lastName": "Wong",
      "middleInitial": "",
      "importedId": "skp6NVSsCcyQNrOC0RiCPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117161,
      "firstName": "Frédérique",
      "lastName": "Krupa",
      "middleInitial": "",
      "importedId": "fYGJSP4g82UsfaNc2Q713Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117162,
      "firstName": "Emmanouil",
      "lastName": "Potetsianakis",
      "middleInitial": "",
      "importedId": "7lpE4Aq0jvT9SCWdJX_e_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117163,
      "firstName": "Madison",
      "lastName": "Dave",
      "middleInitial": "",
      "importedId": "UpTMaEakTbWDT4y_Kb-_bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117164,
      "firstName": "James",
      "lastName": "She",
      "middleInitial": "",
      "importedId": "UwdB4PlxPLuPlRif1Tf7Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117165,
      "firstName": "Kaulyaalalwa",
      "lastName": "Peter",
      "middleInitial": "",
      "importedId": "jgL9dZFkQX3Hoe4QYk18sQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117166,
      "firstName": "Christoph",
      "lastName": "Lürig",
      "middleInitial": "",
      "importedId": "dox9Y9tyxp9GqPjS0z96kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117167,
      "firstName": "Lorenzo",
      "lastName": "Imbesi",
      "middleInitial": "",
      "importedId": "MR245m4oH9I1YFepConodg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117168,
      "firstName": "Antoine",
      "lastName": "Derobertmasure",
      "middleInitial": "",
      "importedId": "jv0sHunc20Zkeos-ReDJ7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117169,
      "firstName": "Konstantinos",
      "lastName": "Gobakis",
      "middleInitial": "",
      "importedId": "_RcEDJfMIgwHlOdG7CL7tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117170,
      "firstName": "Divya",
      "lastName": "Bhargavi",
      "middleInitial": "",
      "importedId": "Rz5jAAUR_uR-TmOG3_Bs7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117171,
      "firstName": "Tilo",
      "lastName": "Mentler",
      "middleInitial": "",
      "importedId": "ibrv0c4ZqtThMEOZ7ogVZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117172,
      "firstName": "Saad",
      "lastName": "Maqbool",
      "middleInitial": "Ahmed",
      "importedId": "6aY6JL5mXe6-xFy2AVM6Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117173,
      "firstName": "Tom",
      "lastName": "ROY",
      "middleInitial": "",
      "importedId": "-EBuWqHLJI5v7_h2wk8IYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117174,
      "firstName": "Daniel",
      "lastName": "Pereira",
      "middleInitial": "",
      "importedId": "izkbDgxvgnTl-JGO0HwRkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117175,
      "firstName": "Minas",
      "lastName": "Katsiokalis",
      "middleInitial": "",
      "importedId": "CnYAYOir_qpUv2boqm-p_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117176,
      "firstName": "Eleuda",
      "lastName": "Nunez",
      "middleInitial": "",
      "importedId": "hEEAtW81k3YKXPYY8EQrng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117177,
      "firstName": "Sidi Ahmed",
      "lastName": "Mahmoudi",
      "middleInitial": "",
      "importedId": "MGpPmsfTo-9KpoTM60pkaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117178,
      "firstName": "Vincent",
      "lastName": "Ricordel",
      "middleInitial": "",
      "importedId": "Dn5UqlS0sAlFOysz0aMSUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117179,
      "firstName": "Selma",
      "lastName": "Auala",
      "middleInitial": "N.O",
      "importedId": "j1X-x5wZwwVXGD38MxRDKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117180,
      "firstName": "Anthony",
      "lastName": "Steed",
      "middleInitial": "",
      "importedId": "L7vbmDhPr9tJmz7gPBK-nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117181,
      "firstName": "Syed",
      "lastName": "Uddin",
      "middleInitial": "G",
      "importedId": "-Vnjl6EDpeeJstucqWy_mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117182,
      "firstName": "Jorge",
      "lastName": "Abreu",
      "middleInitial": "",
      "importedId": "6MoAr8u1FEDx6fsIej_IrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117183,
      "firstName": "Máté",
      "lastName": "Barkóczi",
      "middleInitial": "",
      "importedId": "iiup7iV6pV7-HeqaXdvhiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117184,
      "firstName": "Shu",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "kUX0CQNvcrJjiuQqCEX-fQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117185,
      "firstName": "Conor",
      "lastName": "Keighrey",
      "middleInitial": "",
      "importedId": "U72jHu6y2xSCTnUKotfeiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117186,
      "firstName": "Sia",
      "lastName": "Gholami",
      "middleInitial": "",
      "importedId": "yOFQGS3BgWdkq1V9B2oTjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117187,
      "firstName": "Christian",
      "lastName": "Roth",
      "middleInitial": "",
      "importedId": "YmHTYlyIpvxLwgOC49ppnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117188,
      "firstName": "Valérie",
      "lastName": "Duvivier",
      "middleInitial": "",
      "importedId": "Q813vvLa2KoHQ12gJmJLcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117189,
      "firstName": "Natalia",
      "lastName": "Cieplińska",
      "middleInitial": "",
      "importedId": "EsRSgGBGS5MF_PHioENM5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117190,
      "firstName": "Charlotte",
      "lastName": "Scarpa",
      "middleInitial": "",
      "importedId": "Qnc8kPB-TBtCCcQZy6H72g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117191,
      "firstName": "Quentin",
      "lastName": "GALVANE",
      "middleInitial": "",
      "importedId": "L_ghtUOCN0gqo0km4NUCog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117192,
      "firstName": "Bernd",
      "lastName": "Froehlich",
      "middleInitial": "",
      "importedId": "ZrSKlpvVHOHGINAPpsT0bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117193,
      "firstName": "Ali",
      "lastName": "Ak",
      "middleInitial": "",
      "importedId": "O3f9xSIRyF7Q-vUc3Fx9sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117194,
      "firstName": "Jun",
      "lastName": "Goto",
      "middleInitial": "",
      "importedId": "GO2go8aO1WlbEy0dUClS1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117195,
      "firstName": "Makoto",
      "lastName": "Okuda",
      "middleInitial": "",
      "importedId": "hFf3spOwvhcvLfd9F6yu2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117196,
      "firstName": "Michal",
      "lastName": "Grega",
      "middleInitial": "",
      "importedId": "39N3JhyC0wYE3hDfSM0THA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117197,
      "firstName": "Dominique",
      "lastName": "Archambault",
      "middleInitial": "",
      "importedId": "ZIcXE3XyeHfV8g7TQxeysQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117198,
      "firstName": "Aikaterini",
      "lastName": "Lilli",
      "middleInitial": "",
      "importedId": "teu6-JQ6-hqCCpLqOKEdxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117199,
      "firstName": "Thierry",
      "lastName": "RAVET",
      "middleInitial": "",
      "importedId": "F1PqxEy3vabGs72BANW2eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117200,
      "firstName": "Karan",
      "lastName": "Sindwani",
      "middleInitial": "",
      "importedId": "iVllas_9vNyDG-vCQXqANw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117201,
      "firstName": "Petra",
      "lastName": "Jääskeläinen",
      "middleInitial": "",
      "importedId": "CLLWKt1TUbGAIDEzGOKO1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117202,
      "firstName": "Steve",
      "lastName": "Göring",
      "middleInitial": "",
      "importedId": "kJfJtaMvkZwOY2-B-jdejg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117203,
      "firstName": "Lynda",
      "lastName": "Hardman",
      "middleInitial": "",
      "importedId": "A_miD2RYcnay-osxeFnqkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117204,
      "firstName": "Hiroshi",
      "lastName": "Fujisawa",
      "middleInitial": "",
      "importedId": "Q5EOBw2erX9tBQmH7z637w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117205,
      "firstName": "Satoshi",
      "lastName": "Fujitsu",
      "middleInitial": "",
      "importedId": "wqnI3ar2ZRQQTpk0xLTfwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117206,
      "firstName": "Chen",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "PEWJ_1GmIDe-xMcqC2OQhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117207,
      "firstName": "Victor",
      "lastName": "Duvivier",
      "middleInitial": "Roger, Yves",
      "importedId": "xN42O4stMXA-oHUcxYj7ow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117208,
      "firstName": "Marina",
      "lastName": "Kamimura",
      "middleInitial": "",
      "importedId": "cJFfQvGVhjDyDozFCPIWIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117209,
      "firstName": "wala",
      "lastName": "Elsharif",
      "middleInitial": "",
      "importedId": "n4LXPavM-cYsZUq-DfVJ6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117210,
      "firstName": "Maximo",
      "lastName": "Cobos",
      "middleInitial": "",
      "importedId": "sB6p1cTW6Wu8_6k66D6AUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117211,
      "firstName": "Thierry",
      "lastName": "Dutoit",
      "middleInitial": "",
      "importedId": "Wx4L-XAcFPv1yQUHopjoNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117212,
      "firstName": "Isaac",
      "lastName": "Fraile",
      "middleInitial": "",
      "importedId": "zUjOv61euT0LXT2gmYCY5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117213,
      "firstName": "Romain",
      "lastName": "Cohendet",
      "middleInitial": "",
      "importedId": "upEyu9mxCeFOIrao661UOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117214,
      "firstName": "Rita",
      "lastName": "Santos",
      "middleInitial": "",
      "importedId": "Uyr8HMO6GVFbCmsimuhSqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117215,
      "firstName": "Ege",
      "lastName": "Sezen",
      "middleInitial": "",
      "importedId": "jI2zqzZR_CGyTEXu9oLmZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117216,
      "firstName": "Shinya",
      "lastName": "Abe",
      "middleInitial": "",
      "importedId": "v9ONYJbsVUu1f4nJ1UE3tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117217,
      "firstName": "Stephanie",
      "lastName": "Arevalo Arboleda",
      "middleInitial": "",
      "importedId": "E223_G5YIWw3vmJR-9SoCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117218,
      "firstName": "Gurvan",
      "lastName": "LECUYER",
      "middleInitial": "",
      "importedId": "mKLLqdh30kURgVcO8_Vpxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117219,
      "firstName": "ANNE FLORE",
      "lastName": "PERRIN",
      "middleInitial": "",
      "importedId": "La4fHdZ6H36y66NupZxsWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117220,
      "firstName": "Anna",
      "lastName": "Ferrarotti",
      "middleInitial": "",
      "importedId": "caWt9rZt-x7XZMF847P3_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117221,
      "firstName": "Etienne Faivre",
      "lastName": "D'Arcier",
      "middleInitial": "",
      "importedId": "rmqBvEJT3SrvTkJ99D9yJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117222,
      "firstName": "Gwénaelle",
      "lastName": "Haese",
      "middleInitial": "",
      "importedId": "5bakjkft5mRBLn06KfK1Kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117223,
      "firstName": "DENISE",
      "lastName": "NOYES",
      "middleInitial": "",
      "importedId": "5kZzaVehA66nm3KiUbNaMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117224,
      "firstName": "KAIWEN",
      "lastName": "DONG",
      "middleInitial": "",
      "importedId": "MYaojvyyO46SE2cKhOuYBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 117225,
      "firstName": "Elias",
      "lastName": "Ennadifi",
      "middleInitial": "",
      "importedId": "BiTGxz84-cspAqbHsP9AZA",
      "source": "PCS",
      "affiliations": []
    }
  ],
  "recognitions": []
}