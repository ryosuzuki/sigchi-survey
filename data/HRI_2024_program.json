{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10109,
    "shortName": "HRI",
    "displayShortName": "",
    "year": 2024,
    "startDate": 1710115200000,
    "endDate": 1710460800000,
    "fullName": "19th Annual ACM/IEEE International Conference on Human Robot Interaction",
    "url": "https://humanrobotinteraction.org/2024/",
    "location": "Boulder, Colorado",
    "timeZoneOffset": -360,
    "timeZoneName": "America/Denver",
    "logoUrl": "https://files.sigchi.org/conference/logo/10109/7d2661df-c3da-846c-cd43-49e78d9f6763.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10109/ee9fcdfc-85bd-e4b2-f47d-c6104489ad0d.html",
    "name": "HRI 2024",
    "noteToConference": "Procedings in ACM Digital Library: https://dl.acm.org/doi/proceedings/10.1145/3610977"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 39,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": true,
    "publicationDate": "2024-03-14 03:09:42+00"
  },
  "sponsors": [
    {
      "id": 10503,
      "name": "IEEE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/3862d67b-93d2-0dbc-8db3-18c46cf37375.png",
      "levelId": 10290,
      "url": "https://www.ieee.org/",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10504,
      "name": "IEEE Robotics and Automation Society",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/774825f9-20b3-9658-baba-21fdb22055bb.png",
      "levelId": 10290,
      "url": "https://www.ieee-ras.org",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10505,
      "name": "Association for Computer Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/e69924bc-33e0-c3cb-9a4c-48c2a081004e.png",
      "levelId": 10290,
      "url": "https://acm.org",
      "order": 2,
      "extraPadding": 8
    },
    {
      "id": 10506,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/f4c6d984-6778-c464-5cc6-4bb1a69e843c.png",
      "levelId": 10290,
      "url": "https://sigchi.org/",
      "order": 3,
      "extraPadding": 8
    },
    {
      "id": 10507,
      "name": "SIGAI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/5a4e637a-5b26-a9e7-3bad-e624878b8caf.png",
      "levelId": 10290,
      "url": "https://sigai.acm.org/main/",
      "order": 4,
      "extraPadding": 8
    },
    {
      "id": 10508,
      "name": "AAAI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/494af4ed-283e-c9c6-6544-99f2accb7767.png",
      "levelId": 10305,
      "url": "https://aaai.org/",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10509,
      "name": "Human Factors and Ergonomics Society",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/45fd63ea-fa91-e32f-eb18-57170d8ef5b8.png",
      "levelId": 10305,
      "url": "https://www.hfes.org/",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10510,
      "name": "Furhat Robotics",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/951123bc-eb89-4cbd-1b01-4435b01164a0.png",
      "levelId": 10306,
      "url": "https://furhatrobotics.com/",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10511,
      "name": "Enchanted Tools",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/17842438-d641-a491-9f6e-3bfefe95617b.png",
      "levelId": 10306,
      "url": "https://enchanted.tools/",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10512,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/400d38c2-4da3-f60a-0e31-26b0b6d65975.png",
      "levelId": 10307,
      "url": "https://www.google.com/",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10513,
      "name": "Navel Robotics",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/cbbbe10e-32dc-fbb7-63d6-c6fa8fe0284b.png",
      "levelId": 10307,
      "url": "https://www.google.com/",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10514,
      "name": "Semio",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/eb940c9f-5d85-20fd-f3dc-84c4adb0b75e.png",
      "levelId": 10307,
      "url": "https://semio.ai/",
      "order": 2,
      "extraPadding": 8
    },
    {
      "id": 10515,
      "name": "hello robot",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/4b3131c5-5f95-ab9f-fbb1-537615212ca4.png",
      "levelId": 10308,
      "url": "https://hello-robot.com/",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10516,
      "name": "Tangram Vision",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/e8b2f8fe-e8ba-55ac-eadc-fa0fbeee7718.png",
      "levelId": 10308,
      "url": "https://www.tangramvision.com/",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10517,
      "name": "Malico Inc",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/aaaa9d9c-36a1-2ba4-96a9-aaa90046a1f5.png",
      "levelId": 10308,
      "url": "https://www.malico.com/",
      "order": 2,
      "extraPadding": 8
    },
    {
      "id": 10518,
      "name": "Amazon",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/6d58ac3c-b344-f7e3-e129-504afcaf7fab.png",
      "levelId": 10308,
      "url": "https://www.aboutamazon.com/",
      "order": 3,
      "extraPadding": 8
    },
    {
      "id": 10519,
      "name": "Science Robotics",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/2f8e5e8c-9827-524b-6ce1-25938067c6c6.png",
      "levelId": 10309,
      "url": "https://www.science.org/journal/scirobotics",
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10520,
      "name": "Research",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/1a388677-6169-72e5-856b-a272d385179e.png",
      "levelId": 10309,
      "url": "https://spj.science.org/journal/research",
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10521,
      "name": "Toyota Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10109/logo/bb8fade1-de9b-d653-cafd-625d34547b4d.png",
      "levelId": 10307,
      "url": "https://www.tri.global/",
      "order": 3,
      "extraPadding": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10290,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    },
    {
      "id": 10305,
      "name": "Cooperative Sponsors",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10306,
      "name": "Gold",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10307,
      "name": "Silver",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10308,
      "name": "Bronze",
      "rank": 5,
      "isDefault": false
    },
    {
      "id": 10309,
      "name": "HRI Friends",
      "rank": 6,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10283,
      "name": "UMC First Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10109/dbfae339-b8c9-d72d-a89b-28c53adda682.png",
      "roomIds": []
    },
    {
      "id": 10284,
      "name": "UMC Second Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10109/0c4ba469-6988-b840-60c9-6ef256d0aee1.png",
      "roomIds": [
        11486,
        11487,
        11490,
        11480,
        11485,
        11489,
        11493,
        11494,
        11477
      ]
    },
    {
      "id": 10285,
      "name": "UMC Third Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10109/09fae212-e3f3-9404-b329-4a230ed58dd1.png",
      "roomIds": [
        11496,
        11503,
        11504,
        11508
      ]
    },
    {
      "id": 10286,
      "name": "UMC Fourth Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10109/bed582fe-d03a-d043-8749-ba296c4410c4.png",
      "roomIds": [
        11520,
        11505,
        11506,
        11507
      ]
    },
    {
      "id": 10287,
      "name": "Venue Map (Boulder)",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10109/e65c07f5-df19-d9d5-986b-c699679888e9.png",
      "roomIds": []
    }
  ],
  "rooms": [
    {
      "id": 11477,
      "name": "UMC Glenn Miller Ballroom (West, 208)",
      "setup": "THEATRE",
      "typeId": 13320,
      "capacity": 250,
      "note": "Monday: Workshops, Tues–Thursday: Poster/Break, Friday: TBD"
    },
    {
      "id": 11480,
      "name": "UMC Glenn Miller Ballroom (Center, 210)",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 575,
      "note": "Monday: Workshop, Tues–Thurs: Plenary w/East, Friday: TBD"
    },
    {
      "id": 11485,
      "name": "UMC Glenn Miller Ballroom (East, 212)",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 250,
      "note": "Monday: Workshop, Tues–Thurs: Plenary w/Middle, Friday: TBD"
    },
    {
      "id": 11486,
      "name": "Aspen 285",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 60,
      "note": "Monday: Workshop, Tues, Friday: TBD"
    },
    {
      "id": 11487,
      "name": "Aspen 287",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 20,
      "note": "Monday: Workshop, Tues, Friday: TBD"
    },
    {
      "id": 11489,
      "name": "Aspen 289",
      "setup": "THEATRE",
      "typeId": 13319,
      "capacity": 30,
      "note": "Monday: Workshop, Tues, Friday: TBD"
    },
    {
      "id": 11490,
      "name": "UMC 235",
      "setup": "SPECIAL",
      "typeId": 13323,
      "note": "Tues–Thurs: Exhibits/Break, Friday: TBD"
    },
    {
      "id": 11493,
      "name": "UMC 245",
      "setup": "SPECIAL",
      "typeId": 13322,
      "capacity": 39,
      "note": "Capacity: 18 fixed boardroom + 21 perimeter\nMonday: Workshop, Tues–Thurs: Ancillary Meetings, Friday: TBD"
    },
    {
      "id": 11494,
      "name": "UMC 247",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 70,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11496,
      "name": "UMC 382",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 40,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11503,
      "name": "UMC 384",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 40,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11504,
      "name": "UMC 386",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 40,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11505,
      "name": "UMC 415",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 35,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11506,
      "name": "UMC 417",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 45,
      "note": "Monday: Workshop, Tues–Friday: TBD"
    },
    {
      "id": 11507,
      "name": "UMC 425",
      "setup": "THEATRE",
      "typeId": 13322,
      "capacity": 40,
      "note": "Monday: Workshop, Friday: TBD"
    },
    {
      "id": 11508,
      "name": "UMC 304",
      "setup": "SPECIAL",
      "typeId": 13323,
      "capacity": 8,
      "note": "Quiet Room Monday–Thurs"
    },
    {
      "id": 11520,
      "name": "UMC Dennis Small Cultural Center",
      "setup": "THEATRE",
      "typeId": 13322,
      "note": ""
    }
  ],
  "tracks": [
    {
      "id": 12525,
      "name": "HRI 2024 Main Track",
      "typeId": 13319
    },
    {
      "id": 12622,
      "name": "HRI 2024 Student Design Challenge",
      "typeId": 13317
    },
    {
      "id": 12623,
      "name": "HRI 2024 Short Contributions",
      "typeId": 13320
    },
    {
      "id": 12624,
      "name": "HRI 2024 Workshops",
      "typeId": 13322
    },
    {
      "id": 12625,
      "name": "HRI 2024 alt.HRI",
      "typeId": 13319
    },
    {
      "id": 12626,
      "name": "HRI 2024 Late Breaking Reports",
      "typeId": 13321
    },
    {
      "id": 12627,
      "name": "HRI 2024 Robot Challenge",
      "typeId": 13317
    },
    {
      "id": 12628,
      "name": "HRI 2024 Videos",
      "typeId": 13320
    },
    {
      "id": 12629,
      "name": "HRI 2024 PIONEERS",
      "typeId": 13316
    },
    {
      "id": 12630,
      "name": "HRI 2024 Demos",
      "typeId": 13315
    },
    {
      "id": 12635
    }
  ],
  "contentTypes": [
    {
      "id": 13314,
      "name": "Course",
      "displayName": "Courses",
      "color": "#66c2a4",
      "duration": 90
    },
    {
      "id": 13315,
      "name": "Demo",
      "displayName": "Demos",
      "color": "#006d2c",
      "duration": 5
    },
    {
      "id": 13316,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 13317,
      "name": "Event",
      "displayName": "Events",
      "color": "#ffc034",
      "duration": 0
    },
    {
      "id": 13318,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 13319,
      "name": "Paper",
      "displayName": "Papers",
      "color": "#0d42cc",
      "duration": 20
    },
    {
      "id": 13320,
      "name": "Poster",
      "displayName": "Posters",
      "color": "#ff7a00",
      "duration": 5
    },
    {
      "id": 13321,
      "name": "Work-in-Progress",
      "displayName": "Works-In-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 13322,
      "name": "Workshop",
      "displayName": "Workshops",
      "color": "#f60000",
      "duration": 240
    },
    {
      "id": 13323,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    }
  ],
  "timeSlots": [
    {
      "id": 13715,
      "type": "SESSION",
      "startDate": 1710147600000,
      "endDate": 1710162000000
    },
    {
      "id": 13718,
      "type": "LUNCH",
      "startDate": 1710162000000,
      "endDate": 1710165600000
    },
    {
      "id": 13740,
      "type": "SESSION",
      "startDate": 1710165600000,
      "endDate": 1710180000000
    },
    {
      "id": 13743,
      "type": "SESSION",
      "startDate": 1710230400000,
      "endDate": 1710232200000
    },
    {
      "id": 13744,
      "type": "SESSION",
      "startDate": 1710232200000,
      "endDate": 1710234000000
    },
    {
      "id": 13745,
      "type": "SESSION",
      "startDate": 1710234000000,
      "endDate": 1710236700000
    },
    {
      "id": 13746,
      "type": "BREAK",
      "startDate": 1710236700000,
      "endDate": 1710240300000
    },
    {
      "id": 13747,
      "type": "SESSION",
      "startDate": 1710240300000,
      "endDate": 1710243900000
    },
    {
      "id": 13748,
      "type": "SESSION",
      "startDate": 1710243900000,
      "endDate": 1710247500000
    },
    {
      "id": 13749,
      "type": "LUNCH",
      "startDate": 1710247500000,
      "endDate": 1710252300000
    },
    {
      "id": 13764,
      "type": "SESSION",
      "startDate": 1710493200000,
      "endDate": 1710507600000
    },
    {
      "id": 13767,
      "type": "LUNCH",
      "startDate": 1710507600000,
      "endDate": 1710511200000
    },
    {
      "id": 13768,
      "type": "SESSION",
      "startDate": 1710511200000,
      "endDate": 1710525600000
    },
    {
      "id": 13771,
      "type": "SESSION",
      "startDate": 1710252300000,
      "endDate": 1710256800000
    },
    {
      "id": 13772,
      "type": "SESSION",
      "startDate": 1710256800000,
      "endDate": 1710259800000
    },
    {
      "id": 13773,
      "type": "BREAK",
      "startDate": 1710259800000,
      "endDate": 1710263400000
    },
    {
      "id": 13774,
      "type": "SESSION",
      "startDate": 1710263400000,
      "endDate": 1710268200000
    },
    {
      "id": 13775,
      "type": "SESSION",
      "startDate": 1710316800000,
      "endDate": 1710318600000
    },
    {
      "id": 13776,
      "type": "SESSION",
      "startDate": 1710318600000,
      "endDate": 1710320400000
    },
    {
      "id": 13777,
      "type": "SESSION",
      "startDate": 1710320400000,
      "endDate": 1710323100000
    },
    {
      "id": 13778,
      "type": "BREAK",
      "startDate": 1710323100000,
      "endDate": 1710326700000
    },
    {
      "id": 13779,
      "type": "SESSION",
      "startDate": 1710326700000,
      "endDate": 1710330300000
    },
    {
      "id": 13780,
      "type": "SESSION",
      "startDate": 1710330300000,
      "endDate": 1710333900000
    },
    {
      "id": 13781,
      "type": "LUNCH",
      "startDate": 1710333900000,
      "endDate": 1710338700000
    },
    {
      "id": 13782,
      "type": "SESSION",
      "startDate": 1710338700000,
      "endDate": 1710342600000
    },
    {
      "id": 13783,
      "type": "SESSION",
      "startDate": 1710342600000,
      "endDate": 1710345600000
    },
    {
      "id": 13784,
      "type": "BREAK",
      "startDate": 1710345600000,
      "endDate": 1710349200000
    },
    {
      "id": 13785,
      "type": "SESSION",
      "startDate": 1710349200000,
      "endDate": 1710354000000
    },
    {
      "id": 13786,
      "type": "BREAK",
      "startDate": 1710354000000,
      "endDate": 1710356400000
    },
    {
      "id": 13787,
      "type": "SESSION",
      "startDate": 1710356400000,
      "endDate": 1710363600000
    },
    {
      "id": 13788,
      "type": "SESSION",
      "startDate": 1710403200000,
      "endDate": 1710405000000
    },
    {
      "id": 13789,
      "type": "SESSION",
      "startDate": 1710405000000,
      "endDate": 1710406800000
    },
    {
      "id": 13790,
      "type": "SESSION",
      "startDate": 1710406800000,
      "endDate": 1710409500000
    },
    {
      "id": 13791,
      "type": "BREAK",
      "startDate": 1710409500000,
      "endDate": 1710411300000
    },
    {
      "id": 13792,
      "type": "SESSION",
      "startDate": 1710411300000,
      "endDate": 1710414900000
    },
    {
      "id": 13793,
      "type": "SESSION",
      "startDate": 1710414900000,
      "endDate": 1710418500000
    },
    {
      "id": 13794,
      "type": "LUNCH",
      "startDate": 1710418500000,
      "endDate": 1710423300000
    },
    {
      "id": 13795,
      "type": "SESSION",
      "startDate": 1710423300000,
      "endDate": 1710426900000
    },
    {
      "id": 13796,
      "type": "SESSION",
      "startDate": 1710426900000,
      "endDate": 1710430500000
    },
    {
      "id": 13797,
      "type": "BREAK",
      "startDate": 1710430500000,
      "endDate": 1710432300000
    },
    {
      "id": 13798,
      "type": "SESSION",
      "startDate": 1710432300000,
      "endDate": 1710437100000
    },
    {
      "id": 13799,
      "type": "SESSION",
      "startDate": 1710437100000,
      "endDate": 1710439200000
    }
  ],
  "sessions": [
    {
      "id": 151389,
      "name": "Language and sound",
      "isParallelPresentation": false,
      "importedId": "10452",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151433
      ],
      "contentIds": [
        140146,
        140120,
        140147,
        140097,
        140141
      ],
      "source": "SYS",
      "timeSlotId": 13783
    },
    {
      "id": 151390,
      "name": "Children",
      "isParallelPresentation": false,
      "importedId": "10453",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151426
      ],
      "contentIds": [
        140137,
        140094,
        140079,
        140080,
        140150,
        140157
      ],
      "source": "SYS",
      "timeSlotId": 13796
    },
    {
      "id": 151391,
      "name": "Families",
      "isParallelPresentation": false,
      "importedId": "10454",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        139948
      ],
      "contentIds": [
        140111,
        140109,
        140115,
        140156,
        140131
      ],
      "source": "SYS",
      "timeSlotId": 13772
    },
    {
      "id": 151392,
      "name": "Groups and Teams",
      "isParallelPresentation": false,
      "importedId": "10455",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151427
      ],
      "contentIds": [
        140103,
        140106,
        140100,
        140160,
        140127,
        140145
      ],
      "source": "SYS",
      "timeSlotId": 13748
    },
    {
      "id": 151393,
      "name": "Robots in the Wild",
      "isParallelPresentation": false,
      "importedId": "10456",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        140057
      ],
      "contentIds": [
        140095,
        140083,
        140128,
        140121,
        140099,
        140126
      ],
      "source": "SYS",
      "timeSlotId": 13747
    },
    {
      "id": 151394,
      "name": "Emotion",
      "isParallelPresentation": false,
      "importedId": "10457",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        139862
      ],
      "contentIds": [
        140129,
        140134,
        140135,
        140091,
        140132,
        140102
      ],
      "source": "SYS",
      "timeSlotId": 13793
    },
    {
      "id": 151395,
      "name": "XAI-AVs",
      "isParallelPresentation": false,
      "importedId": "10458",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151428
      ],
      "contentIds": [
        140154,
        140081,
        140164,
        140138,
        140136,
        140155
      ],
      "source": "SYS",
      "timeSlotId": 13782
    },
    {
      "id": 151396,
      "name": "Learning",
      "isParallelPresentation": false,
      "importedId": "10459",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151429
      ],
      "contentIds": [
        140161,
        140149,
        140086,
        140088,
        140089,
        140162,
        140090,
        140153
      ],
      "source": "SYS",
      "timeSlotId": 13774
    },
    {
      "id": 151397,
      "name": "Nonverbal Interaction",
      "isParallelPresentation": false,
      "importedId": "10460",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        139788
      ],
      "contentIds": [
        140117,
        140163,
        140165,
        140151,
        140118,
        140096,
        140113,
        140119
      ],
      "source": "SYS",
      "timeSlotId": 13785
    },
    {
      "id": 151398,
      "name": "Errors and Repair",
      "isParallelPresentation": false,
      "importedId": "10461",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        139868
      ],
      "contentIds": [
        140087,
        140124,
        140108,
        140158,
        140093,
        140101
      ],
      "source": "SYS",
      "timeSlotId": 13779
    },
    {
      "id": 151399,
      "name": "Proxemics and Navigation",
      "isParallelPresentation": false,
      "importedId": "10462",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151430
      ],
      "contentIds": [
        140133,
        140140,
        140148,
        140130,
        140125,
        140107
      ],
      "source": "SYS",
      "timeSlotId": 13795
    },
    {
      "id": 151400,
      "name": "Assistive Robotics",
      "isParallelPresentation": false,
      "importedId": "10463",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151431
      ],
      "contentIds": [
        140112,
        140123,
        140092,
        140082,
        140144,
        140116
      ],
      "source": "SYS",
      "timeSlotId": 13792
    },
    {
      "id": 151401,
      "name": "Autonomy and Helping Hands",
      "isParallelPresentation": false,
      "importedId": "10464",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        139931
      ],
      "contentIds": [
        140152,
        140085,
        140159,
        140098,
        140110,
        140143
      ],
      "source": "SYS",
      "timeSlotId": 13780
    },
    {
      "id": 151402,
      "name": "Ethics and Society",
      "isParallelPresentation": false,
      "importedId": "10465",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        151432
      ],
      "contentIds": [
        140122,
        140114,
        140142,
        140139,
        140104,
        140084,
        140105
      ],
      "source": "SYS",
      "timeSlotId": 13771
    },
    {
      "id": 152708,
      "name": "Human-Interactive Robot Learning",
      "isParallelPresentation": false,
      "importedId": "10485",
      "typeId": 13322,
      "roomId": 11496,
      "chairIds": [
        152365,
        152086,
        139753,
        152210,
        151879,
        151831,
        151652
      ],
      "contentIds": [
        152739,
        152416
      ],
      "source": "SYS",
      "timeSlotId": 13764
    },
    {
      "id": 152710,
      "name": "Inclusive HRI",
      "isParallelPresentation": false,
      "importedId": "10487",
      "typeId": 13322,
      "roomId": 11503,
      "chairIds": [
        151583,
        152223,
        139782,
        152314,
        151599,
        151619
      ],
      "contentIds": [
        152429
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152712,
      "name": "HRI for Aging in Place",
      "isParallelPresentation": false,
      "importedId": "10489",
      "typeId": 13322,
      "roomId": 11506,
      "chairIds": [
        152325,
        139782,
        151978,
        151837,
        139968
      ],
      "contentIds": [
        152433
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152714,
      "name": "Virtual, Augmented, and Mixed Reality for Human-Robot Interaction",
      "isParallelPresentation": false,
      "importedId": "10491",
      "typeId": 13322,
      "roomId": 11506,
      "chairIds": [],
      "contentIds": [
        152399
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152719,
      "name": "Short Contributions",
      "isParallelPresentation": true,
      "importedId": "10496",
      "typeId": 13320,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152649,
        152529,
        152583,
        152569,
        152587,
        152564,
        152588,
        152568,
        152594,
        152582,
        152669,
        152603,
        152589,
        152621,
        152565,
        152666,
        152586,
        152562,
        152827,
        152624
      ],
      "source": "SYS",
      "timeSlotId": 13778
    },
    {
      "id": 152720,
      "name": "alt.HRI",
      "isParallelPresentation": false,
      "importedId": "10497",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        152076,
        139939
      ],
      "contentIds": [
        152439,
        152419,
        152705,
        152560,
        152420,
        152458,
        152706,
        152488
      ],
      "source": "SYS",
      "timeSlotId": 13798
    },
    {
      "id": 152722,
      "name": "Late Breaking Reports - Session 1",
      "isParallelPresentation": true,
      "importedId": "10499",
      "typeId": 13321,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152539,
        152563,
        152654,
        152647,
        152644,
        152684,
        152685,
        152699,
        152452,
        152462,
        152473,
        152468,
        152490,
        152525,
        152516,
        152552,
        152554,
        152536,
        152530,
        152532,
        152567,
        152561,
        152681,
        152415,
        152412,
        152430,
        152424,
        152475,
        152476,
        152559,
        152544,
        152535,
        152593,
        152388,
        152402,
        152394
      ],
      "source": "SYS",
      "timeSlotId": 13746
    },
    {
      "id": 152723,
      "name": "Late Breaking Reports - Session 2",
      "isParallelPresentation": true,
      "importedId": "10500",
      "typeId": 13321,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152504,
        152512,
        152509,
        152531,
        152574,
        152601,
        152609,
        152591,
        152597,
        152584,
        152626,
        152616,
        152657,
        152645,
        152686,
        152688,
        152694,
        152701,
        152450,
        152445,
        152446,
        152474,
        152477,
        152487,
        152494,
        152497,
        152522,
        152510,
        152514,
        152507,
        152508,
        152548,
        152538,
        152541,
        152572,
        152602,
        152604,
        152592,
        152637,
        152623,
        152625,
        152629,
        152658,
        152648,
        152680,
        152673,
        152678,
        152434,
        152447,
        152448,
        152459,
        152478,
        152472,
        152495,
        152496,
        152489,
        152511,
        152515,
        152549,
        152542,
        152571,
        152570,
        152614,
        152600,
        152605,
        152606,
        152595,
        152385,
        152393,
        152395
      ],
      "source": "SYS",
      "timeSlotId": 13773
    },
    {
      "id": 152724,
      "name": "Late Breaking Work - Session 3",
      "isParallelPresentation": true,
      "importedId": "10501",
      "typeId": 13321,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152518,
        152547,
        152551,
        152534,
        152613,
        152622,
        152619,
        152650,
        152652,
        152679,
        152683,
        152672,
        152667,
        152670,
        152690,
        152692,
        152453,
        152693,
        152698,
        152700,
        152697,
        152443,
        152449,
        152481,
        152471,
        152463,
        152465,
        152466,
        152500,
        152493,
        152517,
        152545,
        152540,
        152575,
        152576,
        152566,
        152612,
        152607,
        152608,
        152596,
        152598,
        152631,
        152620,
        152656,
        152646,
        152651,
        152643,
        152682,
        152668,
        152423,
        152428,
        152432,
        152425,
        152426,
        152440,
        152451,
        152442,
        152444,
        152480,
        152470,
        152464,
        152467,
        152469,
        152499,
        152491,
        152519,
        152523,
        152506,
        152579,
        152627,
        152384,
        152411,
        152537
      ],
      "source": "SYS",
      "timeSlotId": 13778
    },
    {
      "id": 152726,
      "name": "Late Breaking Work - Session 4",
      "isParallelPresentation": false,
      "importedId": "10503",
      "typeId": 13321,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152642,
        152696,
        152454,
        152460,
        152633,
        152638,
        152653,
        152661,
        152674,
        152675,
        152413,
        152422,
        152441,
        152456,
        152455,
        152498,
        152492,
        152526,
        152513,
        152550,
        152580,
        152611,
        152610,
        152590,
        152632,
        152634,
        152636,
        152639,
        152628,
        152618,
        152392,
        152401,
        152403,
        152397,
        152410,
        152617
      ],
      "source": "SYS",
      "timeSlotId": 13784
    },
    {
      "id": 152727,
      "name": "Demos - Session 1",
      "isParallelPresentation": true,
      "importedId": "10504",
      "typeId": 13315,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152671,
        152663,
        152660,
        152665
      ],
      "source": "SYS",
      "timeSlotId": 13773
    },
    {
      "id": 152728,
      "name": "Demos - Session 2",
      "isParallelPresentation": true,
      "importedId": "10505",
      "typeId": 13315,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152664,
        152662,
        152703,
        152704
      ],
      "source": "SYS",
      "timeSlotId": 13784
    },
    {
      "id": 152729,
      "name": "Student Design Competition",
      "isParallelPresentation": false,
      "importedId": "10506",
      "typeId": 13317,
      "roomId": 11477,
      "chairIds": [],
      "contentIds": [
        152640,
        152641,
        152457,
        152461,
        152486,
        152484,
        152485,
        152479,
        152503,
        152501,
        152502,
        152505,
        152528,
        152527,
        152520
      ],
      "source": "SYS",
      "timeSlotId": 13773
    },
    {
      "id": 152731,
      "name": "Videos",
      "isParallelPresentation": false,
      "importedId": "10508",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [
        152578,
        152581,
        152553,
        152546,
        152558,
        152556,
        152615,
        152630,
        152635,
        152543
      ],
      "source": "SYS",
      "timeSlotId": 13776
    },
    {
      "id": 152735,
      "name": "THRI Flash Talks",
      "isParallelPresentation": false,
      "importedId": "10512",
      "typeId": 13319,
      "roomId": 11480,
      "chairIds": [
        140030,
        139916,
        151794
      ],
      "contentIds": [
        152848,
        152847,
        152850,
        152849,
        152851
      ],
      "source": "SYS",
      "timeSlotId": 13789
    },
    {
      "id": 152736,
      "name": "Movement Notation",
      "isParallelPresentation": false,
      "importedId": "10513",
      "typeId": 13322,
      "roomId": 11496,
      "chairIds": [
        152107
      ],
      "contentIds": [
        152398
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152740,
      "name": "Assistive Applications, Accessibility, and Disability Ethics (A3DE)",
      "isParallelPresentation": false,
      "importedId": "10517",
      "typeId": 13322,
      "roomId": 11506,
      "chairIds": [],
      "contentIds": [
        152390
      ],
      "source": "SYS",
      "timeSlotId": 13764
    },
    {
      "id": 152742,
      "name": "Assistive Applications, Accessibility, and Disability Ethics (A3DE)",
      "isParallelPresentation": false,
      "importedId": "10519",
      "typeId": 13322,
      "roomId": 11506,
      "chairIds": [],
      "contentIds": [
        152390
      ],
      "source": "SYS",
      "timeSlotId": 13768
    },
    {
      "id": 152744,
      "name": "Causal-HRI: Causal Learning for Human-Robot Interaction",
      "isParallelPresentation": false,
      "importedId": "10521",
      "typeId": 13322,
      "roomId": 11496,
      "chairIds": [],
      "contentIds": [
        152387
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152746,
      "name": "Designing an HRI Course for Undergraduate Education",
      "isParallelPresentation": false,
      "importedId": "10523",
      "typeId": 13322,
      "roomId": 11507,
      "chairIds": [],
      "contentIds": [
        152414
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152748,
      "name": "End-User Development for Human-Robot Interaction",
      "isParallelPresentation": false,
      "importedId": "10525",
      "typeId": 13322,
      "roomId": 11503,
      "chairIds": [],
      "contentIds": [
        152404
      ],
      "source": "SYS",
      "timeSlotId": 13764
    },
    {
      "id": 152750,
      "name": "Ethnography in HRI: Embodied, Embedded, Messy and Everyday",
      "isParallelPresentation": false,
      "importedId": "10527",
      "typeId": 13322,
      "roomId": 11505,
      "chairIds": [],
      "contentIds": [
        152431
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152752,
      "name": "Explainability for Human-Robot Collaboration",
      "isParallelPresentation": false,
      "importedId": "10529",
      "typeId": 13322,
      "roomId": 11507,
      "chairIds": [],
      "contentIds": [
        152435
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152754,
      "name": "Worker-Robot Relationships",
      "isParallelPresentation": false,
      "importedId": "10531",
      "typeId": 13322,
      "roomId": 11504,
      "chairIds": [],
      "contentIds": [
        152391
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152756,
      "name": "Human-Large Language Model Interaction",
      "isParallelPresentation": false,
      "importedId": "10533",
      "typeId": 13322,
      "roomId": 11504,
      "chairIds": [],
      "contentIds": [
        152400
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152758,
      "name": "Jibo Community Social Robot Research Platform @Scale",
      "isParallelPresentation": false,
      "importedId": "10535",
      "typeId": 13322,
      "roomId": 11494,
      "chairIds": [],
      "contentIds": [
        152406
      ],
      "source": "SYS",
      "timeSlotId": 13764
    },
    {
      "id": 152760,
      "name": "Jibo Community Social Robot Research Platform @Scale",
      "isParallelPresentation": false,
      "importedId": "10537",
      "typeId": 13322,
      "roomId": 11494,
      "chairIds": [],
      "contentIds": [
        152406
      ],
      "source": "SYS",
      "timeSlotId": 13768
    },
    {
      "id": 152762,
      "name": "Lifelong Learning and Personalization in Long-Term Human-Robot Interaction",
      "isParallelPresentation": false,
      "importedId": "10539",
      "typeId": 13322,
      "roomId": 11486,
      "chairIds": [],
      "contentIds": [
        152483
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152764,
      "name": "Privacy Aware Robotics",
      "isParallelPresentation": false,
      "importedId": "10541",
      "typeId": 13322,
      "roomId": 11505,
      "chairIds": [],
      "contentIds": [
        152418
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152766,
      "name": "Rebellion and Disobedience in Human-Robot Interaction (RaD-HRI)",
      "isParallelPresentation": false,
      "importedId": "10543",
      "typeId": 13322,
      "roomId": 11496,
      "chairIds": [],
      "contentIds": [
        152427
      ],
      "source": "SYS",
      "timeSlotId": 13768
    },
    {
      "id": 152768,
      "name": "Robo-Identity: Designing for Identity in the Shared World",
      "isParallelPresentation": false,
      "importedId": "10545",
      "typeId": 13322,
      "roomId": 11503,
      "chairIds": [],
      "contentIds": [
        152436
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152770,
      "name": "Scarecrows in Oz: Large Language Models in HRI",
      "isParallelPresentation": false,
      "importedId": "10547",
      "typeId": 13322,
      "roomId": 11489,
      "chairIds": [],
      "contentIds": [
        152389
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152772,
      "name": "Social Signal Modeling in Human-Robot Interaction",
      "isParallelPresentation": false,
      "importedId": "10549",
      "typeId": 13322,
      "roomId": 11520,
      "chairIds": [],
      "contentIds": [
        152386
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152774,
      "name": "Symbiotic Society with Avatars (SSA)",
      "isParallelPresentation": false,
      "importedId": "10551",
      "typeId": 13322,
      "roomId": 11503,
      "chairIds": [],
      "contentIds": [
        152482
      ],
      "source": "SYS",
      "timeSlotId": 13768
    },
    {
      "id": 152776,
      "name": "Taking a Closer Look: Refining Trust and its Impact in HRI",
      "isParallelPresentation": false,
      "importedId": "10553",
      "typeId": 13322,
      "roomId": 11489,
      "chairIds": [],
      "contentIds": [
        152405
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152778,
      "name": "Workshop YOUR Study Design 2024!",
      "isParallelPresentation": false,
      "importedId": "10555",
      "typeId": 13322,
      "roomId": 11520,
      "chairIds": [],
      "contentIds": [
        152396
      ],
      "source": "SYS",
      "timeSlotId": 13715
    },
    {
      "id": 152787,
      "name": "HRI Pioneers",
      "isParallelPresentation": true,
      "importedId": "10564",
      "typeId": 13316,
      "roomId": 11494,
      "chairIds": [],
      "contentIds": [
        152407,
        152409,
        152408,
        152417,
        152421,
        152437,
        152438,
        152655,
        152659,
        152676,
        152677,
        152689,
        152691,
        152687,
        152695,
        152702,
        152533,
        152555,
        152557,
        152577,
        152573,
        152585,
        152599,
        152521,
        152524
      ],
      "source": "SYS",
      "timeSlotId": 13740
    },
    {
      "id": 152788,
      "name": "HRI Pioneers",
      "isParallelPresentation": true,
      "importedId": "10565",
      "typeId": 13316,
      "roomId": 11494,
      "chairIds": [],
      "contentIds": [
        152407,
        152409,
        152408,
        152417,
        152421,
        152437,
        152438,
        152655,
        152659,
        152676,
        152677,
        152689,
        152691,
        152687,
        152695,
        152702,
        152533,
        152555,
        152557,
        152577,
        152573,
        152585,
        152599,
        152521,
        152524
      ],
      "source": "SYS",
      "timeSlotId": 13715
    }
  ],
  "events": [
    {
      "id": 151434,
      "name": "James Kennedy Keynote",
      "isParallelPresentation": false,
      "importedId": "10481",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710234000000,
      "endDate": 1710236700000,
      "description": "Creating Expressive and Engaging Robotic Characters",
      "presenterIds": [
        151796
      ],
      "source": "SYS"
    },
    {
      "id": 152715,
      "name": "Carolina Parada Keynote",
      "isParallelPresentation": false,
      "importedId": "10492",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710320400000,
      "endDate": 1710323100000,
      "description": "Creating Expressive and Engaging Robotic Characters",
      "presenterIds": [
        139937
      ],
      "source": "SYS"
    },
    {
      "id": 152716,
      "name": "Ryan Calo Keynote",
      "isParallelPresentation": false,
      "importedId": "10493",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710406800000,
      "endDate": 1710409500000,
      "description": "Creating Expressive and Engaging Robotic Characters",
      "presenterIds": [
        152717
      ],
      "source": "SYS"
    },
    {
      "id": 152732,
      "name": "Closing & Awards",
      "isParallelPresentation": false,
      "importedId": "10509",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710437100000,
      "endDate": 1710439200000,
      "presenterIds": [
        139832,
        151758,
        152733
      ],
      "source": "SYS"
    },
    {
      "id": 152783,
      "name": "NSF workshop",
      "isParallelPresentation": false,
      "importedId": "10560",
      "typeId": 13317,
      "roomId": 11486,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710165600000,
      "endDate": 1710180000000,
      "link": {
        "href": "https://humanrobotinteraction.org/2024/wp-content/uploads/2024/03/HRI-2024-NSF-Workshop-Description-and-Agenda.pdf",
        "label": "Description and Agenda"
      },
      "description": "The NSF Workshop will be focused on Human-Robot Interaction and how it fits into different National Science Foundation programs. There will be presentations regarding the HCC, FRR and M3X programs.",
      "presenterIds": [
        139769,
        152785
      ],
      "source": "SYS"
    },
    {
      "id": 152786,
      "name": "Conference Banquet",
      "isParallelPresentation": false,
      "importedId": "10563",
      "typeId": 13317,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710356400000,
      "endDate": 1710363600000,
      "location": "Stadium Club",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 152826,
      "name": "HRI 2024 Chairs' Welcome",
      "isParallelPresentation": false,
      "importedId": "10588",
      "typeId": 13317,
      "roomId": 11480,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1710232200000,
      "endDate": 1710234000000,
      "presenterIds": [
        151758,
        152733,
        139832
      ],
      "source": "SYS"
    },
    {
      "id": 153775,
      "name": "Event",
      "isParallelPresentation": false,
      "importedId": "11514",
      "typeId": 13317,
      "chairIds": [],
      "contentIds": [],
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 140079,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Role-Playing with Robot Characters: Increasing User Engagement through Narrative and Gameplay Agency",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1010",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Live entertainment is moving towards a greater participatory culture, with dynamic narratives told through audience interaction. Robot characters offer a unique opportunity to mitigate the challenges of creating personalized entertainment at scale. However, robots often cannot react to audience responses, limiting opportunities for audience participation. In this work, we  explore methods to increase user participation in live entertainment experiences with robot characters to improve user engagement and enjoyment. In a between-subjects study (N=60), we create an immersive story where users role-play as detectives with two distinct robot characters. Users either (1) have greater involvement and self-identification in the story by talking with the robots in-character (narrative condition), (2) have a more active role in solving puzzles (gameplay condition), or (3) follow along without being prompted by the robots for input (control condition). Our results show that increasing user agency in a role-playing experience, in either its narrative or its gameplay, improves users' flow state, sense of autonomy and competence, verbal engagement, and perceptions of the robot characters' engagement. Increasing narrative agency also led to longer unprompted reactions from participants, while gameplay agency improved feelings of immersion and relatedness with the robots. These findings suggest that creating either narrative or gameplay agency can improve user engagement, which can extend to broader robot interactions where gameplay elements and role-playing in stories can be incorporated.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "San Mateo",
              "institution": "Roblox",
              "dsl": ""
            }
          ],
          "personId": 139895
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139836
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago ",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140001
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139948
        }
      ]
    },
    {
      "id": 140080,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Children’s Word Learning from Socially Contingent Robots Under Active vs. Passive Learning Conditions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1372",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Language is learned through social interactions, in which gaze has a special role because it can be used to guide the attention and reference objects easily. Children, starting from very early ages, are also very good at utilizing gaze to map labels to referenced objects. To achieve language teaching robots, we need to understand how these functions of gaze can be implemented most efficiently. To this aim, we allowed children to interact with a social robot to learn the labels of several objects in a naturalistic setting. In some trials the child guided the gaze and chose the object to be learned while the robot was following and in the others they changed the roles and robot guided the gaze and decided on the object to be learned. We measured how much children actually followed the robot’s gaze and how many words they learned in these two conditions, referred to as active and passive learning conditions, respectively. The results indicate that although children followed the robot's gaze and learned words successfully, there were no meaningful differences in word learning between the two conditions. The rate of gaze following and time spent looking at the robot did not influence word learning, either. The implications of these results for use of robots in educational settings are further discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Göttingen",
              "institution": "University of Göttingen",
              "dsl": "Psychology of Language"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Göttingen",
              "institution": "Leibniz Science Campus",
              "dsl": ""
            }
          ],
          "personId": 139897
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Goettingen",
              "institution": "University of Goettingen",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Göttingen",
              "institution": "Leibniz Science Campus",
              "dsl": ""
            }
          ],
          "personId": 139869
        }
      ]
    },
    {
      "id": 140081,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Improving Explainable Object-induced Model through Uncertainty for Automated Vehicles",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1493",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": "The rapid evolution of automated vehicles (AVs) has the potential to provide safer, more efficient, and comfortable travel options. However, these systems face challenges regarding reliability in complex driving scenarios. Recent explainable AV architectures, whether designed end-to-end or through pipelines, neglect crucial information related to inherent uncertainties while providing explanations for actions. To deal with such challenges, our study builds upon the \"object-induced\" model approach that prioritizes the role of objects in scenes for decision-making and integrates uncertainty assessment into the decision-making process using an evidential deep learning paradigm with a Beta prior. Additionally, we explore several advanced training strategies guided by uncertainty, including uncertainty-guided data reweighting and augmentation. Leveraging the BDD-OIA dataset, our findings underscore that the model, through these enhancements, not only offers a clearer comprehension of AV decisions and their underlying reasoning but also surpasses existing baselines across a broad range of scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "Department of Informatics and Networked Systems, School of Computing and Information"
            }
          ],
          "personId": 139734
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": ""
            }
          ],
          "personId": 140022
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 139747
        }
      ]
    },
    {
      "id": 140082,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Driving a Ballbot Wheelchair with Hands-Free Torso Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1053",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "A novel wheelchair called PURE (Personalized Unique Rolling Experience) that uses hands-free (HF) lean-to-steer control has been developed for manual wheelchair users (mWCUs). PURE addresses limitations of current wheelchairs, such as inability to use both hands for life experiences instead of propulsion, and bulky and heavy powered wheelchairs. PURE uses a ball-based robot (ballbot) drivetrain to offer a compact, self-balancing device with the added benefit of omnidirectional movement ability. A custom sensor system converts rider torso motions into direction and speed commands to control PURE, which is especially useful if a rider has minimal torso range of motion. We explored whether PURE’s HF control performed as well as a traditional joystick (JS) human-robot interface and mWCUs, who may have reduced torso motion, performed as well as able-bodied users (ABUs). Twenty test participants (10 mWCUs, 10 ABUs) were trained and tested to drive PURE through courses replicating indoor environments. Each participant adjusted personal sensitivity settings for both HF and JS control. Repeated-measures MANOVA tests suggested that the effectiveness (number of collisions), efficiency (completion time), comfort (NASA TLX scores except physical demand), and robustness (index of performance) were similar for HF and JS control and between mWCUs and ABUs for all sections. These results suggest that PURE provides an effective method for controlling this new omnidirectional wheelchair by only using motions of the torso (including small movements) thus leaving both hands to be used for other tasks while the rider is moving about.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering "
            }
          ],
          "personId": 139906
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Human Dynamics and Controls Lab"
            }
          ],
          "personId": 139982
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering"
            }
          ],
          "personId": 139844
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign ",
              "dsl": "Mechanical Science and Engineering "
            }
          ],
          "personId": 139852
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Department of Mechanical Science and Engineering"
            }
          ],
          "personId": 139960
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": "MechSE/AUVSL/HDCL"
            }
          ],
          "personId": 139822
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Ilinois Urbana-Champaign",
              "dsl": "DRES/AHS"
            }
          ],
          "personId": 139992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois Urbana Champaign",
              "dsl": "School of Art + Design"
            }
          ],
          "personId": 139800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": "Industrial & Enterprise Systems Engineering"
            }
          ],
          "personId": 139911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois",
              "dsl": ""
            }
          ],
          "personId": 139818
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois",
              "dsl": "Disability Resources and Educational Services"
            }
          ],
          "personId": 139880
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "URBANA",
              "institution": "University of Illinois at Urbana-Champaign",
              "dsl": "Mechanical Science and Engineering"
            }
          ],
          "personId": 139774
        }
      ]
    },
    {
      "id": 140083,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Iterative Robot Waiter Algorithm Design: Service Expectations and Social Factors",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1294",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "Mobile robots carrying food in restaurants are here. What service behavior norms do people expect them to follow? This paper evaluates robot waiter algorithms and service parameters for scenarios with two participants at a simulated cocktail event. Varying body-storming inspired context variables such as: “hunger level\" and “relationship to each other,\" robot delivery algorithms (lead, follow, ambient), and participant pose (standing, seated). Due to increasing deployment of robotic systems, companies may need to rapidly iterate on situated, human-cognizant robotic behaviors that take functional and social considerations into account. We utilized a within-subjects design and improvisational methods, in which pairs of people were given a series of context prompts, and told to participate as felt natural. Output variables included whether they took food and post-trial survey ratings of the robot. The results show a positive correlation between food taking (or feelings of obligation to take food) and human or robot initiative, and negative correlation in the mixed-ambient algorithm with no explicit leader. The robot waiter that comes to the table is the clearest and most noticeable. Bringing food one person ordered to the other person was unforgivable. When in doubt, head to the center-point.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 139902
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robots and Intelligent Systems Institute"
            }
          ],
          "personId": 140076
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 139949
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robots and Intelligent Systems Institute"
            }
          ],
          "personId": 139887
        }
      ]
    },
    {
      "id": 140084,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Power in Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1293",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "Power is a fundamental determinant in social interactions, yet it remains elusive in the field of Human-Robot Interaction (HRI). This paper unveils the pervasive yet largely unexplored role of power in HRI by systematically investigating its varied manifestations across the HRI literature. We first introduce the definitions of power and then delve into the existing HRI literature through the lens of power, examining both the studies that directly address power and those that address power-related social configurations and concepts such as authority, dominance, and status. Leveraging Fiske’s model and French and Raven's bases of power framework, we also explore the nuances of power embedded within many HRI studies where power is not explicitly addressed. Finally, we propose power as the core concept to advance HRI---transforming fragmented existing findings into a unified theory and delineating a cohesive theoretical trajectory for future investigations. Along this line, we propose power factors and mechanisms exclusive to HRI and the need to redefine power for HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 140019
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University",
              "dsl": "School of Information Studies"
            }
          ],
          "personId": 139792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139862
        }
      ]
    },
    {
      "id": 140085,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A System for Human-Robot Teaming through End-User Programming and Shared Autonomy",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1414",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "Many industrial tasks-such as sanding, installing fasteners, and wire harnessing-are difficult to automate due to task complexity and variability. We instead investigate deploying robots in an assistive role for these tasks, where the robot assumes the physical task burden and the skilled worker provides both the high-level task planning and low-level feedback necessary to effectively complete the task. In this article, we describe the development of a system for flexible human-robot teaming that combines state-of-the-art methods in end-user programming and shared autonomy and its implementation in sanding applications. We demonstrate the use of the system in two types of sanding tasks, situated in aircraft manufacturing, that highlight two potential workflows within the human-robot teaming setup. We conclude by discussing challenges and opportunities in human-robot teaming identified during the development, application, and demonstration of our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 140011
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139753
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 139846
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 140002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": ""
            }
          ],
          "personId": 139811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 140086,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "PREDILECT: Preferences Delineated with Zero-Shot Language-based Reasoning in Reinforcement Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1458",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "Preference-based reinforcement learning (RL) has emerged as a new field in robot learning, where humans play a pivotal role in shaping robot behavior by expressing preferences on different sequences of state-action pairs. However, formulating realistic policies for robots demands responses from humans to an extensive array of queries. In this work, we approach the sample-efficiency challenge by expanding the information collected per query to contain both preferences and optional text prompting. To accomplish this, we leverage the zero-shot capabilities of a large language model (LLM) to reason from the text provided by humans. To accommodate the additional query information, we reformulate the reward learning objectives to contain flexible highlights -- state-action pairs that contain relatively high information and are related to the features processed in a zero-shot fashion from a pretrained LLM. In both a simulated scenario and a user study, we reveal the effectiveness of our work by analyzing the feedback and its implications. Additionally, the collective feedback collected serves to train a robot on socially compliant trajectories in a simulated social navigation landscape. We provide video examples of the trained policies at https://sites.google.com/view/rl-predilect",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139899
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139803
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Robotics Perception and Learning"
            }
          ],
          "personId": 139916
        }
      ]
    },
    {
      "id": 140087,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "What a Thing to Say! Which Linguistic Politeness Strategies Should Robots Use in Noncompliance Interactions?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1214",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "For social robots to succeed human environments, they must comprehend and follow human norms. In particular, robots must respond in effective, yet appropriate ways when humans violate these norms, e.g., when humans give robots unethical commands. Previous work has shown that humans expect robots to be proportional in their norm-violation responses; but there are a wide range of approaches robots could use to tune the politeness of their utterances to achieve proportionality, and it is not obvious whether all such strategies are appropriate for robots to use. In this work, we present the results of a human-subjects study assessing the use of human-like Face Theoretic proportionality. Our results show that while people expect robots to modulate the politeness of their responses, they do not expect them to strictly mimic human linguistic behaviors. Specifically, linguistic politeness strategies that use direct, formal language are perceived as more effective and more appropriate than strategies that use indirect, informal language.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139945
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 140088,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Online Behavior Modification for Expressive User Control of RL-Trained Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1335",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "Reinforcement Learning (RL) is an effective method for robots to learn tasks. However, in typical RL, end-users have little to no control over how the robot does the task after the robot has been deployed. To address this, we introduce the idea of online behavior modification, a paradigm in which users have control over behavior features of a robot in real-time as it autonomously completes a task using an RL-trained policy. To show the value of this user-centered formulation for human-robot interaction, we present a behavior-diversity–based algorithm, Adjustable Control Of RL Dynamics (ACORD), and demonstrate its applicability to online behavior modification in simulation and a user study. In the study (n=23), users adjust the style of paintings as a robot traces a shape autonomously. We compare ACORD to RL and Shared Autonomy (SA), and show ACORD affords user-preferred levels of control and expression, comparable to SA, but with the potential for autonomous execution and robustness of RL. The code for this paper is available at anon.url",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139950
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139829
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139977
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139974
        }
      ]
    },
    {
      "id": 140089,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Aligning Human and Robot Representations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1412",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "To act in the world, robots rely on a representation of salient task aspects: for example, to carry a coffee mug, a robot may consider movement efficiency or mug orientation in its behaviour. However, if we want robots to act for and with people, their representations must not be just functional but also reflective of what humans care about, i.e. they must be aligned. We observe that current learning approaches suffer from representation misalignment, where the robot's learned representation does not capture the human's representation. We suggest that because humans are the ultimate evaluator of robot performance, we must explicitly focus our efforts on aligning learned representations with humans, in addition to learning the downstream task. We advocate that current representation learning approaches in robotics should be studied from the perspective of how well they accomplish the objective of representation alignment. We mathematically define the problem, identify its key desiderata, and situate current methods within this formalism. We conclude by suggesting future directions for exploring open challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 139764
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "EECS"
            }
          ],
          "personId": 139763
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "EECS"
            }
          ],
          "personId": 140033
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 139914
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 139827
        }
      ]
    },
    {
      "id": 140090,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Modeling Variation in Human Feedback with User Inputs: An Exploratory Methodology",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1378",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "To expedite the development process of interactive reinforcement learning (IntRL) algorithms, prior work often uses perfect oracles as simulated human teachers to furnish feedback signals. Those oracles typically derive from ground-truth knowledge or optimal policies, and provide dense and error-free feedback to a robot learner without delay. However, this machine-like feedback behavior fails to accurately represent the diverse patterns observed in human feedback, which may lead to unstable or unexpected algorithm performance in real-world human-robot interaction. To alleviate this limitation of oracles in oversimplifying user behavior, we propose a method for modeling variation in human feedback that can be applied to a standard oracle. We present a 5-dimensional model with 5 dimensions of feedback variation identified in prior work. This model enables the modification of feedback output from perfect oracles to introduce more human-like features. We demonstrate how each model attribute can impact on the learning performance of an IntRL algorithm through a simulation experiment. We also conduct a proof-of-concept study to illustrate how our model can be populated from people in two ways. The modeling results intuitively present the feedback variation among participants and help to explain the mismatch between oracles and human teachers. Overall, our method is a promising step towards refining simulated oracles by incorporating insights from real users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 140036
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 139977
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139974
        }
      ]
    },
    {
      "id": 140091,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Affective and Cognitive Reactions to Robot-Initiated Social Control of Health Behaviors ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1059",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "Health-related social control refers to intentional attempts to influence people's health behaviors, often seen in personal relationships. Social robots hold promise in influencing people’s health by exerting health-related social control, but it is unclear which social control strategies used by robots would be appropriate and potentially effective. This paper explores the effects of positive versus negative, and relationship-oriented versus target-oriented social control strategies from a robot on people’s affective and cognitive reactions. In an online video prototype study, participants viewed scenarios of a robot companion trying to influence them to change their sedentary behaviors by using varied social control strategies. Our results showed that positive (vs. negative) strategies by the robot elicited stronger positive affect, enjoyment, perceived social appropriateness, reduced perceived threats to freedom, and strengthened behavioral intention. Meanwhile, relationship-oriented (vs. target-oriented) strategies elevated people's negative affect, reduced enjoyment and perceived social appropriateness, elevated perceived threats to freedom, and diminished behavioral intention. Given these findings, we give recommendations for designing influence strategies for social robots in the context of health behavior change.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 139768
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": ""
            }
          ],
          "personId": 140034
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human Technology Interaction group"
            }
          ],
          "personId": 139831
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Technical university of Eindhoven",
              "dsl": "Human Technology Interaction"
            }
          ],
          "personId": 140044
        }
      ]
    },
    {
      "id": 140092,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "RABBIT: A Robot-Assisted Bed Bathing System with Multimodal Perception and Integrated Compliance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1532",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "There is a large, growing demand for assistance with activities of daily living, such as bathing. Bathing is a central component of hygiene, but traditional bathing methods may not be available to people with mobility limitations. This paper introduces RABBIT, a robot-assisted bed bathing system. Our key contributions include: 1) the integration of multimodal perception techniques to differentiate between dry, wet, and soapy skin; 2) the implementation of human caregiving-inspired motion primitives; and 3) the incorporation of both active and passive compliance ensuring safe and comfortable physical human-robot interaction (pHRI). A user study involving 12 participants, including an individual with multiple sclerosis, which underscores the perceived effectiveness and comfort of our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139757
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139825
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139837
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139861
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139931
        }
      ]
    },
    {
      "id": 140093,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Can’t You See I Am Bothered? Human-inspired Suggestive Avoidance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1256",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "We studied how robots could stop people from repeatedly obstructing them by using reactions that people commonly use. From 35 hours of observation of people in a shopping mall, we identified one commonly used reaction, which we call suggestive avoidance. During suggestive avoidance, people avoid a person obstructing them by making a quick movement to the side while rotating their body and gazing toward the target person in a way that seems to imply that they were bothered by the obstruction. We modeled suggestive avoidance behavior and implemented it in a robot and tested it both in a lab experiment and in a field study. The result from the lab study, where we asked participants to bother a robot, confirmed that people perceive a robot using suggestive avoidance as being more bothered, as well as more human-like. The result of the field study confirmed that suggestive avoidance makes people less likely to bother the robot again.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": " Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kansai Science City",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 139873
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kansai Science City",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": "Intelligent Robotics and Communication Laboratory"
            }
          ],
          "personId": 139819
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kansai Science City",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 139909
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kansai Science City",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140094,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Doodlebot: An educational robot for creativity and AI literacy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1377",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Today, Artificial Intelligence (AI) is prevalent in everyday life, with emerging technologies like AI companions, autonomous vehicles, and AI art tools poised to significantly transform the future. The development of AI curricula that shows people how AI works and what they can do with it is a powerful way to prepare everyone, and especially young learners, for an increasingly AI-driven world. Educators often employ robotic toolkits in the classroom to boost engagement and learning. However, these platforms are generally unsuitable for young learners and learners without programming expertise. Moreover, these platforms often serve as either programmable artifacts or pedagogical agents, rarely capitalizing on the opportunity to support students in both capacities. We designed Doodlebot, a mobile social robot for hands-on AI education to address these gaps. Doodlebot is an effective tool for exploring AI with grade school (K-12) students, promoting their understanding of AI concepts such as perception, representation, reasoning and generation. We begin by elaborating Doodlebot's design, highlighting its reliability, user-friendliness, and versatility. Then, we demonstrate Doodlebot's versatility through example curricula about AI character design, autonomous robotics, and generative AI accessible to young learners. Finally, we share the results of a user study with elementary school youth where we found that the physical Doodlebot platform was as effective and user-friendly as the virtual version. This work offers insights into designing interactive educational robots that can inform future AI curricula and tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 139875
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 139838
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 140006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 140029
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139888
        }
      ]
    },
    {
      "id": 140095,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Constructing a Social Life with Robots, From Design Patterns to Interaction Ritual Chains",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1498",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "Robot designers commonly conceptualize robot sociality as a collection of features and capabilities. In contrast, sociologists define sociality as continuously constructed in and through situated interactions among people. Based on the latter perspective, we trace how robots are incorporated by robot companies and their staff and by robot owners into interaction ritual chains across diverse contexts: homes, cafes, robot stores, user organized meetups, and company events for robot users. Our empirical findings from ethnographic field work in Japan relating to three robots – aibo, RoboHon, and LOVOT – show how companies create positive interactions between people and robots through robot design patterns, by modeling successful interactions in person and online, and by bringing owners together in events that establish feelings of belonging and common values of acceptance of social robots as artifacts to live with and nurture. Owners, for their part, develop daily interaction rituals that include robots in their habits and activities and make interpersonal connections around robots in public meetups and\r\nevents. Companies and owners construct the notion of robots as social agents to live with through interconnected moments of situated interaction, related emotional responses, and meaning-making among people as they interact with robots and each other over time and in different contexts. Our work suggests that social robot design therefore needs to consider this broader framing of sociality and create affordances for the establishment of interaction ritual chains more explicitly.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University ",
              "dsl": "Information Science "
            }
          ],
          "personId": 140021
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139862
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 140096,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A Comprehensive User Study on Augmented Reality-Based Data Collection Interfaces for Robot Learning",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1178",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "Future versatile robots need the ability to learn new tasks and behaviors from demonstrations. Recent advances in virtual and augmented reality position these technologies as great candidates for the efficient and intuitive collection of large sets of demonstrations. While there are different possible approaches to control a virtual robot there has not yet been an evaluation of these control interfaces in regards to their efficiency and intuitiveness. These characteristics become particularly important when working with non-expert users and complex manipulation tasks. To this end, this work investigates five different interfaces to control a virtual robot in a comprehensive user study across various virtualized tasks in an AR setting. These interfaces include Hand Tracking, Virtual Kinesthetic Teaching, Gamepad, Motion Controller. Additionally, this work introduces Kinesthetic Teaching as a novel interface to control virtual robots in AR settings, where the virtual robot mimics the movement of a real robot manipulated by the user. This study reveals valuable insights into their usability and effectiveness. It shows that the proposed Kinesthetic Teaching interface significantly outperforms other interfaces in both objective and subjective metrics based on success rate, task completeness, and completion time and User Experience Questionnaires (UEQ+).\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139845
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139879
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139758
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Kalrsruhe Institute of Technology",
              "dsl": "Institute for Anthropomatics and Robotics"
            }
          ],
          "personId": 140031
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139771
        }
      ]
    },
    {
      "id": 140097,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Understanding Large-Language Model (LLM)-powered Human-Robot Interaction",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1211",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151389
      ],
      "eventIds": [],
      "abstract": "Generative AI, particularly large-language models (LLMs), hold significant promise in improving human-robot interaction. LLM-powered robots can not only maintain greater conversational capabilities, but they can also handle open-ended user requests across a wide range of tasks and domains. Despite the potential to transform human-robot interaction, very little is known about the distinctive design requirements for utilizing LLMs in robots, which may differ from other interaction modalities such as text and voice, and how these requirements might change across tasks and contexts. To better understand these requirements, we conducted a user study (n=32) that compared an LLM-powered social robot against two other agents---a text-based agent and a voice-based agent. To understand how these requirements differed across tasks, participants completed one of four conversational tasks: choose, generate, execute, and negotiate. Our findings show that LLM-powered robots elevate expectations for sophisticated non-verbal cues. While they excel in connection-building and deliberation tasks, they are less preferred for challenges in logical communication and anxiety-inducing situations. We provide design implications both for robots integrating LLMs and for fine-tuning LLMs for use with robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 140098,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Autonomy Acceptance Model (AAM): The Role of Autonomy and Risk in Security Robot Acceptance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1210",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "The rapid deployment of security robots across our society calls for further examination of their acceptance. This study explores human acceptance of security robots by theoretically extending the technology acceptance model to include the impact of autonomy and risk. To accomplish this, we conducted an online experiment with 236 participants, employing a 3 (autonomy) × 2 (risk) between-subjects design. Participants watched a video introducing a security robot, which operated at an autonomy level of either low, moderate, or high and presented either a low or high risk to humans. Our findings suggest that higher robot autonomy tends to reduce acceptance. Additionally, we observed the risk associated with security robots moderated the relationship between autonomy and acceptance. Based on these results, we offer recommendations for future research on security robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "Univeristy of Michigan, Ann Arbor",
              "dsl": "School of Information"
            }
          ],
          "personId": 140010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 140023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 139860
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139737
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139928
        },
        {
          "affiliations": [
            {
              "country": "Ethiopia",
              "state": "",
              "city": "Addis Ababa",
              "institution": "Addis Ababa University",
              "dsl": ""
            }
          ],
          "personId": 139797
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139939
        }
      ]
    },
    {
      "id": 140099,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Imagination vs. Reality: Investigating the Acceptance and Preferred Anthropomorphism in Service HRI ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1452",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "While the use of robots in public spaces is increasing, still few studies explore the resulting everyday human-robot interactions (HRI). The present study sought to bridge the disparity between real-world interactions and the frequently examined hypothetical interactions. To do so, we investigate the imagined and actual interaction with an ice cream serving robot. In two studies and an exploratory study comparison, we investigated user acceptance and preference for the degree of anthropomorphic appearance. Although a typical human service task was taken over by a robot, an industrial robot was preferred according to participant’s ratings in both studies. Moreover, both studies demonstrated that robot enthusiasm significantly relates to participants' acceptance of the robot for the task. Besides these commonalities, the results showed also that while humans were preferred over robots in the imagined setting, no clear preference was found in the real-life setting. Additional analyses compared the free text answers of the two studies and provided insights into participants' general attitudes toward robots in the workforce. In line with the higher preferences for humans over robots in the imagined setting, considerably more participants mentioned a better customer experience with humans as important in the imagined study compared to the participants who actually interacted with the robot. The studies strikingly demonstrated that imaginary settings yield similar outcomes to those where participants physically engage with the robot in certain aspects, such as their preference for anthropomorphism. However, this phenomenon does not appear to hold for other facets, such as their favored service agent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technical University Berlin",
              "dsl": ""
            }
          ],
          "personId": 139742
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "Technical University of Denmark",
              "dsl": "Transport Division"
            }
          ],
          "personId": 139759
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human-Agent Collaboration Lab"
            }
          ],
          "personId": 139956
        }
      ]
    },
    {
      "id": 140100,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Navigating Real-World Complexity: A Multi-Medium System for  Heterogeneous Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1132",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "Real-world robot system deployment is often performed in complex and unstructured environments. These complex environments coupled with multi-faceted global tasks often lead to complicated stakeholder structures, making designing for these environments extremely challenging. Magnifying this difficulty, tasks performed in these environments often cannot be accomplished by a single robot or even single robot type because of the broad range of needs and psychical constraints of the robots. In these cases, heterogeneous robot teams may need to be coupled to human team members to perform the global tasks. From a Human-Robot Interaction (HRI) perspective, this increases the complexity of designing and deploying the system significantly, as now complicated stakeholder structures are mixed with complex robot teams. This paper presents a novel real-world system and interface design leveraging multiple mediums to balance stakeholder needs. To this end, the UI presented here incorporates features that support shared mental models (SMMs), trust establishment and development, and utilizes a centralized data distribution architecture to improve team performance. In addition to the interface, this paper presents a detailed look at the design process and the lessons learned from the perspective of a multi-year, real-world deployed system, as part of a large European project consisting of 21 partners from varying countries and backgrounds.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "DreamLab"
            },
            {
              "country": "France",
              "state": "",
              "city": "Metz",
              "institution": "CNRS IRL2958 GT-CNRS",
              "dsl": ""
            }
          ],
          "personId": 139979
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": "Department IV - HCI"
            }
          ],
          "personId": 139986
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": "Business Psychology"
            }
          ],
          "personId": 139912
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 139867
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH",
              "dsl": "ITC/VR-Group"
            }
          ],
          "personId": 140062
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": ""
            }
          ],
          "personId": 139754
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": ""
            }
          ],
          "personId": 139850
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Trier",
              "institution": "Trier University",
              "dsl": "Business Psychology"
            }
          ],
          "personId": 139857
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Metz",
              "institution": "CNRS IRL2958 GT-CNRS",
              "dsl": ""
            }
          ],
          "personId": 139767
        }
      ]
    },
    {
      "id": 140101,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Lie, Repent, Repeat: Exploring Apologies after Repeated Robot Deception",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1508",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "This work presents an empirical study of repeated robot deception and its effects on changes in behavior and trust in a human-robot interaction scenario. 715 online and 50 in-person participants completed a multitrial driving simulation in which the car's robot assistant repeatedly lies and apologies. Through a mixed-method approach, our results show that apologies that offer justifications for deception in our scenario mitigate the negative effects on trust over multiple trials. However, given the time-sensitive, high-risk nature of our scenario, none of the apologies caused people to significantly change their decision to exceed the speed limit while rushing their dying friend to the hospital. These results add much needed knowledge to the understudied area of robot deception and could inform designers and policymakers of\r\nfuture practices when considering deploying robots that may learn\r\nto deceive.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 140048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 140049
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 139799
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 140008
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 139731
        }
      ]
    },
    {
      "id": 140102,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "\"An Emotional Support Animal, Without the Animal\": Design Guidelines for a Social Robot to Address Symptoms of Depression",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1428",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "Socially assistive robots offer an opportunity for utilizing therapeutic technologies within the context of depression and its symptoms. Through two workshop methodologies with individuals living with depression, as well as workshops with clinicians, design guidelines for developing a more personalized robot are presented. Focusing on the design of Therabot™, a customizable robot, individuals living with depression and co-morbidities, as well as clinicians, discussed various aspects of the robots design, sensors, behaviors, and a robot connected app. While similarities between the workshops occurred, such as a soft textured exterior, and natural colors and sounds, certain differences between the groups were present. Such as the robot being able to call for aid were present from clinicians, or who those with depression were comfortable sharing their data collected by the robot with.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University, Bloomington",
              "dsl": ""
            }
          ],
          "personId": 139743
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Stars Lab"
            }
          ],
          "personId": 140013
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 140065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Depaul University",
              "dsl": "Department of Computing and Digital Media"
            }
          ],
          "personId": 139790
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Public Health"
            }
          ],
          "personId": 139962
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Public Health"
            }
          ],
          "personId": 139905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississsippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 139769
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 140103,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Integrating Flow Theory and Adaptive Robot Roles: A Conceptual Model of Dynamic Robot Role Adaptation for the Enhanced Flow Experience in Long-term Multi-person Human-Robot Interactions",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1108",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "In this paper, we introduce a novel conceptual model for a robot's behavioral adaptation in its long-term interaction with humans, integrating dynamic robot role adaptation with principles of flow experience from psychology. This conceptualization introduces a hierarchical interaction objective grounded in the flow experience, serving as the overarching adaptation goal for the robot. This objective intertwines both cognitive and affective sub-objectives and incorporates individual and group-level human factors. The dynamic role adaptation approach is a cornerstone of our model, highlighting the robot’s ability to fluidly adapt its support roles—from leader to follower—with the aim of maintaining equilibrium between activity challenge and user skill, thereby fostering the user's optimal flow experiences. Moreover, this work delves into a comprehensive exploration of the limitations and potential applications of our proposed conceptualization. Our model places a particular emphasis on the multi-person HRI paradigm, a dimension of HRI that is both under-explored and challenging. In doing so, we aspire to extend the applicability and relevance of our conceptualization within the HRI field, contributing to the future development of adaptive social robots capable of sustaining long-term interactions with humans.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 140070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 140006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139888
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": " Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 140057
        }
      ]
    },
    {
      "id": 140104,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Power of Advice: Differential Blame for Human and Robot Advisors and Deciders in a Moral Advising Context",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1263",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "Due to their unique persuasive power, language-capable robots must be able to both act in line with human moral norms and clearly and appropriately communicate those norms. These requirements are complicated by the possibility that people may blame human and robot agents differently for violations of those norms. These complications raise particular challenges for robots giving moral advice to primary decision makers, as the robots and the deciders may be blamed differently for endorsing the same moral action. In this work, we thus explore how people morally evaluate both human and robot advisors for human and robot deciders. In Experiment 1 (𝑛 = 555), we examine human blame judgments of robot and human moral advisors and find clear evidence for an advice as decision hypothesis: advisors are blamed similarly to how they would be blamed for making the decisions they advised. In Experiment 2 (𝑛 = 1326), we examine people’s blame judgments of a robot or human decider following the advice of a robot or human moral advisor. We replicate the results from Experiment 1 and also find clear evidence for a differential dismissal hypothesis, in which moral deciders are penalized for ignoring moral advice, especially when a robot decider ignores a human advisor’s recommendation. Our results raise questions about people’s perception of moral advising situations, especially when they involve robots, and they present challenges for the design of morally competent\r\nlanguage-capable robots more generally.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 140075
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139752
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland, Baltimore County",
              "dsl": "CARDS"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139804
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Cognitive, Linguistic, and Psychological Sciences"
            }
          ],
          "personId": 139958
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 140105,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Robots for Social Justice (R4SJ): Toward a More Equitable Practice of Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1106",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "In this work, we present \\textit{Robots for Social Justice (R4SJ)}: a framework for an equitable engineering practice of Human-Robot Interaction, grounded in the Engineering for Social Justice (E4SJ) framework for Engineering Education. To understand the new insights this framework could provide to the field of HRI, we analyze the past decade of papers published at the ACM/IEEE International Conference on Human-Robot Interaction, and examine how well current HRI research aligns with the principles espoused in the E4SJ framework. Based on the gaps identified through this analysis, we make five concrete recommendations, and highlight key questions that can guide the introspection for engineers, designers, and researchers.\r\nWe believe these considerations are a necessary step not only to ensure that our engineering education efforts encourage students to engage in equitable and societally beneficial engineering practices (the purpose of E4SJ), but also to ensure that the technical advances we present at conferences like HRI promise true advances to society, and not just to fellow researchers and engineers. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "Computer Science/MIRRORLab"
            }
          ],
          "personId": 139826
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland Baltimore County",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139804
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 140106,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Multi-Embodiment and Robot Identity: A Scoping Review",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1028",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "Multi-embodied agents can have both physical and virtual bodies, moving between real and virtual environments to meet user needs, embodying robots or virtual agents alike to support extended human-agent relationships. As a design paradigm, multi-embodiment offers potential benefits to improve communication and access to artificial agents, but there are still many unknowns in how to design these kinds of systems. This paper presents the results of a scoping review of the multi-embodiment and robot identity research, aimed at consolidating the existing evidence and identifying knowledge gaps. Based on our review, we identify key research themes of: multi-embodied systems, identity design, human-agent interaction, environment and context, trust, and information and control. We also identify 16 key research challenges and 12 opportunities for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Canberra",
              "institution": "Australian National University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139953
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "The Australian National University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 140024
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "ACT",
              "city": "Canberra",
              "institution": "The Australian National University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139934
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "South Australia",
              "city": "Adelaide",
              "institution": "Defence Science and Technology (DST)",
              "dsl": ""
            }
          ],
          "personId": 140078
        }
      ]
    },
    {
      "id": 140107,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Follow me: Anthropomorphic appearance and communication impact social perception and joint navigation behavior",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1104",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": "This study addresses how anthropomorphic features shape users’ social perception and trust towards service robots and whether anthropomorphic characteristics influence the way people jointly navigate with them facing several obstacles in a course. Therefore, an experimental study was conducted where two communication and appearance designs (humanlike vs. machinelike) were examined for a service robot that provides transportation of goods by semi-automated following. The results of the study indicate that the humanlike robot design is rated more competent, warmer, less discomforting, and is generally preferred. Furthermore, participants jointly navigating with the humanlike designed robot walked around obstacles significantly more often indicating a more considerate navigation behavior and better remembering of system limits; both probably evoked by the humanlike design characteristics. In sum, the results of this study provide intriguing implications on how to target HRI for the service robot examined to enhance pleasant and error-free interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 139996
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Aachen",
              "institution": "Institute for Automotive Engineering",
              "dsl": "Traffic Psychology and Acceptance"
            }
          ],
          "personId": 139791
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 139851
        }
      ]
    },
    {
      "id": 140108,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Zero-Shot Learning to Enable Error Awareness in Data-Driven HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1269",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "Data-driven social imitation learning is a minimally-supervised approach to generate robot behaviors for human-robot interaction (HRI). However, this type of learning-based approach is error-prone.  Existing error detection methods for HRI rely on data labeling, rendering them inappropriate for the data-driven paradigm. We present a zero-shot error detection strategy that requires no labeled data. We use human interaction data to learn models of normal human behavior, then use these models to extract features that help discriminate abnormal human reactions to robot errors. In this feature space, we frame error detection as a novelty detection task, utilizing human interaction data to learn a model of non-erroneous interactions in an unsupervised fashion. Then, we apply the fitted novelty detector to HRI data to identify erroneous robot behavior. We show that our method obtains an average precision of 0.497 on errors, outperforming unsupervised baselines and supervised approaches with limited training data. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyoto",
              "institution": "Kyoto University, Graduate School of Informatics",
              "dsl": "Department of Social Informatics, HRI Lab"
            }
          ],
          "personId": 139915
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Human-Robot Interaction Lab."
            }
          ],
          "personId": 140039
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140109,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Snitches Get Unplugged: Adolescents' privacy concerns about robots in the home are relationally situated",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1423",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151391
      ],
      "eventIds": [],
      "abstract": "Though teens are a population with growing agency and use of smart technologies, their concerns surrounding privacy with AI and robots are under-represented in research. Using focus group discussions and a mixed methods analysis, we present findings about teens’ comfort levels with robotic information collection and sharing during three hypothetical scenarios involving a child interacting with a robot in the home. We find participant concerns align with an access-based definition of privacy which prioritizes being in control of their information and of when the robot behaves autonomously. Responses also indicate that teens conceptualize Haru not just as an intelligent device, but also as a social entity. Their shifts in comfort and discussions reflect an engagement in social relationship management with robots in the home in cases where the robot mediates a user’s responsibilities and relationships with others.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University - Bloomington",
              "dsl": "Informatics and Cognitive Science"
            }
          ],
          "personId": 139878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "Informatics"
            }
          ],
          "personId": 139796
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 140110,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Presentation of Robot-Intended Handover Position using Vibrotactile Interface during robot-to-human handover task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1301",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "Advancements in robot autonomy and safety enable close interactions such as object handovers with a human. During robot-to-human handovers in assembly tasks, the robot considers the human's state to determine the optimal handover position and timing. However, humans may struggle to focus on their primary tasks due to the need to track the robot's movement. This research aims to develop a vibrotactile interface that helps humans maintain focus on their primary tasks during object-receiving. The interface conveys the robot-intended handover position on the human's forearm in polar coordinates, displaying the angular direction and distance relative to the human hand via vibrations. Experimental results demonstrated that this method allowed participants to receive objects with faster reactions and completion time. Subjective evaluations reveal a perception of improved performance and reduced mental workload compared to baseline making the robot-to-human handover smoother and less distracting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology ",
              "dsl": "Information Science Division, Human Robotics Lab"
            }
          ],
          "personId": 139981
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": "Division of Information Science"
            }
          ],
          "personId": 139766
        }
      ]
    },
    {
      "id": 140111,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Toward Family-Robot Interactions: A Family-Centered Framework in HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1026",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151391
      ],
      "eventIds": [],
      "abstract": "As robotic products are increasingly integrated into day-to-day environments, there is a greater need to understand authentic and real-world human-robot interactions to inform the design of future products. Across many domestic, educational, and public settings, robots interact with not only individuals and groups of users, but also families, including children, parents, relatives, and even pets. However, the focus of products developed to date and research in human-robot and child-robot interactions have primarily been on the interaction with their primary users, neglecting the complex and multifaceted interactions between members of families and with the robot. There is a significant gap in knowledge, methods, and theories for how to design robots to support these interactions. To inform the design of robots that can support and enhance family life, this paper provides (1) a narrative review exemplifying the research gap and opportunities for family-robot interactions and (2) an actionable family-centered framework for research and practices in human-robot and child-robot interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 140037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 140112,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Robot-assisted Inside-mouth Bite Transfer using Robust Mouth Perception and Physical Interaction-Aware Control",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1025",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "Robot-assisted feeding can greatly enhance the lives of those with mobility limitations. Modern feeding systems can pick up and position food in front of a care-recipient's mouth for a bite. However, many with severe mobility constraints cannot lean forward and need direct inside-mouth food placement. Inside-mouth bite transfer demands precision, especially for those with restricted mouth openings, and adeptly managing various physical interactions — incidental contacts as the utensil moves inside, impulsive contacts due to sudden muscle spasms, deliberate maneuvers by the person being fed to guide the utensil, and intentional bites. In this paper, we propose an inside-mouth bite transfer method that addresses these challenges with two key components: a multi-view mouth perception pipeline robust to tool occlusion, and a control mechanism that employs multimodal time-series classification to discern and aptly react to different physical interactions. We demonstrate the individual efficacy of these components through two ablation studies. In a comprehensive user study, our system successfully fed 13 care-recipients with diverse mobility challenges. Participants consistently emphasized the comfort and safety of our inside-mouth bite transfer approach, and gave it high technology acceptance ratings, underscoring the transformative potential of this method in real-world scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 140071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139739
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139770
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139805
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Columbia University",
              "dsl": "Occupational Therapy"
            }
          ],
          "personId": 140064
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139931
        }
      ]
    },
    {
      "id": 140113,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Cyber-Physical Control Room: A Mixed Reality Interface for Mobile Robot Teleoperation and Human-Robot Teaming",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1300",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "In this work, we present the design and evaluation of an immersive Cyber-Physical Control Room interface for remote mobile robots that provides users with both robot-egocentric and robot-exocentric 3D perspectives. We evaluate the Cyber-Physical Control room against a traditional robot interface in a mock disaster response scenario that features a mixed human-robot field team. In our evaluation, we found that the Cyber-Physical Control Room improved robot operator effectiveness by 28% while navigating a complex warehouse environment and performing a visual search. The Cyber-Physical Control Room also enhanced various aspects of human-robot teaming, including conversational engagement, the ability of a remote robot teleoperator to track their human partner in the field, and opinions of human teammate leadership qualities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": ""
            }
          ],
          "personId": 139847
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": ""
            }
          ],
          "personId": 139938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139969
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 139964
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139943
        }
      ]
    },
    {
      "id": 140114,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Investigating the Impact of Gender Stereotypes in Authority on Avatar Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1266",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "We investigate how gender stereotypes in authority influence the perceptions and behavior of avatar robots operators and their interlocutors. Gender stereotypes, which typically place men in more authoritative positions than women, are present in not only inter-human but also human-robot interaction. As avatar robots become more integrated into our lives and serve for diverse usages, they may be utilized in positions in which require authority. We study how avatar robot gender and operator gender affect expressions and perception of gender stereotypes in a customer service scenario with 41 pairs of participants. Operators controlled binary gendered avatar robots one at a time, acting as shopkeepers that had to assert authority over customers behaving improperly. Our operators perceived their authority to be higher with male avatar robots compared to female ones, regardless of operator gender. We did not detect an effect on customer's perception of the shopkeeper's authority. While about half of operators and customers perceived authority for reasons related to traditional gender stereotypes, others observed behaviors that did not align with stereotypes. Avatar embodiment may also help operators assert authority safely due to being physically hidden from the customers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139985
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": ""
            }
          ],
          "personId": 139814
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140115,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "\"Give it Time\": Longitudinal Panels Scaffold Older Adults' Learning and Robot Co-Design",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1342",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151391
      ],
      "eventIds": [],
      "abstract": "Participatory robot design projects with older adults often use multiple sessions to encourage design feedback and active participation from users. These projects have, however, not analyzed the learning outcomes for older adults across co-design sessions and how they support constructive design feedback and meaningful participation. \r\nTo bridge this gap, we examined the learning outcomes within a \"longitudinal panel.\" This panel comprised seven co-design sessions with 11 older adults of varying cognitive abilities over six months, aimed at designing a robot to guide a photograph-based conversational activity. Using Nelson and Stolterman's framework of the hierarchy of design-learning, we demonstrate how older adult panelists achieved multiple design-learning outcomes -- capacity, confidence, capability, competence, courage, and connection -- which allowed them to provide actionable design suggestions. We provide guidelines for conducting longitudinal panels that can enhance user design-learning and participation in robot design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Informatics"
            }
          ],
          "personId": 139957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "Anthropology "
            }
          ],
          "personId": 140025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 140016
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Luddy School of Informatics, Computing, and Engineering"
            }
          ],
          "personId": 139798
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 140116,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "CORAL: A Cognitively Assistive Robot for Personalized Neurorehabilitation at Home",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1385",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "Cognitively assistive robots (CARs) have great potential to extend the reach of clinical interventions to the home. Due to the wide variety of cognitive abilities and rehabilitation goals, it is critical that these systems are flexible and adaptable in order to support rapid and accurate implementation of intervention content that is grounded in existing clinical practice. To this end, we detail the system architecture of CORAL (COgnitively assistive Robot for Adaptation and Learning), an adaptable robot system we developed in collaboration with our key stakeholders: clinicians and people with mild cognitive impairment (PwMCI). We implemented a well-validated compensatory cognitive training (CCT) intervention on CORAL, which it autonomously delivers to PwMCI. We deployed CORAL in the homes of these stakeholders in order to evaluate and gain initial feedback on the system. Our findings inform how HRI researchers can design more longitudinal and autonomous CARs for cognitive interventions. Furthermore, we will release elements of CORAL as open source to support flexible and adaptable home-deployed robots. Thus, CORAL will enable the HRI community to deploy quality interventions to robots, and ultimately increase the accessibility and extendability of these interventions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "San Francisco State University",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 140072
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139904
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Psychiatry"
            }
          ],
          "personId": 140047
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 140117,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "(Gestures Vaguely): The Effects of Robots' Use of Abstract Pointing Gestures in Large-Scale Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1187",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "As robots are deployed into large-scale human environments, they will need to engage in task-oriented dialogues about objects and locations beyond those that can currently be seen. In these contexts, speakers use a wide range of referring gestures beyond those used in the small-scale interaction contexts that HRI research typically investigates. In this work, we thus seek to understand how robots can better generate gestures to accompany their referring language in large-scale interaction contexts. In service of this goal, we present the results of two human-subject studies: (1) a human-human study exploring how human gestures change in large-scale interaction contexts, and to identify human-like gestures suitable to such contexts yet readily implemented on robot hardware; and (2) a human-robot study conducted in a tightly controlled Virtual Reality environment, to evaluate robots' use of those identified gestures. Our results show that robot use of \r\nPrecise Deictic and Abstract Pointing gestures afford different types of benefits when used to refer to visible vs. non-visible referents, leading us to formulate three concrete design guidelines. These results highlight both the opportunities for robot use of more humanlike gestures in large-scale interaction contexts, as well as the need for future work exploring their use as part of multi-modal communication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Greenwood Village",
              "institution": "RAISonance",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 140066
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139919
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139783
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 140118,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "IntentAR: Immersive Authoring of Condition-based AR Robot Visualisations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1022",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "We introduce RoboVisAR, an immersive augmented reality (AR) authoring tool to create in-situ robot visualisations. AR robot visualisations such as the robot’s path, status, and safety zones has shown to benefit human-robot collaboration. However, creating custom AR visualisations requires extensive skills in both robotics and AR programming. RoboVisAR allow users to create custom AR robot visualisations without programming. By recording an example robot program behavior, users can create and test custom visualisations in-situ within a mixed reality environment. RoboVisAR supports six types of visualisations; path, point-of-interest, safety zone, robot state, message, and force/torque. Furthermore, RoboVisAR supports four types of conditions; robot state, proximity, inside-box, and force/torque. Their features enable the users to easily combine different visualisations on demand to make the context-aware assistant without visual clutter. An expert user study with three participants suggests that users generally appreciate the customizability of the visualisation and they easily create robot visualisations in less than ten minutes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140017
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139840
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Konstanz",
              "institution": "University of Konstanz",
              "dsl": "HCI Group"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140043
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "Århus",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139975
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aarhus",
              "institution": "Aarhus University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139926
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 139883
        }
      ]
    },
    {
      "id": 140119,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Power of Opening Encounters in HRI: How initial robotic behavior shapes the interaction that follows",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1066",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "Opening encounters are a fundamental component of every interaction. Psychology research highlights the valence of opening encounters as one of the main factors that shape the nature of the interaction that follows. In this work, we evaluated whether opening encounters would have a similarly powerful effect on human-robot interactions. We tested how positive and negative opening encounters with a robot would impact the subsequent interaction. In the experiment, a robotic dog approached a participant in a waiting room. The robot performed gestures designed to communicate different valences of opening encounters under three conditions: Positive, Negative, or No opening encounter. To evaluate the impact on the subsequent interaction, we measured participants' willingness to comply with a help request presented by the robot and their overall perception of the robot. Objective and subjective measures indicated that most participants in the Positive opening encounter condition helped the robot and reported a positive overall perception. An opposite pattern emerged in the other two conditions. Almost none of the participants helped the robot, and the overall perception of the robot was negative. Our findings suggest that opening encounters with robots should be carefully considered and well-designed due to their profound impact on the interaction that follows.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Reichman University",
              "dsl": "Media Innovation Lab"
            }
          ],
          "personId": 140053
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Media Innovation Lab, Reichman University",
              "dsl": ""
            }
          ],
          "personId": 139733
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya ",
              "institution": "Media Innovation Lab, Reichman University",
              "dsl": ""
            }
          ],
          "personId": 139855
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Herzelia ",
              "institution": "Media innovation lab, Riechman  University ",
              "dsl": ""
            }
          ],
          "personId": 139990
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Media Innovation Lab, Reichman University",
              "dsl": ""
            }
          ],
          "personId": 139908
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Media Innovation Lab, Reichman University",
              "dsl": ""
            }
          ],
          "personId": 139871
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Media Innovation Lab, Reichman University",
              "dsl": ""
            }
          ],
          "personId": 140038
        }
      ]
    },
    {
      "id": 140120,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Preference-Conditioned Language-Guided Abstraction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1516",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151389
      ],
      "eventIds": [],
      "abstract": "Learning from demonstrations is a common paradigm for users to teach robots but is prone to spurious feature correlations. Recent work constructs state abstractions, i.e. visual representations containing only task-relevant features, from language as a way to perform more generalizable learning. However, these abstractions also depend on a user's preference for what matters in a task, which may be hard to describe or infeasible to exhaustively specify with language alone. How do we construct abstractions to capture these latent preferences? We observe that how humans behave reveals how they see the world. Our key insight is that differences in human behavior inform us that there are differences in preferences for how humans see the world, i.e. their state abstractions. In this work, we propose using language models (LMs) to query for those preferences directly given knowledge that a change in behavior has occurred. In our framework, we use the LM in two ways: first, given a text description of the task and knowledge of behavioral change between states, we query the LM for possible hidden preferences; second, given the most likely preference, we query the LM to construct the state abstraction. In this framework, the LM is also able to actively elicit human preferences when uncertain about its own estimate. We demonstrate our framework's ability to construct effective preference-conditioned abstractions in simulated experiments, a user study, as well as on a real Spot robot performing mobile manipulation tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "EECS"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Boston Dynamics AI Institute",
              "dsl": ""
            }
          ],
          "personId": 139763
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Boston Dynamics AI Institute",
              "dsl": ""
            }
          ],
          "personId": 139764
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139817
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139940
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University ",
              "dsl": ""
            }
          ],
          "personId": 139913
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": ""
            }
          ],
          "personId": 139914
        }
      ]
    },
    {
      "id": 140121,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Making Informed Decisions: Supporting Cobot Integration Considering Business and Worker Preferences",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1515",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "Robots are ubiquitous in manufacturing settings from small-scale to large-scale. While collaborative robots (cobots) have significant potential in these settings due to their flexibility and ease of use, they can only reach their full potential when properly integrated. Specifically, cobots need to be integrated in a manner that properly utilizes their strengths, improves the performance of the manufacturing process, and can be used in concert with human workers. Understanding how to properly integrate cobots into existing manufacturing workflows requires careful consideration and the knowledge of roboticists, manufacturing engineers, and business administrators. In this work, we propose an approach to collaborating with manufacturers prior to the integration process that involves planning, analysis, development, and presentation of results. This approach ultimately allows manufacturers to make an informed choice about cobot integration within their facilities. We illustrate the application of this approach through a case study with a manufacturing collaborator and discuss insights learned throughout the process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": ""
            }
          ],
          "personId": 140068
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 140054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 140122,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "More Than Binary: Transgender and Nonbinary Perspectives on Human Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1032",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "  Previous research has shown that gendered robot designs prompt users to carry biases from human-human interaction into human-robot interaction. Yet avoiding gendered designs in human-robot interaction may be infeasible, as humans readily gender robots based on factors like name, voice, and pronouns. One solution to this challenge could be to use an intentionally agender robot design, that is explicitly presented as agender in the way that some humans identify. Yet it is unclear whether trans, nonbinary, or otherwise gender nonconforming people would view this approach to be a positive and inclusive step in robot design, or whether they would view it as appropriative or otherwise problematic. In fact, little is known about trans and nonbinary perspectives on human-robot interaction, which has not been previously studied. In this work, we thus present the first study of trans and nonbinary perspectives on robot design, with a particular focus on perceptions of robot gender and agender robot design. Our results suggest that trans and nonbinary users readily accept robots depicted as agender, and view this as a largely positive design strategy that could help normalize non-cisgender identities. Yet our results also highlight key risks posed by this design strategy, including risks of backlash, caricature, and dehumanization, and the ways these risks are moderated by a number of political and economic factors. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 140123,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Independence in the Home: A Wearable Interface for a Person with Quadriplegia to Teleoperate a Mobile Manipulator",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1074",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "Teleoperation of mobile manipulators within a home environment can significantly enhance the independence of individuals with severe motor impairments, allowing them to regain the ability to perform self-care and household tasks. There is a critical need for novel teleoperation interfaces to offer effective alternatives for individuals with impairments who may encounter challenges in using existing interfaces due to physical limitations. In this work, we iterate on one such interface, HAT (Head-Worn Assistive Teleoperation), an inertial-based wearable integrated into any head worn garment. We evaluate HAT through a 7-day in-home study with John Doe, a non-speaking individual with quadriplegia who has participated extensively in assistive robotics studies. We additionally evaluate HAT with a proposed shared control method for mobile manipulators termed Driver Assistance and we demonstrate how the interface generalizes to other physical devices and contexts. Our results show that HAT is a strong teleoperation interface across key metrics including efficiency, ease of use, errors, learning curve, and workload. Videos can be found on our project website. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 139780
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 140058
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Automation"
            }
          ],
          "personId": 140060
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Carnegie Mellon University"
            }
          ],
          "personId": 140052
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Martinez",
              "institution": "Hello Robot Inc.",
              "dsl": ""
            }
          ],
          "personId": 139740
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 140009
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 139983
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 140028
        }
      ]
    },
    {
      "id": 140124,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "“Sorry to Keep You Waiting”: Recovering from Negative Consequences Resulting from Service Robot Unintended Rejection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1392",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "Robots are increasingly deployed in crowded, large-scale environments where the demands on their services can outweigh their ability to respond. When robots fail to respond, humans may interpret the unintended consequence negatively as forms of rejection, leading to lost of trust. How do service robots recover from such perceived rejection to recover human trust due to unintended rejection? We created a task mimicking shopping malls where the robot arm is asked to provide coffee, juice, or tea to participants. When the robot rendered service elsewhere, participants reported feeling excluded and less trusting of the robot. When the robot subsequently apologized or provided promise of future favor, participants regained trust in the robot, with favor rendering yielding significantly more trusting responses. This study highlights the importance of understanding inadvertently negative consequences of robot behaviors, and suggests design solutions for overcoming this negative perception through remediation strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 140055
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 140027
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "Kowloon Tong",
              "city": "hong kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 140069
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "Kowloon Tong",
              "city": "hong kong",
              "institution": "City University of Hong Kong",
              "dsl": "School of Creative Media"
            }
          ],
          "personId": 140004
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "City University of Hong Kong",
              "dsl": ""
            }
          ],
          "personId": 139784
        }
      ]
    },
    {
      "id": 140125,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Encountering Autonomous Robots in Public Streets",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1071",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": " Robots deployed in public settings enter spaces that humans live and work in. Understandings of robots, such as those in HRI and beyond, tend to prioritise direct, obvious and deliberate interactions with robots. Yet this fails to identify the most common form of mundane, everyday response to robots in public, which ranges from the unobvious and subtle to virtually ignoring them. Drawing on a collection of video recordings, we show how public delivery robots encounter the lived-in spaces of urban streets both from a perspective of the social assembly of the physical environment and the socially organised nature of everyday street life that such robots are entering. Ultimately we show how such robots are effectively `granted passage' through these spaces as a result of the mundane, practical work of the streets' human inhabitants. We demonstrate the importance of studying robots during their whole deployment, in the spaces that they enter, and challenge the current understanding of what `counts' as human-robot interaction, highlighting that we may want to re-think who we consider as `user' as well as the conceptualisation of human-robot interaction itself.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Culture and Society"
            }
          ],
          "personId": 139991
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Nottinghamshire",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Lab, School of Computer Science"
            }
          ],
          "personId": 139959
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Department of Language and Linguistic Science"
            }
          ],
          "personId": 140063
        }
      ]
    },
    {
      "id": 140126,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Bridging HRI Theory and Practice: Design Guidelines for Robot Communication in Dairy Farming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1192",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "Using HRI theory to inform robot development is an important, but difficult, endeavor. This paper explores the relationship between HRI theory and HRI practice through a design project on the development of design guidelines for human-robot communication together with a dairy farming robot manufacturer. The design guidelines, a type of intermediate-level knowledge, were intended to enrich the specialized knowledge of the company on farming context with relevant academic knowledge. In this process, we identified that HRI theories were used as a frame, a tool, best practices, and a reference; while the HRI practice provided a context, a reference, and validation for the theories. Our intended contribution is to propose a means to facilitate exchanges both ways between HRI theory and practice and add to the emerging repertoire of designerly ways of producing knowledge in HRI. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Human-Centered Design"
            }
          ],
          "personId": 139741
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": ""
            }
          ],
          "personId": 139955
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Maasluis",
              "institution": "Lely Industries",
              "dsl": ""
            }
          ],
          "personId": 139997
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "South-Holland",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Industrial Design Engineering"
            }
          ],
          "personId": 139848
        }
      ]
    },
    {
      "id": 140127,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Collabot: A Robotic System That Assists Library Users Through Collaboration Between Robots",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1513",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "A library serves as a repository of knowledge accessible to individuals of all ages, genders, educational backgrounds, social statuses, and economic levels. It stands as a communal space where community members can gather, bridging information disparities among various societal strata. To enhance accessibility to such libraries for a broader spectrum of people, we have introduced the CollaBot system. This system offers tailored services to users through the collaboration of robots. Our investigation encompassed the acceptance of robot types by users, robot characterization, and the prioritization of robot-provided services. Over the course of three stages of user evaluation, it became evident that participants preferred product-type robots over anthropomorphic robots. Furthermore, they expressed a preference for robots that assist other robots, even if these assisting robots exhibit clumsiness, as opposed to robots that exclusively excel in their designated tasks. Lastly, service prioritization varied based on the specific limitations or deficiencies faced by individual users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 139984
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 139787
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 139808
        }
      ]
    },
    {
      "id": 140128,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Field Trial of an Autonomous Shopworker Robot that Aims to Provide Friendly Encouragement and Exert Social Pressure",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1315",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151393
      ],
      "eventIds": [],
      "abstract": "We developed an autonomous hatshop robot for encouraging customers to try on hats by providing comments that appropriately fit their actions, and in such a way also indirectly exerting social pressure. To enable it to offer such a service smoothly in a real shop, we developed a large system (around 150k lines of code with 23 ROS packages) integrated with various technologies, like people tracking, shopping activity recognition and navigation. The robot needed to move in narrow corridors, detect customers, and recognise their shopping activities. We employed an iterative development process, repeating trial-and-error integration with the robot in the actual shop, while also collecting real-world data during field-testing. This process enabled us to improve our shopping activity recognition system by collecting real-world data, and to adapt our software modules to the target shop environment. We report the lessons learnt during our system development process. The results of our 11-day field trial show that our robot was able to provide its services reasonably well. Many customers expressed a positive impression of the robot and its services.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139870
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Souraku-gun,",
              "institution": "ATR",
              "dsl": "IRC"
            }
          ],
          "personId": 139944
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139819
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139909
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140129,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Sprout: Designing Expressivity for Robots Using Fiber-Embedded Actuation",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1116",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "In this paper, we explore how techniques from soft robotics can help create a new form of robotic expression. We present Sprout, a soft expressive robot that conveys its internal states by changing the shape of its body. By integrating fiber-embedded actuators into its construction, Sprout can extend, expand, twist, and bend. These movements enable Sprout to express its internal states, for example, by expanding when it is angry and bending its body forward when it is curious. Through two user studies, we investigated how Sprout's expressions were interpreted by users, how users perceived Sprout, and how users interacted with it. We propose the integration of soft actuators as a novel design space for designing robot expressions to convey emotional and internal states.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Computer Science, People and Robots Lab"
            }
          ],
          "personId": 139828
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "madison",
              "institution": "UW, Madison",
              "dsl": ""
            }
          ],
          "personId": 139775
        }
      ]
    },
    {
      "id": 140130,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Goal-Oriented End-User Development of Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1477",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": "End-user development (EUD) tools must balance a user's desired level of control over a robot's task with the robot's ability to plan and act autonomously. Many existing task-oriented EUD tools enforce a specific level of control, e.g., by requiring that users hand-craft detailed sequences of actions. There is a lack of EUD solutions that offer users the flexibility to choose the level of task detail they wish to express. We thereby created a novel EUD system, Polaris, which affords users a similar level of expressiveness as state-of-the-art EUD systems, yet the fundamental building blocks of tasks within Polaris are goals rather than robot actions. Polaris's goal-oriented programming paradigm enables users to express high-level task objectives or lower-level task checkpoints at their choosing, while an off-the-shelf task planner fills in any remaining task detail. To ensure that goal-specified tasks adhere to user expectations of robot behavior, Polaris is equipped with a Plan Visualizer that exposes the full task plan to the user prior to runtime. In what follows, we describe the system design of Polaris and its evaluation with 32 human participants. Our results support the Plan Visualizer's ability to help users craft higher-quality task plans. Furthermore, there are strong associations between Plan Visualizer usage and perceptions of robot performance, and evidence that users' past familiarity with robots has a key role in shaping user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "U.S. Naval Research Laboratory",
              "dsl": "Navy Center for Applied Research in AI "
            }
          ],
          "personId": 139921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "The US Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 140032
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "US Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 139952
        }
      ]
    },
    {
      "id": 140131,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Effects of Observing Robotic Ostracism on Children's Prosociality and Basic Needs",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1433",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151391
      ],
      "eventIds": [],
      "abstract": "Research on robotic ostracism is still scarce and has only explored its effects on adult populations. Although the results revealed important carryover effects of robotic exclusion, there is no evidence yet that those results occur in child-robot interactions. This paper provides the first exploration of robotic ostracism with children. We conducted a study using the Robotic Cyberball Paradigm in a third-person perspective with a sample of 52 children aged between five to ten years old. The experimental study had two conditions: \r\nExclusion and Inclusion. In the Exclusion condition, children observed a peer being excluded by two robots; while in the Inclusion condition, the observed peer interacted equally with the robots. Notably, even 5-year-old children could discern when robots excluded another child. Children who observed exclusion reported lower levels of belonging and control, and exhibited higher prosocial behaviour than those witnessing inclusion. However, no differences were found in children's meaningful existence, self-esteem, and physical proximity across conditions. Our user study provides important methodological considerations for applying the Robotic Cyberball Paradigm with children. The results extend previous literature on both robotic ostracism with adults and interpersonal ostracism with children. We finish discussing the broader implications of children observing ostracism in human-robot interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "ITI, LARSYS, Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": ""
            }
          ],
          "personId": 139868
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 139946
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "Interactive Technologies Institute"
            }
          ],
          "personId": 139881
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "Interactive Technologies Institute"
            }
          ],
          "personId": 140026
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Reichman University",
              "dsl": "Media Innovation Lab"
            }
          ],
          "personId": 140053
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID, IST"
            }
          ],
          "personId": 139894
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "ITI/LARSyS, Instituto Superior Técnico"
            }
          ],
          "personId": 139824
        }
      ]
    },
    {
      "id": 140132,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "With Every Breath:  Testing the Effects of Soft Robotic Surfaces on Attention and Stress",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1356",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "We report on the impact of a soft robot of our design on emotional wellbeing using a mixed methods approach. Participants (N=93) engaged with our soft robotic surface designed to simulate the benefits of nature and provide therapeutic behavioral interventions to reduce stress. The study assessed sustained attention, perceived restorativeness, and self-reported stress. While significant differences in sustained attention were not found between intervention groups, observed patterns suggested a minor positive effect, with the Breathing Group exhibiting the greatest improvement. Stress emerged as a significant predictor of accuracy on the attention task, supporting the merit of using a robotic surface for guided breathing exercises to reduce stress. Ratings from the Perceived Restorativeness Scale revealed that the Ocean and Breathing Groups experienced the environment as more restorative than the Control after engaging with the prototype, yet group differences lacked statistical significance. Participants were favorable to the surface’s behavior, characterizing it as soothing and fascinating. These findings suggest the promise of soft robots to support emotional wellbeing and improve quality of life.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Environmental Design"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Human Centered Design, College of Human Ecology, Architectural Robotics Lab"
            }
          ],
          "personId": 139877
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Human Centered Design; Mechanical & Aerospace Engineering"
            }
          ],
          "personId": 139776
        }
      ]
    },
    {
      "id": 140133,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "PoseTron: Enabling Close-Proximity Human-Robot Collaboration Through Multi-human Motion Prediction",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1510",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": "As robots enter human workspaces, there is a crucial need for robots to understand and predict human motion to achieve safe and fluent human-robot collaboration (HRC). However, achieving accurate human motion prediction remains a significant challenge due to the lack of large-scale datasets capturing close-proximity HRC and the lack of efficient, generalizable algorithms that can reliably predict the motion of multiple humans in human-robot teams. To address these challenges, we introduce INTERACT, a comprehensive multimodal dataset comprising 3-D Skeleton data, RGB+D data from two viewpoints, ego-view, eye-tracking, and gaze data of two participants, and robot joint data, covering both human-human and human-robot collaboration in teams. Next, to address the gap in learning algorithms to predict multi-human motion accurately, we propose PoseTron, a novel transformer-based encoder-decoder architecture that can generalize to multiple agents and utilize various data modalities. One of PoseTron’s key contributions is the novel conditional attention mechanism in the encoder, enabling efficient extraction and weighing of motion information from all agents to incorporate team dynamics. Additionally, the decoder introduces a novel multimodal attention mechanism, which weights representations from different modalities and the encoder outputs to predict future motion accurately. We extensively evaluated PoseTron by comparing its performance on human-human and human-robot collaboration scenarios from the INTERACT dataset against state-of-the-art multi-agent motion prediction methods. The results suggest that PoseTron outperformed all other methods across all the scenarios and evaluated temporal horizons. Furthermore, we conducted a comprehensive ablation study that underscores the architectural and multimodal design choices. The superior performance of PoseTron provides a promising direction to integrate motion prediction with robot perception and enable safe and effective HRC.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "School of Engineering and Applied Science"
            }
          ],
          "personId": 139834
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "CHARLOTTESVILLE",
              "institution": "University of Virginia",
              "dsl": "Engineering School"
            }
          ],
          "personId": 139988
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "School of Engineering and Applied Science"
            }
          ],
          "personId": 139839
        }
      ]
    },
    {
      "id": 140134,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Artificial Emotions and the Evolving Moral Status of Social Robots",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1036",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "This article aims to explore the potential impact of artificial emotional intelligence (AEI) on the ethical standing of social robots. By examining how AEI interacts with and potentially reshapes the two dominant perspectives on robots’ moral status, namely the property-oriented approach and the social-relational approach, we aim to offer fresh insights into this pressing dilemma. Our analysis reveals that although the incorporation of AEI does not conclusively confer moral status to current social robots, it might challenge the boundaries that separate robots from other entities customarily considered to have more status, thereby increasing the complexity of the debate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Halden",
              "institution": "Østfold University College",
              "dsl": "Computer Science and Communication"
            }
          ],
          "personId": 139924
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Halden",
              "institution": "Østfold University College",
              "dsl": "Computer Science and Communication"
            }
          ],
          "personId": 139917
        }
      ]
    },
    {
      "id": 140135,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Combining Emotional Gestures, Sound Effects, and Background Music for Robotic Storytelling - Effects on Storytelling Experience, Emotion Induction, and Robot Perception",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1399",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151394
      ],
      "eventIds": [],
      "abstract": "Storytelling is a long-established human tradition for entertainment and knowledge transfer. Social robots are emerging as a new storytelling medium, being able to imitate human storytelling using gestures but also extend it by adding, e.g., sound effects to the experience. Due to COVID-19 restrictions, we conducted an online video-based study to investigate the effects of congruent respectively incongruent or no gesture usage in combination with additional non-speech sounds, i.e. sound effects and background music, on recipients' transportation into the story told, emotion induction, and perception of the robot. Results indicate no effect of additional non-speech sound integration on the variables listed above. Contradicting with related findings from in-person studies, we found a no significant differences between congruent, incongruent and no gesture usage. Last, no interplay of additional sounds and gesture congruence was identified. Future studies should provide deeper insights into the importance of multimodal congruence in video-taped robots and the possible advantages of adding non-speech sounds to online but also in-person robotic storytelling as well as their interplay in in-person HRI. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 139882
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 139807
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-University Würzburg",
              "dsl": ""
            }
          ],
          "personId": 139885
        }
      ]
    },
    {
      "id": 140136,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Reactive or Proactive? How Robots Should Explain Failures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1277",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": "As robots tackle increasingly complex tasks, the need for explanations becomes essential for gaining trust and acceptance. Explainable robotic systems should not only elucidate failures when they occur but also predict and preemptively explain potential issues. This paper compares explanations from Reactive Systems, which detect and explain failures after they occur, to Proactive Systems, which predict and explain issues in advance. Our study reveals that the Proactive System fosters higher perceived intelligence and trust and its explanations were rated more understandable and timely. Our findings aim to advance the design of effective robot explanation systems, allowing people to diagnose and provide assistance for problems that may prevent a robot from finishing its task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 139907
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A & M University",
              "dsl": "Department of Mechanical Engineering"
            }
          ],
          "personId": 139815
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 139782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Provo",
              "institution": "Brigham Young University",
              "dsl": ""
            }
          ],
          "personId": 139789
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 139922
        }
      ]
    },
    {
      "id": 140137,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "\"I'm not touching you. It's the robot! \":  Inclusion through a Touch-based Robot among Mixed-Visual Ability Children",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1354",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Children with visual impairments often struggle to fully participate in group activities due to limited access to visual cues. They have difficulty perceiving what is happening, when, and how to act—leading to children with and without visual impairments being frustrated with the group activity, reducing mutual interactions. To address this, we created Touchibo, a tactile storyteller robot acting in a multisensory setting, encouraging touch-based interactions. Touchibo provides an inclusive space for group interaction as touch is a highly accessible modality in a mixed-visual ability context. In a study involving 107 children (37 with visual impairments), we compared Touchibo to an audio-only storyteller.\r\nResults indicate that Touchibo significantly improved children's individual and group participation perception and was more likable and helpful, sparking touch-based interactions. \r\nOur study highlights touch-based robots' potential to enrich children's social interactions by prompting interpersonal touch, particularly in mixed-visual ability settings. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 139946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Mechanical and Aerospace Engineering"
            }
          ],
          "personId": 139942
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "ITI, LARSYS, Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": ""
            }
          ],
          "personId": 139868
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisboa",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 139994
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, University of Lisbon",
              "dsl": "ITI/LARSyS"
            }
          ],
          "personId": 139970
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "ITI/LARSyS",
              "dsl": "Tecnico University Lisbon"
            }
          ],
          "personId": 139801
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 140041
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "ITI/LARSyS, Instituto Superior Técnico"
            }
          ],
          "personId": 139824
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID, IST"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "Radcliffe Institute for Advanced Study"
            }
          ],
          "personId": 139894
        }
      ]
    },
    {
      "id": 140138,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A Social Approach for Autonomous Vehicles: A Robotic Object to Enhance Passengers’ Sense of Safety and Trust",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1233",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": "One of the central challenges in designing autonomous vehicles concerns passenger trust and sense of safety. This challenge is related to passengers' well-established past experience with non-autonomous vehicles, which leads to concern about the absence of a driver. We explored whether it is possible to address this challenge by designing an interaction with a simple robotic object positioned on the vehicle's dashboard. We leveraged the automatic human tendency to interpret non-verbal robotic gestures as social cues and designed an interaction with the robot in the autonomous vehicle. The robotic object greeted the passenger, indicated that the vehicle was attentive to its surroundings, and informed the passenger that the drive was about to begin. We evaluated whether the robot's non-verbal behavior would provide the signals and social experience required to support passengers' trust and sense of safety. In an in-person (in-situ) experiment, participants were asked to enter an autonomous vehicle and take the time to decide if they were willing to go for a drive. As they entered the vehicle, the robot performed the designed behaviors. We evaluated the participants' considerations and experience while they made their decision. Our findings indicated that participants' trust ratings and safety-related experience were higher than those of a baseline group who did not experience the interaction with the robot. Participants also perceived the robot as providing companionship during a lonely experience. We suggest that robotic objects are a promising technology for enhancing passengers' experience in autonomous vehicles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Reichman University",
              "dsl": "Media Innovation Lab, School of Communications"
            }
          ],
          "personId": 140045
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Reichman University",
              "dsl": "Media Innovation Lab, School of Communications"
            }
          ],
          "personId": 139786
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Herzliya",
              "institution": "Reichman University",
              "dsl": "Media Innovation Lab"
            }
          ],
          "personId": 140053
        }
      ]
    },
    {
      "id": 140139,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Evaluating the Impact of Personalized Value Alignment in Human-Robot Interaction: Insights into Trust and Team Performance Outcomes",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1155",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "This paper examines the effect of real-time, personalized alignment of a robot’s reward function to the human’s values on trust and team performance. We present and compare three distinct robot interaction strategies: a non-learner strategy where the robot presumes the human’s reward function mirrors its own; a non-adaptive-learner strategy in which the robot learns the human’s reward function for trust estimation and human behavior modeling, but still optimizes its own reward function; and an adaptive-learner strategy in which the robot learns the human’s reward function and adopts it as its own. Two human-subject experiments with a total number of 𝑁 = 54 participants were conducted. In both experiments, the human-robot team searches for potential threats in a town. The team sequentially goes through search sites to look for threats. We model the interaction between the human and the robot as a trust-aware Markov Decision Process (trust-aware MDP) and use Bayesian Inverse Reinforcement Learning (IRL) to estimate the reward weights of the human as they interact with the robot. In Experiment 1, we start our learning algorithm with an informed prior of the human’s values/goals. In Experiment 2, we start the learning algorithm with an uninformed prior. Results indicate that when starting with a good informed prior, personalized adaptation to values does not seem to benefit trust or team performance. On the other hand, when an informed prior is unavailable, adapting to human values leads to high trust and higher perceived performance while maintaining the same objective team performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 139813
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Wright-Patterson AFB",
              "institution": "Air Force Research Laboratory ",
              "dsl": ""
            }
          ],
          "personId": 139732
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Miami",
              "institution": "University of Miami",
              "dsl": "Miami Herbert Business School"
            }
          ],
          "personId": 139761
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 139999
        }
      ]
    },
    {
      "id": 140140,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Social Cue Analysis using Transfer Entropy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1154",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": "Robots that work close to humans need to understand and use social cues to act in a socially acceptable manner. Social cues are a form of communication (i.e., information flow) between people. In this paper, a framework is introduced to detect and analyse a class of perceptible social cues that are nonverbal and episodic, and the related information transfer using an information-theoretic measure, namely, transfer entropy. We use a group-joining setting to demonstrate the practicality of transfer entropy for analysing communications between humans. Then we demonstrate the framework in two settings involving social interactions between humans: object-handover and person-following. Our results show that transfer entropy can identify information flows between agents and when and where they occur. Potential applications of the framework include information flow or social cue analysis for interactive robot design and socially-aware robot planning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Monash",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 139744
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Electrical and Computer Systems Engineering"
            }
          ],
          "personId": 139989
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne ",
              "institution": "Monash University ",
              "dsl": ""
            }
          ],
          "personId": 139978
        }
      ]
    },
    {
      "id": 140141,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Dimensional Design of Generative Emotive Sounds for Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1077",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151389
      ],
      "eventIds": [],
      "abstract": "Non-Linguistic Utterances (NLUs) are essential parts of emotive exchanges, not only in human-human interactions but also in the context of human-robot interactions. This research aims to deepen our understanding of generative emotive sounds for the domain of human-robot exchanges. We investigated the connections between certain audio qualities and the perception of emotional arousal and pleasure, designing a novel generative algorithm using musical and prosodic parameters capable of expressing a range of emotions and a dimensional model of emotion. To assess the design algorithm, we conducted an end-user evaluation in which participants were asked to interpret the emotive NLUs conveyed by robots. In the evaluation we examined 4 archetypal emotions: excitement, contentment, sadness, and anger. We placed participants’ responses within the pleasure-arousal affect grid to analyze the distinctness of the emotive sounds. The study revealed that participants consistently associated excited, sad and angry NLUs with significantly different emotional states but not for content NLUs. These findings contribute valuable insights into how to design generative NLUs which can enhance the emotional depth of human-robot interactions, with potential applications across various domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Waterville",
              "institution": "Colby College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139874
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Waterville",
              "institution": "Colby College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Waterville",
              "institution": "Colby College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139823
        }
      ]
    },
    {
      "id": 140142,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Anticipating the Use of Robots in Domestic Violence: A Typology of Robot Facilitated Abuse to Support Risk Assessment and Mitigation in Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1209",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151402
      ],
      "eventIds": [],
      "abstract": "Domestic abuse research demonstrates that perpetrators are agile in finding new ways to coerce and to consolidate their control. They may leverage loved ones or cherished objects, and are increasingly exploiting/subverting what have become everyday ``smart\" technologies. Robots sit at the intersection of these categories: they bring together multiple digital and assistive functionalities in an (anthropomorphised) physical body, oftentimes designed explicitly to take on a social companionship role. We present a typology of robot facilitated abuse based on these unique affordances, designed to support systematic risk assessment, mitigation and design work. Whilst we focus on domestic abuse, our typology is relevant for any application context where there exist significant power differentials between different robot users, e.g. in the school or workplace, between carers and the vulnerable, elderly and disabled and/or in institutions which facilitate intimate relations of care.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 140003
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": "School for Policy Studies"
            }
          ],
          "personId": 139736
        }
      ]
    },
    {
      "id": 140143,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Workspace Optimization Techniques to Improve Prediction of Human Motion During Human-Robot Collaboration",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1527",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "Understanding human intentions is critical for safe and effective human-robot collaboration. While state of the art methods for human goal prediction utilize learned models to account for the uncertainty of human motion data, that data is inherently stochastic and high variance, hindering those models' utility for interactions requiring coordination, including safety-critical or close-proximity tasks. Our key insight is that robot teammates can deliberately configure shared workspaces prior to interaction in order to reduce the variance in human motion, realizing classifier-agnostic improvements in goal prediction. In this work, we present an algorithmic approach for a robot to arrange physical objects and project \"virtual obstacles\" using augmented reality in shared human-robot workspaces, optimizing for human legibility over a given set of tasks. We compare our approach against other workspace arrangement strategies using two human-subjects studies, one in a virtual 2D navigation domain and the other in a live tabletop manipulation domain involving a robotic manipulator arm. We evaluate the accuracy of human motion prediction models learned from each condition, demonstrating that our workspace optimization technique with virtual obstacles leads to higher robot prediction accuracy using less training data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 139856
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140077
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 139966
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": ""
            }
          ],
          "personId": 139859
        }
      ]
    },
    {
      "id": 140144,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Intentional User Adaptation to Shared Control Assistance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1404",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151400
      ],
      "eventIds": [],
      "abstract": "Shared control approaches to robot assistance typically assume that user behavior remains the same despite the addition of the the assistance and rely upon this assumption to infer user goals. However, HRI research consistently shows that users are highly sensitive to changes in robot performance, which contradicts this assumption. In this paper, we show that users, in fact, change their control behavior in the presence of assistance and describe these changes as intentional adaptations to the new system dynamics. We present two user studies in which participants controlled robots with various levels of assistance. In a computer-based study, participants report changing their strategies as the assistance changes, and the amount of change in the direction of their control significantly differs between assistance conditions. In an in-the-wild robot study, participants teleoperated a robot to pick up a cup despite the presence of \"assistance\" that drives the system towards nonexistent goals and away from the true goals of the task. The ability of participants to overcome the assistance and still achieve the goal further demonstrates that users can change their behavior to account for the novel dynamics. These results motivate further research in user-centered design and evaluation of assistive systems that treat the user as intentional.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 139977
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139974
        }
      ]
    },
    {
      "id": 140145,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Towards Collaborative Crash Cart Robots that Support Clinical Teamwork",
      "award": "HONORABLE_MENTION",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1241",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "Healthcare workers (HCWs) face many challenges during bedside care that impede team collaboration and often lead to poor patient outcomes. Robots have the potential to support medical decision-making, help identify medical errors, and deliver supplies to clinical teams in a timely manner. However, there is a lack of knowledge about using robots to support clinical team dynamics despite being used in surgery, healthcare operations, and other applications. To address this gap, we engaged in a co-design process of robots that support clinical teamwork. We collaboratively explore how robots can support clinical teamwork with HCWs.  This collaborative process includes understanding the challenges they face during bedside care and envisioning robots that can help mitigate these issues. Our study shows that robots can act as a shared mental model for clinical teams, help close communication gaps, and provide procedural steps to assist HCWs with limited in-hospital experience. This research highlights new ways HRI researchers can deploy robots in acute care settings, as well as define appropriate levels of autonomy to maintain human control in safety-critical settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 139802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 139995
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University",
              "dsl": "Media and Information"
            }
          ],
          "personId": 139890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University",
              "dsl": "Media & Information"
            }
          ],
          "personId": 139777
        }
      ]
    },
    {
      "id": 140146,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Generative Expressive Robot Behaviors using Large Language Models",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1164",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151389
      ],
      "eventIds": [],
      "abstract": "People employ expressive behaviors to effectively communicate and coordinate their actions with others, e.g., nodding to acknowledge a person glancing at them or saying \"excuse me'' to pass people in a busy corridor. We would like robots to also demonstrate expressive behaviors in human-robot interaction. Prior work proposes rule-based methods that struggle to scale to new communication modalities or social situations, while data-driven methods require specialized datasets for each social situation the robot is used in. We propose to leverage the rich social context available from large language models (LLMs) and their ability to generate motion based on instructions or user preferences, to generate expressive robot motion that is daptable and composable, building upon each other. Our framework utilizes few-shot chain-of-thought prompting to translate human language instructions into parametrized control code using the robot's available and learned skills. Through user studies and simulation experiments, we demonstrate that our approach produces behaviors that are equivalent to or better than professionally animated expressive behaviors.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 139925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 139785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 140067
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 139937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 139816
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 140007
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Cruz",
              "institution": "Hoku Labs",
              "dsl": ""
            }
          ],
          "personId": 139760
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Deepmind",
              "dsl": "Robotics"
            }
          ],
          "personId": 139762
        }
      ]
    },
    {
      "id": 140147,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Alchemist: LLM-Aided End-User Development of Robot Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1480",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151389
      ],
      "eventIds": [],
      "abstract": "Large Language Models (LLMs) have the potential to catalyze a paradigm shift in end-user robot programming-moving from the conventional process of user specifying programming logic to an iterative, collaborative process in which the user specifies desired program outcomes while LLM produces detailed specifications.We introduce a novel integrated development system, Alchemist, that leverages LLMs to empower end-users in creating, testing, and running robot programs using natural language inputs, aiming to reduce the required knowledge for developing robot applications.We present a detailed examination of our system design and provide an exploratory study involving true end-users to assess capabilities, usability, and limitations of our system.Through the design, development, and evaluation of our system, we derive a set of lessons learned from the use of LLMs in robot programming.We discuss how LLMs may be the next frontier for democratizing end-user development of robot applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "LCSR"
            }
          ],
          "personId": 139993
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139973
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139963
        }
      ]
    },
    {
      "id": 140148,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "I Need to Pass Through! Understandable Robot Behavior for Passing Interaction in Narrow Environment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1281",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151399
      ],
      "eventIds": [],
      "abstract": "We developed a motion control algorithm for a social mobile robot to intuitively convey its intent to pass through aisles and avoid misunderstanding in passing interactions with people, which frequently occur when a robot navigates in narrow shared environments. \r\nInspired by observations of human behavior, the proposed algorithm estimates the extent to which a person understands the robot’s intent on the basis of the person’s reactions to the oncoming robot and provides the robot with corresponding motion strategies for effective passing interactions.\r\nWe implemented the proposed algorithm onto an omni-directional humanoid robot and conducted a field study over six days in a store with 70 cm wide narrow aisles.\r\nThe resulting behaviors of 50 customers demonstrated that our proposed method provided people with a clearer understanding of the robot's intent in passing interactions, and thus the robot had more opportunity (73.1%) to pass through aisles compared to 16.7% if the robot moved and then waited for people to make space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139903
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139909
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "ATR",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140149,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Interactive Human-Robot Teaching Recovers and Builds Trust, Even With Imperfect Learners",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1205",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "Building and maintaining trust is critical for continued human-robot teaching and the prospect of robot learning social skills from natural environments. Whereas previous work often explored strategies to reduce system errors, mitigate trust loss, or using enhanced interactivity to induce learning success, few studies have investigated the possible benefits of fully engaged, interactive teaching on human trust. Motivated by the discrepancy discovered from a pair of previous investigations, the studies presented in this paper for the first time directly tested the causal impact of interactivity on the loss and recovery of trust in a human-robot social skills training context. Using a novel paradigm, we experimentally manipulated the mode of interaction that participants were able to engage in (robot supervisor or active teacher) and measured the teachers' changing trust in a 15-trial robot training session, centering on the critical social skills of norm-appropriate behavior. Our results demonstrate that interactive teachers were more resilient to initial trust loss, showed increased reliance from baseline to post-training, and attributed more of the robot's improvement to themselves than did supervisors, even when their robots were imperfectly slower learners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": ""
            }
          ],
          "personId": 139778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Cognitive, Linguistic, and Psychological Sciences"
            }
          ],
          "personId": 139958
        }
      ]
    },
    {
      "id": 140150,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Back to School - Sustaining Recurring Child-Robot Educational Interactions After a Long Break",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1326",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Maintaining the child-robot relationship after a significant break, such as a holiday, is an important step for developing sustainable social robots for education. We ran a four-session user study (n = 113 children) that included a nine-month break between the third and fourth session. During the study, participants practiced math with the help of a social robot math tutor. We found that social personalization is an effective strategy to better sustain the child-robot relationship than the absence of social personalization. To become reacquainted after the long break, the robot summarizes a few pieces of information it had stored about the child. This gives children a feeling of being remembered, which is a key contributor to the effectiveness of social personalization. Enabling the robot to refer to information previously shared by the child is another key contributor to social personalization. Conditional for its effectiveness, however, is that children notice these memory references. Finally, although we found that children's interest in the tutoring content is related to relationship formation, personalizing the topics did not lead to more interest in the content. It seems likely that not all of the memory information that was used to personalize the content was up-to-date or socially relevant.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": "Social AI"
            }
          ],
          "personId": 140046
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "University of Applied Sciences Utrecht",
              "dsl": "Research Group Human Experience & Media Design (HEMD)"
            }
          ],
          "personId": 140012
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "NH",
              "city": "Amsterdam",
              "institution": "Amsterdam University of Applied Sciences",
              "dsl": "Digital Life"
            }
          ],
          "personId": 139947
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Amsterdam University of Applied Sciences",
              "dsl": "Digital Life"
            }
          ],
          "personId": 139929
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "University of Applied Sciences Utrecht",
              "dsl": ""
            }
          ],
          "personId": 140020
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "HU University of Applied Sciences Utrecht",
              "dsl": "Institute for ICT"
            }
          ],
          "personId": 139918
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU University",
              "dsl": ""
            }
          ],
          "personId": 139779
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Amsterdam University of Applied Sciences",
              "dsl": "Digital Life"
            }
          ],
          "personId": 139910
        }
      ]
    },
    {
      "id": 140151,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Are You Sure? - Multi-Modal Human Decision Uncertainty Detection in Human-Robot-Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1127",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "In a question-and-answer setting, the respondent is often not only communicating the requested information but also indicating their confidence in the answer through various behavioral cues. Humans excel at interpreting these cues and monitoring the uncertainty of other persons. Being able to detect human uncertainty in human-robot interactions in a similar way can enable future robotic systems to better recognize uncertain and error-prone human input. Additionally, automatic human uncertainty detection can enhance the responsiveness of robots to the user in moments of uncertainty by providing help or clarification. While there is some work on uncertainty detection based on a single modality, only a few works focus on multi-modal uncertainty detection. Even fewer works explore how human uncertainty manifests through behavioral cues in human-robot interactions. In this work, we analyze occurrences of behavioral cues related to self-reported uncertainty on experimental data from 27 participants across two decision-making tasks. Additionally, in the first task, we varied if participants interacted with a human or a robot. On the recorded data, we extract features accessible via a webcam and a microphone and train a multi-modal classifier. Experimental evaluation of our developed classifier shows that it significantly outperforms third-person annotators in accuracy and F1 score. Humans report feeling less observed when responding to a robot compared to a human. Nevertheless, we found that the behavioral differences did not significantly affect the performance of our proposed uncertainty classification.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139972
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 140050
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139863
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139930
        }
      ]
    },
    {
      "id": 140152,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A Taxonomy of Robot Autonomy for Human-Robot Interaction",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1204",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "Robot autonomy is a powerful and ubiquitous factor in human-robot interaction (HRI), but it is rarely discussed beyond a one-dimensional measure of the degree to which a robot operates without human intervention. As robots become more sophisticated, this simple view of autonomy could be expanded to capture the variety of autonomous behaviors robots can exhibit and to match the rich literature on human autonomy in philosophy, psychology, and other fields. In this paper, we conduct a systematic literature review of the use of robot autonomy in HRI and synthesize this with the broader literature into a taxonomy of six distinct forms of autonomy: those based on robot and human involvement at runtime (operational autonomy, intentional autonomy, shared autonomy), human involvement before runtime (non-deterministic autonomy), and expressions of autonomy at runtime (cognitive autonomy, physical autonomy). We discuss future considerations for autonomy in HRI that emerge from this process, including moral consequences, the idealization of “full” robot autonomy, and connections to agency, mind perception, and free will.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139755
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": ""
            }
          ],
          "personId": 139886
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139948
        }
      ]
    },
    {
      "id": 140153,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Autonomous Assessment of Demonstration Sufficiency via Bayesian Inverse Reinforcement Learning",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1126",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "We examine the problem of determining demonstration sufficiency: how can a robot self-assess whether it has received enough demonstrations from an expert to ensure a desired level of performance? To address this problem, we propose a novel self-assessment approach based on Bayesian inverse reinforcement learning and value-at-risk, enabling learning-from-demonstration (\"LfD\") robots to compute high-confidence bounds on their performance and use these bounds to determine when they have a sufficient number of demonstrations. We propose and evaluate two definitions of sufficiency: (1) normalized expected value difference, which measures regret with respect to the human's unobserved reward function, and (2) percent improvement over a baseline policy. We demonstrate how to formulate high-confidence bounds on both of these metrics. We evaluate our approach in simulation for both discrete and continuous state-space domains and illustrate the feasibility of developing a robotic system that can accurately evaluate demonstration sufficiency. We also show that the robot can utilize active learning in asking for demonstrations from specific states while evaluating demonstration sufficiency and that this results in fewer demos needed for the robot to still maintain high confidence in its policy. Finally, we demonstrate the viability of our approach via a user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Center for Human-Compatible Artificial Intelligence"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 139773
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah",
              "dsl": ""
            }
          ],
          "personId": 140018
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139864
        }
      ]
    },
    {
      "id": 140154,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "When Do People Want an Explanation from a Robot?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1203",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": " Explanations are a critical topic in AI and robotics, and their importance in generating trust and allowing for successful human–robot interactions has been widely recognized. However, it is still an open question when and in what interaction contexts users most want an explanation from a robot. In our pre-registered study with 186 participants, we set out to identify a set of scenarios in which users show a strong need for explanations. Participants are shown 16 videos portraying seven distinct situation types, from successful human–robot interactions to robot errors and robot inabilities. Afterwards, they are asked to indicate if and how they wish the robot to communicate subsequent to the interaction in the video.\r\n  \r\n  The results provide a set of interactions, grounded in literature and verified empirically, in which people show the need for an explanation. \r\n  Moreover, we can rank these scenarios by how strongly users think an explanation is necessary and find statistically significant differences.\r\n  Comparing giving explanations with other possible response types, such as the robot apologizing or asking for help, we find that why-explanations are always among the two highest-rated responses, with the exception of when the robot simply acts normally and successfully. This stands in stark contrast to the other possible response types that are useful in a much more restricted set of situations. Lastly, we test for factors of an individual that might influence their response preferences, for example, their general attitude towards robots, but find no significant correlations. Our results can guide roboticists in designing more user-centered and transparent interactions and let explainability researchers develop more pinpointed explanations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 140040
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 139876
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 140073
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 140061
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King’s College London",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 139772
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "King's College London",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 139998
        }
      ]
    },
    {
      "id": 140155,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Towards Balancing Preference and Performance through Adaptive Personalized Explainability",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1368",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": "As robots and digital assistants are deployed in the real world, these agents must be able to communicate their decision-making criteria to build trust, improve human-robot teaming, and enable collaboration. While the field of explainable artificial intelligence (xAI) has made great strides in building a set of mechanisms to enable such communication, these advances often assume that one approach is ideally suited to each problem (e.g., decision trees for explaining how to triage patients in an emergency or feature-importance maps for explaining radiology reports). This fails to recognize that users may have different experiences or preferences for interaction modalities. In this work, we present the design and results of two user-studies set in a simulated autonomous vehicle (AV) domain, a setting that is increasingly important to HRI. We investigate (1) population-level preferences for xAI and (2) different personalization strategies for providing robot explanations. We find significant differences between xAI modes in both preference (p < 0.01) and task-performance (p < 0.05). We also observe that a participant's preferences do not always align with their task-performance, motivating our development of an adaptive personalization strategy that balances the two. We show that this strategy leads to significant performance gains (p < 0.05), and we conclude with a discussion our findings and implications for future work in xAI.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139920
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Interactive Computing"
            }
          ],
          "personId": 139781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Robotics"
            }
          ],
          "personId": 139749
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 139968
        }
      ]
    },
    {
      "id": 140156,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Design and Evaluation of a Socially Assistive Robot Schoolwork Companion for College Students with ADHD",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1522",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151391
      ],
      "eventIds": [],
      "abstract": "Studies have shown that college students with ADHD respond positively to simple socially assistive robots (SARs) that monitor attention and provide non-verbal feedback while completing a task, but the explorations have been done only in lab settings for short sessions. This work presents an initial design and evaluation of an in-dorm socially assistive robot  study companion for college students with ADHD. This work represents the introductory stages of an ongoing user-centered, participatory design process, motivated by, and conducted through a lens of life-long assistive technology, rather than short-term interventions. In a within-subjects user study, a group of university students (N=11) with self-reported symptoms of adult ADHD were randomly assigned to have a study companion robot system or a computer-based system placed in their dorm room for a total of three weeks. With the goal of developing long-term assistive technology, we focus our analysis on: 1) evaluating the usability and desire for SAR study companions among college students with ADHD, and 2) collecting extensive feedback from participants about the design and functionality of the robot. We show that participants did find the robot useful, demonstrated by an excellent average system usability scale (SUS) score of 83.864. Furthermore, after one week of using the robot regularly, 91\\% of participants (10 out of 11) elected to continue using the study companion robot in the second week when they were not required to do so. We found that participants’ perceived usability of the robot was strongly correlated with how long they voluntarily studied with the robot.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 139835
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 139735
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Lod Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 139951
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 139884
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 140005
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 140042
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Interaction Lab"
            }
          ],
          "personId": 139751
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 139976
        }
      ]
    },
    {
      "id": 140157,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Effect of Social Robot’s Role and Behavior on Parent-Toddler Interaction",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1169",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151390
      ],
      "eventIds": [],
      "abstract": "Social robots, designed to interact with people through natural communication modes like speech, body motion, gestures, and facial expressions, have been extensively studied in child-robot interaction for educational purposes. Recently, social robots have been explored in triadic parent-child-robot interactions, showing promise due to their interactivity, computational power, and physical presence, which enable multimodal natural communication and cater to toddlers' developmental stages and physical curiosity. However, these have focused only on shared reading experiences and engaged older children, rather than toddlers.  We developed two games, one with two levels of robot scaffolding, and another with either structured or unstructured design. We then explored, in two studies, how a social robot's assigned role and behaviors influence the engagement of parents and toddlers with the robot and their interaction with each other. Our results show that parents affectively scaffolded their children less when the robot increased its scaffolding behaviors and that parents provided more scaffolding in a structured game with the robot, whereas in an unstructured game the dyad exhibited more cooperation in which children exhibited more independence. These findings can contribute to a better understanding of interaction design, triadic dynamics, and the role of the robot in parent-toddler-robot scenario.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Tel Aviv Univeristy",
              "dsl": "Curiosity Lab"
            },
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Curiosity Robotics",
              "dsl": ""
            }
          ],
          "personId": 139794
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "Curiosity Lab, Department of Industrial Engineering"
            }
          ],
          "personId": 139841
        }
      ]
    },
    {
      "id": 140158,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "\"Oh, sorry, I think I interrupted you\": Designing Repair Strategies for Robotic Longitudinal Well-being Coaching",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1245",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151398
      ],
      "eventIds": [],
      "abstract": "Robotic well-being coaches have been shown to successfully promote people’s mental well-being. In order to provide successful coaching, a robotic coach should have the capability to repair the mistakes it makes. However, past works investigating robot mistakes are limited to game or task-based, one-off and in-lab studies. This paper presents a 4-phase design process to design repair strategies for robotic longitudinal well-being coaching with the involvement of real-world stakeholders. The design process consists of the following phases: 1) designing repair strategies with a professional well-being coach; 2) undertaking a longitudinal study with the involvement of experienced users (i.e., who had already interacted with a robotic coach before) to investigate the repair strategies defined in (1); 3) conducting a design workshop with the users of the study in (2) to gather their perspectives on the robotic coach’s repair strategies; 4) discussing the results obtained in (2) and (3) with the mental well-being professional to reflect on how to design repair strategies for robotic coaching. Our results show that users have different expectations for a robotic coach than a human coach, which influences how repair strategies should be designed. We show that different repair strategies (e.g., apologizing, explaining, or repairing empathically) are appropriate in different scenarios, and that preferences for repair strategies change during longitudinal, repeated interactions with the robotic coach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 139866
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge ",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer & Technology "
            }
          ],
          "personId": 139932
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Computer Lab"
            }
          ],
          "personId": 139889
        }
      ]
    },
    {
      "id": 140159,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Towards human-like handover timing performance with legged manipulators",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1168",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151401
      ],
      "eventIds": [],
      "abstract": "Deploying perception modules for human-robot handovers is challenging because they require a high degree of reactivity, generalizability, and robustness to work reliably for a diversity of objects. Further complications arise as each object can be handed over in a variety of ways, that can cause occlusions and viewpoint changes. On legged robots, deployment is particularly challenging because of the limited computational resources and the additional image-space noise resulting from locomotion.\r\n\r\nIn this paper, we introduce an efficient and object-agnostic real-time tracking framework, specifically designed for handover tasks between a human and a legged manipulator. The proposed method combines fast optical flow with Siamese-network-based tracking and depth segmentation in an adaptive Kalman Filter framework. \r\nWe show that we outperform the state-of-the-art for tracking during human-robot handovers with our legged manipulator system. We demonstrate the generalizability, reactivity, and robustness of our system through experiments in different handover scenarios and by carrying out a user study. Furthermore, as timing has been proven to be more important than spatial accuracy in human-robot interaction tasks, we show that we reach close to human timing performance not only in terms of objective metrics, such as handover and reaction time but also by considering subjective metrics gathered from the participants in the user study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Zurich",
              "city": "Zurich",
              "institution": "ETH Zuch",
              "dsl": "RSL"
            }
          ],
          "personId": 139842
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Enchanted Tools",
              "dsl": ""
            }
          ],
          "personId": 140035
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            }
          ],
          "personId": 140014
        }
      ]
    },
    {
      "id": 140160,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Human perception of swarm fragmentation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1047",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151392
      ],
      "eventIds": [],
      "abstract": "In the context of robot swarms, fragmentation refers to a breakdown in communication and coordination among the robots. This fragmentation can lead to issues in the swarm self-organisation, especially the loss of efficiency or an inability to perform their tasks. Human operators influencing the swarm could prevent fragmentation. To help them in this task, it is necessary to study the ability of humans to perceive and anticipate fragmentation. This article studies the perception of different types of fragmentation occurring in swarms depending on their behaviour selected amongst swarming, flocking, expansion and densification. Thus, we characterise human perception thanks to two metrics based on the distance separating fragmented groups and the separation speed. The experimentation protocol consists of a binary discrimination task in which participants have to assess the presence of fragmentation. The results show that detecting fragmentation for expansion behaviour and anticipating fragmentation, in general, are challenging. Moreover, they show that humans rely on separation distance and speed to infer the presence or absence of fragmentation. Our study paves the way for new research that will provide information to humans to better anticipate and efficiently prevent the occurrence of swarm fragmentation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Univ Brest, CNRS, Lab-STICC",
              "dsl": ""
            }
          ],
          "personId": 139965
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Lab-STICC",
              "dsl": "IMT Atlantique"
            }
          ],
          "personId": 139809
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "Univ Brest, CNRS, Lab-STICC",
              "dsl": ""
            }
          ],
          "personId": 139872
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "ENIB, CNRS, Lab-STICC",
              "dsl": ""
            }
          ],
          "personId": 140000
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Brest",
              "institution": "IMT Atlantique, CNRS, Lab-STICC",
              "dsl": ""
            }
          ],
          "personId": 139896
        }
      ]
    },
    {
      "id": 140161,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Modelling Experts' Sampling Strategy to Balance Multiple Objectives During Scientific Explorations",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1486",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "During scientific explorations, scientists often hold multiple and often conflicting objectives. Understanding how scientists prioritize and balance these objectives is crucial for developing cognitively-compatible robotic teammates and fostering effective human-robot collaboration. In this study, we seek to improve the cognitive compatibility of robotic algorithms by modelling human' decision making processes under multiple objectives. Collected human decision data from 141 sampling steps indicate that the majority of scientists adopt one of the following objective balancing strategies: (i) A Focus mode, where experts select sampling location to primarily optimize their primary objective; (ii) A Hierarchy mode, where experts hierarchically satisfy foremost their primary objective, then, to a lesser extent, their secondary objective; and (iii) A Trade-off mode, where experts select sampling locations to satisfy all objectives, even the location was not ideal for either objective. To understand how experts choose among the different modes, we quantitatively characterize the three types of strategies, by representing the decision data from each sampling step in an objective function space. Analysis of the strategy types reveals that, experts' adaptation of multi-objective coordinating strategies are primarily governed by two key decision factors: current stages of sampling, and outstanding reward values. This discovery allows the robot to use an extremely simple decision algorithm to connect experts' high-level objectives to desired sampling locations when balancing multiple objectives. Deployment of this algorithm at a planetary-analogue field exploration mission on Mt. Hood demonstrates the potential for robots to use cognitively-compatible algorithms to participate in decision making and aid with the adaptation of sampling plans that align with scientists' high-level goals.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Ming Hsieh Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 140015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 139793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Research in Applied Decisions"
            }
          ],
          "personId": 139900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": "Ming Hsieh Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 139971
        }
      ]
    },
    {
      "id": 140162,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Enhancing Safety in Learning from Demonstration Algorithms via Control Barrier Function Shielding",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1045",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151396
      ],
      "eventIds": [],
      "abstract": "Learning from Demonstration (LfD) is a powerful method for non-roboticists end-users to teach robots new tasks, enabling them to customize the robot behavior. However, modern LfD techniques do not explicitly synthesize safe robot behavior, which limits the deployability of these approaches in the real world. To enforce safety in LfD without relying on experts, we propose a new framework, ShiElding with Control barrier fUnctions in inverse REinforcement learning (SECURE), which learns a customized Control Barrier Function (CBF) from end-users that prevents robots from taking unsafe actions while imposing little interference with the task completion. We evaluate SECURE in three sets of experiments. First, we empirically validate SECURE learns a high-quality CBF from demonstrations and outperforms conventional LfD methods on simulated robotic and autonomous driving tasks with improvements on safety by up to 100%. Second, we demonstrate that roboticists can leverage SECURE to outperform conventional LfD approaches on a real-world knife-cutting, meal-preparation task by 12.5% in task completion while driving the number of safety violations to zero. Finally, we demonstrate in a user study that non-roboticists can use SECURE to effectively teach the robot safe policies that avoid collisions with the person and prevent coffee from spilling.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139812
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139821
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139765
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139892
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 139968
        }
      ]
    },
    {
      "id": 140163,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "What is your other hand doing, robot? A model of behavior for shopkeeper robot's idle hand",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1320",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "In retail settings, a robot's one-handed manipulation of objects can come across as thoughtless and impolite, thus creating a negative customer experience. To solve this problem, we first observed how human shopkeepers interact with customers, specifically focusing on their hand movements during object manipulation. From the observation and analysis of shopkeepers' hand movements, we identified an essential element of their idle hand movements: \"support\" provided by the idle hand as the primary hand manipulates an object. Based on this observation, we proposed a model that coordinates the movements of a robot's idle hand with its primary task-engaged hand, emphasizing its supportive behaviors. In a within-subjects study, 20 participants interacted with robot shopkeepers under different conditions to assess the impact of incorporating support behavior with the idle hand. The results show that the proposed model significantly outperforms a baseline in terms of politeness and competence, suggesting enhanced object-based interactions between the robot shopkeepers and customers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 140051
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 140039
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139788
        }
      ]
    },
    {
      "id": 140164,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A Virtual Reality Framework for Human-Driver Interaction Research: Safe and Cost-Effective Data Collection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1286",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151395
      ],
      "eventIds": [],
      "abstract": "The advancement of automated driving technology has led to new challenges in the interaction between automated vehicles and human road users. However, there is currently no complete theory that explains how human road users interact with vehicles, and studying them in real-world settings is often unsafe and time-consuming. This study proposes a 3D Virtual Reality (VR) framework for studying how pedestrians interact with human-driven vehicles and autonomous vehicles. The framework uses VR technology to collect data in a safe and cost-effective way, and deep learning methods are used to predict pedestrian trajectories. Specifically, graph neural networks have been used to model pedestrian future trajectories and the probability of crossing the road. The results of this study show that the proposed framework can be for collecting high-quality data on pedestrian-vehicle interactions in a safe and efficient manner. The data can then be used to develop new theories of human-vehicle interaction and to train autonomous vehicles to better interact with pedestrians.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Tyne and Wear",
              "city": "Newcastle upon Tyne",
              "institution": "Northumbria Unviersity",
              "dsl": "Computer and Information Sciences"
            }
          ],
          "personId": 139961
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": ""
            }
          ],
          "personId": 140056
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "Glasgow University",
              "dsl": ""
            }
          ],
          "personId": 139748
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Belfast",
              "institution": "Queen's University Belfast",
              "dsl": ""
            }
          ],
          "personId": 139858
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Belfast",
              "institution": "Queen's University Belfast",
              "dsl": ""
            }
          ],
          "personId": 139927
        }
      ]
    },
    {
      "id": 140165,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "User-Designed Human-UAV Interaction in a Social Indoor Environment",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24e-1484",
      "source": "PCS",
      "trackId": 12525,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        151397
      ],
      "eventIds": [],
      "abstract": "The purpose of this project is to understand how people would expect to interact with an Unmanned Aerial Vehicle (UAV) in a social indoor environment under friendly, neutral, or adversarial contexts. The three environments will include one setting with the UAV serving as a tour guide, one as a security guard, and one as a food delivery mechanism. This work is novel in its inquiry into the affective nature of the interaction, comparison across situational contexts, and ability to compare preferences both within and between participants. Our findings will help researchers plan for appropriate safety and comfort measures, while being cognizant of the participants’ preferences for and understanding of how drones operate. This study examines realistic indoor scenarios for which each participant designs their preferred interaction and presents exploratory results, including comparison to prior work with respect to motion gestures and comfortable approach distances. Initial findings suggest the importance of visibility of approaches, selecting approach heights relative to the person and based on the context of interaction, and criticality of the initial direction of motion when classifying the communicative content of UAV flight paths.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Lincoln",
              "institution": "University of Nebraska-Lincoln",
              "dsl": "School of Computing"
            }
          ],
          "personId": 140059
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Lincoln",
              "institution": "University of Nebraska-Lincoln",
              "dsl": "School of Computing"
            }
          ],
          "personId": 140074
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Nebraska",
              "city": "Lincoln",
              "institution": "University of Nebraska-Lincoln",
              "dsl": "School of Computing"
            }
          ],
          "personId": 139933
        }
      ]
    },
    {
      "id": 152384,
      "typeId": 13321,
      "title": "Beyond the Black Box: Human Robot Interaction through Human Robot Performances",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1303",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Robots are increasingly becoming visible in our lives while AI technology is going through unprecedented transformations. This is such an important moment to pose questions - how do the particular features of these robot performers — their body morphology and physical design as well as the design of their interactions — shape their modes of relating and correspondingly structure how we (humans) might relate to them? and if/how do human interaction and experience with robots contribute to the changes in human feelings\r\nand perception toward robots? This paper explores the possibility of studying human-robot interactions through the form of human-robot performances. Beyond the Black Box project - the human-robot performance - is the intervention method to observe/study the changes in human perceptions/feelings towards robots, developed from a previous work, Dance with Robots. Through this work, the authors witnessed how the human performer’s perception of and feeling towards robot performers have changed over time through the rehearsals and performances. Inspired by this, this paper introduces the research idea of studying the human-robot interaction through the form of robot-human performance",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Department of Education Studies"
            }
          ],
          "personId": 152223
        }
      ]
    },
    {
      "id": 152385,
      "typeId": 13321,
      "title": "Exploring a Japanese Cooking Database: A robot uses GenAI and a knowledge graph to chat about culinary delights",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1304",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "The paper describes ongoing work applying Generative AI to a real world application. We use Retrieval Augmented Generation and other GenAI tools that combine large language models with knowledge graphs. These tools help a robot to chat in English about Japanese cooking using a knowledge base that is in Japanese.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of Advanced Industrial Science and Technology",
              "dsl": ""
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": ""
            }
          ],
          "personId": 151654
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "CDM Interact Oy",
              "dsl": ""
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": ""
            }
          ],
          "personId": 151833
        }
      ]
    },
    {
      "id": 152386,
      "typeId": 13322,
      "title": "Social Signal Modeling in Human-Robot Interaction",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1032",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152772
      ],
      "eventIds": [],
      "abstract": "This workshop focuses on the understanding and modeling of social signals to create human-aware HRI. The three fundamental themes are: understanding social signals (gain insights into human internal states), modeling social signals for the generation of a human's mental state (translating social signals into actionable computational models), and operationalizing human models for human-aware applications (integrating these cognitive models into robotic systems to develop new human-aware capabilities). The invited speakers, paper presentations, and discussions will aim to focus on the social science background of social signals, acquisition and availability of benchmarking datasets, social signal modeling techniques, integration of models into real-time systems, usage of these models---such as error management, personalization, and mental model alignment---and applications of these models (i.e., healthcare, education, manufacturing). We expect these topics to demonstrate how modeling social signals, both explicit and implicit, is necessary for fluent, intuitive and trustworthy interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152121
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Milan",
              "institution": "Politecnico di Milano",
              "dsl": "Department of Electronics, Information, and Bioengineering"
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Milan",
              "institution": "Politecnico di Milano",
              "dsl": "Department of Electronics, Information, and Bioengineering"
            }
          ],
          "personId": 139932
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 139889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139963
        }
      ]
    },
    {
      "id": 152387,
      "typeId": 13322,
      "title": "Causal-HRI: Causal Learning for Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1031",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152744
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 152105
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 151489
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 152020
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "The Robotics Institute"
            }
          ],
          "personId": 151600
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 139782
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 139889
        }
      ]
    },
    {
      "id": 152388,
      "typeId": 13321,
      "title": "AR-STAR: An Augmented Reality Tool for Online Modification of Robot Point Cloud Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1301",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Robotic solutions are being deployed into the real world to perform increasingly-challenging tasks across many domains. In these domains, robots can encounter uncertainties that prevent task completion. Supervised autonomy can assist in negotiating these challenging situations. Augmented Reality (AR) allows visualization of complex data in situ in an intuitive manner for non-experts. To fill this need, we introduce a comprehensive AR application module called Situational Task Accept and Repair (STAR). STAR is evaluated in the context of industrial repair of corrosion and allows users to examine identified corrosion images, point cloud data, and robot navigation objectives superimposed on the physical environment. Users are able to additionally make adjustments to the robot repair plan in real-time using three modalities to add or subtract point cloud data. This is demonstrated using an AR device with a mobile manipulator. Pilot studies are presented to evaluate user preferences of the AR interaction modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 151477
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 151730
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": ""
            }
          ],
          "personId": 151692
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 152099
        }
      ]
    },
    {
      "id": 152389,
      "typeId": 13322,
      "title": "Scarecrows in Oz: Large Language Models in HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1033",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152770
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "UMBC",
              "dsl": "Interactive Robotics and Language Lab"
            }
          ],
          "personId": 151466
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Independent",
              "dsl": ""
            }
          ],
          "personId": 151625
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Semio",
              "dsl": ""
            }
          ],
          "personId": 151499
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland, Baltimore County",
              "dsl": "CARDS"
            }
          ],
          "personId": 139804
        }
      ]
    },
    {
      "id": 152390,
      "typeId": 13322,
      "durationOverride": 60,
      "title": "Assistive Applications, Accessibility, and Disability Ethics in HRI",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1037",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152740,
        152742
      ],
      "eventIds": [],
      "abstract": "This full-day workshop addresses the problems of accessibility in HRI and the interplay of ethical considerations for disability-centered design and research, accessibility concerns for disabled researchers, and the design of assistive HRI technologies. We invite authors to submit extended abstracts (up to 2 pages, excluding references) and short papers (up to 4 pages, excluding references) on a range of topics relevant to ethics, accessibility, and assistive applications\r\nin HRI, including critical reflections on methodologies, design papers on human-centered or anti-ableist assistive technology, and papers from those outside the HRI community who may have insight to share on these concerns.\r\nThe workshop will use a hybrid format to allow participants who due to disability, geographic, financial, or other constraints, are unable to travel, and will feature keynote speakers, panel discussions, and breakout sessions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 151781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": ""
            }
          ],
          "personId": 139977
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139931
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Interactive Intelligence"
            }
          ],
          "personId": 151549
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "HARP Lab, Robotics Institute"
            }
          ],
          "personId": 152169
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 151453
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University",
              "dsl": "Media & Information"
            }
          ],
          "personId": 139777
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 139946
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 140003
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139974
        }
      ]
    },
    {
      "id": 152391,
      "typeId": 13322,
      "title": "First International Workshop on Worker-Robot Relationships. Exploring Transdisciplinarity for the Future of Work with Robots",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1039",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152754
      ],
      "eventIds": [],
      "abstract": "In Industry 5.0, cognitive robots and workers will engage in evolving and reciprocal relations, which we call worker-robot relationships (WRRs). To enable evidence-based work futures with workers, we must co-develop WRRs and understand their impact on work, workers, management, and society. To this end, we posit that the HRI field should work beyond disciplines and include value-driven and plural perspectives through transdisciplinary research done with and for workers.\r\nHowever, WRRs and transdisciplinarity pose unique technical, design, and methodological challenges yet to be explored. We propose a workshop to engage the HRI community working on Industry 5.0, aiming at 1) taking stock of current WRR-related challenges in relevant disciplines, 2) collectively kick-off the exploration of a joint research agenda, 3) preliminary examining if and how transdisciplinarity could help the HRI community, and 4) start discussing how to deal with such complex knowledge integration in practice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "NY",
              "city": "Enschede",
              "institution": "University of Twente",
              "dsl": "Human-Centred Design"
            }
          ],
          "personId": 151746
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": ""
            }
          ],
          "personId": 152330
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "Cognitive Robotics - HRI"
            }
          ],
          "personId": 152064
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Zuid Holland",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "Human Centred Design"
            }
          ],
          "personId": 151787
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Overijssel",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Delft Haptics Lab"
            }
          ],
          "personId": 152248
        }
      ]
    },
    {
      "id": 152392,
      "typeId": 13321,
      "title": "LaSofa: Integrating Fantasy Storytelling in Human-Robot Interaction through an Interactive Sofa Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1306",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This paper presents LaSofa, an avant-garde interactive sofa that revolutionizes the concept of human-furniture interaction by integrating fantasy storytelling with conventional furniture design. LaSofa is equipped with pressure sensors that recognize user interactions, which in turn trigger character dialogues and adventure narratives set in an embedded fantasy world. These narratives are dynamically generated by advanced large language models (LLMs) and are delivered through state-of-the-art audio technology to create an enveloping auditory experience. This integration not only embodies the convergence of technology and storytelling within the realm of furniture but also marks a pioneering venture into augmenting human experiences through interactive design. The paper elaborates on the genesis, architecture, and prospective influence of LaSofa in the domains of interactive storytelling and innovative furniture design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151607
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "School of Architecture"
            }
          ],
          "personId": 151449
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151561
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151494
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152109
        }
      ]
    },
    {
      "id": 152393,
      "typeId": 13321,
      "title": "How do you like me NAO? Investigating the Privacy and Trust Implications of Interacting with Social Companion Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1309",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Social robots are a type of robotics that focuses on creating intelligent and embodied machines capable of interacting and communicating with humans in a socially acceptable\r\nmanner. However, these robots’ potential to capture user data, including emotions, biometrics, and behavioural habits, raises significant privacy concerns that could influence users’ intention to use and trust social robots. Therefore, there is a pressing need to synthesize a privacy model that helps unravel the complex behavioural processes underlying current and future HRI technologies. This work aims to contribute\r\nto the growing body of privacy-friendly robot design by proposing comprehensive guidelines that enable the development of trustworthy and transparent social robots that\r\nrespect user privacy. We have established a set of theoretical constructs to address people’s concerns regarding privacy across four dimensions: physical, informational, psychological, and social. Finally, recommendations are provided in each construct to enhance transparency and trust through compliance with laws like GDPR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151756
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151491
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 152132
        }
      ]
    },
    {
      "id": 152394,
      "typeId": 13321,
      "title": "Bridging Human-Robot Connections: Perceived Impression of non-verbal Social Cue Mirroring by Humanoid Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1313",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Mirroring non-verbal social cues such as affect or movement can enhance human-human and human-robot interactions in the real world. The robotic platforms and control methods also impact people's perception of human-robot interaction. However, limited studies have compared robot imitation across different platforms and control methods. Our research addresses this gap by conducting two experiments comparing people's perception of affective mirroring between the iCub and Pepper robots and movement mirroring between vision-based iCub control and Inertial Measurement Unit (IMU)-based iCub control. We discovered that the iCub robot was perceived as more humanlike than the Pepper robot when mirroring affect, while a vision-based controlled iCub outperformed the IMU-based controlled one in the movement mirroring task. Our findings suggest that different robotic platforms impact people's perception of robots' mirroring during  HRI. The control method also contributes to the robot's mirroring performance. Our work sheds light on the design and application of different humanoid robots in the real world.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151525
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of informatics"
            }
          ],
          "personId": 152055
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 152106
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Hambrug",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Knowledge Technology Group"
            }
          ],
          "personId": 151636
        }
      ]
    },
    {
      "id": 152395,
      "typeId": 13321,
      "title": "Exploring Foot-Interactive Robotics: A Study on Gobot's Role in Enhancing Daily Walking Experience through Emotion-Infused Design",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1315",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study examines the development and impact of Gobot, an innovative foot-interactive robot, in augmenting the daily walking experience through an emotion-infused design approach. Unlike traditional smart shoes with a primary focus on physical health metrics, Gobot integrates emotional interaction as a core component, aiming to foster a deeper, empathetic connection with users. It features an intuitive design that expresses four distinct emotional states (nervous, happy, angry, and normal) through dynamic adjustments in shoelace tension and changes in light color. This design paradigm not only adds a layer of interactivity to routine walking but also enhances the user's emotional engagement with their environment. The paper delves into Gobot's unique method of tactile interaction tailored for foot-based robotics, exploring its potential in broadening the scope and application of such technology. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151624
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152125
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "tsinghua university",
              "dsl": ""
            }
          ],
          "personId": 151682
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "tsinghua university",
              "dsl": "tsinghua university"
            }
          ],
          "personId": 151514
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151668
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151891
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151895
        }
      ]
    },
    {
      "id": 152396,
      "typeId": 13322,
      "title": "Workshop YOUR Study Design 2024! Participatory Critique and Refinement of Participants' Studies",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1041",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152778
      ],
      "eventIds": [],
      "abstract": "A well-structured study underscores the significance of a research idea. However, designing an effective user study can be daunting for novice Human-Robot Interaction (HRI) researchers. \"Workshop Your Study Design\" seeks to address this gap in knowledge by offering bespoke feedback from seasoned HRI experts. Attendees will have the opportunity to hone their user study blueprints, fine-tune their anticipated analyses, and delve into new research topics during two dedicated sessions with esteemed HRI specialists. Thus, the workshop invites a 2-4 page brief overview of their planned user study, focusing on the methodology and analysis. The workshop promotes active participation, with shot mentor-driven talks on pivotal topics relevant to study design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Stuttgart",
              "institution": "Max Planck Institute for Intelligent Systems",
              "dsl": "Haptic Intelligence Department"
            }
          ],
          "personId": 152311
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Técnico",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 152047
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 152360
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 151795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151733
        }
      ]
    },
    {
      "id": 152397,
      "typeId": 13321,
      "title": "You Look Nice, but I Am Here to Negotiate: The Influence of Robot Appearance on Negotiation Dynamics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1316",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This report presents two experimental studies examining whether relatively subtle differences in the appearances of humanoid robots impact (1) the outcomes of human-robot negotiation (i.e., utility scores) and (2) the participant's attitudes toward their robot negotiation partner. Study I compared Nao and Pepper, and Study II compared Nao and QT in identical negotiation settings. While the appearance of robots influenced the participant's attitudes toward the robot before and after the negotiation, such differences were not manifested in the utility scores. The consistent utility scores across different robots reassure that minor variations in the visual characteristics of robots do not alter how users negotiate with a robot. Yet, as participants felt differently about the three robots, there remains the possibility that the differences in their appearances may influence the user's initial inclination to approach each robot. As among the first to systematically investigate the influence of robot appearance on human-robot negotiations, this study emphasizes the importance of assessing both objective outcome scores and the subjective experience of the user in human-robot interaction (HRI) research and offers valuable insights for designing and implementing social robots in real-world settings including customer service and other AI-based interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Ozyegin University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152362
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Sabancı University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 151613
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "İstanbul",
              "institution": "Sabancı University",
              "dsl": ""
            }
          ],
          "personId": 152240
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Özyeğin University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152160
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "Tuzla/Istanbul",
              "institution": "Sabanci University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 151828
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "--",
              "city": "İstanbul",
              "institution": "Özyeğin University",
              "dsl": "Computer Science"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Interactive Intelligence Group"
            }
          ],
          "personId": 151769
        }
      ]
    },
    {
      "id": 152398,
      "typeId": 13322,
      "title": "Tutorial on Movement Notation: An Interdisciplinary Methodology for HRI to Reveal the Bodily Expression of Human Counterparts via Collecting Annotations from Dancers in a Shared Data Repository",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1040",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152736
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "The Robotics, Automation, and Dance (RAD) Lab",
              "dsl": ""
            }
          ],
          "personId": 152107
        }
      ]
    },
    {
      "id": 152399,
      "typeId": 13322,
      "title": "Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1043",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152714
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Instutue of Technology",
              "dsl": "Robotics, Perception and Learning"
            }
          ],
          "personId": 152134
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Inclusive Human-Robot Interaction"
            },
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 152338
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139969
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140077
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Dept of Computer Science"
            }
          ],
          "personId": 151437
        }
      ]
    },
    {
      "id": 152400,
      "typeId": 13322,
      "title": "Human -- Large Language Model Interaction: The dawn of a new era or the end of it all?",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1042",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152756
      ],
      "eventIds": [],
      "abstract": "The Human - Large Language Model Interaction workshop aims at bringing together researchers and industry experts across all disciplines in the fields of Human-Robot Interaction and Artificial Intelligence for an interactive and interdisciplinary discussion around the enormous opportunities and challenges that emerge from integrating Large Language Models in the interactive, conversational and reasoning abilities of robots. We will discuss the impact that these models will increasingly have on the way that humans and robots interact by focusing on the limitations, challenges, risks and opportunities that such models represent and how can they best be addressed by the HRI community. \r\nThe workshop aims to provide an important venue to encourage debate around issues concerning the deployment of Large Language Models empowered solutions for human-robot interaction. This will be an opportunity to share and discuss ideas, worries, strategies, insights and findings around the application of Large Language Models in interaction scenarios.  \r\nWe hope these discussions will help to inform and guide the community towards the design, development and implementation of safe, ethical, and responsible `new breed' of social robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151804
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152038
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151683
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Midlothian",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Interaction Lab, School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151854
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151850
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "County (optional)",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152298
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152072
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Scotland",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151885
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt University",
              "dsl": "Mathematics and Computer Science"
            }
          ],
          "personId": 151925
        }
      ]
    },
    {
      "id": 152401,
      "typeId": 13321,
      "title": "Towards understanding the entanglement of human stereotypes and system biases in Human--Robot interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1310",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "The reproduction of stereotypes and social biases are critical issues in Artificial Intelligence research. Current research focuses mostly on identifying and minimizing biases in systems. Less research has been done on the interplay between system biases and stereotypes in humans as well as their social effects, such as automation bias and stereotype thread. \r\nIn this paper, we want to bring attention to these topics in the domain of human-robot Interaction. \r\nIn particular, we analyze possible influences on automation bias in a dataset from an empirical human-robot interaction study. \r\nWe observe automation bias when participants believe a Furhat robot's false judgment of their language skills to be accurate. \r\nDespite the limited data, we found that being bilingual significantly influenced participants' belief in the robot's negative assessment of their language skills. \r\nThis result shows that participants' insecurity about their own (language) skills can be reinforced by automation bias and vice versa.  \r\nWe illustrate and discuss the necessity for the awareness of automation bias and the possible reinforcement of this effect due to other social biases. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": ""
            }
          ],
          "personId": 152208
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Faculty of Linguistics and Literary Studies, Digital Linguistics Lab"
            }
          ],
          "personId": 151649
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Faculty of Linguistics and Literary Studies, Digital Linguistics Lab"
            }
          ],
          "personId": 151690
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": " University of Bielefeld",
              "dsl": "Computational Linguistics"
            }
          ],
          "personId": 151694
        }
      ]
    },
    {
      "id": 152402,
      "typeId": 13321,
      "title": "“Don’t judge a book by its cover”: Exploring Discriminatory Behavior in Multi-User-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1311",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "With multi-user scenarios in human-robot interaction, the problem\r\nof predisposed and unfair robot treatments due to biases arises.\r\nThus, this study explores whether individuals recognize discrimination\r\nby a social robot and the impact of the feeling of exclusion.\r\nAs a social consequence, the influence of robot discrimination on\r\nthe perception of interaction partners and the attribution of blame\r\nis in focus. Employing a VR-based multi-user lab experiment simulating\r\na library task, participants experienced discrimination by a\r\nrobot. Results suggest that discriminated individuals felt more discriminated\r\nagainst, albeit not significantly more ostracized. Moreover,\r\ndiscrimination influenced the self-attribution of blame and\r\nobservers’ evaluations of the discriminated user’s competence. This\r\nwork highlights the complex social impact of robot discrimination\r\non human interactions and team dynamics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Computer Science Institute"
            }
          ],
          "personId": 151504
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Computer Science Institute"
            }
          ],
          "personId": 151729
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Nord Rhein Westfalen",
              "city": "Oberhausen",
              "institution": "Hochschule Ruhr West",
              "dsl": ""
            }
          ],
          "personId": 152333
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Institute of Positive Computing and Institute of Computer Science"
            }
          ],
          "personId": 151783
        }
      ]
    },
    {
      "id": 152403,
      "typeId": 13321,
      "title": "Developing a Safety Motion Planning Model for General Users Based on Motion Planning with Speed Variation in Human-Industrial Robot Collaboration (HIRC)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1312",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Emerging cyber-physical and artificial intelligence technology empowers booming applications within human-industrial robot collaboration (HIRC) across different disciplines. Industrial robots provide opportunities for practitioners to rapidly and accurately actualize with 3D data/programs. However, as the arm-type industrial robots move into the dynamic work environment, the unpredictable behaviour of users, especially inexperienced operators, in close interaction will challenge the current HIRC safety features and regulations. This research aims to develop a Safety Motion Planning Model based on the foundation of robotic kinematics in the shared-working space, to release the potential of HIRC by improving user experience. This model will work to provide an intuitive approach for inexperienced users to have an adaptive motion planning solution for their task, to eliminate safety risks hence allowing close interaction and collaboration. The model can potentially illustrate a wide range of safety metrics of the specific HIRC process and may contribute to a tool/interface by combining it with smart manufacturing technologies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "Sydney School of Architecture, Design and Planning"
            }
          ],
          "personId": 151815
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "Sydney School of Architecture, Design and Planning"
            }
          ],
          "personId": 151938
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "The University of Sydney",
              "dsl": "Sydney School of Architecture, Design and Planning"
            }
          ],
          "personId": 152039
        }
      ]
    },
    {
      "id": 152404,
      "typeId": 13322,
      "title": "End-User Development for Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1044",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152748
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "U.S. Naval Research Laboratory",
              "dsl": "Navy Center for Applied Research in AI "
            }
          ],
          "personId": 151551
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "U.S. Naval Research Laboratory",
              "dsl": "Navy Center for Applied Research in AI "
            }
          ],
          "personId": 139921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "US Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 139952
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "PAL Robotics",
              "dsl": ""
            }
          ],
          "personId": 152186
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Semio",
              "dsl": ""
            }
          ],
          "personId": 151499
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 152405,
      "typeId": 13322,
      "title": "Taking a Closer Look: Refining Trust and its Impact in HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1047",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152776
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151996
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "Univeristy of Michigan, Ann Arbor",
              "dsl": "School of Information"
            }
          ],
          "personId": 140010
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 140023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 152187
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151744
        }
      ]
    },
    {
      "id": 152406,
      "typeId": 13322,
      "title": "Jibo Community Social Robot Research Platform @Scale",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1046",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152758,
        152760
      ],
      "eventIds": [],
      "abstract": "The lack of shared community social robot platform has been one of the main reasons that made scalable and replicable research activities in human-robot interaction (HRI) and broader cross-community collaborations difficult. In this tutorial, we introduce Community Social Robot Research Platform, Jibo, which was once a commercialized robot companion for the home. This commercially strengthened robot can now serve as a shared community resource and research infrastructure, amplifying the potential for scalable, affordable, and replicable social intelligence research. Prior to the tutorial, we will recruit interdisciplinary community researchers to try out the Jibo Research Prototype for at least two weeks and bring their lived experience to the tutorial. The tutorial program consists of four parts: (1) introduction of the envisioned community research infrastructure; (2) hands-on prototyping experience using the platform; (3) share-outs from participants that have hosted the prototype platform; and (4) reflection activity for the future of the prototype, including spaces for generating recommendations for the envisioned community research platform. The outcome of the tutorial will include a consolidated report of community needs, desires, and recommendations for the future development of the prototype. We will also gather participants’ reflections of the tutorial and potential of the prototype in their research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": " Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 140057
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 139888
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 140006
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 152314
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Personal Robots"
            }
          ],
          "personId": 151579
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152204
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 151485
        }
      ]
    },
    {
      "id": 152407,
      "typeId": 13316,
      "title": "Beyond Assistance: Robots that Respect Personal Values of Older Adults",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1003",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "As robots increasingly play a role in addressing population aging, one of their key roles is to align with older adults' values. Prior work in HRI often focused on being a compensatory model for aging. However, this approach presents a critical gap: it may overlook older adults’ values, leaving their voices unheard in defining how they wish to be assisted by robots and for what intention. Through Value-Sensitive Design and exploring human intentions regarding robot interactions, we imagine a future where robots not only assist but also adapt to people’s desires by understanding their values. In this future, HRI prioritizes incorporating human values into robot design, helping individuals achieve their intentions while aligning with their values. This value-sensitive approach has the potential to revolutionize the relationship between the elderly and robots, fostering a future where technology is a true reflection of human diversity and individuality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências",
              "dsl": "Lasige"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute - HCII"
            }
          ],
          "personId": 151559
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "Instituto Superior Tecnico, University of Lisbon",
              "dsl": "Institute for Systems and Robotics"
            }
          ],
          "personId": 151983
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": "LASIGE, Faculdade de Ciências"
            }
          ],
          "personId": 152277
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 152003
        }
      ]
    },
    {
      "id": 152408,
      "typeId": 13316,
      "title": "Leveraging Implicit Human Feedback to Better Learn from Explicit Human Feedback in Human-Robot Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1008",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "My work aims to enable robots to more effectively learn how to help people. The way in which people want to be helped by robots can vary by task, person, or time, among other factors. Thus, it is important that robots can learn to tailor their behavior based on a person’s evolving preferences during an interaction. Robots typically learn from humans via explicit feedback, such as evaluative feedback, preferences, demonstrations, or corrections. However, this type of feedback can interrupt the flow of an interaction and it places an additional cognitive burden on the human. We know that humans “leak” information through their non-verbal behavior that gives clues about their internal states during interactions– can this information be used to augment how a robot learns from humans? My research aims to explore how to incorporate feedback that humans provide implicitly into robot learning paradigms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152238
        }
      ]
    },
    {
      "id": 152409,
      "typeId": 13316,
      "title": "Doxo-Physical Planning: A New Paradigm for Safe and Efficient Human-Robot Interaction under Uncertainty",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1005",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Human-robot interaction (HRI) in the real world often bars robots from key information on which their decisions may hinge, such as humans' goals, attention, and willingness to cooperate. Existing HRI planning methods either neglect the robot’s ability to learn and adapt to human intents at runtime, leading to overly conservative robot motion, or optimistically assume (cooperative) human behaviors, potentially resulting in loss of safety. In order to simultaneously achieve safety and efficiency for HRI in uncertain, non-lab environments, my work leverages principles of game theory and safety analysis and proposes a novel HRI planning framework that jointly reasons about the physical states and the robot's internal representation of the human uncertainty in closed loop, leading to scalable computation of safe robot policies in real time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "Princeton University",
              "dsl": "Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": "Human Interactive Driving"
            }
          ],
          "personId": 151883
        }
      ]
    },
    {
      "id": 152410,
      "typeId": 13321,
      "title": "ORBO: The Emotionally Intelligent Anthropomorphic Robot Enhancing Smartphone Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1317",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Smartphones have become an integral part of people's daily lives, closely linked to emotions and needs, making emotional design increasingly important. Therefore, we designed the robot ORBO, expanding the functionality of smartphones. ORBO focuses on peripheral interaction, featuring emotional intelligence and anthropomorphic characteristics, including expressive eyes. This paper constructs the design space of ORBO, including information input, eye expression output, and emotional interaction. ORBO is responsive to the phone's status and user behavior, utilizing eye expressions to convey emotions, such as curiosity, joy, sadness, sleepiness, and anger, enhancing the interactive experience between users and smartphones. Through prototypes, we demonstrate several scenarios of ORBO applications (daily companionship and entertainment, addressing smartphone overuse, displaying phone status). Furthermore, we discuss potential future research opportunities and applications for ORBO.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152111
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152230
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152364
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152102
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua university",
              "dsl": ""
            }
          ],
          "personId": 151558
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151891
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152164
        }
      ]
    },
    {
      "id": 152411,
      "typeId": 13321,
      "title": "Kinematically Constrained Human-like Bimanual Robot-to-Human Handovers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1319",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Bimanual handovers are crucial for transferring large, deformable or delicate objects. This paper proposes a framework for generating kinematically constrained human-like bimanual robot motions to ensure seamless and natural robot-to-human object handovers. We use a Hidden Semi-Markov Model (HSMM) to reactively generate suitable response trajectories for a robot based on the observed human partner's motion. The trajectories are adapted with task space constraints to ensure accurate handovers. Results from a pilot study show that our approach is perceived as more human--like compared to a baseline Inverse Kinematics approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151552
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 152124
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151777
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151831
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139930
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Intelligent Autonomous Systems"
            }
          ],
          "personId": 151699
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Hessen",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151927
        }
      ]
    },
    {
      "id": 152412,
      "typeId": 13321,
      "title": "Beyond the Default: The Effects of Adaptable Robot Speed in Industrial Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1204",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Collaborative robots have gained popularity in industrial manufacturing due to their flexibility. However, one substantial challenge is to utilize this flexibility human-centered.\r\nWhile there is considerable research emphasis on system-controlled approaches, the exploration of user-controlled adaptation has been comparatively overlooked. Therefore, this study investigated user-controlled speed adaptation in an experimental within-subject design (i.e., default vs. adapted speed). We assessed the mental workload of 36 participants using subjective, physiological, and secondary task performance measures. Moreover, we investigated acceptance, perceived control, and the influence of desirability of control. The primary task simulated assembling circuit boards in a sequential assembly scenario. The results indicated that mental workload did not decrease with adapted speed. Conversely, secondary task reaction time was even slower. However, participants generally preferred the adaptation and reported a greater sense of control. In general, the findings suggest that adapting speed could have both positive subjective and possible negative performance-related aspects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human-Agent Collaboration Lab"
            }
          ],
          "personId": 139956
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Industrial Automation Technology"
            }
          ],
          "personId": 151849
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": "Industrial Automation Technology"
            }
          ],
          "personId": 151657
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": ""
            }
          ],
          "personId": 152307
        }
      ]
    },
    {
      "id": 152413,
      "typeId": 13321,
      "title": "Is there really an Effect of Time Delays on Perceived Fluency and Social attributes between Humans and Social Robots? A preliminary study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1206",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Humans are expert percievers of behavioural properties, including the timing of movements. Even very short hesitancies and delays can be salient in the right context. This article presents results from a preliminary study on behavioural delays in a human-robot interaction setting. We arranged for participants to play Tic-Tac-Toe with our humanoid robot Epi, using a \"Wizard of Oz\" paradigm. Participants (n=17) were randomized into one of three groups, where Epi either executed its movements with no delay, a short delay (4s) or a long delay (10s). Results from questionnaires measuring fluency, trust, anthropomorphism, animacy and likability were compared before and after the interaction and between the different groups. The study, while lacking statistically significant results, suggests potential trends in the impact of time delays on perceived social agency in human-robot interaction. We conclude that better statistical power is needed to be sure whether there is indeed an effect of time delays on robot-related attribution of social features. In addition, we propose using measures that better account for the participants’ embodied experiences by taking emotional and bodily states into consideration for future studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lunds University ",
              "dsl": "Department of Philosophy, Division of Cognitive Science"
            }
          ],
          "personId": 152130
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Philosophy "
            }
          ],
          "personId": 151450
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 152145
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 151968
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Philosophy",
              "dsl": "Lund University"
            }
          ],
          "personId": 151588
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Cognitive Science",
              "dsl": "Lund University"
            }
          ],
          "personId": 151732
        }
      ]
    },
    {
      "id": 152414,
      "typeId": 13322,
      "title": "Designing an HRI Course for Undergraduate Education",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1010",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152746
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 151978
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Science"
            }
          ],
          "personId": 152142
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 139943
        },
        {
          "affiliations": [
            {
              "country": "Kazakhstan",
              "state": "",
              "city": "NUR-SULTAN",
              "institution": "Nazarbayev University",
              "dsl": "Robotics and Mechatronics"
            }
          ],
          "personId": 152213
        }
      ]
    },
    {
      "id": 152415,
      "typeId": 13321,
      "title": "Cycling with Robots: How Long-Term Interaction Experience with Automated Shuttle Buses Shapes Cyclist Attitudes",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1200",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Understanding how people's attitudes and trust toward robots change over time based on prolonged interaction is crucial for their successful integration into real-world environments. However, long-term deployments of robotic systems in natural settings are relatively rare. In this work, we administered a survey to bicyclists (N = 94) who shared the road with automated shuttle buses for up to three years on their daily commute. The results of the survey show that cyclists with more interaction experience reported more negative attitudes toward the presence of the buses in the shared space, referring to their low usage as one of the main reasons. This finding underscores the significance of perceived usefulness in shaping long-term acceptance of automated services. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer and Information Science"
            }
          ],
          "personId": 151869
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Dept.of Computer Science"
            }
          ],
          "personId": 151966
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 152211
        }
      ]
    },
    {
      "id": 152416,
      "typeId": 13322,
      "title": "3rd Workshop on Human-Interactive Robot Learning (HIRL)",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1014",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152708
      ],
      "eventIds": [],
      "abstract": "With robots poised to enter our daily environments, they will not only need to work for people, but also learn from them. An active area of investigation in the robotics, machine learning, and human-robot interaction communities is the design of teachable robots that can learn interactively from humans. To refer to these research efforts, we use the umbrella term Human-Interactive Robot Learning (HIRL). In the last 2 years we began consolidating what defines HIRL in terms of long, medium, and short-term research problems and what the different communities can contribute to those problems. With this third installment of the HIRL workshop, we aim at further consolidating this community and, specifically this year, discuss how the recent widespread of Large Language Models (LLMs) will impact the teaching of robots and explore the opportunities and challenges presented by robots' nature of embodied agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Meylan",
              "institution": "NAVER LABS Europe",
              "dsl": ""
            }
          ],
          "personId": 152365
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Texas",
              "city": "Ramat Gan",
              "institution": "Bar Ilan University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152086
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139753
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Everyday Robots",
              "dsl": ""
            }
          ],
          "personId": 152210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence, Rhode Island",
              "institution": "Brown University",
              "dsl": "Robotics Lab"
            }
          ],
          "personId": 151879
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151831
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            }
          ],
          "personId": 151905
        }
      ]
    },
    {
      "id": 152417,
      "typeId": 13316,
      "title": "Flexible Robot Error Detection Using Natural Human Responses for Effective HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1011",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Robot errors during human-robot interaction are inescapable; they can occur during any task and do not necessarily fit human expectations. When left unmanaged, robot errors' impact on an interaction harms task performance and user trust, resulting in user unwillingness to work with a robot. Existing error detection techniques often lack the versatility to appropriately address robot errors across task and error type, as they frequently use task or error specific information for robust management. To achieve flexible error management, my work leverages natural human responses to robot errors in physical HRI for error detection, classification, mitigation, and recovery across task, scenario, and error type.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152121
        }
      ]
    },
    {
      "id": 152418,
      "typeId": 13322,
      "title": "Privacy Aware Robotics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1017",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152764
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University - Bloomington",
              "dsl": "Informatics and Cognitive Science"
            }
          ],
          "personId": 139878
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Offenbach",
              "institution": "Honda Research Institute Europe",
              "dsl": ""
            }
          ],
          "personId": 151524
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan Co. Ltd.",
              "dsl": ""
            }
          ],
          "personId": 151973
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151953
        }
      ]
    },
    {
      "id": 152419,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Spatial Robotic Experiences as a Ground for Future HRI Speculations",
      "recognitionIds": [
        10080
      ],
      "isBreak": false,
      "importedId": "hri24h-1098",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "This work illustrates how artistic robotic systems can provide a reservoir of unfamiliarity and a basis for speculation, to open the field toward new ways of thinking about HRI. We reflect on a collaborative project between design students, a media art studio, and design researchers working with the baggage handling department of a strategic European airport. Engaging with the industrial context, we developed ‘meta-behaviours’ - abstracted ideas of processes carried out on the worksite–and passed these over to the students who translated them into robotic enactions based on hardware and a form language developed by the media art studio. The resulting visit experience challenges the audience to decode the installation in terms of meta-behaviours and their possible relations to industrial HRI. We used this to reflect on the value of conducting artistic and speculative work in HRI and to distil actionable recommendations for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Zuid Holland",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "Human Centred Design"
            }
          ],
          "personId": 151787
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "Human Centred Design"
            }
          ],
          "personId": 152330
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "TU Delft",
              "dsl": "Human Centred Design"
            }
          ],
          "personId": 151517
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "Living Architecture Systems Group",
              "dsl": ""
            }
          ],
          "personId": 152369
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Faculty of Industrial Design Engineering"
            }
          ],
          "personId": 152148
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology ",
              "dsl": ""
            }
          ],
          "personId": 152274
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "School of Architecture"
            }
          ],
          "personId": 151444
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": ""
            }
          ],
          "personId": 151503
        }
      ]
    },
    {
      "id": 152420,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Human Behind the Robot: Rethinking the Low Social Status of Service Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1096",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "Robots in our society are commonly perceived as subordinate servants with a lower social status than humans. This often leads to humans prioritizing themselves during conflict situations. This becomes problematic when robots start to directly represent humans as proxies if people do not think of the human operator behind them. This could be considered a cognitive bias of human representation in HRI. \r\nTo explore the extent of this problem, we conducted a user study featuring several conflict situations. Participants granted more priority to the robot when the human representation was visible. This paper explores the societal consequences and emerging inequities such as potentially deprioritizing humans by deprioritizing a robot in certain situations. Possible strategies to address potential negative consequences are discussed on a design level while acknowledging that a societal change in how we perceive and treat robots that represent humans might be necessary.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Dept.of Computer Science"
            }
          ],
          "personId": 151966
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Cognition & Interaction Lab"
            }
          ],
          "personId": 151873
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 140003
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Gothenburg",
              "institution": "Chalmers University of Technology",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152079
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Department of Computer & Information Science"
            }
          ],
          "personId": 152211
        }
      ]
    },
    {
      "id": 152421,
      "typeId": 13316,
      "title": "Achieving Deployable Autonomy through Human-in-the-Loop and Customizability: A Case Study in Robot-assisted Feeding",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1015",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Despite decades of research on personal physically assistive robots for people with motor impairments, deployments of such robots are still few. Part of the reason is that every user’s needs, environments, and care routines are unique, making it difficult to develop a sufficiently robust and customized robot. I present past and ongoing research with the ultimate aim of enabling a robot-assisted feeding system to feed a meal to any user, in any environment, without researcher intervention, in a way that aligns with the user’s preferences. Our key insight is that the robot and user form a joint human-robot system that is working together to feed the user. Thus, we can achieve deployable autonomy by providing the user with intuitive and transparent controls to make the robot's execution more robust and to customize the robot to their needs and environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 151641
        }
      ]
    },
    {
      "id": 152422,
      "typeId": 13321,
      "title": "Interactive Robot Programming Inspired by Dog Training: an Exploratory Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1207",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Programming a robot takes time, effort, and expert knowledge. As robots find their way to our personal spaces, it becomes urgent to investigate more intuitive methods to program them. An emerging field of research has focused on developing systems that are easy for non-expert users to understand and train. This paper explores the premise that dog training methods could inspire interactive programming methods for robots. In collaboration with dog-trainers, we designed an interactive programming method for a robot. We evaluate our method in a Wizard-of-Oz study with 18 participants, and compared it with programming the same behavior on a graphical programming software. Results show significant differences in usability scores, with the method inspired by dog training being perceived as more usable, easier, more fun, and more personal. This suggests that robot programming methods based on dog training could benefit a broader range of end-users, allowing them to interactively program new behaviors on robots in richer and more intuitive ways.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Noord Holland",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": "Artificial Intelligence"
            }
          ],
          "personId": 152176
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": ""
            }
          ],
          "personId": 151861
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU Amsterdam",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151990
        }
      ]
    },
    {
      "id": 152423,
      "typeId": 13321,
      "title": "Robot noise: Impact on electrophysiological measurements and recommendations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1209",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Evaluating Human-Robot Interaction (HRI) is crucial for understanding the value that robots would bring to daily life. This paper investigates the robustness of machine learning classification techniques in interpreting physiological signals during HRI, considering potential artifacts induced by robot behavior. This phenomenon was explored with a 30 participants user study involving three cognitive efforts levels. This study used various physiological sensors, including Electroencephalography (EEG), Photoplethysmography (PPG), and Electrodermal Activity (EDA). Results reveal that EEG and PPG signal were impacted by robot-induced noise while EDA was not. By changing preprocessing parameters, EEG was also cleaned from robot noise and revealed better performances than EDA. The study highlights the importance of careful signal selection, balancing robustness and informativeness, and underscores the significance of preprocessing to ensure accurate classification aligned with users' mental states.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ISAE-SUPAERO, Université de Toulouse",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS, Université de Toulouse, CNRS",
              "dsl": ""
            }
          ],
          "personId": 151810
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ISAE-SUPAERO",
              "dsl": ""
            }
          ],
          "personId": 151825
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS, Univ de Toulouse",
              "dsl": ""
            }
          ],
          "personId": 151645
        }
      ]
    },
    {
      "id": 152424,
      "typeId": 13321,
      "title": "Designing Social Educational Robots: How User Participation Shapes Acceptance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1215",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Recent trends in human-robot interaction emphasize the importance of user participation to foster acceptance. \r\nHowever, current research overlooks the influence of varying participation levels and lacks insights for non-participating users. \r\nWe conducted two studies to explore user involvement in designing educational robots. \r\nThe first study examined how different levels affected users' acceptance.\r\nHighly involved participants contributed to context, requirements, and prototyping, while those with low involvement only prototyped within predetermined parameters. \r\nResults showed that increased participation led to significantly higher utilitarian attitudes, hedonic attitudes, and intention to use.\r\nTo validate this for non-involved users, a second study was conducted to assess whether prototypes designed with higher participation positively impacted non-participatory groups. \r\nSurprisingly, the positive effects of study one were not found.\r\nThis finding raises questions about how participatory design processes should be structured to yield outcomes applicable to the broader user group not directly involved in the design process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin ",
              "institution": "TU Berlin ",
              "dsl": ""
            }
          ],
          "personId": 152263
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human-Agent Collaboration Lab"
            }
          ],
          "personId": 139956
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "TU Berlin",
              "dsl": ""
            }
          ],
          "personId": 151475
        }
      ]
    },
    {
      "id": 152425,
      "typeId": 13321,
      "title": "Toward Personalized Tour-Guide Robot: Adaptive Content Planner based on Visitor's Engagement",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1216",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In the evolving landscape of human-robot interactions, tour-guide robots hold significant potential for enriching visitor experiences in diverse environments. However, the existing paradigm of these robots relies heavily on pre-recorded content, which limits effective engagement with visitors. We propose to address this issue of visitor engagement by transforming tour-guide robots into dynamic, adaptable companions that cater to individual visitor needs and preferences. Our primary objective is to enhance visitor engagement during tours through a robotic system capable of assessing and reacting to visitor preference and engagement. Leveraging this data, the system can calibrate and adapt the tour-guide robot's content in real-time to meet individual visitor preferences. Through this research, we aim to enhance the tour-guide robots' impact in delivering engaging and personalized visitor experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 151456
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 140023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 139860
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 151871
        }
      ]
    },
    {
      "id": 152426,
      "typeId": 13321,
      "title": "Semantic Gesture in Robot Humor: A New Dimension to Enhance the Humor Expression",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1217",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Humor is pervasive in our daily life. It serves not only to build rapport but also to ease tension during interactions and create stronger social interactions. In the realm of Human-Robot Interaction (HRI), humor also plays a vital role in fostering engaging and positive interactions. However, endowing robots with the ability to express humor appropriately is still a challenge. Drawing inspiration from pantomime and sign language humor, our research focuses on the role of semantic gestures in the social robot's expression of humor. In this work, we conducted an experiment in which NAO robot made humorous performances by using a series of semantic gestures. The results of the online survey show that the semantic gesture can significantly enhance the degree of funniness of the robot humor performance. Furthermore, the impact of the semantic gesture is closely tied to both the clarity of its expression and the appropriateness of the chosen semantic words.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "ESSONNE",
              "city": "Palaiseau",
              "institution": "ENSTA-Paris",
              "dsl": "U2IS"
            }
          ],
          "personId": 152049
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "UCL Interaction Centre"
            }
          ],
          "personId": 151724
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "U2IS, ENSTA Paris"
            }
          ],
          "personId": 152141
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "ENSTA-Pairs",
              "dsl": "Autonomous Systems and Robotics Lab/U2IS"
            }
          ],
          "personId": 152190
        }
      ]
    },
    {
      "id": 152427,
      "typeId": 13322,
      "title": "Rebellion and Disobedience in Human-Robot Interaction (RaD-HRI)",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1021",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152766
      ],
      "eventIds": [],
      "abstract": "The rise of robots in collaborative settings raises questions about their behavior in complex, socially-driven contexts. A key concern is the need for these robots to sometimes act against their programming, seemingly exhibiting ``rebellious\" or ``disobedient\" behavior. While such actions might seem counterproductive, there are scenarios where they might be beneficial for effective human-robot interaction. This workshop explores these nuances, examining situations where robots may need to exhibit disobedience to function optimally in society. Through an interdisciplinary lens, we investigate when and why robots might need to act in this manner and how it impacts human perceptions of them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "U.S. Naval Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 151768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Medford",
              "institution": "Tufts University",
              "dsl": "HRI Lab"
            }
          ],
          "personId": 151945
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Texas",
              "city": "Ramat Gan",
              "institution": "Bar Ilan University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152086
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 140048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "ALPHAS Lab"
            }
          ],
          "personId": 151899
        }
      ]
    },
    {
      "id": 152428,
      "typeId": 13321,
      "title": "Interactions with social robots as a prompt for metatalk and critical technological thinking in preschool children",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1210",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In this study, we investigated preschoolers' metatalk and how social robots can prompt it. We conceptualize metatalk as communication about communication, which requires children's ability to reflect on what is happening in an interaction. We consider this ability to be an essential prerequisite for being able to think critically about emerging technologies from an early age, thus empowering children as digitally literate citizens. Despite the variety of educational domains in which social robots have demonstrated their potential as learning partners, there has been little research into how they can be used to support children's metatalk in collaborative activities. The aim of this paper is twofold: First, we report on a method for eliciting metatalk. Specifically, we explored how groups of preschool children could first interact with two social robots that exhibited peculiarities in their communicative behavior, whereupon this interaction was used as a springboard for debate in guided group discussions. Second, we present preliminary results suggesting that, based on the method developed, the children were able to successfully engage in both the practice of metalinguistic skills and critical reflection on the technology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities, Psycholinguistics"
            }
          ],
          "personId": 151742
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities, Psycholinguistics"
            }
          ],
          "personId": 151816
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Paderborn",
              "institution": "Paderborn University",
              "dsl": "Faculty of Arts and Humanities, Psycholinguistics"
            }
          ],
          "personId": 151445
        }
      ]
    },
    {
      "id": 152429,
      "typeId": 13322,
      "title": "3rd Workshop on Inclusive HRI: Equity and Diversity in Design, Application, Methods, and Community",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1020",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152710
      ],
      "eventIds": [],
      "abstract": "Diversity, equity, and inclusion (DEI) are key factors in the development of robot systems that interact with people in the real world. Without such considerations, biases toward underrepresented groups can exacerbate discrimination and perpetuate harm. The human-robot interaction (HRI) community must urgently take action to prevent this. This workshop addresses these issues by providing a forum to build community, share experiences, and disseminate research findings on DEI considerations in HRI.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Hunter College",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego",
              "dsl": "Department of Education Studies"
            }
          ],
          "personId": 152223
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 139782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 152314
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Telangana",
              "city": "Hyderabad",
              "institution": "IFHE university",
              "dsl": "Communication and soft skills"
            }
          ],
          "personId": 151599
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": ""
            }
          ],
          "personId": 151619
        }
      ]
    },
    {
      "id": 152430,
      "typeId": 13321,
      "title": "A Robot Dialogue Authoring Interface with Smart Capabilities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1211",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Researchers have called for the development of dedicated authoring interfaces that can support caregivers in authoring socially assistive robot content. In this paper, we present an authoring interface specifically designed for authoring robot dialogue for reading a book with a child. Our interface incorporates past research on Socially Assistive Robot dialogue and introduces some automatic capabilities aimed at speeding content authoring. We discuss key challenges that arose while working on this interface and describe future work to evaluate, improve, and generalize this interface.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 152376
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": ""
            }
          ],
          "personId": 151640
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152431,
      "typeId": 13322,
      "title": "Ethnography in HRI: Embodied, Embedded, Messy and Everyday",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1023",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152750
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Vienna",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Human Computer Interaction Group"
            }
          ],
          "personId": 151702
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University",
              "dsl": "Media & Information"
            }
          ],
          "personId": 139777
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Gothenburg",
              "institution": " University of Gothenburg and Chalmers University of Technology",
              "dsl": "Interaction Design, Department of Computer Science and Engineering (CSE)"
            }
          ],
          "personId": 151771
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Gothenburg",
              "institution": "Chalmers University of Technology, University of Gothenburg",
              "dsl": "Interaction Design, CSE"
            }
          ],
          "personId": 152272
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "University of Birmingham",
              "dsl": ""
            }
          ],
          "personId": 151944
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Birmingham",
              "institution": "University of Birmingham",
              "dsl": ""
            }
          ],
          "personId": 151574
        }
      ]
    },
    {
      "id": 152432,
      "typeId": 13321,
      "title": "Voice Command Recognition for Explicit Intent Elicitation in Collaborative Object Transportation Tasks: a ROS-based Implementation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1212",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Voice command recognition remains relatively unexplored in robotics, with limited insight into user acceptance and real-world performance. In this work we try to address this by offering multiple voice command recognition models encapsulated in a single publicly available ROS node ready to be used by the robotics practitioner. We tested its actual performance with 10 volunteers of different nationalities whose first spoken language is not English. The obtained accuracy in these tests varies between 93.14% and 95.63% depending on the number of considered commands and model size. Finally, we conducted a user study with 23 new volunteers performing a human-robot collaborative transport task to test whether humans are willing to use this type of system despite having a non-negligible delay and failure rate. In addition to improvements in parameters such as comfort and trust in the robot, 86.9% of the volunteers chose this system over a technically more robust one.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "IRI-CSIC",
              "dsl": "Mobile Robotics"
            }
          ],
          "personId": 152268
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robòtica i Informàtica Industrial",
              "dsl": ""
            }
          ],
          "personId": 152112
        }
      ]
    },
    {
      "id": 152433,
      "typeId": 13322,
      "title": "HRI for Aging in Place",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1022",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152712
      ],
      "eventIds": [],
      "abstract": "The world’s age is increasing. Yet, as the number of older adults who require physical and cognitive assistance grows, healthcare resources are unable to meet the demand, inducing high care provider turnover and burnout rates, and degrading the quality and availability of care. Care robots could help alleviate the burden on care providers, as they can support care providers with physical and cognitive assistance, such as physically demanding or repetitive house chores and medicine dispensing.\r\n\r\nWhere prior workshops on care robotics have focused on either specific technologies or domains for aging in place, in our workshop, we propose to investigate how to best model and leverage the different stakeholders in the network of care of the older adult (family, friends, nurses, the care robot, etc.) in order to meet evolving needs and preferences of the older adult. As such, the goal of this workshop is to identify key research thrusts that will enable care robots to (1) identify and meet the cognitive and physical needs of an older adult as these needs change over time and (2) coordinate the network of care of the older adult in order to successfully facilitate aging in place.\r\n\r\nThe organizers are affiliated with the AI Institute for Collaborative Assistance and Responsive Interaction for Networked Groups (AI-CARING), whose focus is supporting older adults and their network of care. The findings of the workshop will inform further discussions on how researchers at AI-CARING can successfully improve the quality of life of aging adults living at home.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152325
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Carnegie Mellon University/School of Computer Science/Robotics Institute"
            }
          ],
          "personId": 152035
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 151978
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 151837
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 139968
        }
      ]
    },
    {
      "id": 152434,
      "typeId": 13321,
      "title": "A 3D-CNNs approach to classify users’ emotion through EEG-based topographical maps in HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1213",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Recent research has demonstrated the use of socially assistive ro-\r\nbotics (SAR) in a variety of operational contexts where facilitating\r\nhuman-robot interaction and building rapport depend on elicit-\r\ning positive sensations. The fact that different people express and\r\nfeel emotions in different ways presents a huge bias and makes it\r\nchallenging to identify and differentiate between emotions, even\r\nwith the aid of artificial intelligence techniques. This is one of the\r\nbiggest challenges. Using objective rather than subjective indica-\r\ntors, such as biosignals, as emotional feature discriminators can\r\nclose this gap. Previous studies investigated the use of EEG mea-\r\nsurements to classify emotions in HRI by looking at a range of\r\nclassification methods, such as the use of MLP models and global\r\noptimization algorithms applied to methods like Support Vector\r\nMachine, Random Forest, Decision Tree, K-Nearest Neighbor, and\r\nDeep Neural Network, applied to both raw and to derived signal\r\nfeatures (e.g., valence, arousal, PSD, etc.). This paper introduces a\r\nnovel approach that employs a 3D convolutional neural network\r\n(3D-CNN) to topographic maps obtained from EEG. As far as we\r\nare aware, this method has not yet been researched in this area.\r\nThe proposed model achieved an impressive classification accuracy\r\nof 99.2%, successfully distinguishing between positive and negative\r\nemotions and suggesting that the use of a transformation of EEG\r\ndata into images may be a viable solution because it allows the use\r\nof more accurate classification models. The results of the presented\r\nmodel are consistent with the best state-of-the-art models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "Naples",
              "city": "Naples",
              "institution": "University of Naples \"Federico II\"",
              "dsl": ""
            }
          ],
          "personId": 152163
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Parthenope",
              "dsl": ""
            }
          ],
          "personId": 151594
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Parthenope",
              "dsl": "Department of Science and Technology"
            }
          ],
          "personId": 151857
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Parthenope",
              "dsl": "Department of Science and Technology"
            }
          ],
          "personId": 151483
        }
      ]
    },
    {
      "id": 152435,
      "typeId": 13322,
      "title": "Explainability for Human-Robot Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1025",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152752
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152069
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152038
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Robotics, Perception and Learning"
            }
          ],
          "personId": 151627
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Science"
            }
          ],
          "personId": 152142
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "Utrecht University",
              "dsl": ""
            }
          ],
          "personId": 152173
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "beer sheva",
              "institution": "ben gurion university of the negev",
              "dsl": "cognition, aging and rehabilitation lab department of physiotherapy recanati school for community health proffesions"
            }
          ],
          "personId": 151979
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Robotics Perception and Learning"
            }
          ],
          "personId": 139916
        }
      ]
    },
    {
      "id": 152436,
      "typeId": 13322,
      "title": "Robo-Identity: Designing for Identity in the Shared World",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1027",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152768
      ],
      "eventIds": [],
      "abstract": "Following the success of the first two editions of the Robo-Identity workshop, this workshop seeks to extend the discourse on artificial identity in robots with a particular emphasis on the fluid nature of robo-identity in our shared world. Specifically, this third edition focuses on questions regarding how the fluidity of robot identity can help enable personalized engagement, reverse the perpetuation of harmful social biases, and promote a future of more inclusive and adaptable technologies. This can be an opportunity to discuss questions such as:  How do we design robots that can adapt to individuals' and groups' evolving identities? How can robots cater to the changing needs and preferences of people? How can and should robots analyze and synthesize evolving human identity while effectively adapting over time? How should robot identity be presented? When would it be appropriate to adapt and present a particular robot identity? For a rich discussion on these questions during the workshop, we encourage submissions from various disciplines that present different perspectives and challenges when designing robo-identity in the shared world.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "TU Eindhoven",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 151547
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 152352
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 151633
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 152123
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Lab - School of Computer Science"
            }
          ],
          "personId": 152140
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Laboratory, School of Computer Science"
            }
          ],
          "personId": 151642
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151735
        }
      ]
    },
    {
      "id": 152437,
      "typeId": 13316,
      "title": "Reliable Interactive Autonomy for Multi-Agent Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1020",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "In order to achieve the ultimate goal of harmonious human-robot co-existence, the key is to build autonomous robots that can safely interact with humans for collaboration and coordination, as well as demonstrate reliable behavior that is acceptable to humans. These two requirements slightly differ from each other, with the former addressing the safety and functionality of robots as task performers, and the latter emphasizing the social compliance of robots as entities in society. In this abstract, I will outline my efforts towards enhancing the safety and reliability of interactive robot autonomy from three progressively advancing perspectives, 1) self-level autonomy, aiming to develop reactive behavior that ensures safety for individual robots when encountering non-cooperative agents, 2) peer-level autonomy, emphasizing the establishment of a safe interaction mechanism within an diverse and unconnected multi-robot system, and 3) human-involved autonomy, highlighting the consideration of human factors in the decision-making process for the design of multi-robot systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 152061
        }
      ]
    },
    {
      "id": 152438,
      "typeId": 13316,
      "title": "Supporting Long-Term HRI through Shared Family Routines",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1021",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Technical and practical challenges in human-robot interaction (HRI) often involve facilitating sustained interactions over the long-term, fostering engagement with multiple individuals, and take place in-the-wild. The home environment embodies all three challenges, as multiple family members regularly engage with technology at home. In our research, we take a family-centered approach to understand, design, and evaluate how social robots can take part in setting and maintaining family routines to support long-term HRI. In our prior work, we conducted participatory design sessions with families to understand their preferences for having social robots in their home. Recently, we focused on designing interactions for robot-facilitated family routines. Our future work will include a series of field studies and evaluations investigating whether long-term engagement in robot-facilitated routines can promote stronger family relationships and interpersonal connections.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 140037
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 152439,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "More-than-human Perspective on the Robomorphism Paradigm",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1083",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "This paper proposes a posthuman perspective of the robomorphism theory. We propose to define robomorphism as the attribution of robotlike traits to non-robotic entities. Such a definition embraces the centrality of robots in two aspects. First, by assuming the target of robomorphism is not necessarily a human. Second, by considering the notion of robomorphic traits as inherently crucial to establish the robomorphism paradigm. Embracing robots as relevant non-humans in the robomorphism paradigm constitutes the more-than-human perspective of the proposed approach. The contributions of this paper are threefold. First, we propose the robomorphism paradigm by defining it and its inherent concepts, such as robomorphisation and robomorphic. Second, we discuss the broader implications of the robomorphism theory to the research community of Human-Robot Interaction, raising important new challenges. Third, we created a preliminary inventory of robomorphic traits, which were collected from a speculative workshop activity in order to start answering one of the proposed open challenges. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "ITI, LARSYS, Instituto Superior Técnico, Universidade de Lisboa",
              "dsl": ""
            }
          ],
          "personId": 139868
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "University of Lisbon",
              "dsl": "INESC-ID"
            }
          ],
          "personId": 139946
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "ISCTE",
              "dsl": ""
            }
          ],
          "personId": 152264
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 152011
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Umeå",
              "institution": "Umeå University",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "IST - U. Lisboa",
              "dsl": "ITI/LARSyS"
            }
          ],
          "personId": 152128
        }
      ]
    },
    {
      "id": 152440,
      "typeId": 13321,
      "title": "Measuring Visual Social Engagement from Proxemics and Gaze in the Real World",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1218",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This article presents the progress in measuring visual social engagement in the real-world, relevant for use by social robots in order\r\nto increase their social situational awareness. This study acts as\r\nan extension to the work by [10] on the visual social engagement\r\nmetric by showcasing its applicability in real-world contexts. The\r\noriginal visual social engagement metric was developed using online game data, inherently limited by pre-programmed behaviours.\r\nIn this research, we extend the metric’s utility by applying the same\r\nset of social signals to real-world scenarios. To tailor the previously\r\nestablished interaction profiles for real-world data, human annotators were asked to pinpoint interaction initiation, removing the\r\nuse of arbitrary distance values.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "Bristol Robotics Laboratory",
              "dsl": ""
            }
          ],
          "personId": 151835
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kempten",
              "institution": "Bristol Robotics Laboratory",
              "dsl": "Geriatric Robotics"
            }
          ],
          "personId": 152215
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "PAL Robotics",
              "dsl": ""
            }
          ],
          "personId": 152186
        }
      ]
    },
    {
      "id": 152441,
      "typeId": 13321,
      "title": "Using AI Planning for Managing Affective States in Social Robotics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1219",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Social robotics has recently focused on developing AI agents that recognise and respond to human emotions. The use of plan-based approaches is promising, especially in domains where collecting data in advance is challenging (e.g., medical domains). However, we observe that the appropriate use of the user' affective state will vary with the particular interaction, the expected impact of the robot's behaviours on the user, and the opportunity and accuracy of affective sensing. We observe that there are different ways of modelling the user's affective state, and the appropriate choice will take into consideration the relationship between the user's {affective state} and the robot's behaviour. We propose alternative methods of modelling the user's affective state, and use lessons learnt from a recent project in order to discuss the relevant factors in each approach.  We use simulated data in order to demonstrate the flexibility of model-based generation of interaction strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151989
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Glasgow",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Coputer Science"
            }
          ],
          "personId": 151756
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151491
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152257
        }
      ]
    },
    {
      "id": 152442,
      "typeId": 13321,
      "title": "PriMA-Care: Privacy-Preserving Multi-modal Dataset for Human Activity Recognition in Care Robots",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1225",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In the field of robotics, caregiving robots and personal assistants are assuming an increasingly prominent role, directly impacting human lives. Especially in healthcare domains, these systems are starting to provide continuous 24/7 care by monitoring patients and delivering real-time insights into their activities. The effective deployment of future robots relies on equipping them with sophisticated Human Activity Recognition (HAR) algorithms. Many HAR algorithms are based on Artificial Intelligence (AI) and Machine Learning (ML) models. The development of these models necessitates suitable datasets. This paper introduces a Privacy-preserving Multimodal dataset for HAR in the context of Human-Robot Interaction (HRI) for Care robots (PriMA-Care). Tailored specifically for care robots and personal assistants, PriMA-Care encompasses over 27 diverse user activities, ranging from daily routine tasks to physical HRI. Uniquely engineered for indoor environments, the dataset incorporates data from 10 different privacy-preserving sensors and collected from 17 participants, enabling the development of advanced multimodal and sensor fusion models for HAR. PriMA-Care addresses critical gaps in existing datasets, offering a suitable resource for HAR research in care robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151782
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "SINTEF Digital",
              "dsl": "Sustainable Communication Technologies"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "SINTEF Digital",
              "dsl": "Sustainable Communication Technologies"
            }
          ],
          "personId": 152184
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "norwegian university of science and technology",
              "dsl": "Department of Mechanical engineering and technology management, Faculty of Science and Technology"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "norwegian university of science and technology",
              "dsl": "Department of Mechanical engineering and technology management, Faculty of Science and Technology"
            }
          ],
          "personId": 151763
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "Norway",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Faculty of Mathematics and Natural Sciences, Digitalization, Research Group of Design of Information Systems"
            },
            {
              "country": "Norway",
              "state": "Norway",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Faculty of Mathematics and Natural Sciences, Digitalization, Research Group of Design of Information Systems"
            }
          ],
          "personId": 152037
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151980
        }
      ]
    },
    {
      "id": 152443,
      "typeId": 13321,
      "title": "Multimodal   strategies for  robot-to-human communication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1104",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This paper describes the opportunities of multimodality in the field of robot-to-human communication. In the proposed approach, the coordinated  and integrated use of multimedia elements, i.e., text, images, and animations, with the robot's speech  plays a very important role in the overall effectiveness of the communicative act.  The reference robot used in the research was Pepper, a humanoid robot equipped with a tablet on its front. During the research, various multimodal communication strategies were formalised, implemented and preliminary evaluated by means of a questionnaire.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Turin",
              "institution": "University of Turin",
              "dsl": ""
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "University of Turin",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 151846
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Università di Torino",
              "dsl": "Dipartimento di Informatica"
            }
          ],
          "personId": 152350
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Turin",
              "institution": "University of Turin",
              "dsl": ""
            }
          ],
          "personId": 152255
        }
      ]
    },
    {
      "id": 152444,
      "typeId": 13321,
      "title": "Learning Action Conditions for Automatic Behavior Tree Generation from Human Demonstrations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1226",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The multitude of possible tasks and user preferences in real-world human-robot interaction scenarios renders pure pre-programming of robotic tasks inadequate. Recently, Behavior Trees (BTs) gained more focus as a modular internal task representation and particularly learning BTs directly from human video demonstrations offers also non-programming experts an opportunity to conveniently teach robots. However, automatically building BTs from human task demonstrations requires task constraints in form of action conditions. While existing work on automated BT generation often relies on pre-defined relevant features and heuristic condition computation, here we propose and evaluate different methods to automatically extract action pre- and post-conditions for BT generation from videos of human demonstrations. In particular, we first reduce the feature space using a correlation-based feature pre-selection, as well as a rule-based pre-selection based on a Decision Tree. Then, we select features that are relevant pre- and post-conditions for particular actions based on three different variance-based methods. We compare the different methods for feature selection and condition extraction on two pick-and-place tasks and discuss advantages and shortcomings of all methods in the context of learning BTs from human demonstrations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University Darmstadt, IRG Interactive AI algorithms & cognitive models for human-AI interaction",
              "dsl": ""
            }
          ],
          "personId": 139972
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 152171
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139930
        }
      ]
    },
    {
      "id": 152445,
      "typeId": 13321,
      "title": "Human Robot Interaction through an ontology-based dialogue engine",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1105",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper outlines the evolution of the Anonymous project for high level functioning children affected by autism, focusing on the development of a dialogue system that relies on   an ontology-based knowledge base. The ontology offers a formal representation of knowledge and interrelationships within the movie domain. The dialogue system addresses issues related to predefined answers, emphasizing adaptability for multi-platform use, particularly in the context of the social robot Pepper. The research covers detailed phases of construction and development, highlighting implementation choices and challenges faced.\r\nThis work try to make an advancement in the development of sophisticated and intuitive human-robot interaction systems, capable of adapting to user needs and delivering increasingly accurate and consistent responses.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "University of Turin",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 151892
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Turin",
              "institution": "University of Turin",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 152259
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Turin",
              "institution": "University of Turin",
              "dsl": "Dept. of Computer Science"
            }
          ],
          "personId": 152088
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Università di Torino",
              "dsl": "Dipartimento di Informatica"
            }
          ],
          "personId": 152350
        }
      ]
    },
    {
      "id": 152446,
      "typeId": 13321,
      "title": "How Do Robot Experts Measure the Success of Social Robot Navigation?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1106",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "We interviewed 8 individuals from industry and academia to better understand how they valued different aspects of social robot navigation. Interviewees were asked to rank the importance of 10 measures commonly used to evaluate social navigation policies. Interviewees were then asked open-ended questions about social navigation, and how they think about evaluating the challenges they face. Our interviews with industry and academic experts in social navigation revealed that avoiding collisions was the only universally important measure. Beyond the safety consideration of avoiding collisions, roboticists have varying priorities regarding social navigation. Given the high priority interviewees placed on safety, we recommend that social navigation approaches should first aim to ensure safety. Once safety is ensured, we recommend that each social navigation algorithm be evaluated using the measures most relevant to the intended application domain. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151658
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151734
        }
      ]
    },
    {
      "id": 152447,
      "typeId": 13321,
      "title": "Effects of Transparency in Humanoid Robots - A Pilot Study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1227",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Transparency is recognized as a vital feature for understanding and predicting robot behavior. Another feature that affects interaction with robots is their anthropomorphism. Their relationship, however, remains underexplored. Here, we present a pilot study investigating the effects of robot transparency in human-robot interactions, where the robot has an anthropomorphic appearance. We asked participants to evaluate and interact with the humanoid robot Pepper to examine whether visualizing the robot’s goals and behavior affects perceived intelligence, anthropomorphism, and robot agency. Our preliminary findings suggest that users may attribute higher ratings of agency when interacting with a robot visualizing its goals. In this late-breaking report, we propose our experiment on the interplay between transparency and anthropomorphism in human-robot interaction and summarize insights from our preliminary pilot study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt-Universität zu Berlin",
              "dsl": "Computer Science/Adaptive Systems"
            }
          ],
          "personId": 151862
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt-Universität zu Berlin",
              "dsl": "Computer Science/Adaptive Systems"
            }
          ],
          "personId": 151531
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt-Universität zu Berlin",
              "dsl": "Computer Science/Adaptive Systems"
            }
          ],
          "personId": 151735
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt-Universität zu Berlin",
              "dsl": "Computer Science/Adaptive Systems"
            }
          ],
          "personId": 152073
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Stanford University School of Medicine/Ophthalmology Research/Clinical Trials"
            }
          ],
          "personId": 152321
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt-Universitaet zu Berlin",
              "dsl": "Adaptive Systems Group, Dept. of Computer Science"
            }
          ],
          "personId": 151581
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Hertie School",
              "dsl": "Ethics and Technology"
            }
          ],
          "personId": 151738
        }
      ]
    },
    {
      "id": 152448,
      "typeId": 13321,
      "title": "Exploring Large Language Models for Trajectory Prediction: A Technical Perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1228",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Large Language Models (LLMs) have been recently proposed for trajectory prediction in autonomous driving, where they potentially can provide explainable reasoning capability about driving situations. Most studies use versions of the OpenAI GPT, while there are open-source alternatives which have not been evaluated in this context. In this report, we study their trajectory prediction performance as well as their ability to reason about the situation. Our results indicate that open-source alternatives are feasible for trajectory prediction. However, their ability to describe situations and reason about potential consequences of actions appears limited, and warrants future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 151841
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 151956
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 152304
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 151741
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "School of Electrical Engineering"
            }
          ],
          "personId": 151666
        }
      ]
    },
    {
      "id": 152449,
      "typeId": 13321,
      "title": "PaBo Bot: Paper Box Robots for Everyone",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1107",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Robot building is a fascinating hands-on learning activity, and we want to make it available to more people. Over the past year, we've been experimenting with making robot building more approachable. We presented the design and making of paper box robots PaBo Bot, a set of modular robots made with paper and everyday tools. To enable non-technical users to engage in robot building, we developed (1) a conductive sticker to support the rapid creation of controller circuits, and (2) a way to solder components using a toaster oven. We shared PaBo Bot methods and techniques by organizing hands-on workshops with novice robot builders. All participants successfully built the PaBo Bot. The results show that using our proposed techniques, people without technical backgrounds are able to participate and enjoy robot building.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 151459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 151753
        }
      ]
    },
    {
      "id": 152450,
      "typeId": 13321,
      "title": "Human-Robot Dialogue that Elicits The Alignment of Moral Principles For Driverless Vehicles",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1100",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "The emergence of autonomous vehicles has raised ethical considerations regarding their controlling software. The focus is on defining ethical settings that determine the response to moral dilemmas, akin to the Trolley Problem. The study explores the interaction with a robot that engages users in a dialogue about such ethical dilemmas, allowing users to align vehicles’ behaviour with their ethical preferences. Additionally, providers of these vehicles can have a codified version of user preferences to address potential real-world issues, for instance, by adjusting insurance premiums. The research details designing and implementing a human-robot interaction system for eliciting ethical settings in autonomous vehicles.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": ""
            }
          ],
          "personId": 151637
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "Universitat Pompeu Fabra",
              "dsl": "Department of Information and Communications Technologies"
            }
          ],
          "personId": 151780
        }
      ]
    },
    {
      "id": 152451,
      "typeId": 13321,
      "title": "Measuring State Utilization During Decision Making in Human-Robot Teams",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1222",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Efficient team design necessitates a comprehensive understanding of human factors, encompassing abilities, limitations, and internal states. In human-robot teaming research, recent efforts explore integrating emotions, workload, fatigue, and stress into decision-making using deep reinforcement learning. Despite promising results, the black-box nature of these algorithms raises questions about the consistent reliance on human internal states or their consideration as information or noise in the decision-making process. This study introduces a state utilization (SU) metric to measure the reliance of reinforcement-based agents on each state feature. This metric is validated on data from the Cartpole environment by OpenAI and a human-robot teaming experiment using NASA MATB-II environment. The SU provides insight into the relevance and usage of state features and human data modalities by the robot, showing clear trends based on the nature of the tasks and offering an understanding of why the RL agent takes certain actions. This, in turn, enhances the explainability of the RL agent's policy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "RIT",
              "dsl": ""
            }
          ],
          "personId": 152300
        }
      ]
    },
    {
      "id": 152452,
      "typeId": 13321,
      "title": "Designing Healthcare Robots at Home for Older Adults: A Kano Model Perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1101",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Healthcare robots at home are increasingly essential for promoting the independence of older adults, yet their widespread acceptance is hindered by a lack of clarity regarding optimal design features. To address this, this study employs the Kano model to systematically identify and prioritize the features of healthcare robots that most significantly influence user satisfaction and acceptance among older adults. We conducted a survey study with 253 U.S. older adults to evaluate a variety of robot features. The results highlight design features that markedly affect user satisfaction and acceptance. The findings of this study are significant not only for future research endeavors but are also important for the practical development of healthcare robots, guiding the creation of more effective and user-friendly healthcare solutions at home for older adults.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "ANN ARBOR",
              "institution": "University of Michigan-Ann Arbor",
              "dsl": ""
            }
          ],
          "personId": 152367
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Dearborn",
              "institution": "University of Michigan",
              "dsl": "IMSE"
            }
          ],
          "personId": 152108
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 139999
        }
      ]
    },
    {
      "id": 152453,
      "typeId": 13321,
      "title": "Opportunities for human--multi-agent systems in space: Thematic analysis of interviews with U.S. Space Force Guardians",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1102",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Recent developments have advanced investment in space exploration and operations, and have accelerated opportunities for complex human-robot and human-machine collaborations in space. This work presents the results of a thematic analysis of interviews conducted with U.S. Space Force Guardians to gain a better understanding of the potential role of autonomous robots and artificially intelligent systems in supporting space operations with a particular focus on Space Force operations, especially as they relate to the use of human--multi-agent system teams.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Air Force Academy",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 151793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Air Force Academy",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 152196
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Air Force Academy",
              "institution": "U.S. Air Force Academy",
              "dsl": ""
            }
          ],
          "personId": 152265
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human Factors and Applied Cognition Program"
            }
          ],
          "personId": 151623
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "AF Academy",
              "institution": "United States Air Force Academy",
              "dsl": "Department of Behavioral Sciences and Leadership"
            }
          ],
          "personId": 151856
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Colorado Springs",
              "institution": "United States Air Force Academy",
              "dsl": "Dept of Behavior Sciences and Leadership"
            }
          ],
          "personId": 151766
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 151882
        }
      ]
    },
    {
      "id": 152454,
      "typeId": 13321,
      "title": "Adaptive Second Language Tutoring Using Generative AI and a Social Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1103",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "The most effective second language learning occurs through extensive interpersonal interaction and tutoring. However, limited funding and a lack of language teachers often prevent students from engaging in individualised practice, a lack which can be addressed using AI and social robots. We present a system that leverages generative AI to provide customized educational content in real-time, adapting to students' skills through an engaging, visually-grounded game played alongside a social robot. To test effectiveness, we conducted a study in which Dutch high school students learned Spanish vocabulary either with or without the robot present. Results showed significant vocabulary gains regardless of robot presence, indicating the game itself, not the social embodiment, drove learning. While further refinements are needed, these findings highlight the potential for social robots and generative AI to deliver personalized language tutoring and circumvent the constraints posed by limited resources and staffing in schools. Ongoing work aims to enhance social presence and better align generative content with individuals' abilities and pacing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab - AIRO"
            }
          ],
          "personId": 152293
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab - AIRO"
            }
          ],
          "personId": 151573
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 151842
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab - imec"
            }
          ],
          "personId": 151461
        }
      ]
    },
    {
      "id": 152455,
      "typeId": 13321,
      "title": "Using Proxemics as a Corrective Feedback Signal during Robot Navigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1224",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This paper presents a novel feedback system based on proxemics. PFbS (Proximity Feedback System) allows a robot to dynamically update its navigational behavior by interpreting a human teacher’s distance to the robot as a corrective feedback signal. We demonstrate PFbS on a Pepper robot and evaluate it in a pilot study (N=18) against a joystick interface baseline, in terms of usability, intuitiveness and signal clarity. We also evaluate the learning algorithm for dynamic update of the robot’s trajectory. Based on the insights from the pilot, we believe that with proper sensor accuracy, PFbS can be improved to allow for more intuitive and embodied feedback for non-expert users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": ""
            }
          ],
          "personId": 151819
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU Amsterdam",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151990
        }
      ]
    },
    {
      "id": 152456,
      "typeId": 13321,
      "title": "Impact of body movement and teleoperation interface on operator’s performance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1220",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Teleoperation has become increasingly crucial for military, civil, and space applications. This study examines how dynamic environments, such as the remote control of a robot from a vehicle in movement, affect teleoperation performance. We evaluate the effects of changing gravitational cues provided by a motion platform on operators' teleoperation of a drone in virtual reality and explore different visual interfaces to enhance performance. Preliminary findings suggest that teleoperation from dynamic environments can lead to reduced performance, with 3rd person perspective (3PP) potentially mitigating negative effects. While further research is needed, our results initiate the exploration of the relationship between body movement and teleoperation for more efficient and safer practices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "France",
              "city": "Toulouse",
              "institution": "ISAE-Supaero University of Toulouse",
              "dsl": ""
            }
          ],
          "personId": 151813
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "France",
              "city": "Toulouse",
              "institution": "ISAE-Supaero University of Toulouse",
              "dsl": ""
            }
          ],
          "personId": 152180
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "France",
              "city": "Toulouse",
              "institution": "ISAE-Supaero University of Toulouse",
              "dsl": ""
            }
          ],
          "personId": 152323
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "France",
              "city": "Toulouse",
              "institution": "ISAE-Supaero University of Toulouse",
              "dsl": ""
            }
          ],
          "personId": 151825
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "France",
              "city": "Toulouse",
              "institution": "ISAE-Supaero University of Toulouse",
              "dsl": ""
            }
          ],
          "personId": 152220
        }
      ]
    },
    {
      "id": 152457,
      "typeId": 13317,
      "title": "Brush-E Bot: Your Toothbrushing Companion Bot",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1016",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Cavities is a common disease children experience and consistent brushing has been found to reduce the chances of cavities developing. Despite the importance of brushing many caretakers face resistance when trying to instill consistent brushing habits in children. By making brushing time more fun, children can create positive associations with brushing, establishing habits that can last a lifetime. Enter Brush-E Bot! Our robot is designed to make brushing engaging and educational, teaching children how to brush correctly while making the experience as fun as possible. In this paper we discuss the importance, design, implementation, and contribution of our robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": ""
            }
          ],
          "personId": 151889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": "Computer Science, Ritchie School of Engineering and Computer Science"
            }
          ],
          "personId": 152249
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152005
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": ""
            }
          ],
          "personId": 152288
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "DENVER",
              "institution": "University of Denver",
              "dsl": ""
            }
          ],
          "personId": 152348
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Denver",
              "dsl": ""
            }
          ],
          "personId": 152178
        }
      ]
    },
    {
      "id": 152458,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Theory of Mind abilities of Large Language Models in Human-Robot Interaction : An Illusion?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1110",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "Large Language Models have shown exceptional generative abilities in various natural language and generation tasks. However, possible anthropomorphization and leniency towards failure cases have propelled discussions on emergent abilities of Large Language Models especially on Theory of Mind (ToM) abilities in Large Language Models.  While several false-belief tests exists to verify the ability to infer and maintain mental models of another entity, we study a special application of ToM abilities that has higher stakes and possibly irreversible consequences : Human Robot Interaction. In this work, we explore the task of Perceived Behavior Recognition, where a robot employs a Large Language Model (LLM) to assess the robot's generated behavior in a manner similar to human observer. We focus on four behavior types, namely - explicable, legible, predictable, and obfuscatory behavior which have been extensively used to synthesize interpretable robot behaviors. The LLMs goal is, therefore to be a human proxy to the agent, and to answer how a certain agent behavior would be perceived by the human in the loop, for example \"Given a robot's behavior X, would the human observer find it explicable?\". We conduct a human subject study to verify that the users are able to correctly answer such a question in the curated situations (robot setting and plan) across five domains. A first analysis of the belief test yields extremely positive results inflating ones expectations of LLMs possessing ToM abilities. We then propose and perform a suite of perturbation tests which breaks this illusion, i.e. Inconsistent Belief, Uninformative Context and Conviction Test.  We conclude that, the high score of LLMs on vanilla prompts showcases its potential use in HRI settings, however to possess ToM demands invariance to trivial or irrelevant perturbations in the context which LLMs lack. We report our results on GPT-4 and GPT-3.5-turbo. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": ""
            }
          ],
          "personId": 151534
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            },
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            }
          ],
          "personId": 151601
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing, Informatics and Decision System Engineering"
            },
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing, Informatics and Decision System Engineering"
            }
          ],
          "personId": 152010
        }
      ]
    },
    {
      "id": 152459,
      "typeId": 13321,
      "title": "DogSurf: Quadruped Robot Capable of GRU-based Surface Recognition for Blind Person Navigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1229",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper introduces DogSurf - a new approach of using quadruped robots to help visually impaired people navigate in real world. The presented method allows the quadruped robot to detect slippery surfaces, and to use audio and haptic feedback to inform the user when to stop. A state-of-the-art neural network architecture was proposed for the task of multiclass surface classification for four-legged robots. A dataset was collected on a Unitree Go1 Edu robot, over 10 hours. The dataset and code have been posted to the public domain. Testing on real humans was also done, and NASA-TLX was also conducted.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": "Intelligent Space Robotics Lab"
            }
          ],
          "personId": 151655
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": ""
            }
          ],
          "personId": 151974
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow ",
              "institution": "Skolkovo Institute of Science and Technology (Skoltech)",
              "dsl": "ISR lab"
            }
          ],
          "personId": 151750
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": ""
            }
          ],
          "personId": 152267
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "Moscow",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Science and Technology",
              "dsl": "Space Center"
            }
          ],
          "personId": 152241
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": "Intelligent Space Robotics Laboratory"
            }
          ],
          "personId": 151909
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Science and Technology",
              "dsl": "Intelligent Space Robotics Laboratory"
            }
          ],
          "personId": 151498
        }
      ]
    },
    {
      "id": 152460,
      "typeId": 13321,
      "title": "What we learn on the streets: situated human-robot interactions from an industry perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1108",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "As commercial robots are increasingly present in everyday spaces, more efforts in HRI community are directed at investigating the complexity of interactions in naturalistic settings. Simultaneously, conversations about the potentials and challenges of collaborations between academic HRI and industry are taking place. This report contributes to these topics by presenting two (People not Users, What we Learn on the Streets) out of the five themes qualitatively developed from the interviews conducted with a company developing and deploying commercial delivery robots in public spaces. The themes emphasize plurality of actors in various ways engaging with the robots and shaping iterations on the robots design, the iterative and non-deterministic process of the robot design, as well as the differences between academic HRI and industry as perceived by the interviewees. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Österreich",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 151616
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 151630
        }
      ]
    },
    {
      "id": 152461,
      "typeId": 13317,
      "title": "PosChair: Promoting Productivity & Health in Every Seat",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1017",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Sedentary behaviour, characterized by prolonged periods of sitting has become increasingly prevalent in modern lifestyles. It is an activity that has taken over most aspects of our days, both work and leisure. This way of life is shown to increase various serious health risks. This paper addresses the need for innovative solutions to combat these risks and introduces the “PosChair” – a novel interactive chair designed to promote user health and productivity by encouraging shorter sitting periods through intermittent standing. The paper offers an explanation on how the chair achieves this.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Università degli Studi di Genova",
              "dsl": ""
            }
          ],
          "personId": 151858
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Università degli Studi di Genova",
              "dsl": ""
            }
          ],
          "personId": 151501
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 151855
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 151960
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Università degli Studi di Genova",
              "dsl": ""
            }
          ],
          "personId": 152326
        }
      ]
    },
    {
      "id": 152462,
      "typeId": 13321,
      "title": "Attitudes towards Social Robots (ASOR): revisiting the scale with four types of robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1109",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Our work is theoretically grounded in the notion of sociomorphing contending that not all dimensions of experienced sociality with robots pertain to projection of human-like mental states i.e. anthropomorphism. To investigate dimensions of attributed sociality, we deployed the Attributes towards Social Robots scale (ASOR) in a video-based online study (n=202) with four different robots (Starship Delivery Robot, Telenoid, Blossom, Vector). The four robots were rated slightly differently which aligned with our expectations because of the differences in appearances and how they were contextualised in the videos. However, further evaluation of the statistical properties of the scale and the solicited qualitative feedback to the items pointed to limitations of the scale.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Österreich",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 151616
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg",
              "institution": "University of Salzburg",
              "dsl": "Artificial Intelligence and Human Interfaces"
            }
          ],
          "personId": 151926
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 151630
        }
      ]
    },
    {
      "id": 152463,
      "typeId": 13321,
      "title": "The Way You See Me - Comparing Results from Online Video-Taped and In-Person Robotic Storytelling Research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1115",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "During the covid-19 pandemic researchers had to find alternative ways to conduct experiments since lab-based studies were prohibited due to lock-downs and social distancing. Video-taping robots and conducting online studies is an easy approach to solve this issue, however, it is unclear whether results from online video-based studies are comparable to in-person experiments. As displaying recordings to participants in online studies is especially easy when it comes to robotic storytelling, we chose this use case for a comparison of the two methodologies and conducted a study with a NAO robot telling a humorous story with or without additional sound usage in an online- respectively lab-setting. Although differences between the presentation modes were found for robot perception, emotion induction, and storytelling experience, the main effect of sound integration did not differ between the two presentation modes, suggesting the comparability of results obtained from online video-based studies on robotic storytelling.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 139882
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 152166
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Compuert Interaction"
            }
          ],
          "personId": 139885
        }
      ]
    },
    {
      "id": 152464,
      "typeId": 13321,
      "title": "Transition State Clustering for Interaction Segmentation and Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1237",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Hidden Markov Models with an underlying Mixture of Gaussian structure have proven effective in learning Human-Robot Interactions from demonstrations for various interactive tasks via Gaussian Mixture Regression. However, there is a mismatch when segmenting the interaction using only the observed state of the human compared to the joint state of the human and the robot. To improve this underlying segmentation and subsequently the predictive abilities of such Gaussian Mixture-based approaches, we take a hierarchical approach by learning an additional mixture distribution on the states at the transition boundary, thereby preventing misclassifications that usually occur in such states. We find that our framework improves the performance of the underlying Gaussian Mixture-based approach which we evaluate on various interactive tasks such as waving, handshaking, fistbumps, and handovers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151615
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151777
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151831
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 139930
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Marketing and Human Resource Management"
            }
          ],
          "personId": 152266
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": "Intelligent Autonomous Systems"
            }
          ],
          "personId": 151699
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Hessen",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 151927
        }
      ]
    },
    {
      "id": 152465,
      "typeId": 13321,
      "title": "Nursing Staff’s Attitudes, Needs, and Preferences for Care Robots in Assisted Living Facilities: A Systematic Literature Review",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1116",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Care robots have been proposed in response to nursing shortages in assisted living facilities (ALF), in which the population of older adults is growing. Although the use of care robots can improve the health of these older adults, their introduction fundamentally changes the work of nursing staff and has implications for the entire healthcare system. In the development of such gerotechnology, it is important to include end-users, but so far, the perspectives of nursing staff have largely been ignored. We have conducted a systematic review to examine the literature on nursing staff's attitudes, needs, and preferences for care robots in ALFs, to guide future research. Using the PRISMA method, we identified 15 publications. We found that nursing staff desire robots that can assist with physically demanding tasks and reduce workload. Further research is needed on nursing staff's concerns and the contextual factors that influence nursing staff's perspectives of care robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin",
              "dsl": "School of Nursing"
            }
          ],
          "personId": 151568
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 151470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 151965
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Nursing"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 152329
        }
      ]
    },
    {
      "id": 152466,
      "typeId": 13321,
      "title": "Technology-assisted Journal Writing for Improving Student Mental Wellbeing: Humanoid Robot vs. Voice Assistant",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1117",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Conversational agents have a potential in improving student mental wellbeing while assisting them in self-disclosure activities such as journalling. Their embodiment might have an effect on what students disclose, and how they disclose this, and student’s overall adherence to the disclosure activity. However, the effect of embodiment in the context of agent assisted journal writing has not been studied. Therefore, this study aims to investigate the viability of using social robots (SR) and voice assistants (VA) for eliciting rich disclosures in journal writing that contributes to mental health status improvement in students over time. Forty two undergraduate and graduate students participated in the study that assessed the mood changes (via Brief Mood Introspection Scale, BMIS), level of subjective self-disclosure (via Subjective Self-Disclosure Questionnaire, SSDQ), and perceptions toward the agents (via Robot Social Attributes Scale, RoSAS) with and without agent (SR or VA) assisted journal writing. Results suggest that only in robot condition there are mood improvements, higher levels of disclosure, and positive perceptions over time in technology-assisted journal writing. Our results suggest that robot assisted journal writing has some advantage over voice assistant one for eliciting rich disclosures that contributes to mental health status improvement in students over time.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Pompeu Fabra",
              "dsl": "Department of Information and Communication Technologies "
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Granada",
              "institution": "University of Granada",
              "dsl": "Department of Computer Engineering, Automation and Robotics"
            }
          ],
          "personId": 152335
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 139889
        }
      ]
    },
    {
      "id": 152467,
      "typeId": 13321,
      "title": "Perspectives on Robotic Systems for the Visually Impaired",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1238",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Many roboticists hope to build robots and develop technologies that would one day help vulnerable populations to improve their quality of life. As there are over 2.2 billion people with visual impairments in the world, this vulnerable population is a prime target for robotic assistants to help. In a discussion with a Certified Orientation and Mobility Specialist, someone who helps people with visual impairments manage every day tasks such as navigating while avoiding obstacles and daily living, some interesting and counterintuitive questions were raised about technological developments, particularly robots. While these devices were meant to help the blind and visually impaired (BVI) population, many are, in reality, not practically beneficial. In this article, we highlight certain misconceptions about the BVI population and their needs, especially the mismatch between robotics research and the needs of the individuals with visual impairments, especially from the lens of human-robot interaction (HRI) researchers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 152103
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "RAISE Lab"
            }
          ],
          "personId": 151981
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 152349
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "University of Montreal",
              "dsl": "School of Optometry"
            }
          ],
          "personId": 151970
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "RAISE Lab"
            }
          ],
          "personId": 151493
        }
      ]
    },
    {
      "id": 152468,
      "typeId": 13321,
      "title": "Joint Attention Estimation during Multi-party Facilitation Using Multi-Modal Fusion",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1118",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "This paper presents an enhanced framework for joint attention estimation. Visual attention is an important non-verbal cue to facilitate human-human social interaction. For example, it is natural for humans to look at the person who is speaking, and to make eye contact to indicate their interest in conversation. This unique capability leads to effective social interaction between humans, which is desirable in many intelligent systems to realize natural human-robot interaction. However, it is difficult to replicate humans’ social gaze saliency on agents/robots because it is highly dependent on the dynamic exchanges between humans and the immediate physical environment. Existing off-the-shelf commercial social robots usually boast of gaze behavior to enhance interaction with heuristic rules by looking at human face and who is speaking. Unfortunately, these methods are not likely to generalize well to the real-world environment where social interaction is complicated due to environment contexts and social dynamics. In this paper, we aim to address the above limitations by developing a multi-modal social gaze saliency method based on human-human interaction data in a multiparty facilitation scenario. We integrates talking information, upper-body body language, gaze angle, and head position to estimate joint attention. Such model using the multi-modal fusion is expected to generalize better to different robotic platforms and applications in the wild.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Saitama",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 152004
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Saitama",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151751
        }
      ]
    },
    {
      "id": 152469,
      "typeId": 13321,
      "title": "Meet or Call my Robotic Tutor? - The Effect of a Physically vs. Virtually  Present Social Robot on Learning Outcomes, Engagement and Perception",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1239",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Remote learning has become increasingly popular since its unprecedented rise during the Covid-19 pandemic as it offers flexibility of location. In regard to the future of social robots in education and to find out more about the impact of their physical presence, it is necessary to investigate their application in remote learning scenarios. In this study, we set up a robot-supported learning environment in which students either learned with a a social robot either physically present or virtually present via a live video call. We found no differences in learning outcome and perception of the robot, but gaze towards the robot was significantly higher in the remote learning condition, implying that students might be more socially engaged with the robot or paying more attention to it. Our results demonstrate a potential of social robots as remote learning tools but even more the need for more research in this field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction, University of Wuerzburg",
              "dsl": ""
            }
          ],
          "personId": 151620
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "University of Würzburg",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 151688
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-University Würzburg",
              "dsl": ""
            }
          ],
          "personId": 139885
        }
      ]
    },
    {
      "id": 152470,
      "typeId": 13321,
      "title": "The Nose Knows: Using Thermal Imaging to Approximate Children’s Engagement with Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1232",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "We explore the integration of a visual and thermal camera to approximate physiological changes as markers of cognitive load and child's engagement with a robot. The aim of our data pipeline is to enable non-invasive engagement tracking for a desktop social robot developed by Honda Research Institute named Haru.  From utilizing these two cameras we can recognize engagement during child-robot interactions (CRI) using changes in nose-tip temperature. We tested our algorithm on data collected while a child participant interacted with Haru during a passive activity as well as an active activity with Haru. Then, we did a preliminary modeling of engagement with Hidden Markov models. This paper describes our experimental setup, our data collection, multi-modal pipeline, and some preliminary results from modeling the data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University - Bloomington",
              "dsl": "Informatics and Cognitive Science"
            }
          ],
          "personId": 139878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Luddy School of Informatics, Computing, and Engineering",
              "dsl": "Indiana University"
            }
          ],
          "personId": 152318
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Luddy School of Informatics, Computing, and Engineering"
            }
          ],
          "personId": 151606
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151955
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 152471,
      "typeId": 13321,
      "title": "Towards developing an active listening counseling robot for multiple generations: A Text Mining Study on Emotional Expression of the Elderly and the Young",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1111",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This study examined an active listening counseling chatbot called Miracle Questions for emotional distress in various age groups. It is based on cognitive behavioral therapy (CBT) using text mining. The experiment targeted older people and young university students over three weeks, and changes in the expression of emotions in each generation were analyzed. The results showed qualitative differences in the vocabulary characteristics used by the participants and that their positive emotion-expressive behavior increased more significantly than their negative feelings. This suggests that active listening methods using the Miracle Question are promising for designing a dialogue system suitable for elderly patients. In the future, we will develop a dialogue model that provides topics based on the emotional expressions obtained in this text analysis and further implement this into an active listening counseling robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 151520
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kawasaki",
              "institution": "National Institute of Occupational Safety and Health, Japan Organization of Occupational Health and Safety",
              "dsl": ""
            }
          ],
          "personId": 151510
        }
      ]
    },
    {
      "id": 152472,
      "typeId": 13321,
      "title": "Embo: A Wearable Robot Transforming Child-Directed Verbal Aggression into Tactile Feedback",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1233",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Verbal aggression in child development is a pervasive issue, impacting both victims and perpetrators profoundly. Traditional approaches, such as cognitive education and reactive strategies, lack practicality and intuitive understanding, presenting limitations in addressing this problem effectively. In response, we propose an innovative solution: \"Embo,\" a wearable hand puppet robot that translates verbal aggression intensity into a tactile discomfort, offering a preventative tool for children. This novel intervention aims to mitigate the harmful effects of verbal aggression, contributing to a safer and healthier environment for children.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152273
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151902
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152374
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151874
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152014
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151891
        }
      ]
    },
    {
      "id": 152473,
      "typeId": 13321,
      "title": "A Robot-Administered ICU Confusion Assessment with Brain-Computer Interface Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1112",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "The assessment of patient delirium in hospital Intensive Care Units (ICUs) is a crucial and challenging task, with conventional assessments like CAM-ICU relying largely on verbal and physical communication, making it difficult for patients with limited abilities. To address this, we propose a system that integrates Brain-Computer Interface (BCI) technology and a Socially Assistive Robot (SAR) through brain-controlled mental commands. In a pilot user study, we demonstrate how our system could successfully administer a version of the CAM-ICU to 13 medical professionals and students roleplaying various level of delirium severity. Our work reveals early usability and workload insights, and next steps to improve upon assessment classification accuracy and interaction design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University ",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151716
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152048
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151731
        }
      ]
    },
    {
      "id": 152474,
      "typeId": 13321,
      "title": "Aging with Agency: Real-World Insights into Robot-Assisted Self-Health Monitoring for Older Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1113",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "To enable the successful deployment of autonomously usable health monitoring systems for older adults in uncontrolled, real-life environments, high levels of usability and user experience are essential. \r\nThis work presents the findings of an eight-week field test of a social robot-assisted health monitoring system with older adults in assisted living.\r\nThe results indicate high interest and usability, with users learning to interact with the various system components and gaining a better understanding of the system over time.\r\nWe give a first indication of the prerequisites for the use of such systems in the real world, namely the importance of the involvement of the right target group motivated to use such a system and the participatory design and development of such systems. \r\nFinally, we propose design features that can motivate older adults to use such systems in the long term, to support their self-care and contribute to their health literacy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln - University of Applied Sciences",
              "dsl": "Cologne Cobots Lab"
            }
          ],
          "personId": 151451
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Cologne Cobots Lab"
            }
          ],
          "personId": 151800
        }
      ]
    },
    {
      "id": 152475,
      "typeId": 13321,
      "title": "An Exploration of Mock Scenarios as a Prospective Method for Informing Law and Policy Thinking on Service Robotics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1234",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Modern service robots (such as delivery robots) are becoming more commonplace, it is important to bridge the gap between legal and engineering thinking to be able to propose and implement useful policy. As a step in this direction, our presented work aims to inform lawyers and policy-makers on useful policy thinking surrounding service robots. Specifically, we use a mock robotic technology ordinance and accompanying mock scenarios to assess whether different types of shared context and information seem to influence robot law and policy experts' analysis of ordinances related to robotic technology. The presented work provides preliminary insights on how perceptions and interpretations of robot policy might change across different levels of background grounding and robotics understanding. Our findings can contribute to the ongoing discourse on responsible development and regulation of service robotic systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems"
            }
          ],
          "personId": 152129
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Denton",
              "institution": "University of North Texas",
              "dsl": ""
            }
          ],
          "personId": 151685
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151733
        }
      ]
    },
    {
      "id": 152476,
      "typeId": 13321,
      "title": "A Pilot Investigation of Human Preference for Robot Arm Visual Form",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1235",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "First impressions matter, and a robot's appearance has a significant impact on a user's perception of it, as well as their decision whether to use it.  At the same time, there is a knowledge gap on non-anthropomorphic robot design form's effects on human perceptions and therefore interactions with non-humanoid robots. In this paper, we begin to evaluate the effects of two specific non-anthropomorphic form conditions: robot arm link concavity and roundness. After systematically varying robot arm stimulus models' morphological characteristics, we conducted an online survey-based within-subjects pilot study (N = 10) to gather participant ratings of attributes for each arm. We found that the perceived safety was significantly higher for the most rounded link compared to the intermediately rounded link. Participant free-response feedback can also support future hypothesis generation, such as a possibility that convex forms may seem more humanlike. This work contributes a starting point for future exploration in the realm of non-anthropomorphic robot design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "SHARE Lab"
            }
          ],
          "personId": 151446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Rome",
              "institution": "Berry College",
              "dsl": ""
            }
          ],
          "personId": 151521
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151733
        }
      ]
    },
    {
      "id": 152477,
      "typeId": 13321,
      "title": "Behavioral-economic games with commercially available robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1114",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Behavioral-economic games such as the dictator game have been a popular method for studying human-human interactions which can also be used to study human--robot interaction. In one of the first studies using commercially available robots, we showed participants photographs of 18 robots, and had them play a dictator game against these robots after answering a set of 12 questions regarding each robot's characteristics. Using principal component analysis and linear mixed effects modeling, we found that we could reduce our original 12 robot characteristics to three components---likability for the robot, anthropomorphism, and utility---which individually predicted dictator game offers. The findings are potentially of interest to robot designers and social scientists. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Leiden",
              "institution": "Cognitive Psychology Unit",
              "dsl": "Leiden University"
            }
          ],
          "personId": 151463
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "New Delhi",
              "institution": "Indian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152066
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "New Delhi",
              "institution": "Indian Institute of Technology Delhi",
              "dsl": "Humanities and Social Sciences"
            }
          ],
          "personId": 151644
        }
      ]
    },
    {
      "id": 152478,
      "typeId": 13321,
      "title": "Implementation and Evaluation of a Motivational Robotic Coach for Repetitive Stroke Rehabilitation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1230",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Repetitive, individual exercises can improve the functional ability of stroke survivors over the long term. With the aim of providing extra motivation to adhere to repetitive, individual rehabilitation, this paper presents a robotic coach for stroke rehabilitation. Our system uses the Pepper robot and implements one of twelve data-driven coaching policies. The policies were learned from human-human observations of professional stroke physiotherapists and provide high-level personalisation based on user information and training context. A within subjects evaluation of the system was conducted in-person involving short interactions with 3 stroke survivors. The system was able to engage the target end users and there were indications that decreased workload could be possible when using the system compared to exercising alone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "MACS"
            }
          ],
          "personId": 152252
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152048
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology",
              "dsl": "Interactive Intelligence"
            }
          ],
          "personId": 151549
        }
      ]
    },
    {
      "id": 152479,
      "typeId": 13317,
      "title": "Emotibot: An Interactive Tool for Multi-Sensory Affect Communication",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1030",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Effective emotional communication can have benefits in social interactions between a user and a robot. By developing a tool that can pivot easily between distinct emotional states in response to user presence or input, we have applied a multi-modal method for implementing affect communication in everyday interactions. Our interactive tool engages with users through three emotional avenues and has the potential for usage as an emotional or social support companion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 151465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 152383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 140013
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 140065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississsippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 139769
        }
      ]
    },
    {
      "id": 152480,
      "typeId": 13321,
      "title": "Surveying Adult Perceptions of Privacy and Attitudes towards Social Robots in the Home",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1231",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This work introduces a survey questionnaire about adult perceptions of privacy, attitudes, and comfort with robots in differences spaces in the home. Additionally, in the survey, adult comfort was considered with the preconception that children would share information with robots and other third parties. As for the structure of the survey, it includes likert-style questions, multiple choice, and open responses for qualitative explanations of participant comfort in different situations. In this paper, we give more details about the survey, preliminary qualitative results, and suggestions for further use.  We hope this work brings light to the importance of studying privacy concerns in the home with all family stakeholders. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University - Bloomington",
              "dsl": "Informatics and Cognitive Science"
            }
          ],
          "personId": 139878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": ""
            }
          ],
          "personId": 151526
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151955
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 152481,
      "typeId": 13321,
      "title": "MoBi-LE – A Low-Cost 3D-printable Robot to Educate Children in Waste Disposal",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1110",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In this paper we present the development of a 3D-printable robot\r\nthat is to help children with separating waste. The robot is designed\r\nas a friendly monster that interacts with the children to educate\r\nthem about proper waste disposal. It is a low cost version of a robot\r\nprototype made from papier mâché designed to educate primary\r\nschool children. To make the robot more affordable and easier to\r\nbuild, to deploy in the real world and more widespread, we designed\r\na scaled down 3D-printable version of that prototype. We selected\r\nwidely available components for the inner electronics to make it\r\neasily replicable.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Chair Individual and Technology"
            }
          ],
          "personId": 152198
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            }
          ],
          "personId": 151589
        }
      ]
    },
    {
      "id": 152482,
      "typeId": 13322,
      "title": "Symbiotic Society with Avatars (SSA): Toward Empowering Social Interactions Beyond Space and Time",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1009",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152774
      ],
      "eventIds": [],
      "abstract": "Avatar robots, representations of remote people, help them extend their physical, cognitive, and perceptual capabilities. These avatars can be a range of embodiments from virtual agents to physical robots. With avatar robots, a person (operator) can coexist in multiple locations (beyond space) by controlling multiple avatars simultaneously and move from one place to another without delays (beyond time) by connecting to another avatar in a different location. In the near future, avatar technology would change people’s lives dramatically. However, we face various challenges to reach this future, including uncharted knowledge of social interactions between people and avatar robots, missing standardization on developing robots (robot specs and controls), the lack of related laws and ethical rules, and technical difficulties. With this developing area of human-robot interaction, new key challenges and problems, approaches, and data are rapidly emerging. This workshop would act as a key meeting point to focus this effort and discuss avatar robots and related research and form new collaborations to forge ahead toward symbiotic society with avatars.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": "Social Informatics"
            }
          ],
          "personId": 151924
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "New Brunswick",
              "city": "Fredericton",
              "institution": "University of New Brunswick",
              "dsl": ""
            }
          ],
          "personId": 139814
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 152113
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 139788
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Manitoba",
              "city": "Winnipeg",
              "institution": "University of Manitoba",
              "dsl": ""
            }
          ],
          "personId": 151794
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 151866
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robòtica i Informàtica Industrial, CSIC-UPC",
              "dsl": ""
            }
          ],
          "personId": 151675
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Advanced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 152363
        }
      ]
    },
    {
      "id": 152483,
      "typeId": 13322,
      "title": "Lifelong Learning and Personalization in Long-Term Human-Robot Interaction (LEAP-HRI): Open-World Learning",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24g-1008",
      "source": "PCS",
      "trackId": 12624,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152762
      ],
      "eventIds": [],
      "abstract": "The complex and largely unstructured nature of real-world situations makes it challenging for conventional closed-world robot learning solutions to adapt to such interaction dynamics. These challenges become particularly pronounced in long-term interactions where robots need to go beyond their past learning to continuously evolve with changing environment settings and personalize towards individual user behaviors. In contrast, open world learning embraces the complexity and unpredictability of the real world, enabling robots to be \"lifelong learners\" that continuously acquire new knowledge and navigate novel challenges, making them more context-aware while intuitively engaging the users. Adopting the theme of \"open-world learning\", the fourth edition of the \"Lifelong Learning and Personalization in Long-Term Human-Robot Interaction (LEAP-HRI)\" workshop seeks to bring together interdisciplinary perspectives on real-world applications in human-robot interaction (HRI), including education, rehabilitation, elderly care, service, and companionship. The goal of the workshop is to foster collaboration and understanding across diverse scientific communities through invited keynote presentations and in-depth discussions facilitated by contributed talks, a break-out session, and a debate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152237
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Parthenope",
              "dsl": "Department of Science and Technology"
            }
          ],
          "personId": 151483
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Boston Dynamics AI Institute",
              "dsl": ""
            }
          ],
          "personId": 139764
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 152105
        }
      ]
    },
    {
      "id": 152484,
      "typeId": 13317,
      "title": "HopKeys: Increasing Efficiency and Enjoyability When Typing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1025",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "As the world embraces digital advancements, typing proficiency is increasingly essential for success in education and the workplace. With children beginning to type at younger ages, mastering fundamentals is imperative. Our response is a high-fidelity prototype called HopKeys—a robot frog designed to guide children in proper finger placement. The innovative approach involves associating the colors of letters with corresponding colors on the fingers of a pair of gloves. Our goal is to ensure that HopKeys provides effective and enjoyable experiences for children as they learn to type.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Luddy School of Informatics, Computing, and Engineering"
            }
          ],
          "personId": 151840
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 152342
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Luddy School of Informatics, Computing, and Engineering"
            }
          ],
          "personId": 151697
        }
      ]
    },
    {
      "id": 152485,
      "typeId": 13317,
      "title": "Gobot: A Novel Shoe-Integrated Robot for Enriching Walking Experiences",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1026",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "This paper discusses Gobot, an on-shoe robot companion, aimed at augmenting the walking experience. Gobot utilizes emotional interaction and micro-haptic feedback to add an element of interactivity to everyday walking. Distinct from typical smart shoes that primarily concentrate on health and fitness aspects, Gobot's focus extends to emotional communication and companionship. It features four emotional states—nervous, happy, angry, and normal—which it expresses by adjusting shoelace tightness and light color changes. Additionally, Gobot explores a new method of tactile interaction for foot-based robotics, aiming to expand the application of such technology in enhancing user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152125
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "tsinghua university",
              "dsl": "tsinghua university"
            }
          ],
          "personId": 151514
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "tsinghua university",
              "dsl": ""
            }
          ],
          "personId": 151682
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151624
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151668
        }
      ]
    },
    {
      "id": 152486,
      "typeId": 13317,
      "title": "Development of a Socially Cognizant Robotic Campus Guide",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1021",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "A robotic system to help lost students find their way around a college campus was designed, built, and tested. Socially cognizant design practices, including stakeholder engagement, and interdisciplinary team-building, were practiced. Users can interact with the robot through speech or touchscreen interfaces. The robot can provide verbal instructions on reaching a destination, or can guide the user to the destination, navigating in a socially conscious way. The speech, person detection, and navigation modules perform well in isolation and in concert. Future work includes technical improvements to the person detection and navigation systems, and evaluating social acceptance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Mechanical and Aerospace Engineering"
            }
          ],
          "personId": 151486
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152236
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152278
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152317
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152043
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152306
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 151667
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Piscataway",
              "institution": "Rutgers University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151715
        }
      ]
    },
    {
      "id": 152487,
      "typeId": 13321,
      "title": "Human States and Nonverbal Cues in Multi-party Facilitation: A Statistical Perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1119",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Facilitation robots and agents play a significant role in enhancing the engagement and harmony level during social interactions. However, it is difficult for these intelligent systems to understand human states without humans wearing dedicated sensing devices to measure EEG or ECG responses, thus making implementations in the wild challenging. Non-verbal cues such as body language, gaze interaction, and voice activity could provide promising solutions because they implicitly represent the internal states of humans. This study explores the correlation between human states and nonverbal behaviors, employing a fractional factorial experimental design that considers factors like facilitator type, group size, and stimulus type. Both manual and automated systems were used to annotate video and audio data, capturing body language, gaze, and voice activity. Using physiological data (ECG and EEG), time and frequency domain features were also computed. The MANOVA test suggests statistical significance for all behavioral classes and experimental factors. Notably, facilitator type and upper body movement demonstrated particularly high statistical significance. Such findings are expected to facilitate modeling of human state inference in the wild using only audiovisual data without humans having to wear additional sensors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Saitama",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 152004
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Saitama",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151851
        }
      ]
    },
    {
      "id": 152488,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "A Case for Diverse Social Robot Identity Performance in Education",
      "recognitionIds": [
        10082
      ],
      "isBreak": false,
      "importedId": "hri24h-1104",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "Educational outcomes for students belonging to disadvantaged social identities are unavoidably influenced by overlapping systems of inequity which arise along lines such as gender, ethnicity, and age. Robot platforms such as Furhat require designers to select features which are interpreted by users as these same kinds of social identity. Prior work has posited that social robots might be intentionally designed to leverage these social identities in a \"norm-breaking\" fashion with the aim of disrupting social stereotypes in STEM education. However, research in HRI has been largely limited to the examination of gender only. We present a 2x2, between-subjects study in which 161 participants aged 9-12 are shown a robot-delivered lecture presented by a group of three separate robot personas with varying gender and ethnicity performances. We find that participants place greater trust in the persona groups with high gender diversity. Incorporating ethnic diversity seems to have little impact on our quantitative interaction metrics, however we do find evidence to suggest diversity in robots' language capabilities may be important for trustworthiness. In all, the study contributes nuance to the discussions on the implications of (norm-breaking) social identity performance when using robots to pursue more equitable STEM education.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 152123
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 152322
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Uppsala",
              "institution": "Uppsala University",
              "dsl": "Department of Information Technology"
            }
          ],
          "personId": 140003
        }
      ]
    },
    {
      "id": 152489,
      "typeId": 13321,
      "title": "Challenges and Innovations in Developing Autonomous Robot-Mediated Behavior Coaching Sessions with Haru",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1247",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study presents an empirical investigation into the design and\r\nimpact of autonomous dialogues in human-robot interaction for\r\nbehavior change coaching. We focus on the use of Haru, a tabletop\r\nsocial robot, and explore the implementation of the Tiny Habits\r\nmethod [ 13 ] for fostering positive behavior change. The core of our\r\nstudy lies in developing a fully autonomous dialogue system that\r\nmaximizes Haru’s emotional expressiveness and unique personality.\r\nOur methodology involved iterative design and extensive testing of\r\nthe dialogue system, ensuring it effectively embodied the principles\r\nof the Tiny Habits method while also incorporating strategies for\r\ntrust-raising and trust-dampening. The effectiveness of the final\r\nversion of the dialogue was evaluated in an experimental study\r\nwith human participants (N=12). The results indicated a significant\r\nimprovement in perceptions of Haru’s liveliness, interactivity, and\r\nneutrality. Additionally, our study contributes to the broader un-\r\nderstanding of dialogue design in social robotics, offering practical\r\ninsights for future developments in the field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sønderborg",
              "institution": "University of Southern Denmark ",
              "dsl": ""
            }
          ],
          "personId": 151798
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Saitama",
              "city": "Wakoshi",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151704
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151955
        }
      ]
    },
    {
      "id": 152490,
      "typeId": 13321,
      "title": "Design Principles for Building Robust Human-Robot Interaction Machine Learning Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1126",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Effective collaboration between humans and robots hinges on the robot’s ability to comprehend and adapt to its human teammate. This collaboration demands the development of machine learning models that bridge the gap between human physiological signals and their mental states. However, the challenge lies in developing generalizable machine learning models using data collected in controlled experimental conditions. Prioritizing the dynamics of human-robot interactions in real world conditions allows researchers to create machine-learning models that are effective and adaptable to human-robot teaming in diverse and unpredictable environments. This manuscript proposes a set of principles for designing human subject evaluations, emphasizing the crucial balance between experimental control and ecological validity while also balancing fundamental machine learning trade-offs",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems Institute",
              "dsl": "Oregon State University"
            }
          ],
          "personId": 151908
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems Institute",
              "dsl": "Oregon State University"
            }
          ],
          "personId": 152041
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems Institute",
              "dsl": "Oregon State University"
            }
          ],
          "personId": 151652
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151651
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS/Robotics"
            }
          ],
          "personId": 151604
        }
      ]
    },
    {
      "id": 152491,
      "typeId": 13321,
      "title": "Real-World Implicit Association Task for Studying Mind Perception: Insights for Social Robotics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1248",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In response to the calls for increased integration of implicit measurements in Human-Robot Interaction (HRI) research, the need for studies involving physically present robots, and the broader goal of transitioning from lab experiments to more naturalistic investigations, we present the Real-World Implicit Association Task (RW-IAT). This report outlines the versatile methodology of the RW-IAT; emphasizing its capability to present real-life stimuli and capture behavioral data, including response times and mouse tracking metrics in a controlled manner. Sample analyses, focusing on communicative and noncommunicative actions performed by a human actor and the Pepper robot, reveal significant effects on the Agency and Experience dimensions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Department of Cognitive Sciences"
            }
          ],
          "personId": 152221
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Interdisciplinary Neuroscience Graduate Program"
            }
          ],
          "personId": 152226
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Department of Computer Engineering"
            }
          ],
          "personId": 151967
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "Jagiellonian University",
              "dsl": "Cognitive Science Department"
            }
          ],
          "personId": 151962
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology and Neuroscience"
            }
          ],
          "personId": 152262
        }
      ]
    },
    {
      "id": 152492,
      "typeId": 13321,
      "title": "Using Exploratory Search to Learn Representations for Human Preferences",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1249",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Robots that interact with humans must adapt to the different preferences of human users. However, the time and effort needed for non-expert users to specify their preferences to a robot are a barrier to effective robot adaptation. Better representations of user preferences in the form of learned features have the potential to facilitate robot adaptation. In this work, we propose a method to learn representations using Contrastive Learning from Exploratory Actions (CLEA) that leverages data automatically collected from an interactive signal design processes to better learn user preferences. We show that using data collected automatically from the design process can aid with learning user preferences compared to the alternative of purely self-supervised learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "LOS ANGELES",
              "institution": "University of Southern California",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 152345
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 139976
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 152045
        }
      ]
    },
    {
      "id": 152493,
      "typeId": 13321,
      "title": "Predicting Human Teammate’s Workload",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1123",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "High pressure environments (e.g., disaster response) can result in variable workload that decreases human performance, and degrades the overall mission performance of human-robot teams. Preemptive human workload prediction enables the robot to adapt its behavior or the mission plan as a means of optimizing human performance. However, state-of-the-art workload prediction research only predicts cognitive workload for only a maximum of five seconds into the future. An approach for addressing this research gap is to employ multi-step prediction in additional workload components (e.g., cognitive, auditory, speech, visual, gross motor, fine motor, tactile).\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151651
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS/Robotics"
            }
          ],
          "personId": 151604
        }
      ]
    },
    {
      "id": 152494,
      "typeId": 13321,
      "title": "CognitiveDog: Large Multimodal Model Based System to Translate Vision and Language into Action of Quadruped Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1124",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper introduces CognitiveDog, a pioneering development of quadruped robot with Large Multi-modal Model (LMM) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. The system was realized on Unitree Go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, analyzing its surroundings and independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. These tasks, often composite and implicitly defined, do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. The article delves into the intricacies of this system, detailing the training recipe for the model, dataset characteristics, and the software architecture. Key to this development is the robot's proficiency in navigating space using Visual-SLAM, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. Experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in complex, real-world applications. The dataset used to fine tune the robot-dog behavior generation model is provided at the following link: [Link omitted for submission].",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": "Intelligent Space Robotics Laboratory"
            }
          ],
          "personId": 151909
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Science and Technology",
              "dsl": "Intelligent Space Robotics Laboratory"
            }
          ],
          "personId": 151476
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Science and Technology",
              "dsl": "Intelligent Space Robotics"
            }
          ],
          "personId": 151948
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech ",
              "dsl": "Intelligent Space Robotics "
            }
          ],
          "personId": 151969
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow ",
              "institution": "Skoltech",
              "dsl": ""
            }
          ],
          "personId": 151951
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "Moscow",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Technology ",
              "dsl": "Intelligent Space Robotics(ISR) Lab"
            }
          ],
          "personId": 152271
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": "Intelligent Space Robotics Lab"
            }
          ],
          "personId": 151655
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skoltech",
              "dsl": ""
            }
          ],
          "personId": 151974
        },
        {
          "affiliations": [
            {
              "country": "Russian Federation",
              "state": "",
              "city": "Moscow",
              "institution": "Skolkovo Institute of Science and Technology",
              "dsl": "Intelligent Space Robotics Laboratory"
            }
          ],
          "personId": 151498
        }
      ]
    },
    {
      "id": 152495,
      "typeId": 13321,
      "title": "Getting closer to real-world: monitoring humans working with collaborative industrial robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1245",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Detecting behavior and cognitive states of operators collaborating with industrial robots in the field is among the primary objectives and challenges in current human-robot collaboration research. To achieve this goal, it is first essential to introduce dynamic elements of the industrial settings into the lab to examine how to effectively detect, read, and interpret operators' psychophysical reactions associated with complex and dynamic workflows. As a first step toward this goal, we designed a realistic collaborative assembly task, encompassing common manufacturing operations performed jointly by humans and cobots. The task involves sequential sub-operations in a practical workflow (manual screwing, pick-and-place, supported screwing) and dual-tasking simulating conditions of fatigue and high workload. Early results cover users’ performance, mental workload, and affective reactions, along with acceptance, engagement, and participants’ perception of the experimenter’s influence on the task execution. We finally discuss how such an experimental approach represents a first step toward field-based interventions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "University of Padova",
              "dsl": ""
            }
          ],
          "personId": 151650
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "Università degli Studi di Padova",
              "dsl": ""
            }
          ],
          "personId": 151541
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "Università degli Studi di Padova",
              "dsl": ""
            }
          ],
          "personId": 151700
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "Università degli Studi di Padova",
              "dsl": ""
            }
          ],
          "personId": 151918
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "University of Padova",
              "dsl": "Department of General Psychology"
            }
          ],
          "personId": 152017
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Padova",
              "institution": "University of Padova",
              "dsl": "Human Inspired Technology Research Centre HIT, Department of General Psychology"
            }
          ],
          "personId": 151478
        }
      ]
    },
    {
      "id": 152496,
      "typeId": 13321,
      "title": "DRAWBOT: Making Everyday Objects Interactive",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1246",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "There are few horrors as terrible as waiting, be it at a coffee shop or in line. Often one wonders, why am I not being provided engaging and interactive experiences that could make this time productive, enjoyable, or interesting. In an era dominated by technological advancement, perhaps \"Making Everyday Objects Interactive\" will assist in converting routine events into captivating and unforgettable moments. What are compelling responses to the challenge of enhancing everyday encounters? To answer many such questions, we introduce DRAWBOT (a.k.a., Drawing Robotic Assistant for the Withering of Boredom Or Tediousness).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "OREGON STATE UNIVERSITY",
              "dsl": ""
            }
          ],
          "personId": 151462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State",
              "dsl": "Robotics"
            }
          ],
          "personId": 151554
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 139902
        }
      ]
    },
    {
      "id": 152497,
      "typeId": 13321,
      "title": "Generating Relevant Referring Expressions with GAIA: a Givenness Advised Incremental Algorithm",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1125",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Referring Expression Generation (REG) is the language generation task of selecting attributes to refer to a target entity. While REG is well-studied in linguistics, its introduction into robotics brings new challenges. For real-world robotic environments, robots may have access to a multitude of irrelevant objects that exist outside the scope of conversation, and traditional REG disambiguates the target referent from all other entities, regardless of relevance. While some newer REG methods take relevance into consideration, they are largely limited to potential referents that are part of the same conversation.  In this work, we propose using cognitive statuses to inform the relevance of each entity for REG, narrowing down possible distractors based on cognitive relevance introducing our Givenness Advised Incremental Algorithm (GAIA) which leverages cognitive status for REG. This allows a flexible and enhanced REG, accounting for the context of entities both inside a conversation and within the larger scale environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152498,
      "typeId": 13321,
      "title": "Human-Robot Action Teams: How Robots Can Be Proactive Teammates",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1240",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "People coordinate in action teams to accomplish shared goals in safety- and time-critical contexts such as healthcare and firefighting. Operational failures in these teams can cost lives. Therefore, in our work, we explore how robots that take initiative to support team and task success can augment action teams. We designed and studied proactive robot behaviors in non-dyadic (three humans and a robot) action teams to better understand factors that support the acceptance and effectiveness of proactive robots. Our work will support better human-robot teaming in action teams by addressing gaps in how robot initiative impacts team dynamics, preferences for proactive robot behaviors, and how proactive robots can take initiative based on task states to empower their teammates.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UCSD",
              "dsl": ""
            }
          ],
          "personId": 151747
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UCSD",
              "dsl": ""
            }
          ],
          "personId": 152157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UCSD",
              "dsl": ""
            }
          ],
          "personId": 151959
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Penn State University ",
              "dsl": ""
            }
          ],
          "personId": 151826
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 152499,
      "typeId": 13321,
      "title": "Investigating the Mere Exposure Effect in Relation to Perceived Eeriness and Humaneness of a Social Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1242",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This paper presents an empirical investigation of the perceived eeriness and perceived humaneness of a social robot, taking into consideration the Mere Exposure Effect which postulates that the repeated exposure to a stimulus leads to an improved attitude towards it. We thus assumed that perceived eeriness of the robot is rated lower after several exposures to the robot than after a single exposure. We further assumed that the perceived humaneness of the robot does not change with the number of exposures. In an online study containing four videos of the same robot, we assessed participants ratings in a within-participants design. Our empirical results showed that participants perceived the social robot as significantly less eerie as expected, but contrary to our expectations also less human after it has been presented several times. Notably, participants who had never interacted with the robot before perceived it as significantly less eerie after four exposures, suggesting that the Mere Exposure Effect seems to apply to social robots after more exposures compared to the suggested number of three exposures in a previous meta review. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction, University of Wuerzburg",
              "dsl": ""
            }
          ],
          "personId": 151898
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wuerzburg",
              "institution": "Human-Computer Interaction, University of Wuerzburg",
              "dsl": ""
            }
          ],
          "personId": 151620
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Julius-Maximilians-University Würzburg",
              "dsl": ""
            }
          ],
          "personId": 139885
        }
      ]
    },
    {
      "id": 152500,
      "typeId": 13321,
      "title": "Perspectives on Level of Autonomy Decisions in Space Robotics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1121",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The use of robotics in space exploration and space sustainability has become increasingly more prevalent in recent years. Aerospace contexts pose unique challenges to both robotic capabilities as well as human operator control as these robots often operate in safety-critical situations, unknown environments, and with significant communication latency to Earth. There exist both advantages and potential risks to increased levels of autonomy in these contexts. Therefore, this paper aims to elucidate perspectives on the future role of human operators and the trade-offs when deciding on the level of autonomy for a system. To investigate these perspectives, we conducted qualitative interviews with five professionals in the space robotics industry. Our findings show that, in addition to straightforward technical considerations, financial concerns, operators' willingness to accept new technology, and even humans' emotional experiences during missions will likely play a role in the future of shared control in space robotics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 152357
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152501,
      "typeId": 13317,
      "title": "HugBot: Designing a Persuasive Robot for Smartphone Addiction Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1035",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "The convenience of mobile applications and social networking services in our daily lives has caused an increase in addiction to mobile devices, posing many adverse effects. While there are many products on the market that combat this addiction using coercion, there have been few designed using the conscious abilities of persuasion. In this paper, we develop a persuasive robot, ‘HugBot,’ to help control smartphone overuse. HugBot responds accordingly in verbal and non-verbal cues to the user’s actions, discouraging cellphone use. Furthermore, we hope to discover new paradigms for using stuffed-toy-based persuasive robot systems in user-centric addiction control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": ""
            }
          ],
          "personId": 152232
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": ""
            }
          ],
          "personId": 152062
        }
      ]
    },
    {
      "id": 152502,
      "typeId": 13317,
      "title": "Mechatronic System that Amplifies the Visual and Auditory Signals of Ornamental Plants to Generate Care Habits",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1037",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "This paper addresses the contemporary neglect of ornamental plant care due to the growing disconnection with nature. Motivated by recent findings on plants emitting stress signals through aerial sounds, we propose a mechatronic system for non-invasive plant monitoring. Beyond signal measurement, our solution integrates Human-Computer-Plant Interaction (HCPI) \\cite{Kobayashi2015} to establish a deeper user-plant connection. The framework introduces plant health and emotional states, guiding expressive system movements to effectively communicate information. This fusion of technology, HCPI, and environmental responsibility aims to heighten awareness and care for ornamental plants in today's society.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontificia Universidad Catolica del Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152117
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontificia Universidad Catolica del Peru",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 152101
        }
      ]
    },
    {
      "id": 152503,
      "typeId": 13317,
      "title": "Softy's Magic Touch: Altering Old Toys into Interactive Friends!",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1034",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "The phenomenon of children quickly losing interest in their toys, leading to the accumulation and waste of toys is commonly seen. To address this issue, we have developed \"Softy\" – a modular interactive kit designed for plush toys. Softy comprises various modules and three interaction modes, allowing children to install it on old toys, providing them with the ability to perceive the environment and interact with children. This study aims to rekindle children's interest in old toys by enhancing their interactivity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152179
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Tsinghua University"
            }
          ],
          "personId": 151797
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151515
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Academy of Arts & Design",
              "dsl": "Tsinghua University"
            }
          ],
          "personId": 151759
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151674
        }
      ]
    },
    {
      "id": 152504,
      "typeId": 13321,
      "title": "Allybot: Design Studio to Enhance Girls’ Participation in Technology and Art",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1009",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study introduces AllyBot, an educational robot tailored for early adolescent girls in Peru, aiming to empower them in the domains of Technology and Art. With a focus on collaborative learning, AllyBot enhances the quality of education and contributes to gender equity by providing progressive experiences that foster self-esteem and confidence. The proposed approach incorporates group activities, personalized interactions, and positive feedback mechanisms, creating a supportive environment aligned with the National Curriculum of Peru. This research emphasizes the role of AllyBot in shaping an inclusive educational landscape, fostering a more equitable and enriched learning experience for Peruvian girls.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "-- seleccione una --",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152229
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "-- seleccione una --",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Art and Design"
            }
          ],
          "personId": 151567
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152097
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "-- seleccione una --",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152101
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "-- seleccione una --",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Psychology"
            }
          ],
          "personId": 152290
        }
      ]
    },
    {
      "id": 152505,
      "typeId": 13317,
      "title": "MiRODES: Mini Intelligent Robot for On-campus Domain-specific Event Support",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1039",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Life with robots in everyday life is no longer a picture of science fictions. Robots are expected to interact with people in more natural, real-world contexts, such as crowded indoor spaces beyond controlled environments. This paper introduces our ongoing project of building a robot tour guide that aims to cope with crowds of visitors during on-campus recruiting events. We present a user interface that can effectively interact with guests while answering domain specific questions. Additionally, we discuss challenges in our robot system design with the aim of safely navigating through crowded indoor environments. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Ewing",
              "institution": "The College of New Jersey",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151912
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Ewing",
              "institution": "The College of New Jersey",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151659
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Ewing",
              "institution": "The College of New Jersey",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152282
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Ewing",
              "institution": "The College of New Jersey",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151717
        }
      ]
    },
    {
      "id": 152506,
      "typeId": 13321,
      "title": "Introducing a new instrument for designing HRI dialogues at dialogue-based co-design workshops",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1258",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "For the Human-Robot Interaction to be more effective the dialogues must be centred around the user, alongside their preferences and needs. However, many interactions are being created by designers who can only foresee the possible use cases by using emphatic approaches which have a limiting nature of perceiving the situation firsthand. To avoid this limitation, our study proposes a model which can be used to create HRI dialogues with the inclusion of actual or potential users. To ensure the validity and effectiveness of this model, the model was tested by measuring the workshop’s outcomes with a wider pool of potential users. Our paper aims to contribute to the HRI field by introducing an instrument to facilitate dialogue-based workshops to promote user-centred HRI interactions. This paper is explorative research where the dialogue creation will be facilitated by our own developed Co-design workshop and its exercises. \r\nOur findings surrounding dialogue development in co-design have led us to create a clear set of guidelines which we named Dialogue’s Flower Model. The support base of this model relies on themes, scenarios and storyboarding. It is later enriched by roleplaying. The flower’s blooms contain a dialogue’s utterances and intonation the interaction’s context and levels, all adapted to the robot’s chosen personality type.  \r\nTo measure the effectiveness of the instrument we evaluated its efficiency based on the relevancy of the results which contributed to the quality of generated dialogues.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kolding",
              "institution": "University of Southern Denmark",
              "dsl": "Graduate 2023"
            }
          ],
          "personId": 152343
        }
      ]
    },
    {
      "id": 152507,
      "typeId": 13321,
      "title": "Influencing Human Performance: Investigating the effect of non-humanoid robot feedback on task performance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1138",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper reports the findings of an online between subject study that investigated the effectiveness of a non-humanoid socially assistive robot in providing positive reinforcement feedback to aid in improving performance on a cognitively demanding task. Four different feedback conditions were used, including verbal, expressive, neutral, and text-based feedback, to identify which type of feedback could positively influence behaviour. Results showed no significant differences in task performance, perceived workload or robot perception.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": ""
            }
          ],
          "personId": 152053
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 152215
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of the West of England",
              "dsl": "Bristol Robotics Laboratory"
            }
          ],
          "personId": 152075
        }
      ]
    },
    {
      "id": 152508,
      "typeId": 13321,
      "title": "Can you let me go? Exploring Delivery Robots’ Verbal Response to Physical Bullying and its Impact on Perceived Safety, Comfort, Acceptability, and Existence Acceptance by Co-Present Individuals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1139",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Currently, we are observing the growing numbers of autonomous delivery robots, with the future promising even more sophisticated systems in this domain. These advancements hold the potential to unlock new opportunities, but they also bring forth varied reactions and perceptions, including a concerning issue termed \"robot bullying.\" This online study aims to investigate how different responses from delivery robots (i.e., polite, assertive, or threatening) affect pedestrians’ perceptions of safety, comfort, acceptability, and the existence acceptance of delivery robots. We conducted a between- subject online study, which involved 93 participants exposed to various robot reactions, such as no response, polite requests, as- sertive commands, and threats, in response to bullying behavior. The results showed that despite what kinds of reactions the robot shows, giving a response works better than giving no response to stop bullying behaviors. Additionally, this study sheds light on designing autonomous delivery robots regarding bullying situations, offering recommendations backed by qualitative data from the experiment while also outlining directions for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Chair Individual and Technology"
            }
          ],
          "personId": 152096
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Aachen",
              "institution": "RWTH Aachen University",
              "dsl": "Individual and Technology"
            }
          ],
          "personId": 139851
        }
      ]
    },
    {
      "id": 152509,
      "typeId": 13321,
      "title": "Exploring the Influence of Co-Present and Remote Robots on Persuasiveness and Perception of Politeness",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1019",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Politeness is a crucial aspect of human social interactions. While the influence of politeness is well understood in human groups, it remains underexplored in group interactions with robots. Therefore, in this paper, we explore the influence of the presence of humanoid robots on their persuasiveness and perceived politeness in small groups. We conducted a user study (N = 119) with co-present and remote robots that invited participants to join the group using six politeness behaviors derived from Brown and Levinson's politeness theory. It requests participants to join them at the furthest side of the group, even though a closer side is also available to them, but would ignore the robot's request. The results show that co-present robots are perceived to be less persuasive than remote ones. However, co-presence enhances the clarity of the robot's requests and the perceived freedom of action while decreasing the perceived friendliness and offensiveness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "School of Electrical Engineering and Computer Science (EECS)/Department for Computational Science and Technology (CST)"
            }
          ],
          "personId": 151516
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151994
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Embodied Social Agents Lab"
            }
          ],
          "personId": 151481
        }
      ]
    },
    {
      "id": 152510,
      "typeId": 13321,
      "title": "Improving visual perception of a social robot for controlled and in-the-wild human-robot interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1133",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Social robots, such as those serving in the hospitality sector, often rely on visual perception to understand their users and the environmental context. Recent advancements in data-driven approaches for computer vision have demonstrated great potentials for applying these deep-learning models to enhance a social robot’s visual perception capabilities. However, as deep-learning models often incur high computational costs compared to knowledge-driven, shallow-learning models (e.g., decision trees), it is unclear how will the objective interaction performance and subjective user experience be influenced when a social robot adopts a deep-learning based visual perception model, especially under the complexity of real-world interaction. To answer these questions, we employed SOTA human perception and tracking models to improve the visual perception function of the Pepper social robot. We conducted evaluation in a controlled lab study and an in-the-wild human-robot interaction study to evaluate this novel perception function for following a specific user when other people are present in the scene.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Vision & Learning For Autonomous AI Lab"
            }
          ],
          "personId": 151587
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Engineering"
            }
          ],
          "personId": 151720
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Department of Data Science and AI"
            }
          ],
          "personId": 151550
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Department of Data Science and AI"
            }
          ],
          "personId": 151546
        }
      ]
    },
    {
      "id": 152511,
      "typeId": 13321,
      "title": "In Gaze We Trust: Comparing Eye Tracking, Self-report, and Physiological Indicators of Dynamic Trust during HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1254",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Technical advances in shared-space collaborative robotics have placed recent attention on trust in robots to ensure operator safety as well as to optimize human-robot interactions (HRI). Commonly measured using self-reports, our study explores if eye tracking or physiological indicators offer greater sensitivity in capturing dynamic trust during HRI. We investigated operators’ trust dynamics (i.e., trust build early, trust build late, breach, repair) across 2 different robot reliability levels (100% and 76% reliability). Both the trust ratings, fixation counts, and gaze transition entropy changed significantly between the late trust build and trust breach phases, while heart rate features did not change between any dynamic trust phases. Subjective trust did not change between early and late trust build or between breach and repair phases, however, stationary gaze entropy and gaze transition entropy changes across these phases were found to be sex-specific. Eye-tracking measures have the potential to complement subjective trust ratings to uncover dynamic trust across diverse demographics during HRIs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin Madison",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 152092
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M Univeristy",
              "dsl": ""
            }
          ],
          "personId": 151906
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin Madison",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151737
        }
      ]
    },
    {
      "id": 152512,
      "typeId": 13321,
      "title": "Exploring Preferences in Human-Robot Navigation Plan Proposal Representation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1013",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In this paper, we present our focus on unraveling the intricacies of plan negotiation in human-robot collaborative navigation (HRCN) through a comprehensive exploration of human preferences over robot proposals in search tasks. Via online survey data, we explore the multidimensional landscape of diverse plan representations, negotiation contexts and negotiation domains. Our study seeks to identify the crucial factors that exert a significant influence over human perception, shedding light on the dynamic interplay between humans and robots and contributing valuable insights to advance the understanding of effective navigation plan negotiation strategies in human-robot teams (HRT).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robòtica i Informàtica Industrial (CSIC-UPC)",
              "dsl": ""
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Politècnica de Catalunya (UPC)",
              "dsl": ""
            }
          ],
          "personId": 151890
        }
      ]
    },
    {
      "id": 152513,
      "typeId": 13321,
      "title": "On the Pitfalls of Learning to Cooperate with Self Play Agents Checkpointed to Capture Humans of Diverse Skill Levels",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1255",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "When engaging in collaborative tasks with unknown team members, humans demonstrate the ability to predict the behavior of their partners and adapt to it. Autonomous agents do not exhibit such adaptability, often struggling to integrate with new partners in multi-agent cooperative scenarios. Past work towards tackling this problem includes sampling from a population of diverse training partners. This consists of self-play agents at various skill levels, generated by checkpointing at various points throughout their training. In this work, we show that such a set of agents isn’t representative of human skill levels by evaluating their qualitative and quantitative performance on the Overcooked Domain. Our results demonstrate that self-play agents exhibit distinct learning patterns in contrast to humans and a partially trained self-play agent demonstrates behavior that diverges significantly from that of a lower-skilled human counterpart.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tem",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            }
          ],
          "personId": 151935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and Augmented Intelligence"
            }
          ],
          "personId": 152087
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing, Informatics and Decision System Engineering"
            }
          ],
          "personId": 152010
        }
      ]
    },
    {
      "id": 152514,
      "typeId": 13321,
      "title": "Design of social features for robot-mediated cross-cultural interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1134",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In this paper, we design a hierarchical set of socio-emotional features to understand the effect of cross-cultural mediation facilitated by a social robot between two remote groups of schoolchildren in Japan and in Australia. We also equip the robot with behaviors that maximize children's participation and stimulate socio-emotional interaction which in turn increases the chance of observing the designed features. Differences were observed between the cultures (AUS and JP), suggesting the need of future adaptation of the framework to different cultures as the pilot is expanded to more countries. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": "Research Division"
            }
          ],
          "personId": 151725
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151955
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "School of Design"
            }
          ],
          "personId": 152110
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Seville",
              "institution": "Universidad Pablo de Olavide",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 151726
        }
      ]
    },
    {
      "id": 152515,
      "typeId": 13321,
      "title": "DualTake: Predicting Takeovers across Mobilities for Future Personalized Mobility Services",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1256",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "A hybrid society is expected to emerge in the near future, with different mobilities interacting together, including cars, micro-mobilities, pedestrians, and delivery robots. People will have more chances to utilize multiple types of mobilities in their daily lives. As automation in vehicles advances, driver modeling becomes popular to provide personalized and intelligent services. Thus, modeling driver across mobilities would pave the road for future society mobility-as-aservice, and it is particularly interesting to predict driver behaviors in newer mobilities with data from more traditional mobilities. In this work, we present takeover prediction on a micro-mobility, with the data collected from car simulation. The promising performance from the machine learning model demonstrate the feasibility of driver modeling across mobilities, as the first in the field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Honda Research Institute, USA Inc",
              "dsl": ""
            }
          ],
          "personId": 151757
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Honda Research Institute USA, Inc.",
              "dsl": ""
            }
          ],
          "personId": 151575
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Honda Research Institute USA, Inc.",
              "dsl": ""
            }
          ],
          "personId": 151740
        }
      ]
    },
    {
      "id": 152516,
      "typeId": 13321,
      "title": "A Human-centered Evaluation of Visualization Techniques for Teleoperated Assembly Tasks for Non-expert Users",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1135",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "This study compares two visualization methods of a teleoperation setup for assembly tasks with respect to the performance and workload of human operators. In the baseline condition, participants had an unmediated direct view of the remote location. In the camera condition, visualization was mediated by a live video stream. The study recruited a total of 42 participants. In a between-subjects study design, we evaluated the operator performance of a teleoperated (dis)assembly task and subjectively and objectively compared the workload that the two different visualization methods for (dis)assembly tasks put on its operators. Performance was measured by task completion time, workload was subjectively measured by NASA TLX, and objectively by a secondary task (n-back task). The results show that indirect visualization by a video stream leads to significantly lower performance with respect to task completion times. However, no significant differences were found in subjective or objective workload measurements. The results of the evaluations emphasize the importance of system evaluation in specific use cases and aid in the development of intuitive and efficient human-robot interfaces for teleoperated assembly tasks for non-expert users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Garching b. München",
              "institution": "Technical University of Munich",
              "dsl": "Chair of Ergonomics"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Munich Institute of Robotics and Machine Intelligence (MIRMI)",
              "dsl": ""
            }
          ],
          "personId": 151736
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Garching",
              "institution": "TUM",
              "dsl": ""
            }
          ],
          "personId": 151772
        }
      ]
    },
    {
      "id": 152517,
      "typeId": 13321,
      "title": "Learning to control an Android Robot Head for facial animation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1136",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "An existing approach to evaluate and control facial expressions of an android robot head is applied to a different robot head. Some improvements on how to map facial expressions from human actors onto a robot head are proposed and evaluated using an online survey. While the participants preferred mappings from our proposed approach in most cases, there are still further improvements required.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Hochschule der Medien",
              "dsl": "Humanoid Lab"
            }
          ],
          "personId": 151765
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Hochschule der Medien",
              "dsl": "Humanoid Lab"
            }
          ],
          "personId": 152090
        }
      ]
    },
    {
      "id": 152518,
      "typeId": 13321,
      "title": "Towards a Path Planning and Communication Framework for Seamless Human-Robot Assembly",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1015",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Seamless human-robot collaboration in industrial assembly requires 1) dynamic robot path planning in response to unpredictable human behavior, and 2) communication of the robot’s intentions to the human to avoid surprises. This paper presents initial findings from a study which utilizes communication modalities to convey the robot’s state to humans. Our approach incorporates a motion planner that takes the presence of both humans and the workspace into account. This integration allows the robot to make real-time decisions – it can plan a new path when an obstacle is encountered, or it can temporarily pause until the obstacle clears. The robot’s decisions are communicated to the human either by light, sound, or images+text. Preliminary results of our study show no significant difference in execution time and cognitive workload between the three communication modalities.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151565
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 152122
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151822
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Alabama",
              "city": "Auburn",
              "institution": "Auburn University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 151618
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151999
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151728
        }
      ]
    },
    {
      "id": 152519,
      "typeId": 13321,
      "title": "Interdisciplinary Requirements Engineering for deploying Social Robots in Public Library’s",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1250",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Use of social robots can enhance efficiency, engagement, and customer service of municipalities, but lacks a comprehensive ecosystem. To address this, the following paper explores an interdisciplinary approach to requirements engineering for deploying social robots in the real-world by combining user research with expert assessment of economic factors. We interviewed regular library users and developed a diverse persona set focusing on user experience (UX). Afterwards, a collaborative workshop with experts from the fields of human-computer interaction (HCI) and business administration (BA) was conducted to determine factors of economical validity based on the use case of social robots for customer service in public libraries. First, experts evaluated user personas from a customer experience (CX) perspective, introducing interdisciplinary significance. Then, experts were asked to put themselves in the role of one of two decision-makers to assess what requirements the respective stakeholder would impose in order to agree on the acquisition and use of a social robot. Results yield valuable insights to support and guide the design and deployment of social robots in public libraries with implications extending in relevance to other municipal institutions",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "Ruhr West University of Applied Sciences",
              "dsl": "Institute of Positive Computing"
            }
          ],
          "personId": 152328
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "Institute of Positive Computing",
              "dsl": "Hochschule Ruhr West University of Applied Scienses"
            }
          ],
          "personId": 151648
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "Hochschule Ruhr West",
              "dsl": "Institut Informatik / Institut Positive Computing"
            }
          ],
          "personId": 151647
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "Ruhr West University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 152143
        }
      ]
    },
    {
      "id": 152520,
      "typeId": 13317,
      "title": "Sleep Elf：A Pillow Robot Accompanies Child to Sleep Better",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1050",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Child sleep problems are a significant concern for parents, often persisting from infancy to school-age. These issues not only impact a child's health and development but can also lead to depression and anxiety in parents. In response to this challenge, we have designed a child sleep companion robot aimed at facilitating quick and independent sleep for children. We conducted a detailed product interaction design, and a prototype was built and tested to enhance the intuitiveness of human-robot interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152034
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151778
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152301
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152093
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152279
        }
      ]
    },
    {
      "id": 152521,
      "typeId": 13316,
      "title": "The Use of Conversational Social Robots to Support Ageing and Dementia Care at Home",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1100",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Conversational technology provides an accessible interface for verbal engagement and support in daily living. However, bridging the gap between robotics research for ageing and dementia care and deployment in everyday life remains a significant challenge. My research investigates how conversational social robots can support the independence and well-being of older adults and people living with dementia (PLWD) at home. We analysed interactions with commercial conversational agents in homes with PLWD for 6+ months. We introduced a framework to map behaviour by correlating activities detected by environmental sensors with voice interactions. Using a participatory approach, we underscored key feasibility challenges to support PLWD in different cultural and economic contexts. We discuss future work, which includes promoting and tracking cognitive health by analysing human-robot natural language interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": ""
            }
          ],
          "personId": 151582
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": ""
            }
          ],
          "personId": 151913
        }
      ]
    },
    {
      "id": 152522,
      "typeId": 13321,
      "title": "Design Exploration of Robotic In-Car Accessories for Semi-Autonomous Vehicles",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1131",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This article explores the design of robotic in-car accessories aimed at enhancing the driving experience in semi-autonomous vehicles. Our objective is to empower these artifacts to effectively and pleasantly prepare drivers for handover, thereby strengthening trust between the driver and the vehicle. Our exploration led to the development of robotic rearview mirror ornaments, which utilize movement, voice, and sound effects to provide drivers with supplementary information regarding the likelihood of a handover. This article describes our design journey, encompassing the ideation workshop and exploratory prototyping, and introduces two concepts: Chatty Lips and Turning Trumpet. Our reflection covers lessons learned and areas for improvement while also outlining our plans for future endeavors in this dynamic field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 151706
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 151931
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Dept. of Industrial Design"
            }
          ],
          "personId": 151529
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Dept. of Industrial Design"
            }
          ],
          "personId": 152283
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 151920
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Meguro-ku",
              "institution": "The University of Tokyo",
              "dsl": "DLX Design Lab, Institute of Industrial Science"
            }
          ],
          "personId": 152284
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151603
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Institute of Industrial Science"
            }
          ],
          "personId": 152305
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "Japan Automobile Research Institute",
              "dsl": ""
            }
          ],
          "personId": 152355
        }
      ]
    },
    {
      "id": 152523,
      "typeId": 13321,
      "title": "Conflict Simulation for Shared Autonomy in Autonomous Driving",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1252",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "We present a tool for modeling conflict situations that enables simulation and testing of situation awareness in shared autonomy, in this case in an autonomous driving scenario. The flexibility of the tool allows definition of new conflict situations, integration with various control and conflict detection systems, as well as customization of take-over request signals and different means of communication to the human operator. \r\nWe start with one particular conflict situation - loss of lane markings, for which we demonstrate a simple conflict detection system. We conduct a preliminary user evaluation, which provides useful insights about the usability of the tool. The feedback from the participants indicates that take-over requests without indication feel uncomfortable and providing explicit information about the conflict situation is necessary when switching to manual control is required.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Computer Science"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Freiberg",
              "institution": "Freiberg University of Mining and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151629
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 151956
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Dept of Computer Science"
            }
          ],
          "personId": 152216
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "School of Electrical Engineering"
            }
          ],
          "personId": 151666
        }
      ]
    },
    {
      "id": 152524,
      "typeId": 13316,
      "title": "Robot Companions as Psychosocial Interventions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1101",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "People with cognitive impairments, such as older adults with dementia or children with autism spectrum disorder, can benefit from psychosocial interventions. Such interventions can be therapies or actions used to (re-)integrate a person into society and as of later years - robot companions have become one of the available tools. This doctoral thesis has investigated how robot companions for older adults living in care homes affects both the end users and the staff providing the psychosocial interventions. For older adults, a domestic pet robot can decrease worry, increase person-hood by having something to care for, and even replace sedatives at times, while a humanoid robot might not be as accepted by this user group. For the staff, deployments of robotic welfare technology can lead to technostress and effective usage requires time and motivation, which, for example, decreased during the pandemic. In future research, the use of robot companions for children with autism spectrum disorder during dental visits will be investigated.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": ""
            }
          ],
          "personId": 152308
        }
      ]
    },
    {
      "id": 152525,
      "typeId": 13321,
      "title": "Beyond Dyadic Interactions: Assessing Trust Networks in Multi-Human-Robot Teams",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1132",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Many HRI applications (such as in search and rescue; SAR) require multiple humans to interact with robot agents, making it essential to understand and evaluate both the trust in robots and trust in teams when robots are embedded into such team structures. In the present study, we utilized a virtual urban search and rescue task to compare individual and team trust and associated team performances between (1) all-human and multi-human-robot teams (mHRTs) with reliable robot behavior, and (2) mHRTs with reliable and unreliable robot behaviors. The team structure included a mission specialist (human), a navigator (human or robot), and a safety officer (human). Utilizing apriori pair-wise comparisons, we found that the human navigator was trusted more than the reliable robot navigator by other teammates and that the trust in the robot navigator declined when it performed unreliably. Interestingly, team trust remained comparable between all humans and mHRT (under reliable conditions), but the mHRT team trust levels declined under unreliable robot conditions. Trust between the human dyads was not affected by the actions of the third agent (whether human or robot). Finally, while introducing a reliable robot teammate did not improve team performance, robot unreliability significantly improved performance on the SAR task. The study captures changes in trust networks between human teammates by introducing robots with varying performances.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151954
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin Madison",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151737
        }
      ]
    },
    {
      "id": 152526,
      "typeId": 13321,
      "title": "Expert Perception of Teleoperated Social Exercise Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1253",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Social robots could help address the growing issue of physical inactivity by inspiring users to engage in interactive exercise. Nevertheless, the practical implementation of social exercise robots poses substantial challenges, particularly in terms of personalizing their activities to individuals. We propose that motion-capture-based teleoperation could serve as a viable solution to address these personalization needs. To gather feedback about this idea, we conducted semi-structured interviews with eight exercise-therapy professionals. Our findings indicate that experts' attitudes toward social exercise robots become more positive when considering the prospect of teleoperation to record and customize robot behaviors.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Stuttgart",
              "institution": "Max Planck Institute for Intelligent Systems",
              "dsl": "Haptic Intelligence Department"
            }
          ],
          "personId": 152311
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Max Planck Institute for Intelligent Systems",
              "dsl": "Haptic Intelligence Department"
            }
          ],
          "personId": 151684
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Max Planck Institute for Intelligent Systems",
              "dsl": "Haptic Intelligence Department"
            }
          ],
          "personId": 151676
        }
      ]
    },
    {
      "id": 152527,
      "typeId": 13317,
      "title": "Eye See You: The Emotionally Intelligent Anthropomorphic Robot Enhancing Smartphone Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1048",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Mobile phones have become an indispensable part of people's daily lives, closely connected to emotions and needs, making emotional design for phones increasingly crucial. To expand the functionality of smartphones, we designed ORBO, a robot that features anthropomorphic characteristics with its expressive eyes. ORBO, based on the state of the phone and user behavior, conveys different emotions through eye contact, such as curiosity, joy, sadness, fatigue, and anger, thereby enhancing the interaction between users and phones. Through prototype demonstrations, we showcase ORBO's applications in various scenarios, including daily companionship and entertainment, avoiding excessive smartphone usage, and visualizing device status.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152230
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152111
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152364
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua university",
              "dsl": ""
            }
          ],
          "personId": 151558
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152102
        }
      ]
    },
    {
      "id": 152528,
      "typeId": 13317,
      "title": "Delivery Bot: Enhancing Pedestrian Awareness, Willingness and Ability to Help Delivery Robots Encountering Obstructions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1043",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Delivery robots encounter numerous obstacles, and humans often\r\nfind themselves uncertain about how to respond. Hence, there is a\r\nneed for an inventive approach to improve the interaction between\r\nhumans and autonomous delivery robots as they navigate public\r\nspaces. In this paper, we integrate auditory and visual aids to enable\r\nthe robot to communicate its status and request help. Auditory cues\r\ninform nearby humans, while visual aids like displays and LEDs\r\nimprove transparency. Our objective is to investigate the various\r\nchallenges faced by delivery robots and explore how to enhance a\r\nperson’s ability to perceive and respond to these issues.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151553
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151680
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151767
        }
      ]
    },
    {
      "id": 152529,
      "typeId": 13320,
      "title": "A Lightweight Artificial Cognition Model for Socio-Affective Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1052",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "The software submission presents a fully working artificial cognition model, which controls a NAO social robot. The model was specifically designed to control a socio-affective companion robot for use in a medical setting. It was deployed using embedded hardware: a Raspberry Pi 4B and a Jetson Nano Board, and an external RGB-D camera. Based on the ROS operating system, this software package includes components for social signal processing, behaviour selection, affective behaviour rendering, and a web-based\r\nuser interface. The robot’s behaviours are selected by a planning system, which generates the robot’s behaviours based on the state of the interaction, the progress of the medical procedure, and the user’s affective state. The system has been tested in simulated environments and is currently being used in two clinics to perform a usability test and will subsequently be used to carry out a series of clinical trials.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151756
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151989
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151491
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152257
        }
      ]
    },
    {
      "id": 152530,
      "typeId": 13321,
      "title": "Custom robotic device for quadriplegics rehabilitation enabled by residual muscle activation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1148",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Rehabilitation efforts targeted towards individuals with paralysis\r\nhold considerable potential in enhancing their overall quality of life.\r\nSuch initiatives facilitate a return to employment, foster a greater\r\nsense of bodily comfort, and reduce dependence on caregiver assis-\r\ntance for daily tasks. While recent advancements have led to the\r\ndevelopment of robotic devices for individuals with partial paral-\r\nysis, there remains a limited amount of solutions for individuals\r\nliving with quadriplegia.\r\nThis study presents the design of a custom robotic device specif-\r\nically developed to assist quadriplegics individuals in executing\r\nindependent daily tasks, such as feeding, while using their own\r\nupper limbs. Furthermore, the proposed device aims to leverage\r\nthe user’s residual movements for its activation, thereby contribut-\r\ning to rehabilitation efforts and preserving physical capabilities.\r\nPreliminary studies validate the device’s design while proving the\r\nfeasibility of using electromyography signal to detect muscle con-\r\ntraction activation in quadriplegic individuals.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Interdisciplinary Institute for Technological Innovation",
              "dsl": "Sherbrooke University"
            },
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Research Centre on Aging",
              "dsl": "Sherbrooke University"
            }
          ],
          "personId": 151939
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Research Center of Aging (CdRV), Interdisciplinary Institute for Technological Innovation",
              "dsl": ""
            }
          ],
          "personId": 152188
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Université de Sherbrooke",
              "dsl": "Département de génie électrique et informatique"
            }
          ],
          "personId": 152309
        }
      ]
    },
    {
      "id": 152531,
      "typeId": 13321,
      "title": "Exploring of Discrete and Continuous Input Control for AI-enhanced Assistive Robotic Arms",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1028",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Robotic arms, essential in domestic care for individuals with motor impairments, enable them to perform Activities of Daily Living (ADLs) without relying on human caregivers. These collaborative robots (cobots) require users to manage multiple Degrees-of-Freedom (DoFs) for tasks like grasping and manipulating objects. Traditional input devices, typically limited to two DoFs, necessitate frequent and complex mode switches to control individual DoFs. Modern adaptive controls with feed-forward multimodal feedback reduce the overall task completion time, number of mode switches, and cognitive load. Despite the variety of input devices available, their effectiveness in adaptive settings with assistive robotics has yet to be thoroughly assessed. Investigating this further would aid in determining the optimal balance between manual user input and automated algorithmic assistance. We explore three different input devices by integrating them into an established XR framework for assistive robotics and evaluate them in a preliminary study focusing on empirical findings for future developments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Inclusive Human-Robot Interaction"
            },
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 152338
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Gelsenkirchen",
              "institution": "Westphalian University of Applied Sciences",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 151814
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Inclusive Human-Robot Interaction"
            }
          ],
          "personId": 152203
        }
      ]
    },
    {
      "id": 152532,
      "typeId": 13321,
      "title": "Creating Virtual Patients using Robots and Large Language Models: A Preliminary Study with Medical Students",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1149",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "This paper presents a virtual patient (VP) platform for medical education, combining a social robot, Furhat, with large language models (LLMs). Aimed at enhancing clinical reasoning (CR) training, particularly in rheumatology, this approach introduces more interactive and realistic patient simulations. The use of LLMs both for driving the dialogue, but also for the expression of emotions in the robot's face, as well as automatic analysis and generation of feedback to the student, is discussed. The platform's effectiveness was tested in a pilot study with 15 medical students, comparing it against a traditional semi-linear VP platform. The evaluation indicates a preference for the robot platform in terms of authenticity and learning effect. We conclude that this novel integration of a social robot and LLMs in VP simulations shows potential in medical education, offering a more engaging learning experience. However, further research and expansion of the platform's capabilities are suggested for comprehensive validation and enhancement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Karolinska Institute",
              "dsl": ""
            }
          ],
          "personId": 151662
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Karolinska Institute",
              "dsl": ""
            }
          ],
          "personId": 152183
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 151691
        }
      ]
    },
    {
      "id": 152533,
      "typeId": 13316,
      "title": "Accomplishing Robotic Autonomy: The Complexities of Situated Practices and Agency in The Laboratory",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1071",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Effective ethical interventions in robotic autonomy demand situated understandings of roboticists’ practices that shape the robots. An ethnographic approach focusing on roboticists’ development practices can help build an in-depth understanding of their interactions with the robots that serve to realize robotic autonomy, their motivations and values, as well as the harms derived from specific practices of interaction. Our preliminary work examines the material-discursive practices used to accomplish robotic agency in an engineering research laboratory. Our work reveals the complex dynamics of care that highlights the interactions between the roboticists and the robots. We argue that interventions must acknowledge and engage with technologists’ care to be effective. We hereby propose future work to further our investigation in HRI that occurs beyond the development phase. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin ",
              "dsl": "Communication Studies"
            }
          ],
          "personId": 152191
        }
      ]
    },
    {
      "id": 152534,
      "typeId": 13321,
      "title": "Software Architecture to Generate Assistive Behaviors for Social Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1029",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "To facilitate the design of socially assistive robots (SARs), we present an architecture to generate assistive behavior for social robots given a high level description of the intent of the assistance.  Our approach features an ontology of assistive intents, a hierarchical task network planner, and robot middleware.  We demonstrate the behaviors on two robot platforms and compare the behaviors.  While many of the behaviors are similar, challenges remain in generating behaviors that will be presented consistently across multiple platforms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Lancaster",
              "institution": "Franklin & Marshall College",
              "dsl": ""
            }
          ],
          "personId": 151940
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Lancaster",
              "institution": "Franklin & Marshall College",
              "dsl": ""
            }
          ],
          "personId": 152381
        }
      ]
    },
    {
      "id": 152535,
      "typeId": 13321,
      "title": "Adversarial Robots as Creative Collaborators",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1265",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "This research explores whether the interaction between adversarial robots and creative practitioners can push artists to rethink their initial ideas. It also explores how working with these robots may influence artists’ views of machines designed for creative tasks or collaboration. Many existing robots developed for creativity and the arts focus on complementing creative practices, but what if robots challenged ideas instead? To begin investigating this, I designed UnsTable, a robot drawing desk that moves the paper while a participant draws to interfere with the process. This inquiry invites further research into adversarial robots designed to challenge creative practitioners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "The New School",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "FAR Lab"
            }
          ],
          "personId": 152080
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 151758
        }
      ]
    },
    {
      "id": 152536,
      "typeId": 13321,
      "title": "AntHand: Interaction Techniques for Precise Telerobotic Control Using Scaled Objects in Virtual Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1144",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "This paper introduces AntHand, a set of interaction techniques for enhancing precision and adaptability in telerobotics through the use of scaled objects in virtual environments. \\codename operates in three phases: up-scaling interaction, for detailed control through a magnified virtual model; constraining interaction, which locks movement dimensions for accuracy; and post-editing, allowing manipulation trace optimization and noise reduction. AntHand demonstrates how collaboration between humans and robots can improve precise control of robot actions in telerobotic operations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt University - Flanders Make",
              "dsl": ""
            }
          ],
          "personId": 152261
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt University - Flanders Make",
              "dsl": ""
            }
          ],
          "personId": 151727
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Hasselt",
              "institution": "Flanders Make - Expertise Centre for Digital Media",
              "dsl": "Hasselt University "
            }
          ],
          "personId": 151663
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Hasselt University - tUL - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 151743
        }
      ]
    },
    {
      "id": 152537,
      "typeId": 13321,
      "title": "Considerations for Handover and Co-working with Drones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1024",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Recent progress in aerial robotics foresees that flying robots, a.k.a. drones, can support workers in their jobs, such as by performing complex tasks in hard-to-reach places. As they become increasingly autonomous, we envision co-working drones helping human operators in direct collaborative tasks, such as by carrying tools and handing them over to workers at heights, or helping them lift and precisely position structures on construction sites. Yet, much research is needed to support safe close-body interaction between humans and drones. We here propose specific considerations for human-drone collaboration related to such handover -- from the drone approaching a person in view of interacting with them at close proximity, to the handover itself, and to the drone leaving.\r\nIn addition, we present the results of semi-structured interviews with three professionals in this context of human-drone collaboration. This late-breaking report highlights challenges and opportunities fostered by Human-Aerial Robot Handover (HARH).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Be'er Sheva",
              "institution": "Ben Gurion University of the Negev",
              "dsl": "Magic Lab, Industrial Engineering and Management"
            }
          ],
          "personId": 152030
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ENAC, Université de Toulouse",
              "dsl": ""
            }
          ],
          "personId": 151907
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 152375
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 152201
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "TOULOUSE",
              "institution": "Université de Toulouse, CNRS, UPS",
              "dsl": "LAAS-CNRS"
            }
          ],
          "personId": 152224
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "LAAS-CNRS",
              "dsl": ""
            }
          ],
          "personId": 151823
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ENAC, Université de Toulouse ",
              "dsl": ""
            }
          ],
          "personId": 152269
        }
      ]
    },
    {
      "id": 152538,
      "typeId": 13321,
      "title": "From Human-Human to Human-Robot: How Social Psychology Research Methods Can Inform HRI Evaluation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1145",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study explores the potential use of the video rating method (e.g., thin slices of behavior) commonly used in the social psychology literature (e.g., [9]) in evaluating the quality of human-robot interaction (HRI). Eight independent observers watched 42 video recordings of a participant interacting with a humanoid robot in a \"fast-friendship\" task and rated the warmth of each interaction. Based on the average of the eight ratings, the interactions were classified into “high” and “low” warmth. High-warm participants self-reported feeling more positive intergroup emotions (sympathy and excitement) and stronger approach intentions towards robots, indicating that the warmth score accurately captured the quality of the interaction. This work presents a simple yet sensitive method to incorporate factors to evaluate intergroup attitudes and suggests potential applications of interpersonal research methods in HRI. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "Tuzla/Istanbul",
              "institution": "Sabanci University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 151703
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "Tuzla",
              "institution": "Sabanci University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 152240
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Sabancı University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 151613
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Istanbul",
              "institution": "Sabancı University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 152287
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "Tuzla/Istanbul",
              "institution": "Sabanci University",
              "dsl": "Faculty of Arts and Social Sciences"
            }
          ],
          "personId": 151828
        }
      ]
    },
    {
      "id": 152539,
      "typeId": 13321,
      "title": "Snapbot : Enabling Dynamic Human Robot Interactions for Real-Time Computational Photography",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1025",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Photography remains an expert area requiring right focus, exposure, composition, and even post-processing.\r\nYet, robotic automation can enable precise camera manipulation, focus and exposure adjustment, camera composition, and post-processing by leveraging state-of-the-art computational photography.\r\nExisting proposals for robotic photography focus on adjusting camera angles for static portraits or developing image evaluation metrics, thus falling short in capturing dynamic human-robot interactions. \r\nThis paper describes the design and implementation of SNAPBOT, a human-robot interaction system designed specifically for computational photography.\r\nSNAPBOT dynamically detects face and pose for exposure and focus and interactively controls robot arm for camera composition to perform image scoring and enhancing.\r\nAs perception, control, and computational photography form an end-to-end pipeline, SNAPBOT promises a new future in which image focus, exposure, composition, and generation can be jointly optimized as a unified process.\r\nWe have implemented and deployed SNAPBOT on a UR3 demonstrating the mean image quality score is 1.51X compared to aesthetic visual analysis dataset.\r\nWe also perform ablation study to analyze the impact of each stage of SNAPBOT both visually and quantitatively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 151671
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeongi",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 151722
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 151910
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Gyeonggi",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 151786
        }
      ]
    },
    {
      "id": 152540,
      "typeId": 13321,
      "title": "Who is a better salesperson?: People conform more to a human than robot facilitator in the wild.",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1146",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Robots are being developed as tutors, restaurant staff, nursing assistants, and more. As their presence grows in businesses and other social spaces, it is pertinent to investigate how robot recommendations might influence our opinions and choices? This study examined how people conform to robot recommendations in the real world, using a bake sale or giveaway as a cover. A human or robot facilitator recommended one of two pens and cookies, and we tracked participant choices (N = 100). Results showed that participants were more likely to conform to the human’s recommendation than the robot’s. The low level of conformity to robots indicates that further innovations is necessary to enhance the effectiveness of robots as positive influencers (e.g., when robots recommend healthy diets or track healthy habits).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 151880
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 151863
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 151696
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": ""
            }
          ],
          "personId": 151917
        }
      ]
    },
    {
      "id": 152541,
      "typeId": 13321,
      "title": "How robot comes into our life in Urban Public Space: A participatory study",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1147",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Urban public space is an ideal setting for robot design, with broad application prospects. However, the current consensus of how robots in urban public spaces should interact with people is not yet fully developed. To foster better design of robots in urban public spaces, we organized a user-participatory design workshop. Through video showcases, brainstorming and collaborative design sessions, we delved into users' attitudes and evaluations of existing robot cases. We also explored their expectations and requirements for urban public robots within the scenario of shared most interests, along with individual preferences when conceptualizing robots. Preliminary research findings indicate that users prioritize the practical functionality and emotional connection of robots. Additionally, from the robots designed by users, we identified four common features, offering valuable insights for subsequent research and practical applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": ""
            }
          ],
          "personId": 152000
        }
      ]
    },
    {
      "id": 152542,
      "typeId": 13321,
      "title": "Computational Architecture for Unsupervised Imitation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1268",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This early-stage research work aims to improve online human-robot imitation by translating sequences of joint positions from the domain of human motions to a domain of motions achievable by a given robot, thus constrained by its embodiment. Leveraging the generalization capabilities of deep learning methods, we address this problem by proposing an encoder-decoder neural network model performing domain-to-domain translation. In order to train such a model, one could use pairs of associated robot and human motions. Though, such paired data is extremely rare in practice, and tedious to collect. Therefore, we turn towards deep learning methods for unpaired domain-to-domain translation, that we adapt in order to perform human-robot imitation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "91120",
              "city": "Palaiseau",
              "institution": "Ensta-Paris",
              "dsl": "U2IS"
            },
            {
              "country": "France",
              "state": "F-29238",
              "city": "Brest",
              "institution": "Lab-STICC, UMR CNRS 6285",
              "dsl": "IMT Atlantique"
            }
          ],
          "personId": 151896
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "U2IS, ENSTA Paris, Institut Polytechnique de Paris",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Universite",
              "dsl": "ISIR"
            }
          ],
          "personId": 151497
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "ENSTA Paris",
              "dsl": ""
            }
          ],
          "personId": 152344
        }
      ]
    },
    {
      "id": 152543,
      "typeId": 13320,
      "title": "A Multi-party Conversational Social Robot Using LLMs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1013",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151850
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151799
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151922
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 151804
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Midlothian",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "Interaction Lab, School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151854
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "County (optional)",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152298
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152038
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151683
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152072
        }
      ]
    },
    {
      "id": 152544,
      "typeId": 13321,
      "title": "\"What Will You Do Next?\" Designing and Evaluating Explanation Generation Using Behavior Trees for Projection-Level XAI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1261",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Explainable AI (XAI) is a subfield of human-agent interaction that involves the design and development of methods that generate explanations and exhibit more transparent behavior from AI agents.\r\nIn this work we present three contributions that advance XAI research in the context of Human-Robot Interaction (HRI).\r\nFirst, we extend explanation generation using behavior trees to include projection-level XAI, i.e. the ability to query an agent for explanations on future actions.\r\nSecond, we developed algorithms that answer pre- and post-conditions of an action, which we hypothesize improves comprehension of an agent.\r\nThird, we present an experimental design using a robot arm and GUI to evaluate the efficacy of the explanation generation approach on various levels of user situational awareness, workload, and trust.\r\nAll code is open-source to allow researchers to explore using explanation generation with behavior trees for future human-robot interaction studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": ""
            }
          ],
          "personId": 152185
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "Human/Robotics/Vehicle Integration and Performance Laboratory"
            }
          ],
          "personId": 151964
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": ""
            }
          ],
          "personId": 151701
        }
      ]
    },
    {
      "id": 152545,
      "typeId": 13321,
      "title": "The Conversation is the Command: Interacting with Real-World Autonomous Robots Through Natural Language",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1140",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In recent years, autonomous agents have surged in real-world environments such as our homes, offices, and public spaces. However, natural human-robot interaction remains a key challenge. In this paper, we introduce an approach that synergistically exploits the capabilities of large language models (LLMs) and multimodal vision-language models (VLMs) to enable humans to interact naturally with autonomous robots through conversational dialogue. We leveraged the LLMs to decode the high-level natural language instructions from human users and abstract them into precise robot actionable commands or queries. Further, we utilised the VLMs to provide a visual and semantic understanding of the robot’s task environment. Our results with 99.13% command recognition accuracy and 97.96% commands execution success show that our approach can enhance human-robot interaction in real-world applications. The video demonstrations of this paper can be found at https://osf.io/4qb9u/ and the code is available at our repository.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Leoben",
              "institution": "Montanuniversität Leoben",
              "dsl": "Chair of Cyber-Physical Systems"
            }
          ],
          "personId": 151865
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Leoben",
              "institution": "Montanuniversität, Leoben",
              "dsl": "Chair of Cyber-Physical Systems"
            }
          ],
          "personId": 151761
        }
      ]
    },
    {
      "id": 152546,
      "typeId": 13320,
      "title": "Robots in autonomous buses: Who hosts when no human is there?",
      "recognitionIds": [
        10081
      ],
      "isBreak": false,
      "importedId": "hri24d-1012",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "EECS",
              "dsl": "Division of Speech, Music and Hearing"
            }
          ],
          "personId": 151455
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Integrated Transport Research Lab"
            }
          ],
          "personId": 151560
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": "EECS MID"
            }
          ],
          "personId": 152115
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 151691
        }
      ]
    },
    {
      "id": 152547,
      "typeId": 13321,
      "title": "Mobile Robots Meet Augmented Reality Technologies: Transforming Human-Robot Interaction in Industry 4.0 Scenarios",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1020",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Mobile robots combine digital technologies and automation, enabling a new era of operations in Industry 4.0 scenarios. These robots can navigate through assembly lines and logistics warehouse autonomously, making them catalysts for increased efficiency and  productivity. Despite this, various challenges arise, including maintaining multiple robots up and running over large spaces, handling navigation during unexpected situations, ensuring operators safety, among others. To help overcome these, Augmented Reality (AR), a key pillar of Industry 4.0 can be used to assist with Human-Robot Interaction (HRI), contributing to a more productive and optimized logistics environment. This work proposes a framework for enhancing the understanding of the status of logistics robots through the use of a mobile AR tool. Its goal is to provide operators with an effective way of obtaining more information about the surrounding robots, as well as remote control them. The framework was iteratively tested during the various phases of design and development, allowing to integrate the feedback collected in each stage.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "DETI-IEETA"
            }
          ],
          "personId": 152227
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": ""
            }
          ],
          "personId": 151888
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "DETI-IEETA"
            }
          ],
          "personId": 152313
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": ""
            }
          ],
          "personId": 152006
        }
      ]
    },
    {
      "id": 152548,
      "typeId": 13321,
      "title": "Is Now a Good Time? Opportune Moments for Interacting with an Ikigai Support Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1141",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "One of the questions human-robot interaction (HRI) research needs to address prior to in-home robot deployment is when optimal moments for everyday interaction might occur. These can vary based on robot users' existing routines and personal preferences. As part of a larger project to design a conversational robot that can assist older adults in recognizing and maintaining their ikigai (sense of meaning and purpose in life), we explored the question \"when might be good times for the robot to engage older adults in activities?\". 11 older adults who were familiar with our prototype robot from prior participation in our research took part in a week-long \"diary study\" to identify their habits and preferred times of engagement with the robot. The diary was performed by sending text messages to the older adults twice daily, asking what they were doing at the moment and whether this was a suitable time for interacting with the robot. The findings of the study allowed us to determine optimal times for interaction with the robot' - commonly before and after lunch and before sleep. Insights from this approach contribute to designing robots that can be integrated into the daily lives of older adults.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Informatics"
            }
          ],
          "personId": 139957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 140016
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "Luddy School of Informatics"
            },
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Jackson",
              "institution": "Jackson State University",
              "dsl": "Jackson State University"
            }
          ],
          "personId": 151985
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "Anthropology "
            }
          ],
          "personId": 140025
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        }
      ]
    },
    {
      "id": 152549,
      "typeId": 13321,
      "title": "VR-based Assistance System for Semi-Autonomous Robotic Boats",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1262",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In this paper we present the technical concept for a teleoperation system for semi-autonomous robotic boats using virtual reality. This system can be used for monitoring autonomous driving as well as for direct manual control. The integration of live sensor data is possible as well as the integration of past measurement results and their correct registration within the virtual representation. Initial field tests have shown that the technical concept can be used for the envisaged areas of application, from which specific challenges can be derived that are addressed in this paper.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Freiberg",
              "institution": "Freiberg University of Mining and Technology",
              "dsl": "Computer Science"
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Lund",
              "institution": "Lund University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151629
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Freiberg",
              "institution": "Technical University Freiberg",
              "dsl": "Institute for Informatics"
            }
          ],
          "personId": 151991
        }
      ]
    },
    {
      "id": 152550,
      "typeId": 13321,
      "title": "MOE-Hair: Toward Soft and Compliant Contact-rich Hair Manipulation and Care",
      "award": "BEST_POSTER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1263",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Hair-care robots have the potential to alleviate labor shortages in elderly care and enable those with limited mobility to express their identities through hair styling. However, the proximity to the scalp also makes hair-care tasks difficult for conventional rigid robotic systems. In this work, we highlight two advantages that soft robotic manipulators have in hair-care applications: safety through mechanical compliance and sensing through observing deformation. To demonstrate these advantages, we introduce a soft robotic end-effector which we call Multi-finger Omnidirectional End-effector (MOE) for hair-care applications. We validate that in hair-grasping tasks, MOE exerts 74.1% less force on the head while being able to grasp a similar amount of hair compared to rigid grippers. We further demonstrate that we can reliably estimate the mesh shape of MOE during interaction with a head and that we can infer useful information about the head such as its shape that is occluded by voluminous hair from the observed deformations. The results suggest that soft robots are uniquely advantaged in hair-care tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University ",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 151943
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 151817
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 151447
        }
      ]
    },
    {
      "id": 152551,
      "typeId": 13321,
      "title": "Towards Unlimited Task Coverage and Direct (far-off) Manipulation in eXtended Reality Remote Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1021",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The growing interest in remote collaboration is driven by multiple factors. Advances in digital technology have made it increasingly feasible to collaborate from anywhere, breaking down geographical barriers. Despite the rise of new tools several challenges still exist. These include limitations on the remote member's perspective, confined to what on-site members can capture, as well as potential shortcomings for interacting with on-site physical objects. This position paper discusses the potential of leveraging mobile robots within eXtended Reality (XR) environments to transform how remote collaboration is performed. Mobile robots, equipped with cameras, microphones, articulated arms and various sensors, can serve as physical avatars for remote team members, allowing to navigate through physical spaces, enabling real-time, immersive experiences. Combined with XR tools, these robots may empower on-site and remote collaborators to engage in shared activities. These and other arguments in favor of this position are presented and future directions are proposed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "DETI-IEETA"
            }
          ],
          "personId": 152227
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 152172
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": ""
            }
          ],
          "personId": 152118
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": ""
            }
          ],
          "personId": 151864
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "IEETA / DETI, University of Aveiro",
              "dsl": ""
            }
          ],
          "personId": 152244
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DETI - Department of Electronics, Telecommunications and Informatics"
            }
          ],
          "personId": 151544
        }
      ]
    },
    {
      "id": 152552,
      "typeId": 13321,
      "title": "An Advanced Desktop Humanoid Robot for Emotional Support with Flexible Motion and Artificial Intelligence",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1142",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "In the modern era, a significant number of young individuals, often living alone due to occupational or educational commitments, encounter challenges of loneliness, particularly during prolonged periods of solitude at their workstations. Addressing this issue, we present \"Fibo,\" a desktop robot designed to serve as a companionable figure. Fibo is equipped with the capability to detect and respond to indications of a user’s diminished spirits. Through a variety of actions and voice interactions, Fibo demonstrates empathetic behaviors and exhibits emotional responsiveness, aiming to mitigate feelings of loneliness and assuage negative emotions. This paper presents our inaugural prototype of Fibo, outlines our future objectives, and delves into the creative rationale behind its diverse motion designs. Furthermore, we explore the prospective interactions afforded by desktop humanoid robots, with the aspiration of contributing innovative insights to the desktop robot community.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151790
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151778
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152301
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Changchun",
              "institution": "Jilin University",
              "dsl": ""
            }
          ],
          "personId": 152022
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Jinan",
              "institution": "Shandong University",
              "dsl": ""
            }
          ],
          "personId": 151528
        }
      ]
    },
    {
      "id": 152553,
      "typeId": 13320,
      "title": "Dynamic Spaces with Diverse Furnituroids: Exploring the Synergy of Multiple Shape-Changing Mobile Furniture Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1010",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Fujisawa",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 151928
        }
      ]
    },
    {
      "id": 152554,
      "typeId": 13321,
      "title": "Approaching Future Robot Technologies via Speculative Role-Playing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1143",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Playful research approaches have become more popular in Human-Robot Interaction (HRI). In this paper, a tabletop role-playing game is introduced as a means to explore future robot technologies in a playful, immersive and human-centred approach. The trial study included four participants with expertise in HRI or care technologies to elicit novel application opportunities for robots in home environments. Creating a fictional character as part of the gameplay enabled participants to immerse themselves in their characters and opened up a narrative speculative design space. The gameplay resulted in a deep immersion of the participants and provided insights into potential (robot) technologies supporting daily life activities. All technologies mentioned by participants were collected in a mind map, and from the mind map four fictional robots were created. The fictional robots were also created into a design fiction card deck as a tangible prop to utilize in a future game study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Vienna",
              "city": "Vienna",
              "institution": "TU Wien",
              "dsl": "HCI Group"
            }
          ],
          "personId": 152168
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 151630
        }
      ]
    },
    {
      "id": 152555,
      "typeId": 13316,
      "title": "The Question Is Not Whether; It Is How!",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1074",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "This submission explores the implications of robot embodiment in language learning. Through various innovative studies, it investigates how factors tied to robot usage, such as personality characteristics and learning settings, influence learner outcomes. It incorporates advancements in artificial intelligence by utilising large language models and further contributes to pivotal understanding through a planned longitudinal study in the migrant context. Lastly, an intensive speech analysis further examines the specifics of robot-human interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Division of Speech, Music and Hearing"
            }
          ],
          "personId": 151689
        }
      ]
    },
    {
      "id": 152556,
      "typeId": 13320,
      "title": "KoPro - Configurable Process Chains for Human-Robot Collaborative Assembly",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1015",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 152122
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151565
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151591
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151915
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg-Schweinfurt",
              "institution": "Technical University of Applied Sciences",
              "dsl": "IDEE"
            }
          ],
          "personId": 151999
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Schweinfurt",
              "institution": "University of Applied Sciences Würzburg-Schweinfurt",
              "dsl": "IDEE"
            }
          ],
          "personId": 151728
        }
      ]
    },
    {
      "id": 152557,
      "typeId": 13316,
      "title": "Building Each Other Up: Using Variation Theory to Build and Correct Human Mental Models of Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1075",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "As we begin to live and work in the same environments as robots, we must be able to anticipate each other’s future actions to avoid dangerous or inefficient situations. How can we build up human and robot conceptual models to predict how each other will behave? My current work uses human-concept learning theories such as variation theory to teach users how their robot will behave. I have found that showing contrasting examples of policies when learning about robot motions improves people’s ability to predict motion in novel settings. For my proposed PhD, I seek to integrate this knowledge of how to improve a human’s mental model of the robot with enabling the robot to correct the human’s model or alter its own behavior based on its model of the short-comings in the human’s understanding.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Wellesley",
              "institution": "Wellesley College",
              "dsl": ""
            }
          ],
          "personId": 152246
        }
      ]
    },
    {
      "id": 152558,
      "typeId": 13320,
      "title": "Robots for Humanity: In-Home Deployment of Stretch RE2",
      "recognitionIds": [
        10079
      ],
      "isBreak": false,
      "importedId": "hri24d-1014",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152332
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Martinez",
              "institution": "Hello Robot Inc.",
              "dsl": ""
            }
          ],
          "personId": 139740
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Robots for Humanity",
              "dsl": ""
            }
          ],
          "personId": 152068
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Robots for Humanity",
              "dsl": ""
            }
          ],
          "personId": 151803
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Martinez",
              "institution": "Hello Robot",
              "dsl": ""
            }
          ],
          "personId": 152197
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": ""
            }
          ],
          "personId": 151557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": "Department of Applied Health Sciences"
            }
          ],
          "personId": 151442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Martinez",
              "institution": "Hello Robot",
              "dsl": ""
            }
          ],
          "personId": 152167
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Martinez",
              "institution": "Hello robot",
              "dsl": ""
            }
          ],
          "personId": 152151
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152299
        }
      ]
    },
    {
      "id": 152559,
      "typeId": 13321,
      "title": "Deploying a Robotic ride-on Car in the Hospital to Reduce the Stress of Pediatric Patients before Surgery",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1260",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Pediatric patients often experience stress before surgery, which im- pacts their overall well-being. Our work introduces a novel robotic ride-on car designed to alleviate preoperative stress in pediatric patients aged 3-10. Inspired by assistive driving technologies, the car incorporates sensors and intervention mechanisms that can be modulated manually by a human or automatically by an AI. We performed a first user study at Anonymous Hospital to make a first assessment of the impact of the ride-on robotic car’s on ob- served and reported stress, by collecting feedback from tutors and healthcare professionals. Results indicate the car’s potential as a technology-based intervention for preoperative stress reduction. The paper also discusses broader implications, challenges, and in- sights for designing empathetic technology in pediatric healthcare.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "Institute for Experiential AI"
            }
          ],
          "personId": 152258
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "LIGHTHOUSE - DIG",
              "dsl": ""
            }
          ],
          "personId": 152042
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 152233
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 151533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Techology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 151887
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Media Lab"
            }
          ],
          "personId": 151469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Microsoft",
              "dsl": "Microsoft Research"
            }
          ],
          "personId": 152373
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 151543
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "Hospital Sant Joan de Déu Barcelona",
              "dsl": ""
            }
          ],
          "personId": 152074
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Hospital Sant Joan de Deu",
              "dsl": ""
            }
          ],
          "personId": 151785
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Hospital Sant Joan de Déu",
              "dsl": ""
            }
          ],
          "personId": 151839
        }
      ]
    },
    {
      "id": 152560,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Intelligent humanoids in manufacturing∗ Addressing the worker shortage and skill gaps in assembly cells",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1032",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "Technological evolution in the field of robotics is emerging with major breakthroughs in recent years. This was especially fostered by revolutionary new software applications leading to humanoid robots. Humanoids are being envisioned for manufacturing applications to form human-robot teams. But their implication in the manufacturing practices especially for industrial safety standards and lean manufacturing practices have been minimally addressed. Humanoids will also be competing with conventional robotic arms and effective methods to assess their return on investment are needed. To study the next generation of industrial automation, we used the case context of Tesla’s humanoid robot. The company has recently unveiled its project on an intelligent humanoid robot named ‘Optimus’ to achieve an increased level of manufacturing automation. This article proposes a framework to integrate humanoids for manufacturing automation and also presents the significance of safety standards of human-robot collaboration. A case of lean assembly cell for the manufacturing of an open source medical ventilator was used for human-humanoid automation. Simulation results indicate that humanoids can increase the level of manufacturing automation. Managerial and research implications are presented.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Rochester",
              "institution": "Oakland University",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151875
        }
      ]
    },
    {
      "id": 152561,
      "typeId": 13321,
      "title": "Biased Attention Near iCub’s Hand After Collaborative HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1159",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Earlier research has indicated that humans prioritize attention to the space close to their hands, commonly known as the \"near-hand effect\". This phenomenon may also extend to a human partner's hand, but specifically following a shared physical joint action. Consequently, within human dyads, collaborative interaction results in a shared body representation that might impact fundamental attentional mechanisms. Our project investigates whether a similar effect can emerge from a human-robot interaction scenario. To this aim, we designed an experiment to assess whether a collaborative human-robot interaction with the humanoid robot iCub could bias the human partner's attention toward the robot's hand. After the interaction, we replicated a classical psychological paradigm by adding a robotic condition to measure this attentional bias (i.e., the near-hand effect). Our findings indicate the existence of a near-hand effect triggered by the robot's hand, suggesting that HRI can replicate a shared body representation similar to that observed in human dyads, which may influence our basic attentional mechanisms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "University of Genoa",
              "dsl": "DIBRIS"
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "Italian Institute of Technology",
              "dsl": "RBCS"
            }
          ],
          "personId": 151443
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "Italian Institute of Technology",
              "dsl": "CONTACT Unit"
            }
          ],
          "personId": 151695
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "Italian Institute of Technology",
              "dsl": "CONTACT Unit"
            }
          ],
          "personId": 152032
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Cognitive, Linguistic, and Psychological Sciences"
            }
          ],
          "personId": 152026
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genoa",
              "institution": "Italian Institute of Technology",
              "dsl": "CONTACT Unit"
            }
          ],
          "personId": 151807
        }
      ]
    },
    {
      "id": 152562,
      "typeId": 13320,
      "title": "Understanding Fatigue Through Biosignals: A Comprehensive Dataset",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1042",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Fatigue is a multifaceted construct, that represents an important part of human experience. The two main aspects of fatigue are the mental one and the physical one, that often intertwine, intensifying their collective impact on daily life and overall well-being.\r\nTo soften this impact, understanding and quantifying fatigue is crucial. Physiological data play a pivotal role in the comprehension of fatigue, allowing a precious insight into the level and type of fatigue experienced.\r\nThough the analysis of these biosignals, researchers can determine whether the person is feeling mental fatigue, physical fatigue or a combination of both. \r\nThis paper introduces MePhy, a comprehensive dataset containing various biosignals, gathered while inducing different fatigue conditions, in particular mental and physical fatigue. Among the biosignals closely associated with stress situations, we chose: eye activity, cardiac activity, electrodermal activity (EDA) and electromyography (EMG). Data were collected using different devices, including a camera, a chest strap and different sensors from the BITalino kit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "University of Modena and Reggio Emilia",
              "dsl": ""
            }
          ],
          "personId": 151639
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "University of Modena and Reggio Emilia",
              "dsl": ""
            }
          ],
          "personId": 152256
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "University of Modena and Reggio Emilia",
              "dsl": "Department of Sciences and Methods for Engineering"
            }
          ],
          "personId": 152366
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio nell'Emilia",
              "institution": "University of Modena and Reggio Emilia",
              "dsl": "Department of Sciences and Methods for Engineering"
            }
          ],
          "personId": 151677
        }
      ]
    },
    {
      "id": 152563,
      "typeId": 13321,
      "title": "Collaborative Robots Can Support People with Disabilities in Vocational Education and Training",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1034",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "To explore the opportunities of collaborative robots (cobots) as a support for people with disabilities, we accompanied the deployment of a cobot in a vocational training workshop. The study with eleven participants investigated how trainees with intellectual disabilities react to a cobot, which tasks can be supported, and which challenges and opportunities arise. The study includes two surveys on negative attitudes toward robots, two workshops followed by interviews, a group interview, and an e-mail survey with supervisors. Surveys were analyzed qualitatively using descriptive statistics. The results indicate a low negative attitude toward robots before and after working with the cobot. The cobot can be used for assembly, handling, and quality control. However, challenges such as the cost and the identification of suitable users and applications must be overcome to fully use the opportunities like improving workplace ergonomics, expanding users' skills, and preparing employees for the primary job market.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 151975
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Garching",
              "institution": "Technische Universität München Garching",
              "dsl": ""
            }
          ],
          "personId": 152368
        }
      ]
    },
    {
      "id": 152564,
      "typeId": 13320,
      "title": "Dataset and Evaluation of Automatic Speech Recognition for Multi-lingual Intent Recognition on Social Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1045",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "While Automatic Speech Recognition (ASR) systems excel in controlled environments, challenges arise in robot-specific setups due to unique microphone requirements and \r\n dded noise sources. In this paper, we create a dataset of common robot instructions in 5 European languages, and we systematically evaluate current state-of-art ASR systems (Vosk, OpenWhisper, Google Speech and NVidia Riva). Besides standard metrics like word error rate (WER), we also look at two critical downstream tasks for human-robot\r\nverbal interaction: intent recognition rate and entity extraction, using the open-source Rasa chatbot. Overall, we found that open-source solutions as Vosk performs competitively with closed-source solutions while running on the edge, on a low compute budget (CPU only).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Pal Robotics",
              "dsl": ""
            }
          ],
          "personId": 152379
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Pal Robotics",
              "dsl": "HRI"
            }
          ],
          "personId": 151669
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel-Aviv",
              "institution": "Bar-Irfan University",
              "dsl": ""
            }
          ],
          "personId": 151580
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel-Aviv",
              "institution": "Bar-Irfan University",
              "dsl": ""
            }
          ],
          "personId": 151998
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "PAL Robotics",
              "dsl": ""
            }
          ],
          "personId": 152186
        }
      ]
    },
    {
      "id": 152565,
      "typeId": 13320,
      "title": "Sobotify: A Framework for Turning Robots into Social Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1046",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Sobotify is a software framework, which aims at simplifying the process of using robots in the fi eld of social robotics. This paper delineates the descriptions of the framework. With Sobotify, even non-technical people should be enabled to use robots for their specific purposes, such as teachers in a classroom, a childcare workers in kindergarten or therapists in a physiological or psychological therapy. During the development process of sobotify, feedback from the teachers at a vocational school have been taken into account in order to adjust the tools to their needs. The framework is designed to work with diff erent robots including both humanoid robots (NAO and Pepper) and non-humanoid robots such as toy robots (Cozmo) with advanced abilities as well as very simple toy robots (MyKeepon). The framework was tested by two research work which proved that Sobotify is applicable in diff erent setups. Further developement is already planned for the next months, e.g. integration ofadditional robots and extension of tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Hochschule für Technik und Wirtschaft (HTW) Berlin",
              "dsl": ""
            }
          ],
          "personId": 151821
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Humboldt University in Berlin",
              "dsl": ""
            }
          ],
          "personId": 151997
        }
      ]
    },
    {
      "id": 152566,
      "typeId": 13321,
      "title": "Prototyping a User Interface for Multi-Robot Speech Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1156",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The Wizard-of-Oz (WoZ) method is a common and beneficial means of enabling researchers with control over robots in experimental settings. However, there is a lack of general robot control tools for WoZ that are publicly available and easily adaptable to various research domains and needs. In particular, existing control interfaces that may be used in WoZ experiments often do not support the control of multi-robot interactions. As such, in this work, we present the design of three prototypes for a multi-robot speech control interface that would enable the control of multi-robot dialogue interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 151633
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 152222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152567,
      "typeId": 13321,
      "title": "Challenges in Annotating Gesture-Based Cognitive Status in Human-Robot Collaboration Datasets",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1157",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "For robots to be effective at collaborating with humans, they must be able to effectively communicate about entities in open-world tasks. Existing research on natural language generation and referring expression generation has yet to address how gesture and cognitive status impact how humans or robots decide how to refer to entities, a process known as Referring Form Selection. To address these issues we present a novel experimental testbed that leverages the Givenness Hierarchy to produce an entity's cognitive status. We also discuss challenges in developing this testbed and how we surmounted them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 151488
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152568,
      "typeId": 13320,
      "title": "GARRY: The Gait Rehabilitation Robotic System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1047",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Gait rehabilitation is a critical aspect of post-stroke recovery, and emerging technologies such as virtual reality and wearables are playing a pivotal role in facilitating this process. However, despite the potential benefits, there is a significant gap in robot-based rehabilitative systems that facilitate repeated use by maintaining users' attention long-term. Our research aims to bridge this gap by creating a comprehensive system that utilizes different feedback types and robotic assistance to support users' gait rehabilitation outcomes. In this paper, we introduce GARRY (Gait Rehabilitation Robotic System), a new robotic system that provides interactive feedback during locomotor training. It promotes engagement by gamifying the rehabilitation process, offering a fun means for the user to meet their rehabilitation goals defined and set by physical therapists. GARRY also incorporates behavioral feedback to introduce a sense of companionship during a session. We make GARRY open-source to other researchers in hopes of encouraging accessibility and to promote research in the field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151719
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152019
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "San Francisco State University",
              "dsl": "School of Engineering"
            }
          ],
          "personId": 140072
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 152569,
      "typeId": 13320,
      "title": "A Python Wrapper for Integrating Robots, Sensors, and Applications across Multiple Middleware",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1048",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Message oriented and robotics middleware play an important role in facilitating robot control, abstracting complex functionality, and unifying communication patterns across networks of sensors and devices. However, using multiple middleware frameworks presents a challenge in integrating different robots within a single system. To address this challenge, we present YAW, a Python wrapper supporting multiple message oriented and robotics middleware, including ZeroMQ, YARP, ROS, and ROS~2. YAW also provides plugins for exchanging deep learning framework data, without additional encoding or preprocessing steps. Using YAW eases the development of scripts that run on multiple machines, thereby enabling cross-platform communication and workload distribution. We finally present the three communication schemes that form the cornerstone of YAW's communication model, along with examples that showcase their benefits.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of informatics"
            }
          ],
          "personId": 152055
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 152106
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hamburg",
              "institution": "University of Hamburg",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151525
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Hambrug",
              "city": "Hamburg",
              "institution": "Universität Hamburg",
              "dsl": "Knowledge Technology Group"
            }
          ],
          "personId": 151636
        }
      ]
    },
    {
      "id": 152570,
      "typeId": 13321,
      "title": "If [YourName] can code, so can you! End-user robot programming for non-experts",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1279",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Social robots are being studied for a wide variety of user populations, such as older adults, but programming these social robots typically requires deep technical knowledge. Here we report a user study with a no-code end-user robot programming interface that we developed. The goal of our interface is to empower people with no programming background to easily create social robot interactions with older adults using natural language. We report our findings and recommendations on how to further improve no-code end-user robot programming interfaces.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": "Informatics"
            }
          ],
          "personId": 139957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 151634
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University Bloomington",
              "dsl": ""
            }
          ],
          "personId": 152342
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": "School of Informatics, Computing and Engineering"
            }
          ],
          "personId": 140030
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 139987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Bloomington",
              "institution": "Indiana University",
              "dsl": ""
            }
          ],
          "personId": 140016
        }
      ]
    },
    {
      "id": 152571,
      "typeId": 13321,
      "title": "Multiplayer Space Invaders: A Platform for Studying Evolving Fairness Perceptions in Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1272",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Current methods of measuring fairness in human-robot interaction (HRI) research often gauge perceptions of fairness at the conclusion of a task. However, this methodology overlooks the dynamic nature of fairness perceptions, which may shift and evolve as a task progresses. To help address this gap, we introduce a platform designed to help investigate the evolution of fairness over time: the Multiplayer Space Invaders game. This three-player game is structured such that two players work to eliminate as many of their own enemies as possible while a third player makes decisions about which player to support throughout the game. In this paper, we discuss different potential experimental designs facilitated by this platform. A key aspect of these designs is the inclusion of a robot that operates the supporting ship and must make multiple decisions about which player to aid throughout a task. We discuss how capturing fairness perceptions at different points in the game could give us deeper insights into how perceptions of fairness fluctuate in response to different variables and decisions made in the game.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152192
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151734
        }
      ]
    },
    {
      "id": 152572,
      "typeId": 13321,
      "title": "Drone Fail Me Now: How Drone Failures Affect Trust and Risk-Taking Decisions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1151",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "So far, research on drone failures has been mostly limited to understanding the technical causes of failures and how to recover from them. In contrast, there is very little work looking at how failures of drones are perceived by users. We address this gap by conducting a real-world study where participants experience drone failures leading to monetary loss whilst navigating a drone over an obstacle course. We tested 46 participants where they experienced both a failure and failure-free (control) interaction. Participants' trust in the drone, their enjoyment of the interaction, perceived control, and future use intentions were all negatively impacted by drone failures. However, risk-taking decisions during the interaction were not affected. These findings suggest that experiencing a failure whilst operating a drone in real-time is detrimental to participants' subjective experience of the interaction. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151870
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151919
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 139916
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152245
        }
      ]
    },
    {
      "id": 152573,
      "typeId": 13316,
      "title": "Telepresence Robots for Dynamic, Safety-Critical Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1089",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Robots are increasingly being introduced into dynamic, safety-critical spaces, like emergency departments (EDs). In EDs, healthcare workers (HCWs) must regularly manage multiple tasks while making decisions that directly affect patient outcomes. Thus, if robots are not well-situated in EDs, they could exacerbate existing problems and increase the risk of patient harm. Our work explores how to design robots to integrate into these spaces to support HCWs. We investigate important factors for situating robots in EDs without disrupting workflow, explore how to support HCWs during interruptions, and develop methods to increase safety and reduce errors during robot control. By informing the development of robots that are designed for safety-critical spaces, we hope to increase access to care and reduce patient harm.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California, San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 152574,
      "typeId": 13321,
      "title": "Designing Interactive Agents to Support Emotion Regulation in the Workplace through Guided Art-Making",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1032",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Workplaces are high-pressure environments where employees often deal with inflexible deadlines, instability in work relationships due to conflict, and the expectations of deliverables -- factors that exacerbate occupational stress and anxiety. While studies have demonstrated the effectiveness of therapeutic art-making interventions for supporting emotion regulation and alleviating occupational stress, there are few deliberate opportunities for employees to regulate their emotional state within the workplace. In this work, we present the design of a voice agent that can guide a user through a therapeutic art-making intervention to promote emotion regulation within the workplace. We share preliminary insights regarding the design of our voice agent, including the importance of embodiment and personalization. We also share insights about the feasibility of our proposed user study, which is aimed at evaluating the effectiveness of our voice agent at promoting emotion regulation in employees through therapeutic art-making.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 152114
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 151626
        }
      ]
    },
    {
      "id": 152575,
      "typeId": 13321,
      "title": "Using Robot Social Agency Theory to Understand Robots' Linguistic Anthropomorphism",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1153",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Robots' use of natural language is one of the key factors that leads humans to anthropomorphize them. But it is not yet well understood what types of language most lead to such language-based anthropomorphization (or, Linguistic Anthropomorphism). In this paper, we present a brief literature survey that suggests six broad categories of linguistic factors that lead humans to anthropomorphize robots: autonomy, adaptability, directness, politeness, proportionality, and humor. By contextualizing these six factors through the lens of Jackson and Williams' Theory of Social Agency for Human-Robot Interaction, we are able to show how and why these particular factors are those responsible for language-based robot anthropomorphism.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139980
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152576,
      "typeId": 13321,
      "title": "Language, Camera, Autonomy! Prompt-engineered Robot Control for Rapidly Evolving Deployment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1154",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The Context-observant LLM-Enabled Autonomous Robots (CLEAR) platform offers a general solution for large language model (LLM)-enabled robot autonomy. CLEAR-controlled robots use natural language to perceive and interact with their environment: contextual description deriving from computer vision and optional human commands prompt intelligent LLM responses that map to robotic actions. By emphasizing prompting, system behavior is programmed without manipulating code, and unlike other LLM-based robot control methods, we do not perform any model fine-tuning. CLEAR employs off-the-shelf pre-trained machine learning models for controlling robots ranging from simulated quadcopters to terrestrial quadrupeds. We provide the open-source CLEAR platform, along with sample implementations for a Unity-based quadcopter and Boston Dynamics Spot robot. Each LLM used, GPT-3.5, GPT-4, and LLaMA2, exhibited behavioral differences when embodied by CLEAR, contrasting in actuation preference, ability to apply new knowledge, and receptivity to human instruction. GPT-4 demonstrates best performance compared to GPT-3.5 and LLaMA2, showing successful task execution 97% of the time. The CLEAR platform contributes to HRI by increasing the usability of robotics for natural human interaction.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lexington",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Lincoln Laboratory"
            },
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson ",
              "institution": "Clemson University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152154
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "Human-Centered Computing"
            }
          ],
          "personId": 151848
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lexington",
              "institution": "MIT",
              "dsl": "Lincoln Laboratory"
            }
          ],
          "personId": 151513
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lexington",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "Lincoln Laboratory"
            }
          ],
          "personId": 151878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University ",
              "dsl": "Human-Centered Computing"
            }
          ],
          "personId": 151963
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lexington",
              "institution": "MIT Lincoln Laboratory",
              "dsl": ""
            }
          ],
          "personId": 152008
        }
      ]
    },
    {
      "id": 152577,
      "typeId": 13316,
      "title": "Modeling, Engendering and Leveraging Trust in Human-Robot Interaction: A Mental Model based Framework",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1084",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Trust between team members is a necessary part of any successful cooperation. Therefore, in mixed human-robot teams, the robot must possess the ability to model, assess and leverage human trust to not only successfully participate in the task but to ensure the team achieves its goals. In our research, we try to model and leverage trust in human-robot interaction, especially in situations where the human and the robot may have different models about the task at hand and thus may have different expectations. In our research, we focus on developing a comprehensive mental model-based framework. This framework captures the often-overlooked multidimensional aspects of trust, thus providing a strong foundation for various trust-aware decision-making approaches. and serves as a tool for inferring and estimating trust.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": ""
            }
          ],
          "personId": 151812
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing and AI"
            }
          ],
          "personId": 152010
        }
      ]
    },
    {
      "id": 152578,
      "typeId": 13320,
      "title": "Allybot: An Edutainment Robot to Improve Girls Participation in STEAM",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1004",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152229
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Art and Design"
            }
          ],
          "personId": 151567
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152097
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": ""
            }
          ],
          "personId": 152101
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": ""
            }
          ],
          "personId": 152290
        }
      ]
    },
    {
      "id": 152579,
      "typeId": 13321,
      "title": "Virtual Reality-based Human-Robot Interaction for Remote Pick-and-Place Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1270",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) has emerged as a promising medium, enhancing\r\nhuman-robot interaction by offering a more immersive and intuitive\r\ninterface. This paper presents a novel approach utilizing VR, partic-\r\nularly the Meta Quest 2 device, as an interface for remote control\r\nof a robot performing a pick-and-place task in a proxy data center\r\nenvironment. A user interface was created in VR, enabling users to\r\ncontrol the Fetch Mobile Manipulator robot and monitor its pose\r\nfor secure and safe remote operation. This work integrates virtual\r\nreality, computer vision, and human-robot interaction methodolo-\r\ngies. The system demonstrates the effectiveness of this VR-based\r\nhuman-robot interaction approach, showcasing its potential to en-\r\nhance productivity and safety in industrial applications involving\r\nremote robot control.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152024
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 151608
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 151661
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 151511
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152091
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152320
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152331
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152040
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152358
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 151495
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "SEATTLE",
              "institution": "University of Washington",
              "dsl": "Department of Electrical and Computer Engineering, BioRobotics Lab"
            }
          ],
          "personId": 152347
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 152231
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "Global Innovation Exchange (GIX)",
              "dsl": "University of Washington"
            }
          ],
          "personId": 152251
        }
      ]
    },
    {
      "id": 152580,
      "typeId": 13321,
      "title": "Robotic Gestures, Human Moods: Investigating Affective Responses in Public Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1271",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "In the evolving landscape of human-robot interaction, this study delves into the nuanced dynamics of humans react when encountering a robot designed to bid for an individual's attention, offer a reward, and prompt contemplation of the perennial \"trick or treat?\" question. Positioned in a busy public setting, Pepper engaged passersbyers, exploring diverse gestural bids to attract interaction, ranging from animated to meek with varied speech to simulate dynamic personalities.  In the second phase, the robot comments on those that take candy. Analyzing responses to Halloween candy distribution, the results show the very significant influence of non-verbal cues on participant's taking candy, and of post-candy-taking comments on participant emotive response, shedding light on the complex interplay between programmed actions and human affective experience. Future work might replicate such studies across varied cultural backdrops exploring how timing, gesture, and speech norms might benefit from local calibration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems, Oregon State University",
              "dsl": "Robotics/Oregon State University/ Charisma"
            }
          ],
          "personId": 151597
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Robotics"
            }
          ],
          "personId": 151946
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 151933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "Collaborative Robotics and Intelligent Systems Institute"
            }
          ],
          "personId": 139902
        }
      ]
    },
    {
      "id": 152581,
      "typeId": 13320,
      "title": "A Naturalistic Laboratory Setup for Real-World HRI Studies",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1009",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Department of Cognitive Sciences"
            }
          ],
          "personId": 152221
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Interdisciplinary Neuroscience Graduate Program"
            }
          ],
          "personId": 152226
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Department of Computer Engineering"
            }
          ],
          "personId": 151967
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Neuroscience"
            }
          ],
          "personId": 151537
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "Jagiellonian University",
              "dsl": "Cognitive Science Department"
            }
          ],
          "personId": 151962
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology and Neuroscience"
            }
          ],
          "personId": 152262
        }
      ]
    },
    {
      "id": 152582,
      "typeId": 13320,
      "title": "OpenVP: A Customizable Visual Programming Environment for Robotics Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1050",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Authored robotics applications have a diverse set of requirements for their authoring interfaces, being dependent on the underlying architecture of the program, the capability of the programmers and engineers using them, and the capabilities of the robot. Visual programming approaches have long been favored for both novice-level accessibility, and clear graphical representations, but current tools are limited in their customizability and ability to be integrated holistically into larger design interfaces. OpenVP attempts to address this by providing a highly configurable and customizable component library that can be integrated easily into other modern web-based applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139967
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 152583,
      "typeId": 13320,
      "title": "A Long-Range Mutual Gaze Detector for HRI",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1051",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "The detection of mutual gaze in the context of human-robot interaction is crucial for the understanding of human partners’ behavior. Indeed, the monitoring of the users’ gaze from a long distance enables the prediction of their intention and allows the robot to be proactive. Nonetheless, current implementations suffer from the limitations of the sensors, which struggle to detect face landmarks from long distances. In this work, we propose a software ROS2 pipeline that detects mutual gaze up to 5 m of distance. The code relies on robust off-the-shelf perception algorithms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Ticino",
              "city": "Lugano",
              "institution": "USI-SUPSI",
              "dsl": "Dalle Molle Institute for Artificial Intelligence (IDSIA)"
            }
          ],
          "personId": 152051
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "IDSIA - SUPSI",
              "dsl": ""
            }
          ],
          "personId": 151593
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "Dalle Molle Institute for Artificial Intelligence (IDSIA, SUPSI/USI)",
              "dsl": ""
            }
          ],
          "personId": 152207
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "Dalle Molle Institute for Artificial Intelligence (IDSIA, SUPSI/USI)",
              "dsl": ""
            }
          ],
          "personId": 152382
        }
      ]
    },
    {
      "id": 152584,
      "typeId": 13321,
      "title": "Investigating the Effect of Driving Support Robot Depending on Driver Characteristics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1049",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Driving support robots (DSRs) can change drivers' behavior and decrease traffic accidents; however, a one-size-fits-all approach cannot change the behavior of a wide range of drivers. To develop personalized DSRs, this study analyzed the improvement of driving behavior related to speeding and sudden acceleration/deceleration resulting from an interaction between humans and DSRs. We found differences in the effect of the support provided by a DSR due to driver characteristics. This result indicates that the personalization of support based on driver characteristics has the potential to improve drivers' behavior more effectively.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151872
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151942
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nagoya",
              "institution": "Nagoya Univeristy",
              "dsl": "Institutes of Innovation for Future Society"
            }
          ],
          "personId": 151502
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151536
        }
      ]
    },
    {
      "id": 152585,
      "typeId": 13316,
      "title": "Explainable Guidance and Justification for Mental Model Alignment in Human-Robot Teams",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1092",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "There is potential for humans and autonomous robots, each possessing their own capabilities and strengths, to perform tasks collaboratively as teammates, achieving greater performance than either could on their own. Productive teamwork, however, requires a great deal of coordination, with human and robot agents maintaining well-aligned mental models regarding the shared task and each agent's role within it. Achieving this requires live and effective communication, especially as plans change due to shifts in environment knowledge. Our work has leveraged augmented reality and natural language interfaces to facilitate this synchronization in partially observable, collaborative human-robot domains. These interfaces serve to integrate humans into multi-agent planning algorithms by not only recommending policies to human teammates, but also explaining the rationale behind those policies and strategically justifying guidance during times of policy mismatch.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 140077
        }
      ]
    },
    {
      "id": 152586,
      "typeId": 13320,
      "title": "UASOS: an experimental environment for assessing mental fatigue & cognitive flexibility during drone operations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1031",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Mental fatigue from continuous operations without breaks, represents a safety issue for military drone operations as these systems\r\nare complex and operated during long shifts. Military operations are hard to study due to their sensitive nature. The open access\r\nprogram UASOS serves as a testbed to examine the effects of mental fatigue in an ecologically valid environment. UASOS recreates\r\nfundamental aspects of military drone operations in a controllable environment, that is easy enough for novices to understand, but\r\ndemanding enough to elicit mental fatigue. Participants alternate between navigating a drone –using either a trackball/mouse or a\r\njoystick– and searching for visual targets. The protocol is set up in a way to tax the cognitive flexibility of participants by constantly\r\nrequiring the participants to alternate between tasks. In addition, several parameters can be adapted with regard to aspects such as\r\ndifficulty, duration, questionnaires, training phases, and more. The task also allows for synchronization with physiological data using\r\nLabStreamingLayer. Implemented in python, the code is set up to be easily installed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "Université de Toulouse",
              "dsl": "ISAE-SUPAERO"
            },
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "Université de Toulouse",
              "dsl": "ENAC"
            }
          ],
          "personId": 151611
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "Université de Toulouse",
              "dsl": "ISAE-SUPAERO"
            }
          ],
          "personId": 152083
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "Université de Toulouse",
              "dsl": "ENAC"
            }
          ],
          "personId": 151710
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "Université de Toulouse",
              "dsl": "ISAE-SUPAERO"
            }
          ],
          "personId": 151852
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Toulouse",
              "institution": "ENAC, Université de Toulouse ",
              "dsl": ""
            }
          ],
          "personId": 152269
        }
      ]
    },
    {
      "id": 152587,
      "typeId": 13320,
      "title": "Benchmark Movement Data Set for Trust Assessment in Human Robot Collaboration",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1032",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Trust is a factor that is becoming more prominent in human robot interaction research. Only few approaches so far tackle the challenge of data-driven trust assessment. In this paper, we present a data set consisting of motion tracking data from an industrial human robot collaboration task. The data is collected during a trust manipulation experiment that has been designed to elicit different trust levels in the participants. Additionally, participants filled out a standard trust questionnaire. The data set allows for developing and testing data-driven trust assessment algorithms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            },
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Architecture & Media Technology"
            }
          ],
          "personId": 151687
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Deptartment of Architecture, Design, and Media Technology",
              "dsl": ""
            }
          ],
          "personId": 152044
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Deptartment of Architecture, Design, and Media Technology",
              "dsl": ""
            }
          ],
          "personId": 151473
        }
      ]
    },
    {
      "id": 152588,
      "typeId": 13320,
      "title": "Evaluation Tools for Human-AI Interactions Involving Older Adults with Mild Cognitive Impairments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1033",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "As artificial intelligence (AI) systems have already proven useful in human lives generally, there is an opportunity for specialized human-AI interaction (HAI) systems to support and provide care for older adults with mild cognitive impairment (MCI). However, the integration of this technology in this population must be thoughtfully designed to accommodate specific needs and limitations. This includes careful measurement of both humans and systems. We developed an evolving dataset categorizing relevant measurement tools into five groups: cognitive ability, demographics & personality, activity level, state of mind, and perceptions of the AI system. This dataset will serve as a valuable resource for future research on AI for MCI, aiding in the identification of promising areas and trends in AI systems for older adults with MCI as well as providing essential tools for future studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts, Lowell",
              "dsl": ""
            }
          ],
          "personId": 151548
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": ""
            }
          ],
          "personId": 151457
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 152071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts, Lowell",
              "dsl": ""
            }
          ],
          "personId": 151893
        }
      ]
    },
    {
      "id": 152589,
      "typeId": 13320,
      "title": "REACT: Two Datasets for Analyzing Both Human Reactions and Evaluative Feedback to Robots Over Time",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1034",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Recent work in Human-Robot Interaction (HRI) has shown that robots can leverage implicit communicative signals from users to understand how they are being perceived during interactions. For example, these signals can be gaze patterns, facial expressions, or body motions that reflect internal human states. To facilitate future research in this direction, we contribute the REACT database, a collection of two datasets of human-robot interactions that display users’ natural reactions to robots during a collaborative game and a photography scenario. Further, we analyze the datasets to show that interaction history is an important factor that can influence human reactions to robots. As a result, we believe that future models for interpreting implicit feedback in HRI should explicitly account for this history. REACT opens up doors to this possibility in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152238
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152078
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151693
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152002
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151734
        }
      ]
    },
    {
      "id": 152590,
      "typeId": 13321,
      "title": "The Space Between Us: Bridging Human and Robotic Worlds in Space Exploration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1287",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Robots are key to human space exploration goals, as they are able to perform tasks and reach locations that humans cannot. Research on Human-Robot Interaction for these types of robot applications is crucial for mission success, but relevant findings have not yet been synthesized into a corpus of knowledge. We are in the process of conducting a scoping review of Human-Robot Interaction research in the context of outer space exploration. Our initial findings suggest that space Human-Robot Interaction research falls within 8 interconnected themes. We hope that these preliminary results will be useful to researchers aiming to investigate Human-Robot Interaction for space exploration applications. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Hamilton",
              "institution": "McMaster University",
              "dsl": ""
            }
          ],
          "personId": 151934
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Hamilton",
              "institution": "McMaster University",
              "dsl": "Department of Computing and Software"
            }
          ],
          "personId": 152200
        }
      ]
    },
    {
      "id": 152591,
      "typeId": 13321,
      "title": "Introducing Personas and Scenarios to highlight Older Adults’ Perspectives on Robot-Mediated Communication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1045",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Little is known about the expectations of older adults (60+ years old) in robot-mediated communication when leaving aside care-related activities. To bridge this gap, we carried out 30 semi-structured interviews with older adults to explore their experiences and expectations related to technology-mediated communication. We present the results of the collected data through personas that portray three archetype users, Conny Connected, Stephan Skeptical, and Thomas TechFan. These personas are presented in a specific communication scenario with individual goals that go beyond mere communication, such as the desire for closeness (Conny Connected), a problem-free experience (Stephan Skeptical), and exploring affordances of telepresence robots (Thomas Tech-Fan). Also, we provide two considerations when aiming at positive experiences\r\nfor older adults with robots: balance generalizable aspects and individual needs and identify and challenge preconceptions of telepresence robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Ilmenau University of Technology",
              "dsl": "Audiovisual Technology Group"
            }
          ],
          "personId": 151681
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Thüringen",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": "Media Psychology and Media Design"
            }
          ],
          "personId": 152336
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "Technische Universität Ilmenau",
              "dsl": ""
            }
          ],
          "personId": 151988
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ilmenau",
              "institution": "TU Ilmenau",
              "dsl": ""
            }
          ],
          "personId": 151932
        }
      ]
    },
    {
      "id": 152592,
      "typeId": 13321,
      "title": "Uncovering Patterns in Humans that Teach Robots through Demonstrations and Feedback",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1167",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Human-in-the-loop robot learning allows a robot to learn tasks more effectively with the help of humans in the role of teacher. While there is a large body of work on algorithms that leverage human input for better robot learning, there has been little attention to understanding how humans teach robots. In this paper, we provide preliminary results on how users strategize the use of demonstrations and evaluative feedback under a budget, and how these choices are influenced by demographic variables such as gender. We implemented a learning algorithm that allows a simulated robot arm to learn three reaching tasks with the help of a human. We collected interaction data for a total of 58 participants, which shows that participants demonstrate a tendency to provide evaluative feedback earlier in their interactions compared to demonstrations, and that gender may have an influence on teaching strategy. This preliminary analysis lays the foundation for future research aimed at developing tuneable computational models of different human teachers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": ""
            }
          ],
          "personId": 151595
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU Amsterdam",
              "dsl": "Social AI group"
            }
          ],
          "personId": 152076
        }
      ]
    },
    {
      "id": 152593,
      "typeId": 13321,
      "title": "Adapting Task Difficulty in a Cup-Stacking Rehabilitative Task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1288",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "As the need for accessible upper-body stroke rehabilitation grows, it becomes increasingly important to investigate how the difficulty level of rehabilitation tasks can be personalized to a patient and automatically adapted based on the patient's progress in therapy. We introduce a framework that uses Fitts' law to define task difficulty and iteratively apply it to dynamically adjust difficulty levels and to assign therapy tasks within the context of a cup-stacking occupational therapy activity. Our preliminary simulation results support the hypothesis that the model can adapt its difficulty levels based on a user’s time taken to stack a cup at various points on a table. Future work includes exploring the impact of different variables on the model's adaptability and integrating personalized verbal feedback from a socially assistive robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": ""
            }
          ],
          "personId": 152228
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "LOS ANGELES",
              "institution": "University of Southern California",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 152345
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 139976
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 152045
        }
      ]
    },
    {
      "id": 152594,
      "typeId": 13320,
      "title": "Moving Horizon Planning for Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1036",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "The collaboration and interaction between humans and robots intensify with ongoing research and industry needs. Robots require a motion planner that contributes to a safe environment for humans. This paper provides the online trajectory planner Moving Horizon Planning for Human-Robot Interaction (MHP4HRI), customizable for various robots, considering obstacles and humans in their environment. The planner generates motion commands in a moving horizon manner, similar to Model Predictive Control. This enables robots to react to dynamic changes in the environment in real-time. Descriptions of the planner and the underlying algorithms are given, as well as details about the provided framework regarding the benefits and usage for the community. Furthermore, we aim to provide a growing framework with new features in the future regarding the optimization and interaction with the environment, especially humans. The code, implemented mainly in C++ for the Robot Operating System (ROS), is available at GitHub: https://github.com/Renzo9710/mhp_review.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Institute of Control Theory and Systems Engineering"
            }
          ],
          "personId": 152007
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Institute of Control Theory and Systems Engineering"
            }
          ],
          "personId": 151540
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Institute of Control Theory and Systems Engineering"
            }
          ],
          "personId": 151714
        }
      ]
    },
    {
      "id": 152595,
      "typeId": 13321,
      "title": "Improving Human-Robot Team Transparency with Eye-tracking based Situation Awareness Assessment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1289",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Human-robot interactions rely on transparency to foster effective collaboration. Transparency can be assessed through metrics associated with factors such as situational awareness. This manuscript presents an ocular metric to assess situation awareness for human-machine teams. Participants used a decision support system to select a grasp for underwater manipulation. The participants' gaze behavior and visual awareness was analyzed using a wearable eye tracker. An initial analysis provides insight into the requirements of future techniques for objectively assessing situation awareness. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University ",
              "dsl": ""
            }
          ],
          "personId": 152135
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems Institute",
              "dsl": "Oregon State University"
            }
          ],
          "personId": 151908
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151651
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Collaborative Robotics and Intelligent Systems Institute",
              "dsl": "Oregon State University"
            }
          ],
          "personId": 151652
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS/Robotics"
            }
          ],
          "personId": 151604
        }
      ]
    },
    {
      "id": 152596,
      "typeId": 13321,
      "title": "Social Robots in Healthcare: Characterizing Privacy Considerations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1168",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "As healthcare robots gain traction, human-robot interaction (HRI) researchers are exploring the factors that impact user adoption and trust in these robots. Due to the sensitive nature of care, privacy concerns play a significant role in determining robot utility, usefulness, and adoption. In our work, we conducted a 3x3x3 online study ($N=239$) to explore peoples' perceptions of privacy and utility of 3 robots at varying levels of Human-Likeness (HL) across 3 realistic healthcare contexts. The results show that the context of care delivery is a key driver of perceptions of privacy and acceptable privacy-utility trade-offs. Interestingly, the HL of robot design may not significantly impact peoples' privacy perceptions of healthcare robots. We plan to leverage these key findings to develop privacy-aware robot behaviors that are context adaptable in order to improve privacy outcomes for healthcare robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 151882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Academy of Art University",
              "dsl": "School of Animation and Visual Effects"
            }
          ],
          "personId": 151712
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 152597,
      "typeId": 13321,
      "title": " Hands-On Robotics: Enabling Communication Through Direct Gesture Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1048",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Effective uman-Robot Interaction (HRI) is fundamental to seamlessly integrating robotic systems into our daily lives. However, current communication modes require additional technological interfaces, which can be cumbersome and indirect. This paper presents a novel approach, using direct motion-based communication by moving a robot's end effector. Our strategy enables users to communicate with a robot by using four distinct gestures -- two handshakes ('formal' and 'informal') and two letters ('W' and 'S'). As a proof-of-concept, we conducted a user study with 16 participants, capturing subjective experience ratings and objective data for training machine learning classifiers. Our findings show that the four different gestures performed by moving the robot's end effector can be distinguished with close to 100% accuracy. Our research offers implications for the design of future HRI interfaces, suggesting that motion-based interaction can empower human operators to communicate directly with robots, removing the necessity for additional hardware.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Inclusive Human-Robot Interaction"
            },
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "Human-Computer Interaction"
            }
          ],
          "personId": 152338
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen ",
              "institution": "University Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 151914
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 151458
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 152046
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Inclusive Human-Robot Interaction"
            }
          ],
          "personId": 152203
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 151665
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Essen",
              "institution": "University of Duisburg-Essen",
              "dsl": "HCI Group"
            }
          ],
          "personId": 152147
        }
      ]
    },
    {
      "id": 152598,
      "typeId": 13321,
      "title": "Name Pronunciation Extraction and Reuse in Human-Robot Conversations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1169",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Personalization in human-robot interaction (HRI) has been shown to have powerful effects on both users' perception of robots and objective interaction outcomes. Calling a human user by their name, an important signal to communicate understanding the user and memorizing information about them, remains an ongoing challenge in HRI research as typical text-to-speech algorithms struggle correctly pronouncing the numerous names that exist even just in the English language. This paper presents a pipeline for fusing text and audio features to extract and re-use user information like names with the correct pronunciation. We discuss technical guidelines for implementation and remaining challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Glendale",
              "institution": "Disney Research",
              "dsl": ""
            }
          ],
          "personId": 151796
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "Walt Disney Imagineering",
              "dsl": "Disney Research"
            }
          ],
          "personId": 151776
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "Disney Research",
              "dsl": ""
            }
          ],
          "personId": 152085
        }
      ]
    },
    {
      "id": 152599,
      "typeId": 13316,
      "title": "Multi-modal Language Models for Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1099",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "The recent progress in language models is enabling more flexible and natural conversation abilities for social robots. However, these language models were never made to be used in a physically embodied social agent. They lack the ability to process the other modalities humans use in conversation, such as vision, to make references to the environment and understand non-verbal communication. My work promotes the design of language models for physically embodied social interactions, shows how current technologies can be leveraged to enrich language models with these abilities, and explores how such multi-modal language models can be used to improve interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab - AIRO"
            }
          ],
          "personId": 151573
        }
      ]
    },
    {
      "id": 152600,
      "typeId": 13321,
      "title": "Does Performative Autonomy improve Situation Awareness in Highly Demanding Collaborative Tasks?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1283",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In human-robot collaboration tasks, robots have the opportunity\r\nto communicate relevant information to human teammates, inform\r\nthem about critical situations, and seek guidance during decisionmaking.\r\nResearchers have recently introduced the concept of Performative\r\nAutonomy, am autonomy design approach in which robots\r\nperforms lower levels of autonomy than they actually possess (e.g.,\r\nseeking advice that they do not genuinely require) to enhance human\r\nsituational awareness. In our study (n = 404), we implemented\r\nPerformative Autonomy in a resource management game, where\r\nthe robot periodically interacts with human teammates in ways\r\nintended to enhance situational awareness. Unfortunately, the experimental\r\ntestbed used in this work did not lead to demonstrated\r\nimpacts of this strategy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Hammond",
              "institution": "Purdue University Northwest",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152601,
      "typeId": 13321,
      "title": "Interactively Explaining Robot Policies to Humans in Integrated Virtual and Physical Training Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1041",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Policy summarization is a computational paradigm for explaining the behavior and decision-making processes of autonomous robots to humans.\r\nIt summarizes robot policies via exemplary demonstrations, aiming to improve human understanding of robotic behaviors.\r\nThis understanding is crucial, especially since users often make critical decisions about robot deployment in the real world.\r\nPrevious research in policy summarization has predominantly focused on simulated robots and environments, overlooking its application to physically embodied robots. \r\nOur work fills this gap by combining current policy summarization methods with a novel, interactive user interface that involves physical interaction with robots. \r\nWe conduct human-subject experiments to assess our explanation system, focusing on the impact of different explanation modalities in policy summarization.\r\nOur findings underscore the unique advantages of combining virtual and physical training environments to effectively communicate robot behavior to human users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": ""
            }
          ],
          "personId": 151987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151584
        }
      ]
    },
    {
      "id": 152602,
      "typeId": 13321,
      "title": "Handling Working Memory Knowledge Through a Consultant-Level Resource Management Strategy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1162",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Working memory is an important component of cognition that influences key cognitive processes, such as language. As such, working memory should play a key role in cognitive models for language-capable robots. The ways in which working memory buffers are organized within a robot's architecture can inform processes such as Referring Expression Generation. Thus, it is important to understand how information and resources within working memory may be organized to lead to human-like robotic language.\r\nPrevious work on the DIARC cognitive architecture introduced an entity-level, feature-based working memory framework in which each known entity had its own dedicated working memory buffer. This paper expands on that framework and proposes a new resource management strategy in which sets of entities that belong to the same type share a single working memory buffer. We end the paper with a brief discussion of how this novel strategy compares to the previously implemented entity-level strategy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 152295
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152603,
      "typeId": 13320,
      "title": "Probabilistic fusion of persons’ body features: the Mr. Potato algorithm",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1038",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Multi-modal social perception usually involves several independent software modules, detecting for instance faces, voices, body skeletons. Those features need then to be matched to each other, to create a complete model of a person. While the problem is simple in one-to-one interactions, multi-party interactions require to optimize a probabilistic graph in order to find the most likely persons–features associations, while ensuring practical properties like stability over time. This paper presents an open-source algorithm that search over all possible partitions of the relationship graph to identify the best partition. We playfully call this algorithm Mr. Potato, after the eponymous children’ game.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "PAL Robotics",
              "dsl": ""
            }
          ],
          "personId": 152186
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "PAL Robotics",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Technische Universität Wien",
              "dsl": ""
            }
          ],
          "personId": 152239
        }
      ]
    },
    {
      "id": 152604,
      "typeId": 13321,
      "title": "A bilingual social robot with sign language and natural language",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1163",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In situations where both deaf and non-deaf individuals are present in a public setting, it would be advantageous for a robot to communicate using both sign and natural languages simultaneously. This would not only address the needs for diverse users but also pave the way for a richer and more inclusive spectrum of human-robot interactions. To achieve this, a framework for a bilingual robot has been proposed in this paper. The robot exhibits the ability to articulate messages in spoken language, complemented by non-verbal cues such as expressive gestures,  all while concurrently conveying information through sign language. The system can generate natural language expressions with speech audio, spontaneous prosody-based gestures, and sign language displayed on a virtual avatar on a robot's screen. The preliminary findings from this research showcase the robot's capacity to seamlessly blend natural language expressions with synchronized gestures and sign language, underlining its potential to revolutionize communication dynamics in diverse settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Palaiseau",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "U2IS, ENSTA Paris"
            }
          ],
          "personId": 152141
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "UCL Interaction Centre"
            }
          ],
          "personId": 151724
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "ESSONNE",
              "city": "Palaiseau",
              "institution": "ENSTA-Paris",
              "dsl": "U2IS"
            }
          ],
          "personId": 152049
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "ENSTA-Pairs",
              "dsl": "Autonomous Systems and Robotics Lab/U2IS"
            }
          ],
          "personId": 152190
        }
      ]
    },
    {
      "id": 152605,
      "typeId": 13321,
      "title": "Discovering latent state in Human Robot Verbal Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1284",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Despite the abilities of automatic speech recognition systems such\r\nas CMU Sphinx, Google Speech-to-Text API, and Amazon Transcribe\r\nto recognize a variety of voices, they often face challenges\r\nin accurately processing complete information. To overcome this\r\nlimitation, we propose a novel approach utilizing Markov Decision\r\nProcesses. Our research involves an intelligent agent that evaluates\r\nhuman speech and identifies new states through learning,\r\nenabling it to process more comprehensive information compared\r\nto traditional systems. The paper illustrates two scenarios using\r\ninformation gain: one where the intelligent agent detects a new\r\nlatent state and ultimately reaches the goal state, and another where\r\nit revisits a previous state.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Hammond",
              "institution": "Purdue University Northwest",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Hammond",
              "institution": "Purdue Northwest",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151916
        }
      ]
    },
    {
      "id": 152606,
      "typeId": 13321,
      "title": "A Decision Framework for AR, Dialogue and Eye Gaze to Enhance Human-Robot Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1285",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Enabling an intuitive, bidirectional communication with real-time\r\nfeedback to convey intentions and goals is essential in human-robot\r\ncollaboration (HRC). In this paper, we propose ARDIE (Augmented\r\nReality with Dialogue and Eye Gaze), a novel intelligent agent that\r\nleverages multi-modal feedback cues to enhance HRC. Our system\r\nemploys a partially observable Markov decision process (POMDP)\r\nto formulate a joint decision policy integrating interactive aug-\r\nmented reality (AR), natural language, and eye gaze to provide\r\nreal-time visual feedback to humans. Through object-specific AR\r\nrenderings, ARDIE enables users to visualize current and future\r\nstates of the environment, ultimately improving situational aware-\r\nness and enhancing collaborative interactions between humans and\r\nrobots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Binghamton",
              "institution": "Binghamton University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152253
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Binghamton",
              "institution": "SUNY - University at Binghamton",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151460
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Binghamton",
              "institution": "SUNY - University at Binghamton",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151774
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Binghamton",
              "institution": "SUNY - University at Binghamton",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152380
        }
      ]
    },
    {
      "id": 152607,
      "typeId": 13321,
      "title": "Pilot Observations of an Autonomous Red Light, Green Light Robot for Interactions with Children with Disabilities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1164",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In the United States, approximately 7% of infants are born with a developmental disability that will impact their motor and social skill development. Assistive robotics are one method for promoting physical activity for children with disabilities, as they are motivating, repeatable, and adaptable. We implemented the popular children's game `Red Light, Green Light' (RLGL) with an assistive robot mediator to assess whether robots can motivate children to play the game and engage in physical activity. We conducted two pilot RLGL sessions in each of two groups of children with disabilities. We saw that children actively played the game multiple times and wanted to continue playing with the robot. This paper can help inform future studies on robot-mediated physical activity promotion and play.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151635
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 152152
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151733
        }
      ]
    },
    {
      "id": 152608,
      "typeId": 13321,
      "title": "Measuring Variations in Workload during Human-Robot Collaboration through Automated After-Action Reviews",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1165",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Human collaborator's workload plays a central role in human-robot collaboration. Algorithms designed to minimize cognitive workload enhance fluent human-robot teamwork. Time series data of workload is vital for both designing and assessing these algorithms. However, accurately quantifying and measuring cognitive workload, particularly at high temporal resolution, poses a substantial challenge. Towards addressing this challenge, we explore the potential of after-action reviews (AARs) as a tool for gauging workload during human-robot collaboration. First, through a case study, we present and demonstrate AutoAAR for measuring human workload post-task at a high temporal resolution. Second, through a user study, we quantify the validity and utility of measurements derived using AutoAAR for human-robot teamwork.The paper concludes with guidelines and future directions to extend this method to measure other internal states, such as trust and intent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151982
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Aberdeen ",
              "institution": "Humans in Complex Systems",
              "dsl": "DEVCOM Army Research Lab"
            }
          ],
          "personId": 151788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Adelphi",
              "institution": "US Army Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 152181
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151584
        }
      ]
    },
    {
      "id": 152609,
      "typeId": 13321,
      "title": "Drawings for Insight on Preschoolers' Perception of Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1044",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "The design of robots and robot-mediated activities for children needs to be informed by their expectations on robots, to ensure acceptability and effectiveness. Children drawings are a powerful tool to understand and describe these expectations, containing actionable insights for robot designers. We report a preliminary study of 50 drawings made by preschool children and investigate (i) their perception of robots and (ii) the change in perception induced by a short experience with real robots. Our analyses reveal that the children's age not only influences their perception of robots, but also how their perception changes after encountering robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Freiburg im Breisgau",
              "institution": "self-employed",
              "dsl": ""
            }
          ],
          "personId": 151770
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute for Technology Assessment and Systems Analysis (ITAS)"
            }
          ],
          "personId": 152275
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 151500
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Renningen",
              "institution": "Robert Bosch GmbH",
              "dsl": ""
            }
          ],
          "personId": 151653
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute for Technology Assessment and Systems Analysis (ITAS)"
            }
          ],
          "personId": 151832
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 152285
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 151571
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 151506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 151577
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology (KIT)",
              "dsl": "Institute of Anthropomatics and Robotics (IAR)"
            }
          ],
          "personId": 151936
        }
      ]
    },
    {
      "id": 152610,
      "typeId": 13321,
      "title": "Three Challenges in Utilizing Machine Learning to Predict Human Behavior from Observational Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1286",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This paper outlines the three principal challenges encountered during the machine learning efforts of the Real-Time Adaptive Systems (R-TAPS) project to learn the behavior of chemical plant workers and provides recommendations for future HRI projects that face similar problems. This paper specifically focuses on data labeling, annotation processes, and model evaluation. The R-TAPS machine learning efforts aimed to predict worker behavior during task execution in real-time. It employed a step-level label system, which caused difficulties in predicting worker behavior on a timestamp level. The annotation process that was carried out lacked uniformity, leading to inconsistencies in the data entries. The model performance presentation caused confusion due to multiple performance values and a lack of understanding on what metric to evaluate on. In response, this paper offers recommendations that addresses each challenge for future efforts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152193
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Statiion",
              "institution": "Texas A & M University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151673
        }
      ]
    },
    {
      "id": 152611,
      "typeId": 13321,
      "title": "Re-visting the Ultimatum Game: Understanding Responses to Robotic Opponents",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1280",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "The ultimatum game is a test of decision making in which par-\r\nticipants either accept or reject an offer on how to split a hypo-\r\nthetical pot of money versus an opponent. While simple, in the\r\nfields of psychology and human-computer interaction, this task has\r\nbeen used to examine how people respond to perceptions of fair-\r\nness/unfairness when making decisions. In the current study this\r\ntask is used to examine how human decision makers interact with\r\nintelligent computer systems, in an effort to see whether human\r\nopponents are treated the same or differently as artificial opponents.\r\nThis concept is increasingly relevant to robotics due to growing\r\nadvancements in robot teaming, in which humans and robots act\r\ntowards common goals, and to further explore the growing agency\r\nof systems in decision making. The results of a 𝑁 = 108 participant\r\nonline study indicate that the embodiment an intelligent computer\r\nsystem has no effect on human responses to the ultimatum game\r\nand suggests that intelligent computer systems may be treated the\r\nsame as humans.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151961
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151708
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "SHARE Lab"
            }
          ],
          "personId": 151563
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151733
        }
      ]
    },
    {
      "id": 152612,
      "typeId": 13321,
      "title": "Textile Robotic Interaction Designer-Robot Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1161",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This late-breaking report describes lab-based robot experiment involving two robot arms scanning and interaction with a set of 12 novel sustainable materials programmed with handfeel gestures inspired by how designers evaluate textile materials. The aim of gathering this data is to spur research in robot perception of soft materials and to contribute towards human-robot collaborative design systems. The complete dataset including scanned images, video of interactions accompanied by the robot motion paths is available with code at http://github.com/<anonymised-for-review>.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Art",
              "dsl": "Material Science Research Centre"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151824
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151748
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Art",
              "dsl": "Material Science Research Centre"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151752
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151672
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Art",
              "dsl": "School of Design"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 152158
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Art",
              "dsl": "Material Science Research Centre"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151572
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University",
              "dsl": "School of Design"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 151482
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University",
              "dsl": "School of Design"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Laboratory for Artificial Intelligence in Design",
              "dsl": ""
            }
          ],
          "personId": 152260
        }
      ]
    },
    {
      "id": 152613,
      "typeId": 13321,
      "title": "Introducing Social Robots to Assess Frailty in Older Adults",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1040",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Frailty is a crucial indicator in determining the well-being of older adults in terms of their health. With the growing number of elderly people, the demand for geriatricians is increasing, which means that they have less time to spend with each patient. The current methods for frailty assessment use simple tests that are time-consuming and do not require specific medical expertise. \r\n  To address this issue, this paper proposes the use of social robots to assess frailty autonomously. It presents a practical proposal that defines the robot's behavior and explains the design and implementation concepts. Finally, it discusses some of the challenges that may arise from introducing social robots as frailty evaluators.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Universitat Politècnica de Catalunya",
              "dsl": "Institut de Robòtica i Informàtica Industrial"
            }
          ],
          "personId": 151609
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Pal Robotics",
              "dsl": ""
            }
          ],
          "personId": 152379
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "CSIC-UPC",
              "dsl": "Institut de Robòtica i Informàtica Industrial"
            }
          ],
          "personId": 152084
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Hospitalet de Llobregat",
              "institution": "Institut Català d’Oncologia",
              "dsl": "Oncohematogeriatrics Unit"
            }
          ],
          "personId": 152359
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Hospitalet de Llobregat",
              "institution": "Institut Català d’Oncologia",
              "dsl": "Oncohematogeriatrics Unit"
            }
          ],
          "personId": 152242
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robotica i Informàtica Industrial",
              "dsl": ""
            }
          ],
          "personId": 151884
        }
      ]
    },
    {
      "id": 152614,
      "typeId": 13321,
      "title": "Emotion-Behavior Interplay in Human Animal-Robot  Interaction (HARI)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1282",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This preliminary study investigates the emotional and behavioral responses of neurotypical (NT) and neurodivergent (ND) individuals during interactions with a robotic dog. Previous research has demonstrated the advantages of using robots and animals in therapies for autistic individuals. However, specific behaviors that elicit positive responses are not well-documented. The aim of the study is to identify these behaviors. The study involves 9 participants (NT and ND) engaging with a robotic dog in individual sessions. Pre- and post-surveys, along with interviews, were conducted to assess the participants' perceptions. Both qualitative and quantitative analyses were conducted. Results indicate significant differences in the frequency and nature of positive responses between the two groups, highlighting distinctions in the robot behaviors that evoke positive reactions. This study contributes valuable insights into the potential therapeutic and recreational benefits of robotic dogs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "ArtMed Lab"
            }
          ],
          "personId": 151509
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151621
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 152334
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 151952
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "George Washington University",
              "dsl": "Biomedical Engineering"
            }
          ],
          "personId": 152187
        }
      ]
    },
    {
      "id": 152615,
      "typeId": 13320,
      "title": "Extending ReRun, Annotating Interactions in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1020",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Leland High School",
              "dsl": ""
            }
          ],
          "personId": 151789
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 151519
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 151758
        }
      ]
    },
    {
      "id": 152616,
      "typeId": 13321,
      "title": "Investigating Causality in Parent-Toddler-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1056",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Human robot interaction is often a very dynamic scenario, where both robots and humans perform verbal and non-verbal behaviors in response to each other. However, most user studies do not analyze the intricate dynamics of the interaction. Here, we present a novel methodology of a multi-participants, multi-dimensional, time-series Granger-causality analysis of a parent-toddler-robot triadic interaction. Our results show a specific causal flow of behaviors, namely, that the parent mediates the toddler-robot interaction by both verbal and non-verbal behaviors. Our proposed novel analysis can highly enrich understanding of the complex dynamics of human robot interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Tel Aviv Univeristy",
              "dsl": "Curiosity Lab"
            },
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Curiosity Robotics",
              "dsl": ""
            }
          ],
          "personId": 139794
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "Israel",
              "city": "Tel Aviv",
              "institution": "Tel-Aviv University",
              "dsl": "Curiosity Lab, Department of Industrial Engineering"
            }
          ],
          "personId": 139841
        }
      ]
    },
    {
      "id": 152617,
      "typeId": 13321,
      "title": "Investigating the Efficacy of Pain Relief Through a Robot’s Stroking with Speech",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1177",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Stroking has been recognized as a means of pain relief. Recent research indicates that combining \"stroking with speech\" by a robot generates greater pleasurable emotions in individuals than either \"speech\" or \"stroking\" alone. It has also been acknowledged that pain includes a psychological dimension and that pleasant emotions can mitigate the perception of pain. The objective of this study is to determine whether the robot’s \"stroking with speech\" action modifies the perception of pain. Based on the Gate Control Theory, the study aims to ascertain which action between tactile stimulation via \"stroking\" and the \"stroking with speech\" approach can facilitate pain relief more effectively, with the latter purportedly having a greater psychological impact. The results of the evaluation study involving 37 participants confirmed the effectiveness of both actions in providing pain relief.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152070
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151860
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152009
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152082
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152116
        }
      ]
    },
    {
      "id": 152618,
      "typeId": 13321,
      "title": "An Evolution of Assistive Robot Control to Meet End-User Ability",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1299",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "In this work, we present an evolution of system designs and studies that aim to facilitate the operation of high-DoF assistive robotic arms by persons with upper limb paralysis. We highlight the experimental pipeline and note developments in our efforts to design a suitable control map that can convert low variance residual body motions from neuromotor-impaired populations into 6-D velocity control signals for use in teleoperating a 7-DOF robotic arm. Notably, we provide results from variance analyses on raw IMU control signals from both neuromotor-impaired and unimpaired populations, and an analysis of the intrinsic dimensionality of map-building datasets gathered with and without movement guidance. We then present a preliminary 13-session study that vets the control map developed in light of these findings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 151564
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 152133
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 152104
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 151508
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 151518
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 152292
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 151773
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 151479
        }
      ]
    },
    {
      "id": 152619,
      "typeId": 13321,
      "title": "More Effective Robotic UV Disinfection of Objects Through Human Guidance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1058",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Ultraviolet-C (UV-C) robot irradiation is a promising approach for disinfecting surfaces contaminated by pathogens in healthcare settings. However, limitations exist with current UV disinfection robots, including coverage for complex surface geometries. This research presents a system for human-guided robotic UV disinfection that uses empirical sensor measurements rather than relying on high-accurate models for UV map coverage. Human guidance is integrated into the methodology to enhance disinfection, aiding in addressing complex shaped objects and topologies. Further, a validation test confirmed that our estimation approach reliably underestimates the UV exposure, which is beneficial for ensuring thorough disinfection. Initial pilot studies demonstrated that while autonomous disinfection was effective for simple objects like tabletops, human-guided disinfection, especially with feedback, improved coverage and speed for complex shapes like mugs. Combining human intuition with autonomy shows promise for enhancing robotic disinfection effectiveness.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151440
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151545
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 151953
        }
      ]
    },
    {
      "id": 152620,
      "typeId": 13321,
      "title": "MuModaR: A Multi-modal Cyber-physical World to Enhance Sim2Real Transfer in HRC",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1179",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Real-Time Human Autonomous Systems Collaborations (RealTHASC), is a novel extended reality (XR) testbed that interfaces humans and robots with photorealistic simulated environments. The testbed functions as an innovative facility, enabling the conduction of experiments focused on human-robot collaboration (HRC). It bridges the gap between traditional laboratory settings, often utilized for these experiments, and real-world deployment scenarios for robots, achieved through high-fidelity virtual environments. This paper presents an early stage development of Multi-Modal RealTHASC (MuModaR), a framework that augments the fundamental architecture of the RealTHASC testbed. The aim of this development is to enable simultaneous multi modal interactions involving Human Multi-robot Autonomy Teams (HMATs). To illustrate the effectiveness of the devised framework, a preliminary experiment is conducted in a scenario involving detection of multiple targets by HMATs that exchange information using vision and auditory modalities of perception. The framework combines a vision transformer with a large language model to leverage the collective strengths of these large-scale models, facilitating real-time feedback. The development of this new framework enriches the testbed's potential by enabling more robust assessments of HMATs and improving the transition from simulation/laboratory testing to real-world scenarios in HRC.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Mechanical Engineering"
            }
          ],
          "personId": 151809
        }
      ]
    },
    {
      "id": 152621,
      "typeId": 13320,
      "title": "RW4T Dataset: Data of Human-Robot Behavior and Cognitive States in Simulated Disaster Response Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1026",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "To forge effective collaborations with humans, robots require the capacity to understand and predict the behaviors of their human counterparts. There is a growing body of computational research on human modeling for human-robot interaction (HRI). However, a key bottleneck in conducting this research is the relative lack of data of cognitive states -- like intent, workload, and trust -- which undeniably affect human behavior. Despite their significance, these states are elusive to measure, making the assembly of datasets a challenge and hindering the progression of human modeling techniques. To help address this, we first introduce Rescue World for Teams (RW4T): a configurable testbed to simulate disaster response scenarios requiring human-robot collaboration. Next, using RW4T, we curate a multimodal dataset of human-robot behavior and cognitive states in dyadic human-robot collaboration. This RW4T dataset includes state, action and reward sequences, and all the necessary data to replay a visual task execution. It further contains psychophysiological metrics like heart rate and pupillometry, complemented by self-reported cognitive state measures. With data from 20 participants, each undertaking five  human-robot collaborative tasks, this dataset (comprising of 100 unique trajectories) accompanied with the simulator can serve as a valuable benchmark for human behavior modeling.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151982
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152015
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Adelphi",
              "institution": "US Army Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 152181
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Aberdeen ",
              "institution": "Humans in Complex Systems",
              "dsl": "DEVCOM Army Research Lab"
            }
          ],
          "personId": 151788
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Adelphi",
              "institution": "US Army Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 151791
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151584
        }
      ]
    },
    {
      "id": 152622,
      "typeId": 13321,
      "title": "Operationally Realistic Human-Autonomy Teaming Task Simulation to Study Multi-Dimensional Trust",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1052",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Autonomous systems are increasingly integrated into performing critical tasks, including in the space domain. Humans need to appropriately trust their autonomous teammate if the team is to perform effectively. Historically, many trust studies have used obtrusive surveys to measure trust as a one-dimensional, static construct when it is actually dynamic and multi-dimensional in nature. Furthermore, some previous work uses trusting tasks that lack operational validity. This paper presents an operationally-realistic task to study dynamic, multiple dimensions of operator trust. It enables collecting and time-synchronizing biosignals and embedded measures with less than millisecond precision. Our results show that this task simulation is capable of collecting unobtrusive, multidimensional trust-relevant signals with the end goal of improving human-autonomy teaming performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 152297
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 151439
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 151670
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 151472
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California Davis",
              "dsl": "Mechanical & Aerospace Engineering"
            }
          ],
          "personId": 151492
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "Mechanical and Aerospace Engineering"
            }
          ],
          "personId": 151947
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado-Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 152302
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado - Boulder",
              "dsl": "Smead Aerospace Engineering Sciences"
            }
          ],
          "personId": 152036
        }
      ]
    },
    {
      "id": 152623,
      "typeId": 13321,
      "title": "Egg-Laying Robot to Enhance Mind Perception of Children and Parents",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1173",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "The lifelikeness of social robots is important for building more natural relationships with humans. Although the lifelikeness of the way a robot behaves and its appearance has been explored, few studies have focused on mimicking the life cycle. In this study, we focused on egg-laying, one of the life cycles, and developed a robot capable of simulated egg-laying. Using the developed robot, we experimentally evaluated the effects of egg-laying behavior on the robot's lifelikeness and mind perception. After observing the egg-laying, there was no statistically significant difference in the lifelikeness, however, the children and parents of the participants in the experiment perceived the robot's mind significantly more.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152137
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152131
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152370
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152070
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151471
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151860
        }
      ]
    },
    {
      "id": 152624,
      "typeId": 13320,
      "title": "Using 3D Mice to Control Robot Manipulators",
      "award": "BEST_PAPER",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1027",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "Fluid 6DOF teleoperation of robot manipulators enables telemanipulation where autonomy is not possible, facilitates the collection of demonstration data, and aids routine robotics development. Amongst 6DOF input devices, 3D mice stand apart for their ergonomic design and low cost, but their sensitivity and users’ relative inexperience with them require special design considerations. We contribute a web software package that makes integrating 3D mice in robot manipulation interfaces easy. The package consists of configurable input signal processing schemes that can make the device more forgiving by, for instance, rejecting small inputs or emphasizing a dominant axis, and an interactive visual representation of the device’s 6DOF twist input, which helps with operator familiarization and provides a visual aide during teleoperation. We provide a demonstration interface illustrating a typical integration with a ROS/ROS2 robot system and give usage advice based on our research experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152353
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 151986
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152299
        }
      ]
    },
    {
      "id": 152625,
      "typeId": 13321,
      "title": "Initial Study on Robot Emotional Expression Using Manpu",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1174",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In recent years, robots have started to play an active role in various places in society. \r\nThe ability of robots not only to convey information but also to interact emotionally, is necessary to realize a human-robot symbiotic society. Many studies have been conducted on the emotional expression of robots. However, as robots come in a wide variety of designs, it is difficult to construct a generic expression method, and some robots are not equipped with expression devices such as faces or displays. To address these problems, this research aims to develop technology that enables robots to express emotions, using Manpu (a symbolic method used in comic books, expressing not only the emotions of humans and animals but also the states of objects) and mixed reality technology. As the first step of the research, we categorize manpu and use large language models to generate manpu expressions according to the dialogue information.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 152175
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "TOKYO",
              "institution": "AIST",
              "dsl": ""
            }
          ],
          "personId": 151569
        }
      ]
    },
    {
      "id": 152626,
      "typeId": 13321,
      "title": "Goes to the Heart: Speaking the User’s Native Language",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1053",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "We are developing a social robot to work alongside human support workers who help new arrivals in a country to navigate the necessary bureaucratic processes in that country. The ultimate goal is to develop a robot that can support refugees and asylum seekers in the UK. As a first step, we are targeting a less vulnerable population with similar support needs: international students in a UK university. As the target users are in a new country and may be in a state of stress when they seek support, forcing them to communicate in a foreign language will only fuel their anxiety, so a crucial aspect of the robot design is that it should speak the users' native language if at all possible. We provide a technical description of the robot hardware and software, and describe the user study that will shortly be carried out. At the end, we explain how we are engaging with refugee support organisations to extend the robot into one that can also support refugees and asylum seekers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151923
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 151691
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "English Language & Linguistics"
            }
          ],
          "personId": 152286
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 151491
        }
      ]
    },
    {
      "id": 152627,
      "typeId": 13321,
      "title": "Yachabot: A Novel Approach for Teaching Mathematics through Interactive Blocks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1295",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The use of robotic objects and platforms has the potential to provide interactive methods that generate engagement. This study proposes the design and implementation of YachaBot as an educational tool to level and reinforce topics of mathematics for fourth-grade elementary students. YachaBot consists of three learning blocks based on the curriculum followed throughout the school year: measurement units, number operations, and geometric analysis. Through this system, users can develop cognitive skills while having fun.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 152229
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 151977
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 151801
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 151764
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Faculty of Science and Engineering"
            }
          ],
          "personId": 151530
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": ""
            }
          ],
          "personId": 152101
        }
      ]
    },
    {
      "id": 152628,
      "typeId": 13321,
      "title": "Envisioning Mobile Telemanipulator Robots for Long Covid",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1296",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Long Covid (LC) is a debilitating disease impacting over 65 million people worldwide. The heterogeneity, severity, and unpredictability of LC symptoms cause episodic disability, leading to widespread social isolation among People with Long Covid (PwLC). Mobile Telemanipulator Robots (MTRs) offer the potential to support the social inclusion of PwLC. However, nuanced MTR design is needed to accommodate PwLC's fluctuating needs and avoid perpetuating stigma. In this work, we engaged PwLC in a participatory design process to explore how MTRs can be designed to support them. We present design considerations, grounded in the lenses of critical disability studies and critical access studies, for accessible MTR systems to support PwLC. We plan to expand on this to develop novel shared control algorithms and explore the social and ethical implications of autonomous behaviors in such MTR systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151718
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152157
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 152019
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "Patient-Led Research Collaborative",
              "dsl": ""
            }
          ],
          "personId": 151480
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "UC San Diego",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 139936
        }
      ]
    },
    {
      "id": 152629,
      "typeId": 13321,
      "title": "End-User Programming of Robot-Assisted Physical Training Activities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1175",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In this paper, we introduce an end-user programming approach for allowing physical trainers to program robot-assisted physical training activities without the assistance of a robotics engineer. The approach relies on a textual domain-specific language (DSL) to allow end users to specify the expected robot behavior through Behavior-Driven Development (BDD) scenarios. To evaluate the feasibility of our approach, we conducted a workshop with a physical therapist who was tasked with programming two different routines for a training robot. Results of the study highlighted the cognitive strategies employed by the end user to solve the task and also the pain points which required higher efforts from the user and should be therefore prioritized in our future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "region syddanmark",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "Health Information Technologies"
            }
          ],
          "personId": 151760
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "Health Technology"
            }
          ],
          "personId": 152119
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "The Maersk Mc-Kinney Moller Institute"
            }
          ],
          "personId": 151555
        }
      ]
    },
    {
      "id": 152630,
      "typeId": 13320,
      "title": "Promoting STEAM Education and AI/Robot Ethics in a Child-Robot Theater Afterschool Program",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1021",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151996
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "BLACKSBURG",
              "institution": "Virginia Polytechnic Institute & State University (Virginia Tech)",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 152165
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 152310
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human Development and Family Science/Psychology"
            }
          ],
          "personId": 151992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 152027
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 152354
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Engineering Education"
            }
          ],
          "personId": 152327
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151744
        }
      ]
    },
    {
      "id": 152631,
      "typeId": 13321,
      "title": "Work Tempo Instruction Framework for Balancing Human Workload and Productivity in Repetitive Task",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1176",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This paper proposes a feedback framework that adjusts human workload and productivity by instructing work tempo and personalizes work according to individual differences. For feedback of optimal work tempo, this study proposes a human state model to investigate the effects of work tempo instructions on workload and productivity. Based on the analysis results obtained, we proposed a feedback policy using our proposed human state model. By testing our proposed framework in a picking task, we showed the possibility to balance productivity and workload.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "The National Institute of Advanced Industrial Science and Technology",
              "dsl": "Industrial Cyber-Physical Systems Research Center"
            }
          ],
          "personId": 152077
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "The National Institute of Advanced Industrial Science and Technology",
              "dsl": "Industrial Cyber-Physical Systems Research Center"
            }
          ],
          "personId": 151784
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of  Advanced Industrial Science and Technology",
              "dsl": "Digital Human Research Team, Artificial Intelligence Research Center"
            }
          ],
          "personId": 151995
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "National Institute of Advanced Industrial Science and Technology",
              "dsl": "Industrial Cyber-Physical Systems Research Center"
            }
          ],
          "personId": 151867
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Waseda University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "The National Institute of Advanced Industrial Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 152094
        }
      ]
    },
    {
      "id": 152632,
      "typeId": 13321,
      "title": "What Should a Robot Do: Comparing Human and Large Language Model Recommendations for Robot Deception",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1290",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This study compares human ethical judgments with Large Language Models (LLMs) on robotic deception in various scenarios. Surveying human participants and querying LLMs like OpenAI's GPT-3.5 and GPT-4, we presented ethical dilemmas in high-risk (e.g., healthcare) and low-risk (e.g., games) contexts. Findings reveal alignment between humans and LLMs in high-risk scenarios, prioritizing safety, but notable divergences in low-risk situations, reflecting challenges in AI development to accurately capture human social nuances and moral expectations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 140048
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 140049
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 140008
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Public Policy and Office of Graduate Studies"
            }
          ],
          "personId": 151638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "State College",
              "institution": "Pennsylvania State University",
              "dsl": "Aerospace Engineering/Pennsylvania State University"
            }
          ],
          "personId": 151957
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "State College",
              "institution": "Pennsylvania State",
              "dsl": "Aerospace Engineering"
            }
          ],
          "personId": 152013
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Tech",
              "dsl": ""
            }
          ],
          "personId": 151827
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 151585
        }
      ]
    },
    {
      "id": 152633,
      "typeId": 13321,
      "title": "Bridging the Gap: Early Education on Robot and AI Ethics through the Robot Theater Platform in an Informal Learning Environment ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1170",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "With the rapid advancement of robotics and AI, educating the next generation on ethical coexistence with these technologies is crucial. Our research explored the potential of a child-robot theater afterschool program in introducing and discussing robot and AI ethics with elementary school children. Conducted with 30 participants from a socioeconomically underprivileged school, the program blended STEM (Science, Technology, Engineering & Mathematics) with the arts, focusing on ethical issues in robotics and AI. Using interactive scenarios and a theatrical performance, the program aimed to enhance children’s understanding of major ethical issues in robotics and AI, such as bias, transparency, privacy, usage, and responsibility. Preliminary findings indicate the program’s success in engaging children in meaningful ethical discussions, demonstrating the potential of innovative, interactive educational methods in early education. This study contributes significantly to integrating ethical robotics and AI in early learning, preparing young minds for a technologically advanced and socially responsible future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 151996
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "BLACKSBURG",
              "institution": "Virginia Polytechnic Institute & State University (Virginia Tech)",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 152165
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 152310
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human Development and Family Science/Psychology"
            }
          ],
          "personId": 151992
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Mind Music Machine Lab"
            }
          ],
          "personId": 152027
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human Development and Family Science"
            }
          ],
          "personId": 152354
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Engineering Education"
            }
          ],
          "personId": 152327
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Industrial and Systems Engineering"
            }
          ],
          "personId": 151744
        }
      ]
    },
    {
      "id": 152634,
      "typeId": 13321,
      "title": "Effects of Gestures and Negative Attitudes on Impressions of Lecturing Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1291",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Robot avatars have a potential to be applied to various real-world scenarios. We have conducted lectures using our android in diverse settings such as conferences. Several previous studies indicated that gestures can play an important role in human-robot interaction (HRI). The current study focused on the fundamental question of whether gestures are necessary for android avatars and explored the relationship between gestures and negative attitudes toward robots. Experimental results showed that android with gestures enhances their anthropomorphism and animacy. The results also showed that without gestures, people with higher negative attitude toward situations of interaction with robots evaluated the lecturing robot lower. With gestures, people with higher negative attitude toward emotions in interaction with robots evaluated the robot highly. This might suggest that people with high negative attitudes toward robots pay more attention to the android with gestures and thus appreciate it more in lectures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 151632
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 152218
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": "Department of Sociology"
            }
          ],
          "personId": 151859
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 152060
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Keihanna Science City",
              "institution": "ATR",
              "dsl": "Hiroshi Ishiguro Laboratory"
            }
          ],
          "personId": 152025
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kyoto",
              "city": "Keihanna Science City",
              "institution": "ATR",
              "dsl": "Hiroshi Ishiguro Laboratories"
            }
          ],
          "personId": 151881
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Advnced Telecommunications Research Institute International",
              "dsl": ""
            }
          ],
          "personId": 151664
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "KiQ Inc.",
              "dsl": ""
            }
          ],
          "personId": 152195
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 152363
        }
      ]
    },
    {
      "id": 152635,
      "typeId": 13320,
      "title": "The Role of Visual Perspective Taking in Human-Robot Collaboration​",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24d-1026",
      "source": "PCS",
      "trackId": 12628,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152731
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Riccarton",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152291
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot Watt University",
              "dsl": "School of Mathematical and Computer sciences"
            }
          ],
          "personId": 152339
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "London",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Personal Robotics Lab"
            }
          ],
          "personId": 152100
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": ""
            }
          ],
          "personId": 151605
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Social Sciences"
            }
          ],
          "personId": 151903
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": ""
            }
          ],
          "personId": 152038
        }
      ]
    },
    {
      "id": 152636,
      "typeId": 13321,
      "title": "How to Decrease Negative Impressions towards Multiple Social Robots: A Preliminary Investigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1292",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Recently, research using multiple social robots has been actively conducted for advanced communication. Using multiple social robots has a positive effect of enhancing their influence. However, interacting with multiple robots may impose a cognitive load on people, potentially yielding negative impressions towards the social robots and adversely affecting human-robot interaction. In this study, we sought to decrease negative impressions in a scenario where participants could perceive the robot as a friend or family member. Specifically, we conducted a preliminary experiment using multiple robots, where a group of friends and family members participated, and investigated impressions towards multiple robots. The finding in this study revealed that approximately 70\\% of the participants had no negative impressions and more than 80\\% of the participants felt that 10 social robots had the most enhanced influence. This study provided interesting perspectives for the design of interactions aimed at enhancing the influence of multiple robots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 151805
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "CyberAgent, Inc.",
              "dsl": ""
            }
          ],
          "personId": 152319
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka Univ.",
              "dsl": ""
            }
          ],
          "personId": 152146
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 152127
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Osaka",
              "institution": "Osaka University",
              "dsl": "Graduate School of Engineering Science"
            }
          ],
          "personId": 152363
        }
      ]
    },
    {
      "id": 152637,
      "typeId": 13321,
      "title": "Exploratory Evaluation of a Tabletop Robot With Older Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1171",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Over the years, research on interactions between older adults (OAs) and robots suggest that robots can address needs of OAs and are generally well perceived by them. \r\nTo synchronize user needs with technological capabilities, we follow an iterative and incremental approach: prototypes are showcased to users to collect observations on their interactions with the robots, and the design is refined until suitable interaction scenarios can be implemented before initiating trials over longer periods of time. \r\nThis paper presents the use of Consolidated Framework for Implementation Research (CFIR) to conduct short exploratory studies as part of the robot design process.\r\nSurveys and observations gathered before and after the interactions document the interaction experience using metrics such as utility, design, and reliability. \r\nEleven OAs from two long-term care facilities interacted with a tabletop robot we designed. \r\nThis paper reports on how the interactions were perceived by the OAs, illustrating how CFIR can be used for such type of studies. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Research Center of Aging (CdRV), Interdisciplinary Institute for Technological Innovation, Université de Sherbrooke",
              "dsl": ""
            }
          ],
          "personId": 152188
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Université de Sherbrooke",
              "dsl": "Interdisciplinary Institute for Technological Innovation"
            }
          ],
          "personId": 151900
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Université de Sherbrooke",
              "dsl": "Interdisciplinary Institute for Technological Innovation"
            }
          ],
          "personId": 151868
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Université de Sherbrooke",
              "dsl": "Interdisciplinary Institute for Technological Innovation"
            }
          ],
          "personId": 152098
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Sherbrooke",
              "institution": "Université de Sherbrooke",
              "dsl": "Interdisciplinary Institute for Technological Innovation"
            }
          ],
          "personId": 152303
        }
      ]
    },
    {
      "id": 152638,
      "typeId": 13321,
      "title": "Development of Meal Partner Robot and Applications Towards Enriching Mealtime Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1172",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Co-eating which means eating with others has many positive effects and enriches our lives. However, many people, including elderly people living alone and children whose parents are both working, tend to eat alone in the current social situation. We believe that a robot that embodies in the real world and can interact in real-time can be a good meal partner. In this paper, we described the development and applications of a meal partner robot. We present the robot's design concept and the configuration of hardware and software, from the viewpoint that a robot that serves as a meal partner needs to have the ability to behave as if it were eating a meal with humans. We also developed the dialogue interaction system for food delivery and eating behavior expression using the chest and hand monitors of the robot, as an application example of the meal partner robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 152175
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": "JSK Robotics Lab"
            }
          ],
          "personId": 151490
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 152294
        }
      ]
    },
    {
      "id": 152639,
      "typeId": 13321,
      "title": "Exploring Cognition and Affect during Human-Cobot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1293",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "This project merges Human-Robot Interaction (HRI) and Brain-Computer Interfaces (BCI) to enhance collaboration in a manufacturing context. It addresses the challenge of human variability in cognitive and affective states by 1)leveraging a consumer-grade EEG device to collect brain signals 2). and transmitting emotional and cognitive data to a Raspberry Pi-powered robotic unit. The robot then employs an adaptive algorithm, responding in real-time to stress and concentration levels in the operator. \r\nThe robot's adaptive responses contributed to smother interaction by (1) actively adjusting its motor speed in response to high or low levels of concentration and stress (2) activating RGB lighting to alert the operator when stress levels exceed a predetermined threshold – prompting them to take a break for optimal well-being.This success paves the way for future endeavors involving multiple users and robots collaborating simultaneously.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University - San Luis Obispo",
              "dsl": "Computer Science and Software Engineering"
            }
          ],
          "personId": 152377
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Computing, Informatics, and Decision Systems Engineering"
            }
          ],
          "personId": 151622
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Luis Obispo",
              "institution": "California Polytechnic State University - San Luis Obispo",
              "dsl": ""
            }
          ],
          "personId": 152052
        }
      ]
    },
    {
      "id": 152640,
      "typeId": 13317,
      "title": "EcoSquirriz: Mobile Environmental Awareness Robot for for University Institutions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1001",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Currently, climate change at the international level has raised a number of questions about the link between people and nature, but it is a topic that does not motivate all young people in universities. EcoSquirriz is an interactive robot that will be deployed within the green areas or courtyards of universities to raise awareness among students about the environment and provide information about simple actions to be more responsible with the environment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontificia Universidad Católica del Perú ",
              "dsl": "Faculty of Art and Design"
            }
          ],
          "personId": 151448
        },
        {
          "affiliations": [
            {
              "country": "Peru",
              "state": "Lima",
              "city": "Lima",
              "institution": "Pontifical Catholic University of Peru",
              "dsl": "Engineering Department"
            }
          ],
          "personId": 152023
        }
      ]
    },
    {
      "id": 152641,
      "typeId": 13317,
      "title": "Enhancing Dining Experience for Parkinson's Patients: Self-Stabilizing Spinning Utensils",
      "addons": {},
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24k-1007",
      "source": "PCS",
      "trackId": 12622,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152729
      ],
      "eventIds": [],
      "abstract": "Parkinson's disease, a neurodegenerative disorder characterized by tremors, significantly impedes daily activities, including the simple act of eating. This impairment is particularly evident when consuming dishes like noodles, where precise utensil control becomes a challenge. Recognizing the intricate connection between Parkinson's symptoms and the prevalence of noodle- based cuisines worldwide, this project aims to develop self- stabilizing spinning utensils specifically designed to enhance the dining experience of Parkinson's patients. By minimizing tremor- induced utensil movements, these innovative utensils will promote independent eating and contribute to an improved quality of life for individuals with Parkinson's.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "",
              "city": "Bangkok",
              "institution": "King’s Mongkut University of Technology Thonburi",
              "dsl": ""
            }
          ],
          "personId": 151586
        },
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "",
              "city": "Bangkok",
              "institution": "Shrewsbury International School Bangkok",
              "dsl": ""
            }
          ],
          "personId": 152159
        },
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "",
              "city": "Bangkok",
              "institution": "Shrewsbury International School Bangkok",
              "dsl": ""
            }
          ],
          "personId": 151646
        },
        {
          "affiliations": [
            {
              "country": "Thailand",
              "state": "Bangkok",
              "city": "bangkok",
              "institution": "Darunsikkhalai School",
              "dsl": ""
            }
          ],
          "personId": 152371
        }
      ]
    },
    {
      "id": 152642,
      "typeId": 13321,
      "title": "Would you Trust a Robot that Distrusts you?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1067",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Trust is an important antecedent to successful human-robot collaboration and is conceptualized as a reciprocal process. The current study evaluated whether a robot could influence participants’ trust levels through the disclosure of its own trust. We conducted a 3 (trust disclosure) × 2 (trust repair) between-subjects experiment in which N = 194 participants sorted boxes together with a humanoid robot that either disclosed low trust, high trust, or no trust information (control condition) towards the participant in a virtual warehouse environment. Additionally, participants either received an apology or did not (control) in response to a subsequent robotic error. Our analyses so far reveal that the disclosure of distrust from a robot towards a human indeed negatively impacts human trust in the robot. However, the disclosure of high trust did not significantly differ from no disclosure. Our findings contribute to a better understanding of reciprocal trust calibration and highlight the importance to consider trust as a reciprocal interaction. Moreover, our findings illustrate that disclosing low levels of trust by a robot might serve as a promising strategy to proactively mitigate overtrust.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bochum",
              "institution": "Ruhr-University Bochum (RUB)",
              "dsl": "Chair for Industrial Sales and Service Engineering (ISSE)"
            }
          ],
          "personId": 151562
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Bochum",
              "institution": "Ruhr-Universität Bochum",
              "dsl": "Faculty of Psychology/ IO Psychology"
            }
          ],
          "personId": 151570
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bochum",
              "institution": "Ruhr University Bochum",
              "dsl": ""
            }
          ],
          "personId": 151596
        }
      ]
    },
    {
      "id": 152643,
      "typeId": 13321,
      "title": "Speaking Transparently: Social Robots in Educational Settings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1188",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The recent surge in popularity of Large Language Models, known for their inherent opacity, has increased the interest in fostering transparency in technology designed for human interaction. This concern is equally prevalent in the development of Social Robots, particularly when these are designed to engage in critical areas of our society, such as education or healthcare. In this paper we propose an experiment to investigate how users can be made aware of the automated decision processes when interacting in a discussion with a social robot. We describe the proposed interactive settings, system design, and our approach to extend the transparency in a robot's decision-making process for multi-party interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Speech, Music and Hearing (TMH)"
            }
          ],
          "personId": 152162
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology",
              "dsl": "Speech, Music and Hearing (TMH)"
            }
          ],
          "personId": 152177
        }
      ]
    },
    {
      "id": 152644,
      "typeId": 13321,
      "title": "A Visual Design Space for One-Dimensional Intelligible Human-Robot Interaction Visualizations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1068",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "To enable effective communication between users and autonomous robots, it is crucial to have a shared understanding of goals and actions. This is made possible through an intelligible interface that can communicate relevant information. This intelligibility enhances user comprehension, enabling them to anticipate the robot's actions and respond appropriately. However, because robots can perform a wide variety of actions and communication resources are limited, such as the number of available ''pixels'', visualizations must be carefully designed. To tackle this challenge, we have developed a visual design framework and design space that can be used to create intelligible visualizations for human-robot interaction. Our framework focuses on three key components: information type, pixel layout, and robot type. We demonstrate how intelligibility can be integrated into interactions through prototype visualizations featuring a one-dimensional pixel layout, laying the groundwork for developing more detailed and understandable visualizations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Expertise Centre for Digital Media, Hasselt University - Flanders Make",
              "dsl": ""
            }
          ],
          "personId": 151727
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Hasselt",
              "institution": "Hasselt University - tUL - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 152361
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Diepenbeek",
              "institution": "Hasselt University - tUL - Flanders Make",
              "dsl": "Expertise Centre for Digital Media"
            }
          ],
          "personId": 151743
        }
      ]
    },
    {
      "id": 152645,
      "typeId": 13321,
      "title": "Investigating Effects of Multimodal Topic-continuance Recognition on Human-Robot Interviewing Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1063",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study's long-term goal is to realize a communication robot as a partner that can keep talking about specific things that the user would like to talk about and is interested in.\r\n\r\nToward the goal, we developed an interviewer robot which adapts topics based on the user's multimodal attitudes.\r\n\r\nThe robot, utilizing the Japanese GPT-NeoX-3.6, selects questions based on the estimated topic-continuance level. We regard the topic-continuance level as the degree of the user's speaking willingness (willing to continue the current topic).\r\n\r\nThis paper aims to validate the multimodal topic-continuance recognition model and its adaptive question selection strategy.\r\n\r\nFirst, we trained the model on the \"Hazumi\" dialogue corpus, which includes user multimodal behavior in human-virtual agent interactions.\r\n\r\nSecond, the 10 participants interviewed with the robot equipped the trained model. After the interviews, we asked the participants if the topic continuance/change by robot was appropriate, and validated the estimation accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 152270
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 151536
        }
      ]
    },
    {
      "id": 152646,
      "typeId": 13321,
      "title": "The Effect of Emotional Expression on the Use of a Hand-Sanitizing Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1184",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "People tend to favor robots that express positive emotions over ones expressing negative emotions. On the other hand, people have been found to use hand sanitizers more when the user's feelings of disgust and guilt are exploited. The goal of this paper is to explore what happens when these two scenarios merge in a hand-sanitizing robot; that is, will people use a hand-sanitizer robot more when it employs positive emotions or when it creates a sense of guilt? The hand-sanitizer robot RIMEPHAS was programmed to express six different emotions Joyful, Angry, Sad, Sick, Surprised, and Neutral on its display, each accompanied by a voice line,  to the user. The recognizability of the modes was tested in a within-subject experiment before testing the influence of the expressions on the usage of the robot in a field trial. The results show a significant difference in the amount of usage, favoring negative emotions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense ",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 151660
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 151808
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 151976
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Erlangen ",
              "institution": "University of Erlangen-Nuremberg",
              "dsl": ""
            }
          ],
          "personId": 152225
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": ""
            }
          ],
          "personId": 151507
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Sonderborg",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Design; Media and Education"
            }
          ],
          "personId": 151643
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 151592
        }
      ]
    },
    {
      "id": 152647,
      "typeId": 13321,
      "title": "Sleep Elf: Pillow Robot that Pats and Sings Child to Sleep Mimicing A Parent",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1064",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Child sleep problem is one of the major pain points for parents, which usually last from infancy to school-age. Child sleep problem not only affects child's health and development but also causes depression and anxiety in parents. To address this, we designed a child sleep companion robot to help child fall asleep quickly and encourage child to sleep without parent. We conducted a detailed product interaction design, and a prototype was built and tested to explore the pillow-child interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152279
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152093
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Harbin",
              "institution": "Harbin Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 152301
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151778
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152034
        }
      ]
    },
    {
      "id": 152648,
      "typeId": 13321,
      "title": "An Exploration of Multimodal Communication for Developing Extrovert, Ambivert, and Introvert Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1185",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "As humanoid robots advance, their presence in public spaces and human-humanoid robot interaction (HHRI) is growing. These robots, unlike standard information systems, resemble humans and can interact physically, using both verbal and nonverbal communication. Recognizing non-verbal cues and their setup, as well as the significance of these cues is critical in HHRI, as it greatly aids in expressing emotions and intentions, distinguishing it from human-human interaction. This study delves into non-verbal cues reflecting robot personalities, incorporating features like head, arms, hands, and body orientation. It explores ambivert traits, a mostly populous yet less-studied aspect of personality, to develop designs blending extrovert and introvert characteristics. These insights are crucial for enhancing human-robot communication and advancing Human-Humanoid Robot Interaction (HHRI) systems. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 151775
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 152316
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 152351
        }
      ]
    },
    {
      "id": 152649,
      "typeId": 13320,
      "title": "Unlocking Human-Robot Dynamics: Introducing SenseCobot, a Novel Multimodal Dataset on Industry 4.0",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1017",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "In the era of Industry 4.0, the importance of human-robot collaboration (HRC) in the advancement of modern manufacturing and automation is paramount. Understanding the intricate physiological responses of the operator when they interact with a cobot is essential, especially during programming tasks. To this aim, wearable sensors have become vital for real-time monitoring of worker well-being, stress, and cognitive load.\r\nThis article presents an innovative dataset (SenseCobot) of physiological signals recorded during several collaborative robotics programming tasks. This dataset includes various measures like ElectroCardioGram (ECG), Galvanic Skin Response (GSR), ElectroDermal Activity (EDA), body temperature, accelerometer, ElectroEncephaloGram (EEG), Blood Volume Pulse (BVP), emotions and subjective responses from NASA-TLX questionnaires for a total of 21 participants.\r\nBy sharing dataset details, collection methods, and task designs, this article aims to drive research in HRC advancing understanding of the User eXperience (UX) and fostering efficient, intuitive robotic systems. This could promote safer and more productive HRC amid technological shifts and helps decipher intricate physiological signals in different scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Modena",
              "institution": "Department of Engineering “Enzo Ferrari” University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 152021
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "Department of Sciences and Methods for Engineering University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 152016
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "Department of Sciences and Methods for Engineering University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 151984
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "Department of Sciences and Methods for Engineering University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 152217
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "Department of Sciences and Methods for Engineering University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 152366
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Reggio Emilia",
              "institution": "Department of Sciences and Methods for Engineering University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 151677
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Modena",
              "institution": "Department of Engineering “Enzo Ferrari” University of Modena and Reggio Emilia.",
              "dsl": ""
            }
          ],
          "personId": 151656
        }
      ]
    },
    {
      "id": 152650,
      "typeId": 13321,
      "title": "NYAM: the role of configurable engagement strategies\\\\ in robotic-assisted feeding",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1065",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In some contexts, like geriatric hospitals, the number of patients requiring assistance with feeding is very high and robots may be an effective tool for caregivers to provide better assistance.\r\nThis article introduces a robot designed to aid in the feeding process for individuals.\r\nOur robot is equipped with a mechanism to effectively recapture the person's attention whenever necessary. The mechanism is easily adjustable by the caregivers, allowing the straightforward customisation of the feeding service. \r\n\r\nThe approach was evaluated, within a geriatric hospital, with 9 patients who used the robot for 5 consecutive days. We argue that incorporating enhanced social aspects into the robot is imperative to enhance the effectiveness and acceptance of this solution.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "CSIC-UPC",
              "dsl": "Institut de Robòtica i Informàtica Industrial"
            }
          ],
          "personId": 152084
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "CSIC-UPC",
              "dsl": "Institut de Robòtica i Informàtica Industrial"
            }
          ],
          "personId": 151886
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "Vall d'Hebron Institute of Research (VHIR) and Parc Sanitari Pere Virgili",
              "dsl": "RE-FiT Barcelona Research Group"
            },
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "Universitat Oberta de Catalunya (UOC)",
              "dsl": "Faculty of Health Sciences"
            }
          ],
          "personId": 151556
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "Vall d'Hebron Institute of Research (VHIR) and Parc Sanitari Pere Virgili",
              "dsl": "RE-FiT Barcelona Research Group"
            },
            {
              "country": "Spain",
              "state": "Barcelona",
              "city": "Barcelona",
              "institution": "CIBER de Epidemiología y Salud Pública (CIBERESP)",
              "dsl": ""
            }
          ],
          "personId": 152205
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robotica i Informàtica Industrial",
              "dsl": ""
            }
          ],
          "personId": 151884
        }
      ]
    },
    {
      "id": 152651,
      "typeId": 13321,
      "title": "Touch from Robots Loses to Chatting via Text: A Comparative Study of Sympathy Demonstrated Using Text, Robot Arm, and Qoobo",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1186",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Social touch is helpful for improving social connection, and it is one  potential method to improve online communication with people at a distance. Although prior work examined if social touch from robot is effective for communicating sympathy compared to text and GIFs, the results indicated that touch from a robotic arm was not effective as a means of conveying sympathy. Therefore, in order to examine how to make social touch from robots more effective, we conducted an experiment using Qoobo a more lifelike robot. Results (50 dyads) indicate that the touching method we used was not effective for conveying sympathy. Interviews with participants suggested the ways to improve robot; that is, making the robot look like an animal, such as Qoobo, with adding warmth would be effective.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 152341
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 151802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Mexico",
              "city": "Las Cruces",
              "institution": "New Mexico State University",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 151917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139941
        }
      ]
    },
    {
      "id": 152652,
      "typeId": 13321,
      "title": "Towards Remote Expert Supported Autonomous Assistant Robots in Shopping Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1066",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Autonomous robots that assist customers during their shopping trips may become ubiquitous in the near future. However, these systems are limited in their ability to appropriately reply to all of a customer's requests. As a result, a dedicated human expert has to remotely \"take over\" and resolve the customer's issue to warrant a satisfying experience. This creates the problem that the expert needs to quickly review what has happened prior to the take-over to provide optimal assistance. To address this, we designed and implemented an interactive summary concept allowing experts to quickly filter and review the most relevant information to solve the given issue. Finally, we provide an example use case illustrating how a remote expert may solve a request using our system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland Informatics Campus",
              "dsl": "DFKI"
            }
          ],
          "personId": 151522
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 151834
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 152356
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 152214
        }
      ]
    },
    {
      "id": 152653,
      "typeId": 13321,
      "title": "Unlocking Potentials of Virtual Reality as a Research Tool in Human-Robot Interaction: A Wizard-of-Oz Approach",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1187",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Wizard-of-Oz (WoZ) systems represent a widespread method in HRI research. While they are cost-effective, flexible and are often preferred over developing autonomous dialogs in experimental settings, they are typically tailored to specific use cases. In addition, WoZ systems are mainly used in lab studies that deviate from real world scenarios. Here, virtual reality (VR) can be used to immerse the user in a real world interaction scenario with robots. This article highlights the necessity for a modularized and customizable WoZ system, using the benefits of VR. The proposed system integrates well-established features like speech and gesture control, while expanding functionality to encompass a data dashboard and dynamic robot navigation using VR technology. The discussion emphasizes the importance of developing technical systems, like the WoZ system, in a modularized and customizable way, particularly for non-technical researchers. Overcoming usability hurdles is crucial to establishing this tool's role in the HRI research field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Computer Science Institute"
            }
          ],
          "personId": 152372
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Computer Science Institute"
            }
          ],
          "personId": 151504
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bottrop",
              "institution": "University of Applied Sciences Ruhr West",
              "dsl": "Computer Science Institute"
            }
          ],
          "personId": 151783
        }
      ]
    },
    {
      "id": 152654,
      "typeId": 13321,
      "title": "An Exploratory Study on People's Intuitive Understanding of Expressive Robot Behavior",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1060",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Robots are becoming part of our social environment, in which they are expected to communicate effectively with their nonverbal behavior. Since nonverbal behavior is understood intuitively in human-to-human interaction, it is a proper starting point to study intuition in HRI. This study investigated the robot's characteristics that help humans to intuitively understand expressive robots' motions, focusing on what contributes to effortless understanding as an element of intuition. We asked 50 participants to watch eighteen nine-second video clips of three different robot types performing expressive robot behaviors and then answer an open-ended survey. Our findings highlight the inputs, mediating factors, and outputs that users reported on the basis of observing examples of expressive robot behavior. These insights are a starting point for constructing a theoretical framework for intuition in HRI and, thus, how to make robotic behavior more intuitive to users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151843
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "Norway",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Faculty of Mathematics and Natural Sciences, Digitalization, Research Group of Design of Information Systems"
            }
          ],
          "personId": 152037
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion"
            },
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Psychology"
            }
          ],
          "personId": 152031
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151980
        }
      ]
    },
    {
      "id": 152655,
      "typeId": 13316,
      "title": "Developing a Zoomorphic Robot for Animal Welfare Education",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1030",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Animal welfare education (AWE) could greatly benefit from customized robotics to help children learn about animal sentience and emotions, and thereby promote compassion towards animals. As part of my work to realize this, I used an existing zoomorphic robot to investigate children's perception of its mental abilities and their attitudes to cruelty to robots, with findings promising for use as a parallel to live animals. Subsequently, I used the participatory design methodology to engage animal welfare educators and children to discover key design features for a zoomorphic robot for AWE, which are being included in a prototype. Future work will evaluate the effectiveness of this robot in an AWE program. This interdisciplinary approach, combining principles from psychology, pedagogy, and human-robot interaction, aims to help AWE organizations change children's attitudes towards animals via robotics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 151602
        }
      ]
    },
    {
      "id": 152656,
      "typeId": 13321,
      "title": "Wizard-of-Oz vs. GPT-4: A Comparative Study of Perceived Social Intelligence in HRI Brainstorming",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1182",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Human-robot interaction often employs the Wizard-of-Oz (WoZ) paradigm, where a human controls the robot. However, this approach has limitations, such as a lack of autonomy that impedes real-world applications. Large language models (LLMs) can replace WoZ in conversational tasks, such as brainstorming. We propose that in such application domains, LLM-controlled robots can achieve comparable perceived social intelligence to WoZ-controlled robots. An experiment (n=27, within-subject design) tested this by having participants brainstorm health and well-being solutions with an LLM- or WoZ-controlled Furhat robot. Bayesian analyses revealed substantial evidence for the null model regarding perceived social intelligence, social presentation, and social information processing, indicating similar perceptions of social intelligence for WoZ- and LLM-controlled robots. Participants tentatively preferred the LLM-controlled robot, and reliably identified when the robot was WoZ- or LMM-controlled. This study highlights the potential of LLMs to replace the WoZ paradigm and transform HRI in various research and application domains.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Noord-Brabant",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 152056
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 151576
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 152156
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 151441
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": "Department of Communication and Cognition"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Breda",
              "institution": "Avans University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 152296
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 151845
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Tilburg",
              "institution": "Tilburg University",
              "dsl": ""
            }
          ],
          "personId": 152120
        }
      ]
    },
    {
      "id": 152657,
      "typeId": 13321,
      "title": "Integrating ChatGPT with Blockly for End-User Development of Robot Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1061",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper presents an End-User Development environment for collaborative robot programming, which integrates Open AI ChatGPT with Google Blockly. Within this environment, a user, who is neither expert in robotics nor in computer programming, can define the items characterizing the application domain (e.g., objects, actions, and locations) and define pick-and-place tasks involving these items. Task definition can be achieved with a combination of natural language and block-based interaction, which exploits the computational capabilities of ChatGPT and the graphical interaction features offered by Blockly, to check the correctness of generated robot programs and modify them through direct manipulation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Brescia",
              "institution": "University of Brescia",
              "dsl": "Department of Information Engineering"
            }
          ],
          "personId": 151941
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Brescia",
              "institution": "University of Brescia",
              "dsl": "Department of Information Engineering"
            }
          ],
          "personId": 151836
        }
      ]
    },
    {
      "id": 152658,
      "typeId": 13321,
      "title": "A Rosbag Tool to Improve Dataset Reliability",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1183",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Datasets are cornerstones of research in Human-Robot Interaction\r\n(HRI) and allow researchers to structure observations for other\r\npeers to work on. These often store temporal sensitive information\r\nabout the behaviour of humans and robots involved in the study, and\r\ntake advantage of the state of the art in robot logging, e.g., rosbags.\r\nDepending on the research goal, an approach commonly adopted\r\nis to publish datasets alongside annotated semantic information\r\nabout the interaction. However, validating and assessing the quality\r\nof the datasets has not been the main concern of the community.\r\nThis work highlights the risk of publishing datasets without\r\nensuring the synchronicity between objective and subjective mea-\r\nsures and proposes a simple yet effective tool to mitigate it. The\r\ntool is evaluated on the rosbags of a popular dataset. Results show\r\nthat 31.48% of its content contains indeterministic delays, causing\r\nthe original synchronicity with the respective annotations to be\r\nlost.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": ""
            }
          ],
          "personId": 151527
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Pal Robotics",
              "dsl": ""
            }
          ],
          "personId": 152379
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Naples",
              "institution": "University of Naples Federico II",
              "dsl": "DIETI"
            }
          ],
          "personId": 151523
        }
      ]
    },
    {
      "id": 152659,
      "typeId": 13316,
      "title": "Modeling Politeness in Human–Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1032",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Politeness is a linguistic phenomenon that is central to human communication. The many influences on the choice and interpretation of politeness in language make the phenomenon very complex to model and to account for in Human--Robot Interaction. I therefore question whether and how the implementation of such a social linguistic phenomenon in a robot is desirable. To find possible answers I conducted studies on politeness in Human--Robot Interaction from different perspectives: users' expectations, users' politeness towards a robot and users' perception of politeness use by a robot. In future research, based on and informed by my study results, I aim to model two politeness strategies and implement them in a Furhat robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": "Faculty of Linguistics and Literary Studies, Digital Linguistics Lab"
            }
          ],
          "personId": 151649
        }
      ]
    },
    {
      "id": 152660,
      "typeId": 13315,
      "title": "Accessible Tele-Operation Interfaces for Assistive Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1030",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152727
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152332
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152299
        }
      ]
    },
    {
      "id": 152661,
      "typeId": 13321,
      "title": "Creating a Framework for a User-Friendly Cobot Failure Management in Human-Robot Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1190",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "Solving failures is part of our private and work lives. With the ongoing changes in the industrial production setting, we have to deal with new failure originators: collaborative robots (cobots). Failure communication and subsequent recovery are essential to improve performance and restore trust after cobot failures. Therefore, we propose a framework for cobot failure management (FCFM) to support failure communication and solving in the production context. In a study with workers (N = 35), we investigate the impact of the helpfulness of the FCFM for workers. The first preliminary results demonstrate that the FCFM helps facilitate failure communication and rectification.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "University of Augsburg",
              "dsl": "Chair for Human-Centered Artifical Intelligence"
            }
          ],
          "personId": 151679
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "KUKA Deutschland GmbH",
              "dsl": ""
            }
          ],
          "personId": 152057
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Augsburg",
              "institution": "KUKA Deutschland GmbH",
              "dsl": ""
            }
          ],
          "personId": 151971
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Augsburg",
              "institution": "University of Augsburg",
              "dsl": "Chair for Human-Centered Artificial Intelligence"
            }
          ],
          "personId": 151739
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "University of Augsburg",
              "dsl": "Human-Centered AI Lab"
            }
          ],
          "personId": 151754
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Chair of Human-Centered Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 152059
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Augsburg",
              "institution": "Augsburg University",
              "dsl": "Human-Centered Artificial Intelligence"
            }
          ],
          "personId": 152247
        }
      ]
    },
    {
      "id": 152662,
      "typeId": 13315,
      "title": "PATHWiSE: An AI-Assisted Teacher Authoring Tool for Creating Custom Robot-Assisted Learning Activities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1035",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152728
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": ""
            }
          ],
          "personId": 151762
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": ""
            }
          ],
          "personId": 151838
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago",
              "dsl": ""
            }
          ],
          "personId": 151487
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois at Chicago",
              "dsl": ""
            }
          ],
          "personId": 151464
        }
      ]
    },
    {
      "id": 152663,
      "typeId": 13315,
      "title": "Sprout: Demonstration of Soft Expressive Robot",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1034",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152727
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Computer Science, People and Robots Lab"
            },
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Computer Science, People and Robots Lab"
            }
          ],
          "personId": 139828
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "madison",
              "institution": "UW, Madison",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "madison",
              "institution": "UW, Madison",
              "dsl": ""
            }
          ],
          "personId": 139775
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            },
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 152664,
      "typeId": 13315,
      "title": "An Adaptable, Safe, and Portable Robot-Assisted Feeding System",
      "award": "BEST_DEMO",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1037",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152728
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 152254
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 140071
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 151641
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 139770
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 151542
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 151590
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139739
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 151566
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 151779
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 152209
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Woodinville",
              "institution": "The Tyler Schrenk Foundation",
              "dsl": ""
            }
          ],
          "personId": 151811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Linko PLLC",
              "dsl": ""
            }
          ],
          "personId": 152089
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 151453
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 139931
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle ",
              "institution": "University of Washington ",
              "dsl": "School of Computer Science and Engineering "
            }
          ],
          "personId": 151993
        }
      ]
    },
    {
      "id": 152665,
      "typeId": 13315,
      "title": "Shutter: A Low-Cost and Flexible Social Robot Platform for In-the-Wild Deployments",
      "award": "HONORABLE_MENTION",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1036",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152727
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152155
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 152199
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": ""
            }
          ],
          "personId": 152012
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 151734
        }
      ]
    },
    {
      "id": 152666,
      "typeId": 13320,
      "title": "Towards Reproducible Language-Based HRI Experiments: Open-Sourcing a Generalized Choregraphe Project",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1000",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "We are witnessing increasing calls for reproducibility and replicability in HRI studies to improve reliability and confidence in empirical findings. One solution to facilitate this is using a robot platform that other researchers already and frequently use, making it easier to replicate studies to verify results. In this work, we focus on a popular, affordable, and rich-in-functionality robot platform, NAO/Pepper, and contribute a generalized experiment project specifically for conducting language-based HRI experiments, including objective data collection. Specifically, we first describe a concrete workflow from an existing experiment and how it is generalized. We then evaluate the generalized project with a case study to show how adopters can quickly adapt to their specific experiment needs, and end with step-by-step instructions for adaptation. Through this work, we hope it provides inspiration for HRI researchers to not only provide their experiment code as supplementary material but also generalize them to benefit other researchers to advance empirical research in HRI. The generalized Choregraphe project with documentation, demo, and usage notes is freely available under MIT license on GitHub (currently anonymized on OSF) at https://bit.ly/hri-1000.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 151505
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 152202
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Tampa",
              "institution": "University of South Florida",
              "dsl": "Computer Science/RARE Lab"
            }
          ],
          "personId": 139782
        }
      ]
    },
    {
      "id": 152667,
      "typeId": 13321,
      "title": " Multi-modal Language Learning: Explorations on learning Japanese Vocabulary ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1078",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "We explore robot-assisted language learning with a social robot, in which the robot teaches Japanese vocabulary. Specifically, we study if the mode of presentation of referents of nouns influences learning outcomes, and hypothesise that multimodal presentation of referents leads to improved learning outcomes. Three conditions are tested: referents are either presented as Japanese audio only, referents are visually presented, or referents are presented as actual objects that learners could pick up and manipulate. The learners were taught 4 words per condition and were distracted between the conditions with general questions related to the robot. \r\nThere was a significant increase in the number of learned words between the audio-only and visual conditions, as well as between the audio-only and tactile conditions.\r\nNo significant difference was found between the visual and tactile conditions. \r\nHowever, from our study, it follows that both these conditions are preferred over learning through only audio. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Nijmegen",
              "institution": "Radboud University",
              "dsl": "Donders Institute"
            }
          ],
          "personId": 152054
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "AIRO-IDLab"
            }
          ],
          "personId": 151538
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab - AIRO"
            }
          ],
          "personId": 151573
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab - imec"
            }
          ],
          "personId": 151461
        }
      ]
    },
    {
      "id": 152668,
      "typeId": 13321,
      "title": "Inclusive Dialogues: WokeBot Engaging Diversity Dilemmas",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1199",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In today's world, understanding different viewpoints is key for societal cohesion and progress. Robots have the potential to provide aid in discussing tough topics like ethnicity and gender. However, comparably to humans, the appearance of a robot can trigger inherent prejudices. This study delves into the interplay between robot appearance and decision-making in ethical dilemmas. Employing  a Furhat robot that can change faces in an instant, we looked at how robot appearance affects decision-making and the perception of the robot itself. Pairs of participants were invited to discuss a dilemma presented by a robot, covering sensitive topics of ethnicity or gender. The robot either adopted a first-person or third-person perspective, and altered its appearance accordingly. Following the explanation, participants were encouraged to discuss their choice of action in the dilemma situation. We found that participants rated the robot as significantly less animate when the robot adopted a first person perspective and that its appearance matched the main character in the dilemma, compared to when it did not. This sheds light on the intricate dynamics of human-robot interaction, emphasizing the need for thoughtful consideration in designing robot appearances to promote unbiased engagement in discussions of societal significance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "University of Applied Sciences Utrecht",
              "dsl": ""
            }
          ],
          "personId": 140020
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "HU University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 152029
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Utrecht",
              "institution": "HU University of Applied Sciences Utrecht",
              "dsl": "Institute for ICT"
            }
          ],
          "personId": 139918
        }
      ]
    },
    {
      "id": 152669,
      "typeId": 13320,
      "title": "PedSUMO: Simulacra of Automated Vehicle-Pedestrian Interaction Using SUMO To Study Large-Scale Effects",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24j-1001",
      "source": "PCS",
      "trackId": 12623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "abstract": "As automated vehicles become more widespread but lack a driver to communicate in uncertain situations, external communication, for example, via LEDs or displays, is evaluated. However, the concepts are mostly evaluated in simple scenarios, such as one person trying to cross in front of one automated vehicle. The traditional empirical approach fails to study the large-scale effects of these in this not-yet-real scenario. Therefore, we built PedSUMO, an enhancement to SUMO for the simulacra of automated vehicles' effects on public traffic, specifically how pedestrian attributes affect their respect for automated vehicle priority at unprioritized crossings. \r\nWe explain the algorithms used and the derived parameters relevant to the crossing. We open-source our code and demonstrate an initial data collection and analysis of Ingolstadt, Germany. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Ulm University",
              "dsl": "Institute of Media Informatics"
            }
          ],
          "personId": 151678
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Universität Ulm",
              "dsl": ""
            }
          ],
          "personId": 151709
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "Ulm University",
              "dsl": ""
            }
          ],
          "personId": 152067
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Ulm",
              "institution": "Ulm University",
              "dsl": "Institute of Media Informatics"
            }
          ],
          "personId": 151512
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ulm",
              "institution": "University of Ulm",
              "dsl": ""
            }
          ],
          "personId": 151496
        }
      ]
    },
    {
      "id": 152670,
      "typeId": 13321,
      "title": "Isolated by Robotic Co-Workers: The Impact of Verbal Ostracism on Psychological Needs and Human Behavior",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1079",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "This study explores the impact of robotic ostracism by language on fundamental psychological needs and its carryover effect on subsequent human–human interaction. In a laboratory experiment, n = 68 participants experienced ostracism or inclusion by two co-present robots during a teamwork task. During ostracism, the robots communicated in an unknown language. Findings reveal that verbal ostracism by robots negatively influences basic psychological needs and fosters prosocial behavior towards other humans in later \r\ninteractions. Implications for mixed human-robot teams in the workplace are discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bochum",
              "institution": "Ruhr University Bochum",
              "dsl": ""
            }
          ],
          "personId": 152337
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bochum",
              "institution": "Ruhr University Bochum",
              "dsl": ""
            }
          ],
          "personId": 151596
        }
      ]
    },
    {
      "id": 152671,
      "typeId": 13315,
      "title": "RainbowBot: Robotic Tableware for Children's Dietary Habits",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1028",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152727
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent and Interactive Robotics"
            }
          ],
          "personId": 152126
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea Institute of Science and Technology",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 151921
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent & Interactive Robotics"
            }
          ],
          "personId": 151894
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "KIST",
              "dsl": "Center for Intelligent and Interactive Robotics"
            }
          ],
          "personId": 139808
        }
      ]
    },
    {
      "id": 152672,
      "typeId": 13321,
      "title": "The Effect of Predictive Formal Modelling at Runtime on Performance in Human-Swarm Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1074",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Formal Modelling is often used as part of the design and testing process of software development to ensure that components operate within suitable bounds even in unexpected circumstances. In this paper, we use predictive formal modelling (PFM) at runtime in a human-swarm mission and show that this integration can be used to improve the performance of human-swarm teams. We recruited 60 participants to operate a simulated aerial swarm to deliver parcels to target locations. In the PFM condition, operators were informed of the estimated completion times given the number of drones deployed, whereas in the No-PFM condition, operators did not have this information. The operators could control the mission by adding or removing drones from the mission and thereby, increasing or decreasing the overall mission cost. Our results show that PFM modelling at runtime improves mission performance without significantly affecting the operator's workload or the system's usability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Hampshire",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "Electronics and Computer Science"
            }
          ],
          "personId": 151844
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "Electronics and Computer Science"
            }
          ],
          "personId": 151614
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "Computing Science"
            }
          ],
          "personId": 152189
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 151535
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bournemouth",
              "institution": "Bournemouth University",
              "dsl": "Computing and Informatics"
            }
          ],
          "personId": 151904
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Laboratory, School of Computer Science"
            }
          ],
          "personId": 151642
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": "ECS"
            }
          ],
          "personId": 151711
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": ""
            }
          ],
          "personId": 151929
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 151721
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 151930
        }
      ]
    },
    {
      "id": 152673,
      "typeId": 13321,
      "title": "Electromyography-based Kinesthetic Teaching of Industrial Collaborative Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1195",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Current methods for robot teaching still lack intuitiveness and efficiency. This poses problems, especially for companies that cannot afford dedicated robot programmers. Classical online teaching with a teach pendant (TP) can be tedious and confusing, and kinesthetic teaching (KT) is often perceived as inefficient since toggling gravity compensation mode forces users to switch between interfaces or constrains them physically. We propose a novel robot teaching method that allows users to activate gravity compensation mode, confirm positions along trajectories, and manipulate the end-effector. In our system, this is done via hand gestures that occur naturally while handling the robot and that we detect using an electromyography (EMG) armband. To evaluate our system, we compared it to a commercially available KT system in a user study that yielded statistical evidence that our approach is significantly faster while no difference regarding the perceived usability of the systems was found. Additionally, expert interviews confirm that the baseline system is state of the art and confirmed the market potential of EMG-based KT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": "Interactions Research Group"
            }
          ],
          "personId": 151617
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": "Interactions Research Group"
            }
          ],
          "personId": 152161
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": "Interactions Research Group"
            }
          ],
          "personId": 152063
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "St. Gallen",
              "city": "St. Gallen",
              "institution": "University of St. Gallen",
              "dsl": "Interactions Research Group"
            }
          ],
          "personId": 152194
        }
      ]
    },
    {
      "id": 152674,
      "typeId": 13321,
      "title": "Transparency Classification for HRI with Humanoid Service Robots",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1196",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "As humanoid service robots are increasingly becoming part of our everyday life, their acceptance in society requires a general understanding of those robots. Transparency can be a means to establish this. Complementary to the general IEEE Standard 7001-2021, a new transparency classification for humanoid service robots is proposed in this work as a baseline for the goal-oriented definition of concrete transparency mechanisms. Here, Content Transparency and Interaction Transparency are introduced, separating transparency of service content information from effective communication and robot usage. Interaction Transparency is further subdivided to include interaction aspects that are unique to humanoid robots. Further, it integrates Traceability and Privacy. Overall, it covers a larger range of interaction aspects than common transparency models. The proposed transparency classification also supports the systematic investigation of individual transparency mechanism’s influence on the interaction. For this, novel classification-specific scales should be developed to asses the perception of, e.g., Interaction Transparency.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt ",
              "dsl": ""
            }
          ],
          "personId": 152219
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": "Simulation, Systems Optimization and Robotics Group"
            }
          ],
          "personId": 151820
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": "Simulation, Systems Optimization and Robotics Group"
            }
          ],
          "personId": 151468
        }
      ]
    },
    {
      "id": 152675,
      "typeId": 13321,
      "title": "Unveiling the Dynamics of Human Decision-Making: From Strategies to False Beliefs in Collaborative Human-Robot Co-Learning Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1197",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "As robots become more integrated into humans' daily activities, it is essential to understand how human decision varies during co-learning with robots in real-world scenarios. Despite great advances in developing humanoid robots, which aims to foster a seamless collaborative world where humans and robots coexist, a gap remains in the social bond between humans and robots, particularly in tasks demanding optimal teamwork. In alignment with current pioneering efforts in the human-robot collaboration field, this paper presents an experimental study leading to a rationale analysis and classification of human behavioral dynamics during a joint collaborative pick-and-place task with a robotic arm. Our post-experimental analysis categorized human behavioral dynamics into three distinct broad categories, which are ``strategic explorers and decoders\", ``reactive navigators and dynamic responders\", and ``score maximizers and ideal collaborators\". We provide in-depth analysis for each group, exploring potential reasons for their observed behavioral patterns and irrational decisions substantiated by intuitions from psychological and behavioral game theory, including concepts of false belief and strategy development.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 152276
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Electrical and Computer Engineering Dept"
            }
          ],
          "personId": 152136
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 151949
        }
      ]
    },
    {
      "id": 152676,
      "typeId": 13316,
      "title": "Enhancing Robot Perception for Real-World HRI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1046",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Robot perception often fails in uncontrolled environments due to unfamiliar object classes, different domains, or hardware issues. This poses significant challenges for human-robot interaction outside of a lab or user study settings. There are multiple ways of recovering from robot perceptual failures. In my work, I focus on two separate approaches: first, improving robot perception systems directly by enhancing the robustness of deep learning models for 3D object detection and designing them so they are less prone to corrupted data (sparse, partially missing, or coming from misaligned sensors). Additionally, I developed an advanced domain adaptation method that allows the robots to build their perception system on large open-source datasets. Notwithstanding, such systems are unlikely to always be correct. Consequently, the second part of my research focuses on developing systems where users can directly correct robot errors themselves, thereby increasing transparency and the robustness of the interaction. In doing so, my research strives to bridge the gap between controlled lab studies and unpredictable real-world scenarios, fostering more robust and reliable human-robot collaboration by minimizing vision errors and allowing users to deal with them if they occur.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Instutue of Technology",
              "dsl": "Robotics, Perception and Learning"
            }
          ],
          "personId": 152134
        }
      ]
    },
    {
      "id": 152677,
      "typeId": 13316,
      "title": "End User Interfaces for Collaborative Robot Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1047",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Collaborative robots (cobots) are increasingly utilized within the manufacturing industry. However, despite the promise of collaboration and easier programming when compared to traditional industrial robots, cobots introduce new interaction paradigms that require additional thought to fully realize their collaboration capabilities. Due to these added complexities, cobots have been found to be underutilized for their collaboration capabilities in current manufacturing. Therefore, in order to make cobots more accessible and easy to use, new systems need to be developed that abstract and simplify their interaction. In this extended abstract, I propose a set of tools that target the use of cobots for multiple groups of individuals that use them, in order to better support users and simplify cobot collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 140054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin-Madison",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 139750
        }
      ]
    },
    {
      "id": 152678,
      "typeId": 13321,
      "title": "Are robots’ gestures understood? A study on the factors influencing how humans perceive information present in robot gestures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1198",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Social robots become increasingly important in various social do-\r\nmains like healthcare, education, and industry. In this paper, we\r\nexplore whether humans understand the meaning conveyed by ro-\r\nbot gestures when they occur alongside speech and other co-speech\r\ngestures with no meaning. We analyzed human comprehension\r\nof basic shapes presented through robot gestures varying gesture\r\nsize and verbal context. Our findings show that humans notice\r\nrobot gestures but struggle to understand the information provided\r\nby them. Explicitly directing attention to robot gestures improves\r\nunderstanding. Moreover, providing indirect information about\r\nrobot’s capabilities to gesture enhances the human ability to ex-\r\ntract the correct information from gestures, with the effect linearly\r\nincreasing with the number of observations of the robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "University Bielefeld",
              "dsl": ""
            }
          ],
          "personId": 151598
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bielefeld",
              "institution": "Bielefeld University",
              "dsl": ""
            }
          ],
          "personId": 152340
        }
      ]
    },
    {
      "id": 152679,
      "typeId": 13321,
      "title": "More Than Meets the Eye? An Experimental Design to Test Robot Visual Perspective-Taking Facilitators Beyond Mere-Appearance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1070",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Visual Perspective Taking (VPT) underpins human social interaction, from joint action to predicting others' future actions and mentalizing about their goals and affective/mental states. Substantial progress has been made in developing artificial VPT capabilities in robots. However, as conventional VPT tasks rely on the (non-situated, disembodied) presentation of robots on computer screens, it is unclear how a robot’s socially reactive and goal-directed behaviours prompt people to take its perspective. We provide a novel experimental paradigm that robustly measures the extent to which human interaction partners take a robot’s visual perspective during face-to-face human-robot-interactions, by measuring how much a robot’s visual perspective is spontaneously integrated with one’s own. The experimental task design of our upcoming user study allows us to investigate the role of robot features beyond its human-like appearance, which have driven research so far, targeting instead its socially reactive behaviour and task engagement with the human interaction partner.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Aberdeen",
              "institution": "University of Aberdeen",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 151853
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Norwich",
              "institution": "University of East Anglia",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 152280
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": ""
            }
          ],
          "personId": 152234
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Please select region, state or province",
              "city": "Aberdeen",
              "institution": "University of Aberdeen",
              "dsl": ""
            }
          ],
          "personId": 152001
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Scotland",
              "city": "Aberdeen",
              "institution": "University of Aberdeen",
              "dsl": "School of Psychology"
            }
          ],
          "personId": 152081
        }
      ]
    },
    {
      "id": 152680,
      "typeId": 13321,
      "title": "Unraveling Human-Robot Interaction in E-Commerce: The Role of Personality Traits and Chatbot Mechanisms – A Neuromarketing Research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1192",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This paper explores the intersection of neuromarketing, e-commerce, and human-robot interaction by investigating the impact of personality traits on user satisfaction, purchase intention, and brainwave patterns across different chatbot models (rule-Based vs generative AI) and platforms (virtual chatbots vs humanoid robots). The study introduces Generative AI chatbots to e-commerce websites, comparing their effectiveness with rule-based chatbots. Additionally, physical robots are included as a reference group to assess the effects of virtual and physical robots in shopping assistance. The manipulation of three personality traits (introvert, ambivert, and extrovert) in both online and offline settings enriches the understanding of user behavior in diverse scenarios. Data collection involves surveys, EEG measurements, and system logs to capture subjective perceptions, unconscious reactions, and decision-making processes. The research aims to provide insights for human-computer interaction design, contributing to the evolving field of neuromarketing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 152144
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "University of Texas at Austin",
              "dsl": "School of Information"
            }
          ],
          "personId": 152316
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Chengchi University",
              "dsl": ""
            }
          ],
          "personId": 152351
        }
      ]
    },
    {
      "id": 152681,
      "typeId": 13321,
      "title": "Ain’t Misbehavin’ – Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1193",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Social robots aim to establish long-term bonds with humans through engaging conversation.However, traditional conversational approaches, reliant on scripted interactions, often fall short in maintaining engaging conversations. This paper addresses this limitation by integrating Large Language Models (LLMs) into social robots to achieve more dynamic and expressive conversations.\r\n\r\nWe introduce a fully-automated conversation system that leverages LLMs to generate robot responses with expressive behaviors, congruent with the robot's personality. We incorporate robot behavior with two modalities: 1) a  TTS engine capable of various delivery styles, and 2) a library of physical actions for the robot. We develop a custom, state-of-the-art emotion recognition model to dynamically select the robot's tone of voice and utilize emojis from LLM output as cues for generating robot actions.\r\n\r\nTo shed light on potential design and implementation issues, we conduct a pilot study where human volunteers chat with a social robot using our proposed system, and we analyze their opinions of the chat experience, conducting a rigorous error analysis of chat transcripts. The feedback was overwhelmingly positive, with participants commenting on the robot's empathy, helpfulness, naturalness, and entertainment. Most of the negative feedback was due to ASR errors which had limited impact on conversation. However we observed a small class of LLM errors, such as the LLM repeating itself or hallucinating fictitious information and human responses, that have the potential to derail conversations. Our findings raise important issues to consider in applying LLMs to social robots.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": ""
            }
          ],
          "personId": 152235
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Saitama",
              "city": "Wakoshi",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151704
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Beyond Reason",
              "dsl": ""
            }
          ],
          "personId": 151806
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Wako",
              "institution": "Honda Research Institute Japan",
              "dsl": ""
            }
          ],
          "personId": 151955
        }
      ]
    },
    {
      "id": 152682,
      "typeId": 13321,
      "title": "Social robots in hospital settings: An initial exploration of the services provided, interaction style and in the field evaluation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1194",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "There is interest in using social robots in hospitals but little understanding of how they engage with users to fulfil their roles. Research in the field helps understand how HRI may occur naturally. We conducted a scoping review of literature on social robots in hospital settings, using the Arksey and O’Malley 5-step method. This report presents an initial synthesis of 19 studies. The robots performed various tasks, from greeting and educating visitors to social companionship, supporting healthcare delivery, carrying goods, and educating staff. Most were physically embodied, but three were embodied conversational agents (virtual robots). To engage with interaction partners, 89% used speech and 79% used motions, gestures, and facial expressions. Less commonly used was written text and tactile interaction. Further personalizing interactions, introducing creativity, and focusing on the wild aspects of HRI could help support the application of social robots in hospital settings. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": ""
            },
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": ""
            }
          ],
          "personId": 152149
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": ""
            }
          ],
          "personId": 151631
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Psychological Medicine"
            }
          ],
          "personId": 152174
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": ""
            }
          ],
          "personId": 151705
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Electrical, Computer and Software Engineering"
            }
          ],
          "personId": 151950
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": ""
            }
          ],
          "personId": 152281
        }
      ]
    },
    {
      "id": 152683,
      "typeId": 13321,
      "title": "Pseudo-haptics Interfaces for Robotic Teleoperation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1073",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "When remotely operating robotic systems, the situation awareness and the absence of the possibility of direct human touch in such a remote environment constitute major challenges in telerobotics. Haptic feedback has been playing an important role when people interact with remote environments (e.g., in robotic teleoperation) or provide more immersive experiences in virtual environments. Like haptic devices, pseudo-haptic techniques aim to simulate haptic sensations in human–computer interaction between real and remote or virtual worlds, by exploring multimodal feedback, mainly the visual, and the brain’s capabilities and limitations, without needing a haptic device to be attached or applied to the body. The authors discuss the possibility of exploring pseudo-haptic techniques, notably combined multimodal techniques, to improve robotic teleoperation, in remote vehicle driving, object manoeuvering, situation awareness, and collaborative tasks, which as per the best authors' knowledge has not been explored in the literature.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Instituto Superior Tecnico",
              "dsl": ""
            }
          ],
          "personId": 152050
        }
      ]
    },
    {
      "id": 152684,
      "typeId": 13321,
      "title": "BCI Exploration of User Responses to Vulnerable and Expressive Robot Behaviors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1080",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "The manufacturing sector serves as a hub for human-robot interaction (HRI), employing robots for diverse tasks. Collaborative robots, working alongside humans in shared workspaces, aim to enhance workplace efficiency. However, this close proximity can pose challenges, leading to feelings of stress and concern among human users. Ensuring safety, both physically and psychologically, is crucial in HRI, especially during physical collaboration. Assessing the physiological safety of users involves various methods, including utilizing physiological signals as novel approaches. This study employs a brain-computer interface (BCI) experiment using noninvasive EEG signals to explore how these signals can interpret human responses to different robotic behaviors in HRI scenarios. Results reveal distinct human brain responses triggered by various robot behaviors, detected through the analysis of electrical brain activity, however this is just a preliminary study and more analyses are needed. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Southfield",
              "institution": "Lawrence Technological University",
              "dsl": "Mechanical, Robotics, and Industrial Engineering"
            }
          ],
          "personId": 152212
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Alabama",
              "city": "Tuscaloosa",
              "institution": "The University of Alabama",
              "dsl": "mechanical Engineering"
            }
          ],
          "personId": 152033
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Alabama",
              "city": "Tuscaloosa ",
              "institution": "University of Alabama",
              "dsl": "Computer Science"
            }
          ],
          "personId": 152170
        }
      ]
    },
    {
      "id": 152685,
      "typeId": 13321,
      "title": "A Generalizable Architecture for Explaining Robot Failures Using Behavior Trees and Large Language Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1085",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "As robots are more commonly being deployed in shared human-robot environments, the need for robots to communicate their failures and answer questions about them becomes increasingly important. This paper describes a generalizable architecture, using Behavior Trees and Large Language Models, to generate explanations and answer follow up questions. We compare responses from our new system to those from existing templated systems, and find that our system produces comparable and accurate results. Finally, we propose a set of user studies to evaluate the effectiveness and understandability of our new explanation architecture. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 151484
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 152018
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 139907
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Lowell",
              "institution": "University of Massachusetts Lowell",
              "dsl": "Richard A. Miner School of Computer & Information Sciences"
            }
          ],
          "personId": 139922
        }
      ]
    },
    {
      "id": 152686,
      "typeId": 13321,
      "title": "From Aisles to Smiles: Carrey’s Impact on Customer Journey",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1087",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "In today’s retail landscape, customer experience reigns supreme. To thrive, businesses must strive for deeper connections with their patrons, extending beyond mere transactions. This paper explores the potential of Carrey, a novel AI-powered robot designed specifically for the retail environment. We argue that Carrey’s unique capabilities can foster visual, emotional, and cultural connections with customers, leading to a significantly enhanced shopping experience. In Visual Connect, Carrey’s expressive design and dynamic movements create a visually engaging presence, drawing customers in and sparking curiosity. Its interactive interface allows for intuitive and natural interaction, fostering a sense of familiarity and ease. For Emotional Connect, Carrey’s AI capabilities enable it to respond to customer emotions and adapt its communication style accordingly. This personalized approach fosters empathy and builds trust, creating a sense of genuine connection that goes beyond a traditional sales interaction. In Cultural Connect, Carrey’s ability to adapt to different cultural norms and languages allows it to connect with customers on a deeper level. This cultural sensitivity creates a more inclusive and welcoming environment, catering to a diverse customer base. Through its ability to forge these connections, Carrey can revolutionize the way customers interact with retail spaces.\r\nBy fostering a more engaging, emotionally resonant, and culturally aware experience, Carrey paves the way for increased customer satisfaction, loyalty, and ultimately, business success.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Robotics Institute"
            }
          ],
          "personId": 152206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "The University of Texas at Arlington",
              "dsl": ""
            }
          ],
          "personId": 151972
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Carlsbad",
              "institution": "Adtech",
              "dsl": ""
            }
          ],
          "personId": 152153
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft ",
              "dsl": ""
            }
          ],
          "personId": 151713
        }
      ]
    },
    {
      "id": 152687,
      "typeId": 13316,
      "title": "Customizing Tele-Operation Interfaces of Assistive Robots at Home with Occupational Therapists",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1057",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "To assist individuals with motor limitations in activities of daily living (ADLs), robots need to adapt to their user's unique needs, preferences, and environments. In this work we developed an Accessible Teleop Toolkit for the Stretch RE2 robot, employing tele-operation to overcome the challenges of operating in highly unstructured homes, while enabling accessibility of the interface through customization. We evaluated the impact of customization via a user study involving 10 individuals with motor impairments and 13 without. This lead to a deployment in a home for several weeks facilitated by an occupational therapist and enabling a user with quadriplegia to perform tasks more independently. We are currently working on increasing autonomy of the robot and developing a framework for integrating assistive robots into occupational therapy practices. This work aims to empower users and occupational therapists in optimizing assistive robots for individual needs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": ""
            }
          ],
          "personId": 152332
        }
      ]
    },
    {
      "id": 152688,
      "typeId": 13321,
      "title": "Effect of Wait Time on Trust and Reliance in Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1088",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study investigated how waiting time for a robot's action influences trust in and reliance on a robot. Experiment 1 was conducted as an online experiment in which the waiting time for the task partner's (human, AI or robot) action was manipulated from 1 to 20 seconds. The results confirmed that participants' trust in and reliance on each partner was influenced by the length of the waiting time. They were also tolerant of waiting 16 to 20 seconds for a robot's action. Experiment 2 was conducted to confirm the effects of waiti time on trust and reliance in a face-to-face human-robot situation. The results confirmed that participants' trust and reliance were affected by the length of the wait time period. Moreover, the effect of wait time on reliance was also mediated by trust. This study showed that wait time is a strong and controllable factor influencing trust in and reliance on a robot.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shizuoka",
              "institution": "Shizuoka University",
              "dsl": "Center for Research and Development in Admissions"
            }
          ],
          "personId": 151698
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of Informatics",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "SOKENDAI (The Graduate University for Advanced Studies)",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151877
        }
      ]
    },
    {
      "id": 152689,
      "typeId": 13316,
      "title": "Investigating Mind Perception in HRI through Real-Time Implicit and Explicit Measurements",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1051",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Social robots have revolutionized societal interaction and communication. Our perception of robots involves multiple facets establishing the communicative background for interaction. In our study, we investigate the factors influencing Agency and Experience, which are two dimensions of mental capacity attribution that shape the quality of human-robot interaction (HRI). Our research highlights three distinctive aspects: Firstly, we concurrently investigate perceiver determinants, such as individual and generational traits, along with those linked to perceived agents and their actions, enabling a comprehensive exploration of potential causes and consequences. Second, our innovative, naturalistic setup allows us to present live actions by physically present human and robot actors while maintaining experimental control. Lastly, we employ a comprehensive approach, combining both implicit and explicit measurements using the stimuli that we normed and validated in our previous works. We collected in-person data from 160 individuals spanning four generations, aged 18-73. Upcoming steps involve data analysis and result discussion. This study contributes to a deeper understanding of mind perception in HRI, a topic of significant interest and ongoing debate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Middle East Technical University",
              "dsl": "Department of Cognitive Sciences"
            }
          ],
          "personId": 152221
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Krakow",
              "institution": "Jagiellonian University",
              "dsl": "Cognitive Science Department"
            }
          ],
          "personId": 151962
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "",
              "city": "Ankara",
              "institution": "Bilkent University",
              "dsl": "Psychology and Neuroscience"
            }
          ],
          "personId": 152262
        }
      ]
    },
    {
      "id": 152690,
      "typeId": 13321,
      "title": "Towards Explainable Proactive Robot Interactions for Groups of People in Unstructured Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1081",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "For social robots to be able to operate in unstructured public spaces, they need to be able to gauge complex factors such as human-robot engagement and inter-person social groups, and be able to decide how and with whom to interact. Additionally, such robots should be able to explain their decisions after the fact, to improve accountability and confidence in their behavior. To address this, we present a two-layered proactive system that extracts high-level social features from low-level perceptions and uses these features to make high-level decisions regarding the initiation and maintenance of human-robot interactions. With this system outlined, the primary focus of this work is then a novel method to generate counterfactual explanations in response to a variety of contrastive queries. We provide an early proof of concept to illustrate how these explanations can be generated by leveraging the two-layer system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robotica i Informàtica Industrial",
              "dsl": "Perception and Manipulation"
            }
          ],
          "personId": 152312
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Pal Robotics",
              "dsl": ""
            }
          ],
          "personId": 152379
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Barcelona",
              "institution": "Institut de Robotica i Informàtica Industrial",
              "dsl": ""
            }
          ],
          "personId": 151884
        }
      ]
    },
    {
      "id": 152691,
      "typeId": 13316,
      "title": "Investigating Robot Influence on Human Behaviour By Leveraging Entrainment Effects",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1052",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "Humans naturally tend to synchronize their movements with others, a phenomenon known as the entrainment effect, whether voluntarily or involuntarily. This phenomenon extends to interactions between humans and robots, which could have either positive or negative consequences for the human partner. We propose a human-subject study aimed at investigating the use of robots to influence human behaviour through entrainment in diverse Human-Robot Interaction (HRI) scenarios. The current work involves two human-subject experiments investigating the impact of robots on short-term human behaviour, encompassing human-human and human-robot interactions. The goal is to comprehend how variations in robot actions, such as movement frequency during repetitive tasks, influence human perceptions and behaviours in collaborative lab-based settings. Another objective is to investigate the factors that make participants aware of the entrainment effect during HRI. The preliminary results of the HHI experiment provide evidence that individuals tend to synchronize their movements with another person.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": "Responsible Autonomous & Intelligent System Ethics (RAISE) lab"
            }
          ],
          "personId": 152139
        }
      ]
    },
    {
      "id": 152692,
      "typeId": 13321,
      "title": "Synergizing Natural Language Towards Enhanced Shared Autonomy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1083",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Shared autonomy can be beneficial in allowing users of assistive robots to refine the robot's behavior and ensure it is adapted to their needs. However, current methodologies mostly focus on using joysticks or physical pushes to modify robots' trajectories, which may not be feasible for people with reduced mobility. In this paper, we present our initial work toward voice-based shared autonomy, more specifically, we describe a language model which can use sequences of verbal commands to understand the intended correction direction. Our fine-tuned model shows improved efficiency in complex and realistic sentences compared to recent generative pre-trained transformer (GPT) models, and can turn locally on a simple CPU.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institute",
              "dsl": ""
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "École Polytechnique Fédérale de Lausanne",
              "dsl": ""
            }
          ],
          "personId": 151532
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institue",
              "dsl": ""
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "École Polytechnique Fédérale de Lausanne",
              "dsl": ""
            }
          ],
          "personId": 151686
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "École Polytechnique Fédérale de Lausanne",
              "dsl": ""
            }
          ],
          "personId": 152378
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Martigny",
              "institution": "Idiap Research Institute",
              "dsl": ""
            }
          ],
          "personId": 139753
        }
      ]
    },
    {
      "id": 152693,
      "typeId": 13321,
      "title": "Probing the Inductive Biases of a Gaze Model for Multi-party Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1084",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The behavior management controls proposed for social robots are mostly designed for highly controlled scenarios. In the real world though, robots have to adapt to new situations, generalizing learned behaviors. To address this adaptation challenge, neural network models with embedding layers could be used. We present here an approach to better understand the inductive biases of our robotic gaze model. It was trained with multimodal features as inputs -- either endogenous or exogenous to the robot. Inductive biases were explored by observing feature representations in the embedding spaces. We found that the model was able to distinguish between the robot verbal intentions that either request or provide information. Similarly, pairs of partners seem grouped according to their social behavior (speaking time, gaze). Finally, we checked that these groupings had a real impact on the model's performance. Driving these biases when facing new people should allow to generate adapted behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "Grenoble INP-UGA",
              "dsl": "GIPSA-lab"
            },
            {
              "country": "France",
              "state": "",
              "city": "Echirolles",
              "institution": "ATOS",
              "dsl": ""
            }
          ],
          "personId": 152315
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "CNRS, UGA & INP-Grenoble",
              "dsl": "GIPSA-Lab"
            }
          ],
          "personId": 151958
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "ATOS",
              "dsl": ""
            }
          ],
          "personId": 151707
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Grenoble",
              "institution": "CNRS, UGA & INP-Grenoble",
              "dsl": "GIPSA-Lab"
            }
          ],
          "personId": 152095
        }
      ]
    },
    {
      "id": 152694,
      "typeId": 13321,
      "title": "\"Noisy\" matching of motion velocity of an assistive robot to the users' walking velocity",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1090",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "This study investigates the impact of dynamic matching of robot motion velocity to users' walking velocity when a robot and user approach each other on perceived comfort, interactivity, and naturalness. Considering age diversity, participants were divided into two age groups. Young participants showed increased ratings and perceived naturalness with dynamic matching. In contrast, elderly participants preferred a steady and slow robot motion for comfort and predictability. The findings highlight the importance of tailoring robot behaviors based on user demographics for positive Human-Robot Interactions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": ""
            }
          ],
          "personId": 151467
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "OSLO",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151782
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion"
            }
          ],
          "personId": 151843
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "Department of Informatics, University of Oslo",
              "dsl": ""
            }
          ],
          "personId": 151829
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 151980
        }
      ]
    },
    {
      "id": 152695,
      "typeId": 13316,
      "title": "Shaping Relatable Robots: A Child-Centered Approach to Social Personalization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1060",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "While social robots present significant potential in education, not all children find the interaction with the robot to be relatable. We present a child-centered research approach, through which we a)actively involve children in shaping the interaction content and b)personalize the interaction with the co-designed content. We applied this method in the first study (𝑛 = 102, 8-13 y.o) where we designed robot humor that is tailored to different age groups. Results indicate that children found age-personalized humor more amusing\r\nand felt more affinity with it, both personally and at the group level. Our forthcoming longitudinal study will focus on enhancing children’s reading motivation through robot interactions. The co-design activity will not only include children, but also domain experts. We plan to investigate how generative AI may efficiently scale the co-design process to match the increased demand for personalized content, while still keeping the stakeholders involved",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit ",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit ",
              "dsl": ""
            }
          ],
          "personId": 151610
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Twente",
              "institution": "University of Twente",
              "dsl": ""
            }
          ],
          "personId": 151897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "VU University",
              "dsl": ""
            }
          ],
          "personId": 139779
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Vrije Universiteit Amsterdam",
              "dsl": "Social AI"
            }
          ],
          "personId": 140046
        }
      ]
    },
    {
      "id": 152696,
      "typeId": 13321,
      "title": "What Is It Like for Visually Impaired Individuals To Touch a Table-Top Humanoid Robot?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1096",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152726
      ],
      "eventIds": [],
      "abstract": "To ensure that robots can effectively serve all individuals in public facilities, it is essential to consider accessibility and inclusivity. In particular, tactile interaction might play a significant role in engaging with visually impaired individuals. This study explored the potential of hands-on interaction between visually impaired individuals and a table-top humanoid robot. Five participants with visual impairments were recruited and asked to interact with the robot. The study consisted of two conditions: one with gestures from the robot and one without gestures. When they interacted with the robot showing gestures, they reported a better impression of the robot and improved accessibility. The findings of this research provide valuable insights into the potential of table-top humanoid robots in public spaces for all.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 151937
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Institute of Library, Information and Media Science"
            }
          ],
          "personId": 152250
        }
      ]
    },
    {
      "id": 152697,
      "typeId": 13321,
      "title": "Let’s make this fun!: Activities to motivate children and teens to complete questionnaires",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1099",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "Human-robot interaction research frequently uses questionnaires, but these are boring, and participants, particularly children and teenagers, lack motivation to complete them. To improve participants' enjoyment, engagement, and motivation during this process, we propose adding a fun activity after questionnaire completion, where a completed questionnaire is necessary for participation. We trialed several ideas during a slow game jam with 11--16-year-olds and a participatory design workshop for zoomorphic robots with 8--11-year-olds. Based on this initial exploratory work, we feel that these ideas were engaging for participants, helped with quick completion of the questionnaires, and with building of rapport between the researchers and participants. Therefore, we present some guidelines for researchers wanting to use a fun activity or a series of fun activities like this. Further investigation is needed to establish if this approach has an impact on response quality.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 151578
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 151602
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Mathematical and Computer Sciences"
            }
          ],
          "personId": 152048
        }
      ]
    },
    {
      "id": 152698,
      "typeId": 13321,
      "title": "Spot Report: An Open-Source and Real-Time Secondary Task For Human-Robot Interaction User Experiments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1092",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "The human-robot interaction (HRI) community is interested in a range of research questions, many of which are investigated through user experiments. Robots that occasionally require human input allow for humans to engage in secondary tasks. However, few secondary tasks transmit data in real-time and are openly available, which hinders interaction with the primary task and limits the ability of the community to build upon others' research. Also, the need for a secondary task relevant to the military was identified by subject matter experts. To address these concerns, this paper presents the spot report task as an open-source secondary task with real-time communication for use in HRI experiments. The spot report task requires counting target objects in static images. This paper includes details of the spot report task and real-time communication with a primary task. We developed the spot report task considering the military domain, but the software architecture is domain-independent. We hope that others can leverage the spot report task in their own user experiments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 139860
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 151438
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan - Ann Arbor",
              "dsl": "Robotics Department"
            }
          ],
          "personId": 140023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "McLean",
              "institution": "MITRE Corporation",
              "dsl": ""
            }
          ],
          "personId": 152324
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Warren",
              "institution": "Ground Vehicle Systems Center",
              "dsl": ""
            }
          ],
          "personId": 151818
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Warren",
              "institution": "Ground Vehicle Systems Center",
              "dsl": ""
            }
          ],
          "personId": 151628
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 139939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 151871
        }
      ]
    },
    {
      "id": 152699,
      "typeId": 13321,
      "title": "Compete, Cooperate, or Both? Exploring How Interaction Settings Shape Human-Robot Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1093",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152722
      ],
      "eventIds": [],
      "abstract": "Robots and humans interact in different interaction settings, typically encompassing cooperation and competition. However, research in human-human interaction indicates that a combination of both settings might be most beneficial. Moreover, as the cooperative competition builds a potential in-group with the robot and a potential out-group of another team, the mind perception might be influenced. In a laboratory experiment participants (N =66) interacted with a social robot in either cooperation, competition, or cooperative competition. The results, despite a successful manipulation check, revealed no significant differences in enjoyment and engagement. However, the robot was ascribed with significantly more agency in the cooperative competition compared to the competition. Moreover, exploratory analyses revealed higher performance in the competitive compared to the cooperative interaction but not compared to the cooperative competition. Our study, while needing validation with larger sample sizes, suggests a potential positive effect of combining cooperation and competition, notably in increasing agency.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human-Agent Collaboration Lab"
            }
          ],
          "personId": 139956
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": ""
            }
          ],
          "personId": 152243
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Technische Universität Berlin",
              "dsl": ""
            }
          ],
          "personId": 152307
        }
      ]
    },
    {
      "id": 152700,
      "typeId": 13321,
      "title": "Respiration-enhanced Human-Robot Communication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1094",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152724
      ],
      "eventIds": [],
      "abstract": "In the field of Human-Robot Interaction (HRI), enhancing a robot's impression, affinity, and interaction smoothness is crucial for real-world applications. \r\nThis study explores the potential of respiratory information in achieving these goals. \r\nWe develop a robotic system that presents pseudo-respiration and utilizes human respiratory patterns to prevent overlapping speech between humans and robots, aiming to create smoother and more engaging interactions.\r\nOur current experiments focus on evaluating the impact of these features on the robotic system's impression and affinity. \r\nThe primary objective is to assess how integrating respiratory information can improve the quality of HRI and contribute to enhancing interaction dynamics. \r\nThrough this research, we seek to provide concrete evidence of the benefits of incorporating respiratory information into HRI, offering insights that could inform future developments in robot-human interaction.\r\nA preliminary analysis from the ongoing experiments confirms a promising result.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 151474
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Tokyo Institute of Technology",
              "dsl": "FIRST, IIR"
            }
          ],
          "personId": 152138
        }
      ]
    },
    {
      "id": 152701,
      "typeId": 13321,
      "title": "Softy: an Interactive Kit to Revitalize the Plush Toys of Children.",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24f-1095",
      "source": "PCS",
      "trackId": 12626,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152723
      ],
      "eventIds": [],
      "abstract": "Children across the globe experience the phenomenon of quickly losing interest in their toys, leading to the accumulation and wasteful disposal of toys. After conducting research and considering the characteristics of toys along with their potential use cases, we propose Softy, a modular interactive kit designed for plush toys. Corresponding to envisioned application scenarios, we have devised two types of modules and three interaction modes. This paper elucidates the prototype design and development of Softy, delving into the possibilities of rekindling children's interest in toys by incorporating interactive movements into them.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151674
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Academy of Arts & Design",
              "dsl": "Tsinghua University"
            }
          ],
          "personId": 151759
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 151515
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 152179
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Tsinghua University"
            }
          ],
          "personId": 151797
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Future Laboratory"
            }
          ],
          "personId": 151891
        }
      ]
    },
    {
      "id": 152702,
      "typeId": 13316,
      "title": "Towards Heterogeneous Multi-Agent Systems in Space",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24a-1065",
      "source": "PCS",
      "trackId": 12629,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152787,
        152788
      ],
      "eventIds": [],
      "abstract": "As people venture into space, they will bring with them fleets of\r\nrobots. Maintaining people’s trust in their robot teammates needs\r\nto be studied in a way that is realistic about robots being less than\r\ncompletely reliable. It has been shown in the past that when one\r\nagent in a system is unreliable, people tend to stop using the whole\r\nsystem. To mitigate this, we created a single identity that was\r\nshared among all the agents in a multi-agent system. We conducted\r\na study where participants worked to make repairs on a lunar base\r\nwith robots that made errors. Preliminary results show that a shared\r\nidentity mental model is better at maintaining system-wide trust\r\nthan a model where the agents had separate identities. In my future\r\nwork, I wish to investigate how this shared identity could be used\r\nto make the loss of robots in space (like the Opportunity Rover)\r\nless distressing to their human teammates, and study methods for\r\nvisualizing shared identity and its transfer through a system of\r\nmachine agents.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": "Human Factors and Applied Cognition"
            }
          ],
          "personId": 151847
        }
      ]
    },
    {
      "id": 152703,
      "typeId": 13315,
      "title": "The Socially Therapeutic Assistive Robot (STAR) Therabot and Its Modular Sensor Collar",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1045",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152728
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 140065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Stars Lab"
            }
          ],
          "personId": 140013
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Starkville",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science & Engineering"
            }
          ],
          "personId": 151465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 152383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississippi State University",
              "dsl": "Social Science Research Center"
            }
          ],
          "personId": 152289
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Mississippi",
              "city": "Mississippi State",
              "institution": "Mississsippi State University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 139769
        }
      ]
    },
    {
      "id": 152704,
      "typeId": 13315,
      "title": "A Humanoid Robot Platform for Efficient Gaze Interaction and Physical  Manipulation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24b-1048",
      "source": "PCS",
      "trackId": 12630,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152728
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Odense",
              "institution": "University of Southern Denmark",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 151592
        }
      ]
    },
    {
      "id": 152705,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "The Purr-suit of Happiness: A Tale of Three Kittens. Robots, Humans, Cats, and AI",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1047",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "This paper showcases Cat Royale, an exploration of the impact of artificial intelligence (AI) on animal happiness situated at the intersection of Art, Computer Science, and Animal Welfare. We argue for the inclusion of non-human actors when designing autonomous systems, as animals increasingly interact with them. In this endeavour, we emphasise multidisciplinarity when designing trustworthy autonomous system. To design, implement, and deploy such systems, diverse voices must be heard. Finally, by highlighting parallels between Cat Royale’s animal-robot interactions and human-AI interactions, this project invites reflections on the trustworthiness, risks, and the price we might pay for AI.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 151535
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Mixed Reality Lab"
            }
          ],
          "personId": 151745
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "Blast Theory",
              "dsl": ""
            }
          ],
          "personId": 152346
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "Blast Theory",
              "dsl": ""
            }
          ],
          "personId": 151749
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Brighton",
              "institution": "Blast Theory",
              "dsl": ""
            }
          ],
          "personId": 151755
        }
      ]
    },
    {
      "id": 152706,
      "typeId": 13319,
      "durationOverride": 10,
      "title": "Understanding Roboticists' Power through Matrix Guided Power Analysis",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "hri24h-1048",
      "source": "PCS",
      "trackId": 12625,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152720
      ],
      "eventIds": [],
      "abstract": "Roboticists wield substantial power through the ways we choose to design and deploy robots. But understanding the nature of this power requires us to consider the different types of power wielded through different types of robot design choices, and the social and historical factors that shape the power landscape into which robots are embedded. To facilitate this type of analysis, I present Matrix-Guided Power Analysis (MGPA), a framework for analyzing the different types of power that technologists wield across different domains of power, with sensitivity to the social and historical forces that determine the default and alternative trajectories of those technologies. Further, I show how MGPA can be used to better understand the specific types of power that roboticists wield.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Golden",
              "institution": "Colorado School of Mines",
              "dsl": "MIRRORLab"
            }
          ],
          "personId": 139832
        }
      ]
    },
    {
      "id": 152739,
      "typeId": 13323,
      "title": "Session Break",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "10516",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152708
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152827,
      "typeId": 13323,
      "title": "Session Break",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "10589",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152719
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152847,
      "durationOverride": 6,
      "title": "Interaction-Shaping Robotics: Robots that Influence Interactions between Other Agents",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "THRI2",
      "source": "CSV",
      "trackId": 12635,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152735
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152848,
      "durationOverride": 6,
      "title": "In the arms of a robot: Designing autonomous hugging robots with intra-hug gestures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "THRI1",
      "source": "CSV",
      "trackId": 12635,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152735
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152849,
      "durationOverride": 6,
      "title": "What Can a Robot's Skin Be? Designing Texture-changing Skin for HumanÐRobot Social Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "THRI4",
      "source": "CSV",
      "trackId": 12635,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152735
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152850,
      "durationOverride": 6,
      "title": "Nonverbal Sound in Human-Robot Interaction: a Systematic Review.Ê",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "THRI3",
      "source": "CSV",
      "trackId": 12635,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152735
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 152851,
      "durationOverride": 6,
      "title": "Forging Productive Human-Robot Partnerships Through Task Training",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "THRI5",
      "source": "CSV",
      "trackId": 12635,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        152735
      ],
      "eventIds": [],
      "authors": []
    }
  ],
  "people": [
    {
      "id": 139731,
      "firstName": "Ayanna",
      "lastName": "Howard",
      "middleInitial": "",
      "importedId": "OBIVbDrjeuyvL9_GsQizdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139732,
      "firstName": "Joseph",
      "lastName": "Lyons",
      "middleInitial": "",
      "importedId": "8QvfCzi0fsGeQC45CHFeNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139733,
      "firstName": "Agam",
      "lastName": "Oberlender",
      "middleInitial": "",
      "importedId": "9oO7QTU7MN60m5CeMSR5SQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139734,
      "firstName": "Shihong",
      "lastName": "Ling",
      "middleInitial": "",
      "importedId": "yD7O6ZxLAH1Xh45rkb7FoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139735,
      "firstName": "Ashveen",
      "lastName": "Banga",
      "middleInitial": "",
      "importedId": "9e8_3wYr6AG0CsdeqVneeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139736,
      "firstName": "Natasha",
      "lastName": "Mulvihill",
      "middleInitial": "",
      "importedId": "btnuRP6NL9onPjt7GmXlqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139737,
      "firstName": "Samia",
      "lastName": "Bhatti",
      "middleInitial": "Cornelius",
      "importedId": "qkxbtsAwgiM-5tvzC-lQeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139738,
      "firstName": "Randi",
      "lastName": "Williams",
      "middleInitial": "",
      "importedId": "cvO_XhiIrQkcRC9K0rod3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139739,
      "firstName": "Daniel",
      "lastName": "Stabile",
      "middleInitial": "",
      "importedId": "mu6to7LGRnuhQFR3s5QyEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139740,
      "firstName": "Vy",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "FjLqHoom5lgfZBxqXkCNzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139741,
      "firstName": "Nazli",
      "lastName": "Cila",
      "middleInitial": "",
      "importedId": "UNW0oDjXQuRA8TAtodLE7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139742,
      "firstName": "Katharina",
      "lastName": "Wzietek",
      "middleInitial": "",
      "importedId": "pK-biwi48YoXtkiWcixFUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139743,
      "firstName": "Sawyer",
      "lastName": "Collins",
      "middleInitial": "",
      "importedId": "4FsP_z2_avWZYDH69FaEpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139744,
      "firstName": "Haoyang",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "CnGZ0lm4STnUkpFQOh0i2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139745,
      "firstName": "Michael",
      "lastName": "Stolp-Smith",
      "middleInitial": "",
      "importedId": "saQ4qmh60v-Jz60Ng0aoVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139746,
      "firstName": "Karthik",
      "lastName": "Mahadevan",
      "middleInitial": "",
      "importedId": "ZTJVejnFuGJVN931XGNp4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139747,
      "firstName": "Na",
      "lastName": "Du",
      "middleInitial": "",
      "importedId": "JNX3ychup70pUN4dnUSmCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139748,
      "firstName": "Edmond S. L.",
      "lastName": "Ho",
      "middleInitial": "",
      "importedId": "iO-V2Eog_COyqz6kgIV77w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139749,
      "firstName": "Mariah",
      "lastName": "Schrum",
      "middleInitial": "",
      "importedId": "JLlpn0yz3jSdxGyjJHgeJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139750,
      "firstName": "Bilge",
      "lastName": "Mutlu",
      "middleInitial": "",
      "importedId": "t1PasIb_0kcSj7Bx7HjLYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139751,
      "firstName": "Andrew",
      "lastName": "Le",
      "middleInitial": "",
      "importedId": "8DiLcppwtnG4C7HcB9aAMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139752,
      "firstName": "Nichole",
      "lastName": "Starr",
      "middleInitial": "",
      "importedId": "9qB9LCQKxhmTHvOxu7XWoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139753,
      "firstName": "Emmanuel",
      "lastName": "Senft",
      "middleInitial": "",
      "importedId": "KqKXnl_FU5RSZhHlT3dHZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139754,
      "firstName": "Torsten",
      "lastName": "Kuhlen",
      "middleInitial": "Wolfgang",
      "importedId": "X04CYKYDVKKapfPiPeGdsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139755,
      "firstName": "Stephanie",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "NV71lHlCUi9gnsFGYL1QpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139756,
      "firstName": "Ilia",
      "lastName": "Sucholutsky",
      "middleInitial": "",
      "importedId": "0rNUmqnhKhdJsWfG_kXHEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139757,
      "firstName": "Rishabh",
      "lastName": "Madan",
      "middleInitial": "",
      "importedId": "M_MMWT-1wKHa5641hBjbog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139758,
      "firstName": "Xiaogang",
      "lastName": "Jia",
      "middleInitial": "",
      "importedId": "UqMnFt1CAgN6b4raQny_1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139759,
      "firstName": "Felix",
      "lastName": "Siebert",
      "middleInitial": "Wilhelm",
      "importedId": "hORqlTGIE9b6J4RkXy7SCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139760,
      "firstName": "Leila",
      "lastName": "Takayama",
      "middleInitial": "",
      "importedId": "EUYdiRpIrI2pwze3Ti24eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139761,
      "firstName": "Cong",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "TWoBaHdez8QKFjLnyRNoGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139762,
      "firstName": "Dorsa",
      "lastName": "Sadigh",
      "middleInitial": "",
      "importedId": "6IAS2Kx5iOJfkSqO5oxMYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139763,
      "firstName": "Andi",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "XhLL3OiwJF84KhxvYeYtaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139764,
      "firstName": "Andreea",
      "lastName": "Bobu",
      "middleInitial": "",
      "importedId": "0fLTkT6VBMhdtXpRB83JVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139765,
      "firstName": "Zulfiqar",
      "lastName": "Zaidi",
      "middleInitial": "",
      "importedId": "5qDFqXcN7vzqtXLBFrSq9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139766,
      "firstName": "Takahiro",
      "lastName": "Wada",
      "middleInitial": "",
      "importedId": "eSj_pyIFJEBDo-vd1xtUZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139767,
      "firstName": "Cedric",
      "lastName": "Pradalier",
      "middleInitial": "",
      "importedId": "Al19PDMkcHb5rrmmxBz4rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139768,
      "firstName": "Jiaxin",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "KFXR6nZoKZi6J6_d2j50FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139769,
      "firstName": "Cindy",
      "lastName": "Bethel",
      "middleInitial": "",
      "importedId": "LBOgSK3hYZ-o1VPlVVewwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139770,
      "firstName": "Ziang",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "otarocMD8MLOxmjYlVjZLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139771,
      "firstName": "Rudolf",
      "lastName": "Lioutikov",
      "middleInitial": "",
      "importedId": "3xAc6jv8CfYfedpXmH4kVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139772,
      "firstName": "Oya",
      "lastName": "Celiktutan",
      "middleInitial": "",
      "importedId": "oskOq9Oh32qM0tEfIr_aUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139773,
      "firstName": "Tu",
      "lastName": "Trinh",
      "middleInitial": "Ngoc",
      "importedId": "br4nxQjPPkdX-VqaTtV1lA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139774,
      "firstName": "Elizabeth",
      "lastName": "Hsiao-Wecksler",
      "middleInitial": "T.",
      "importedId": "5MDSLaNN-5TT8ZlwVXt3vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139775,
      "firstName": "Michael",
      "lastName": "Wehner",
      "middleInitial": "",
      "importedId": "6iUPDYWp49zZJrIIZ1l2DA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139776,
      "firstName": "Keith",
      "lastName": "Green",
      "middleInitial": "Evan",
      "importedId": "XP9JCfhFusojyKhUUfXnBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139777,
      "firstName": "Hee Rin",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "F82F8LexYeQX1Z3GmlKbhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139778,
      "firstName": "Vivienne Bihe",
      "lastName": "Chi",
      "middleInitial": "",
      "importedId": "2MopaofKySIcGX14E1Vstg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139779,
      "firstName": "Koen",
      "lastName": "Hindriks",
      "middleInitial": "",
      "importedId": "QkpeEF7dokKZFmvqWxEBBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139780,
      "firstName": "Akhil",
      "lastName": "Padmanabha",
      "middleInitial": "",
      "importedId": "GmOtqStPfgHAorV1IxRs2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139781,
      "firstName": "Pradyumna",
      "lastName": "Tambwekar",
      "middleInitial": "",
      "importedId": "QRNTYXlIuQI5dHhV3TLqFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139782,
      "firstName": "Zhao",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "MVg5DO8bQoxb9J3DzSxQ3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139783,
      "firstName": "Keenan",
      "lastName": "Schott",
      "middleInitial": "",
      "importedId": "bwD5g7qrHduk0MfkXvm5IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139784,
      "firstName": "RAY",
      "lastName": "LC",
      "middleInitial": "",
      "importedId": "bZTPx-Xcqg_ZozQDila34g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139785,
      "firstName": "Noah",
      "lastName": "Brown",
      "middleInitial": "",
      "importedId": "6h3LYM17AQxS6YYbwea27Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139786,
      "firstName": "Toam",
      "lastName": "Bechor",
      "middleInitial": "",
      "importedId": "G1ILuJnP8mOtMWxlDAjuDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139787,
      "firstName": "Haeri",
      "lastName": "Hwang",
      "middleInitial": "",
      "importedId": "uAbpOV_At23Dv6epJzFMug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139788,
      "firstName": "Takayuki",
      "lastName": "Kanda",
      "middleInitial": "",
      "importedId": "1e71DSF1TaOFU3xWS8fiiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139789,
      "firstName": "Jacob",
      "lastName": "Crandall",
      "middleInitial": "",
      "importedId": "K-W68287HRbIAyf6zxko5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139790,
      "firstName": "Casey",
      "lastName": "Bennett",
      "middleInitial": "C",
      "importedId": "7F8OZwR1XZwBT-tPEVruPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139791,
      "firstName": "Stefan",
      "lastName": "Ladwig",
      "middleInitial": "",
      "importedId": "RJGTCWmCrzsMTrzKqC41xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139792,
      "firstName": "EunJeong",
      "lastName": "Cheon",
      "middleInitial": "",
      "importedId": "9htgdg0y7Qs-Mwl_OXwepw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139793,
      "firstName": "Cristina",
      "lastName": "Wilson",
      "middleInitial": "G",
      "importedId": "dhQpWaUipj5Khs3TviGk-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139794,
      "firstName": "Goren",
      "lastName": "Gordon",
      "middleInitial": "",
      "importedId": "zSB8vZvmrISTKcXY-Qfy8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139795,
      "firstName": "Ulas Berk",
      "lastName": "Karli",
      "middleInitial": "",
      "importedId": "_Gq9dTcgICyRFUc5mdSUog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139796,
      "firstName": "Christena",
      "lastName": "Nippert-Eng",
      "middleInitial": "",
      "importedId": "YC_H6GX3nrOMvMPH0KztIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139797,
      "firstName": "Hana Andargie",
      "lastName": "Kassie",
      "middleInitial": "",
      "importedId": "L2b-wMrswSFk8AV1F4bCPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139798,
      "firstName": "Hiroki",
      "lastName": "Sato",
      "middleInitial": "",
      "importedId": "6ViOt28Vn4t8loZ2sRw-Iw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139799,
      "firstName": "Jinhee",
      "lastName": "Chang",
      "middleInitial": "C",
      "importedId": "tVOXwtR12YyTmTVybs4RpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139800,
      "firstName": "Deana",
      "lastName": "McDonagh",
      "middleInitial": "",
      "importedId": "l5MkRza9yGIJec3Z6QwQOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139801,
      "firstName": "Katharina",
      "lastName": "Buckmayer",
      "middleInitial": "",
      "importedId": "eIn_zra_UfiKpsBJXo-Kbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139802,
      "firstName": "Angelique",
      "lastName": "Taylor",
      "middleInitial": "",
      "importedId": "HB4AE6fNBF5HqSd12hitxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139803,
      "firstName": "Daniel",
      "lastName": "Marta",
      "middleInitial": "",
      "importedId": "SUHmBBgEloq2xDjwqFdICw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139804,
      "firstName": "Ruchen",
      "lastName": "Wen",
      "middleInitial": "",
      "importedId": "ep10R5Q2rVSq1ha999ipxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139805,
      "firstName": "Abrar",
      "lastName": "Anwar",
      "middleInitial": "",
      "importedId": "uyafF1BP2GtSYezuEIqcjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139806,
      "firstName": "Yiheng",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "QkLsclTsUb1FFaaoBVgssQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139807,
      "firstName": "Ramona",
      "lastName": "Piller",
      "middleInitial": "",
      "importedId": "uaKmVEJH4vU3UhZq30tuRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139808,
      "firstName": "Sonya",
      "lastName": "Kwak",
      "middleInitial": "S.",
      "importedId": "843jyL7ozlWVuDjMl75xtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139809,
      "firstName": "Etienne",
      "lastName": "Peillard",
      "middleInitial": "",
      "importedId": "IGKLtYZx34ipN6xxpKsBxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139810,
      "firstName": "Arjun",
      "lastName": "Krishna",
      "middleInitial": "",
      "importedId": "DnTG1Zyig9gBh325bC_23Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139811,
      "firstName": "Michael",
      "lastName": "Zinn",
      "middleInitial": "",
      "importedId": "QNttvlRwRjzEvkVndc2Mkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139812,
      "firstName": "Yue",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "tCvYAzeode9daGakCcCh5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139813,
      "firstName": "Shreyas",
      "lastName": "Bhat",
      "middleInitial": "",
      "importedId": "W37HYrE_pYu4XPIkhmJSNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139814,
      "firstName": "Daniel",
      "lastName": "Rea",
      "middleInitial": "J.",
      "importedId": "tTLRUmONojKOxWEw8DpcKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139815,
      "firstName": "Alvika",
      "lastName": "Gautam",
      "middleInitial": "",
      "importedId": "Fg0H_TiOxOZeUdAp9HPSCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139816,
      "firstName": "Fei",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "HqMCUKOnC3UBR_DmIfDdQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139817,
      "firstName": "Ted",
      "lastName": "Sumers",
      "middleInitial": "",
      "importedId": "z-JgF7Eeu2JEpUqdr5XBmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139818,
      "firstName": "Jeannette",
      "lastName": "Elliott",
      "middleInitial": "",
      "importedId": "l7RHMNEHUbZMu5AEwBf2Dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139819,
      "firstName": "Drazen",
      "lastName": "Brscic",
      "middleInitial": "",
      "importedId": "nsXuNH6W5rjVIxr4R5rSvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139820,
      "firstName": "Anya",
      "lastName": "Bouzida",
      "middleInitial": "",
      "importedId": "Jk7YB6v67y11tJG0YpTYfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139821,
      "firstName": "Letian",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "YrDylySueqGRd2pzrjgDdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139822,
      "firstName": "YU",
      "lastName": "CHEN",
      "middleInitial": "",
      "importedId": "lRNdFNF5ObqYh75y2onVrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139823,
      "firstName": "Jue",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "N4kjHBancJo8lrioUG4_nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139824,
      "firstName": "Hugo",
      "lastName": "Nicolau",
      "middleInitial": "",
      "importedId": "6pg_rrG91DnuYzl1ZCvEJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139825,
      "firstName": "Skyler",
      "lastName": "Valdez",
      "middleInitial": "",
      "importedId": "pYm9YkSoWMGs5kYwNLemPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139826,
      "firstName": "Yifei",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "v7Dzfp_BjOoyh0LGbJ2jkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139827,
      "firstName": "Anca",
      "lastName": "Dragan",
      "middleInitial": "",
      "importedId": "Ufa9t-NqKJazADi9s298Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139828,
      "firstName": "Amy",
      "lastName": "Koike",
      "middleInitial": "",
      "importedId": "dGo8wYptMkSaSnzJ-jbe8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139829,
      "firstName": "Emma",
      "lastName": "Bethel",
      "middleInitial": "",
      "importedId": "FYZagUuDiKIHMkdHK_KUhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139830,
      "firstName": "Xiaowei",
      "lastName": "Jia",
      "middleInitial": "",
      "importedId": "FvrbyyYbYCGZKD-VEHfc3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139831,
      "firstName": "Raymond",
      "lastName": "Cuijpers",
      "middleInitial": "H.",
      "importedId": "ZgzlGnCeweWClGydrwgwxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139832,
      "firstName": "Tom",
      "lastName": "Williams",
      "middleInitial": "",
      "importedId": "gPB6RgpbDViRprNvwtDBuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139833,
      "firstName": "Manasi",
      "lastName": "Swaminathan",
      "middleInitial": "",
      "importedId": "qUYzJh9STsr-eQV1gwt9IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139834,
      "firstName": "Mohammad",
      "lastName": "Yasar",
      "middleInitial": "Samin",
      "importedId": "WI5_fqXeYsR_vilC8JZDWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139835,
      "firstName": "Amy",
      "lastName": "O'Connell",
      "middleInitial": "",
      "importedId": "c3KFLTJ-r-arZz9uteIuCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139836,
      "firstName": "Ting-Han",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "WbSzwriD6NNZvJ4QoZ89Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139837,
      "firstName": "David",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "H4uuOnVea_VzmOfTX7dXkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139838,
      "firstName": "Raúl",
      "lastName": "Alcantara",
      "middleInitial": "",
      "importedId": "MrI_0HfV4gz216dFueI-Dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139839,
      "firstName": "Tariq",
      "lastName": "Iqbal",
      "middleInitial": "",
      "importedId": "vDaO5PufSBtR9oXXZ52Dcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139840,
      "firstName": "Mille",
      "lastName": "Lunding",
      "middleInitial": "Skovhus",
      "importedId": "z7U9k3V8W9dtOfEqd3nZrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139841,
      "firstName": "Omer",
      "lastName": "Gvirsman",
      "middleInitial": "",
      "importedId": "bo23iLX7ZHDkgQ8lFtEUSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139842,
      "firstName": "Andreea",
      "lastName": "Tulbure",
      "middleInitial": "",
      "importedId": "miFr8c1duU-MKiLyh8r2qQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139843,
      "firstName": "Christine",
      "lastName": "Lee",
      "middleInitial": "P.",
      "importedId": "WDP9UfJu3PsQJRTXBCzpgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139844,
      "firstName": "Chenzhang",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "cVwP0g1Xkna6ina17aG-9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139845,
      "firstName": "Xinkai",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "QCVwtGFCQk_-czcl2zSImA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139846,
      "firstName": "Robert",
      "lastName": "Radwin",
      "middleInitial": "",
      "importedId": "4wuCzZLPyE-RAtDTGRFjtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139847,
      "firstName": "Michael",
      "lastName": "Walker",
      "middleInitial": "",
      "importedId": "g7_zgWkT2uQgcg4GRQstGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139848,
      "firstName": "Marco C.",
      "lastName": "Rozendaal",
      "middleInitial": "",
      "importedId": "hJt9g7pmETtbrs2m0j5aKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139849,
      "firstName": "Kyrie",
      "lastName": "Amon",
      "middleInitial": "Jig",
      "importedId": "o8lCqCSlCtz9WBsF-aMerw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139850,
      "firstName": "Benjamin",
      "lastName": "Weyers",
      "middleInitial": "",
      "importedId": "Lp_V5AGjKnaENLx8IYyfpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139851,
      "firstName": "Astrid",
      "lastName": "Rosenthal-von der Pütten",
      "middleInitial": "Marieke",
      "importedId": "xixkuUbP7KK1jj9gM8d1pw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139852,
      "firstName": "Mahshid",
      "lastName": "Mansouri",
      "middleInitial": "",
      "importedId": "7omsv8IfiaHrdjDBz011Hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139853,
      "firstName": "Callie",
      "lastName": "Kim",
      "middleInitial": "Y.",
      "importedId": "9fDXUWlnq8raWooIb2btjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139854,
      "firstName": "Isaac",
      "lastName": "Sheidlower",
      "middleInitial": "S",
      "importedId": "2ticW-6AM3NpaX9mLX2ilw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139855,
      "firstName": "Juna",
      "lastName": "Khatib",
      "middleInitial": "",
      "importedId": "GsqJNEVAFqnvt9K4cjekuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139856,
      "firstName": "Yi-Shiuan",
      "lastName": "Tung",
      "middleInitial": "",
      "importedId": "JUwX41RKWys3YOJES2A7fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139857,
      "firstName": "Thomas",
      "lastName": "Ellwart",
      "middleInitial": "",
      "importedId": "ej2zTRNdA4urtpmDZOj-tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139858,
      "firstName": "Chongfeng",
      "lastName": "Wei",
      "middleInitial": "",
      "importedId": "TM-ZIbbya3V3E6T2fvE19w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139859,
      "firstName": "Bradley",
      "lastName": "Hayes",
      "middleInitial": "",
      "importedId": "5Em6tsarJaD9b536UqgQ-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139860,
      "firstName": "Arsha",
      "lastName": "Ali",
      "middleInitial": "",
      "importedId": "j2CezTHF2eGgFHcuRjvPQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139861,
      "firstName": "Sujie",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "aZQVIJ7-hBgjVySwHPbfyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139862,
      "firstName": "Malte",
      "lastName": "Jung",
      "middleInitial": "F",
      "importedId": "XF9lJ3POydYEPlVqavSqQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139863,
      "firstName": "Eya",
      "lastName": "Chemangui",
      "middleInitial": "",
      "importedId": "iBdo33iYKu-buWWq4kG6yQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139864,
      "firstName": "Daniel",
      "lastName": "Brown",
      "middleInitial": "S",
      "importedId": "gsQxrklVvxB_eg-VilIrJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139865,
      "firstName": "Terran",
      "lastName": "Mott",
      "middleInitial": "",
      "importedId": "a96lhQHbI9FPK8U_nN7jaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139866,
      "firstName": "Minja",
      "lastName": "Axelsson",
      "middleInitial": "",
      "importedId": "p0zYkCGu3RXq-BNeRgOEHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139867,
      "firstName": "Simon",
      "lastName": "Oehrl",
      "middleInitial": "",
      "importedId": "KBurPfh7AxqorZ2aARWqtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139868,
      "firstName": "Filipa",
      "lastName": "Correia",
      "middleInitial": "",
      "importedId": "UP2-Hu_N1iMBjOT72UgWVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139869,
      "firstName": "Nivedita",
      "lastName": "Mani",
      "middleInitial": "",
      "importedId": "q8RQnjbKt0IDFmlK2zrFpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139870,
      "firstName": "Sachi",
      "lastName": "Edirisinghe",
      "middleInitial": "Natasha",
      "importedId": "jMKVoZr-5neLwngW-tTbMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139871,
      "firstName": "Julian",
      "lastName": "Waksberg",
      "middleInitial": "",
      "importedId": "iqgTzLRBXI4mmGgoPdJR-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139872,
      "firstName": "Jérémy",
      "lastName": "Rivière",
      "middleInitial": "",
      "importedId": "Q41hkUjmHydZ5AL6arAQeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139873,
      "firstName": "Kanghui",
      "lastName": "Du",
      "middleInitial": "",
      "importedId": "aaQHZXPyaGQHPsvYJWDiSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139874,
      "firstName": "Hannen",
      "lastName": "Wolfe",
      "middleInitial": "E.",
      "importedId": "wzgSx24SvbYm9DSiKT0uhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139875,
      "firstName": "Safinah",
      "lastName": "Ali",
      "middleInitial": "",
      "importedId": "RfHuulSHTJVp2tk94H6rMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139876,
      "firstName": "Andrew",
      "lastName": "Fenn",
      "middleInitial": "",
      "importedId": "LPW6y0ObtldFuQhtZxPAdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139877,
      "firstName": "Elena",
      "lastName": "Sabinson",
      "middleInitial": "",
      "importedId": "peCUHHW5sIDdF38suNouHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139878,
      "firstName": "Leigh",
      "lastName": "Levinson",
      "middleInitial": "M",
      "importedId": "bqJEX30kZ5kH38dkxiA2nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139879,
      "firstName": "Paul",
      "lastName": "Mattes",
      "middleInitial": "",
      "importedId": "iugz1TLNVfmK6hoi-ZnHAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139880,
      "firstName": "Patricia",
      "lastName": "Malik",
      "middleInitial": "",
      "importedId": "KxM8x_h2mhnEW-869TRigA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139881,
      "firstName": "Soraia",
      "lastName": "F Paulo",
      "middleInitial": "",
      "importedId": "84ZTutwOnpRwFUsy2oRUBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139882,
      "firstName": "Sophia C.",
      "lastName": "Steinhaeusser",
      "middleInitial": "",
      "importedId": "9MnA49h12M6RUMc-ZWfaWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139883,
      "firstName": "Ryo",
      "lastName": "Suzuki",
      "middleInitial": "",
      "importedId": "GN-jA9I12mj5OyFiQL9Cxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139884,
      "firstName": "Nikki",
      "lastName": "Yaminrafie",
      "middleInitial": "",
      "importedId": "KgczMRSFj6Qj5HMxSjIBwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139885,
      "firstName": "Birgit",
      "lastName": "Lugrin",
      "middleInitial": "",
      "importedId": "NLlcwIjpssFxBsHjacRP3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139886,
      "firstName": "Jacy",
      "lastName": "Anthis",
      "middleInitial": "Reese",
      "importedId": "Aug4efp1Zmo_Hr98WLdg1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139887,
      "firstName": "Julia",
      "lastName": "Hansen",
      "middleInitial": "",
      "importedId": "H913Xe1zVYmZSqD_7Twb-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139888,
      "firstName": "Cynthia",
      "lastName": "Breazeal",
      "middleInitial": "",
      "importedId": "0Ve7yhWXx-q7lbQCpky_fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139889,
      "firstName": "Hatice",
      "lastName": "Gunes",
      "middleInitial": "",
      "importedId": "_MtINyjQZLCUsEGPLjmtBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139890,
      "firstName": "Huajie",
      "lastName": "Cao",
      "middleInitial": "",
      "importedId": "OSJs8zR_-Ssk6xBj8fTBIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139891,
      "firstName": "Diego",
      "lastName": "Virtue",
      "middleInitial": "T",
      "importedId": "4JNu8jEeaoo97g_JJNTNkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139892,
      "firstName": "Sanne",
      "lastName": "van Waveren",
      "middleInitial": "",
      "importedId": "Ifz09ivzmcYKSZYNLdBzjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139893,
      "firstName": "Alyson",
      "lastName": "Ranucci",
      "middleInitial": "",
      "importedId": "G57cWt6ksYdo2aEcWFccgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139894,
      "firstName": "Ana",
      "lastName": "Paiva",
      "middleInitial": "",
      "importedId": "P5UYtmO7rwDcMq9R6AIN2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139895,
      "firstName": "Spencer",
      "lastName": "Ng",
      "middleInitial": "",
      "importedId": "bzT3fGYaRsX8r9s0G1aTcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139896,
      "firstName": "Gilles",
      "lastName": "Coppin",
      "middleInitial": "",
      "importedId": "kvLPGi7fn_wOPv2cG9b2tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139897,
      "firstName": "Fatih",
      "lastName": "Sivridag",
      "middleInitial": "",
      "importedId": "TLO0azS5CBfMC1R2miEttA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139898,
      "firstName": "Mark",
      "lastName": "Higger",
      "middleInitial": "",
      "importedId": "Mc94nZ_brHa8S9XQHtDgvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139899,
      "firstName": "Simon",
      "lastName": "Holk",
      "middleInitial": "",
      "importedId": "17YMapOashub4SBQzPPWxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139900,
      "firstName": "Zachary",
      "lastName": "Lee",
      "middleInitial": "I",
      "importedId": "Z5BaG4kLy5UjQ5WC6XfHJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139901,
      "firstName": "Annie",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "gC485tBaH_KO8QqN8bZplw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139902,
      "firstName": "Heather",
      "lastName": "Knight",
      "middleInitial": "",
      "importedId": "4l8ACcViZn36JXhCR4MSgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139903,
      "firstName": "Yusuke",
      "lastName": "Fujioka",
      "middleInitial": "",
      "importedId": "fk91J8TAV5GUn-ZeKw1t4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139904,
      "firstName": "Dagoberto",
      "lastName": "Cruz-Sandoval",
      "middleInitial": "",
      "importedId": "J09xGzdrwX10HEYdnKS5kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139905,
      "firstName": "Jennifer",
      "lastName": "Piatt",
      "middleInitial": "",
      "importedId": "VNgRkeHO5J0PulZqk2U4Fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139906,
      "firstName": "Seung Yun",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "_dhs1cC6KeTsGH8iPRiX-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139907,
      "firstName": "Gregory",
      "lastName": "LeMasurier",
      "middleInitial": "",
      "importedId": "W2ZuWwZfMNRQ2E810PjUpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139908,
      "firstName": "Omer",
      "lastName": "Sadeh",
      "middleInitial": "",
      "importedId": "3V69eUHWTwDUd0SWga5fdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139909,
      "firstName": "Yuyi",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "Dr6BFT5aEBGQ8GxHUX6nMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139910,
      "firstName": "Somaya",
      "lastName": "Ben Allouch",
      "middleInitial": "",
      "importedId": "ZqJuWFN88b_I0tyb0qOulA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139911,
      "firstName": "William",
      "lastName": "Norris",
      "middleInitial": "Robert",
      "importedId": "4ybd7KZLu_Bd_JCjnJuSbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139912,
      "firstName": "Nathalie",
      "lastName": "Schauffel",
      "middleInitial": "",
      "importedId": "jXcmldE-cavgTx7xhZISNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139913,
      "firstName": "Tom",
      "lastName": "Griffiths",
      "middleInitial": "",
      "importedId": "dtQ-WKGClCK2AD4Y7DeFUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139914,
      "firstName": "Julie",
      "lastName": "Shah",
      "middleInitial": "",
      "importedId": "HEttY6ix9PDsIDg8vmji0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139915,
      "firstName": "Joshua",
      "lastName": "Ravishankar",
      "middleInitial": "",
      "importedId": "3LGoJFJl40tpQAgIKl9_yQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139916,
      "firstName": "Iolanda",
      "lastName": "Leite",
      "middleInitial": "",
      "importedId": "790Z41soBKaHf7CEP6Ry8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139917,
      "firstName": "Henrik Skaug",
      "lastName": "Sætra",
      "middleInitial": "",
      "importedId": "wZbDYkixEYrDXGsAeVlCIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139918,
      "firstName": "Matthijs",
      "lastName": "Smakman",
      "middleInitial": "",
      "importedId": "5PZqpL7a79GU2oj0Zl4VeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139919,
      "firstName": "Grace",
      "lastName": "Clark",
      "middleInitial": "",
      "importedId": "4ypRgV6YqiRaFyv427HpwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139920,
      "firstName": "Andrew",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "oVL4KzNyDZ8aoDrnpI6hMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139921,
      "firstName": "David",
      "lastName": "Porfirio",
      "middleInitial": "",
      "importedId": "k6GwYlILBm8eJskLzsuKAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139922,
      "firstName": "Holly",
      "lastName": "Yanco",
      "middleInitial": "",
      "importedId": "b8W7Yn8yEGOgcGFW3OQ9tQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139923,
      "firstName": "Belinda",
      "lastName": "Li",
      "middleInitial": "Zou",
      "importedId": "VRC94boPPhnNNoMRnvc4_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139924,
      "firstName": "Arianna",
      "lastName": "Sica",
      "middleInitial": "",
      "importedId": "32km6GRxYbyXkV83xMnL8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139925,
      "firstName": "Jonathan",
      "lastName": "Chien",
      "middleInitial": "",
      "importedId": "AkhDl6MTLGA4k8XcjyWUKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139926,
      "firstName": "Kaj",
      "lastName": "Grønbæk",
      "middleInitial": "",
      "importedId": "7q6wCAlX85zIMCKTWiMQbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139927,
      "firstName": "Yuzhu",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "Qqzs-Xu6gcJhLcXFWfVD9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139928,
      "firstName": "Connor",
      "lastName": "Esterwood",
      "middleInitial": "",
      "importedId": "03RbvQdT1GblowX39xXuOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139929,
      "firstName": "Lamia",
      "lastName": "Elloumi",
      "middleInitial": "",
      "importedId": "XDlSTVYZsummAb0iHQeyGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139930,
      "firstName": "Dorothea",
      "lastName": "Koert",
      "middleInitial": "",
      "importedId": "svm6_DULxywNnBBOfa_2tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139931,
      "firstName": "Tapomayukh",
      "lastName": "Bhattacharjee",
      "middleInitial": "",
      "importedId": "8QP69f4cPBTgC5quOyWpJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139932,
      "firstName": "Micol",
      "lastName": "Spitale",
      "middleInitial": "",
      "importedId": "xNxeBx0780FxEkgzigTgqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139933,
      "firstName": "Brittany",
      "lastName": "Duncan",
      "middleInitial": "A.",
      "importedId": "8ShvE5wxy6dtrXqmMRZ5nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139934,
      "firstName": "Sabrina",
      "lastName": "Caldwell",
      "middleInitial": "",
      "importedId": "1dBMm5_Phf9HJuYmRcdQaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139935,
      "firstName": "Jack",
      "lastName": "Neiberg",
      "middleInitial": "",
      "importedId": "_S65BLADBdCjQ0WepqECaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139936,
      "firstName": "Laurel D.",
      "lastName": "Riek",
      "middleInitial": "",
      "importedId": "3FAXZ8BHCn-RSrUaADN4Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139937,
      "firstName": "Carolina",
      "lastName": "Parada",
      "middleInitial": "",
      "importedId": "hxDAFOu2Mzh4G73yBqVB1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139938,
      "firstName": "Maitrey",
      "lastName": "Gramopadhye",
      "middleInitial": "",
      "importedId": "ikW7WfBoUAZeZ6Y86AzjrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139939,
      "firstName": "Lionel",
      "lastName": "Robert",
      "middleInitial": "Peter",
      "importedId": "Exyrh3f3q3HyP_H3eeB5Xw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139940,
      "firstName": "Nishanth",
      "lastName": "Kumar",
      "middleInitial": "",
      "importedId": "bDs8MSIb87T4rsBq6K8P4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139941,
      "firstName": "Kate",
      "lastName": "Tsui",
      "middleInitial": "",
      "importedId": "sDR1qdIAR7uV4iSNIwvzEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139942,
      "firstName": "Yuhan",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "IEGXnMudE5kZK0tuI3AZnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139943,
      "firstName": "Daniel",
      "lastName": "Szafir",
      "middleInitial": "",
      "importedId": "FJ-4cZEJF9DPmgZJR7iSBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139944,
      "firstName": "Satoru",
      "lastName": "Satake",
      "middleInitial": "",
      "importedId": "EGohmcpqXtsq6zAKS04hVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139945,
      "firstName": "Aaron",
      "lastName": "Fanganello",
      "middleInitial": "",
      "importedId": "dqaNGo20pV58nk_34Daebw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139946,
      "firstName": "Isabel",
      "lastName": "Neto",
      "middleInitial": "",
      "importedId": "8OR5Vqg5QLvic17Yw62Asg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139947,
      "firstName": "Marianne",
      "lastName": "Bossema",
      "middleInitial": "",
      "importedId": "hq33FRL4S5RVmVi-O83s9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139948,
      "firstName": "Sarah",
      "lastName": "Sebo",
      "middleInitial": "",
      "importedId": "pHFE20YL0hktl3lHCac8Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139949,
      "firstName": "Theing Mwe",
      "lastName": "Oo",
      "middleInitial": "",
      "importedId": "ue-dSeC7iCk4tKh-XoxkHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139950,
      "firstName": "Mavis",
      "lastName": "Murdock",
      "middleInitial": "",
      "importedId": "8TjBznz2kU1ZTgrLvJ3fEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139951,
      "firstName": "Jennifer",
      "lastName": "Ayissi",
      "middleInitial": "",
      "importedId": "Cih2Dm7UL9jyZhL0iDJe6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139952,
      "firstName": "Laura",
      "lastName": "Hiatt",
      "middleInitial": "M.",
      "importedId": "dP_sjYuwhzWFCZFbB4XVWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139953,
      "firstName": "Karla",
      "lastName": "Kelly",
      "middleInitial": "Bransky",
      "importedId": "ohyZb4URYaMG49HHgBae8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139954,
      "firstName": "Luoyan",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "IYfg5nSLmSt1aIzySZ9iJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139955,
      "firstName": "Irene Gonzalez",
      "lastName": "Gonzalez",
      "middleInitial": "",
      "importedId": "q69-lzttNDuPEmE2sw1h7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139956,
      "firstName": "Eileen",
      "lastName": "Roesler",
      "middleInitial": "",
      "importedId": "RMqb2JPm8kRyTSKRBtXXGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139957,
      "firstName": "Long-Jing",
      "lastName": "Hsu",
      "middleInitial": "",
      "importedId": "Y4OcS4Eua5a3rlm1OmwkVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139958,
      "firstName": "Bertram",
      "lastName": "Malle",
      "middleInitial": "F.",
      "importedId": "5_7N7IcdBChJkpJ_FKbHrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139959,
      "firstName": "Stuart",
      "lastName": "Reeves",
      "middleInitial": "",
      "importedId": "OSYByNymQP_oapDlZKaHVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139960,
      "firstName": "João",
      "lastName": "Ramos",
      "middleInitial": "",
      "importedId": "IcCdey_PFSd0Aqsypk96Ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139961,
      "firstName": "Luca",
      "lastName": "Crosato",
      "middleInitial": "",
      "importedId": "RRqYHlMDX4GpL_bKRVnE7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139962,
      "firstName": "Cedomir",
      "lastName": "Stanojevic",
      "middleInitial": "",
      "importedId": "SeneUui5_Xe8cQYU4R24yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139963,
      "firstName": "Chien-Ming",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "I2-92weHFtAyAxD_4mddew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139964,
      "firstName": "Jack",
      "lastName": "Burns",
      "middleInitial": "",
      "importedId": "w4Xj6xc3I-czNP1IW7_jnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139965,
      "firstName": "Aymeric",
      "lastName": "Hénard",
      "middleInitial": "",
      "importedId": "efIVfEi33Hg1laVlLDNymw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139966,
      "firstName": "Alessandro",
      "lastName": "Roncone",
      "middleInitial": "",
      "importedId": "XeUt2-c58UD59WdFD6Obow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139967,
      "firstName": "Andrew",
      "lastName": "Schoen",
      "middleInitial": "",
      "importedId": "opuTjA9T1eRc8DOXFWYCiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139968,
      "firstName": "Matthew",
      "lastName": "Gombolay",
      "middleInitial": "",
      "importedId": "5i0JBT3YbVJ8YEipINkuBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139969,
      "firstName": "Bryce",
      "lastName": "Ikeda",
      "middleInitial": "",
      "importedId": "P5d1m00mpH_oZ1rqmYD5gQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139970,
      "firstName": "João",
      "lastName": "Nogueira",
      "middleInitial": "",
      "importedId": "XF8Ffuh7RMf8tZoKunu9nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139971,
      "firstName": "Feifei",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "Pc0aGc_1jvYMsQ0pMqd04A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139972,
      "firstName": "Lisa",
      "lastName": "Scherf",
      "middleInitial": "Katharina",
      "importedId": "lBAfQjIKqVILqqBYPzHTcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139973,
      "firstName": "Victor",
      "lastName": "Antony",
      "middleInitial": "Nikhil",
      "importedId": "lQW_oOnDqzewEIoBmg1UEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139974,
      "firstName": "Elaine",
      "lastName": "Short",
      "middleInitial": "Schaertl",
      "importedId": "q0Xc9EJPIQDkxrJ7xujbbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139975,
      "firstName": "Marianne Graves",
      "lastName": "Petersen",
      "middleInitial": "",
      "importedId": "Vi9CdMpluVc9678vsAxX4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139976,
      "firstName": "Maja",
      "lastName": "Mataric",
      "middleInitial": "",
      "importedId": "Qj4BzCAFJO15QzgsHd_hMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139977,
      "firstName": "Reuben",
      "lastName": "Aronson",
      "middleInitial": "M",
      "importedId": "NWJbJknxsyVekrdyOVJUhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139978,
      "firstName": "Michael",
      "lastName": "Burke",
      "middleInitial": "",
      "importedId": "plh2WLnKs3vnp6QRZxJwAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139979,
      "firstName": "Pete",
      "lastName": "Schroepfer",
      "middleInitial": "",
      "importedId": "hFPqvtOMl9rzLyaKWeZnzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139980,
      "firstName": "Cloe",
      "lastName": "Emnett",
      "middleInitial": "Z",
      "importedId": "_2pMWNyt-08JIcllqFVe-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139981,
      "firstName": "Muhammad Akmal",
      "lastName": "Mohammed Zaffir",
      "middleInitial": "Bin",
      "importedId": "EHyqPJlRKspmBbFQz7fdQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139982,
      "firstName": "Nadja",
      "lastName": "Marin",
      "middleInitial": "",
      "importedId": "oUr_wJ2SxMoR7zdHx1Prwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139983,
      "firstName": "Carmel",
      "lastName": "Majidi",
      "middleInitial": "",
      "importedId": "RoW834kWcaWIPakMTwdhcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139984,
      "firstName": "Dahyun",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "WJKTrUYr0hG7DlkZibnbrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139985,
      "firstName": "Yuan-Chia",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "Z4J9d3PrACu6W7P9PfmYUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139986,
      "firstName": "Jan",
      "lastName": "Gründling",
      "middleInitial": "Philipp",
      "importedId": "vsV1z1cs2EU-e8Ll6RS_dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139987,
      "firstName": "David",
      "lastName": "Crandall",
      "middleInitial": "",
      "importedId": "eHp8rsrAKyZ6JArWWzeP9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139988,
      "firstName": "Md Mofijul",
      "lastName": "Islam",
      "middleInitial": "",
      "importedId": "645-awTFcpwnSVSvhqSCPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139989,
      "firstName": "Elizabeth",
      "lastName": "Croft",
      "middleInitial": "A",
      "importedId": "conLjMaVRERbie7QdHKIPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139990,
      "firstName": "Noam",
      "lastName": "Freund",
      "middleInitial": "",
      "importedId": "kkiL-rkTpICOTA8XVdXNQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139991,
      "firstName": "Hannah",
      "lastName": "Pelikan",
      "middleInitial": "RM",
      "importedId": "5rpd0V4zdEdxeX5EnrkTTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139992,
      "firstName": "Adam",
      "lastName": "Bleakney",
      "middleInitial": "",
      "importedId": "9GOtaSrOlwU03DCmadnPBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139993,
      "firstName": "Juo-Tung",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "IY3TRUvelKyBCsnhvux8tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139994,
      "firstName": "Filipa",
      "lastName": "Rocha",
      "middleInitial": "",
      "importedId": "9PBooNrZG65feIsXhNYriA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139995,
      "firstName": "Tauhid",
      "lastName": "Tanjim",
      "middleInitial": "",
      "importedId": "PUbIjE4gotSAnDnexWgdAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139996,
      "firstName": "Pia",
      "lastName": "Dautzenberg",
      "middleInitial": "",
      "importedId": "Gjmd2RdCjCz2OcMyEsBopQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139997,
      "firstName": "Jan",
      "lastName": "Jacobs",
      "middleInitial": "",
      "importedId": "1TZB9nB12QDdHoOASTb5xQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139998,
      "firstName": "Gerard",
      "lastName": "Canal",
      "middleInitial": "",
      "importedId": "OQFYu7j8KsRJDjgSanEzmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 139999,
      "firstName": "X. Jessie",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "r4Hsd-VQzk3RQPBzjiwJ7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140000,
      "firstName": "Sébastien",
      "lastName": "Kubicki",
      "middleInitial": "",
      "importedId": "-SjDoE9ZqrKPdK0YDDiYKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140001,
      "firstName": "You",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "sJ73jvH-r3jvomm_070EOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140002,
      "firstName": "Michael",
      "lastName": "Gleicher",
      "middleInitial": "",
      "importedId": "xwZbWq-IeKeHbFf4lJI84Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140003,
      "firstName": "Katie",
      "lastName": "Winkle",
      "middleInitial": "",
      "importedId": "Fz8azM3OVBB6DDlXuVDHIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140004,
      "firstName": "LING",
      "lastName": "MA",
      "middleInitial": "",
      "importedId": "62EC8amcZ8AGl_ZP5ZNApg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140005,
      "firstName": "Ellen",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "Ghk1c2XDwfB7qWN7u0TEFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140006,
      "firstName": "Sharifa",
      "lastName": "Alghowinem",
      "middleInitial": "",
      "importedId": "odys456kyEmDnk9-sgHZgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140007,
      "firstName": "Andy",
      "lastName": "Zeng",
      "middleInitial": "",
      "importedId": "2WEHBmwroPcezttmg3BMIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140008,
      "firstName": "Geronimo",
      "lastName": "Gorostiaga Zubizarreta",
      "middleInitial": "",
      "importedId": "l39fU9W6q4P6Gfg17lefrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140009,
      "firstName": "Douglas",
      "lastName": "Weber",
      "middleInitial": "",
      "importedId": "gmf9BWn2bAwFjxW8N0HWFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140010,
      "firstName": "Xin",
      "lastName": "Ye",
      "middleInitial": "",
      "importedId": "pEw_56zR_ddr62Wv4ylBBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140011,
      "firstName": "Michael",
      "lastName": "Hagenow",
      "middleInitial": "",
      "importedId": "pAjirOntZZCkqqgCCfWxsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140012,
      "firstName": "Simone",
      "lastName": "de Droog",
      "middleInitial": "Marijke",
      "importedId": "t-NfBeMlICSZTY3c5oiaQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140013,
      "firstName": "Kenna",
      "lastName": "Baugus Henkel",
      "middleInitial": "",
      "importedId": "iGTSn1nJ16_PluvJ0wx-xA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140014,
      "firstName": "Marco",
      "lastName": "Hutter",
      "middleInitial": "",
      "importedId": "_GM62qt_oDZ4av51inYazw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140015,
      "firstName": "Shipeng",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "R0pE0fvPjocVrBewSi4G3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140016,
      "firstName": "Weslie",
      "lastName": "Khoo",
      "middleInitial": "",
      "importedId": "aL-wuVOGIepMbB2pLoXpIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140017,
      "firstName": "Rasmus",
      "lastName": "Lunding",
      "middleInitial": "Skovhus",
      "importedId": "xTIkRWWje3vw7v3qSD4ptQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140018,
      "firstName": "Haoyu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "JgX7k2EYqgctnWJ2ZOxqdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140019,
      "firstName": "Yoyo",
      "lastName": "Hou",
      "middleInitial": "Tsung-Yu",
      "importedId": "LpdVaL2fVHc1fJOZZKkHOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140020,
      "firstName": "Mirjam",
      "lastName": "De Haas",
      "middleInitial": "",
      "importedId": "2nfYzWRnJyrQmllnu331jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140021,
      "firstName": "Waki",
      "lastName": "Kamino",
      "middleInitial": "",
      "importedId": "voPfc9TKnYdQg_V-uyoGYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140022,
      "firstName": "Yue",
      "lastName": "Wan",
      "middleInitial": "",
      "importedId": "VCwuOJfX5f42XyXz3Q0nkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140023,
      "firstName": "Wonse",
      "lastName": "Jo",
      "middleInitial": "",
      "importedId": "Jp5_nYbJ778CWlMhhunpgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140024,
      "firstName": "Penny",
      "lastName": "Sweetser",
      "middleInitial": "",
      "importedId": "JKpCcZdLvwdFwqTnHAPgfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140025,
      "firstName": "Philip",
      "lastName": "Stafford",
      "middleInitial": "B.",
      "importedId": "RKQtJxSYaAwnF0_dA8XWaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140026,
      "firstName": "Patricia",
      "lastName": "Piedade",
      "middleInitial": "",
      "importedId": "Zdy_jpEglnf0tEkLR2FMAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140027,
      "firstName": "Yanheng",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "7NC_Mwog1moqBbhK1PQNHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140028,
      "firstName": "Zackory",
      "lastName": "Erickson",
      "middleInitial": "",
      "importedId": "vN_ktv4MtGGIu12hwEoxyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140029,
      "firstName": "Tasneem",
      "lastName": "Burghleh",
      "middleInitial": "",
      "importedId": "WX9AwO9SBj1VIDp_Y03hoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140030,
      "firstName": "Selma",
      "lastName": "Sabanovic",
      "middleInitial": "",
      "importedId": "AG42RofSkDtyuBdRVXcm_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140031,
      "firstName": "Gerhard",
      "lastName": "Neumann",
      "middleInitial": "",
      "importedId": "R-xclsi4M2mrMkivhnq3ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140032,
      "firstName": "Mark",
      "lastName": "Roberts",
      "middleInitial": "",
      "importedId": "5nPzIrPSFEYyiYvd9iG4FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140033,
      "firstName": "Pulkit",
      "lastName": "Agrawal",
      "middleInitial": "",
      "importedId": "jggHlJ_pE0IPWSGqsXJyOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140034,
      "firstName": "Chao",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "UfPA4bkgjb9EudPKVqJC0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140035,
      "firstName": "Firas",
      "lastName": "Abi-Farraj",
      "middleInitial": "",
      "importedId": "Y68mubzeOTRQ5VQ2mEIymg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140036,
      "firstName": "Jindan",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "NnwGQLIBXU1v1fTK2cGv1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140037,
      "firstName": "Bengisu",
      "lastName": "Cagiltay",
      "middleInitial": "",
      "importedId": "wmkCgYPJNbcjkSDGtczpIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140038,
      "firstName": "Elior",
      "lastName": "Carsenti",
      "middleInitial": "",
      "importedId": "3GajoGpHXrav8V8xlb4IJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140039,
      "firstName": "Malcolm",
      "lastName": "Doering",
      "middleInitial": "",
      "importedId": "uUsVvwFOyHARlJu981dSdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140040,
      "firstName": "Lennart",
      "lastName": "Wachowiak",
      "middleInitial": "",
      "importedId": "un5XeA4KjyDVT3uMpg_Ajg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140041,
      "firstName": "Guy",
      "lastName": "Hoffman",
      "middleInitial": "",
      "importedId": "TB0t8BqJB_-ZFgcF6dzbug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140042,
      "firstName": "Bailey",
      "lastName": "Cislowski",
      "middleInitial": "",
      "importedId": "QujVCKHRs7hq_KxgvgAwzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140043,
      "firstName": "Tiare",
      "lastName": "Feuchtner",
      "middleInitial": "",
      "importedId": "JihlC4aKWjzTUU-0xSTBPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140044,
      "firstName": "Wijnand",
      "lastName": "IJsselsteijn",
      "middleInitial": "",
      "importedId": "wIdgfbLScR_gBQLw_nzkXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140045,
      "firstName": "Srivatsan",
      "lastName": "Chakravarthi Kumaran",
      "middleInitial": "",
      "importedId": "rqE1f90QWAAyULMYoaCDSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140046,
      "firstName": "Mike",
      "lastName": "Ligthart",
      "middleInitial": "E.U.",
      "importedId": "z8Rlbt1kpBSYXN2KeBlSjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140047,
      "firstName": "Elizabeth",
      "lastName": "Twamley",
      "middleInitial": "",
      "importedId": "3COnxcqNor-WS3s_8DVBzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140048,
      "firstName": "Kantwon",
      "lastName": "Rogers",
      "middleInitial": "",
      "importedId": "Xf0W1PtRI59mhpKAs5DqsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140049,
      "firstName": "Reiden John Allen",
      "lastName": "Webber",
      "middleInitial": "",
      "importedId": "-GKyWZcp_568XvFqbEvHnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140050,
      "firstName": "Lisa",
      "lastName": "Gasche",
      "middleInitial": "Alina",
      "importedId": "9ZTtYS-53rxqK60mm1FuZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140051,
      "firstName": "Xiang",
      "lastName": "Pan",
      "middleInitial": "",
      "importedId": "t0kaHiyvLU4IwS3DU6W8-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140052,
      "firstName": "Jehan",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "V3EtKBT4896Ft84VgedaoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140053,
      "firstName": "Hadas",
      "lastName": "Erel",
      "middleInitial": "",
      "importedId": "Ry_K14hx2ejrJqwv_kP1HQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140054,
      "firstName": "Nathan",
      "lastName": "White",
      "middleInitial": "Thomas",
      "importedId": "MZpNyzepQYl11pRhMMxgSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140055,
      "firstName": "Xiaoyu",
      "lastName": "CHANG",
      "middleInitial": "",
      "importedId": "EnIttGnI4A5jF2xlloqZIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140056,
      "firstName": "Hubert P. H.",
      "lastName": "Shum",
      "middleInitial": "",
      "importedId": "eWDq0onigkgAdC1acPnTOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140057,
      "firstName": "Hae Won",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "4ochVRKol5N3gqiawk2vlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140058,
      "firstName": "Janavi",
      "lastName": "Gupta",
      "middleInitial": "",
      "importedId": "BET0_I5tg_V7cRsYnGAr8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140059,
      "firstName": "Alisha",
      "lastName": "Bevins",
      "middleInitial": "",
      "importedId": "wyiPVWncnKljzS9g5hw7Ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140060,
      "firstName": "Chen",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "OYH2kiGc7ia0p1Ke9HxEUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140061,
      "firstName": "Andrew",
      "lastName": "Coles",
      "middleInitial": "I",
      "importedId": "doMzyPsWJPRX0Yjf5GPx5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140062,
      "firstName": "Sebastian",
      "lastName": "Pape",
      "middleInitial": "",
      "importedId": "Tbj03rOfoCrO7Aud7pQM7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140063,
      "firstName": "Marina",
      "lastName": "Cantarutti",
      "middleInitial": "",
      "importedId": "H1wRInbcySAyYFmog_gddA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140064,
      "firstName": "Katherine",
      "lastName": "Dimitropoulou",
      "middleInitial": "",
      "importedId": "8-yFokfE4NAItcINIOpy_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140065,
      "firstName": "Zachary",
      "lastName": "Henkel",
      "middleInitial": "",
      "importedId": "VJhGkqcbpLHl4uJD01lTRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140066,
      "firstName": "Adam",
      "lastName": "Stogsdill",
      "middleInitial": "",
      "importedId": "rAH42O8MgsG7CYGgkQbC0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140067,
      "firstName": "Zhuo",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "eoUeIrsvuXb9nQxpvdX7Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140068,
      "firstName": "Dakota",
      "lastName": "Sullivan",
      "middleInitial": "",
      "importedId": "HaG_ttGN75OYDRSfW4Moww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140069,
      "firstName": "Sijia",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "HH6aggsKfANsxPb1Z6Blfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140070,
      "firstName": "Huili",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "1S_C32aR40HajSnADiEV1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140071,
      "firstName": "Rajat Kumar",
      "lastName": "Jenamani",
      "middleInitial": "",
      "importedId": "V67JlTSP205KGfg5tYYwGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140072,
      "firstName": "Alyssa",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "IW-yuf7AKo3sTl7zVXtbAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140073,
      "firstName": "Haris",
      "lastName": "Kamran",
      "middleInitial": "",
      "importedId": "992JQKGhM1_srE591iFgMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140074,
      "firstName": "Siya",
      "lastName": "Kunde",
      "middleInitial": "",
      "importedId": "DUUknFXcXVB-IGKYiMRB3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140075,
      "firstName": "Alyssa",
      "lastName": "Hanson",
      "middleInitial": "",
      "importedId": "3gAiWTnglhqAFXt97Rwbnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140076,
      "firstName": "Deanna",
      "lastName": "Flynn",
      "middleInitial": "E",
      "importedId": "iqHRpI59YflGzWPYg7_2TA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140077,
      "firstName": "Matthew",
      "lastName": "Luebbers",
      "middleInitial": "B",
      "importedId": "es4ismBU_MBucmVLYs59PQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 140078,
      "firstName": "Kingsley",
      "lastName": "Fletcher",
      "middleInitial": "",
      "importedId": "hrn5us5r02zNfhMsgZoGlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151426,
      "firstName": "Tony",
      "lastName": "Belpaeme",
      "importedId": "10473",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Belgium",
          "institution": "Universiteit Gent"
        }
      ]
    },
    {
      "id": 151427,
      "firstName": "Eduardo",
      "lastName": "Benitez Sandoval",
      "importedId": "10474",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Australia",
          "institution": "Scientia Fellow UNSW"
        }
      ]
    },
    {
      "id": 151428,
      "firstName": "Maartje",
      "lastName": "de Graaf",
      "importedId": "10475",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Netherlands",
          "institution": "Utrecht University"
        }
      ]
    },
    {
      "id": 151429,
      "firstName": "Kim",
      "lastName": "Baraka",
      "importedId": "10476",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Netherlands",
          "institution": "Vrije Universiteit Amsterdam"
        }
      ]
    },
    {
      "id": 151430,
      "firstName": "Ross",
      "lastName": "Mead",
      "importedId": "10477",
      "source": "SYS",
      "affiliations": [
        {
          "country": "United States",
          "institution": "Semio"
        }
      ]
    },
    {
      "id": 151431,
      "firstName": "Henny",
      "lastName": "Admoni",
      "importedId": "10478",
      "source": "SYS",
      "affiliations": [
        {
          "country": "United States",
          "institution": "Carnegie Mellon University"
        }
      ]
    },
    {
      "id": 151432,
      "firstName": "Beth",
      "lastName": "Phillips",
      "importedId": "10479",
      "source": "SYS",
      "affiliations": [
        {
          "country": "United States",
          "institution": "George Mason University"
        }
      ]
    },
    {
      "id": 151433,
      "firstName": "Kerstin",
      "lastName": "Fischer",
      "importedId": "10480",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Denmark",
          "institution": "Southern Denmark University"
        }
      ]
    },
    {
      "id": 151437,
      "firstName": "Ayesha",
      "lastName": "Jena",
      "middleInitial": "",
      "importedId": "t5CoHbhdmt3g2S5CyPkl6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151438,
      "firstName": "Rohit",
      "lastName": "Banerjee",
      "middleInitial": "",
      "importedId": "An1oTwZh2bYbZbAP-C2KxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151439,
      "firstName": "Sarah",
      "lastName": "Leary",
      "middleInitial": "",
      "importedId": "z3M9T62irLqEPAp--D7AKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151440,
      "firstName": "Alan",
      "lastName": "Sanchez",
      "middleInitial": "G.",
      "importedId": "SjaSGF_Fh1DmJgPzRQLNpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151441,
      "firstName": "Jos",
      "lastName": "Prinsen",
      "middleInitial": "",
      "importedId": "Sol7v_IudMwZn6soL_LnFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151442,
      "firstName": "Wendy",
      "lastName": "Rogers",
      "middleInitial": "",
      "importedId": "ilee2Hc4UrH7DM2T6AdotA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151443,
      "firstName": "Giulia Scorza",
      "lastName": "Azzarà",
      "middleInitial": "",
      "importedId": "GAhiREN3uNNEYy7clF8E-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151444,
      "firstName": "Adrian",
      "lastName": "Chiu",
      "middleInitial": "",
      "importedId": "dSb4u4YdklGShMTlWG19cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151445,
      "firstName": "Katharina",
      "lastName": "Rohlfing",
      "middleInitial": "",
      "importedId": "xqtR5pVE-YYNtioudXBGHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151446,
      "firstName": "Rhian",
      "lastName": "Preston",
      "middleInitial": "",
      "importedId": "vRwtimAz6X6DpHbJ-h8VuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151447,
      "firstName": "Jeffrey",
      "lastName": "Ichnowski",
      "middleInitial": "",
      "importedId": "bUh3vJ0dIizQzaK8ujgiVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151448,
      "firstName": "Jose",
      "lastName": "Cortazar",
      "middleInitial": "Carlos",
      "importedId": "96NvYIfQSbYGgXUP2vy55A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151449,
      "firstName": "Meizhu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "zB7IDfmvj59h2O_E-rgnCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151450,
      "firstName": "Trond A.",
      "lastName": "Tjøstheim",
      "middleInitial": "",
      "importedId": "BB0OaVBx76auvpyZz7MF1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151451,
      "firstName": "Caterina",
      "lastName": "Neef",
      "middleInitial": "",
      "importedId": "UNXZ0CLgOgFX8O_NkJMiBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151452,
      "firstName": "Ayan",
      "lastName": "Robinson",
      "middleInitial": "",
      "importedId": "g1w8qqzcrIXV6ZxXRa9Z3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151453,
      "firstName": "Taylor",
      "lastName": "Kessler Faulkner",
      "middleInitial": "",
      "importedId": "Qi4wAqimwfEdp-TLsO0ySQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151454,
      "firstName": "Sandhya",
      "lastName": "Jayaraman",
      "middleInitial": "",
      "importedId": "r3SfvIIU0GwWOkSZsp6OLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151455,
      "firstName": "Agnes",
      "lastName": "Axelsson",
      "middleInitial": "Johanna",
      "importedId": "Xkpt9jhPfLNdfR1dBNywPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151456,
      "firstName": "Yanran",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "QS97aW_SYRwP_k55s0SUlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151457,
      "firstName": "Jasmin",
      "lastName": "Marward",
      "middleInitial": "",
      "importedId": "5OhCzeYr6bBKqZ4jORB2fA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151458,
      "firstName": "Jonathan",
      "lastName": "Liebers",
      "middleInitial": "",
      "importedId": "hjN3Tf8t6kx12-GGBe24TQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151459,
      "firstName": "Ruhan",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "FEExj5K21jryCaYjdOzQLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151460,
      "firstName": "Yan",
      "lastName": "Ding",
      "middleInitial": "",
      "importedId": "fxzOhSWo3F648Oeouxnp2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151461,
      "firstName": "Tony",
      "lastName": "Belpaeme",
      "middleInitial": "",
      "importedId": "QJyidODV0Mn-ZEOxzZ9I4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151462,
      "firstName": "Chirag",
      "lastName": "Jain",
      "middleInitial": "",
      "importedId": "G6_0czN3FpvNsSlCFxGVyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151463,
      "firstName": "Roy",
      "lastName": "de Kleijn",
      "middleInitial": "",
      "importedId": "nubgLdq_uqKDtkI1jWjBOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151464,
      "firstName": "Joseph",
      "lastName": "Michaelis",
      "middleInitial": "E",
      "importedId": "yFDwZgDpzVmcE_6-OITpTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151465,
      "firstName": "Jade",
      "lastName": "Thompson",
      "middleInitial": "",
      "importedId": "DKKlwxFtHHB0iT0ir-60dA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151466,
      "firstName": "Cynthia",
      "lastName": "Matuszek",
      "middleInitial": "",
      "importedId": "j4jT53xc1oMOuigw_51DxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151467,
      "firstName": "Dongho",
      "lastName": "Kwak",
      "middleInitial": "",
      "importedId": "sRrOnHqvvBD2UalKQEjJxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151468,
      "firstName": "Oskar",
      "lastName": "von Stryk",
      "middleInitial": "",
      "importedId": "9YDCDHED1Au8oPFat0Llig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151469,
      "firstName": "Jinmo",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Xzxid3ALvJgxgG_EElv5nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151470,
      "firstName": "Jiaying",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "rKN9ff2nNH8aLrrV4Xd1vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151471,
      "firstName": "Kaito",
      "lastName": "Nakamura",
      "middleInitial": "",
      "importedId": "qAg1rGptuQRFVoUE7IOWWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151472,
      "firstName": "Christian",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "HFXLvDou0-7oyAaCvS7-Hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151473,
      "firstName": "Ioannis",
      "lastName": "Pontikis",
      "middleInitial": "",
      "importedId": "QWDdH0HTqx1dCyFXdhF3Dg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151474,
      "firstName": "Takao",
      "lastName": "Obi",
      "middleInitial": "",
      "importedId": "2fDVH0UZkHXnCsSEFZYZLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151475,
      "firstName": "Eva",
      "lastName": "Wiese",
      "middleInitial": "",
      "importedId": "oKDGZnc0q3EvJ9oYPdP91A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151476,
      "firstName": "Mikhail",
      "lastName": "Litvinov",
      "middleInitial": "",
      "importedId": "yuAxQtIKyRiaILMpOpmm8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151477,
      "firstName": "Frank",
      "lastName": "Regal",
      "middleInitial": "",
      "importedId": "jVT9EDH9k__JeI5NZNBwkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151478,
      "firstName": "Luciano",
      "lastName": "Gamberini",
      "middleInitial": "",
      "importedId": "049O5nWANyLwwCx7KDOnCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151479,
      "firstName": "Brenna",
      "lastName": "Argall",
      "middleInitial": "",
      "importedId": "5Ws0ckPAB3BO2QpTe9nj4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151480,
      "firstName": "Signe",
      "lastName": "Redfield",
      "middleInitial": "",
      "importedId": "4-2QwObHeCOnO_lE-wfrVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151481,
      "firstName": "Christopher",
      "lastName": "Peters",
      "middleInitial": "",
      "importedId": "LJQzJyerYYfO1mAbvfEdog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151482,
      "firstName": "Stephen Jia",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "dZohprFB4_yT3pEVAOJxfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151483,
      "firstName": "Mariacarla",
      "lastName": "Staffa",
      "middleInitial": "",
      "importedId": "QvT2G8H6TCziQ7vErfQO9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151484,
      "firstName": "Christian",
      "lastName": "Tagliamonte",
      "middleInitial": "",
      "importedId": "kCXF-TcjTv4PnRe0qKcPXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151485,
      "firstName": "Dong Won",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "_ZuXPQkzDVVZuoFw__ecjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151486,
      "firstName": "Benjamin",
      "lastName": "Greenberg",
      "middleInitial": "",
      "importedId": "zG56gGP6YRabGi-ZV8_BKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151487,
      "firstName": "Usman",
      "lastName": "Shahid",
      "middleInitial": "",
      "importedId": "vxnTFFskwFDEua_dElu9_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151488,
      "firstName": "Logan",
      "lastName": "Daigler",
      "middleInitial": "",
      "importedId": "zvvxBeAV__AOpZ4xgIFbew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151489,
      "firstName": "Jiaee",
      "lastName": "Cheong",
      "middleInitial": "",
      "importedId": "NFSo3YtGkG_MDWenvhkVYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151490,
      "firstName": "Kei",
      "lastName": "Okada",
      "middleInitial": "",
      "importedId": "KfeU2U04wD5h5c6y1PmW1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151491,
      "firstName": "Mary Ellen",
      "lastName": "Foster",
      "middleInitial": "",
      "importedId": "4CBHKvQY6mHopIa7UkFgXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151492,
      "firstName": "Yimin",
      "lastName": "Qin",
      "middleInitial": "",
      "importedId": "rmscl9LYkmj0gBRClEOmWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151493,
      "firstName": "AJung",
      "lastName": "Moon",
      "middleInitial": "",
      "importedId": "sPHyLboj1f647zD8KI4BaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151494,
      "firstName": "Deehsiao",
      "lastName": "Lew",
      "middleInitial": "",
      "importedId": "13fQUaCFklaR9ctPpCdLSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151495,
      "firstName": "Siyang",
      "lastName": "Ye",
      "middleInitial": "",
      "importedId": "9AMtE46fu1kov3ITRvDpeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151496,
      "firstName": "Enrico",
      "lastName": "Rukzio",
      "middleInitial": "",
      "importedId": "FrzgGkMaoVPeiaBR8A7mbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151497,
      "firstName": "Louis",
      "lastName": "Annabi",
      "middleInitial": "",
      "importedId": "KX387frlMWyWViOBg9ap0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151498,
      "firstName": "Dzmitry",
      "lastName": "Tsetserukou",
      "middleInitial": "",
      "importedId": "GOzgKmYbtSBqAbZpZP9tEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151499,
      "firstName": "Ross",
      "lastName": "Mead",
      "middleInitial": "",
      "importedId": "qb6l7AhGr1-UEG_OecxpCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151500,
      "firstName": "Romain",
      "lastName": "Maure",
      "middleInitial": "",
      "importedId": "iobhdR_3zVvd9t_Q2QoHUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151501,
      "firstName": "Ewen",
      "lastName": "Gay-Semenkoff",
      "middleInitial": "",
      "importedId": "R6S6FJP5O0JXBlpZNFc48g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151502,
      "firstName": "Takahiro",
      "lastName": "Tanaka",
      "middleInitial": "",
      "importedId": "COjkKXsca2hyP8KwkxZhzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151503,
      "firstName": "Philip",
      "lastName": "Beesley",
      "middleInitial": "",
      "importedId": "6Xt09BYlR1OUYNgr3Jh7pQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151504,
      "firstName": "Carolin",
      "lastName": "Strassmann",
      "middleInitial": "",
      "importedId": "6uEO0MswSK5W78preKuhnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151505,
      "firstName": "Uthman",
      "lastName": "Tijani",
      "middleInitial": "",
      "importedId": "PQjWeLeDTVIAy58t4sdGgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151506,
      "firstName": "Fabian",
      "lastName": "Reister",
      "middleInitial": "",
      "importedId": "FYzGXgme4Q1fyClDd2I-Ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151507,
      "firstName": "Peter Hye",
      "lastName": "Nielsen",
      "middleInitial": "",
      "importedId": "weIgACARrZWjYHBDEIYwDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151508,
      "firstName": "Demiana",
      "lastName": "Barsoum",
      "middleInitial": "",
      "importedId": "cBNvn_4PQ2IKxcMfhwVbrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151509,
      "firstName": "Arielle",
      "lastName": "Maignan",
      "middleInitial": "F",
      "importedId": "3QDU4A6_hkCzR4JNddDIlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151510,
      "firstName": "Keita",
      "lastName": "Kiuchi",
      "middleInitial": "",
      "importedId": "QidZSgb21odNhRyu2SZL2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151511,
      "firstName": "Eric",
      "lastName": "Morales",
      "middleInitial": "",
      "importedId": "SKd5Q7djdEBD-nJSa6H9GQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151512,
      "firstName": "Pascal",
      "lastName": "Jansen",
      "middleInitial": "",
      "importedId": "5PZJ8aUpIK1ammYj7rvU1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151513,
      "firstName": "Allan",
      "lastName": "Wollaber",
      "middleInitial": "B",
      "importedId": "EhWgH2E-oI-TOZoBFctitQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151514,
      "firstName": "Xun",
      "lastName": "Cui",
      "middleInitial": "",
      "importedId": "twQK9M2UGsmt69dXGOIOCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151515,
      "firstName": "Angela",
      "lastName": "Pan Ding",
      "middleInitial": "",
      "importedId": "kOlloyR_toMME81_kpAfAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151516,
      "firstName": "Sahba",
      "lastName": "Zojaji",
      "middleInitial": "",
      "importedId": "tXHN5FTRL3T4eJo5hf7ykw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151517,
      "firstName": "Alessandro",
      "lastName": "Ianniello",
      "middleInitial": "",
      "importedId": "ANw5k4JNO10gEvXb2A6LLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151518,
      "firstName": "Lucy E.",
      "lastName": "Ammon",
      "middleInitial": "",
      "importedId": "r226fomXuHRvhJSGT3-0vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151519,
      "firstName": "David",
      "lastName": "Goedicke",
      "middleInitial": "",
      "importedId": "i8M6tXXubf_qr5xQ3kx9vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151520,
      "firstName": "Yugo",
      "lastName": "Hayashi",
      "middleInitial": "",
      "importedId": "brcYO0KNc58HZYwGEaU_Lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151521,
      "firstName": "Jessica",
      "lastName": "Herring",
      "middleInitial": "",
      "importedId": "jiTtsj0Azrm1WvmgbrHGdQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151522,
      "firstName": "Niko",
      "lastName": "Kleer",
      "middleInitial": "",
      "importedId": "Ukjo85Qc5lXnajCiNbAoyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151523,
      "firstName": "Silvia",
      "lastName": "Rossi",
      "middleInitial": "",
      "importedId": "i4gKVvVpFin6UrGcGlaMKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151524,
      "firstName": "Manuel",
      "lastName": "Dietrich",
      "middleInitial": "",
      "importedId": "Ja7MVMf1tzXUzwt-sYJIeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151525,
      "firstName": "Di",
      "lastName": "Fu",
      "middleInitial": "",
      "importedId": "vyTxtlMXUxM3rnTLVwIU6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151526,
      "firstName": "Tyler",
      "lastName": "Barrett",
      "middleInitial": "",
      "importedId": "Kvx2GXCkbYkLInkaP_fLEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151527,
      "firstName": "Francesco",
      "lastName": "Vigni",
      "middleInitial": "",
      "importedId": "cRn5dF0FkHY5NxnK0KSchQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151528,
      "firstName": "Tianhao",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "_TFjlBIMa2QBW99r-AwEow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151529,
      "firstName": "Jongik",
      "lastName": "Jeon",
      "middleInitial": "",
      "importedId": "XXVwAujG0GRWKXZvkFuW5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151530,
      "firstName": "Sebastian",
      "lastName": "Chion Alvarado",
      "middleInitial": "Alberto",
      "importedId": "U60cdtADgheMs4WkN0QDhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151531,
      "firstName": "Polina",
      "lastName": "Arbuzova",
      "middleInitial": "",
      "importedId": "o5ZMd_-lc2kTVk4hqBg0gA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151532,
      "firstName": "Shalutha",
      "lastName": "Rajapakshe Mudiyanselage",
      "middleInitial": "Navindu Rajapakshe",
      "importedId": "Iz6G4hqiLSo-hMahCR8HUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151533,
      "firstName": "Javier",
      "lastName": "Hernandez",
      "middleInitial": "",
      "importedId": "_14Y_DoET_yZq1_Upsb76Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151534,
      "firstName": "Mudit",
      "lastName": "Verma",
      "middleInitial": "",
      "importedId": "YcMxOXmX4ujrXMzOXayYvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151535,
      "firstName": "Eike",
      "lastName": "Schneiders",
      "middleInitial": "",
      "importedId": "Diro3aYqe9FSgymyAhBCNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151536,
      "firstName": "Shogo",
      "lastName": "Okada",
      "middleInitial": "",
      "importedId": "8QHhfZcs0qT22_UwaUoKtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151537,
      "firstName": "Tuvana Dilan",
      "lastName": "Karaduman",
      "middleInitial": "",
      "importedId": "NPRDQzzdi2PqafnpBhtP7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151538,
      "firstName": "Lisa",
      "lastName": "De Gersem",
      "middleInitial": "",
      "importedId": "kcrfBZ8iM-FRAQua4n6eDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151539,
      "firstName": "Mai Lee",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "MHAHhS1A1BxR6qRZeirQzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151540,
      "firstName": "Maximilian",
      "lastName": "Krämer",
      "middleInitial": "",
      "importedId": "-L_Y7eC-Mfn5VHvZtCAF9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151541,
      "firstName": "Davide",
      "lastName": "Zanardi",
      "middleInitial": "",
      "importedId": "BIDCNETu5wqyFpyLrhUorw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151542,
      "firstName": "Haya",
      "lastName": "Bolotski",
      "middleInitial": "",
      "importedId": "5kd_RDotNlO1jaVwvjCgqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151543,
      "firstName": "Kyung Yun",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "Vk74aWSs-AGXlQmNF2824Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151544,
      "firstName": "Beatriz Sousa",
      "lastName": "Santos",
      "middleInitial": "",
      "importedId": "64iY7Hm9BGAbtCE5Nv_1oA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151545,
      "firstName": "Nash",
      "lastName": "Bernhart",
      "middleInitial": "",
      "importedId": "ony-VG6Oqo6UMvwm5KGffg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151546,
      "firstName": "Duy Tho",
      "lastName": "Le",
      "middleInitial": "",
      "importedId": "-pJjZNH3KZKSOhnyCo52jw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151547,
      "firstName": "Rucha",
      "lastName": "Khot",
      "middleInitial": "",
      "importedId": "SGgdtY3Dfb1GEk5uJitWFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151548,
      "firstName": "Daisy",
      "lastName": "Kiyemba",
      "middleInitial": "",
      "importedId": "8uBxF1YXeCAKb5EebVbo2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151549,
      "firstName": "Frank",
      "lastName": "Broz",
      "middleInitial": "",
      "importedId": "atAcm8ZAtuv7_XsYERdLmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151550,
      "firstName": "Hamid",
      "lastName": "Rezatofighi",
      "middleInitial": "",
      "importedId": "maPpHY89uSK9DQzNST-YoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151551,
      "firstName": "Laura",
      "lastName": "Stegner",
      "middleInitial": "",
      "importedId": "J0KsYKDUiJQM6he3wq532Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151552,
      "firstName": "Yasemin",
      "lastName": "Göksu",
      "middleInitial": "",
      "importedId": "aJQlZDiNaOlYrVTkzsbaPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151553,
      "firstName": "Divyasha",
      "lastName": "Pahuja",
      "middleInitial": "",
      "importedId": "GxBZjTa7NTu3KZgdcZ3SCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151554,
      "firstName": "Hunter",
      "lastName": "Brown",
      "middleInitial": "L",
      "importedId": "LKchjpPTmNg4PJJHZChTPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151555,
      "firstName": "Thiago",
      "lastName": "Rocha Silva",
      "middleInitial": "",
      "importedId": "nAPL8ly6uRb6FYA4kbmKrg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151556,
      "firstName": "Marco",
      "lastName": "Inzitari",
      "middleInitial": "",
      "importedId": "vni49RTVFSUA1Skbc9Mt4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151557,
      "firstName": "Samuel",
      "lastName": "Olatunji",
      "middleInitial": "",
      "importedId": "rMudOSOTjouoeJrDS4zx7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151558,
      "firstName": "Valentina",
      "lastName": "Campo",
      "middleInitial": "",
      "importedId": "1pYby8XkIGGPBy6grwImQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151559,
      "firstName": "Hugo",
      "lastName": "Simão",
      "middleInitial": "",
      "importedId": "OmbzajD3oWpzeHFbfXnjqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151560,
      "firstName": "Bhavana",
      "lastName": "Vaddadi",
      "middleInitial": "",
      "importedId": "VxGnUzUWttQ9x19w8OMRVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151561,
      "firstName": "Ya",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "N9E1CFUeQUGrbF6rCdLWbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151562,
      "firstName": "Yngve",
      "lastName": "Kelch",
      "middleInitial": "",
      "importedId": "Q_Rhl21u5bW79l129bqU4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151563,
      "firstName": "Brian",
      "lastName": "Zhang",
      "middleInitial": "John",
      "importedId": "WMn3cfvDpiXccPdbAeLKoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151564,
      "firstName": "Andrew",
      "lastName": "Thompson",
      "middleInitial": "",
      "importedId": "9svznwIt3OQoz7tsnxmZHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151565,
      "firstName": "Fabian",
      "lastName": "Schirmer",
      "middleInitial": "",
      "importedId": "wVJsPmejmMCT7ttek1iGPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151566,
      "firstName": "Atharva",
      "lastName": "Kashyap",
      "middleInitial": "",
      "importedId": "J50iHR1Hxf3Uiwk-pqP5UA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151567,
      "firstName": "Samantha",
      "lastName": "Avalos",
      "middleInitial": "",
      "importedId": "n5-k2MOerk2Zf4tenapOyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151568,
      "firstName": "Katie",
      "lastName": "Trainum",
      "middleInitial": "",
      "importedId": "4_TeDd0BQ4DqlRBG1h76Ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151569,
      "firstName": "Ken",
      "lastName": "Fukuda",
      "middleInitial": "",
      "importedId": "7_m1ECs3y911N9qX9G2d3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151570,
      "firstName": "Annette",
      "lastName": "Kluge",
      "middleInitial": "",
      "importedId": "v4Js6pdQdQrw2Eimlx4QSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151571,
      "firstName": "Fabian",
      "lastName": "Peller-Konrad",
      "middleInitial": "",
      "importedId": "wFKWoC62YVObMx56VvZI7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151572,
      "firstName": "Sharon",
      "lastName": "Baurley",
      "middleInitial": "",
      "importedId": "QBDGS4tWLm3a6EuR6mOVaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151573,
      "firstName": "Ruben",
      "lastName": "Janssens",
      "middleInitial": "",
      "importedId": "gbMGgDyk2l41pBo6VKYdWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151574,
      "firstName": "Masoumeh",
      "lastName": "Mansouri",
      "middleInitial": "",
      "importedId": "v8zXW8KOzH70Mfuv46QoPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151575,
      "firstName": "Kumar",
      "lastName": "Akash",
      "middleInitial": "",
      "importedId": "eWsnUs9ZGVmmpa1vBEphMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151576,
      "firstName": "Ethel",
      "lastName": "Pruss",
      "middleInitial": "",
      "importedId": "tzsVvw31MkfIFq_4rrd_FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151577,
      "firstName": "Tamim",
      "lastName": "Asfour",
      "middleInitial": "",
      "importedId": "DZP0Pq48MiLbSEKj79Pm1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151578,
      "firstName": "Shenando",
      "lastName": "Stals",
      "middleInitial": "",
      "importedId": "4NPwLlB7gi-l3oZoaFh-fA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151579,
      "firstName": "Jon",
      "lastName": "Ferguson",
      "middleInitial": "",
      "importedId": "6KiQgy2hylpCH3JMVCTUDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151580,
      "firstName": "Yoav",
      "lastName": "Ellinson",
      "middleInitial": "",
      "importedId": "wofBZLHKsaBVvvdHqfLD3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151581,
      "firstName": "Verena",
      "lastName": "Hafner",
      "middleInitial": "Vanessa",
      "importedId": "s7Mb-AAXOgVjO_tsSSgS1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151582,
      "firstName": "Maria",
      "lastName": "Lima",
      "middleInitial": "R.",
      "importedId": "LdE3vM2UWytFpPzId8mVfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151583,
      "firstName": "Raj",
      "lastName": "Korpan",
      "middleInitial": "",
      "importedId": "c50xEOQoG2aJ-PKyhTmTYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151584,
      "firstName": "Vaibhav",
      "lastName": "Unhelkar",
      "middleInitial": "",
      "importedId": "TTe8qObM6NtaHAiNMtTFtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151585,
      "firstName": "Chen",
      "lastName": "Shengkang",
      "middleInitial": "",
      "importedId": "8TPhV1j6vzrjfm1LEARDqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151586,
      "firstName": "Chacharin",
      "lastName": "Lertyosbordin",
      "middleInitial": "",
      "importedId": "hsqdm9IyUyXTqGg5fyKmJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151587,
      "firstName": "Wangjie",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "-2PDHLzfVoQRphHsK0TWXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151588,
      "firstName": "Ingar",
      "lastName": "Brinck",
      "middleInitial": "",
      "importedId": "I5u50LuBjR3v6oAhv7tcsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151589,
      "firstName": "Amy",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "MKYjdBtzMEKjAKGTPeKoaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151590,
      "firstName": "Raida",
      "lastName": "Karim",
      "middleInitial": "",
      "importedId": "_ZfoPcTZJ86lRJobViAPGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151591,
      "firstName": "Adrian",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "eOw71xr-U3fPQn-5urKyDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151592,
      "firstName": "Oskar",
      "lastName": "Palinko",
      "middleInitial": "",
      "importedId": "vcNz05OYMrSeSuS0jb6K1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151593,
      "firstName": "Gabriele",
      "lastName": "Abbate",
      "middleInitial": "",
      "importedId": "WKwJrgRdn72TSEAtSqQcFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151594,
      "firstName": "Emanuel",
      "lastName": "Di Nardo",
      "middleInitial": "",
      "importedId": "B_3UkiD3RWaR7lz5VfAMsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151595,
      "firstName": "Konstantinos",
      "lastName": "Christofi",
      "middleInitial": "",
      "importedId": "bURnil2LGZ_R-6FAxCu2mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151596,
      "firstName": "Laura",
      "lastName": "Kunold",
      "middleInitial": "",
      "importedId": "FQaPG4aDFuI-N0SQBu3Brg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151597,
      "firstName": "Luke",
      "lastName": "Sanchez",
      "middleInitial": "",
      "importedId": "tf0wPz_IVgOw0wshpo6fyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151598,
      "firstName": "Lisa Michelle",
      "lastName": "Bohnenkamp",
      "middleInitial": "",
      "importedId": "IJyOMTLOBw5dXkvVuGwKlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151599,
      "firstName": "Sindhu",
      "lastName": "Ravindranath",
      "middleInitial": "",
      "importedId": "soCXTS5WOisr5dfPZIIkjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151600,
      "firstName": "Tabitha",
      "lastName": "Lee",
      "middleInitial": "Edith",
      "importedId": "pu6B6u4LEjOroHpNQWvydg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151601,
      "firstName": "Siddhant",
      "lastName": "Bhambri",
      "middleInitial": "",
      "importedId": "YX-TA2GF6IqLinR-SwSeOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151602,
      "firstName": "Isobel",
      "lastName": "Voysey",
      "middleInitial": "",
      "importedId": "tMA4u5pA6FWukN8JA_mPvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151603,
      "firstName": "Hailong",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "8S-QGbW66K5XTz93ZukW_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151604,
      "firstName": "Julie A.",
      "lastName": "Adams",
      "middleInitial": "",
      "importedId": "LsImYtdDneBU5pGm8NcJkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151605,
      "firstName": "Yiannis",
      "lastName": "Demiris",
      "middleInitial": "",
      "importedId": "2-PECFHR5C24H7j7lrLmYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151606,
      "firstName": "Andrew",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "BfsgSZQxG2f_SbMgBfvreA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151607,
      "firstName": "Tongge",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "zMXG1Ieq4nOr-loJ0ccZbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151608,
      "firstName": "Tim",
      "lastName": "Huff",
      "middleInitial": "",
      "importedId": "MdzZKnDuPqv9794P6hMQNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151609,
      "firstName": "Aniol",
      "lastName": "Civit",
      "middleInitial": "",
      "importedId": "9t8Df72hA0qU-TvGMgkQpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151610,
      "firstName": "Elena",
      "lastName": "Malnatsky",
      "middleInitial": "",
      "importedId": "vODMzjLSx2GF26a2jO6MuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151611,
      "firstName": "Marcel",
      "lastName": "Hinss",
      "middleInitial": "F.",
      "importedId": "40YArzRovEefq609QSbcbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151612,
      "firstName": "Nathan",
      "lastName": "Tsoi",
      "middleInitial": "",
      "importedId": "8YK--vHDT1tHb9NXGaP8Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151613,
      "firstName": "Selen",
      "lastName": "Akay",
      "middleInitial": "",
      "importedId": "32adkXPxk2T4CFhzEHdcMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151614,
      "firstName": "William",
      "lastName": "Hunt",
      "middleInitial": "",
      "importedId": "srdhnylVuNGu57xi5JppcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151615,
      "firstName": "Fabian",
      "lastName": "Hahne",
      "middleInitial": "",
      "importedId": "Uq1ietXrKaxQfSa7CVl_Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151616,
      "firstName": "Anna",
      "lastName": "Dobrosovestnova",
      "middleInitial": "",
      "importedId": "4qiayYQV4rOlB3gB-Ldftw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151617,
      "firstName": "Lukas",
      "lastName": "Eichelberger",
      "middleInitial": "",
      "importedId": "28y-gM8eBOngorWbH0WmAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151618,
      "firstName": "Chad",
      "lastName": "Rose",
      "middleInitial": "",
      "importedId": "updJHiE8RgcE_s8ok3L-lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151619,
      "firstName": "Ana",
      "lastName": "Tanevska",
      "middleInitial": "",
      "importedId": "xATvqeJyunYbFdgWl_iSTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151620,
      "firstName": "Melissa",
      "lastName": "Donnermann",
      "middleInitial": "",
      "importedId": "368_BJyFWZrdaWE8kXHatQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151621,
      "firstName": "Derek",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "Lam3ZG3qOXr2T38ypUXrfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151622,
      "firstName": "Javier",
      "lastName": "Gonzalez-Sanchez",
      "middleInitial": "",
      "importedId": "W02mBkSCV8DpD0fQKurMtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151623,
      "firstName": "Lydia",
      "lastName": "Melles",
      "middleInitial": "",
      "importedId": "SqfTOJt-vHf2R3G8GrtKAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151624,
      "firstName": "Yao",
      "lastName": "Lu",
      "middleInitial": "",
      "importedId": "hfyqXWLVehmx4FSo8OoJ_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151625,
      "firstName": "Nicholas",
      "lastName": "DePalma",
      "middleInitial": "",
      "importedId": "HKIOPi-bX5BjtkJ1cJJTKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151626,
      "firstName": "Minjung",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "X9O4ykD3KtSbK6V91oEpwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151627,
      "firstName": "Fethiye Irmak",
      "lastName": "Doğan",
      "middleInitial": "",
      "importedId": "6km0UTmPSyquebmdPBUVlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151628,
      "firstName": "Jonathon M.",
      "lastName": "Smereka",
      "middleInitial": "",
      "importedId": "LS4eMHSFBtJ8070KWrB-Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151629,
      "firstName": "Stefan",
      "lastName": "Reitmann",
      "middleInitial": "",
      "importedId": "5w1p-ALV8MZbaJ-LMpdjgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151630,
      "firstName": "Astrid",
      "lastName": "Weiss",
      "middleInitial": "",
      "importedId": "VtCzGsrErRroezU6X0x7yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151631,
      "firstName": "Tingting",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "nfxIyAWuLSHMJq0rlBmhhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151632,
      "firstName": "Naoki",
      "lastName": "Kodani",
      "middleInitial": "",
      "importedId": "MgZ3KuGAl-2_B31Bal0eMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151633,
      "firstName": "Alexandra",
      "lastName": "Bejarano",
      "middleInitial": "",
      "importedId": "qifDTj85uE7H7-51tL6kVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151634,
      "firstName": "Min Min",
      "lastName": "Thant",
      "middleInitial": "",
      "importedId": "rQNYfxeHHB_wPm8hAfDJHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151635,
      "firstName": "Susan",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "py3lHzqbEaDuePcTPcnlvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151636,
      "firstName": "Stefan",
      "lastName": "Wermter",
      "middleInitial": "",
      "importedId": "0dNT-UqQZ-QbfQGumhBcAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151637,
      "firstName": "Georgios",
      "lastName": "Angelopoulos",
      "middleInitial": "",
      "importedId": "MsXUudUNE-fc9idKe81Z-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151638,
      "firstName": "Jason",
      "lastName": "Borenstein",
      "middleInitial": "",
      "importedId": "98LqKemI3Lj0pB7eCRqqXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151639,
      "firstName": "Marta",
      "lastName": "Gabbi",
      "middleInitial": "",
      "importedId": "yzOz9gyi2iVQWPX0-T7eRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151640,
      "firstName": "Michelle",
      "lastName": "Lieng",
      "middleInitial": "",
      "importedId": "Zi6HHYymN7SX-8KbkhrnvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151641,
      "firstName": "Amal",
      "lastName": "Nanavati",
      "middleInitial": "",
      "importedId": "OHcKRvNc04QV4P-1Kdp-MQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151642,
      "firstName": "Joel",
      "lastName": "Fischer",
      "middleInitial": "E",
      "importedId": "kSkLnPfXXNiNwJ8eCcWIbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151643,
      "firstName": "Kerstin",
      "lastName": "Fischer",
      "middleInitial": "",
      "importedId": "q7-g9R2bwGZXGf0Z4PQpCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151644,
      "firstName": "Sumitava",
      "lastName": "Mukherjee",
      "middleInitial": "",
      "importedId": "2aGlk28yPqPo36hq8SRuxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151645,
      "firstName": "Aurélie",
      "lastName": "Clodic",
      "middleInitial": "",
      "importedId": "hbkz7dLb8IEI3LB3xqrKsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151646,
      "firstName": "Nichaput",
      "lastName": "Khurukitwanit",
      "middleInitial": "",
      "importedId": "C3nPEFJ3B5jo7eI3I99Uqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151647,
      "firstName": "Aysegül",
      "lastName": "Dogangün",
      "middleInitial": "",
      "importedId": "UXZVT7TnjjzDkhOQi0r6LQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151648,
      "firstName": "Nadine",
      "lastName": "Jansen",
      "middleInitial": "",
      "importedId": "7-g92TQW4yW5NBuA6Ifsyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151649,
      "firstName": "Eleonore",
      "lastName": "Lumer",
      "middleInitial": "",
      "importedId": "HVcwNLILLu9fKGIOYHrfYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151650,
      "firstName": "Federica",
      "lastName": "Nenna",
      "middleInitial": "",
      "importedId": "nPY5UClzdzRJ7GEq0iMF3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151651,
      "firstName": "Mark-Robin",
      "lastName": "Giolando",
      "middleInitial": "",
      "importedId": "PBjzsewgGwmzGtWLV_eIuA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151652,
      "firstName": "Prakash",
      "lastName": "Baskaran",
      "middleInitial": "",
      "importedId": "sYZVg2JRaLSUcO3DKHh_dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151653,
      "firstName": "Andrey",
      "lastName": "Rudenko",
      "middleInitial": "",
      "importedId": "a2Gz6TNop4mvqMaVgFUi9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151654,
      "firstName": "Kristiina",
      "lastName": "Jokinen",
      "middleInitial": "",
      "importedId": "Deu50g6cuRLC48uS2rzAsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151655,
      "firstName": "Artem",
      "lastName": "Bazhenov",
      "middleInitial": "",
      "importedId": "D-EPBSBYCoeZtBeEzA6npg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151656,
      "firstName": "Margherita",
      "lastName": "Peruzzini",
      "middleInitial": "",
      "importedId": "DnVjlG9L2qO5sbP81t652A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151657,
      "firstName": "Joerg",
      "lastName": "Krueger",
      "middleInitial": "",
      "importedId": "ypPfQivJWo7w2qQZMXxnMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151658,
      "firstName": "Jessica",
      "lastName": "Romero",
      "middleInitial": "",
      "importedId": "VWYnvSPm66QgCmTc9tBMvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151659,
      "firstName": "Kristen",
      "lastName": "O’Donnell",
      "middleInitial": "",
      "importedId": "LOhUFawlMfsCESAWAy8T-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151660,
      "firstName": "Frederike",
      "lastName": "Durow",
      "middleInitial": "",
      "importedId": "VZ5phTbsQdDSyGD8kjhM_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151661,
      "firstName": "Matthew",
      "lastName": "Klein",
      "middleInitial": "",
      "importedId": "18HzT48TrYAepJqapwF0Bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151662,
      "firstName": "Alexander",
      "lastName": "Borg",
      "middleInitial": "",
      "importedId": "vPfMUwm-qUNDL9YVjgc2tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151663,
      "firstName": "Raf",
      "lastName": "Ramakers",
      "middleInitial": "",
      "importedId": "D5UPmdoyrXN_yI0FHEzJgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151664,
      "firstName": "Takashi",
      "lastName": "Minato",
      "middleInitial": "",
      "importedId": "nUYmxPRxvew7MNColRnuzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151665,
      "firstName": "Stefan",
      "lastName": "Schneegass",
      "middleInitial": "",
      "importedId": "QMo-7Iuectu0tMAaNm6edg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151666,
      "firstName": "Ville",
      "lastName": "Kyrki",
      "middleInitial": "",
      "importedId": "o4zzxR0k6UmoGkvv9lrPkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151667,
      "firstName": "Uriel",
      "lastName": "Gonzalez-Bravo",
      "middleInitial": "",
      "importedId": "Sr0m7cdevb3XoaQTMc-8Wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151668,
      "firstName": "Peizhong",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "B2n5Tkbaj0Z8No6T27aPVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151669,
      "firstName": "Raquel",
      "lastName": "Ros",
      "middleInitial": "",
      "importedId": "c_Iuo5WXjRuDQfBnQ66Pgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151670,
      "firstName": "Victoria",
      "lastName": "Hurd",
      "middleInitial": "",
      "importedId": "-21-s8B9-nnvd2TcLr17Wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151671,
      "firstName": "Chanyeok",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "qSnKov3Xf4lnMUemGmEJrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151672,
      "firstName": "Boyuan",
      "lastName": "Tuo",
      "middleInitial": "",
      "importedId": "mlADa0tLGiG3iX-oxYzKLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151673,
      "firstName": "Robin",
      "lastName": "Murphy",
      "middleInitial": "R",
      "importedId": "oo-uZTqGwRVI0apfPKxkcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151674,
      "firstName": "Qi",
      "lastName": "Xin",
      "middleInitial": "",
      "importedId": "YO35dqL4GhPuh0Fi7KMBBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151675,
      "firstName": "Alberto Sanfeliu",
      "lastName": "Cortés",
      "middleInitial": "",
      "importedId": "oCl62Tokx1RVwhukOrc6Rg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151676,
      "firstName": "Katherine",
      "lastName": "Kuchenbecker",
      "middleInitial": "J.",
      "importedId": "TOqXlWBpO6e3aPf0zAGmlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151677,
      "firstName": "Lorenzo",
      "lastName": "Sabattini",
      "middleInitial": "",
      "importedId": "DqSJv3NVEk5Vu0SMpQBpTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151678,
      "firstName": "Mark",
      "lastName": "Colley",
      "middleInitial": "",
      "importedId": "k9GwjoFmtOKP1dRf6dW8sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151679,
      "firstName": "Stina",
      "lastName": "Klein",
      "middleInitial": "",
      "importedId": "ZwbhmINX79uZTC1dO2aenA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151680,
      "firstName": "Saahil",
      "lastName": "Sabnis",
      "middleInitial": "",
      "importedId": "WnYr2_pvyk1LgYPsFo28ag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151681,
      "firstName": "Stephanie",
      "lastName": "Arevalo Arboleda",
      "middleInitial": "",
      "importedId": "E223_G5YIWw3vmJR-9SoCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151682,
      "firstName": "ruilin",
      "lastName": "Xiong",
      "middleInitial": "",
      "importedId": "sCI305P8vKjwsv8xLCnGYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151683,
      "firstName": "Christian",
      "lastName": "Dondrup",
      "middleInitial": "",
      "importedId": "RMsfRR6jaQWERhQJ-8bClw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151684,
      "firstName": "Haliza",
      "lastName": "Mat Husin",
      "middleInitial": "",
      "importedId": "UUl4SWuwC8PkUlL5ry2SSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151685,
      "firstName": "Ruth",
      "lastName": "West",
      "middleInitial": "",
      "importedId": "2mLE3LIRq5yrg8w0E-0klw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151686,
      "firstName": "Atharva",
      "lastName": "Dastenavar",
      "middleInitial": "",
      "importedId": "Ks4Fy4IU5NbIxAY_2MG8kg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151687,
      "firstName": "Matthias",
      "lastName": "Rehm",
      "middleInitial": "",
      "importedId": "Yfs6F4RZr2T4BG3Tq8oJrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151688,
      "firstName": "Pauline",
      "lastName": "Heinzmann",
      "middleInitial": "",
      "importedId": "46NpfW9GHJ4Xq0fh-PPMQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151689,
      "firstName": "Alireza",
      "lastName": "M. Kamelabad",
      "middleInitial": "",
      "importedId": "K5hVgSoWYSNFK8G_1f_Yww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151690,
      "firstName": "Hendrik",
      "lastName": "Buschmeier",
      "middleInitial": "",
      "importedId": "5jdHxg0vY4oYm6yPsPHwqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151691,
      "firstName": "Gabriel",
      "lastName": "Skantze",
      "middleInitial": "",
      "importedId": "rTkfxkhfcbaVcB4Zap7nEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151692,
      "firstName": "Fabian",
      "lastName": "Parra Gil",
      "middleInitial": "",
      "importedId": "I0Fot7jNiPS7J4aoLO9W5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151693,
      "firstName": "Sidney",
      "lastName": "Richardson",
      "middleInitial": "",
      "importedId": "FXvQcLAXP_9jLBf3pwvwsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151694,
      "firstName": "Sina",
      "lastName": "Zarrieß",
      "middleInitial": "",
      "importedId": "eYUk2FaHL_sCWwT9KE0dAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151695,
      "firstName": "Joshua",
      "lastName": "Zonca",
      "middleInitial": "",
      "importedId": "S0KCj1_HYTXxfAjXxZvBDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151696,
      "firstName": "Corey",
      "lastName": "Clarke",
      "middleInitial": "D",
      "importedId": "7RqPWNALuqiTKOAIaz6qXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151697,
      "firstName": "Alex",
      "lastName": "Foster",
      "middleInitial": "",
      "importedId": "n6oclFmlkxj_JfW1TjKXqw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151698,
      "firstName": "Akihiro",
      "lastName": "Maehigashi",
      "middleInitial": "",
      "importedId": "G4MZFhIpyFQK--hxBdj2XA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151699,
      "firstName": "Jan",
      "lastName": "Peters",
      "middleInitial": "",
      "importedId": "UcStGorm-4n3pBcKfQ0llA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151700,
      "firstName": "Egle Maria",
      "lastName": "Orlando",
      "middleInitial": "",
      "importedId": "gRxKtyTmos-mp0KXt9UJ9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151701,
      "firstName": "Stephen",
      "lastName": "Robinson",
      "middleInitial": "",
      "importedId": "KrUhQ8WY6VStYxdNUhLJAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151702,
      "firstName": "Anna",
      "lastName": "Dobrosovestnova",
      "middleInitial": "",
      "importedId": "wGsIeODDgqWfuQFfSkyHOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151703,
      "firstName": "İrem",
      "lastName": "Kuyucu",
      "middleInitial": "",
      "importedId": "RMzna5anTrCwNSdatEgWkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151704,
      "firstName": "Eric",
      "lastName": "Nichols",
      "middleInitial": "",
      "importedId": "sq4scTBCeTeqAI5Dnyrv8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151705,
      "firstName": "Bruce",
      "lastName": "MacDonald",
      "middleInitial": "",
      "importedId": "vxmGV73Rqbk-JsWj4Ftpsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151706,
      "firstName": "Hyunjung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "GOmoTEdaKM0q1nbLl1TBZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151707,
      "firstName": "Brice",
      "lastName": "Varini",
      "middleInitial": "",
      "importedId": "SkYaaS1XioITHNdEISBmmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151708,
      "firstName": "Christopher",
      "lastName": "Sanchez",
      "middleInitial": "",
      "importedId": "OJbMh8B700vVJtWxlAsiAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151709,
      "firstName": "Julian",
      "lastName": "Czymmeck",
      "middleInitial": "",
      "importedId": "Nz_ptWev1EX463R44aUKfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151710,
      "firstName": "PHAN",
      "lastName": "Nhat Tien",
      "middleInitial": "",
      "importedId": "rqtLujSnipefGEMXRgS_pA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151711,
      "firstName": "Sarvapali",
      "lastName": "Ramchurn",
      "middleInitial": "",
      "importedId": "Nq7G-NwHv8EhST1W5uP4Ig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151712,
      "firstName": "Daisy",
      "lastName": "Church",
      "middleInitial": "",
      "importedId": "OPfE9bA5Y8C29riGVg5xOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151713,
      "firstName": "Rolly",
      "lastName": "Seth",
      "middleInitial": "",
      "importedId": "lZFWN90XRQTmoA4rLMNZDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151714,
      "firstName": "Torsten",
      "lastName": "Bertram",
      "middleInitial": "",
      "importedId": "YiY0trvQaAU1MIedCyvuiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151715,
      "firstName": "Noah R.",
      "lastName": "Carver",
      "middleInitial": "",
      "importedId": "Rbh6jf5tb55Ne366K5dy-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151716,
      "firstName": "Ajay",
      "lastName": "Joshi",
      "middleInitial": "",
      "importedId": "WAtY-w5gBCrI4KLJoORiQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151717,
      "firstName": "Sejong",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "Eum6M9HHoLv1Zf2Oukq9YA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151718,
      "firstName": "Pratyusha",
      "lastName": "Ghosh",
      "middleInitial": "",
      "importedId": "52DJJCCN-wYsVSjAqHtu5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151719,
      "firstName": "Benjamin",
      "lastName": "Bestmann",
      "middleInitial": "O",
      "importedId": "_yvqb2mFfmCHhTS_DGg-kQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151720,
      "firstName": "Leimin",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "8xhQcrFq7V0_ioFwaYEaAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151721,
      "firstName": "Blair",
      "lastName": "Archibald",
      "middleInitial": "",
      "importedId": "CjpAFGvch7Jvm1zZMfl8pw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151722,
      "firstName": "Jeonghan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "te3njn8ndcCGFh1a0F2S3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151723,
      "firstName": "Stayce",
      "lastName": "Mockel",
      "middleInitial": "",
      "importedId": "iuFjpCLiaZxh40VCUP_OJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151724,
      "firstName": "Chuang",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "JYb3Wpw1NnABiE1M_HX0iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151725,
      "firstName": "Sara",
      "lastName": "Cooper",
      "middleInitial": "",
      "importedId": "MkDYXLr2PH282Ayyo6s0sA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151726,
      "firstName": "Luis",
      "lastName": "Merino",
      "middleInitial": "",
      "importedId": "WDc9CHvipZn4KzkQuJZtYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151727,
      "firstName": "Bram",
      "lastName": "van Deurzen",
      "middleInitial": "",
      "importedId": "ZJaf-bS0kAo13-KV5tprFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151728,
      "firstName": "Tobias",
      "lastName": "Kaupp",
      "middleInitial": "",
      "importedId": "hejPBM8rTpd4xCnHrhaOsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151729,
      "firstName": "Cindy",
      "lastName": "Eudenbach",
      "middleInitial": "",
      "importedId": "s-WK6mymcveltaXQ2OzLIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151730,
      "firstName": "Steven",
      "lastName": "Swanbeck",
      "middleInitial": "",
      "importedId": "FLfYp201rIcrqkUR3jMWIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151731,
      "firstName": "Carl",
      "lastName": "Bettosi",
      "middleInitial": "",
      "importedId": "At_sheVZuFDVIL2X7SfC9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151732,
      "firstName": "Christian",
      "lastName": "Balkenius",
      "middleInitial": "",
      "importedId": "6EV5cZD9Uj2_2G9wHKzp4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151733,
      "firstName": "Naomi",
      "lastName": "Fitter",
      "middleInitial": "",
      "importedId": "AxE-bMeeon8z3wdjDDCEPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151734,
      "firstName": "Marynel",
      "lastName": "Vázquez",
      "middleInitial": "",
      "importedId": "jm83ElHKGSO0rF6efAkk5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151735,
      "firstName": "Dimosthenis",
      "lastName": "Kontogiorgos",
      "middleInitial": "",
      "importedId": "Dt8_jpgTSYlcNcVoadkdag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151736,
      "firstName": "Theresa",
      "lastName": "Prinz",
      "middleInitial": "",
      "importedId": "U0Sa5wLSn8UTg6kaXg6SKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151737,
      "firstName": "Ranjana",
      "lastName": "Mehta",
      "middleInitial": "",
      "importedId": "BwnzbPXvFAZdCRiuF_5pAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151738,
      "firstName": "Joanna",
      "lastName": "Bryson",
      "middleInitial": "",
      "importedId": "HaiKvP9VzTJrx2IzPsHLyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151739,
      "firstName": "Pamina",
      "lastName": "Zwolsky",
      "middleInitial": "",
      "importedId": "MxqYol1_vq3-adRZyf_oXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151740,
      "firstName": "Teruhisa",
      "lastName": "Misu",
      "middleInitial": "",
      "importedId": "wiWhE97skWpakud9LFGVpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151741,
      "firstName": "Tomasz Piotr",
      "lastName": "Kucner",
      "middleInitial": "",
      "importedId": "T7kHUlVuJ-_m3Zpo9l5oZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151742,
      "firstName": "Nils",
      "lastName": "Tolksdorf",
      "middleInitial": "F",
      "importedId": "CyfjTYTHZPtbQGDufrn2AQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151743,
      "firstName": "Kris",
      "lastName": "Luyten",
      "middleInitial": "",
      "importedId": "uKnd7nN3OsOEy2epODWgQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151744,
      "firstName": "Myounghoon",
      "lastName": "Jeon",
      "middleInitial": "",
      "importedId": "Jj0c3guSwqoC-DO7ttWDZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151745,
      "firstName": "Steven",
      "lastName": "Benford",
      "middleInitial": "David",
      "importedId": "GCh41u6Rcut74gQt9aUsjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151746,
      "firstName": "Cristina",
      "lastName": "Zaga",
      "middleInitial": "",
      "importedId": "03sjsJW2hgRnJfx0PFoKKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151747,
      "firstName": "Rabeya",
      "lastName": "Jamshad",
      "middleInitial": "",
      "importedId": "uJfJgp2CVZJOVeT5jlF7WQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151748,
      "firstName": "Zhengtao",
      "lastName": "Ma",
      "middleInitial": "",
      "importedId": "0DsK24oDr72UIChXvog0Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151749,
      "firstName": "Nick",
      "lastName": "Tandavanitj",
      "middleInitial": "",
      "importedId": "c8wbYCsiC0Cp-Iuu51lKnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151750,
      "firstName": "Sergei",
      "lastName": "Satsevich",
      "middleInitial": "",
      "importedId": "PDR_aaH_2V61WDJoahY3Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151751,
      "firstName": "Wang",
      "lastName": "Xiaohan",
      "middleInitial": "",
      "importedId": "ZTwDQWaOB5H2gK2ZOIY97Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151752,
      "firstName": "Lissy",
      "lastName": "Hatfield",
      "middleInitial": "",
      "importedId": "ieIDicXxjc-nU3sqVUiIZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151753,
      "firstName": "Ellen Yi-Luen",
      "lastName": "Do",
      "middleInitial": "",
      "importedId": "70aIp-Di9unATr7X4njXTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151754,
      "firstName": "Katharina",
      "lastName": "Weitz",
      "middleInitial": "",
      "importedId": "LeG-wEQjmX1a17wsu_OFkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151755,
      "firstName": "Matt",
      "lastName": "Adams",
      "middleInitial": "",
      "importedId": "fTsU405TneSJ0hgMI_2EDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151756,
      "firstName": "Andres",
      "lastName": "Ramirez Duque",
      "middleInitial": "Alberto",
      "importedId": "mIRlqdaBjl1HuNdXdfNI-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151757,
      "firstName": "Zhaobo",
      "lastName": "Zheng",
      "middleInitial": "",
      "importedId": "t5brCZXYsVZ5aLQoojnXpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151758,
      "firstName": "Wendy",
      "lastName": "Ju",
      "middleInitial": "",
      "importedId": "p5BP52mcA_s2AjtD8cejnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151759,
      "firstName": "Xinyu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "TP-MB6mnT4YexMVYPETgDg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151760,
      "firstName": "Jose Pablo",
      "lastName": "De la Rosa Gutierrez",
      "middleInitial": "",
      "importedId": "1Tgyi1X7yAGgMxP-0FPSww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151761,
      "firstName": "Elmar",
      "lastName": "Rueckert",
      "middleInitial": "",
      "importedId": "MXkMZ8qKARNzJzIS_RIukg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151762,
      "firstName": "Md Anisur",
      "lastName": "Rahman",
      "middleInitial": "",
      "importedId": "IJmsBoZ_GEfqwOvssKHYvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151763,
      "firstName": "Weria",
      "lastName": "Khaksar",
      "middleInitial": "",
      "importedId": "NppWazF8wY1eHBsfM8kW5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151764,
      "firstName": "Itala",
      "lastName": "Latorre Dueñas",
      "middleInitial": "Valeria",
      "importedId": "yBj9BxsC0tulMVlw38JMAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151765,
      "firstName": "Marcel",
      "lastName": "Heisler",
      "middleInitial": "",
      "importedId": "kj1t90pGQ9RxaRpXxN7gxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151766,
      "firstName": "Chad",
      "lastName": "Tossell",
      "middleInitial": "",
      "importedId": "S91UpMsAW7pUwhktCo3IIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151767,
      "firstName": "Uday",
      "lastName": "Nair",
      "middleInitial": "",
      "importedId": "X3Mu4mkN5OWTyJ1FSQ8aHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151768,
      "firstName": "Gordon",
      "lastName": "Briggs",
      "middleInitial": "",
      "importedId": "wk7VeFJslFHygYFtsMduiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151769,
      "firstName": "Reyhan",
      "lastName": "Aydoğan",
      "middleInitial": "",
      "importedId": "ws6rFSHjNMyqcv-HtToVXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151770,
      "firstName": "Irina",
      "lastName": "Rudenko",
      "middleInitial": "",
      "importedId": "KlLFtRmAVnwLlAB_XLgFyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151771,
      "firstName": "Sara",
      "lastName": "Ljungblad",
      "middleInitial": "",
      "importedId": "kAsFt5DdX7ZXwxp5uXA5gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151772,
      "firstName": "Klaus",
      "lastName": "Bengler",
      "middleInitial": "",
      "importedId": "Yo0yFFW1bUlq5vHPH8QiSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151773,
      "firstName": "Lee",
      "lastName": "Miller",
      "middleInitial": "",
      "importedId": "5CNd-3OKpueyhCBThvfC2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151774,
      "firstName": "Kishan",
      "lastName": "Chandan",
      "middleInitial": "",
      "importedId": "p-lEJLvUcq9vprD74rFsZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151775,
      "firstName": "Wen-I",
      "lastName": "lu",
      "middleInitial": "",
      "importedId": "g9mTqoEjwmL4ILRKXYIZ5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151776,
      "firstName": "Naveen",
      "lastName": "Kumar",
      "middleInitial": "",
      "importedId": "rarM7sfi3mozzV7AXFZuEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151777,
      "firstName": "Vignesh",
      "lastName": "Prasad",
      "middleInitial": "",
      "importedId": "jORgtAO5aJUpHv9tVwTO1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151778,
      "firstName": "Changdan",
      "lastName": "Cao",
      "middleInitial": "",
      "importedId": "DqkrnUU7R60VVJs_xe2ESg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151779,
      "firstName": "Bernie Hao",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "lV3Uj_sbbjf4w9PRvLfYJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151780,
      "firstName": "Vladimir",
      "lastName": "Estivill-Castro",
      "middleInitial": "",
      "importedId": "q9IKhN2YX2mbh47Xlfz7Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151781,
      "firstName": "Katherine",
      "lastName": "Allen",
      "middleInitial": "H.",
      "importedId": "dCjCEPfQS6CtwLemwOuLXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151782,
      "firstName": "Adel",
      "lastName": "Baselizadeh",
      "middleInitial": "",
      "importedId": "FdJ_CnV5GKWF6UqB-nJgYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151783,
      "firstName": "Sabrina",
      "lastName": "Eimler",
      "middleInitial": "Cornelia",
      "importedId": "IN9mITjaUEqFz2deFUWq2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151784,
      "firstName": "Natsuki",
      "lastName": "Yamanobe",
      "middleInitial": "",
      "importedId": "TZ6y-6-iiwC38eycfBhiHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151785,
      "firstName": "Marta",
      "lastName": "Ayats Soler",
      "middleInitial": "",
      "importedId": "AL_0ysOIbWqeU_QkunYBzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151786,
      "firstName": "Youngmoon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "RMPKR5OEQUTyd-_X6iYPqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151787,
      "firstName": "Dave",
      "lastName": "Murray-Rust",
      "middleInitial": "",
      "importedId": "kCayt7WL98Jzn3jKYAmfDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151788,
      "firstName": "Catherine",
      "lastName": "Neubauer",
      "middleInitial": "",
      "importedId": "R34wIkyZk93-fWIuKVb8Vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151789,
      "firstName": "Leonard",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "HKs0rJf45OdpMRRaZE8AVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151790,
      "firstName": "Chenghao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "HDXX84FYyefxKh4ZapR33A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151791,
      "firstName": "Jonroy",
      "lastName": "Canady",
      "middleInitial": "",
      "importedId": "FTYZoQ5ynm7lxiX1z-vSJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151792,
      "firstName": "Saurav",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "qh9aF6Kr9daMtCgP9bBXyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151793,
      "firstName": "Sarah",
      "lastName": "Scrapchansky",
      "middleInitial": "",
      "importedId": "OtUugOMY6_WUeZALhdlcLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151794,
      "firstName": "James",
      "lastName": "Young",
      "middleInitial": "Everett",
      "importedId": "eQtWNJ--G-BZLahgX7WsJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151795,
      "firstName": "Patricia",
      "lastName": "Alves-Oliveira",
      "middleInitial": "",
      "importedId": "GkjoH8ctblSs8YLfJj6h2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151796,
      "firstName": "James",
      "lastName": "Kennedy",
      "middleInitial": "",
      "importedId": "Fk9B0fotN3yw32i5tLj62g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151797,
      "firstName": "Zipeng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "OoLM4OkXDte7tEb8CTQaMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151798,
      "firstName": "Matous",
      "lastName": "Jelinek",
      "middleInitial": "",
      "importedId": "oWcazE7eeqOpPMRRkxpvKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151799,
      "firstName": "Neeraj",
      "lastName": "Cherakara",
      "middleInitial": "",
      "importedId": "AFotDhks1KDW-zPck4ChkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151800,
      "firstName": "Anja",
      "lastName": "Richert",
      "middleInitial": "",
      "importedId": "RIwybZnE1pni-wWAXEgtGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151801,
      "firstName": "William Alexander",
      "lastName": "Meza Alburqueque",
      "middleInitial": "William",
      "importedId": "2E50otWqdohePhmmZJmPVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151802,
      "firstName": "Takanori",
      "lastName": "Komatsu",
      "middleInitial": "",
      "importedId": "GBT4e82kNQBDSJBORWidNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151803,
      "firstName": "Jane",
      "lastName": "Evans",
      "middleInitial": "",
      "importedId": "AUI9QK3ZEC6x37U2gNHceQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151804,
      "firstName": "Daniel",
      "lastName": "Hernandez Garcia",
      "middleInitial": "",
      "importedId": "cUZfaHNi5SBsJGNiG3y0gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151805,
      "firstName": "Yota",
      "lastName": "Hatano",
      "middleInitial": "",
      "importedId": "rT37zqbrGcPSrvGYUSRBkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151806,
      "firstName": "Paul",
      "lastName": "Reisert",
      "middleInitial": "",
      "importedId": "ebBBNW0nwl9_1x30Tm6mcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151807,
      "firstName": "Alessandra",
      "lastName": "Sciutti",
      "middleInitial": "",
      "importedId": "WTkiuyoaHBPp_7-RWnJYng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151808,
      "firstName": "John Allan",
      "lastName": "Øllgaard",
      "middleInitial": "Brøndum Holm",
      "importedId": "SjhmDorN8jj3DG_SvnPcGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151809,
      "firstName": "Jovan",
      "lastName": "Menezes",
      "middleInitial": "Clive",
      "importedId": "awrr7qcQvRnRlKtZF2t5CQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151810,
      "firstName": "Mathias",
      "lastName": "Rihet",
      "middleInitial": "",
      "importedId": "aoMn-k8VfQSeYrjKZ5PMug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151811,
      "firstName": "Tyler",
      "lastName": "Schrenk",
      "middleInitial": "",
      "importedId": "uFLnkAnLyy7XUS0ZXGW8og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151812,
      "firstName": "Zahra",
      "lastName": "Zahedi",
      "middleInitial": "",
      "importedId": "KHM04DW7gsXdnn8OL_R9_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151813,
      "firstName": "Maëlis",
      "lastName": "Lefebvre",
      "middleInitial": "",
      "importedId": "U-FxPmZuQ166d245uWng0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151814,
      "firstName": "Kevin",
      "lastName": "Zinta",
      "middleInitial": "",
      "importedId": "fuQ3c-3tp0piNMpaiKNlXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151815,
      "firstName": "Zhongyi",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "B2qoVziW_XADVSdF-QFBsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151816,
      "firstName": "Eugenia",
      "lastName": "Wildt",
      "middleInitial": "",
      "importedId": "c-ONTRv0gxAonWo8MhZIXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151817,
      "firstName": "Jean",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "froTMDN07Q_loaXoAkdKZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151818,
      "firstName": "Kayla",
      "lastName": "Riegner",
      "middleInitial": "",
      "importedId": "I8dSZETHJRMryuYtYATm9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151819,
      "firstName": "Adwitiya",
      "lastName": "Mandal",
      "middleInitial": "",
      "importedId": "vRENtC_xqzAcc5KmBtqoiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151820,
      "firstName": "Jérôme",
      "lastName": "Kirchhoff",
      "middleInitial": "",
      "importedId": "rnZZ3nOpitW7NOPvGf1lfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151821,
      "firstName": "Heiko",
      "lastName": "Hübert",
      "middleInitial": "",
      "importedId": "TUhKaT_H97c0VERMc3xDOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151822,
      "firstName": "Balachandra",
      "lastName": "Bhat",
      "middleInitial": "",
      "importedId": "r82Rs_n4NQQ7Bvg4xA47WA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151823,
      "firstName": "Simon",
      "lastName": "Lacroix",
      "middleInitial": "",
      "importedId": "9O-xGO13To7hex7lFqzuZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151824,
      "firstName": "Chipp",
      "lastName": "Jansen",
      "middleInitial": "",
      "importedId": "jrUvKzayQzf4nvipzGAz7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151825,
      "firstName": "Raphaëlle",
      "lastName": "Roy",
      "middleInitial": "N.",
      "importedId": "QxN1ivkCQnhfAGcoBe7HMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151826,
      "firstName": "Susan",
      "lastName": "Mohammed",
      "middleInitial": "",
      "importedId": "lV9EQAsyXlyO2rM5ISV1og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151827,
      "firstName": "Ronald",
      "lastName": "Arkin",
      "middleInitial": "",
      "importedId": "-F4p4S36b9RNrDFA3n_0IA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151828,
      "firstName": "Junko",
      "lastName": "Kanero",
      "middleInitial": "",
      "importedId": "Rl4if5kK-tfHwUDm2oMOZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151829,
      "firstName": "Diana",
      "lastName": "Saplacan",
      "middleInitial": "",
      "importedId": "nEEJcPja2GYlnAoPVcH9WA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151830,
      "firstName": "Qiping",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "JVPGfxCFFuiIx-LJ_Ms-XQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151831,
      "firstName": "Alap",
      "lastName": "Kshirsagar",
      "middleInitial": "",
      "importedId": "KzRhtWwDUmljyOhBT5qK3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151832,
      "firstName": "Nora",
      "lastName": "Weinberger",
      "middleInitial": "",
      "importedId": "CY9dQxvRWCoW3x-ACrssWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151833,
      "firstName": "Graham",
      "lastName": "Wilcock",
      "middleInitial": "",
      "importedId": "AF8LBNJXoUHo0dOO1BUUBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151834,
      "firstName": "Martin",
      "lastName": "Feick",
      "middleInitial": "",
      "importedId": "0D4Zr0Q_AnaOfaM9Sq655Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151835,
      "firstName": "Nicola",
      "lastName": "Webb",
      "middleInitial": "",
      "importedId": "jvD-BMeTNOQREqwnbiS3Jw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151836,
      "firstName": "Daniela",
      "lastName": "Fogli",
      "middleInitial": "",
      "importedId": "eYXfIg8n3Obs1VC3SV3IFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151837,
      "firstName": "Reid",
      "lastName": "Simmons",
      "middleInitial": "",
      "importedId": "TyukSMduIDeWNeNYadYYHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151838,
      "firstName": "Isaac",
      "lastName": "Felix",
      "middleInitial": "Aurelio",
      "importedId": "GDSXZM1INk4R4-Zacf_d-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151839,
      "firstName": "Carmen",
      "lastName": "Jerez Molina",
      "middleInitial": "",
      "importedId": "TSyXVJSvk1Mmg3Bxy-tlvg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151840,
      "firstName": "Pengfei",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "1w5aSS2pOY5ruqDPz6irZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151841,
      "firstName": "Farzeen",
      "lastName": "Munir",
      "middleInitial": "",
      "importedId": "abC0e31OzIpoAFZFQ7vGNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151842,
      "firstName": "Thomas",
      "lastName": "Demeester",
      "middleInitial": "",
      "importedId": "FFXn0YfcoY-IlBe0cR4KvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151843,
      "firstName": "Marieke",
      "lastName": "van Otterdijk",
      "middleInitial": "",
      "importedId": "RS6Vg3qSbjtED_U80HtrWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151844,
      "firstName": "Ayodeji",
      "lastName": "Abioye",
      "middleInitial": "Opeyemi",
      "importedId": "Rh-wBKV7ANChCFSFI0-w3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151845,
      "firstName": "Maryam",
      "lastName": "Alimardani",
      "middleInitial": "",
      "importedId": "AsuXYz1yImraV7TOIVreXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151846,
      "firstName": "Massimo",
      "lastName": "Donini",
      "middleInitial": "",
      "importedId": "0vhhiTFXOlGwSf4m4N5XUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151847,
      "firstName": "Lydia",
      "lastName": "Melles",
      "middleInitial": "B",
      "importedId": "LBxkzDKr2vBaYNKmiGygwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151848,
      "firstName": "Rohit",
      "lastName": "Mallick",
      "middleInitial": "",
      "importedId": "jC6VDZ5qMmTutyLjllBgZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151849,
      "firstName": "Jule",
      "lastName": "Meerwein",
      "middleInitial": "",
      "importedId": "poXUDTYSAomxUCa-HXeKgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151850,
      "firstName": "Angus",
      "lastName": "Addlesee",
      "middleInitial": "",
      "importedId": "x493Wmzeca_wP3P6F3pkNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151851,
      "firstName": "Isuru",
      "lastName": "Jayarathne",
      "middleInitial": "",
      "importedId": "9yHCTurNd4j0t5zKnSj1Aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151852,
      "firstName": "Raphaëlle",
      "lastName": "Roy",
      "middleInitial": "N.",
      "importedId": "dBv3waNDxH9ZWmXGh2T4vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151853,
      "firstName": "Joel",
      "lastName": "Currie",
      "middleInitial": "William George",
      "importedId": "ezOsptodoEaCCevTxZPdLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151854,
      "firstName": "Nancie",
      "lastName": "Gunson",
      "middleInitial": "A",
      "importedId": "zfE6ufUU3JF5QEITsT-Ibg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151855,
      "firstName": "Makoto",
      "lastName": "Mezawa",
      "middleInitial": "",
      "importedId": "wTUtLc-o6jXL_v_SWSlhgg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151856,
      "firstName": "Ewart",
      "lastName": "de Visser",
      "middleInitial": "",
      "importedId": "wrThBYShWpYWC15AqhfORw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151857,
      "firstName": "Angelo",
      "lastName": "Ciaramella",
      "middleInitial": "",
      "importedId": "qQ8XVucliR1AMeolRY-JcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151858,
      "firstName": "Michał",
      "lastName": "Krępa",
      "middleInitial": "",
      "importedId": "sHuTkx7aiKFYtAen4cZFzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151859,
      "firstName": "Nahoko",
      "lastName": "Kameo",
      "middleInitial": "",
      "importedId": "xylpeioZduwQbh4yCfj73A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151860,
      "firstName": "Taishi",
      "lastName": "Sawabe",
      "middleInitial": "",
      "importedId": "nIJYD-XV3GEe-q-CRltYWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151861,
      "firstName": "Daniel Fernando",
      "lastName": "Preciado Vanegas",
      "middleInitial": "",
      "importedId": "SaSL3KDiBl0IgQA2hMbiHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151862,
      "firstName": "Heinrich",
      "lastName": "Mellmann",
      "middleInitial": "",
      "importedId": "GC73EWdVtsIRuVKeBNxrmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151863,
      "firstName": "Martin",
      "lastName": "Simecek",
      "middleInitial": "",
      "importedId": "w9LJoHMOy65zJASPnTfhfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151864,
      "firstName": "Fábio",
      "lastName": "Barros",
      "middleInitial": "",
      "importedId": "qCQROyn2yS4jSsrLQSs7EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151865,
      "firstName": "Linus",
      "lastName": "Nwankwo",
      "middleInitial": "Ebere",
      "importedId": "6JfM3_jn1Uzvaja-qAvRCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151866,
      "firstName": "Yukiko",
      "lastName": "Nakano",
      "middleInitial": "",
      "importedId": "ZajyTJ7YIoiCzdHiS6baOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151867,
      "firstName": "Yukiyasu",
      "lastName": "Domae",
      "middleInitial": "",
      "importedId": "HjK8ejNAM1ngqFaJRvUWVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151868,
      "firstName": "Philippe",
      "lastName": "Warren",
      "middleInitial": "",
      "importedId": "tvSWWeg2_NvBx6Z77nabrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151869,
      "firstName": "Sam",
      "lastName": "Thellman",
      "middleInitial": "",
      "importedId": "lMtnRA9jgzxFfMypl1yhKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151870,
      "firstName": "Noah",
      "lastName": "Rahimzadagan",
      "middleInitial": "",
      "importedId": "XZ-3ZzGtOJhsOwckIBA9KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151871,
      "firstName": "Dawn",
      "lastName": "Tilbury",
      "middleInitial": "",
      "importedId": "OSQ9FSfS1-bkGAdioZg2fg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151872,
      "firstName": "Ryusei",
      "lastName": "Kimura",
      "middleInitial": "",
      "importedId": "wsV3k7oZr7tZDBfl4TLJiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151873,
      "firstName": "Philipp",
      "lastName": "Hock",
      "middleInitial": "",
      "importedId": "ckuj20bD-h3w540gdAZxug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151874,
      "firstName": "Wei",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "D2sxixEUqIKwVH0jSZ_wEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151875,
      "firstName": "Ali Ahmad",
      "lastName": "Malik",
      "middleInitial": "",
      "importedId": "G9NqsQ9FSjzLBXgl_Y0yTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151876,
      "firstName": "Jennifer",
      "lastName": "Mitchell",
      "middleInitial": "J",
      "importedId": "CehYAXHrPBAR8d92xEjahg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151877,
      "firstName": "Seiji",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "BxiJd73kUrSz8QjN1B8Orw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151878,
      "firstName": "Jaime",
      "lastName": "Peña",
      "middleInitial": "D",
      "importedId": "SSwTZ1oVS6GmBSjuWEKPCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151879,
      "firstName": "Ifrah",
      "lastName": "Idrees",
      "middleInitial": "",
      "importedId": "9aGF-ZuRHjuyC3zsiy8e4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151880,
      "firstName": "Tristan",
      "lastName": "Driver",
      "middleInitial": "B",
      "importedId": "bdEBtwAjq3r5Uw4tIECaMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151881,
      "firstName": "Tomo",
      "lastName": "Funayama",
      "middleInitial": "",
      "importedId": "Hg-Fua4ph2SPOXDfdRNrlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151882,
      "firstName": "Elizabeth",
      "lastName": "Phillips",
      "middleInitial": "",
      "importedId": "irKqLjVNHEYepRNNxHnqrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151883,
      "firstName": "Haimin",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "5G0OEFMMbb8YmY-fcpUxzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151884,
      "firstName": "Guillem",
      "lastName": "Alenyà",
      "middleInitial": "",
      "importedId": "jz8NtbI6Hw3hPOVU4CzgYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151885,
      "firstName": "Alessandro",
      "lastName": "Suglia",
      "middleInitial": "",
      "importedId": "rO-t1SScsAuMGVAXCJYb1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151886,
      "firstName": "Alejandro",
      "lastName": "Suárez",
      "middleInitial": "",
      "importedId": "eirk-EkEAeeIKLuSmf8PEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151887,
      "firstName": "Rosalind",
      "lastName": "Picard",
      "middleInitial": "",
      "importedId": "3TLA-dhLA9ZsOpEqp4wi-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151888,
      "firstName": "Eurico",
      "lastName": "Pedrosa",
      "middleInitial": "",
      "importedId": "Ekp5revMeEx_5WpJVNsddA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151889,
      "firstName": "Kerstin",
      "lastName": "Haring",
      "middleInitial": "S",
      "importedId": "jdEbMOnU1FrNQpF29JrJfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151890,
      "firstName": "Marc",
      "lastName": "Dalmasso",
      "middleInitial": "",
      "importedId": "dWZTuBLUDj_HNbV4Ql8Bbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151891,
      "firstName": "Yijie",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "mrXDKKRv2c7OmXm628ratw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151892,
      "firstName": "Alessandro",
      "lastName": "Saracco",
      "middleInitial": "",
      "importedId": "dQJiA-gH3iNZZoLDbjGf9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151893,
      "firstName": "Adam",
      "lastName": "Norton",
      "middleInitial": "",
      "importedId": "vBKf2E4axkqGLkP2Kch0pQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151894,
      "firstName": "JongSuk",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "SPIurJ3eN0Ule0ZbsxOL9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151895,
      "firstName": "Haipeng",
      "lastName": "Mi",
      "middleInitial": "",
      "importedId": "xhJhgCfvYh1iyoqbNPme0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151896,
      "firstName": "Sao Mai",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "qiTHLPf9HkjUwZmcebGwMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151897,
      "firstName": "Sheinghui",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "YbmXkyxJOohkqfxn036oOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151898,
      "firstName": "Anastasia",
      "lastName": "Fiolka",
      "middleInitial": "Klara",
      "importedId": "YlxYBSd51JMsNZ5oiKxSlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151899,
      "firstName": "Andres",
      "lastName": "Rosero",
      "middleInitial": "",
      "importedId": "ZJ0QfYbHrUNeeeNEuc_eXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151900,
      "firstName": "Marc-Antoine",
      "lastName": "Maheux",
      "middleInitial": "",
      "importedId": "ECt6qM0BjQRTfFBA0UUDtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151901,
      "firstName": "Priyankari",
      "lastName": "Perali",
      "middleInitial": "",
      "importedId": "lQ3_MEUQNWuwgckuRLPuNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151902,
      "firstName": "Yuhan",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "TAzE7TmPU7eltODL52lUag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151903,
      "firstName": "Peter",
      "lastName": "McKenna",
      "middleInitial": "Edward",
      "importedId": "8vN5wk2MfK1hjd8rNkHMxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151904,
      "firstName": "Mohammad",
      "lastName": "Naiseh",
      "middleInitial": "",
      "importedId": "zS4e7QFvKTJXdB8iLkNjdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151905,
      "firstName": "Ravi",
      "lastName": "Prakash",
      "middleInitial": "",
      "importedId": "MQV32sISlESKl6d0oJ8p4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151906,
      "firstName": "Sarah",
      "lastName": "Hopko",
      "middleInitial": "",
      "importedId": "roHLeItPH29Qyo4orMn4Tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151907,
      "firstName": "Charles",
      "lastName": "Dutau",
      "middleInitial": "",
      "importedId": "BBSr0qCf4XZjKyhGwzuhfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151908,
      "firstName": "Josh",
      "lastName": "Bhagat Smith",
      "middleInitial": "",
      "importedId": "SNolcjFt4lGwE4JkFDDZ4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151909,
      "firstName": "Artem",
      "lastName": "Lykov",
      "middleInitial": "",
      "importedId": "ijY342PVa-HQRu-djgSzew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151910,
      "firstName": "Yunjae",
      "lastName": "Nam",
      "middleInitial": "",
      "importedId": "kDocuDmxVQCQjC8B06KA_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151911,
      "firstName": "Sayanti",
      "lastName": "Roy",
      "middleInitial": "",
      "importedId": "eCIQrP0X1gVNsVkJ5cquPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151912,
      "firstName": "Mila",
      "lastName": "Manzano",
      "middleInitial": "",
      "importedId": "ZjbgRAp0a51elMUCiwa35w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151913,
      "firstName": "Ravi",
      "lastName": "Vaidyanathan",
      "middleInitial": "",
      "importedId": "ag_Myw15DhNBujVRlFR6eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151914,
      "firstName": "Alia",
      "lastName": "Saad",
      "middleInitial": "",
      "importedId": "6kb1qmH9IBi8m3ZjQSwTYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151915,
      "firstName": "Usama",
      "lastName": "Ali",
      "middleInitial": "",
      "importedId": "ha3HUFQnW0WzyFU2E2_lSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151916,
      "firstName": "Sai Nitish",
      "lastName": "Vemuri",
      "middleInitial": "",
      "importedId": "J-AX7JymEWtCHAVtFivNJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151917,
      "firstName": "Marlena",
      "lastName": "Fraune",
      "middleInitial": "",
      "importedId": "Bk1uxIrdQ8ekmxjsfpSWuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151918,
      "firstName": "Margherita",
      "lastName": "Nannetti",
      "middleInitial": "",
      "importedId": "iZuY2UkXOoewo1F2vGRlww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151919,
      "firstName": "Matti",
      "lastName": "Vahs",
      "middleInitial": "",
      "importedId": "QPL3XjIC4YfB5o2qAzOUvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151920,
      "firstName": "Kentaro",
      "lastName": "Honma",
      "middleInitial": "",
      "importedId": "JNMNsgxBzyz3iLGqobTeIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151921,
      "firstName": "Dahyun",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "faXC5FwaOWKDB5_TmKfM4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151922,
      "firstName": "Nivan",
      "lastName": "Nelson",
      "middleInitial": "",
      "importedId": "DVYQ1gG0zmqMgNrhy-9ZrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151923,
      "firstName": "Shaul",
      "lastName": "Ashkenazi",
      "middleInitial": "",
      "importedId": "SxdHjP2rZHqcs73m_mYudA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151924,
      "firstName": "Stela",
      "lastName": "Seo",
      "middleInitial": "Hanbyeol",
      "importedId": "s-FT4pCIFgFLP5k6Q73R6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151925,
      "firstName": "Matthew",
      "lastName": "Aylett",
      "middleInitial": "Peter",
      "importedId": "QDjVwvn1C9zWcvJshearvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151926,
      "firstName": "Ralf",
      "lastName": "Vetter",
      "middleInitial": "",
      "importedId": "OfRNQDIfJCqmWl7y-r_5LA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151927,
      "firstName": "Georgia",
      "lastName": "Chalvatzaki",
      "middleInitial": "",
      "importedId": "zwbX9oRx376Xmbzv2dNFFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151928,
      "firstName": "Yasuto",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "importedId": "lQn0gxgvpBHyPgLRwrGkhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151929,
      "firstName": "Mohammad",
      "lastName": "Soorati",
      "middleInitial": "",
      "importedId": "BPckPjGJy0SsgCKA-9Wxjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151930,
      "firstName": "Michele",
      "lastName": "Sevegnani",
      "middleInitial": "",
      "importedId": "P7XP0PR0Z4CicbZKoPCp1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151931,
      "firstName": "Max",
      "lastName": "Fischer",
      "middleInitial": "",
      "importedId": "0vkQjGvXXGQN7h-CnZCJFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151932,
      "firstName": "Alexander",
      "lastName": "Raake",
      "middleInitial": "",
      "importedId": "169_SECnbge0qGbMUaaKAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151933,
      "firstName": "Breanne",
      "lastName": "Oo",
      "middleInitial": "",
      "importedId": "1y18m44QHQ-Cr8WAJaOjiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151934,
      "firstName": "Divya",
      "lastName": "Patel",
      "middleInitial": "",
      "importedId": "nFXteuQ2pAeWuenapp_S4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151935,
      "firstName": "Upasana",
      "lastName": "Biswas",
      "middleInitial": "",
      "importedId": "rWrIqhbodYzuwjTFgGERQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151936,
      "firstName": "Barbara",
      "lastName": "Bruno",
      "middleInitial": "",
      "importedId": "HnYhRdQpfAWckqNHktyXHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151937,
      "firstName": "Hayato",
      "lastName": "Kobayashi",
      "middleInitial": "",
      "importedId": "tkntfpTSbjGo6mxQjzsqYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151938,
      "firstName": "YI",
      "lastName": "ZHAO",
      "middleInitial": "",
      "importedId": "9Y8wrqxU6SDeqxYLBHTBSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151939,
      "firstName": "Isaac Malette",
      "lastName": "Cayer",
      "middleInitial": "",
      "importedId": "U6dtkN8zeWdOQpC7JgKDmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151940,
      "firstName": "Jason",
      "lastName": "Wilson",
      "middleInitial": "",
      "importedId": "nbpjlMNJFPzQ-7lDVaYwYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151941,
      "firstName": "Luigi",
      "lastName": "Gargioni",
      "middleInitial": "",
      "importedId": "V1mv4RhX2ATeZ8U5e-kprg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151942,
      "firstName": "Hiroto",
      "lastName": "Horimoto",
      "middleInitial": "",
      "importedId": "EMlCn3m3f-35yi2kpbRkfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151943,
      "firstName": "Uksang",
      "lastName": "Yoo",
      "middleInitial": "",
      "importedId": "tg5WMkL7w6BSY4DAWCEp_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151944,
      "firstName": "Toby",
      "lastName": "Gosnall",
      "middleInitial": "",
      "importedId": "PEDUlAmWGXPoZvOlCXpXsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151945,
      "firstName": "Theresa",
      "lastName": "Law",
      "middleInitial": "",
      "importedId": "2y83NOmkw3BZStmZYKYU_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151946,
      "firstName": "Douglas",
      "lastName": "Crocker",
      "middleInitial": "",
      "importedId": "w490C2RL9CqOVbNz5N6grw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151947,
      "firstName": "Zhaodan",
      "lastName": "Kong",
      "middleInitial": "",
      "importedId": "29FAqhs5OC7RX2TZRqBiow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151948,
      "firstName": "Mikhail",
      "lastName": "Konenkov",
      "middleInitial": "",
      "importedId": "kAw9r4sosK65KZ0Vh7DwtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151949,
      "firstName": "Ningshi",
      "lastName": "Yao",
      "middleInitial": "",
      "importedId": "MM-9hm3t1FzYuY2D9qS6aA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151950,
      "firstName": "JongYoon",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "6yuHnCIXu5gZhGnWcYtEnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151951,
      "firstName": "Nikita",
      "lastName": "Burtsev",
      "middleInitial": "",
      "importedId": "b6d9BZue_mhzMpVvFMVxIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151952,
      "firstName": "Jennifer",
      "lastName": "Kim",
      "middleInitial": "G",
      "importedId": "TgQEc3ore7X_lFwtsmlxaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151953,
      "firstName": "Bill",
      "lastName": "Smart",
      "middleInitial": "",
      "importedId": "k_TnTbkOzjv0C68blAXWLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151954,
      "firstName": "Aakash",
      "lastName": "Yadav",
      "middleInitial": "",
      "importedId": "2oP01GB5QeyQywaVjqBaNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151955,
      "firstName": "Randy",
      "lastName": "Gomez",
      "middleInitial": "",
      "importedId": "uw5Uxfg4euWxRT3C_ljoNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151956,
      "firstName": "Tsvetomila",
      "lastName": "Mihaylova",
      "middleInitial": "",
      "importedId": "o_EZ_cKhH6yfmRQK8rcnhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151957,
      "firstName": "Alan",
      "lastName": "Wagner",
      "middleInitial": "",
      "importedId": "H-gxgNf-683D7j3eqESnEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151958,
      "firstName": "Frédéric",
      "lastName": "Elisei",
      "middleInitial": "",
      "importedId": "ekClFUbvOURSJfjUX4Uuhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151959,
      "firstName": "Advika",
      "lastName": "Sonti",
      "middleInitial": "",
      "importedId": "n7pvDpWTfrZNdl0Fpmwp5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151960,
      "firstName": "Kohei",
      "lastName": "Tateyama",
      "middleInitial": "",
      "importedId": "sXzXImrizcjVF30emCCLuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151961,
      "firstName": "Nnamdi",
      "lastName": "Nwagwu",
      "middleInitial": "",
      "importedId": "xd9CONsavcdCwmd7FyMPcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151962,
      "firstName": "Cengiz",
      "lastName": "Acarturk",
      "middleInitial": "",
      "importedId": "QG2MaGD9DeIOhDR5A-b5BA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151963,
      "firstName": "Nathan",
      "lastName": "McNeese",
      "middleInitial": "",
      "importedId": "Jf8WSrEP2jbIQKh3UvJ-Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151964,
      "firstName": "Ian",
      "lastName": "Chuang",
      "middleInitial": "T",
      "importedId": "yzHMj0enZJ45boTNPuc-jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151965,
      "firstName": "Elliott",
      "lastName": "Hauser",
      "middleInitial": "",
      "importedId": "K3LoNzD6UuGhSFwQVr3b7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151966,
      "firstName": "Franziska",
      "lastName": "Babel",
      "middleInitial": "",
      "importedId": "Xz2Pmfyx0AXUypC1Dp-7HA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151967,
      "firstName": "Serkan",
      "lastName": "Pekçetin",
      "middleInitial": "",
      "importedId": "vRBDhV_d7yaX1_U8fBk72Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151968,
      "firstName": "Birger",
      "lastName": "Johansson",
      "middleInitial": "",
      "importedId": "6DFB-zNZIxrEKNLI11twiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151969,
      "firstName": "Rinat",
      "lastName": "Prochii",
      "middleInitial": "",
      "importedId": "Ge1XZMxZu4A4XT5XS33IqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151970,
      "firstName": "Joseph",
      "lastName": "Nemargut",
      "middleInitial": "Paul",
      "importedId": "PI3_dNhYmT2a_XlwCQ95Fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151971,
      "firstName": "Nadine",
      "lastName": "Reißner",
      "middleInitial": "",
      "importedId": "778zWMxkox71wHZqPNBPuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151972,
      "firstName": "Kanishka",
      "lastName": "Tyagi",
      "middleInitial": "",
      "importedId": "Kz1eb2jq0_y5uODXHTZkXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151973,
      "firstName": "Alan",
      "lastName": "Sarkisian",
      "middleInitial": "",
      "importedId": "3ay0ECVmLT0gLsPdkIC17A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151974,
      "firstName": "Vladimir",
      "lastName": "Berman",
      "middleInitial": "",
      "importedId": "vLIT0KhtmuVPmguo09Fd6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151975,
      "firstName": "Olivia",
      "lastName": "Herzog",
      "middleInitial": "",
      "importedId": "qBsJqKyZW0JQUrervfA8eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151976,
      "firstName": "Kirsten",
      "lastName": "Bundgaard",
      "middleInitial": "",
      "importedId": "hk12Zg5pb_kg76MJi9sskA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151977,
      "firstName": "Adrian",
      "lastName": "Anhuaman",
      "middleInitial": "",
      "importedId": "shlPJ9zsyyh0dRLMTcfrFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151978,
      "firstName": "Henny",
      "lastName": "Admoni",
      "middleInitial": "",
      "importedId": "PvQifVLg7sDurlZBviRebQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151979,
      "firstName": "Shelly Levy",
      "lastName": "Tzedek",
      "middleInitial": "",
      "importedId": "v9Ctv3ckeLroRWMV9RXCbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151980,
      "firstName": "Jim",
      "lastName": "Torresen",
      "middleInitial": "",
      "importedId": "8or_ghQG589pKCYTssvPxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151981,
      "firstName": "Rahatul Amin",
      "lastName": "Ananto",
      "middleInitial": "",
      "importedId": "dwx8i5-J2IZkq4s7IN4s3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151982,
      "firstName": "Liubove",
      "lastName": "Orlov-Savko",
      "middleInitial": "",
      "importedId": "kpQLVKOWOCO7XBwFbY9oMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151983,
      "firstName": "Alexandre",
      "lastName": "Bernardino",
      "middleInitial": "",
      "importedId": "W-LsqaqOuLzItYngB2vMeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151984,
      "firstName": "Elisa",
      "lastName": "Prati",
      "middleInitial": "",
      "importedId": "f7zKK2NLrRW12s6TGzyqig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151985,
      "firstName": "Peter",
      "lastName": "Goshomi",
      "middleInitial": "Lenon",
      "importedId": "eb7fc5APDKMzAwMemWu_TQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151986,
      "firstName": "Nick",
      "lastName": "Walker",
      "middleInitial": "",
      "importedId": "Hm-9shkr3bt_e-kxLaVtpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151987,
      "firstName": "Peizhu",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "c8FlCViAY5DVZctXyua9XQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151988,
      "firstName": "Nicola",
      "lastName": "Döring",
      "middleInitial": "",
      "importedId": "4zvkXnv6DwHKZPYn_Dujog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151989,
      "firstName": "Alan",
      "lastName": "Lindsay",
      "middleInitial": "",
      "importedId": "seEjEZTsWuRkY0aDfNfChA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151990,
      "firstName": "Kim",
      "lastName": "Baraka",
      "middleInitial": "",
      "importedId": "nWqFFsfOITgSDbTYuuKKqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151991,
      "firstName": "Bernhard",
      "lastName": "Jung",
      "middleInitial": "",
      "importedId": "clGWtDHlNtFywPV6J9URoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151992,
      "firstName": "Alethia",
      "lastName": "Holstein",
      "middleInitial": "",
      "importedId": "adSpIMrZQcFXtaZxnrII0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151993,
      "firstName": "Siddhartha",
      "lastName": "Srinivasa",
      "middleInitial": "",
      "importedId": "56DFA19DTtQvE-hGI3NRIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151994,
      "firstName": "Andrii",
      "lastName": "Matviienko",
      "middleInitial": "",
      "importedId": "9rvoK6J9Tzu8oPE6naZiEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151995,
      "firstName": "Tsubasa",
      "lastName": "Maruyama",
      "middleInitial": "",
      "importedId": "ba3INOtXDpePHSs-NwQABA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151996,
      "firstName": "Jiayuan",
      "lastName": "Dong",
      "middleInitial": "",
      "importedId": "anPTsOCKq_LudT-Swopk2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151997,
      "firstName": "haeseon",
      "lastName": "yun",
      "middleInitial": "",
      "importedId": "grI7mMWlyTBkkJhOM3_QAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151998,
      "firstName": "Sharon",
      "lastName": "Gannot",
      "middleInitial": "",
      "importedId": "zAhG7qpyRWLJF8V115k-Gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 151999,
      "firstName": "Jan",
      "lastName": "Schmitt",
      "middleInitial": "",
      "importedId": "E_vFk3RMTW-H8N8wFsrIzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152000,
      "firstName": "Jinwei",
      "lastName": "Cao",
      "middleInitial": "",
      "importedId": "yim5guIdMVv39gXMV0CPPg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152001,
      "firstName": "Maria Elena",
      "lastName": "Giannaccini",
      "middleInitial": "",
      "importedId": "vGvd4M-szwuew2Pbfky8Bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152002,
      "firstName": "Brian",
      "lastName": "Scassellati",
      "middleInitial": "",
      "importedId": "DYtlnZlFWejNe_V9hYZcLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152003,
      "firstName": "Jodi",
      "lastName": "Forlizzi",
      "middleInitial": "",
      "importedId": "6y7SK9aQ8xlFR9ACnRo7Ow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152004,
      "firstName": "Jouh Yeong",
      "lastName": "Chew",
      "middleInitial": "",
      "importedId": "2f8xPhV4cQWLezUg51fyRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152005,
      "firstName": "Maisey",
      "lastName": "Toczek",
      "middleInitial": "",
      "importedId": "GLeOcFPnb8DS65OxOI-vfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152006,
      "firstName": "Gonçalo",
      "lastName": "Junqueira",
      "middleInitial": "",
      "importedId": "zgrH7TLUHAFjYx90Ocuvsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152007,
      "firstName": "Heiko",
      "lastName": "Renz",
      "middleInitial": "",
      "importedId": "HDBja7jgZULdNa-4eHBSiA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152008,
      "firstName": "Ho Chit",
      "lastName": "Siu",
      "middleInitial": "",
      "importedId": "NLfaZZx3DKxiIXA4738Vug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152009,
      "firstName": "Masayuki",
      "lastName": "Kanbara",
      "middleInitial": "",
      "importedId": "90hAt2_N-EAdmm7ZhIgPeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152010,
      "firstName": "Subbarao",
      "lastName": "Kambhampati",
      "middleInitial": "",
      "importedId": "eX_6vqhwVEralFpUs6nzJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152011,
      "firstName": "Doenja",
      "lastName": "Oogjes",
      "middleInitial": "",
      "importedId": "xTkDuXgvXwc3joODle6QXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152012,
      "firstName": "Alexander",
      "lastName": "Lew",
      "middleInitial": "",
      "importedId": "qLnMrmKi-9XRlic2hyEUtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152013,
      "firstName": "Arthur",
      "lastName": "Melo Cruz",
      "middleInitial": "",
      "importedId": "6wiZrJYIp6e-CiHi5W_FcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152014,
      "firstName": "Xiaoyue",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "BTXYv8mfu599rJeUz-20Ag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152015,
      "firstName": "Zhiqin",
      "lastName": "Qian",
      "middleInitial": "",
      "importedId": "hfIC00MJz2-JN-CaUKuh9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152016,
      "firstName": "Federica",
      "lastName": "Zucchi",
      "middleInitial": "",
      "importedId": "jRKZu5E0ZIRzIgAMzr6nBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152017,
      "firstName": "Giulia",
      "lastName": "Buodo",
      "middleInitial": "",
      "importedId": "kEGbI35yQPiM7tMOQPumAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152018,
      "firstName": "Daniel",
      "lastName": "Maccaline",
      "middleInitial": "",
      "importedId": "gu5NEhYDql3YwHi6IdYo8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152019,
      "firstName": "Alex",
      "lastName": "Chow",
      "middleInitial": "",
      "importedId": "ukgwHnrC3f32D_L3EAQkmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152020,
      "firstName": "Luke",
      "lastName": "Guerdan",
      "middleInitial": "",
      "importedId": "MRduhrn8uDHHxpm7DBb1Mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152021,
      "firstName": "Simone",
      "lastName": "Borghi",
      "middleInitial": "",
      "importedId": "M761xePBNtL7SHTiEOI6mw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152022,
      "firstName": "Yunqi",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "SeznA57lWEfMRdkMkZV0-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152023,
      "firstName": "Sebastian",
      "lastName": "Caballa Barrientos",
      "middleInitial": "Rony",
      "importedId": "o_3vzSdlxOMI_sUGPy0f6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152024,
      "firstName": "Yusam",
      "lastName": "Hsu",
      "middleInitial": "",
      "importedId": "dJv8qOCHDzlZP7XA9aBMZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152025,
      "firstName": "Kurima",
      "lastName": "Sakai",
      "middleInitial": "",
      "importedId": "4LgHlYPLkTVdP0o_xVD2gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152026,
      "firstName": "Joo-Hyun",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "c69MN6ez0Ep_4Cyl_zq8BA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152027,
      "firstName": "Joon Hyun",
      "lastName": "Shim",
      "middleInitial": "",
      "importedId": "EP8WtaXIZR8lXtBImgt7vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152028,
      "firstName": "Nicholas",
      "lastName": "Georgiou",
      "middleInitial": "C.",
      "importedId": "ApgFb0YAD7XzpAGmnuuZWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152029,
      "firstName": "Veerle",
      "lastName": "Hobbelink",
      "middleInitial": "",
      "importedId": "CqK9zdllLh8eED6JHEoflQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152030,
      "firstName": "Jessica",
      "lastName": "Cauchard",
      "middleInitial": "R.",
      "importedId": "beYeovjkMGSBXCwSLbFomA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152031,
      "firstName": "Bruno",
      "lastName": "Laeng",
      "middleInitial": "",
      "importedId": "QygfDIkUiM1eub6URaCm2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152032,
      "firstName": "Rea",
      "lastName": "Francesco",
      "middleInitial": "",
      "importedId": "SLZVy6sN8mhDO3bRhehaPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152033,
      "firstName": "Harish",
      "lastName": "Bezawada",
      "middleInitial": "",
      "importedId": "lqAvYNUJGzXENS0aV6dn9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152034,
      "firstName": "Ruisi",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "8FmJa3GAGhstkYQMrcDa8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152035,
      "firstName": "Michelle",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "ncOyL2k9VUWZjFI_vbcblg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152036,
      "firstName": "Allison",
      "lastName": "Anderson",
      "middleInitial": "",
      "importedId": "KwUzf-rY-b51FaaRY3V_8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152037,
      "firstName": "Diana",
      "lastName": "Lindblom",
      "middleInitial": "Saplacan",
      "importedId": "nXuY1UZ7RGHzwkrEWsmHNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152038,
      "firstName": "Marta",
      "lastName": "Romeo",
      "middleInitial": "",
      "importedId": "-Jb-8MbaVukq2Adz-ipa3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152039,
      "firstName": "Lynn",
      "lastName": "Masuda",
      "middleInitial": "",
      "importedId": "p2wWY9QKJiMadXDmJ3JThw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152040,
      "firstName": "Han Ling",
      "lastName": "Tung",
      "middleInitial": "",
      "importedId": "stwhNvfZeYa0NSb368F-4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152041,
      "firstName": "Vivek",
      "lastName": "Mallampati",
      "middleInitial": "",
      "importedId": "VseS9CoCuTelXXqwI8Npjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152042,
      "firstName": "Jordi",
      "lastName": "Albo Canals",
      "middleInitial": "",
      "importedId": "eQCgVKofWCkWvR5Mem5Qjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152043,
      "firstName": "Sanskar",
      "lastName": "Shah",
      "middleInitial": "",
      "importedId": "4QKY9M5MCGgAOk8VaeSmHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152044,
      "firstName": "Kasper",
      "lastName": "Hald",
      "middleInitial": "",
      "importedId": "8nWRZHIYrVsKtp6239ZO-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152045,
      "firstName": "Stefanos",
      "lastName": "Nikolaidis",
      "middleInitial": "",
      "importedId": "3xIQxws42Gu8S61quc82xQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152046,
      "firstName": "Roman",
      "lastName": "Heger",
      "middleInitial": "",
      "importedId": "AqLFdtQ9GBgoK4GHMYMqAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152047,
      "firstName": "Joana",
      "lastName": "Brito",
      "middleInitial": "",
      "importedId": "Lqfc6EMGBkVDTs4oSmMz9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152048,
      "firstName": "Lynne",
      "lastName": "Baillie",
      "middleInitial": "",
      "importedId": "UImiWKoGhgRJ-D9iKGIMMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152049,
      "firstName": "Heng",
      "lastName": "ZHANG",
      "middleInitial": "",
      "importedId": "WMHmn4pv7z-PiZWLjUNVSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152050,
      "firstName": "Rui",
      "lastName": "Xavier",
      "middleInitial": "",
      "importedId": "KPiv2AXkkFZe9C8E9rKdoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152051,
      "firstName": "Simone",
      "lastName": "Arreghini",
      "middleInitial": "",
      "importedId": "Hy6G1PAPM62OsRLJI56DXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152052,
      "firstName": "Rafael",
      "lastName": "Guerra-Silva",
      "middleInitial": "",
      "importedId": "O6jFd55ArT6ke6QCCm5w5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152053,
      "firstName": "Mohammed",
      "lastName": "Ahmed",
      "middleInitial": "S",
      "importedId": "70B7BQnIKMvRhnj-DWp6cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152054,
      "firstName": "Pieter",
      "lastName": "Wolfert",
      "middleInitial": "",
      "importedId": "oyetq0wjt6NJIziR-kMTfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152055,
      "firstName": "Fares",
      "lastName": "Abawi",
      "middleInitial": "",
      "importedId": "EFc8TWGzoN9yYBjlwt0KWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152056,
      "firstName": "Anita",
      "lastName": "Vrins",
      "middleInitial": "",
      "importedId": "Ainr5HDNvr1yxEpAAITYGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152057,
      "firstName": "Jenny",
      "lastName": "Huch",
      "middleInitial": "",
      "importedId": "3lsOupPLnzuEvCNdTIvhug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152058,
      "firstName": "Davy",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "Ck9pItb9kYCBidkWS-WuJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152059,
      "firstName": "Matthias",
      "lastName": "Kraus",
      "middleInitial": "",
      "importedId": "45PYVclbqw6UfqFokCG8VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152060,
      "firstName": "Midori",
      "lastName": "Ban",
      "middleInitial": "",
      "importedId": "4kHbYTowgHMAJZBZCTDalg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152061,
      "firstName": "Yiwei",
      "lastName": "Lyu",
      "middleInitial": "",
      "importedId": "YELxyK2EvBWi6sy88o67Ow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152062,
      "firstName": "Tomoya",
      "lastName": "Sasao",
      "middleInitial": "",
      "importedId": "EMqYUQjIYHDYMLPAvQi6Gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152063,
      "firstName": "Felix",
      "lastName": "Wohlgemuth",
      "middleInitial": "",
      "importedId": "A3nOo_vGSsA99vj7vhB5Sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152064,
      "firstName": "Deborah",
      "lastName": "Forster",
      "middleInitial": "",
      "importedId": "UyGtE-LLYvv20LJHEf7Knw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152065,
      "firstName": "Jack",
      "lastName": "Hatcher",
      "middleInitial": "",
      "importedId": "3TqthvkxnNisFbHmknj9lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152066,
      "firstName": "Avantika",
      "lastName": "Dev",
      "middleInitial": "",
      "importedId": "922KDqOmZXWXQTFcOA9xNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152067,
      "firstName": "Mustafa",
      "lastName": "Kücükkocak",
      "middleInitial": "",
      "importedId": "IkdtOyHRUaxbp8PV1lLnzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152068,
      "firstName": "Henry",
      "lastName": "Evans",
      "middleInitial": "",
      "importedId": "mhFu6_ITScf1JaKvaoDgMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152069,
      "firstName": "Elmira",
      "lastName": "Yadollahi",
      "middleInitial": "",
      "importedId": "bvHV4jLGinPR4WcopA4xNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152070,
      "firstName": "Kota",
      "lastName": "Nieda",
      "middleInitial": "",
      "importedId": "MAOeQnT6URODjPJOUVsC7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152071,
      "firstName": "Elizabeth",
      "lastName": "Carter",
      "middleInitial": "Jeanne",
      "importedId": "gw_Ja8irh0vb0xTxxdxT4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152072,
      "firstName": "Oliver",
      "lastName": "Lemon",
      "middleInitial": "",
      "importedId": "ziiBRfYRm93wktienxv5lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152073,
      "firstName": "Magdalena",
      "lastName": "Yordanova",
      "middleInitial": "",
      "importedId": "_rUaKXOvkr5MlI5rKkC8hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152074,
      "firstName": "Sofia",
      "lastName": "Ferreira",
      "middleInitial": "",
      "importedId": "sQRqGgD24cQobnzUZ8_FLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152075,
      "firstName": "Paul",
      "lastName": "Bremner",
      "middleInitial": "A",
      "importedId": "ERzMLhEs0S21JfEB8emtiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152076,
      "firstName": "Kim",
      "lastName": "Baraka",
      "middleInitial": "",
      "importedId": "V6uC4kMpZglfrn1ibTgoyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152077,
      "firstName": "Naoki",
      "lastName": "Shirakura",
      "middleInitial": "",
      "importedId": "IM_0trNFIR83itgW9mhQJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152078,
      "firstName": "Helen",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "SLaHXxmPflFJOCjqDVSESQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152079,
      "firstName": "Ilaria",
      "lastName": "Torre",
      "middleInitial": "",
      "importedId": "TzAqXK66Ih2Rqza77qU-jA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152080,
      "firstName": "Shayla",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "Kcx9SSnqfMHbY7F7QrOb9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152081,
      "firstName": "Patric",
      "lastName": "Bach",
      "middleInitial": "",
      "importedId": "ncXBEGjiF--HE9NLEMWe2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152082,
      "firstName": "Yuichiro",
      "lastName": "Fujimoto",
      "middleInitial": "",
      "importedId": "VUrJyVPWtb5lq8C-xWt1Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152083,
      "firstName": "Vincenzo Maria",
      "lastName": "Vitale",
      "middleInitial": "",
      "importedId": "rc6oAmDKsKAjl_5-B1GU_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152084,
      "firstName": "Cristian",
      "lastName": "Barrue",
      "middleInitial": "",
      "importedId": "BrCc6EUdD2bxzuw_yrmkVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152085,
      "firstName": "Maike",
      "lastName": "Paetzel-Prüsmann",
      "middleInitial": "",
      "importedId": "wIjDuvCYKhjLUWiEHB9PUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152086,
      "firstName": "Reuth",
      "lastName": "Mirsky",
      "middleInitial": "",
      "importedId": "EyNERVUIwc4lxCWDFjLKzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152087,
      "firstName": "Lin",
      "lastName": "Guan",
      "middleInitial": "",
      "importedId": "ROYrJBO4IEDkQoNw2F-VzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152088,
      "firstName": "Marco",
      "lastName": "Stranisci",
      "middleInitial": "",
      "importedId": "ywk-mjBfjy5ZmPS6k7hiRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152089,
      "firstName": "Jonathan",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "vfH8u3O0tIX6mqgwoUI93Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152090,
      "firstName": "Christian",
      "lastName": "Becker-Asano",
      "middleInitial": "",
      "importedId": "Y5YaK7VkmYKWnprT1fQNzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152091,
      "firstName": "Darius",
      "lastName": "Rose",
      "middleInitial": "",
      "importedId": "e56hAmM5Y0n5AMYNUyLa3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152092,
      "firstName": "Yinsu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "Pg65ySxwiDTOXTJnjH7qBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152093,
      "firstName": "Qingwei",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Lt6TaEzUIP9mfbOw2JVAMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152094,
      "firstName": "Tetsuya",
      "lastName": "Ogata",
      "middleInitial": "",
      "importedId": "vyLX8bdM69nfXWfqHKbEzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152095,
      "firstName": "Gerard",
      "lastName": "Bailly",
      "middleInitial": "",
      "importedId": "6U1ge2vGuhgHSrVZ6q-rlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152096,
      "firstName": "Heqiu",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "egLk2fw229x37GV1ivsrXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152097,
      "firstName": "Mayli Alejandra",
      "lastName": "Tafur Gutierrez",
      "middleInitial": "",
      "importedId": "L4-NRharFXbTpni7cQlEHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152098,
      "firstName": "Dominic",
      "lastName": "Létourneau",
      "middleInitial": "",
      "importedId": "c_K-woPyZ6-VnnibdNiO9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152099,
      "firstName": "Mitch",
      "lastName": "Pryor",
      "middleInitial": "",
      "importedId": "-v0x5lm3VqpbOndjaK4Qtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152100,
      "firstName": "Mohammed",
      "lastName": "Diab",
      "middleInitial": "",
      "importedId": "kjCq1uVOkPVcwvC1cGvEWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152101,
      "firstName": "Dante",
      "lastName": "Arroyo",
      "middleInitial": "",
      "importedId": "EusS-kRqlmjGnO1r3qRGjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152102,
      "firstName": "Qi",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "cfIPKKRHT76jlpAsjJ-t4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152103,
      "firstName": "Christopher Yee",
      "lastName": "Wong",
      "middleInitial": "",
      "importedId": "-8f1yHrk-RHj3MW_Yb_8uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152104,
      "firstName": "Fiona A.",
      "lastName": "Neylon",
      "middleInitial": "",
      "importedId": "u_nJ068lekNKGTdK9IvqPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152105,
      "firstName": "Nikhil",
      "lastName": "Churamani",
      "middleInitial": "",
      "importedId": "6KuBiaG9sqIF2gzqbiKpzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152106,
      "firstName": "Philipp",
      "lastName": "Allgeuer",
      "middleInitial": "",
      "importedId": "LFys13N9r_Xep8tk1Stxow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152107,
      "firstName": "Amy",
      "lastName": "LaViers",
      "middleInitial": "",
      "importedId": "zVHdsm4EVreDwBE_4HwqoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152108,
      "firstName": "Feng",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "Jby-aDyHb3ydUXizxDMlgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152109,
      "firstName": "KEJIN",
      "lastName": "YU",
      "middleInitial": "",
      "importedId": "GGnQS9Lu-mlVWCLcshYaxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152110,
      "firstName": "Deborah",
      "lastName": "Szapiro",
      "middleInitial": "",
      "importedId": "ImpVXvrMNtlUKvYuJqXISQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152111,
      "firstName": "Ningning",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "bwjNZX1eON2Kb0C3AuB8yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152112,
      "firstName": "Alberto",
      "lastName": "Sanfeliu",
      "middleInitial": "",
      "importedId": "mPMtefHlP0AIfLliTd6SWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152113,
      "firstName": "Kanae",
      "lastName": "Kochigami",
      "middleInitial": "",
      "importedId": "sFI-3DcDrTv12IOixWtL7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152114,
      "firstName": "Abena",
      "lastName": "Boadi-Agyemang",
      "middleInitial": "",
      "importedId": "S_Ho8VrarV7xcRpcwRqZ_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152115,
      "firstName": "Cristian",
      "lastName": "Bogdan",
      "middleInitial": "",
      "importedId": "L8_daidd2AExtN_AfjbZeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152116,
      "firstName": "Hirokazu",
      "lastName": "Kato",
      "middleInitial": "",
      "importedId": "T4mOyqeW_S5cKIC0Hm8Y2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152117,
      "firstName": "Sharina",
      "lastName": "Calsin",
      "middleInitial": "",
      "importedId": "IGw9Ev9nEiPRf6Ai-DpPLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152118,
      "firstName": "Eurico",
      "lastName": "Pedrosa",
      "middleInitial": "",
      "importedId": "dN4FY2ivt4JjpQOVgxScNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152119,
      "firstName": "Anders",
      "lastName": "Sørensen",
      "middleInitial": "Stengaard",
      "importedId": "I-r55VhE309K2QIin6QAZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152120,
      "firstName": "Jan",
      "lastName": "de Wit",
      "middleInitial": "",
      "importedId": "GwBdWvWFU8qwlsWSal0KHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152121,
      "firstName": "Maia",
      "lastName": "Stiber",
      "middleInitial": "",
      "importedId": "XFUIzCTwfv-JeRcUan4qGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152122,
      "firstName": "Philipp",
      "lastName": "Kranz",
      "middleInitial": "",
      "importedId": "pWiiRh1moscMJ7Spp4CkgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152123,
      "firstName": "Lux",
      "lastName": "Miranda",
      "middleInitial": "",
      "importedId": "DMTNMf-9RtAbZ9pk0iT0Og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152124,
      "firstName": "Antonio",
      "lastName": "De Almeida Correia",
      "middleInitial": "",
      "importedId": "5I-vfZ45UnqUKt4GGZ903Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152125,
      "firstName": "Xuezhu",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "nZoV8K1VkiIq1KRsHnUCMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152126,
      "firstName": "Sangmin",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "eMudpCE9XLP0XjNfN21QNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152127,
      "firstName": "Yuichiro",
      "lastName": "Yoshikawa",
      "middleInitial": "",
      "importedId": "MbLcxokhZTEOpdVTPT1-aw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152128,
      "firstName": "Teresa",
      "lastName": "Almeida",
      "middleInitial": "",
      "importedId": "Vq8cM_4mWpu2WHp0ztJoxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152129,
      "firstName": "Cindy",
      "lastName": "Grimm",
      "middleInitial": "",
      "importedId": "Yuy0F26HpPYGsdjb7pHfAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152130,
      "firstName": "Samantha",
      "lastName": "Stedtler",
      "middleInitial": "",
      "importedId": "8AkMryI1r8AqoabHnN4EOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152131,
      "firstName": "Kenta",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "YVe07gTtckWbFHG45jxkQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152132,
      "firstName": "Nicholas",
      "lastName": "Callander",
      "middleInitial": "",
      "importedId": "fpT7GfARNt2e7BORD3LxtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152133,
      "firstName": "Fabio",
      "lastName": "Rizzoglio",
      "middleInitial": "",
      "importedId": "R28dWdKdr0x-oZcI2-XuFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152134,
      "firstName": "Maciej",
      "lastName": "Wozniak",
      "middleInitial": "",
      "importedId": "cabVGS1nP8E7ipLIWtcAWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152135,
      "firstName": "Favour",
      "lastName": "Aderinto",
      "middleInitial": "",
      "importedId": "EmhKMZOeiKF1DoMxm7cPZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152136,
      "firstName": "Yidi",
      "lastName": "Huang",
      "middleInitial": "",
      "importedId": "QeFqH6_3yIj9p7Wo82f2ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152137,
      "firstName": "Takuto",
      "lastName": "Akiyoshi",
      "middleInitial": "",
      "importedId": "z9XqtvJutXJSLXDsqMqZ0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152138,
      "firstName": "Kotaro",
      "lastName": "Funakoshi",
      "middleInitial": "",
      "importedId": "FYskODJLV8VVgXBj41hHOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152139,
      "firstName": "Lixiao",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "bnYjEQM7705hbuCSjw4ZPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152140,
      "firstName": "Gisela",
      "lastName": "Reyes-Cruz",
      "middleInitial": "",
      "importedId": "auooOhBtWJ8iGFKv-rX65A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152141,
      "firstName": "Xiaoxuan",
      "lastName": "Hei",
      "middleInitial": "",
      "importedId": "rH5dVZ6FMSqSg2rKpPqiRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152142,
      "firstName": "Wafa",
      "lastName": "Johal",
      "middleInitial": "",
      "importedId": "Z0weddL5B8bX6jJKaH42Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152143,
      "firstName": "Aiden",
      "lastName": "Mäder",
      "middleInitial": "Danny",
      "importedId": "7gEJUHqV08ajtYPKwU_4hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152144,
      "firstName": "Yu-Wen",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "-tp6ifzaU6wKq0h4dCDMcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152145,
      "firstName": "Valentina",
      "lastName": "Fantasia",
      "middleInitial": "",
      "importedId": "L5Wdif1XtFG6fo66oqNKRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152146,
      "firstName": "Junya",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "importedId": "ESgfdznlp5SptHUoszShRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152147,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "middleInitial": "",
      "importedId": "oLvfjZm0xHwl7reMcJOYcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152148,
      "firstName": "Aadjan",
      "lastName": "van der Helm",
      "middleInitial": "",
      "importedId": "PDLJl3aNDkNRZiqml3Qhog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152149,
      "firstName": "Norina",
      "lastName": "Gasteiger",
      "middleInitial": "",
      "importedId": "6y0OmSzH3eXvqSbEpF33nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152150,
      "firstName": "Houston",
      "lastName": "Claure",
      "middleInitial": "",
      "importedId": "c3Hf4uidTGHm3KiJjNwj6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152151,
      "firstName": "Charles",
      "lastName": "Kemp",
      "middleInitial": "",
      "importedId": "nH238pakDz1f_2-MBkFlWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152152,
      "firstName": "Ameer",
      "lastName": "Helmi",
      "middleInitial": "",
      "importedId": "J2cnNyzjETyuBA6IY8K83A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152153,
      "firstName": "Adi",
      "lastName": "Mopidevi",
      "middleInitial": "",
      "importedId": "WAzCVPps3t8SepEhS9I2sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152154,
      "firstName": "Jacob",
      "lastName": "Macdonald",
      "middleInitial": "P",
      "importedId": "dVbF8uD_g6GN45OI4mJE5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152155,
      "firstName": "Sydney",
      "lastName": "Thompson",
      "middleInitial": "",
      "importedId": "GtEOFAhSqWZzKPFcLRgUYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152156,
      "firstName": "Caterina",
      "lastName": "Ceccato",
      "middleInitial": "",
      "importedId": "2B2y9r5w9wMs-Jt5kG_cNw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152157,
      "firstName": "Arthi",
      "lastName": "Haripriyan",
      "middleInitial": "",
      "importedId": "EopgcueRhTAVcM5FHwk8Og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152158,
      "firstName": "Elif Ozden",
      "lastName": "Yenigun",
      "middleInitial": "",
      "importedId": "r_SFcmxx3X_CyfOV4v6bkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152159,
      "firstName": "Teeratas",
      "lastName": "Asavareongchai",
      "middleInitial": "",
      "importedId": "mNTlXxukiQbmw0UjSrkD4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152160,
      "firstName": "Berkecan",
      "lastName": "Koçyiğit",
      "middleInitial": "",
      "importedId": "hvLNRLLsq7dgy8PKsw7GtA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152161,
      "firstName": "Iori",
      "lastName": "Mizutani",
      "middleInitial": "",
      "importedId": "zjDHDmJCZxI_oble-b1TcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152162,
      "firstName": "Ronald",
      "lastName": "Cumbal",
      "middleInitial": "",
      "importedId": "FsshF1Q2HBH2gGioPgRlUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152163,
      "firstName": "Lorenzo",
      "lastName": "D'Errico",
      "middleInitial": "",
      "importedId": "DtWELOnixWHVwHwYQMtkvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152164,
      "firstName": "Haipeng",
      "lastName": "Mi",
      "middleInitial": "",
      "importedId": "BDw8B7BArYd3guR0WQqY0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152165,
      "firstName": "Shuqi",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "030NrJ1aNTaefoN4PM0b_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152166,
      "firstName": "Martin",
      "lastName": "Heckel",
      "middleInitial": "",
      "importedId": "nSWp3CwsgB_2XCM-CqRbFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152167,
      "firstName": "Aaron",
      "lastName": "Edsinger",
      "middleInitial": "",
      "importedId": "4iiUkxgFJNG91YxAcMcyLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152168,
      "firstName": "Katharina",
      "lastName": "Brunnmayr",
      "middleInitial": "",
      "importedId": "dBj7y_tFldEcvtQXcgcsXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152169,
      "firstName": "Maggie",
      "lastName": "Collier",
      "middleInitial": "",
      "importedId": "BEboaZ81HkOzw7SEJTfelQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152170,
      "firstName": "Chris",
      "lastName": "Crawford",
      "middleInitial": "S",
      "importedId": "drCZEWprxLYE8b1MuGaVnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152171,
      "firstName": "Kevin",
      "lastName": "Fröhlich",
      "middleInitial": "",
      "importedId": "0OBQy9HvLbQs5U16Xpnr3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152172,
      "firstName": "Samuel",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "4lvfV8jLOCKyYXuNw-hoKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152173,
      "firstName": "Maartje",
      "lastName": "de Graaf",
      "middleInitial": "M.A.",
      "importedId": "0iG5Ek9_TsCmTlATNDPkvg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152174,
      "firstName": "Elizabeth",
      "lastName": "Broadbent",
      "middleInitial": "",
      "importedId": "zwZnkXzCDUUc6FQ_vGw7IA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152175,
      "firstName": "Ayaka",
      "lastName": "Fujii",
      "middleInitial": "",
      "importedId": "mEXaPCMcPq-E-Qc3bReXQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152176,
      "firstName": "Nienke",
      "lastName": "Schrage-Prent",
      "middleInitial": "",
      "importedId": "D-MJcI38rqRdOvgLGOE_MA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152177,
      "firstName": "Olov",
      "lastName": "Engwall",
      "middleInitial": "",
      "importedId": "9XTsqosEKp699rG6cdkhCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152178,
      "firstName": "Cora",
      "lastName": "Rhodes",
      "middleInitial": "",
      "importedId": "OTuXfrRYxeROyAsah3fYdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152179,
      "firstName": "Zihui",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "8hRnDRZwUDNdQhb0I8QIQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152180,
      "firstName": "Thomas",
      "lastName": "Doppia",
      "middleInitial": "",
      "importedId": "BHJVrHBqtHVO4RfNDnzl7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152181,
      "firstName": "Gregory",
      "lastName": "Gremillion",
      "middleInitial": "M",
      "importedId": "Gkguuwj4w0LZgttg3CfTJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152182,
      "firstName": "Sachiko",
      "lastName": "Matsumoto",
      "middleInitial": "",
      "importedId": "t5hjdDoKIgnZWRRCBNspHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152183,
      "firstName": "Ioannis",
      "lastName": "Parodis",
      "middleInitial": "",
      "importedId": "In6JOs_6-9Qv25Sj2ISjmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152184,
      "firstName": "Md Zia",
      "lastName": "Uddin",
      "middleInitial": "",
      "importedId": "sVDT4Y-URTkZ7migi7zYcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152185,
      "firstName": "Tammer",
      "lastName": "Barkouki",
      "middleInitial": "",
      "importedId": "vfSP__ANEo7Pjgbazgn8og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152186,
      "firstName": "Séverin",
      "lastName": "Lemaignan",
      "middleInitial": "",
      "importedId": "1zM86dDhpUim-9CHSxS7mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152187,
      "firstName": "Chung Hyuk",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "4NFgmP0SlpLpE4Su0wL0eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152188,
      "firstName": "Adina",
      "lastName": "Panchea",
      "middleInitial": "",
      "importedId": "Jsz7j5VG7cKOrMm2OtEHKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152189,
      "firstName": "Yue",
      "lastName": "Gu",
      "middleInitial": "",
      "importedId": "FzJYLw6bNVG3L2l-9QJjyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152190,
      "firstName": "Adriana",
      "lastName": "Tapus",
      "middleInitial": "",
      "importedId": "Px2GIoSXb8IHU8nOIyry1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152191,
      "firstName": "Yifan",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "Iry6kMWQtBBWVKpncGrzyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152192,
      "firstName": "Olivia",
      "lastName": "Clark",
      "middleInitial": "C",
      "importedId": "yNzYDherKWZB7RJcBA6PjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152193,
      "firstName": "Thomas",
      "lastName": "Manzini",
      "middleInitial": "",
      "importedId": "xDlRlTCTf-JCE5BQUOk53Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152194,
      "firstName": "Simon",
      "lastName": "Mayer",
      "middleInitial": "",
      "importedId": "lQlyjM0cV0yiAZV5GU0-CA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152195,
      "firstName": "Akane",
      "lastName": "Kikuchi",
      "middleInitial": "",
      "importedId": "yMoPfN6qntAPdHqnE5_0Hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152196,
      "firstName": "Samantha",
      "lastName": "Miles",
      "middleInitial": "",
      "importedId": "0kVDopxNw_NQSMU1Y7wj3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152197,
      "firstName": "Julian",
      "lastName": "Mehu",
      "middleInitial": "",
      "importedId": "GfWt5tZL3zCLrP51PWz_3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152198,
      "firstName": "Stefan",
      "lastName": "Schiffer",
      "middleInitial": "",
      "importedId": "YEiOlkruYfGL1SUV01eR1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152199,
      "firstName": "Austin",
      "lastName": "Narcomey",
      "middleInitial": "",
      "importedId": "_p0vOgraw-u542gqzlZXww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152200,
      "firstName": "Denise Y.",
      "lastName": "Geiskkovitch",
      "middleInitial": "",
      "importedId": "d96PhcSzJYXcMYj2VoTwWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152201,
      "firstName": "Marco",
      "lastName": "Cognetti",
      "middleInitial": "",
      "importedId": "-MkBFMgg4wodgUn6zK6LEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152202,
      "firstName": "Hong",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "2qdHhNbvNGv0VdGD_j1TJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152203,
      "firstName": "Jens",
      "lastName": "Gerken",
      "middleInitial": "",
      "importedId": "CKaEbk3FSaip4T5uu9uQyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152204,
      "firstName": "Xiajie",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "wSLVusge0GpZiOMsbsR3YA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152205,
      "firstName": "Aida",
      "lastName": "Ribera",
      "middleInitial": "",
      "importedId": "TpHeBCD_5_yPBbh4cc9X8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152206,
      "firstName": "Tushar",
      "lastName": "Chugh",
      "middleInitial": "",
      "importedId": "8xXWjhWllY_LxQ_VOlkfcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152207,
      "firstName": "Alessandro",
      "lastName": "Giusti",
      "middleInitial": "",
      "importedId": "p8vEjhMEkYqeMZzT4rwzwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152208,
      "firstName": "Clara",
      "lastName": "Lachemaier",
      "middleInitial": "",
      "importedId": "75t12biTekF4B3OZi-oxPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152209,
      "firstName": "Xilai",
      "lastName": "Dai",
      "middleInitial": "",
      "importedId": "aEtglwJLJ6ZEo01m1W2_6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152210,
      "firstName": "Xuesu",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "vKdDXgZIDJggsmg2nh_mTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152211,
      "firstName": "Tom",
      "lastName": "Ziemke",
      "middleInitial": "",
      "importedId": "0RK4Zm5VnG2njWEg39HNNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152212,
      "firstName": "Roya",
      "lastName": "Salehzadeh",
      "middleInitial": "",
      "importedId": "nsCImneTPo8ma3QzdN9ZDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152213,
      "firstName": "Anara",
      "lastName": "Sandygulova",
      "middleInitial": "",
      "importedId": "OAlJWuHrRCw8UT2LgXs1_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152214,
      "firstName": "Michael",
      "lastName": "Feld",
      "middleInitial": "",
      "importedId": "nxEMdJRc1rie0_Xm7vSq3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152215,
      "firstName": "Manuel",
      "lastName": "Giuliani",
      "middleInitial": "",
      "importedId": "wMPkvoxXJ14UqO704ObRCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152216,
      "firstName": "Elin Anna",
      "lastName": "Topp",
      "middleInitial": "",
      "importedId": "ND9YFfAg5Xn_MCz16jwYXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152217,
      "firstName": "Andrea",
      "lastName": "Ruo",
      "middleInitial": "",
      "importedId": "UBuclOI_xBhyCtPczc_L8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152218,
      "firstName": "Takahisa",
      "lastName": "Uchida",
      "middleInitial": "",
      "importedId": "qMHhd6fWi40MZ8KS8X7psQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152219,
      "firstName": "Lejla",
      "lastName": "Nukovic",
      "middleInitial": "",
      "importedId": "RrgCl5ZviCsYH6bVlj5prw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152220,
      "firstName": "Vsevolod",
      "lastName": "Peysakhovich",
      "middleInitial": "",
      "importedId": "jraG3SWqllSWjuLdG-RKBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152221,
      "firstName": "Tuğçe Nur",
      "lastName": "Pekçetin",
      "middleInitial": "",
      "importedId": "D9WRLkMDKa0qGS-34KnkzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152222,
      "firstName": "Sebastian",
      "lastName": "Negrete-Alamillo",
      "middleInitial": "",
      "importedId": "5OEcNzA-vpo6oxoXgq7dmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152223,
      "firstName": "Amy",
      "lastName": "Eguchi",
      "middleInitial": "",
      "importedId": "GFzKeElWgX-AO4e_sJLNmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152224,
      "firstName": "Daniel",
      "lastName": "Sidobre",
      "middleInitial": "",
      "importedId": "rM596-sOgyQLIu0XzjDpSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152225,
      "firstName": "Mhd Aghyad",
      "lastName": "Jamal Eddin",
      "middleInitial": "",
      "importedId": "SCm8HYYT92Cegza0KSNRsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152226,
      "firstName": "Şeyda",
      "lastName": "Evsen",
      "middleInitial": "",
      "importedId": "VF2aHOe47oE63u5Fc9MIMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152227,
      "firstName": "Bernardo",
      "lastName": "Marques",
      "middleInitial": "",
      "importedId": "NsTQ_P818lArhhYcKzG6yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152228,
      "firstName": "Melina",
      "lastName": "Daniilidis",
      "middleInitial": "",
      "importedId": "5AHb2fmqvEju2JIR25T6sg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152229,
      "firstName": "Carlos",
      "lastName": "Granados Ybarra",
      "middleInitial": "Alberto",
      "importedId": "7DYIJAJ888zuNrbiJSyNIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152230,
      "firstName": "Zhiling",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "rRYmfsPL-kpZ4lQRbOl6Yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152231,
      "firstName": "Zubin",
      "lastName": "Assadian",
      "middleInitial": "",
      "importedId": "3Wc91ucL0JKRdDIeg9g1LA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152232,
      "firstName": "Fuka",
      "lastName": "Ikeda",
      "middleInitial": "",
      "importedId": "yKMs2NMrM6Hg25mg59Ci4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152233,
      "firstName": "Neska",
      "lastName": "ElHaouij",
      "middleInitial": "",
      "importedId": "jkM1Npe2947NFxWSlI2Lbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152234,
      "firstName": "Agnieszka",
      "lastName": "Wykowska",
      "middleInitial": "",
      "importedId": "dxUWflyuqWGukswbIYMCaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152235,
      "firstName": "Zining",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "D_yuGu82Kl6rQPoQlPxRxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152236,
      "firstName": "Daniel",
      "lastName": "Nakhimovich",
      "middleInitial": "Saul",
      "importedId": "Fgs32IhAM2aHAziwbVM_gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152237,
      "firstName": "Bahar",
      "lastName": "Irfan",
      "middleInitial": "",
      "importedId": "sifKZ_teXhLsiiddFpqilw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152238,
      "firstName": "Kate",
      "lastName": "Candon",
      "middleInitial": "",
      "importedId": "TUhfsOsToaTVg7pJCSyDXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152239,
      "firstName": "Lorenzo",
      "lastName": "Ferrini",
      "middleInitial": "",
      "importedId": "s8AZA78t3giP6NstaciDGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152240,
      "firstName": "Ayşe",
      "lastName": "Doğan",
      "middleInitial": "",
      "importedId": "cilg8PvJdh2zKYmNaITyPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152241,
      "firstName": "Miguel",
      "lastName": "Altamirano Cabrera",
      "middleInitial": "",
      "importedId": "zQ4-dMUvWS-8atB7EAEXYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152242,
      "firstName": "Concepción",
      "lastName": "Boqué",
      "middleInitial": "",
      "importedId": "C8yJM237Gq3AKJERNPyzOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152243,
      "firstName": "Lena",
      "lastName": "Winhold",
      "middleInitial": "",
      "importedId": "FOu1rOO7hkFNeoUDHfKX5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152244,
      "firstName": "António",
      "lastName": "Teixeira",
      "middleInitial": "",
      "importedId": "OdnJvr0Q_079q6cm3mBwQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152245,
      "firstName": "Rebecca",
      "lastName": "Stower",
      "middleInitial": "",
      "importedId": "WQ2yk0jMob9yofrB_WNgkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152246,
      "firstName": "Tiffany",
      "lastName": "Horter",
      "middleInitial": "",
      "importedId": "a14eqGtvpEMDxUTPZHiZQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152247,
      "firstName": "Elisabeth",
      "lastName": "André",
      "middleInitial": "",
      "importedId": "6w_cEWrV7A4eedbQz6Repg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152248,
      "firstName": "Michah",
      "lastName": "Prendergast",
      "middleInitial": "",
      "importedId": "f40UEHfnPCOS_4kWV7-A9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152249,
      "firstName": "Benjamin",
      "lastName": "Dossett",
      "middleInitial": "",
      "importedId": "7FbCwIO2g0NZxP-zmJQTsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152250,
      "firstName": "SunKyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "CoT5NGWHCp8l-wApjtrpgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152251,
      "firstName": "John",
      "lastName": "Raiti",
      "middleInitial": "",
      "importedId": "GxpLwdtcaWze-2T6sLIYmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152252,
      "firstName": "Martin",
      "lastName": "Ross",
      "middleInitial": "K",
      "importedId": "S4Iym1YrE8du77kuQD9WjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152253,
      "firstName": "Chelsea",
      "lastName": "Zou",
      "middleInitial": "",
      "importedId": "gelxptLv6GAwXwUgR-oE6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152254,
      "firstName": "Ethan",
      "lastName": "Gordon",
      "middleInitial": "Kroll",
      "importedId": "S0nK3slEPqF51WALORSLKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152255,
      "firstName": "Alessandro",
      "lastName": "Mazzei",
      "middleInitial": "",
      "importedId": "OBE4MQkGtISfciWAGc9lHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152256,
      "firstName": "Luca",
      "lastName": "Cornia",
      "middleInitial": "",
      "importedId": "bEdzOEiUn6qkqvvd39BTKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152257,
      "firstName": "Ron",
      "lastName": "Petrick",
      "middleInitial": "",
      "importedId": "o2_PdaQLz4FwD7-06XTjCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152258,
      "firstName": "Agata",
      "lastName": "Lapedriza",
      "middleInitial": "",
      "importedId": "EeL7GI6lnFTrXiVXz9EjrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152259,
      "firstName": "Alberto",
      "lastName": "Lillo",
      "middleInitial": "",
      "importedId": "NlUtWAAG-pPz5NSsgDwfjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152260,
      "firstName": "Kun-Pyo",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "F70kd9mylujJKbtJ_Xirkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152261,
      "firstName": "Dries",
      "lastName": "Cardinaels",
      "middleInitial": "",
      "importedId": "KEIegCdJmfVEFuJRDB3M5g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152262,
      "firstName": "Burcu A.",
      "lastName": "Urgen",
      "middleInitial": "",
      "importedId": "WwmPQ-Taa5Wx7HQAd7yGMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152263,
      "firstName": "Carola",
      "lastName": "Weidmann",
      "middleInitial": "",
      "importedId": "CAI9kTGZ0rXVA07Zuksh7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152264,
      "firstName": "Margarida",
      "lastName": "Ferreira",
      "middleInitial": "Fortes",
      "importedId": "JGhlgRndVH9OTyoV3ZFI6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152265,
      "firstName": "Kiara",
      "lastName": "Morales",
      "middleInitial": "",
      "importedId": "Xdb7PB1s_JSRxk8DoTr7IQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152266,
      "firstName": "Ruth",
      "lastName": "Stock-Homburg",
      "middleInitial": "Maria",
      "importedId": "VGC9UqLhTSIPVdRYKM7BMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152267,
      "firstName": "Olga",
      "lastName": "Shalopanova",
      "middleInitial": "",
      "importedId": "mNfkv8Nn5YzqUHZcczoYoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152268,
      "firstName": "José Enrique",
      "lastName": "Domínguez-Vidal",
      "middleInitial": "",
      "importedId": "LQw8Bf-B7IbNsNYd-5jQvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152269,
      "firstName": "Anke",
      "lastName": "Brock",
      "middleInitial": "M.",
      "importedId": "-Lb3HGn70iLIp4NIPRWnbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152270,
      "firstName": "Fuminori",
      "lastName": "Nagasawa",
      "middleInitial": "",
      "importedId": "GqMJbiFdj95mDT_qzsCJng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152271,
      "firstName": "Ali Alridha",
      "lastName": "Abdulkarim",
      "middleInitial": "",
      "importedId": "NevbJ-TZRiBNfUO8oujl9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152272,
      "firstName": "Mafalda",
      "lastName": "Gamboa",
      "middleInitial": "",
      "importedId": "rkUvu5mHwj4rWx5emj1w6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152273,
      "firstName": "Shihan",
      "lastName": "Qiu",
      "middleInitial": "",
      "importedId": "OrxDy9VjH68jBp1_35JtPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152274,
      "firstName": "Liliane",
      "lastName": "Filthaut",
      "middleInitial": "",
      "importedId": "ciu8EBp4EG1oZDxN__qQjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152275,
      "firstName": "Utku",
      "lastName": "Norman",
      "middleInitial": "",
      "importedId": "ZWD_PgZopKwz7JazhHHLLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152276,
      "firstName": "Rajul",
      "lastName": "Kumar",
      "middleInitial": "",
      "importedId": "vGVqA3TzHLTtL-wwQ2628Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152277,
      "firstName": "Tiago",
      "lastName": "Guerreiro",
      "middleInitial": "",
      "importedId": "JJKBzLc6Yvr-SWSx6ZUm1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152278,
      "firstName": "Richard",
      "lastName": "Magnotti",
      "middleInitial": "",
      "importedId": "fvLQfzy8LCqYI2WLQznQNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152279,
      "firstName": "Yilin",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "bEcsdoRJy5JLFgfH5g0LCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152280,
      "firstName": "Katrina",
      "lastName": "McDonough",
      "middleInitial": "Louise",
      "importedId": "3tvme4bYQgBttBUPh19sQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152281,
      "firstName": "Ho Seok",
      "lastName": "AHN",
      "middleInitial": "",
      "importedId": "IiGhXqoOs_slI2E0eY5maw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152282,
      "firstName": "Eden",
      "lastName": "Espinosa",
      "middleInitial": "",
      "importedId": "FPOwVIPqxkdPJ3puPGBf9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152283,
      "firstName": "Seunghwa",
      "lastName": "Pyo",
      "middleInitial": "",
      "importedId": "-vej5SjdSHq-DRPjUw_9HQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152284,
      "firstName": "Miles",
      "lastName": "Pennington",
      "middleInitial": "",
      "importedId": "IcFXVQ2ZZJwKAgGc65YyVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152285,
      "firstName": "Franziska",
      "lastName": "Krebs",
      "middleInitial": "",
      "importedId": "-Pb36RJDolgedNYLMvMWzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152286,
      "firstName": "Jane",
      "lastName": "Stuart-Smith",
      "middleInitial": "",
      "importedId": "_OUQtEF1Wphos2ZJu3AfRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152287,
      "firstName": "Sabahat C.",
      "lastName": "Bagci",
      "middleInitial": "",
      "importedId": "GtRiGvVlH7J1sJmbBj6x6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152288,
      "firstName": "Robel",
      "lastName": "Mamo",
      "middleInitial": "",
      "importedId": "o86m8sKprDFnauM9P0rMtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152289,
      "firstName": "Megan",
      "lastName": "Stubbs-Richardson",
      "middleInitial": "",
      "importedId": "8XvpUlbkIMgQHebqCpYG5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152290,
      "firstName": "Silvia Julissa",
      "lastName": "Roncal Briceno",
      "middleInitial": "MSc",
      "importedId": "zAPTBEE1zBfRe_cug_RTrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152291,
      "firstName": "Meriam",
      "lastName": "Moujahid",
      "middleInitial": "",
      "importedId": "T74cGAxPdeod4duxn55cZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152292,
      "firstName": "Maximus N.",
      "lastName": "McCune",
      "middleInitial": "",
      "importedId": "WXQBYYccisF7hnTdMf7Njw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152293,
      "firstName": "Eva",
      "lastName": "Verhelst",
      "middleInitial": "",
      "importedId": "wA3AE0TBSkDdIbOb9iYfFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152294,
      "firstName": "Masayuki",
      "lastName": "Inaba",
      "middleInitial": "",
      "importedId": "Su-HUnvAY-bvNY5Q2Zca1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152295,
      "firstName": "Rafael",
      "lastName": "Sousa Silva",
      "middleInitial": "",
      "importedId": "6EnTypr0Fx7ACoqd0Rdb8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152296,
      "firstName": "Alwin",
      "lastName": "de Rooij",
      "middleInitial": "",
      "importedId": "bo3C50N0D9AF4wNlxPY_dA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152297,
      "firstName": "Jaekeun",
      "lastName": "Sung",
      "middleInitial": "",
      "importedId": "xm2LHclWyOskDW4U8MzPeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152298,
      "firstName": "Weronika",
      "lastName": "Sieińska",
      "middleInitial": "Maria",
      "importedId": "VsGHC3bVFW0KF3lPXVOoYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152299,
      "firstName": "Maya",
      "lastName": "Cakmak",
      "middleInitial": "",
      "importedId": "YGo_6Y2veS53wLbFNiWijw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152300,
      "firstName": "Jamison",
      "lastName": "Heard",
      "middleInitial": "",
      "importedId": "FHVbSaxN5TTcZRJnF-cZaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152301,
      "firstName": "Xiangyu",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "n47DcLqV5l2KWYO1BzXI-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152302,
      "firstName": "Torin",
      "lastName": "Clark",
      "middleInitial": "K",
      "importedId": "vsBWO0gB5dnTUbz1kbw5lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152303,
      "firstName": "François",
      "lastName": "Michaud",
      "middleInitial": "",
      "importedId": "HItSAC6PWYECmEMxKrZS5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152304,
      "firstName": "Shoaib",
      "lastName": "Azam",
      "middleInitial": "",
      "importedId": "0i3Y2Ffy-IJiixO15ACypQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152305,
      "firstName": "Keisuke",
      "lastName": "Shimono",
      "middleInitial": "",
      "importedId": "Os1jwT_e6ROivFxs236mEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152306,
      "firstName": "Aniket Satish",
      "lastName": "Kulkarni",
      "middleInitial": "",
      "importedId": "EjmQEcCYgDl1asM78pObiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152307,
      "firstName": "Linda",
      "lastName": "Onnasch",
      "middleInitial": "",
      "importedId": "ME-E1F22y1b2MNDq5zzwhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152308,
      "firstName": "Sofia",
      "lastName": "Thunberg",
      "middleInitial": "",
      "importedId": "AgwpLjChRlmVoTPWwE7s2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152309,
      "firstName": "François",
      "lastName": "Ferland",
      "middleInitial": "",
      "importedId": "v3Mgjt4y8yh_gyZbI2rYdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152310,
      "firstName": "Madison",
      "lastName": "Harmon",
      "middleInitial": "",
      "importedId": "LCgN34Dhw1NKEqvBqXUD9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152311,
      "firstName": "Mayumi",
      "lastName": "Mohan",
      "middleInitial": "",
      "importedId": "FDd_SWfiUIlwLWdVh9yFIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152312,
      "firstName": "Tamlin",
      "lastName": "Love",
      "middleInitial": "",
      "importedId": "Hj58emVWRv8exKeP_a3AaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152313,
      "firstName": "João",
      "lastName": "Alves",
      "middleInitial": "",
      "importedId": "0SGhhRNJk7TGN0SA3RfBaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152314,
      "firstName": "Anastasia",
      "lastName": "Ostrowski",
      "middleInitial": "K.",
      "importedId": "4hzGeOzsPz7BYgSNUQvq9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152315,
      "firstName": "Léa",
      "lastName": "Haefflinger",
      "middleInitial": "",
      "importedId": "aVyI2E1GBaWWNYl9duRpag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152316,
      "firstName": "Yao-Cheng",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "GqP_irJp-VFBHYBkDffaFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152317,
      "firstName": "Hriday",
      "lastName": "Purohit",
      "middleInitial": "",
      "importedId": "oN1AX9BOwFSUONn0M8rpLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152318,
      "firstName": "Zachary",
      "lastName": "Kaufman",
      "middleInitial": "",
      "importedId": "hilPK4nCmyLx6NAN8e3v3A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152319,
      "firstName": "Jun",
      "lastName": "Baba",
      "middleInitial": "",
      "importedId": "4EBsmWcnwxoJmoVqkCf3Wg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152320,
      "firstName": "Joshua Rafael",
      "lastName": "Sanchez",
      "middleInitial": "",
      "importedId": "dFEQQTcbHq5T9e_gbpHA0w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152321,
      "firstName": "Jennifer",
      "lastName": "Haensel",
      "middleInitial": "X",
      "importedId": "UdJ5mxVMTFyrHh_QT96FQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152322,
      "firstName": "Ginevra",
      "lastName": "Castellano",
      "middleInitial": "",
      "importedId": "4e0RaGx3xSPjG59Qc5VMZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152323,
      "firstName": "Agnes",
      "lastName": "Doue",
      "middleInitial": "",
      "importedId": "S2MNHE2hb5mR7dfy6qUDBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152324,
      "firstName": "Samantha",
      "lastName": "Dubrow",
      "middleInitial": "",
      "importedId": "VUG4BnIeGHkZ1lyByFi5TQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152325,
      "firstName": "Nina",
      "lastName": "Moorman",
      "middleInitial": "Marie",
      "importedId": "NNuLGfw7VvRpdagjdvsr7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152326,
      "firstName": "Ouassim",
      "lastName": "Milous",
      "middleInitial": "",
      "importedId": "zXPIWUNF7Jx0B9YKet94hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152327,
      "firstName": "Qin",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "hjwL13QIrsCDHh2hCzbQlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152328,
      "firstName": "Artur",
      "lastName": "Lisetschko",
      "middleInitial": "",
      "importedId": "FnPtkftobg7gf1mNjGkBxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152329,
      "firstName": "Bo",
      "lastName": "Xie",
      "middleInitial": "",
      "importedId": "4dqFGzQRGMmlhi9baoOCzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152330,
      "firstName": "Maria Luce",
      "lastName": "Lupetti",
      "middleInitial": "",
      "importedId": "mYUqZLXdGubtC-1N_798FA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152331,
      "firstName": "Yuang",
      "lastName": "Tong",
      "middleInitial": "",
      "importedId": "NV-I2T0nN9OhnYZUZW9yag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152332,
      "firstName": "Vinitha",
      "lastName": "Ranganeni",
      "middleInitial": "",
      "importedId": "dMLK8_ABy5jnRp90xVGnoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152333,
      "firstName": "Alexander",
      "lastName": "Arntz",
      "middleInitial": "",
      "importedId": "dvYOlrWxInxGb0IclhCW7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152334,
      "firstName": "Sehoon",
      "lastName": "Ha",
      "middleInitial": "",
      "importedId": "A1DWa7Pw1efUAAAEqkzFDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152335,
      "firstName": "Batuhan",
      "lastName": "Sayis",
      "middleInitial": "",
      "importedId": "nw31F8Y4QLdVRMOPfuJkvg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152336,
      "firstName": "Melisa",
      "lastName": "Conde",
      "middleInitial": "",
      "importedId": "t2_LZ5mHZ-ulRW6k4TLBrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152337,
      "firstName": "Lena",
      "lastName": "Stachnick",
      "middleInitial": "",
      "importedId": "QKyZZ-2WJw5KDdwR4SlhvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152338,
      "firstName": "Max",
      "lastName": "Pascher",
      "middleInitial": "",
      "importedId": "RDkw4QN4sWBUEtRUcxvj9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152339,
      "firstName": "Jhielson",
      "lastName": "Montino Pimentel",
      "middleInitial": "",
      "importedId": "jbz1trvTSmBxalVI9scWqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152340,
      "firstName": "Olga",
      "lastName": "Abramov",
      "middleInitial": "",
      "importedId": "kux_UoCzynJzxN8KhXdKAA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152341,
      "firstName": "Hina",
      "lastName": "Fujita",
      "middleInitial": "",
      "importedId": "YQ26O7Pzia6ij5YaUldiag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152342,
      "firstName": "Anna",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "RLpa2NWAYrs4WS3UrhHZjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152343,
      "firstName": "Paula",
      "lastName": "Bonea",
      "middleInitial": "",
      "importedId": "aOtDZJR56gt-vGECyNk39Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152344,
      "firstName": "Ziqi",
      "lastName": "MA",
      "middleInitial": "",
      "importedId": "6mfVQG0aYd9GaFl8gCwbpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152345,
      "firstName": "Nathaniel",
      "lastName": "Dennler",
      "middleInitial": "Steele",
      "importedId": "Nc6bYyfH8cqeamgHsDunrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152346,
      "firstName": "Ju",
      "lastName": "Row Farr",
      "middleInitial": "",
      "importedId": "Q1_lTPI6J-W2enOR4pqSsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152347,
      "firstName": "Haonan",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "Bi1oFgJ_EysPI9vfkG3ngA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152348,
      "firstName": "Matthew",
      "lastName": "Hessler",
      "middleInitial": "",
      "importedId": "Xj_ij32dg3qfPocuPWE5ZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152349,
      "firstName": "Tanaka",
      "lastName": "Akiyama",
      "middleInitial": "",
      "importedId": "oFTsB2nFbc67LItV2DEkzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152350,
      "firstName": "Cristina",
      "lastName": "Gena",
      "middleInitial": "",
      "importedId": "2nqu7cpN8L6WfxCTnkc0Qw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152351,
      "firstName": "Shih-Yi",
      "lastName": "Chien",
      "middleInitial": "",
      "importedId": "-VCKOl1_GRwHObddwF47WQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152352,
      "firstName": "Minha",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "tPufI9IKI7i4KD4WnNurTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152353,
      "firstName": "Varad",
      "lastName": "Dhat",
      "middleInitial": "",
      "importedId": "g9w-hd59lNPZLLin_T7h8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152354,
      "firstName": "Koeun",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "ZJhtt8i0fR7Q9WfKOtwmBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152355,
      "firstName": "Toshihiro",
      "lastName": "Hiraoka",
      "middleInitial": "",
      "importedId": "2YRz7sB-9yY-lebvc2tX6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152356,
      "firstName": "Florian",
      "lastName": "Daiber",
      "middleInitial": "",
      "importedId": "V7xTeq008LZvqi1nDsfEZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152357,
      "firstName": "Cailyn",
      "lastName": "Smith",
      "middleInitial": "",
      "importedId": "D-88mtLZOzqz_HoAFxKChw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152358,
      "firstName": "Wei",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "WyE3WGAl8_qQPRVJGhUa5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152359,
      "firstName": "Maite",
      "lastName": "Antonio Rebollo",
      "middleInitial": "",
      "importedId": "RfpzHiIMwLij-YAyj5mh-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152360,
      "firstName": "Anouk",
      "lastName": "Neerincx",
      "middleInitial": "",
      "importedId": "gmNEZO8P1LdhTFuMH5QqZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152361,
      "firstName": "Gustavo",
      "lastName": "Rovelo Ruiz",
      "middleInitial": "Alberto",
      "importedId": "ruSDK92__LIXRdSnxf_pKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152362,
      "firstName": "Mehmet Onur",
      "lastName": "Keskin",
      "middleInitial": "",
      "importedId": "BUAZvsE0cCdpFKuorOjQpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152363,
      "firstName": "Hiroshi",
      "lastName": "Ishiguro",
      "middleInitial": "",
      "importedId": "5CfUjiobezY8oEH_8zYRpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152364,
      "firstName": "Ziwei",
      "lastName": "Chi",
      "middleInitial": "",
      "importedId": "R4-J2OiqLrezQvsnzELkuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152365,
      "firstName": "Mattia",
      "lastName": "Racca",
      "middleInitial": "",
      "importedId": "WjkptgJ-uAdvOW96uLmtKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152366,
      "firstName": "Valeria",
      "lastName": "Villani",
      "middleInitial": "",
      "importedId": "xBg-aiP4ZH0IEYkkRdvtRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152367,
      "firstName": "Qiaoning",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "M3dOSzGOHBXxFF3A8BFmvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152368,
      "firstName": "Elisa",
      "lastName": "Mina",
      "middleInitial": "",
      "importedId": "7ATD2n6mX1X-vm_J6kA_Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152369,
      "firstName": "Matt",
      "lastName": "Gorbet",
      "middleInitial": "",
      "importedId": "BaIa_T-_mIikhrq_41Hh-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152370,
      "firstName": "Yusaku",
      "lastName": "Takahama",
      "middleInitial": "",
      "importedId": "WcfM5xyJOMXxcHHnHp111A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152371,
      "firstName": "Sirin",
      "lastName": "Liukasemsarn",
      "middleInitial": "",
      "importedId": "TpKpb3bWdJ_gKDDmLIlJlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152372,
      "firstName": "Andre",
      "lastName": "Helgert",
      "middleInitial": "",
      "importedId": "nE_t6Y_zOEjqhxtWTIqDCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152373,
      "firstName": "Judith",
      "lastName": "Amores",
      "middleInitial": "",
      "importedId": "bDq9j9SLRhHpC7OsSFKR1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152374,
      "firstName": "Shuowen",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "ScQ0O_yeZdcE8riczc2Hlw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152375,
      "firstName": "Gianluca",
      "lastName": "Corsini",
      "middleInitial": "",
      "importedId": "f_1Fc1Ro20EQF0rYxQRwXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152376,
      "firstName": "Saad",
      "lastName": "Elbeleidy",
      "middleInitial": "",
      "importedId": "dnXYQSwRFHxKQqWQXGy_ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152377,
      "firstName": "Angelika",
      "lastName": "Canete",
      "middleInitial": "",
      "importedId": "VgE3SncX-gOp2CcdjRaXWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152378,
      "firstName": "André",
      "lastName": "Schakkal",
      "middleInitial": "",
      "importedId": "8XFHK5IHcfpEqmNowBIkTA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152379,
      "firstName": "Antonio",
      "lastName": "Andriella",
      "middleInitial": "",
      "importedId": "UuG6Mom43RBlVjM2UtwWnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152380,
      "firstName": "Shiqi",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "KAlrIhiRV4HkbJD0Akw0dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152381,
      "firstName": "Yuqi",
      "lastName": "Yang",
      "middleInitial": "",
      "importedId": "iU7GyQaHZtSWKC_rt2hP6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152382,
      "firstName": "Antonio",
      "lastName": "Paolillo",
      "middleInitial": "",
      "importedId": "IjX2T1ubhnynih4CN_QiSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152383,
      "firstName": "Kyler",
      "lastName": "Smith",
      "middleInitial": "",
      "importedId": "Mc7_YxgLvYfrM1xTw27FsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 152717,
      "firstName": "Ryan",
      "lastName": "Calo",
      "importedId": "10494",
      "source": "SYS",
      "affiliations": [
        {
          "country": "United States",
          "institution": "University of Washington Law School"
        }
      ]
    },
    {
      "id": 152733,
      "firstName": "Harald",
      "lastName": "Soh",
      "importedId": "10510",
      "source": "SYS",
      "affiliations": []
    },
    {
      "id": 152785,
      "firstName": "Alex",
      "lastName": "Leonessa",
      "importedId": "10562",
      "source": "SYS",
      "affiliations": [
        {
          "institution": "National Science Foundation"
        }
      ]
    }
  ],
  "recognitions": [
    {
      "id": 10079,
      "name": "Best Video",
      "iconName": "star"
    },
    {
      "id": 10080,
      "name": "Best alt.HRI",
      "iconName": "star"
    },
    {
      "id": 10081,
      "name": "Video HM",
      "iconName": "hand-like"
    },
    {
      "id": 10082,
      "name": "alt.HRI HM",
      "iconName": "hand-like"
    },
    {
      "id": 10083,
      "name": "Competition Winner",
      "iconName": "crown"
    }
  ]
}