{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10097,
    "shortName": "UbiComp-ISWC",
    "displayShortName": "UbiComp/ISWC",
    "year": 2023,
    "startDate": 1696723200000,
    "endDate": 1697068800000,
    "fullName": "International joint conference on Pervasive and Ubiquitous Computing / International Symposium on Wearable Computers",
    "url": "https://services.sigchi.org/qoala/ubicomp-iswc/2023",
    "location": "Cancun, Mexico",
    "timeZoneOffset": -300,
    "timeZoneName": "America/Cancun",
    "logoUrl": "https://files.sigchi.org/conference/logo/10097/262258d4-fee7-3026-f9c0-102cbaf9bdeb.png",
    "name": "UbiComp-ISWC 2023"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 16,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2023-10-11 14:59:09+00"
  },
  "sponsors": [
    {
      "id": 10445,
      "name": "Intel",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10097/logo/ff6640a2-9eed-6680-9ee8-44bb20163038.png",
      "levelId": 10266,
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10446,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10097/logo/0a054b59-9f40-6535-e97c-197d8643871b.png",
      "levelId": 10266,
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10447,
      "name": "Microsoft research",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10097/logo/7bfa1272-724e-0677-fc1d-ed0677274504.png",
      "levelId": 10267,
      "order": 0,
      "extraPadding": 8
    },
    {
      "id": 10448,
      "name": "Springer Science & Business Media",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10097/logo/352f69ac-d9c9-7b23-f206-99c3fa39877e.png",
      "levelId": 10267,
      "order": 1,
      "extraPadding": 8
    },
    {
      "id": 10449,
      "name": "Frontiers",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10097/logo/28925c48-93e6-76a4-223a-7bce17c154f6.png",
      "levelId": 10268,
      "order": 0,
      "extraPadding": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10236,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    },
    {
      "id": 10266,
      "name": "Silver",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10267,
      "name": "Bronze",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10268,
      "name": "Bespoke",
      "rank": 4,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10223,
      "name": "Ground floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10097/f9a66fe3-c82d-d7e3-06cb-78364c5af308.png",
      "roomIds": []
    },
    {
      "id": 10224,
      "name": "1st floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10097/8f5f4f54-cc58-0ebb-9c31-c9336ec0cc8e.png",
      "roomIds": []
    },
    {
      "id": 10225,
      "name": "2nd floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10097/038908a5-a326-0b47-704a-a2131a27cea4.png",
      "roomIds": [
        11265,
        11343,
        11266,
        11344,
        11271,
        11256,
        11260,
        11261,
        11262,
        11263,
        11264,
        11257,
        11258,
        11259,
        11270,
        11267,
        11268,
        11272,
        11269,
        11303
      ]
    },
    {
      "id": 10226,
      "name": "3rd floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10097/86607650-d1e1-e92f-5269-28f1fb43d85b.png",
      "roomIds": []
    }
  ],
  "rooms": [
    {
      "id": 11256,
      "name": "Cozumel (Plenary)",
      "setup": "THEATRE",
      "typeId": 12922,
      "capacity": 1296
    },
    {
      "id": 11257,
      "name": "Cozumel A (Track A)",
      "setup": "THEATRE",
      "typeId": 12924,
      "capacity": 396,
      "note": "Only for Main Conference"
    },
    {
      "id": 11258,
      "name": "Cozumel B (Track B)",
      "setup": "THEATRE",
      "typeId": 12924,
      "capacity": 450,
      "note": "Only for Main Conference"
    },
    {
      "id": 11259,
      "name": "Cozumel C (Track C)",
      "setup": "THEATRE",
      "typeId": 12924,
      "capacity": 441,
      "note": "Only for Main Conference"
    },
    {
      "id": 11260,
      "name": "Cozumel 1 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 102,
      "note": "Only for Pre-Conference"
    },
    {
      "id": 11261,
      "name": "Cozumel 2 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 95,
      "note": "Only for Pre-Conference"
    },
    {
      "id": 11262,
      "name": "Cozumel 3 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 77,
      "note": "Only for Pre-Conference"
    },
    {
      "id": 11263,
      "name": "Cozumel 4 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 106,
      "note": "Only for Pre-Conference"
    },
    {
      "id": 11264,
      "name": "Cozumel 5 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12919,
      "capacity": 114,
      "note": "Only for Pre-Conference"
    },
    {
      "id": 11265,
      "name": "Tulum 1 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 45,
      "note": "Pre-Conference Day 1 and Day 2"
    },
    {
      "id": 11266,
      "name": "Tulum 3 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 44,
      "note": "Pre-Conference: Only Day 1 (Day 2 is preparation for Relaxation Room)"
    },
    {
      "id": 11267,
      "name": "Tulum (Relaxation Room)",
      "setup": "SPECIAL",
      "typeId": 12928,
      "note": "Only for Main Conference"
    },
    {
      "id": 11268,
      "name": "Xcaret (Family Room)",
      "setup": "SPECIAL",
      "typeId": 12928,
      "note": "Pre-Conference and Main Conference"
    },
    {
      "id": 11269,
      "name": "Coba (Luncheons)",
      "setup": "ROUNDS",
      "typeId": 12922,
      "capacity": 122
    },
    {
      "id": 11270,
      "name": "Isla Mujeres (SV Lounge)",
      "setup": "ROUNDS",
      "typeId": 12924,
      "capacity": 85
    },
    {
      "id": 11271,
      "name": "Contoy (Conference Committee)",
      "setup": "SPECIAL",
      "typeId": 12922
    },
    {
      "id": 11272,
      "name": "Akumal Terrace (Reception)",
      "setup": "SPECIAL",
      "typeId": 12922,
      "capacity": 706
    },
    {
      "id": 11303,
      "name": "2nd Floor Hallway (Coffee break)",
      "setup": "NO_ROOM",
      "typeId": 12928
    },
    {
      "id": 11343,
      "name": "Tulum 2 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 45,
      "note": "Pre-Conference Day 1 and Day 2"
    },
    {
      "id": 11344,
      "name": "Tulum 4 (Pre-Conference)",
      "setup": "ROUNDS",
      "typeId": 12927,
      "capacity": 44,
      "note": "Pre-Conference: Only Day 1 (Day 2 is preparation for Relaxation Room)"
    },
    {
      "id": 11345,
      "name": "Hotel Presidente Intercontinental",
      "setup": "THEATRE",
      "typeId": 12924
    }
  ],
  "tracks": [
    {
      "id": 12286,
      "name": "UbiComp/ISWC 2023 Posters and Demos",
      "typeId": 13124
    },
    {
      "id": 12287,
      "name": "UbiComp/ISWC 2023 ISWC Notes and Briefs ",
      "typeId": 12924
    },
    {
      "id": 12288,
      "name": "UbiComp/ISWC 2023 ISWC 2023 Design Exhibition",
      "typeId": 13124
    },
    {
      "id": 12438,
      "typeId": 12924
    },
    {
      "id": 12439,
      "name": "UbiComp/ISWC 2023 Workshops Proposals",
      "typeId": 12927
    },
    {
      "id": 12440,
      "typeId": 13119
    },
    {
      "id": 12443,
      "typeId": 13122
    },
    {
      "id": 12444,
      "typeId": 12928
    },
    {
      "id": 12445,
      "typeId": 13121
    },
    {
      "id": 12446,
      "typeId": 13125
    },
    {
      "id": 12447,
      "typeId": 13123
    },
    {
      "id": 12448,
      "typeId": 13126
    },
    {
      "id": 12449,
      "typeId": 13124
    }
  ],
  "contentTypes": [
    {
      "id": 12919,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12922,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12924,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 8,
      "displayName": "Papers"
    },
    {
      "id": 12927,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12928,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 30
    },
    {
      "id": 13119,
      "name": "Symposium",
      "color": "#8e008b",
      "duration": 0
    },
    {
      "id": 13120,
      "name": "Tutorial",
      "color": "#ff7a00",
      "duration": 0,
      "displayName": "Tutorials"
    },
    {
      "id": 13121,
      "name": "Doctoral Colloquium",
      "color": "#26e5f1",
      "duration": 0
    },
    {
      "id": 13122,
      "name": "Plenary",
      "color": "#6baed6",
      "duration": 75
    },
    {
      "id": 13123,
      "name": "Luncheon",
      "color": "#ff99ca",
      "duration": 0
    },
    {
      "id": 13124,
      "name": "Exhibition",
      "color": "#006d2c",
      "duration": 180
    },
    {
      "id": 13125,
      "name": "Social",
      "color": "#32d923",
      "duration": 0
    },
    {
      "id": 13126,
      "name": "Award",
      "color": "#969696",
      "duration": 180
    }
  ],
  "timeSlots": [
    {
      "id": 13162,
      "type": "SESSION",
      "startDate": 1696755600000,
      "endDate": 1696768200000
    },
    {
      "id": 13163,
      "type": "LUNCH",
      "startDate": 1696768200000,
      "endDate": 1696773600000
    },
    {
      "id": 13164,
      "type": "SESSION",
      "startDate": 1696773600000,
      "endDate": 1696786200000
    },
    {
      "id": 13165,
      "type": "BREAK",
      "startDate": 1696761000000,
      "endDate": 1696762800000
    },
    {
      "id": 13166,
      "type": "BREAK",
      "startDate": 1696779000000,
      "endDate": 1696780800000
    },
    {
      "id": 13167,
      "type": "SESSION",
      "startDate": 1696842000000,
      "endDate": 1696854600000
    },
    {
      "id": 13168,
      "type": "BREAK",
      "startDate": 1696847400000,
      "endDate": 1696849200000
    },
    {
      "id": 13169,
      "type": "LUNCH",
      "startDate": 1696854600000,
      "endDate": 1696860000000
    },
    {
      "id": 13170,
      "type": "SESSION",
      "startDate": 1696860000000,
      "endDate": 1696872600000
    },
    {
      "id": 13171,
      "type": "BREAK",
      "startDate": 1696865400000,
      "endDate": 1696867200000
    },
    {
      "id": 13172,
      "type": "SESSION",
      "startDate": 1696928400000,
      "endDate": 1696941000000
    },
    {
      "id": 13173,
      "type": "BREAK",
      "startDate": 1696932900000,
      "endDate": 1696935600000
    },
    {
      "id": 13174,
      "type": "LUNCH",
      "startDate": 1696941000000,
      "endDate": 1696946400000
    },
    {
      "id": 13175,
      "type": "SESSION",
      "startDate": 1696946400000,
      "endDate": 1696959000000
    },
    {
      "id": 13176,
      "type": "BREAK",
      "startDate": 1696951800000,
      "endDate": 1696953600000
    },
    {
      "id": 13182,
      "type": "SESSION",
      "startDate": 1697101200000,
      "endDate": 1697113800000
    },
    {
      "id": 13183,
      "type": "BREAK",
      "startDate": 1697106600000,
      "endDate": 1697108400000
    },
    {
      "id": 13184,
      "type": "LUNCH",
      "startDate": 1697113800000,
      "endDate": 1697119200000
    },
    {
      "id": 13185,
      "type": "SESSION",
      "startDate": 1697119200000,
      "endDate": 1697131800000
    },
    {
      "id": 13186,
      "type": "BREAK",
      "startDate": 1697124600000,
      "endDate": 1697127300000
    },
    {
      "id": 13187,
      "type": "SESSION",
      "startDate": 1697014800000,
      "endDate": 1697027400000
    },
    {
      "id": 13188,
      "type": "BREAK",
      "startDate": 1697019300000,
      "endDate": 1697022000000
    },
    {
      "id": 13189,
      "type": "LUNCH",
      "startDate": 1697027400000,
      "endDate": 1697032800000
    },
    {
      "id": 13190,
      "type": "SESSION",
      "startDate": 1697032800000,
      "endDate": 1697045400000
    },
    {
      "id": 13191,
      "type": "BREAK",
      "startDate": 1697038200000,
      "endDate": 1697040000000
    },
    {
      "id": 13192,
      "type": "SESSION",
      "startDate": 1697047200000,
      "endDate": 1697061600000
    },
    {
      "id": 13193,
      "type": "SESSION",
      "startDate": 1696960800000,
      "endDate": 1696975200000
    },
    {
      "id": 13322,
      "type": "SESSION",
      "startDate": 1696755600000,
      "endDate": 1696786200000
    },
    {
      "id": 13329,
      "type": "SESSION",
      "startDate": 1696842000000,
      "endDate": 1696872600000
    },
    {
      "id": 13330,
      "type": "SESSION",
      "startDate": 1696928400000,
      "endDate": 1696932900000
    },
    {
      "id": 13331,
      "type": "SESSION",
      "startDate": 1696960800000,
      "endDate": 1696971600000
    },
    {
      "id": 13483,
      "type": "SESSION",
      "startDate": 1696849200000,
      "endDate": 1696872600000
    },
    {
      "id": 13484,
      "type": "SESSION",
      "startDate": 1696935600000,
      "endDate": 1696941000000
    },
    {
      "id": 13485,
      "type": "SESSION",
      "startDate": 1696946400000,
      "endDate": 1696951800000
    },
    {
      "id": 13486,
      "type": "SESSION",
      "startDate": 1696953600000,
      "endDate": 1696959000000
    },
    {
      "id": 13487,
      "type": "SESSION",
      "startDate": 1697014800000,
      "endDate": 1697019300000
    },
    {
      "id": 13488,
      "type": "SESSION",
      "startDate": 1697022000000,
      "endDate": 1697027400000
    },
    {
      "id": 13489,
      "type": "SESSION",
      "startDate": 1697032800000,
      "endDate": 1697038200000
    },
    {
      "id": 13490,
      "type": "SESSION",
      "startDate": 1697040000000,
      "endDate": 1697045400000
    },
    {
      "id": 13491,
      "type": "SESSION",
      "startDate": 1697047200000,
      "endDate": 1697058000000
    },
    {
      "id": 13492,
      "type": "SESSION",
      "startDate": 1697101200000,
      "endDate": 1697106600000
    },
    {
      "id": 13493,
      "type": "SESSION",
      "startDate": 1697108400000,
      "endDate": 1697113800000
    },
    {
      "id": 13494,
      "type": "SESSION",
      "startDate": 1697119200000,
      "endDate": 1697124600000
    },
    {
      "id": 13495,
      "type": "SESSION",
      "startDate": 1697127300000,
      "endDate": 1697129100000
    },
    {
      "id": 13523,
      "type": "SESSION",
      "startDate": 1696964400000,
      "endDate": 1696971600000
    }
  ],
  "sessions": [
    {
      "id": 121841,
      "name": "S1 - UbiComp4All: 1st International Symposium on Inclusive and Equitable Ubiquitous Computing",
      "isParallelPresentation": true,
      "importedId": "f762f62d-fe6a-4bac-9d77-be0366c3fc43",
      "typeId": 13119,
      "roomId": 11261,
      "chairIds": [],
      "contentIds": [
        128570
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121842,
      "name": "S2 - ASSET 2023: Americas Student Symposium on Emerging Technologies",
      "isParallelPresentation": true,
      "importedId": "7f43db2d-ad76-4ba4-b693-d2970e678bfe",
      "typeId": 13119,
      "roomId": 11263,
      "chairIds": [],
      "contentIds": [
        128576
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121844,
      "name": "W1 - 11th International Workshop on Human Activity Sensing Corpus and Applications (HASCA)",
      "isParallelPresentation": true,
      "importedId": "3bd9fea9-e02b-40e9-8593-306da1fbf9eb",
      "typeId": 12927,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        128566
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121845,
      "name": "W2 - 8th International Workshop on Mental Health and Well-being: Sensing and Intervention",
      "isParallelPresentation": true,
      "importedId": "bf6b8257-243d-45c9-a33b-abb956478452",
      "typeId": 12927,
      "roomId": 11260,
      "chairIds": [],
      "contentIds": [
        128571
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121846,
      "name": "W3 - CPD 2023: The 6th International Workshop on Combining Physical and Data-Driven Knowledge in Ubiquitous Computing",
      "isParallelPresentation": false,
      "importedId": "2e405ce2-465b-4f0f-9317-6ae840aa2123",
      "typeId": 12927,
      "roomId": 11262,
      "chairIds": [],
      "contentIds": [
        128564
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121847,
      "name": "W4 - Digital Therapeutics Evolution What kind of research will make the difference in this area?",
      "isParallelPresentation": true,
      "importedId": "6ad06cb7-fa28-468f-9fcb-228f3ee9cc93",
      "typeId": 12927,
      "roomId": 11266,
      "chairIds": [],
      "contentIds": [
        128574
      ],
      "source": "SYS",
      "timeSlotId": 13162
    },
    {
      "id": 121856,
      "name": "T1 - ARDUOUS: Developing a Data Annotation Protocol",
      "isParallelPresentation": false,
      "importedId": "a01a98fc-5b00-4732-adbd-d2f17b724893",
      "typeId": 13120,
      "roomId": 11344,
      "chairIds": [],
      "contentIds": [
        128569
      ],
      "source": "SYS",
      "timeSlotId": 13162
    },
    {
      "id": 121857,
      "name": "T2 - Research Methodologies across the Physical - Virtual Reality Spectrum",
      "isParallelPresentation": false,
      "importedId": "44e14f83-63c8-47c5-ac9b-81cb564aff0c",
      "typeId": 13120,
      "roomId": 11344,
      "chairIds": [],
      "contentIds": [
        128572
      ],
      "source": "SYS",
      "timeSlotId": 13164
    },
    {
      "id": 121859,
      "name": "Doctoral Colloquium",
      "isParallelPresentation": true,
      "importedId": "e0d6963f-dbf8-419c-b14d-1c783cfbbc85",
      "typeId": 13121,
      "roomId": 11264,
      "chairIds": [],
      "contentIds": [
        129572
      ],
      "source": "SYS",
      "timeSlotId": 13322
    },
    {
      "id": 121924,
      "name": "S3: GenAI4PC: Generative AI for Pervasive Computing",
      "isParallelPresentation": false,
      "importedId": "d0ed2b2c-2ec2-4572-a1cc-b763bcd44634",
      "typeId": 13119,
      "roomId": 11344,
      "chairIds": [],
      "contentIds": [
        128577
      ],
      "source": "SYS",
      "timeSlotId": 13483
    },
    {
      "id": 121925,
      "name": "W5 - FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing",
      "isParallelPresentation": false,
      "importedId": "e0f943bc-b4a3-4ab8-9955-024ae548a939",
      "typeId": 12927,
      "roomId": 11264,
      "chairIds": [],
      "contentIds": [
        128563
      ],
      "source": "SYS",
      "timeSlotId": 13329
    },
    {
      "id": 121926,
      "name": "W6 - WellComp 2023: 6th International Workshop on Computing for Well-Being",
      "isParallelPresentation": false,
      "importedId": "2e319362-5731-4580-afc6-7851ad3c180c",
      "typeId": 12927,
      "roomId": 11261,
      "chairIds": [],
      "contentIds": [
        128575
      ],
      "source": "SYS",
      "timeSlotId": 13329
    },
    {
      "id": 121927,
      "name": "W7 - The 3rd Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI)",
      "isParallelPresentation": false,
      "importedId": "88b4178f-f68e-4e61-8e99-3c0f5d8b97b7",
      "typeId": 12927,
      "roomId": 11260,
      "chairIds": [],
      "contentIds": [
        128565
      ],
      "source": "SYS",
      "timeSlotId": 13329
    },
    {
      "id": 121928,
      "name": "W8 - EarComp 2023: 4th International Workshop on Earable Computing",
      "isParallelPresentation": false,
      "importedId": "746f9f38-196a-45b8-a900-06163741b903",
      "typeId": 12927,
      "roomId": 11262,
      "chairIds": [],
      "contentIds": [
        128567
      ],
      "source": "SYS",
      "timeSlotId": 13167
    },
    {
      "id": 121929,
      "name": "T3 - SOAR: Tutorial on SOlving the sensor-based Activity Recognition problem",
      "isParallelPresentation": false,
      "importedId": "f50b270f-b13a-4078-8140-718d28938d3a",
      "typeId": 13120,
      "roomId": 11263,
      "chairIds": [],
      "contentIds": [
        128562
      ],
      "source": "SYS",
      "timeSlotId": 13167
    },
    {
      "id": 121930,
      "name": "T4 - UbiCHAI - Experimental Methodologies for Cognitive Human Augmentation",
      "isParallelPresentation": false,
      "importedId": "20d4e84b-4b63-49cf-b667-db313d5e1dd5",
      "typeId": 13120,
      "roomId": 11263,
      "chairIds": [],
      "contentIds": [
        128568
      ],
      "source": "SYS",
      "timeSlotId": 13170
    },
    {
      "id": 127782,
      "name": "Welcome and Opening Keynote",
      "isParallelPresentation": true,
      "importedId": "ae58aa7a-c3f2-48ef-a24d-923e1169aa3c",
      "typeId": 13122,
      "roomId": 11256,
      "chairIds": [],
      "contentIds": [
        129565
      ],
      "source": "SYS",
      "timeSlotId": 13330
    },
    {
      "id": 127783,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "42a370d3-eede-42de-94b1-0fa5db5a2b6d",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129573
      ],
      "source": "SYS",
      "timeSlotId": 13173
    },
    {
      "id": 127784,
      "name": "1A - Human Sensing: Health & Behavior",
      "isParallelPresentation": false,
      "importedId": "dd875014-65a9-4f05-bc21-f1f89c4bdd6a",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        121691
      ],
      "contentIds": [
        128317,
        128281,
        128290,
        128286,
        128332,
        128427,
        128367,
        128430,
        128431
      ],
      "source": "SYS",
      "timeSlotId": 13484
    },
    {
      "id": 127785,
      "name": "1B - Learning and Education",
      "isParallelPresentation": false,
      "importedId": "2da1a61f-980e-442c-ad71-319716b726e6",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129699
      ],
      "contentIds": [
        128310,
        128341,
        128395,
        128357,
        128368,
        128396,
        121768,
        128433,
        128434
      ],
      "source": "SYS",
      "timeSlotId": 13484
    },
    {
      "id": 127786,
      "name": "8B - Smart Environments",
      "isParallelPresentation": false,
      "importedId": "13fe2bc2-df5a-4f7d-a244-bedeae68f334",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129121
      ],
      "contentIds": [
        128316,
        128330,
        128285,
        128315,
        128378,
        128435,
        128436
      ],
      "source": "SYS",
      "timeSlotId": 13493
    },
    {
      "id": 127787,
      "name": "Diversity & N2Women Luncheon (only registered guests)",
      "isParallelPresentation": false,
      "importedId": "0240f9f6-a7e6-4816-94ae-802a64bb16c1",
      "typeId": 13123,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        129570
      ],
      "source": "SYS",
      "timeSlotId": 13174
    },
    {
      "id": 127788,
      "name": "2A -  Human Sensing: Physio",
      "isParallelPresentation": false,
      "importedId": "96de2e80-d4f2-4420-ab62-e296ea581568",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        129696
      ],
      "contentIds": [
        128353,
        128271,
        128323,
        128381,
        128382,
        128425,
        128387,
        128437,
        128438
      ],
      "source": "SYS",
      "timeSlotId": 13485
    },
    {
      "id": 127789,
      "name": "2B - All Forms of Input",
      "isParallelPresentation": false,
      "importedId": "36bb6572-593c-4ace-857d-adae2e4eb173",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129719
      ],
      "contentIds": [
        128292,
        128282,
        128287,
        128299,
        128351,
        128399,
        121771,
        121819,
        128439,
        128440
      ],
      "source": "SYS",
      "timeSlotId": 13485
    },
    {
      "id": 127790,
      "name": "2C - Context, Activities and Behavior",
      "isParallelPresentation": false,
      "importedId": "262d867d-e809-4be2-9db4-e7295a05c8a1",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        129709
      ],
      "contentIds": [
        128280,
        128308,
        128303,
        128375,
        128296,
        121801,
        121772,
        128442,
        128443
      ],
      "source": "SYS",
      "timeSlotId": 13485
    },
    {
      "id": 127791,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "408483ce-e6bc-40ab-a5c8-f2c5792bb3dd",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129571
      ],
      "source": "SYS",
      "timeSlotId": 13176
    },
    {
      "id": 127792,
      "name": "9A - Interaction and Interfaces",
      "isParallelPresentation": false,
      "importedId": "e0181df6-4ddc-4969-9804-e6331a925633",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        129698
      ],
      "contentIds": [
        128275,
        128307,
        128403,
        128364,
        128416,
        121785,
        127859,
        127853
      ],
      "source": "SYS",
      "timeSlotId": 13494
    },
    {
      "id": 127793,
      "name": "7B - Brain and Mental Health",
      "isParallelPresentation": false,
      "importedId": "552dba05-f3d0-4adf-bc93-24a2a93f3e60",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129263
      ],
      "contentIds": [
        128345,
        128291,
        128406,
        128369,
        128344,
        128385,
        128354,
        128444,
        128445
      ],
      "source": "SYS",
      "timeSlotId": 13492
    },
    {
      "id": 127794,
      "name": "3C - Location and Trajectories",
      "isParallelPresentation": false,
      "importedId": "0329bbf6-77b4-483c-bd4a-4cf3fcfead41",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        128721
      ],
      "contentIds": [
        128325,
        128408,
        128359,
        128338,
        128397,
        128398,
        128428,
        128446,
        128447
      ],
      "source": "SYS",
      "timeSlotId": 13486
    },
    {
      "id": 127795,
      "name": "Opening Reception",
      "isParallelPresentation": false,
      "importedId": "35981caa-d6eb-4943-a3c0-fdbd69a03236",
      "typeId": 13125,
      "roomId": 11272,
      "chairIds": [],
      "contentIds": [
        129568
      ],
      "source": "SYS",
      "timeSlotId": 13331
    },
    {
      "id": 127796,
      "name": "Posters",
      "isParallelPresentation": true,
      "importedId": "0614d1dc-3748-4183-aadd-83fddc153e88",
      "typeId": 13124,
      "roomId": 11272,
      "chairIds": [],
      "contentIds": [
        121773,
        121811,
        121808,
        121818,
        121796,
        121790,
        121789,
        121770,
        121766,
        121775,
        121824,
        121835,
        121832,
        121829,
        121839,
        121803,
        121802,
        121807,
        121805,
        121810,
        121809,
        121783,
        121782,
        121786,
        121784,
        121778,
        121833,
        121830,
        121837,
        121840,
        121817,
        121816,
        121815,
        121822,
        121821,
        121820,
        121793,
        121791,
        121788,
        121798,
        121797
      ],
      "source": "SYS",
      "timeSlotId": 13331
    },
    {
      "id": 127797,
      "name": "Demos",
      "isParallelPresentation": true,
      "importedId": "04604968-3a79-46ef-803d-fde4c2ba7ef6",
      "typeId": 13124,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        121776,
        121814,
        121799,
        121774,
        121779,
        121827,
        121836,
        121812,
        121777,
        121780,
        121823
      ],
      "source": "SYS",
      "timeSlotId": 13331
    },
    {
      "id": 127798,
      "name": "Design Exhibit",
      "isParallelPresentation": true,
      "importedId": "d1e3f6d5-5b2d-404a-bfa6-590d741de96d",
      "typeId": 13124,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        121475,
        121476,
        121473,
        121478,
        121479,
        121474,
        121472,
        121480,
        121481,
        121482,
        121477
      ],
      "source": "SYS",
      "timeSlotId": 13174
    },
    {
      "id": 127799,
      "name": "Gadget Show",
      "isParallelPresentation": true,
      "importedId": "8693bd61-4ec6-4627-85d4-e6397bac4364",
      "typeId": 13124,
      "roomId": 11257,
      "chairIds": [],
      "contentIds": [
        129566
      ],
      "source": "SYS",
      "timeSlotId": 13523
    },
    {
      "id": 127800,
      "name": "Panel on Resilience in an Era of Destruction",
      "isParallelPresentation": false,
      "importedId": "e4ec172a-3e21-4e16-8107-b26bad38b3db",
      "typeId": 13122,
      "roomId": 11256,
      "chairIds": [],
      "contentIds": [
        129561
      ],
      "source": "SYS",
      "timeSlotId": 13487
    },
    {
      "id": 127801,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "84a7a296-1c07-4a54-99da-dbaed189a30f",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129574
      ],
      "source": "SYS",
      "timeSlotId": 13188
    },
    {
      "id": 127802,
      "name": "4A - Accessibility",
      "isParallelPresentation": false,
      "importedId": "46cf9b45-271d-432b-a935-5975927720d6",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        128540
      ],
      "contentIds": [
        128336,
        128349,
        128347,
        128360,
        128404,
        128302,
        128448,
        128449
      ],
      "source": "SYS",
      "timeSlotId": 13488
    },
    {
      "id": 127803,
      "name": "4B - Authentication",
      "isParallelPresentation": false,
      "importedId": "acb75660-681a-4009-a8cc-7b9c25e54538",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129525
      ],
      "contentIds": [
        128288,
        128380,
        128311,
        128372,
        128335,
        121831,
        121828,
        121763,
        128450,
        128451
      ],
      "source": "SYS",
      "timeSlotId": 13488
    },
    {
      "id": 127804,
      "name": "4C - Sensing, Tracking, and Recognition",
      "isParallelPresentation": false,
      "importedId": "95cca9d3-de95-49ca-a876-1adbd720343e",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        121686
      ],
      "contentIds": [
        128301,
        128269,
        128401,
        128423,
        128333,
        128413,
        121826,
        128452,
        128453
      ],
      "source": "SYS",
      "timeSlotId": 13488
    },
    {
      "id": 127805,
      "name": "Diversity Luncheon (only registered guests)",
      "isParallelPresentation": false,
      "importedId": "8ee512c2-1318-463d-aec2-354aaa069421",
      "typeId": 13123,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        129575
      ],
      "source": "SYS",
      "timeSlotId": 13189
    },
    {
      "id": 127806,
      "name": "5A -  Mood, Affect and Stress",
      "isParallelPresentation": false,
      "importedId": "a57d8140-3e61-4ec3-8929-211e913b26f9",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        129160
      ],
      "contentIds": [
        128319,
        128272,
        128273,
        128377,
        128337,
        128417,
        121806,
        128454,
        128455
      ],
      "source": "SYS",
      "timeSlotId": 13489
    },
    {
      "id": 127807,
      "name": "5B - Activity Recognition Methods",
      "isParallelPresentation": false,
      "importedId": "14a81887-d9c6-4dc2-b07d-31250e720289",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        129712
      ],
      "contentIds": [
        128331,
        128314,
        128313,
        128352,
        128400,
        121813,
        121781,
        121765,
        128456,
        128457
      ],
      "source": "SYS",
      "timeSlotId": 13489
    },
    {
      "id": 127808,
      "name": "5C - RF and Wireless",
      "isParallelPresentation": false,
      "importedId": "808222db-ed0b-4da4-8a72-1a9637595da3",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        129235
      ],
      "contentIds": [
        128363,
        128384,
        128391,
        128343,
        128370,
        128426,
        128458,
        128459
      ],
      "source": "SYS",
      "timeSlotId": 13489
    },
    {
      "id": 127809,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "2dd63aec-feea-4c40-8f05-7ae043ca3155",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129557
      ],
      "source": "SYS",
      "timeSlotId": 13191
    },
    {
      "id": 127810,
      "name": "6A - Personal Informatics",
      "isParallelPresentation": false,
      "importedId": "fab29209-39c8-4a79-9d6f-20ed05b28e11",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        129444
      ],
      "contentIds": [
        128293,
        128274,
        128279,
        128305,
        128283,
        128371,
        128392,
        128460,
        128461
      ],
      "source": "SYS",
      "timeSlotId": 13490
    },
    {
      "id": 127811,
      "name": "6B - Novel Sensing",
      "isParallelPresentation": false,
      "importedId": "1e04652a-9768-4d3a-a532-8e3abfb090d6",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        121688
      ],
      "contentIds": [
        128342,
        128379,
        128324,
        128278,
        128424,
        121767,
        121787,
        121792,
        128462,
        128463
      ],
      "source": "SYS",
      "timeSlotId": 13490
    },
    {
      "id": 127812,
      "name": "6C - Riding the (mm)Wave",
      "isParallelPresentation": false,
      "importedId": "2bfd1769-ab76-4f31-93d9-05cdd8e40b75",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        121535
      ],
      "contentIds": [
        128356,
        128276,
        128412,
        128410,
        128411,
        128407,
        128421,
        128464,
        128465
      ],
      "source": "SYS",
      "timeSlotId": 13490
    },
    {
      "id": 127813,
      "name": "Gala Dinner",
      "isParallelPresentation": false,
      "importedId": "6667b9e3-8ffb-4673-839c-987eda21cda0",
      "typeId": 13125,
      "roomId": 11345,
      "chairIds": [],
      "contentIds": [
        129562
      ],
      "source": "SYS",
      "timeSlotId": 13491
    },
    {
      "id": 127814,
      "name": "7A - Towards Good Health",
      "isParallelPresentation": false,
      "importedId": "3839af39-28ac-4278-8018-5d7c309efb36",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        121609
      ],
      "contentIds": [
        128419,
        128388,
        128284,
        128297,
        128361,
        128362,
        128466,
        128467
      ],
      "source": "SYS",
      "timeSlotId": 13492
    },
    {
      "id": 127815,
      "name": "3B - Motion Capture and Gestures",
      "isParallelPresentation": false,
      "importedId": "b028a545-85e0-4508-b1e3-bc350bd68133",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        121468
      ],
      "contentIds": [
        128320,
        128339,
        128309,
        128355,
        128374,
        128365,
        121764,
        121838,
        128468,
        128469
      ],
      "source": "SYS",
      "timeSlotId": 13486
    },
    {
      "id": 127816,
      "name": "7C - Sensing here, there, everywhere",
      "isParallelPresentation": false,
      "importedId": "dcfbce9e-4168-4572-a1ba-e228332318d4",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        128646
      ],
      "contentIds": [
        128318,
        128326,
        128405,
        128394,
        128358,
        121769,
        128470,
        128471
      ],
      "source": "SYS",
      "timeSlotId": 13492
    },
    {
      "id": 127817,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "3a2aefc8-1ef4-4ed0-b48e-f0904a11d565",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129569
      ],
      "source": "SYS",
      "timeSlotId": 13183
    },
    {
      "id": 127818,
      "name": "8A - Understanding Users",
      "isParallelPresentation": false,
      "importedId": "49031bac-f21b-409e-ad5b-e40b5913cfa3",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        128545
      ],
      "contentIds": [
        128418,
        128393,
        128277,
        128295,
        128334,
        128329,
        128472,
        128473
      ],
      "source": "SYS",
      "timeSlotId": 13493
    },
    {
      "id": 127819,
      "name": "1C - Mixed and Immersive Reality",
      "isParallelPresentation": false,
      "importedId": "7efdc489-4387-4354-916d-97afed358d64",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        128912
      ],
      "contentIds": [
        128383,
        128300,
        128422,
        128270,
        128414,
        121795,
        121825,
        121800,
        128474,
        128475
      ],
      "source": "SYS",
      "timeSlotId": 13484
    },
    {
      "id": 127820,
      "name": "8C - Privacy and Security",
      "isParallelPresentation": false,
      "importedId": "9dcfc7bd-4253-4ea2-b68b-ad2b59ebf87e",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        128726
      ],
      "contentIds": [
        128321,
        128346,
        128298,
        128420,
        128409,
        128402,
        128350,
        128476,
        128477
      ],
      "source": "SYS",
      "timeSlotId": 13493
    },
    {
      "id": 127821,
      "name": "UbiComp/ISWC TownHall",
      "isParallelPresentation": false,
      "importedId": "680307fb-e3a0-43fa-8a3e-8557a94c3c05",
      "typeId": 13122,
      "roomId": 11257,
      "chairIds": [],
      "contentIds": [
        129567
      ],
      "source": "SYS",
      "timeSlotId": 13184
    },
    {
      "id": 127822,
      "name": "3A -  Human-Centered",
      "isParallelPresentation": false,
      "importedId": "a8be3bae-89da-42e8-aec9-5470f8689ba8",
      "typeId": 12924,
      "roomId": 11257,
      "chairIds": [
        129708
      ],
      "contentIds": [
        128327,
        128340,
        128312,
        128415,
        121834,
        121804,
        128304,
        128478,
        128479
      ],
      "source": "SYS",
      "timeSlotId": 13486
    },
    {
      "id": 127823,
      "name": "9B - Smart, Mobile and Embedded",
      "isParallelPresentation": false,
      "importedId": "59edb359-eafb-4273-9433-385dcd708a65",
      "typeId": 12924,
      "roomId": 11258,
      "chairIds": [
        128517
      ],
      "contentIds": [
        128328,
        128322,
        128429,
        128366,
        128389,
        129690,
        129692,
        128306,
        128480,
        128481
      ],
      "source": "SYS",
      "timeSlotId": 13494
    },
    {
      "id": 127824,
      "name": "9C - Connecting sensing",
      "isParallelPresentation": false,
      "importedId": "634b5159-322e-4436-afc5-341a4cfe647d",
      "typeId": 12924,
      "roomId": 11259,
      "chairIds": [
        128608
      ],
      "contentIds": [
        128294,
        128386,
        128373,
        128390,
        128289,
        121794,
        129691,
        128482,
        128483
      ],
      "source": "SYS",
      "timeSlotId": 13494
    },
    {
      "id": 127825,
      "name": "Coffee Break",
      "isParallelPresentation": false,
      "importedId": "ceb2755a-6330-4834-9373-ac32002a2ed9",
      "typeId": 12928,
      "roomId": 11303,
      "chairIds": [],
      "contentIds": [
        129576
      ],
      "source": "SYS",
      "timeSlotId": 13186
    },
    {
      "id": 127826,
      "name": "Conference Closing",
      "isParallelPresentation": true,
      "importedId": "76ed6517-cf05-40d4-8530-cd7bb9c3d088",
      "typeId": 13122,
      "roomId": 11256,
      "chairIds": [],
      "contentIds": [
        129563
      ],
      "source": "SYS",
      "timeSlotId": 13495
    },
    {
      "id": 129528,
      "name": "Lunch Break",
      "isParallelPresentation": false,
      "importedId": "9b558ea4-23c7-4673-bda3-bb79cb13f60f",
      "typeId": 12928,
      "chairIds": [],
      "contentIds": [
        129559
      ],
      "source": "SYS",
      "timeSlotId": 13174
    },
    {
      "id": 129529,
      "name": "Lunch Break",
      "isParallelPresentation": false,
      "importedId": "d19b0052-c6d8-440a-9b7c-6c4c6eb9c497",
      "typeId": 12928,
      "chairIds": [],
      "contentIds": [
        129560
      ],
      "source": "SYS",
      "timeSlotId": 13189
    },
    {
      "id": 129530,
      "name": "Lunch Break",
      "isParallelPresentation": false,
      "importedId": "05d442c0-dbcd-4f6b-b3bb-dd2ae1012d83",
      "typeId": 12928,
      "chairIds": [],
      "contentIds": [
        129564
      ],
      "source": "SYS",
      "timeSlotId": 13184
    },
    {
      "id": 129531,
      "name": "Design Exhibit",
      "isParallelPresentation": true,
      "importedId": "af6dc372-83db-40e9-98eb-c773df153c48",
      "typeId": 13124,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        121475,
        121476,
        121473,
        121478,
        121479,
        121474,
        121472,
        121480,
        121481,
        121482,
        121477
      ],
      "source": "SYS",
      "timeSlotId": 13176
    },
    {
      "id": 129532,
      "name": "Design Exhibit",
      "isParallelPresentation": true,
      "importedId": "2f2fdda7-d60a-44f4-a522-c65a123210cf",
      "typeId": 13124,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        121475,
        121476,
        121473,
        121478,
        121479,
        121474,
        121472,
        121480,
        121481,
        121482,
        121477
      ],
      "source": "SYS",
      "timeSlotId": 13191
    },
    {
      "id": 129533,
      "name": "Design Exhibit",
      "isParallelPresentation": true,
      "importedId": "d15bd121-2760-4752-9833-6bf276ebf9d7",
      "typeId": 13124,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        121475,
        121476,
        121473,
        121478,
        121479,
        121474,
        121472,
        121480,
        121481,
        121482,
        121477
      ],
      "source": "SYS",
      "timeSlotId": 13189
    },
    {
      "id": 129534,
      "name": "Design Exhibit",
      "isParallelPresentation": true,
      "importedId": "159d038b-8261-4fe4-982b-021af9c7c3ad",
      "typeId": 13124,
      "roomId": 11269,
      "chairIds": [],
      "contentIds": [
        121475,
        121476,
        121473,
        121478,
        121479,
        121474,
        121472,
        121480,
        121481,
        121482,
        121477
      ],
      "source": "SYS",
      "timeSlotId": 13331
    },
    {
      "id": 129535,
      "name": "Award Ceremony",
      "isParallelPresentation": true,
      "importedId": "633513d7-b36b-4eff-b820-e4260c17bd15",
      "typeId": 13126,
      "roomId": 11345,
      "chairIds": [],
      "contentIds": [
        129558
      ],
      "source": "SYS",
      "timeSlotId": 13491
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 121472,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Privee: A Wearable for Real-Time Bladder Monitoring System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1060",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Urinary incontinence (UI) is a prevalent condition affecting millions of individuals worldwide, leading to various physical, social, and psychological challenges that diminish their quality of life. Current management approaches primarily focus on containment rather than proactive monitoring and warning systems. This paper presents the development and evaluation of a novel wearable technology called Privee, designed as an unobtrusive undergarment to monitor bladder fullness in real-time. Privee utilizes e-textile-based bioimpedance spectroscopy technology, which noninvasively assesses bladder fullness by analyzing the electrical properties of body tissues and fluids. The undergarment incorporates eight embroidered electrodes and textile transmission lines seamlessly integrated into the fabric. By continuously monitoring the bioimpedance signals from the bladder, Privee provides real-time information about the bladder's fullness level. This data is processed using a specialized algorithm to estimate the need for urination. The noninvasive nature of Privee eliminates the discomfort and risks associated with invasive monitoring methods, offering a user-friendly and convenient solution for individuals with UI, overactive bladder, or post-operative care needs. This innovative technology has the potential to improve patients' quality of life and optimize healthcare costs associated with UI management.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 121467
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 121453
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 121433
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 121445
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Design"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Design"
            }
          ],
          "personId": 121434
        }
      ]
    },
    {
      "id": 121473,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Social Prosthesis: Social Interaction Through 3D Dynamic Makeup",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1050",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Prosthetic makeup is the use of prosthetic materials for cosmetic or makeup effects to extend the skin and features. Commonly used to simulate wounds or exaggerate physical characteristics, prosthetic makeup is usually created for film or theatrical purposes, rather than for everyday fashion or social wearability. Social Prosthesis is a design project which aims to introduce interactivity, movement, and aesthetic within silicone prosthetics by providing design considerations and fabrication techniques unique to on-face wearables. Through opening up opportunities for cosmetic expression and storytelling through dynamic makeup, Social Prosthesis invokes the sociality of beauty—the change and movement that happens when we alter our appearances in contact with others.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Hybrid Body Lab"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 121457
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 121452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Cornell University"
            }
          ],
          "personId": 121441
        }
      ]
    },
    {
      "id": 121474,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Reconfigurable, Adhesive-Free, Wearable Skin Strain Device",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1059",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Skin-strain – the act of stretching the skin – is an interesting (but generally understudied) mode of haptic stimulation. Producing artificial skin strain on the body using a wearable device requires a clear understanding of the body product relationship between the device and the user. We present a wearable device capable of creating dozens of unique skin strain experiences on the wearer through a novel shape memory alloy (SMA) actuator + reconfigurable hook-and-eye attachment architecture. Not only can this architecture create spatially- and temporally-customizable skin strain experiences, it does so without the use of temporary / permanent adhesives (a typical limitation of other skin strain device designs). We present the iterative design process from early working prototypes to the most recently developed devices, including the underlying design criteria and decisions",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121447
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St. Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121438
        }
      ]
    },
    {
      "id": 121475,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Breezy the Calm Monster: Soft Toy Design Combined with Pervasive Technology to Teach Deep Breathing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1049",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Anger dysregulation can lead to aggression and peer rejection in early childhood. Anger episodes and temper tantrums frequently escalate, leaving parents and young children needing effective emotion regulation interventions. This research investigates how designing a playful interaction with ubiquitous technology can help parents teach young children emotion regulation strategies such as deep breathing. Breezy is a soft toy with an app and a storybook that helps parents and engages them in teaching deep breathing as an anger regulation strategy. The soft toy has sensory and physical features and can be held as a puppet, which helps young children to connect with the toy. The digital app addresses facial expressions and physiological components of emotions through sensory and interactive features that encourage identifying emotions. The storybook facilitates a familiar context for parents' engagement through playful learning. The interactive system provides parents with opportunities to teach their young children to identify and label anger and practice and model age-appropriate breathing exercises. This research aims to inform the design of future educational and playful interactions for emotional competence skills, as well as to broaden their application and promote their use in homes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington ",
              "institution": "Victoria University of Wellington ",
              "dsl": "Industrial Design/ School of Design Innovation/ Smart Interactions Lab"
            },
            {
              "country": "Saudi Arabia",
              "state": "",
              "city": "Jeddah",
              "institution": "King AbdulAziz University",
              "dsl": "Faculty of Human Sciences and Design"
            }
          ],
          "personId": 121443
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "School of Design"
            }
          ],
          "personId": 121451
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Psychology"
            }
          ],
          "personId": 121471
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "Architecture and Design"
            }
          ],
          "personId": 121440
        }
      ]
    },
    {
      "id": 121476,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Designing Dissolving Wearables",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1017",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Bio-based materials facilitate the development of more sustainable devices and wearables, expanding the range of design possibilities beyond conventional materials. Our work with biofoam explores one such quality, dissolving, as a unique affordance for designing and interacting with wearables. We developed techniques to make biofoam yarns, and used them to craft three wearables: \"Seasonal Footwear,\" a \"Reveal Bralette,\" and an \"Unfolding Lace Top.\" These wearables incorporate sections that dissolve in water, allowing customization to suit the user's needs. These wearables illustrate short-term use cases, such as a one-time reveal or shape change. We explore this novel design space as sustainable ephemeral fashion, where bio-based dissolving materials enable revealing, transformative, and interactive functionalities.\r\n\r\n\r\n\r\n\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 121461
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 121431
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 121465
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 121442
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Intermedia Art Writing and Performance"
            }
          ],
          "personId": 121455
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute & Information Science"
            }
          ],
          "personId": 121462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute & Computer Science"
            }
          ],
          "personId": 121444
        }
      ]
    },
    {
      "id": 121477,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Haikeus:  Transmuting Ecological Grieving into Action",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1066",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Combined with the unprecedented stress of the COVID-19 crisis and the increase in social unrest, human-caused environmental disasters are having a profound impact on well-being, resulting in a dramatic spike in mental health issues. Studies are emerging daily around concepts of ecological grieving stress, depression, anxiety, and a host of emotions that are surfacing and increasing in our modern times. From eco-nostalgia to eco-anxiety and eco-grief, our responses to climate change, environmental devastation, and social unrest can prevent us from taking positive action, often leading to existential crises. Our proposed project, Haikeus: Transmuting Ecological Grieving into Action, works directly at the interface of some of humanity’s wicked problems, which are complex, challenging to solve, and hard to fully understand. The aim of this project is to bring awareness and motivation for transmuting such an emotion into an action through the power of creativity. We further argue that the established methods could facilitate a more nuanced understanding of organizational barriers to communicate its potential value to proceed with the change.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Phoenix",
              "institution": "ASU",
              "dsl": "Herberger School of Art"
            }
          ],
          "personId": 121450
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": "School of Music,Dance and Theatre"
            }
          ],
          "personId": 121466
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "Arizona State University",
              "dsl": ""
            }
          ],
          "personId": 121470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Arizona",
              "city": "Tempe",
              "institution": "ASU",
              "dsl": "Herberger Institute of design and the arts"
            }
          ],
          "personId": 121427
        }
      ]
    },
    {
      "id": 121478,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Plug-and-Play Wearables: A Repositionable E-Textile Garment System to Support Custom Fit for Lower-Limb Rehabilitation Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1056",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "Fit of an e-textile garment has become increasingly important as more advanced wearable technology applications demand precise placement of sensors and actuators. Human anthropometry is complex and varied, and for many applications, a single e-textile garment cannot accurately fit a variety of users. One such example is lower-limb rehabilitation applications, which rely on precise placement of sensors and actuators with respect to joints and muscles for each individual user. Here, we present the development of a multi-layer, stretchable, flexible e-textile system, which affords quick and easy repositioning of components on the garment surface. Beyond custom fitting, this infrastructure also affords dynamic functionality of the garment, by adding and replacing sensors and actuators to enable a wide variety of applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St Paul",
              "institution": "University of Minnesota",
              "dsl": "Department of Design, Housing, and Apparel"
            }
          ],
          "personId": 121435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 121437
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St. Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121438
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121428
        }
      ]
    },
    {
      "id": 121479,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "MoCa’Collection: Normalizing Dynamic Textile Geometry with Capacitive Sensing in Design Centric Wearables",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1058",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "In this work, we promote capacitive sensing as a versatile smart textile modality through a collection of functional wearable designs. Considering the large variety of possible garment design concepts, we outline an approach to implement smart sensing technology into garments while maintaining these diverse design possibilities. After introducing the basic functionalities of capacitive sensing and the process of designing and building a smart garment, we present an assortment of garments enabled by this technology within the MoCa’Collection. Each of the projects serves a different purpose, built by people representing different backgrounds from electrical engineers, computer scientists, digital artists to smart fashion designers, starting from technical design over digital art to our latest design of a strongly design-oriented full-body capturing suit implementing the proposed technology.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121436
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Berlin",
              "city": " Berlin,",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 121426
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 121439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121460
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence (DFKI) GmbH",
              "dsl": ""
            }
          ],
          "personId": 121432
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "German Research Centerfor Artificial Intelligence(DFKI)",
              "dsl": ""
            }
          ],
          "personId": 121449
        }
      ]
    },
    {
      "id": 121480,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Kirigami Antennas",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1062",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "With the progression of technology as integrated into daily life, physical tech has become increasingly embedded or hidden from the user's view. Because of this design change, many of the aesthetics that previously defined everyday technology have disappeared from the public eye. Our ability to connect the capability of technology with the spacial world it utilizes has disappeared with it. This notion is especially true for antenna design. An object that was once visible on cars, houses, and phones is now so embedded in the devices that use it that its technology is essentially formless. Kirigami Antennas is a research exploration centering on e-textile meta-materials, designing antennas that are cognizant of their use and relation to space. This collection of antennas are not embedded nor hidden. They instead borrow from the art of Kirigami to re-insert themselves into the 3D space from which they receive their signals.\r\nThrough experimentation with Kirigami antenna shapes, we are able to design freestanding lace antennas that effectively received electromagnetic signals at a wide range of frequencies, picking up AM, FM, and HAM radio, along with other data transmissions. Kirigami Antennas provides a space for experimentation with antennas as objects that help us reach and search through space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Digital Arts & Experimental Media (DXARTS)"
            }
          ],
          "personId": 121456
        }
      ]
    },
    {
      "id": 121481,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "Weaving Augmented Reality Markers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1063",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "This paper presents the use of weaving as a technique to create functional augmented reality (AR) markers using different textile structures and colors. We conducted experiments with plain, twill, and satin weaves, as well as varying colors in the warp, to test the effectiveness of the markers. Our findings show that weaving is a viable method for creating AR markers, and the software can detect markers even with varying colors and slightly misaligned quadrants. This work opens up new possibilities for weaving and textile structures in AR design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121464
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 121461
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute"
            }
          ],
          "personId": 121448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Intermedia Art Writing and Performance"
            }
          ],
          "personId": 121455
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "ATLAS Institute & Information Science"
            }
          ],
          "personId": 121462
        }
      ]
    },
    {
      "id": 121482,
      "typeId": 13124,
      "durationOverride": 90,
      "title": "BioSparks: Jewelry as  Electrochemical Sweat Biosensors with Modular, Repurposing and Interchangeable Approaches",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23d-1065",
      "source": "PCS",
      "trackId": 12288,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127798,
        129531,
        129532,
        129533,
        129534
      ],
      "eventIds": [],
      "abstract": "This paper presents BioSparks, a wearable device that detects glucose levels in sweat through electrochemical biosensors created with traditional jewelry techniques. It incorporates interchangeable electrodes that facilitates their replacement after timelife, and employs a repurposing method to reuse the discarded electrodes within the jewelry's chain. The modular design enables the wearable to be placed on various body parts, including the neck, wrist and waist. The paper outlines our design considerations for Wearability Factors for Jewelry Biosensors, and the fabrication process combining traditional jewelry techniques and electromistry. Our technical evaluation shows the performance of our biosensor under ten different glucose concentrations. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": ""
            }
          ],
          "personId": 121458
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California, Davis",
              "dsl": "Design"
            }
          ],
          "personId": 121459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "University of California",
              "dsl": "Department of Design "
            }
          ],
          "personId": 121463
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "UC Davis",
              "dsl": "Department of Design"
            }
          ],
          "personId": 121430
        }
      ]
    },
    {
      "id": 121763,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "User Authentication Method for Wearable Ring Devices using Active Acoustic Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-9486",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "The ring-type devices currently on the market are not typically equipped with authentication. This paper proposes an automatic and continuous user authentication method using active acoustic sensing with a speaker and microphone mounted on the ring device. The proposed method authenticates the wearer by extracting the fre-quency response of the acoustic signal acquired by the microphone and calculating the similarity between the frequency response of the current wearer and that of a pre-registered individual. It takes advantage of the fact that the ring device is in constant contact with the finger and that the shape and composition of each user’s finger have unique acoustic characteristics. The ring device was created by fixing a flexible piezoelectric element to a 3D-printed ring and was evaluated using seven participants in two states: a relaxing hand and a gripping hand. The average EER was 0.034 for the relaxing hand and 0.027 for the gripping hand.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121706
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121569
        }
      ]
    },
    {
      "id": 121764,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Selecting the Motion Ground Truth for Loose-fitting Wearables: Benchmarking Optical MoCap Methods",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-7420",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "To help smart wearable researchers choose the optimal ground truth methods for motion capturing (MoCap) for all types of loose garments, we present a benchmark, DrapeMoCapBench (DMCB), specifically designed to evaluate the performance of optical marker-based and marker-less MoCap. High-cost marker-based MoCap systems are well-known as precise golden standards. However, a less well-known caveat is that they require skin-tight fitting markers on bony areas to ensure the specified precision, making them questionable for loose garments. On the other hand, marker-less MoCap methods powered by computer vision models have matured over the years, which have meager costs as smartphone cameras would suffice. To this end, DMCB uses large real-world recorded MoCap datasets to perform parallel 3D physics simulations with a wide range of diversities: six levels of drape from skin-tight to extremely draped garments, three levels of motions and six body type - gender combinations to benchmark state-of-the-art optical marker-based and marker-less MoCap methods to identify the best-performing method in different scenarios. In assessing the performance of marker-based and low-cost marker-less MoCap for casual loose garments both approaches exhibit significant performance loss (>10cm), but for everyday activities involving basic and fast motions, marker-less MoCap slightly outperforms marker-based MoCap, making it a favorable and cost-effective choice for wearable studies. The code is available at github.com/lalasray/DMCB/.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121460
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "Rheinland-Pfalz",
              "city": "Kaiserslautern",
              "institution": "RPTU Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "Rheinland-Pfalz",
              "city": "Kaiserslautern",
              "institution": "RPTU Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "Rheinland-Pfalz",
              "city": "Kaiserslautern",
              "institution": "RPTU Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121765,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Evaluating Spiking Neural Network on Neuromorphic Platform for Human Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-1361",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "Energy efficiency and low latency are crucial requirements for designing wearable AI-empowered human activity recognition systems, due to the hard constraints of battery operations and closed-loop feedback. While neural network models have been extensively compressed to match the stringent edge requirements, spiking neural networks and event-based sensing are recently emerging as promising solutions to further improve performance due to their inherent energy efficiency and capacity to process spatiotemporal data in very low latency. This work aims to evaluate the effectiveness of spiking neural networks on neuromorphic processors in human activity recognition for wearable applications. The case of workout recognition with wrist-worn wearable motion sensors is used as a study. A multi-threshold delta modulation approach is utilized for encoding the input sensor data into spike trains to move the pipeline into the event-based approach. The spikes trains are then fed to a spiking neural network with direct-event training, and the trained model is deployed on the research neuromorphic platform from Intel, Loihi,  to evaluate energy and latency efficiency. Test results show that the spike-based workouts recognition system can achieve a comparable accuracy (87.5\\%) comparable to the popular milliwatt RISC-V bases multi-core processor GAP8 with a traditional neural network ( 88.1\\%) while achieving two times better energy-delay product (0.66 \\si{\\micro\\joule\\second} vs. 1.32 \\si{\\micro\\joule\\second}). ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürih",
              "dsl": ""
            }
          ],
          "personId": 121572
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "D-ITET",
              "dsl": "PBL"
            }
          ],
          "personId": 121577
        }
      ]
    },
    {
      "id": 121766,
      "typeId": 13124,
      "title": "SonarAuth: Using Around Device Sensing to Improve Smartwatch Behavioral Biometrics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1062",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Smartwatches are used by millions of people for applications in health, finance, and communication. As the computational power and range of applications supported by these devices expand, it is becoming more and more important to secure access to them. While various user authentication technologies have been extensively explored in smartphone use scenarios (e.g., FaceID, fingerprint, PIN, or pattern) the applicability of these approaches to smartwatches is typically limited due to the small watch form factor. To improve authentication on smartwatches, we propose SonarAuth, a novel user authentication system for unmodified commercial smartwatches using behavioral biometrics derived from motion, touch, and around-device motions. To capture in-air hand motions, we adapted an existing sonar system for smartwatches. We collected data from 24 participants from single touch to the watch screen with the thumb, index, and middle fingers. Using a multimodal deep learning classifier, we achieved a promising mean Equal Error Rate(EER) of 6.41% for user authentication based on a single thumb tap. We note that our system is usable and has good potential to be combined with other authentication modalities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": ""
            }
          ],
          "personId": 121626
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": ""
            }
          ],
          "personId": 121624
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ulsan",
              "institution": "UNIST",
              "dsl": ""
            }
          ],
          "personId": 121523
        }
      ]
    },
    {
      "id": 121767,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Modeling and Evaluation of Soft Force Sensors using Recurrent and Feed-Forward Neural Networks and Exponential Methods to Compensate for Force Measurement Error in Curved Conditions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-3387",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "A wide variety of applications of wearable technology require information about forces and pressures exerted on the body, either by a device (e.g. to sense active wearing periods or to provide feedback to a force-sensitive therapy like compression) or by other objects or body parts (e.g. for bedsore prevention or force-based gait monitoring). However, typical force sensing mechanisms are often difficult to translate to the wearable environment because the geometry and mechanics of body tissues introduce error into the sensor response. Previous studies have shown that soft force sensors are significantly affected by deformation leading to erroneous force measurement, but no effort has been made yet to rectify force data estimated by soft textile-based sensors under deformed conditions. In this study, we model the responses of three low-cost textile-based sensors and one off-the-shelf force-sensitive resistor using an Exponential model, a Recurrent Neural Network (RNN) and a Multi-Layer Perceptron (MLP) Network. Results show that RNN outperforms in modelling hysteresis with an RMSE of 4.2%. Further, we evaluate sensor performance in human-like curvatures, and refine the models by fusing the degree of curvature. Results show that the refined models can reduce measurement error from 46% to 6.8% and from 26.33 to 1.06% in some cases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 121437
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St. Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121438
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121428
        }
      ]
    },
    {
      "id": 121768,
      "typeId": 12924,
      "title": "Learning Effects and Retention of Electrical Muscle Stimulation in Piano Playing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-5486",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "Electrical muscle stimulation (EMS)-based systems have been proposed to assist in the learning of motor skills for piano playing. However, learning effects and retention have not been thoroughly evaluated. To address this, we conducted two user studies to investigate the learning effects and retention of EMS for piano playing. Twenty-four novice participants practiced the technique of tremolo, a rapid change between two notes, with both hands under three conditions: without EMS, with EMS on one hand, and with EMS on both hands. The results showed that practicing with EMS on both hands significantly improved tempo accuracy compared to practicing without EMS. A follow-up study of 15 participants confirmed that the improved performance achieved with EMS on both hands was maintained after one week and was not significantly different from practicing without EMS.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 121634
        }
      ]
    },
    {
      "id": 121769,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Estimating Attention Allocation by Electrodermal Activity",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-1286",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "Electrodermal activity (EDA) represents changes in the electrical activity of the palmar skin and serves as an indicator of sympathetic nervous system activity. This paper presents a novel method for estimating attention allocation under divided attention conditions using only EDA data. Our approach involves the use of the low-frequency power spectrum derived from the phasic component of EDA associated with attentional focus, combined with a machine learning classification model. We conducted three user studies aimed at estimating participants' attention allocation during the performance of simple tasks under both visual and auditory stimuli where the frequencies of the stimuli were different, identical, or ambiguous. The goal was to estimate whether participants focused on visual or auditory stimuli. The results showed that our method could estimate attention allocation with the accuracy of 96% and 73% when the frequencies of the two stimuli were different and ambiguous, respectively, and could not estimate when the frequencies were identical.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 121634
        }
      ]
    },
    {
      "id": 121770,
      "typeId": 13124,
      "title": "SleepABP: Noninvasive Ambulatory Blood Pressure Monitoring Based on Ballistocardiogram in Sleep State",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1061",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Blood pressure is one of the vital signssignals used to evaluate the health status of the human body, a. And continuous ambulatory blood pressure (ABP) monitoring can accurately and comprehensively reflect the physiological status of the cardiovascular system. ABP is mainly measured using ambulatory sphygmomanometer, by regularly collecting the pressure changes in blood vessels.. However, this method is usually intrusive, inconvenient, and expensive. In this paper, we develop SleepABP, a novel natural, continuous and accurate ABP monitoring system for daily sleep scenarios. SleepABP is based on the Ballistocardiogram (BCG) signal, which relies on measuring the tiny fluctuations (impulses) in the body caused by the continuous beating of the heart. Specifically, the proposed BCG sensing system monitors the pulses from the user’s head applying the piezoelectric ceramic sensor. The BCG signals are recorded simultaneously, and the BCG cycle restoration algorithm is used to extract the continuous and complete BCG cycles for constructing the multi-dimensional features of the blood pressure prediction. SleepABP was evaluated on 16 participants under various sleeping conditions, positions, and personal internal factors. The RMSE results of diastolic blood pressure 3.22 mmHg and systolic blood pressure 3.5 mmHg demonstrates the system's effectiveness and feasibility in a daily sleeping environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical  University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical  University",
              "dsl": ""
            }
          ],
          "personId": 121692
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University",
              "dsl": ""
            }
          ],
          "personId": 121733
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xi’an",
              "institution": "Northwestern Polytechnical  University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xi’an",
              "institution": "Northwestern Polytechnical  University",
              "dsl": ""
            }
          ],
          "personId": 121690
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Institute of Technology",
              "dsl": ""
            },
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121549
        }
      ]
    },
    {
      "id": 121771,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Input Interface with Touch and Non-touch Interactions using Atmospheric Pressure for Hearable Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-1120",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "A hearable device is a wearable computer that is worn on the ear. In addition to offering conventional music listening functions when utilized as earphones, a hearable device can be linked to smartphones and various onboard sensors to recognize user actions and voice assistants. Although some devices recognize command operations when the earpiece is touched with a hand directly, there are limitations related to the shape of the earpiece and the problem of false recognition that occurs when the earpiece is touched unintentionally. Hands-free input methods utilizing voice assistants and acceleration sensors that measure head movement are available, but these run into problems such as low command recognition accuracy due to noise in public spaces and low social acceptability. In this study, we implement a device that measures the atmospheric pressure in the ear canal and around the ear by installing an atmospheric pressure sensor inside canal-type earphones. We propose a method that recognizes 12 types of gesture based on the pattern of pressure change caused by pressing and releasing the earphone with a finger (touch interaction) and by covering the auricle with a hand and putting pressure on it (non-touch interaction). In our method, six types of gesture are performed with two different interaction methods. We evaluated the recognition accuracy of each gesture by having five participants perform each gesture 50 times. Our findings showed that the ``quick press and quick release'' gestures were recognized with 0.99 accuracy for touch and 0.82 for non-touch.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121603
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121569
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 121709
        }
      ]
    },
    {
      "id": 121772,
      "typeId": 12924,
      "title": "Wireless Sensor Collar for Automatic Recognition of Canine Agility Activities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-6255",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "Canine agility is a rapidly growing sport where dogs and their handlers navigate obstacle courses. Recent studies show that over 40% of all agility dogs suffer an injury while training or competing. By collecting and analyzing sensor measurements from a wearable computer while dogs participate in the sport, we hope to better inform dog handlers and trainers and improve the performance and overall health of their dogs. As a first step towards this long-term project goal, we present the initial validation of a bespoke collar-worn activity tracker and machine learning classifier for recognizing agility activities. The ability to classify agility activities from collar-worn sensors will provide the groundwork for further analysis of relative activity exertion levels, activity variance with repetitions, and gait regularity. To validate our system, we conducted a pilot study of six dogs performing a short agility course including three different agility obstacles. Our wireless sensor collar was able to provide data in real time via WiFi while dogs navigated the obstacles. Our MINIROCKET-based machine learning classifier achieved 85% accuracy across the pilot study data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121503
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121674
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121694
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121585
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121495
        }
      ]
    },
    {
      "id": 121773,
      "typeId": 13124,
      "title": "WiCross: I Can Know When You Cross Using COTS WiFi Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1023",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Detecting whether a target crosses the given zone (e.g., a door) can enable various practical applications in smart homes, including intelligent security and people counting. The traditional infrared-based approach only covers a line and can be easily cracked. In contrast, reusing the ubiquitous WiFi devices deployed in homes has the potential to cover a larger area of interest as WiFi signals are scattered throughout the entire space. By detecting the walking direction (i.e., approaching and moving away) with WiFi signal strength change, existing work can identify the behavior of crossing between WiFi transceiver pair. However, this method mistakenly classifies the turn-back behavior as crossing behavior, resulting in a high false alarm rate. In this paper, we propose WiCross, which can accurately distinguish the turn-back behavior with the phase statistics pattern of WiFi signals and thus robustly identify whether the target crosses the area between the WiFi transceiver pair. We implement WiCross with commercial WiFi devices and extensive experiments demonstrate that WiCross can achieve an accuracy higher than 95% with a false alarm rate of less than 5%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Peking University",
              "dsl": ""
            }
          ],
          "personId": 121747
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Peking University",
              "dsl": ""
            }
          ],
          "personId": 121586
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Peking University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Beijing Xiaomi Mobile Software Company Ltd.",
              "dsl": ""
            }
          ],
          "personId": 121613
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Peking University",
              "dsl": ""
            }
          ],
          "personId": 121752
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Peking University",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Institut Polytechnique de Paris",
              "dsl": "Telecom SudParis"
            }
          ],
          "personId": 121640
        }
      ]
    },
    {
      "id": 121774,
      "typeId": 13124,
      "title": "Active 3D Mapping Leveraging Heterogeneous Crowd Robot based-on Reinforcement Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1067",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "The paper demonstrates that the introduction of heterogeneous crowd robots improves the accuracy and completeness of indoor 3D mapping. First, three ground robots (sweeping robot, inspection robot, and guidance robot) are used to perform active exploration and mapping tasks in the iGibson indoor environment. However, the mapping results of the ground robot reveal that there are obvious blind spots in key areas (such as tables, stoves, and beds), resulting in the lack of point cloud data. To overcome this challenge, we introduce a drone for active 3D mapping using its bird's eye view and powerful perception capabilities. Experimental results show that by introducing drones, we have successfully eliminated the blind areas of vision existing in-ground robot mapping and achieved more comprehensive and accurate mapping results. This demonstration fully demonstrates the advantages of introducing heterogeneous crowd robots, and how the complementary capabilities of different types of robots can work together to improve the indoor 3D mapping process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "School of Informatics"
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 121504
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            }
          ],
          "personId": 121649
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            }
          ],
          "personId": 121728
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            }
          ],
          "personId": 121748
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            }
          ],
          "personId": 121754
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": ""
            }
          ],
          "personId": 121528
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Fujian",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "School of Informatics"
            },
            {
              "country": "China",
              "state": "Fujian",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 121743
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "China",
              "state": "",
              "city": "Xiamen",
              "institution": "Xiamen University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121627
        }
      ]
    },
    {
      "id": 121775,
      "typeId": 13124,
      "title": "Investigating Mobile Mental Health App Designs to Foster Engagement Among Adolescents",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1069",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "We identify features of mood-tracking apps for managing mental health that foster engagement and sustained use by adolescents—a population that expresses a preference for digital apps over face-to-face support, yet demonstrates low levels of engagement with such apps. We developed a prototype of an adolescent-focused mood- tracking app, informed by literature about existing apps’ approaches to recording patients’ symptoms, the role of data representations in long-term mental health management, and the potential benefits of peer support tools. We then conducted a survey (n = 88) to assess adolescents’ preferences for various aspects of this prototype. We found that participants prefer tools for self-reflection and self-awareness over those for gamification or social support, and that they value function over entertainment when choosing wellness apps, especially among participants who disclosed a history of managing mental health. Qualitative analysis of open-ended responses revealed that customization and self-reflection are important design themes. Our findings have implications for the design of mental health apps that cater to the specific needs and preferences of adolescent users.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Lafayette",
              "institution": "Peak to Peak Charter School",
              "dsl": ""
            }
          ],
          "personId": 121717
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Department of Information Science"
            }
          ],
          "personId": 121715
        }
      ]
    },
    {
      "id": 121776,
      "typeId": 13124,
      "title": "Demonstrating AHA: Boosting Unmodified AI's Robustness by Proactively Inducing Favorable Human Sensing Conditions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1025",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Imagine a near-future smart home. Home-embedded visual AI sensors continuously monitor the resident, inferring her activities and internal states that enable higher-level services. Here, as home-embedded sensors passively monitor a free person, good inferences happen inconsistently. The inferences' confidence highly depends on how congruent her momentary conditions are to the conditions favored by the AI models, e.g., front-facing or unobstructed.\r\nWe envision new strategies of AI-to-Human Actuation (AHA) that boost the sensory AI's robustness by inducing favorable conditions from the person with proactive actuations. To demonstrate our concept, in this demo, we show how the inference quality of the AI model changes relative to the person's conditions and introduce possible actuations, used in our full paper experiments, that could drive more favorable conditions for visual AIs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 121598
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 121496
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 121699
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121688
        }
      ]
    },
    {
      "id": 121777,
      "typeId": 13124,
      "title": "CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time on Edge Hand Gesture Recognition for Drone Control",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1102",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "We present CaptAinGlove, a textile-based, low-power ($\\leq 1.15 Watts$), privacy-conscious, real-time on-the-edge (RTE) glove-based solution with a tiny memory footprint ($\\le 2MB$), designed to recognize hand gestures used for drone control.\r\nWe employ lightweight convolutional neural networks as the backbone models and a hierarchical multimodal fusion to reduce power consumption and improve accuracy.\r\nThe system yields an F1-score of 80\\% for the offline evaluation of nine classes; eight hand gesture commands and null activity.\r\nFor the RTE, we obtained an F1-score of 67\\% (one user).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 121439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technical Univeristy of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121436
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121460
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121778,
      "typeId": 13124,
      "title": "Mazi Umntanakho “Know Your Child”: An Accessible Social-Emotional Assessment Tool for Children in Low-Income South African Communities",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1101",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Ensuring proper developmental and behavioral assessments for children is crucial, particularly in contexts where resources are scarce, developmental delays carry a stigma, and specialists are lacking. However, adequate and culturally appropriate methods for screening and assessing children in the Majority World still need to be improved. In this context, mobile technology can be crucial in providing accessible assessments and screenings that better fulfill the needs. In this work, we present the design and development of Mazi Umntanakho \"Know Your Child,\" a conversational agent on WhatsApp that supports South African home visitors in assessing and tracking children's socio-emotional skills. The agent is based on the Strengths and Difficulties Questionnaire (SDQ) and International Development and Early Learning Assessment (IDELA) to provide culturally appropriate assessments. The agent is personalized to user preferences and language and provides feedback and advice in various multimedia formats, including infographics, video, audio, and photos.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UCI",
              "dsl": "Informatics"
            }
          ],
          "personId": 121583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Department of Electrical Engineering and Computer Science"
            }
          ],
          "personId": 121731
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "UCI Department of Informatics"
            }
          ],
          "personId": 121614
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": ""
            }
          ],
          "personId": 121718
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California - Irvine",
              "dsl": ""
            }
          ],
          "personId": 121664
        },
        {
          "affiliations": [
            {
              "country": "South Africa",
              "state": "",
              "city": "Johannesburg",
              "institution": "University of Witwatersrand",
              "dsl": "Developmental Pathways for Health Research Unit"
            }
          ],
          "personId": 121684
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Orange",
              "institution": "Chapman University",
              "dsl": "Fowler School of Engineering"
            }
          ],
          "personId": 121551
        },
        {
          "affiliations": [
            {
              "country": "South Africa",
              "state": "",
              "city": "Johannesburg",
              "institution": "University of the Witwatersrand",
              "dsl": "SAMRC/Wits Developmental Pathways for Health Research Unit, School of Clinical Medicine, Faculty of Health Sciences"
            }
          ],
          "personId": 121513
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Riverside",
              "institution": "University of California Riverside",
              "dsl": "Psychiatry & Neuroscience"
            }
          ],
          "personId": 121501
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Informatics"
            }
          ],
          "personId": 121661
        }
      ]
    },
    {
      "id": 121779,
      "typeId": 13124,
      "title": "Anticipatory Hand Glove: Understanding Human Actions for Enhanced Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1068",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Perceived real-time interactions between humans and a metaverse often require synchronizing actions in the virtual and real world. Latency is a main blocking factor due to processing and transmitting data over long distances because cloud data centres that deploy metaverse locate far away. We present an anticipatory computing solution to predict precisely human actions grasping objects based on hand kinesthetics leveraging smart gloves with IMU and flexion sensors. We demonstrated that the gain from the anticipation of up to several hundred milliseconds could compensate for computing and transmission latency, enabling immersive interactions over extremely long distances. Our solution is flexible and is free from locking into particular smart glove vendors, making it easy for future extensions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "TU Dresden",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Deutsche Telekom Chair of Communication Networks",
              "dsl": ""
            }
          ],
          "personId": 121742
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Deutsche Telekom Chair of Communication Networks",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "TU Dresden",
              "dsl": ""
            }
          ],
          "personId": 121592
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Haptic Communication Systems",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Centre for Tactile Internet with Human-in-the-Loop (CeTI)",
              "dsl": ""
            }
          ],
          "personId": 121725
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Deutsche Telekom Chair of Communication Networks",
              "dsl": ""
            },
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Danang",
              "institution": "The University of Danang - Vietnam-Korea University of Information and Communication Technology",
              "dsl": ""
            }
          ],
          "personId": 121556
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Mount Peasant",
              "institution": "Department of Computer Science at Central Michigan University",
              "dsl": ""
            }
          ],
          "personId": 121682
        }
      ]
    },
    {
      "id": 121780,
      "typeId": 13124,
      "title": "Tangible E-Textile Interactive Interface for Digital Patternmaking ",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1107",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "A common method of generating a garment pattern is by draping fabric on a 3D form. Many apparel designers prefer this tangible process rather than the more abstract process of drafting patterns in 2D using CAD software, but the visualization benefits of 3D rendering remain important to product development. Currently, draped patterns must be manually digitized to transfer the shape to the CAD environment, a cumbersome process. Here, we explore augmenting the fabric used to drape patterns with e-textile components, such that pinning the fabric to the form directly digitizes the pattern shape. The e-textile interface uses a keypad matrix approach with rows and columns on opposite sides of the fabric. Passing a metallic pin through a row/column intersection creates a circuit connection that a microcontroller reads and maps to a 2D spatial layout on the screen. The proof-of-concept device developed here is a first step toward an automatic digitizing system that allows designers to preserve manual skills and processes while integrating more efficiently with digital systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121428
        }
      ]
    },
    {
      "id": 121781,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "How much Unlabeled Data is Really Needed for Effective Self-Supervised Human Activity Recognition?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-2938",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "The prospect of learning effective representations from unlabeled data alone has led to a boost in developing self-supervised learning (SSL) methods for sensor-based Human Activity Recognition (HAR). Typically, (large-scale) unlabeled data are used for pre-training, with the learned weights being used as feature extractors for recognizing activities. While prior works have focused on the impact of increased data scale on performance, instead, we aim to discover the pre-training data efficiency of self-supervised methods. We empirically determine the minimal quantities of unlabeled data required for obtaining comparable performance to using all available data. Our investigation assesses three established SSL methods for HAR on three target datasets. Out of these three methods, we discover that Contrastive Predictive Coding (CPC) is the most efficient in terms of pre-training data requirements: just 15 minutes of sensor data across participants is sufficient to obtain competitive activity recognition performance. Further, around 5 minutes of source data is enough when there are sufficient amounts of target application data available. These findings can serve as starting point for more efficient data collection practices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121540
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Electrical and Computer Engineering"
            }
          ],
          "personId": 121604
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121679
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121494
        }
      ]
    },
    {
      "id": 121782,
      "typeId": 13124,
      "title": "A Contactless and Non-Intrusive System for Driver's Stress Detection",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1091",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Stress plays a significant role in fatal accidents, highlighting the importance of timely monitoring of driver stress to facilitate effective interventions and reduce road accidents. However, monitoring driver stress presents numerous challenges in the context of driving. First, state-of-the-art techniques such as self-stress evaluation and periodic cortisol level checks are not suitable for the driving scenario. Second, existing unimodal solutions does not provide a com- prehensive and holistic assessment of the driver’s stress. Although some research utilizes multimodal features, the use of wearables attached to the driver’s body in real-life situations is impractical and highly discomforting. Our proposed solution tackles these challenges by offering a contactless and non-intrusive approach that prioritizes the driver’s comfort during the collection of multimodal data, which includes capturing heart rate variability (HRV), respiration rate, and microfacial expressions. Through feature-level data fusion, we combine and integrate these diverse modalities to generate comprehensive insights. These insights are then utilized by the multimodal learning pipeline to predict the driver’s stress levels in real driving scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Select",
              "city": "Naju-si",
              "institution": "KENTECH,",
              "dsl": "Energy AI, IMC Lab,"
            }
          ],
          "personId": 121587
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": "Energy AI/School of Energy Engineering/Intelligent Mobile Computing Lab"
            }
          ],
          "personId": 121536
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121576
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 121539
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 121531
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 121574
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Data Science"
            }
          ],
          "personId": 121622
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Hwaseong-si",
              "institution": "Commercial Vehicle Development Team",
              "dsl": "Hyundai Motor Group"
            }
          ],
          "personId": 121714
        }
      ]
    },
    {
      "id": 121783,
      "typeId": 13124,
      "title": "WMGPT: Towards 24/7 Online Prime Counseling with ChatGPT",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1090",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Traditional in-person counseling encounters limitations in terms of accessibility, flexibility, and social stigma.\r\nAdditionally,\r\nlow mental health literacy and embarrassment among individuals hinder help-seeking behavior. \r\n%\r\nMeanwhile,\r\nthe introduction of more sophisticated sensors embedded in ubiquitous devices such as smartphones and smartwatches, and the release of a powerful large language model, i.e., chatGPT, create new opportunities to address the existing limitations of traditional counseling services.\r\n% \r\nIn that regard,\r\nwe propose \r\n% \r\n\\textit{WMGPT}, \r\na system that offers round-the-clock mental health counseling services. \r\n%\r\nBy leveraging continuous analysis of user context and digital phenotype, \\textit{WMGPT} delivers personalized counseling support. \r\n%\r\nThrough 24/7 passive monitoring, it continuously assesses individuals' mental state, initiates conversations on their behalf, and potentially triggers counseling services. \r\n%\r\nThese specialized counseling services are facilitated by a fine-tuned chatGPT model. \r\n%\r\n\\textit{WMGPT}\r\n% \r\npresents a promising solution to overcome the limitations of traditional counseling by providing accessible, personalized, and timely mental health support, \r\npaving the way for\r\na convenient and effective\r\nservice for improving well-being.\r\n% ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Naju",
              "institution": "Korea Institute of Energy Technology",
              "dsl": ""
            }
          ],
          "personId": 121659
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Naju",
              "institution": "Korea Institute of Energy Technology",
              "dsl": ""
            }
          ],
          "personId": 121561
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Naju",
              "institution": "Korea Institute of Energy Technology",
              "dsl": ""
            }
          ],
          "personId": 121530
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "BRFrame Inc.",
              "dsl": ""
            }
          ],
          "personId": 121567
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "SEOUL",
              "institution": "Hanyang University",
              "dsl": "Department of Cognitive Sciences"
            }
          ],
          "personId": 121588
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121576
        }
      ]
    },
    {
      "id": 121784,
      "typeId": 13124,
      "title": "Preliminary Study on Effect of Secretly Increasing or Decreasing Predicted Number of Steps to Promote Walking",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1095",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "With the widespread use of wearable devices, it is becoming easier to predict the future of an individual's behavior regarding health. However, little is known about how presenting future predictions about an individual's behavior affects awareness and behavior. The ``StepUp Forecast'' project is investigating walking behavior to clarify the impact of presenting predictions of the number of steps an individual will take on human behavior. In a previous study, it was found that self-efficacy and the number of steps taken increased significantly when the predicted number of steps was presented, compared with when only a record of the number of steps taken was shown. To advance this research, we investigated the effects of intentionally and secretly increasing or decreasing the predicted value rather than simply presenting it. Specifically, we observed changes in awareness and behavior when users were presented with a predicted value with 1,000 steps added or subtracted.\r\nIn our 5-week experiment, 40 participants used StepUp Forecast, an application that presents the predicted number of steps on the basis of past life logs. The results indicate a non-significant but increasing trend in self-efficacy and number of steps added or subtracted compared with when only the step-count record was presented.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "1-1 Hikarinooka, Yokosuka-shi, Kanagawa-ken",
              "institution": "NTT",
              "dsl": "Service Evolution Laboratories"
            }
          ],
          "personId": 121756
        }
      ]
    },
    {
      "id": 121785,
      "typeId": 12924,
      "title": "UltrasonicWhisper: Ultrasound Can Generate Audible Sound in Your Hearable",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-9614",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "Recent studies have shown that ultrasound can be used for voice input to microphones such as smart speakers by taking advantage of the nonlinearity of the microphones. \r\nA similar attack on the hearing of a user wearing a hearable with an outside microphone is also possible. Specifically, information modulated by ultrasound from an attacker is demodulated into audible sound inside the hearable, and audio information can be presented to the wearer via its inner loudspeaker. This process could result in the presentation of false information disguised as instructions from the hearable and possible interference with the user's hearing. In light of those issues, this study experimentally evaluated the possibility of ultrasonic attacks on hearables. Evaluation results confirmed that mean Mel-cepstral distortion (MCD) and mean opinion score (MOS) of the demodulated sound were 7.90 and 2.53, respectively. We also confirmed that The participants followed 14.9% of the false instructions presented by ultrasound even when they were alerted to the ultrasonic attack.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 121709
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hyogo",
              "city": "Kobe",
              "institution": "Kobe University",
              "dsl": ""
            }
          ],
          "personId": 121639
        }
      ]
    },
    {
      "id": 121786,
      "typeId": 13124,
      "title": " Exploring the Impact of Virtual Reality-based Simulated Symptoms Towards Schizophrenia on Public Empathy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1093",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Empathy is the ability to perceive and understand other people's emotions and thoughts. Virtual reality (VR), as a new technology, offers a unique opportunity to induce empathy in individuals by allowing them to immerse themselves in realistic situations from another person's perspective. In this study, we have developed a virtual reality system that incorporates situational learning and immersive interaction. Through first-person perspective simulations, participants can experience the challenges and situations encountered by individuals with positive symptoms of schizophrenia. The aim is to investigate the potential for altering public empathy and attitudes through this simulated experiential system. We hope our study could provide a new approach to health education strategies to enhance public empathy and attitudes toward people with schizophrenia. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": ""
            }
          ],
          "personId": 121560
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": ""
            }
          ],
          "personId": 121643
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": ""
            }
          ],
          "personId": 121656
        }
      ]
    },
    {
      "id": 121787,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Robo-Coverstitch: Coverstitched Auxetic Shape Memory Actuators For Soft Wearable Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-7630",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "As wearable technologies proliferate there is a growing need for compact, easy-to-produce textile-based mechanical actuators. This paper presents a novel coverstitched auxetic actuator using shape memory alloys (SMAs) -- which we call the Robo-coverstitch – that can be easily sewn onto fabrics using an industrial coverstitch machine. Multiple Robo-coverstitch samples were manufactured and tested to characterize their force vs. strain properties. The samples were actuated in both the length- and width-wise direction between 0-15% strain at constant electrical current (0.4A). The actuators produced increased force in both length- and width-wise directions when actuated (i.e., auxetic actuation behavior), and these active forces increased with actuator strain. The Robo-coverstitch offers an unobtrusive mechanical actuation solution that can be readily deployed into commercial garments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "minneapolis",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121447
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Paul",
              "institution": "University of Minnesota",
              "dsl": "Wearable Technology Lab"
            }
          ],
          "personId": 121469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "St. Paul",
              "institution": "University of Minnesota",
              "dsl": ""
            }
          ],
          "personId": 121438
        }
      ]
    },
    {
      "id": 121788,
      "typeId": 13124,
      "title": "Comparing Methods to Study Social Acceptance of Smart Glasses",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1133",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Conducting user studies on social perception of wearable technology requires a framework with sufficient sensitivity to emulate perception in real-life. Using videos can efficiently reach a large number of participants, but what level of video fidelity is necessary to achieve results similar to live interaction? In a study of 48 participants, we examine four different viewing conditions: phone, laptop, big screen, and live demonstration, while investigating the social weight of wearing different glasses (Vuzix Blade, Tooz Devkit, North Focals, or prescription glasses). Contrary to our hypothesis, the live condition was not the most sensitive.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "ATLANTA",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "ATLANTA",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121527
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121669
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121710
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121611
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121599
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121749
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121585
        }
      ]
    },
    {
      "id": 121789,
      "typeId": 13124,
      "title": "Lightron: A Wearable Sensor System that Provides Light Feedback to Improve Punching Accuracy for Boxing Novices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1056",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "This work presents ‘Lightron’, a wearable sensor system designed for boxing training assistance, improving punching accuracy for novices. This system combines accelerometers, stretch sensors, and flex sensors to detect the user's movements, providing LED light feedback to the user. This adjustable design ensures a tight fit of the device to the body, allowing the sensor to collect accurate arm movement data without impeding training movements. A simple neural network is used to enable real-time motion detection and analysis, which can run on low-cost embedded devices. Contrary to merely using accelerometers on the wrist, Lightron collects motion data from the elbow and shoulder, enhancing the precision of punch accuracy assessment. Primary user studies conducted among boxing amateurs have shown that using Lightron in boxing training increases the performance of amateur players both in single and periodic training sessions, demonstrating its potential utility in the sports training domain.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Global Innovation Exchange (GIX) Institute"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 121529
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Global Innovation Exchange (GIX) Institute"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 121739
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 121584
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "University of Washington",
              "dsl": "Global Innovation Exchange"
            }
          ],
          "personId": 121510
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bellevue",
              "institution": "Global Innovation Exchange (GIX)",
              "dsl": "University of Washington"
            }
          ],
          "personId": 121617
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Computer Science and Technology"
            }
          ],
          "personId": 121663
        }
      ]
    },
    {
      "id": 121790,
      "typeId": 13124,
      "title": "TeleVIP: On-site Person Removal and Context Distillation Platform for Dedicated Telepresence Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1055",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "For the application of telepresence robots, the on-site person usually has been regarded as a main element in remote interaction. However, in the context of a remote tour, the visual presence of an on-site person unexpectedly interrupting a robot's view may worsen the remote visitor's viewing experience. Moreover, the presence of a telepresence robot with its camera on may arise the privacy concerns of an on-site person. In this sense, we developed TeleVIP, a platform that provides telepresence applications with easy-to-use APIs to remove the figures of on-site people and selectively distill the useful information from the presence or context of on-site people, according to the application's demand. We implemented a working prototype of TeleVIP and performed pilot experiments in terms of both quantitative metrics and qualitative results, indicating the preliminary usability and feasibility of TeleVIP.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121660
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121688
        }
      ]
    },
    {
      "id": 121791,
      "typeId": 13124,
      "title": "AirSpec: A Smart Glasses Platform, Tailored for Research in the Built Environment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1132",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "AirSpec is an extensible, environment-focused, research and development smart glasses platform that evolved from an existing open-source, psychophysiological monitoring system. We created custom supporting software toolkits that allow users to interact with the device, easily view real-time data, and perform remote data collection. In its base configuration, the system includes a variety of sensors that sample physiological and environmental signals and stream that data to a Bluetooth-connected client, either a phone running the AirSpec App or a Bluetooth-equipped computer via our website or a python script. In addition, AirSpecs have been made more extensible with multiple external electrical connections to support more applications and future sensor subsystems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 121601
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Responsive Environments Group",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 121532
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments Group"
            }
          ],
          "personId": 121652
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Media Lab"
            }
          ],
          "personId": 121723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "cambridge",
              "institution": "MIT",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 121648
        }
      ]
    },
    {
      "id": 121792,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "EchoNose: Sensing Mouth, Breathing and Tongue Gestures inside Oral Cavity using a Non-contact Nose Interface",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-1452",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "Sensing movements and gestures inside the oral cavity has been a long-standing challenge for the wearable research community. This paper introduces EchoNose, a novel nose interface that explores a unique sensing approach to recognize gestures related to mouth, breathing, and tongue by analyzing the acoustic signal reflections inside the nasal and oral cavities. The interface incorporates a speaker and a microphone placed at the nostrils, emitting inaudible acoustic signals and capturing the corresponding reflections. These received signals were processed using a customized data processing and machine learning pipeline, enabling the distinction of 16 gestures involving speech, tongue, and breathing. A user study with 10 participants demonstrates that EchoNose achieves an average accuracy of 93.7% in recognizing these 16 gestures. Based on these promising results, we discuss the potential opportunities and challenges associated with applying this innovative nose interface in various future applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121705
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 121683
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121675
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 121745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121701
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121621
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Linguistics"
            }
          ],
          "personId": 121579
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121671
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITHACA",
              "institution": "Cornell University",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 121686
        }
      ]
    },
    {
      "id": 121793,
      "typeId": 13124,
      "title": "VeinXam: A Low-Cost Deep Veins Function Assessment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1131",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "A venous insufficiency, to which the deep veins of the lower human extremities are particularly susceptible, can lead to serious diseases, such as a deep vein thrombosis (DVT) with subsequent risks of severe implications, e.g. pulmonary embolism or a post-thrombotic syndrome (PTS). The current standard procedure to diagnose venous insufficiency is performed exclusively in medical offices and hospitals in the form of in-patient treatments with special medical equipment. This hurdle for the patient, combined with the often diffuse symptoms of venous insufficiency, may lead to a late discovery of diseases such as DVTs and increases the risk of secondary diseases as well as treatment costs. To address these issues, we propose a novel method for continuous monitoring of the current venous function by adapting the Light Reflection Rheography (LLR) and using low-cost wearable sensor technology and a smartphone app, aiming to deliver critical early stage information about pathological changes of the blood flow in the lower limbs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Fraunhofer Institute for Computer Graphics Research IGD",
              "dsl": ""
            }
          ],
          "personId": 121657
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Fraunhofer Institute for Computer Graphics Research IGD",
              "dsl": ""
            }
          ],
          "personId": 121738
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Fraunhofer Institute for Computer Graphics Research IGD",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Wiesbaden",
              "institution": "Hochschule RheinMain",
              "dsl": ""
            }
          ],
          "personId": 121573
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Fraunhofer Institute for Computer Graphics Research IGD",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technische Universität Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 121507
        }
      ]
    },
    {
      "id": 121794,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Detecting Thumb-Posture for One-handed Interactions with Smartphone using Acoustic Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-7038",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "This paper presents a novel approach for expanding one-handed interactions using the thumb positioned above the smartphone screen. Our approach is based on acoustic sensing, a technique for leveraging the built-in speaker and microphone of the smartphone without requiring additional sensors or attachments. We explored the feasibility of our approach on smartphones with the conventional speaker and microphone arrangement and investigated the enhancement of recognition accuracy by using smartphones equipped with Acoustic Surface, which is a technology enabling the entire screen to vibrate and emit sound over a wider area and installed in several commercial smartphones such as LG G8 ThinQ and Huawei P30 Pro. We focused on classifying 12 different thumb postures and developed models that achieve prediction accuracies of 78.6% (conventional smartphone) and 87.0% (Acoustic Surface).",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo University of Technology",
              "dsl": ""
            }
          ],
          "personId": 121547
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 121727
        }
      ]
    },
    {
      "id": 121795,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Going Blank Comfortably: Positioning Monocular Head-Worn Displays When They are Inactive",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-4004",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "Head-worn displays like Tooz and North's Focals are designed to be worn all day as smart eyeglasses. When the display is not lit (often to save battery life), the optical combiners may remain visible to the user as an out-of-focus seam or discoloration in the lens. We emulate seven shapes and positions of optical combiners which 30 participants rank for comfort. Based on these results, we run a second user study with 12 participants comparing the comfort of a combiner with various offset distances from the user's primary position of gaze (PPOG) towards the nose. Results suggest that a combiner's nearest edge should be more than 15° from the PPOG.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121508
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121545
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121712
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "West Point",
              "institution": "United States Military Academy",
              "dsl": ""
            }
          ],
          "personId": 121575
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121585
        }
      ]
    },
    {
      "id": 121796,
      "typeId": 13124,
      "title": "BaroDepth: A Method of Estimating Depth with Barometric Pressure Sensors in Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1053",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Underwater activities are enjoyed around the world.\r\nDivers wear dive computers that allow visualization of diving information including depth and dive time, to avoid the risk of decompression sickness. \r\nHowever, dive computers are so expensive that they burden novice divers.\r\nA method of estimating water depth with barometric pressure sensors in smartphones, instead of dive computers, was studied. \r\nUnfortunately, the conventional method for estimating water depth using a barometric pressure sensor in a smartphone is limited to estimating shallow depths of about 1m to 2 m due to the sensor only being able to detect up to about 1100 hPa to 1200 hPa.\r\nWe propose a method of estimating water depths of several tens of meters with a barometric pressure sensor in a smartphone by adopting a waterproof hard case with a decompression adjustment function and a polynomial regression model.\r\nWe implemented a prototype of the proposed method and conducted experiments to evaluate the precision of estimating water depths down to 20 m in the sea.\r\nThe experiments revealed that the proposed method could cover water from 0 to 20 m depths and estimate water depths with a root mean squared error (RMSE) of less than 1 m.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO",
              "dsl": ""
            }
          ],
          "personId": 121580
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT DOCOMO ",
              "dsl": "Research Labs, "
            }
          ],
          "personId": 121695
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 121665
        }
      ]
    },
    {
      "id": 121797,
      "typeId": 13124,
      "title": "Capafoldable: self-tracking foldable smart textiles with capacitive sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1135",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Folding is a unique structural technique to equip planar materials with motion or 3D mechanical properties. Textile-based capacitive sensing has shown to be sensitive to the geometry deformation and relative motion of conductive textiles. In this work, we propose a novel self-tracking foldable smart textile by combining folded fabric structures and capacitive sensing to detect the structural motions using state-of-the-art sensing circuits and deep learning technologies. We created two folding patterns, Accordion and Chevron, each with two layouts of capacitive sensors in the form of thermobonded conductive textile patches. In an experiment of manually moving patches of the folding patterns, we developed deep neural network to learn and reconstruct the vision-tracked shape of the patches. Through our approach, the geometry primitives defining the patch shape can be reconstructed from the capacitive signals with R-squared value of up to 95% and tracking error of 1cm for 22.5cm long patches. With mechanical, electrical and sensing properties, Capafoldable could enable a new range of smart textile applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121460
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121436
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Berlin",
              "institution": "Universität der Künste Berlin",
              "dsl": ""
            }
          ],
          "personId": 121654
        }
      ]
    },
    {
      "id": 121798,
      "typeId": 13124,
      "title": "Don't freeze: Finetune encoders for better Self-Supervised HAR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1134",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Recently self-supervised learning (SSL) has been proposed in the field of human activity recognition (HAR) as a solution to the labelled data availability problem. The idea being that by using pretext tasks such as reconstruction or contrastive predictive coding, useful representations can be learned that then can be used for classification. Those approaches follow the pretrain, freeze and fine-tune procedure. In this work we investigate how a simple change - not freezing the representation - leads to substantial performance gains across pretext tasks. The improvement was found in all four investigated datasets and across all four pretext tasks and is inversely proportional to amount of labelled data. Moreover the effect is present whether the pretext task is carried on the Capture24 dataset or directly in unlabelled data of the target dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121564
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121750
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "RPTU",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121799,
      "typeId": 13124,
      "title": "CityScouter: Exploring the Atmosphere of Urban Landscapes and Visitor Demands with Multimodal Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1057",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "This paper proposes a novel demo application named CityScouter that utilizes multimodal data to analyze various aspects of urban characteristics quantitatively. Existing studies have proposed systems to examine either the physical characteristics of cities or the nature of people residing there. However, there is a lack of systems that analyze the characteristics of cities from both the physical and the residents' aspects. CityScouter addresses this challenge by leveraging computer vision technologies to quantify the quality of the urban landscape atmosphere and combining it with location information and user search history to reveal the desires of people visiting the area. The application is user-friendly and compatible with mobile devices, enabling users to conveniently enhance their understanding of cities while exploring them. Additionally, we provide reviews from urban development experts, offering insights into the applicability of our application. Furthermore, we showcase the usefulness and user experience of CityScouter through live demonstrations at the conference venue.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121633
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121593
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "TANIGUCHI TOMOMI DESIGN",
              "dsl": ""
            }
          ],
          "personId": 121515
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hyogo",
              "institution": "Hyogo University",
              "dsl": ""
            }
          ],
          "personId": 121526
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Nikken Sekkei Ltd",
              "dsl": ""
            }
          ],
          "personId": 121595
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Nikken Sekkei Ltd",
              "dsl": ""
            }
          ],
          "personId": 121570
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Nikken Sekkei Ltd",
              "dsl": ""
            }
          ],
          "personId": 121636
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 121637
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 121631
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121514
        }
      ]
    },
    {
      "id": 121800,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "\"Hello I am here'': Proximal Nonverbal Cues Role in Initiating Social Interactions in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-6904",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) has revolutionized social interactions, but limited field of view (FoV) remains a significant obstacle. Users often fail to notice others within the virtual environment, hindering social engagement. To facilitate initiating social interactions, we developed a novel social signaling technique that utilizes proximal nonverbal cues to indicate users' location, name, and interests within a social distance. In a 2 x 2 mixed user study, we found that this technique greatly enhanced social presence and interaction quality among users with prior social ties. Our signaling technique has tremendous potential to facilitate social interactions across various social virtual events, such as staff meetings and reunions. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Egypt",
              "state": "",
              "city": "Cairo",
              "institution": "German International University in Cairo",
              "dsl": "Faculty of Informatics and Computer Science"
            }
          ],
          "personId": 121735
        },
        {
          "affiliations": [
            {
              "country": "Egypt",
              "state": "Cairo ",
              "city": "Cairo ",
              "institution": "German International University in Cairo ",
              "dsl": ""
            }
          ],
          "personId": 121685
        }
      ]
    },
    {
      "id": 121801,
      "typeId": 12924,
      "title": "On the Utility of Virtual On-body Acceleration Data for Fine-grained Human Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-3283",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "Previous work has demonstrated that virtual accelerometry data, extracted from videos using cross-modality transfer approaches like IMUTube, is beneficial for training complex and effective human activity recognition (HAR) models. Systems like IMUTube were originally designed to cover activities that are based on substantial body (part) movements. Yet, life is complex, and a range of activities of daily living is based on only rather subtle movements, which bears the question to what extent systems like IMUTube are of value also for fine-grained HAR? In this work we first introduce a measure to quantitatively assess the subtlety of human movements that are underlying activities of interest--the motion subtlety index (MSI)--which captures local pixel movements and pose changes in the vicinity of target virtual sensor locations, and correlate it to the eventual activity recognition accuracy. We explore for which activities with underlying subtle movements a cross-modality transfer approach works, and for which not. As such, the work presented in this paper allows us to map out the landscape for IMUTube-like system applications in practical scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121666
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121607
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Emory University",
              "dsl": "Department of Biomedical Informatics"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121608
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121494
        }
      ]
    },
    {
      "id": 121802,
      "typeId": 13124,
      "title": "NotiSummary: Exploring the Potential of AI-Driven Text Summarization on Smartphone Notification Management",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1081",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "As smartphone notifications proliferate, it becomes increasingly challenging for users to review them all. In response, we've developed NotiSummary, an application leveraging ChatGPT to present a concise summary of notifications, thereby reducing the time and effort required to review them. The application also integrates customization capabilities to further enhance the user-centric experience. Our study investigates the potential applicability of AI-based summarization techniques in notifications and explores user interaction patterns with these summaries. Preliminary results show that users find these summaries helpful in quickly accessing specific information, with preferences emerging for receiving summaries early in the morning and late at night. These findings underscore the potential value of notification summarization while highlighting the need to further investigate user preferences for summary appearance and the impact of summary generation on notification management behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121673
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121625
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121719
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121702
        }
      ]
    },
    {
      "id": 121803,
      "typeId": 13124,
      "title": "Recognition of Engagement from Electrodermal Activity Data Across Different Contexts",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1080",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Engagement is a human experience relevant in multiple contexts, including classrooms, presentations and workplaces. Stemming from flow theory, engagement in these contexts has been studied using wearable devices, which can unobtrusively measure physiological changes, specifically Electrodermal Activity (EDA). However, researchers have not explored how EDA markers might be similar or different between various engagement scenarios, namely student, audience and workplace engagement. In this study, we investigated possible similarities through the use of three datasets containing EDA data and engagement self-report labels, collected in the wild in different settings using research-grade wrist-worn wearable devices. We analysed the correlation between hand-crafted EDA features and the engagement level and we leveraged a machine learning framework for engagement prediction. We found that similar features are correlated with the engagement level across the various settings. We also found that our machine learning model identified related markers as important across the three engagement contexts. Our results highlight that similarities are present in the EDA features between different engagement contexts, while also identifying possible dataset specific differences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "Ticino",
              "city": "Lugano",
              "institution": "Università della Svizzera Italiana (USI)",
              "dsl": ""
            }
          ],
          "personId": 121704
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "Università della Svizzera italiana (USI)",
              "dsl": ""
            }
          ],
          "personId": 121534
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "Università della Svizzera Italiana (USI)",
              "dsl": ""
            }
          ],
          "personId": 121542
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lugano",
              "institution": "Università della Svizzera italiana",
              "dsl": ""
            }
          ],
          "personId": 121662
        }
      ]
    },
    {
      "id": 121804,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "HPSpeech: Silent Speech Interface for Commodity Headphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-9427",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "We present HPSpeech, a silent speech interface for commodity headphones. HPSpeech utilizes the existing speakers of the headphones to emit inaudible acoustic signals. The movements of the temporomandibular joint (TMJ) during speech modify the reflection pattern of these signals, which are captured by a microphone positioned inside the headphones. To evaluate the performance of HPSpeech, we tested it on two headphones with a total of 18 participants. The results demonstrated that HPSpeech successfully recognized 8 popular silent speech commands for controlling the music player with an accuracy over 90%. While our tests use modified commodity hardware (both with and without active noise cancellation), our results show that sensing the movement of the TMJ could be as simple as a firmware update for ANC headsets which already include a microphone inside the hear cup. This leaves us to believe that this technique has great potential for rapid deployment in the near future. We further discuss the challenges that need to be addressed before deploying HPSpeech at scale.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121721
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 121680
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121676
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121701
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121671
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell ",
              "dsl": "Information Science"
            }
          ],
          "personId": 121686
        }
      ]
    },
    {
      "id": 121805,
      "typeId": 13124,
      "title": "Captivating the Senses: Crafting a Multisensory Virtual Experience for Enhanced Realism",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1083",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "This paper explores the challenges and opportunities in achieving a high level of realism and presence in Virtual Reality (VR) experiences. Realism and presence, which refer to the level of authenticity of the virtual environment and the user's subjective perception of being in it, respectively, are fundamental components of engaging and satisfactory VR experiences. Despite advancements in VR technology, users can often distinguish between real and virtual experiences, thereby limiting the potential of VR in various fields. This paper reviews various challenges inherent in achieving indistinguishable realism in VR,  encompassing visual, auditory, and tactile components. These insights are critical in guiding the continued development of more immersive and realistic VR experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 121630
        }
      ]
    },
    {
      "id": 121806,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Towards a Haptic Taxonomy of Emotions: Exploring Vibrotactile Stimulation in the Dorsal Region",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-4573",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "The implicit communication of emotional states between persons is a key use case for novel assistive and augmentation technologies. It can serve to expand individuals' perceptual capabilities and assist neurodivergent individuals. Notably, vibrotactile rendering is a promising method for delivering emotional information with minimal interference with visual or auditory perception. To date, the subjective individual association between vibrotactile properties and emotional states remains unclear. Previous approaches relied on analogies or arbitrary variations, limiting generalization. To address this, we conducted a study with 40 participants, analyzing associations between attributes of self-generated vibrotactile patterns (\\textsc{amplitude}, \\textsc{frequency}, \\textsc{spatial location} of stimulation) and four emotional states (\\textsc{Anger}, \\textsc{Happiness}, \\textsc{Neutral}, \\textsc{Sadness}). We fin a preference for symmetrically arranged patterns, as well as distinct amplitude and frequency profiles for different emotions. These insights can aid in creating standardized vibrotactile patterns for universal emotional communication.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 121552
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 121761
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            },
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": "School of Computing and Information Systems"
            }
          ],
          "personId": 121653
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 121744
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Chemnitz",
              "institution": "TU Chemnitz",
              "dsl": "Institute for Media Research"
            }
          ],
          "personId": 121500
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Oslo",
              "institution": "University of Oslo",
              "dsl": ""
            }
          ],
          "personId": 121655
        }
      ]
    },
    {
      "id": 121807,
      "typeId": 13124,
      "title": "AR for the Masses: Attina, the Low-Cost Accessible Headset for Inclusive Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1082",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Current commercial Augmented Reality (AR) headsets are expensive and limited in availability, posing challenges for researchers and designers in resource-constrained settings. To address this, we introduce Attina, an open-source, low-cost optical see-through AR headset using a smartphone as its display. Overcoming limitations of previous work, the system features stereoscopic rendering for enhanced depth perception and a 3-DoF controller for virtual interactions. We discuss Attina's design and outline opportunities for a usability study to evaluate its user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder",
              "dsl": "Institute of Cognitive Science"
            }
          ],
          "personId": 121630
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Bangalore Endoscopic Surgery Training Institute & Research Centre",
              "dsl": ""
            }
          ],
          "personId": 121521
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Bangalore Endoscopic Surgery Training Institute & Research Centre",
              "dsl": ""
            }
          ],
          "personId": 121668
        }
      ]
    },
    {
      "id": 121808,
      "typeId": 13124,
      "title": "An Experimental Video Conference Platform to Bridge the Gap Between Digital and in-Person Communication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1044",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "With many contemporary video conferencing platforms available, there is still a need for platforms that afford a researcher workflow to conduct controlled online experiments. \r\nWe have developed an open source experimental video conferencing platform that enables researchers to design and conduct remote experiments. Our platform provides a high level of control over the user interface and video streams, which is essential for studying the differences between remote and in-person social interactions. We give an overview of our platform's usage and architecture and conduct a take-home study (N=9) to evaluate how accessible our system is to potential new contributors. We also follow up with an initial evaluation of technical performance bottlenecks for when our experimental platform is deployed, and show that the computational resources increases per each video stream as well as the type of filters applied to each participant. We end with a short discussion on next steps and the experimental hub's potential to be extended as a sandbox for testing browser based augmented reality (WebAR) filters to be adopted in interdisciplinary experimental procedures.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121548
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121602
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121559
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121628
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121505
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 121672
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": "Computing"
            }
          ],
          "personId": 121594
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 121757
        }
      ]
    },
    {
      "id": 121809,
      "typeId": 13124,
      "title": "Evaluating the Effect of the Color-Word Stroop Test and VR as a Psychological Stressor",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1088",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) makes use of psychological  stressors to enable users to feel immersed in different domains. Although stressor tests, like the Color-Word Stroop Test (CWST), have been shown to be valid and reliable, there are limited deployments of such stressors that exhibit appropriate immersion and user experience in VR. In this paper, we present the development and evaluation of two VR simulators of the CWST. We conducted a between-subjects study with 27 participants who completed the conventional CWST, a VR-only simulator of the CWST, and a gamified VR simulator of the CWST. We measured and conducted a statistical analysis of participants’ CWST scores, perceived stress, immersion, user experience, playability, and heart rate. Our results show that there are no significant differences in using the two CWST simulators of the VR. We found that users’ heart rate is significantly higher when using the VR simulators than the conventional CWST and that the VR game simulator significantly shows a higher score in immersion than the VR-only simulator. However, users’ perceived stress was significantly less when using the VR game simulator. Finally, we reflect on design insights for stressors tests in VR and discuss directions for future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Sonora",
              "institution": "ITSON",
              "dsl": ""
            }
          ],
          "personId": 121597
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "Baja California",
              "city": "Ensenada",
              "institution": "Centro de Investigación Científica y de Educación Superior de Ensenada, Baja California",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 121726
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Sonora",
              "institution": "Sonora Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121499
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Ensenada",
              "institution": "CICESE",
              "dsl": ""
            }
          ],
          "personId": 121711
        }
      ]
    },
    {
      "id": 121810,
      "typeId": 13124,
      "title": "Stay Ahead of the Competition: An Approach for Churn Prediction by Leveraging Competitive Service App Usage Logs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1087",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "With the widespread adoption of smartphones, users now have easy access to similar services, leading to increased churn. As a result, it has become essential for service providers to prevent churn caused by customers' switch to competing services. The most common approach for service providers to prevent their customers' churn is to make churn predictions by monitoring customers' usage patterns of their own services. However, despite the importance of insights concerning customers' usage of competing services for the retention of customers, such information are yet to be integrated into churn prediction models due to the lack of suitable monitoring methods. Here, we propose an approach to predict user churn leveraging the event logs from smartphones and tablets. Instead of conventional churn prediction methods that solely rely on the users' usage patterns of their own service, our approach predicts churn by utilizing users' usage patterns of competing services, including their trial use of service before switch to competitor's. We evaluated the prototyped prediction model using smartphone logs collected between April 2020 and March 2021. The results demonstrated that our proposed method improved the performance of the conventional method by 1.8% to 7.5%, achieving AUC values ranging from 0.844 to 0.923.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 121707
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 121544
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 121695
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT DOCOMO, INC.",
              "dsl": ""
            }
          ],
          "personId": 121665
        }
      ]
    },
    {
      "id": 121811,
      "typeId": 13124,
      "title": "Binary Social Game: A Principle for Enhancing Desired Behaviors through Asynchronous Social Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1042",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Within the HCI community, the impact of social dynamics on behavior change has gained considerable interest. This paper introduces the Binary Social Game (BSG), a novel principle that utilizes asynchronous social interactions to motivate desired behaviors. Initially, we developed GrowFlower based on the BSG principle, and its effectiveness was evaluated through a two-week wizard-of-oz study. After gaining insights from the preliminary study, we refined our approach and created GENGO, a bingo-style game that is specifically designed to encourage exercise as a targeted behavior. A subsequent two-month user study in real-world conditions, involving 82 participants, validated GENGO's effectiveness in fostering consistent exercise habits. These findings highlight the potential of BSG as a useful tool for promoting desired behaviors within a group context, especially through asynchronous social interactions. This study contributes to the understanding of how social dynamics can be leveraged for behavior change.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Research",
              "dsl": ""
            }
          ],
          "personId": 121697
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Research",
              "dsl": ""
            }
          ],
          "personId": 121736
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Suwon",
              "institution": "Samsung Electronics",
              "dsl": ""
            }
          ],
          "personId": 121565
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Electronics",
              "dsl": ""
            }
          ],
          "personId": 121557
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung",
              "dsl": ""
            }
          ],
          "personId": 121502
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Samsung Electronics",
              "dsl": ""
            }
          ],
          "personId": 121533
        }
      ]
    },
    {
      "id": 121812,
      "typeId": 13124,
      "title": "SpectraVue - An Interactive Web Application Enabling Rapid Data Visualization and Analysis for Wearable Spectroscopy Research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1086",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Spectroscopic analysis of physiological phenomena has remained an important yet underutilized application in wearable technology today. Lumos has recently been introduced as an open-source wearable device capable of on-body spectroscopic research across the visible spectrum, enabling scientists and researchers to study optical properties of various clinical biomarkers in real-time. However, a key limitation in the data output of this device is the lengthy process required to visualize and plot spectral responses of observed mediums. In this paper, we present SpectraVue, an interactive web application that allows for visualization of Lumos spectral data. Utilizing a user-friendly interface, SpectraVue enables researchers to quickly generate three-dimensional plots from Lumos data stored in csv or text files, providing a comprehensive view of spectral responses of mediums under investigation. Additionally, SpectraVue offers features such as comparison of spectral data with clinical biomarkers, various data export options, and interactive plotting, further enhancing the user experience and researcher efficiency. The output graphs can be used to provide standardization of spectral responses across a wide range of mediums, including characterization of these responses in clinical biomarkers like glucose. SpectraVue aims to facilitate these investigations by streamlining the data processing and visualization workflow, thereby accelerating clinical diagnostic research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "PRECISE Center"
            }
          ],
          "personId": 121629
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania",
              "dsl": "PRECISE Center"
            }
          ],
          "personId": 121516
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 121632
        }
      ]
    },
    {
      "id": 121813,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-9304",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "The development of robust, generalized models for human activity recognition (HAR) has been hindered by the scarcity of large-scale, labeled data sets. Recent work has shown that virtual IMU data extracted from videos using computer vision techniques can lead to substantial performance improvements when training HAR models combined with small portions of real IMU data. Inspired by recent advances in motion synthesis from textual descriptions and connecting Large Language Models (LLMs) to various AI models, we introduce an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities. These textual descriptions are then used to generate 3D human motion sequences via a motion synthesis model, T2M-GPT, and later converted to streams of virtual IMU data. We benchmarked our approach on three HAR datasets (RealWorld, PAMAP2, and USC-HAD) and demonstrate that the use of virtual IMU training data generated using our new approach leads to significantly improved HAR model performance compared to only using real IMU data. Our approach contributes to the growing field of cross-modality transfer methods and illustrate how HAR models can be improved through the generation of virtual training data that do not require any manual effort.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "College of Computing"
            }
          ],
          "personId": 121666
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Emory University School of Medicine",
              "dsl": "Department of Biomedical Informatics"
            },
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121608
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121494
        }
      ]
    },
    {
      "id": 121814,
      "typeId": 13124,
      "title": "Demonstrating ProxiFit: Proximal Magnetic Sensing using a Single Commodity Mobile toward Holistic Weight Exercise Monitoring",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1049",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Although many works bring exercise monitoring to smartphones and smartwatches, inertial sensors used in such systems require the device to be in motion to detect exercises. We demonstrate our full paper ProxiFit, a practical on-device exercise monitoring system capable of classifying and counting exercises despite the device being still. ProxiFit remotely detects adjacent exercises with magnetic field fluctuations induced by the motions of ferrous exercise equipment. Novel proximal sensing nature of ProxiFit (1) extends coverage of wearable exercise monitoring to exercises that do not involve device motion such as lower-body machine exercise, and (2) brings a new off-body exercise monitoring mode with line-of-sight screen visibility, namely signage mode, to a smartphone mounted in front of the user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 121509
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 121563
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": ""
            }
          ],
          "personId": 121693
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "GSAI"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "CSE Dept."
            }
          ],
          "personId": 121734
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121688
        }
      ]
    },
    {
      "id": 121815,
      "typeId": 13124,
      "title": "iEat: Human-food interaction with bio-impedance sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1126",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "We explore an atypical use of bio-impedance by leveraging the unique temporal impedance patterns caused by the dynamic circuit changes between a pair of electrodes due to the body motions, and interactions with metal utensils and food during dining activities.\r\nSpecifically, we present iEat, a wearable impedance-sensing device for automatic food intake monitoring without using external devices such as smart utensils.\r\nUsing only one impedance channel with one electrode on each wrist, iEat detects food intake activities (e.g. cutting, putting food in the mouth with or without utensils, drinking, etc.) and food types from a defined category.\r\nAt idle, iEat measures the normal body impedance between the wrists; while eating, new parallel circuits will be formed between the hands through the utensils and food.\r\nTo quantitatively evaluate iEat in real-world settings, a food intake experiment was conducted including 40 meals performed by ten volunteers in a realistic table-dining environment.\r\nWith a light-weight convolutional neural network and leaving one subject out cross-validation, iEat could detect five food intake-related activities with 86.27 \\% average accuracy, and classify eight types of foods with 77.73 \\% average accuracy. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121555
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 121623
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürih",
              "dsl": ""
            }
          ],
          "personId": 121572
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Embedded Intelligence",
              "dsl": "DKFI"
            }
          ],
          "personId": 121641
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121816,
      "typeId": 13124,
      "title": "Estimating Temperature of Grasped Object using PPG Sensor",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1125",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "We propose a method to estimate the temperature of an object being grasped using a PPG sensor. Evaluation experiments showed a 94% accuracy for three subjects in two temperature ranges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121670
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121569
        }
      ]
    },
    {
      "id": 121817,
      "typeId": 13124,
      "title": "WatchPPG: An Open-Source Toolkit for PPG-based Stress Detection using Off-the-shelf Smartwatches",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1124",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "We present WatchPPG, an open-source toolkit that enables raw photoplethysmography (PPG) data collection and stress detection using off-the-shelf smartwatches.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science & Technology (KAIST)",
              "dsl": ""
            }
          ],
          "personId": 121755
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Naju-si, Jeollanam-do",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121530
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Incheon",
              "institution": "Inha University",
              "dsl": "Dept. of Artificial Intelligence"
            }
          ],
          "personId": 121650
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121576
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 121609
        }
      ]
    },
    {
      "id": 121818,
      "typeId": 13124,
      "title": "SocializeChat: a GPT-based AAC Tool for Social Communication Through Eye Gazing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1047",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "People with motor and speech impairments has limited communication abilities, resulting in a heavy reliance on Augmentative and Alternative Communication (AAC) tools. Existing commercial AAC tools provide simple combination of fixed words, satisfying basic physiological needs while encounter huge challenges in social communication. This social communication is significant for users’ mental health, especially for those with limited motor abilities. This article thus developed SocializeChat, an AAC tool that employs LLM technology to boost social chat with gaze inputs. Specifically, SocializeChat generates multiple sentences of conversation in real time, offers suggestions tailoring to users' preferences of topics, and phrases sentences in a style in accord with the relationship of people in conversation. This is achieved through a user dataset containing content preferences and social closeness, and through dedicated design of prompts and procedures. In a brief testing, SocializeChat was rated as effectively embodying subjects' own content preferences and communication style.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 121541
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 121582
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Collage of Computer Science and Technology"
            }
          ],
          "personId": 121732
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "College of Computer Science and Technology"
            }
          ],
          "personId": 121722
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": "Modern Industrial Design Institute"
            }
          ],
          "personId": 121644
        }
      ]
    },
    {
      "id": 121819,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Acoustic+Pose: Adding Input Modality to Smartphones with Near-Surface Hand-Pose Recognition using Acoustic Surface",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-5548",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "To achieve mid-air interactions for smartphones, acoustic-sensing, a technique using the built-in speaker and microphone of smart- phones, is promising. However, detecting hand poses on the near- surface of touchscreens remains challenging due to the arrange- ment of the built-in speaker and microphone. To address this, we present Acoustic+Pose, a novel approach for combining conven- tional touch interactions with near-surface hand-pose estimation to enable a wide range of interactions. We focused on smartphones incorporating Acoustic Surface, a technology that vibrates the en- tire smartphone screen to emit sound over a wide area. We used this technology to extend the input space to the near surface of touchscreens. We trained machine-learning models to recognize hand poses in the near-surface area and demonstrated interaction techniques to use the recognized poses for a new modality of smart- phone input. Through an evaluation, we confirmed that the trained models recognized 10 hand poses with 90.2% accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo University of Technology",
              "dsl": ""
            }
          ],
          "personId": 121547
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation",
              "dsl": ""
            }
          ],
          "personId": 121727
        }
      ]
    },
    {
      "id": 121820,
      "typeId": 13124,
      "title": "Unsupervised Diffusion Model for Sensor-based Human Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1129",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Recognizing human activities from sensor data is a vital task in various domains, but obtaining diverse and labeled sensor data remains challenging and costly. In this paper, we propose an unsupervised statistical feature-guided diffusion model for sensor-based human activity recognition. The proposed method aims to generate synthetic time-series sensor data without relying on labeled data, addressing the scarcity and annotation difficulties associated with real-world sensor data. By conditioning the diffusion model on statistical information such as mean, standard deviation, Z-score, and skewness, we generate diverse and representative synthetic sensor data. We conducted experiments on public human activity recognition datasets and compared the proposed method to conventional oversampling methods and state-of-the-art generative adversarial network methods. The experimental results demonstrate that the proposed method can improve the performance of human activity recognition and outperform existing techniques.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "请选择...",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 121751
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Deutsches Forschungszentrum für Künstliche Intelligenz",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Informatik"
            }
          ],
          "personId": 121677
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technical Univeristy of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121506
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 121535
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121821,
      "typeId": 13124,
      "title": "Estimating Sampling Rate of Human Activity Data from Accelerometer using Transformer-based Regression Model",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1128",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "We propose a method to estimate the sampling frequency from only the 3-axis acceleration data. The proposed method calculates the absolute difference between two consecutive samples, creates a histogram, and constructs a Transformer-based regression model. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan  University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan  University",
              "dsl": ""
            }
          ],
          "personId": 121493
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University ",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University ",
              "dsl": ""
            }
          ],
          "personId": 121518
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121569
        }
      ]
    },
    {
      "id": 121822,
      "typeId": 13124,
      "title": "FaceEat: Facial and Eating Activities Recognition with Inertial and Mechanomyography Fusion using a Glasses-Based Design for Real-Time and on-the-Edge Inference",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1127",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Facial expressions recognition and eating monitoring technologies can detect stress levels and emotional triggers that lead to unhealthy eating behaviors. Wearables offer a ubiquitous solution to help the individual develop coping mechanisms to manage stress and maintain a healthy lifestyle. \r\nIntroducing FaceEat, a privacy-focused, real-time, and on-the-edge (RTE) wearable solution with minimal power consumption ($\\leq 0.55$ Watts) and utilizing a tiny memory space (11 − 19KB).\r\nIts purpose is to recognize facial expressions and eating/drinking activities.\r\nAt the heart of FaceEat are lightweight convolutional neural networks, serving as the backbone models for both facial and eating scenarios.\r\nDuring the RTE evaluation, the system achieved an F1-score of over 86\\% in facial expression recognition.\r\nAdditionally, we achieved an F1-score of 90\\% for monitoring eating and drinking activities for the user-independent case with an unseen volunteer for the RTE.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 121439
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technical Univeristy of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": "Embedded Intelligence"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121429
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 121823,
      "typeId": 13124,
      "title": "ToozKit: System for Experimenting with Captions on a Head-worn Display",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1109",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "The advent of Automatic Speech Recognition (ASR) has made real-time captioning for the Deaf and Hard-of-Hearing (DHH) community possible, and integration of ASR into Head-worn Displays (HWD) is gaining momentum.  \r\nWe propose a demonstration of an open source, Android-based, captioning toolkit intended to help researchers and early adopters more easily develop interfaces and test usability. Attendees will briefly learn about the the technical architecture, use-cases and features of the toolkit as well as have the opportunity to experience using the captioning glasses on the tooz HWD while engaging in conversation with the demonstrators.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121753
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121667
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121585
        }
      ]
    },
    {
      "id": 121824,
      "typeId": 13124,
      "title": "AdJustMoment: Customize Your Ad Watching Experience",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1074",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Ads are ubiquitous on online video platforms, providing significant commercial value but also often interrupting viewers at inopportune moments. This can be particularly disruptive for mobile users, whose diverse contexts of use can mean time for video watching is limited. Prior research indicates that allowing users to negotiate or defer interruptions can reduce disruption. Inspired by this, we developed AdJustMoment, a mobile video player application that imports content from online platforms and offers users the option to defer ads through \"Snooze\" or \"Ads-debt\" - user-determined ads-viewing times. Our preliminary qualitative feedback indicates a preference for \"Ads-debt\", as users felt it provided greater control over when ads appear. Additionally, users also desire to receive positive feedback upon completing an ad, as it serves as a motivational factor.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121713
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121512
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121571
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121702
        }
      ]
    },
    {
      "id": 121825,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "WeightMorphy: A dynamic weight-shifting method to enhance the virtual experience with body deformation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-4102",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "We propose WeightMorphy, a hand-mounted system designed to improve the operability and immersive experience of teleoperation manipulation by changing the moment of inertia. This system reduces the discrepancy between the visual shape of the virtual hand and its corresponding moment of inertia, enabling instantaneous control by the user while maintaining accuracy. We have provided a detailed description of the design and concept of our system and conducted experiments to examine the effect of shifting the center of gravity on the operability of the deformable virtual hand using WeightMorphy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RCAST",
              "dsl": "The University of Tokyo"
            }
          ],
          "personId": 121708
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Toyota Motor Corporation",
              "dsl": ""
            }
          ],
          "personId": 121522
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Research Center for Advanced Science and Technology"
            }
          ],
          "personId": 121497
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 121620
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 121696
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo Metropolis",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Research Center for Advanced Science and Technology"
            }
          ],
          "personId": 121546
        }
      ]
    },
    {
      "id": 121826,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Exploring Recognition Accuracy of Vibrotactile Stimuli in Sternoclavicular Area",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-2166",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "Growing popularity of wearable haptic devices encouraged researchers to implement on-body interfaces that appropriate different form factors and interaction techniques. Among vibrotactile wearable interfaces, neck-worn devices gathered limited attention in HCI. While the ``necklace area'' offers wide opportunities for subtle haptic interaction, we lack knowledge of its tactile acuity to design interactive systems effectively. In this work, we present a prototype of HaptiNecklace - a vibrotactile necklace designed to study tactile acuity of sternoclavicular area. In the experimental study with N=19 participants, we compared recognition accuracy and cognitive load between different numbers of vibrotactile motors attached to the prototype in two scenarios -- static and mobile. The results show that directional patterns ensure better recognition than single-point vibrations in both mobile and static context. Moreover, introducing mobile scenario does not influence recognition accuracy but highly increases cognitive load. In this work, we provide practical hints to designing vibrotactile necklaces. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 121589
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Łódź",
              "institution": "Lodz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 121645
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Łódź",
              "institution": "Lodz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 121720
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 121553
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Łódź",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 121517
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 121606
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "University of Oldenburg",
              "dsl": "Media Informatics and Multimedia Systems"
            }
          ],
          "personId": 121596
        },
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Lodz",
              "institution": "Lodz University of Technology",
              "dsl": "Institute of Applied Computer Science"
            }
          ],
          "personId": 121562
        }
      ]
    },
    {
      "id": 121827,
      "typeId": 13124,
      "title": "An Interactive Workplace for Improving  Human Robot Collaboration",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1072",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Ubiquitous computing refers to the concept of integrating computers and technology into everyday objects and environments, making them constantly available and seamlessly interconnected. In recent times, there has been a growing adoption of the concept in industrial settings as well. This contribution targets the replacement of an industrial manual repair process through a robotic solution. The focus of this work is on the novel interface utilizing a digital pen and spatial augmented reality, which mimics the original job preparation process but ubiquitously enables automated robot programming.  This shifts the work of the human from a dull, dirty and dangerous process to the cognitively more demanding part of inspection and process strategy definition, whereas the robot is used as a dexterous and tenacious intelligent process tool to perform the surface processing operation itself.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Steyr",
              "institution": "Profactor GmbH",
              "dsl": ""
            }
          ],
          "personId": 121498
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Steyr",
              "institution": "Profactor GmbH",
              "dsl": ""
            }
          ],
          "personId": 121638
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Steyr",
              "institution": "Profactor GmbH",
              "dsl": ""
            }
          ],
          "personId": 121525
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121616
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 121642
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Steyr",
              "institution": "Profactor GmbH",
              "dsl": ""
            }
          ],
          "personId": 121511
        }
      ]
    },
    {
      "id": 121828,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "User Authentication Method for Hearables Using Sound Leakage Signals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-9715",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "We propose a novel biometric authentication method that leverages sound leakage signals from hearables that are captured by an external microphone. A sweep signal is played from hearables, and sound leakage is recorded using an external microphone. This sound leakage signal represents the acoustic characteristics of the ear canal, auricle, or hand. Then, our system analyzes the echoes and authenticates the user. The proposed method is highly adaptable to hearables because it leverages widely available sensors, such as speakers and external microphones. In addition, the proposed method has the potential to be used in combination with existing methods. In this study, we investigate the characteristics of sound leakage signals using an experimental model and measure the authentication performance of our method using acoustic data from 16 people. The results show that the balanced accuracy (BAC) scores were in the range of 87.0%-96.7% in several scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Lifestyle Computing Lab"
            }
          ],
          "personId": 121758
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "Information Science and Technology"
            }
          ],
          "personId": 121709
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Hokkaido",
              "city": "Sapporo",
              "institution": "Hokkaido University",
              "dsl": "School of Information Science and Technology"
            }
          ],
          "personId": 121651
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 121635
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "IPLAB"
            }
          ],
          "personId": 121703
        }
      ]
    },
    {
      "id": 121829,
      "typeId": 13124,
      "title": "Sandbox AI: We Don't Trust Each Other but Want to Create New Value Efficiently Through Collaboration Using Sensitive Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1078",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "This research deals with how to build reliable AI models using shared sensitive data. Confidential computing is gaining attention in AI services for a ubiquitous computing field. It makes legal and social regulations strict, including protecting privacy and ensuring security for sensitive data and AI models. As is also the case in ubiquitous computing research. The research phase also requires a manual trial-and-error process to create value through new combinations of shared data and AI models. Conventional confidential computing cannot realize the human workflow. This paper proposes and verifies ``Sandbox AI'' that can build precise AI models through efficient manual processes without revealing the shared data or models to one another. Sandbox AI utilizes privacy-preserving synthetic data generation and active learning on a confidential computing architecture. Specifically, Sandbox AI works in a sandbox container and generates synthetic data on the basis of real data, interactively extracts synthetic data that are useful for efficient model training while considering the real data distribution, discloses only the extracted synthetic data for annotation, and re-trains the model on the labeled synthetic data. Experimental results show that the achieved model accuracy is as good as that of conventional learning using real data with privacy violations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Social Informatics Laboratories",
              "dsl": ""
            }
          ],
          "personId": 121760
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Software Innovation Center",
              "dsl": ""
            }
          ],
          "personId": 121578
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Software Innovation Center",
              "dsl": ""
            }
          ],
          "personId": 121558
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NTT Social Informatics Laboratories",
              "dsl": ""
            }
          ],
          "personId": 121590
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Social Informatics Laboratories",
              "dsl": "NTT"
            }
          ],
          "personId": 121716
        }
      ]
    },
    {
      "id": 121830,
      "typeId": 13124,
      "title": "LayTex: A  Design Tool for Generating Customized Textile Sensor Layouts in Wearable Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1111",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Abstract：Smart textile sensors have attracted increasing interest in the domain of wearable computing for human motion monitoring. Previous studies have shown that textile sensor layout has a major impact on the effectiveness and performance of wearable prototypes. However, it is still a trick and time-consuming issue to determine textile sensor layout in a quantitative approach as it involves figuring out the number, placement, and even orientations of sensors, yet there is no streamlined digital platform or tool specifically addressing this issue. In this paper, we introduce LayTex, a digital tool capable of generating layout proposals for personalized scenarios, which aims at facilitating designers and researchers to construct prototypes efficiently. The preliminary evaluation with designers on smart garments for scoliosis indicates that LayTex has great potential to lower the barriers and simplify the process of textile prototype construction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "College of Design and Innovation",
              "dsl": ""
            }
          ],
          "personId": 121538
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "College of Design and Innovation",
              "dsl": ""
            }
          ],
          "personId": 121600
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121550
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Hangzhou City University",
              "dsl": ""
            }
          ],
          "personId": 121612
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hangzhou",
              "institution": "Zhejiang University",
              "dsl": ""
            }
          ],
          "personId": 121647
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121591
        }
      ]
    },
    {
      "id": 121831,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "C-Auth: Exploring the Feasibility of Using Egocentric View of Face Contour for User Authentication on Glasses",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-4348",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "In this paper, we present C-Auth, a novel authentication method for smart glasses that explores the feasibility of authenticating users using spatial facial information. Our system uses a down-facing camera in the middle of the glasses to capture facial contour lines from the nose and cheeks. The images captured by the camera are then processed and learned by a customized algorithm for authentication. To evaluate the system, we conducted a user study with 20 participants in three sessions on different days. Our system correctly identified the 20 users with a true positive rate of 98.0% (SD: 2.96%) and a false positive rate of 4.97% (2.88 %) across all three days. We conclude by discussing current limitations and challenges as well as the potential future applications for C-Auth.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITACA",
              "institution": "Cornell",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 121678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121566
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121676
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121721
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 121618
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 121675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITHACA",
              "institution": "Cornell University",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 121686
        }
      ]
    },
    {
      "id": 121832,
      "typeId": 13124,
      "title": "A Tailored Textile Sensor-based Wrap for Shoulder Complex Angles Monitoring",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1077",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "The shoulder joint plays a crucial role in the  recovery of upper limb function. However, conventional wearable technologies employed for monitoring shoulder joint movements predominantly rely on inertial sensing units (IMUs), which may suffer alignment errors and compromise the freedom and wearability experienced by patients during their daily activities. This paper contributes in two facets, first, it presents the design, implementation, and technical evaluation of a new wearable system, a customized unilateral shoulder wrap that utilizes flexible and breathable textile sensors. Diverging from earlier studies, our system not only facilitates the monitoring of glenohumeral joint angles but also concurrently tracks the movement angles of the scapula. Secondly, to estimate joint angles, we propose a specific model called the Channel-Temporal Encoding Network (CTEN), which leverages Transformer and Long Short-Term Memory (LSTM) architectures. In a preliminary technical evaluation, the results demonstrate root mean square errors (RMSEs) of 2.24˚ and 1.13˚ for the glenohumeral joint and scapula, respectively. This study is intended to contribute to the development of more advanced wearables tailored for shoulder joint rehabilitation training.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121550
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121554
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "College of Design and Innovation, Tongji University",
              "dsl": ""
            }
          ],
          "personId": 121537
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "College of Design and Innovation, Tongji University",
              "dsl": ""
            }
          ],
          "personId": 121581
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 121737
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121762
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Tongji University",
              "dsl": "College of Design and Innovation"
            }
          ],
          "personId": 121591
        }
      ]
    },
    {
      "id": 121833,
      "typeId": 13124,
      "title": "Physiological Indices to Predict Driver Situation Awareness in VR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1110",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Understanding drivers' states is essential for providing personalized interventions and adaptive feedback in vehicles, thereby ensuring safer driving and a more comfortable driver experience. As driving tasks necessitate understanding and reaction to the rapidly changing road environment and successful management of working memory to prevent dual-task interference, driver situation awareness (SA) should be primarily considered for such interventions and feedback. However, due to its complex nature, most of the current methods for measuring SA rely on questionnaires. In this work, we aim to develop sensor-based assessments of driver SA, especially towards road signs, which play an important role in immediate decision-making during driving. We collected eye-tracking data and physiological responses from 32 participants during simulated driving and annotated this data according to the levels of SA that drivers achieved. Our ensemble-based machine learning model that uses physiological measures and gaze-related features demonstrated an accuracy of 78.02% in a three-class driver SA prediction. Since SA is a key component in evaluating vehicular interfaces, our VR-based approach holds the potential for the iterative design of in-car infotainment applications and road environments. The method also lays a foundation for SA-adaptive cooperation between human drivers and AI in the context of advanced driver assistance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology",
              "dsl": "Human-Centered Intelligent Systems Lab"
            }
          ],
          "personId": 121729
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology",
              "dsl": "Human-Centered Intelligent Systems Lab"
            }
          ],
          "personId": 121681
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology",
              "dsl": "Human-Centered Intelligent Systems Lab."
            }
          ],
          "personId": 121610
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology",
              "dsl": "Human-Centered Intelligent Systems Lab"
            }
          ],
          "personId": 121759
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology",
              "dsl": "Human-Centered Intelligent Systems Lab"
            }
          ],
          "personId": 121687
        }
      ]
    },
    {
      "id": 121834,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "Challenges in Using Skin Conductance Responses for Assessments of Information Worker Productivity",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-3014",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "Breaks as discretionary self-interruptions can have beneficial effects on information worker productivity and well-being. This has design implications for potential productivity tools that can assess opportune moments to suggest these breaks. Electrodermal Activity (EDA) is a good psychophysiological metric to capture changes in autonomic activity resulting from affective states that necessitate breaks. Wrist-worn sensing platforms have been heralded as effective means for EDA-based affective state assessments in real-life scenarios. However, our study finds no correlation even in a controlled setting with a constrained operational definition of productivity and well-researched EDA measurement and processing techniques. We reflect on our rationale against prior success reported in laboratory and ambulatory assessments of EDA.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121740
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121494
        }
      ]
    },
    {
      "id": 121835,
      "typeId": 13124,
      "title": "Towards Detecting Tonic Information Processing Activities with Physiological Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1076",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Characterizing Information Processing Activities (IPAs) such as reading, listening, speaking, and writing, with physiological signals captured by wearable sensors can broaden the understanding of how people produce and consume information. However, sensors are highly sensitive to external conditions that are not trivial to control – not even in lab user studies. We conducted a pilot study (𝑁 = 7) to assess the robustness and sensitivity of physiological signals across four IPAs (READ, LISTEN, SPEAK, and WRITE) using multiple sensors. The collected signals include Electrodermal Activities, Blood Volume Pulse, gaze, and head motion. We observed consistent trends across participants, and ten features with statistically significant differences across the four IPAs. Our results provide preliminary quantitative evidence of differences in physiological responses when users encounter IPAs, revealing the necessity to inspect the signals separately according to the IPAs. The next step of this study moves into a specific context, information retrieval, and the IPAs are considered as the interaction modalities with the search system, for instance, submitting the search query by speaking or typing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC ",
              "city": "Melbourne ",
              "institution": "RMIT",
              "dsl": "School of Computing Technologies"
            },
            {
              "country": "Australia",
              "state": "VIC ",
              "city": "Melbourne ",
              "institution": "RMIT",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 121700
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 121605
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Computing Technologies"
            }
          ],
          "personId": 121730
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 121619
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "University of New South Wales",
              "dsl": "School of Computer Science and Engineering"
            }
          ],
          "personId": 121691
        }
      ]
    },
    {
      "id": 121836,
      "typeId": 13124,
      "title": "VoiceCogs: Interlocking Concurrent Voices for Separable Compressed Browsing with Screen Readers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1075",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127797
      ],
      "eventIds": [],
      "abstract": "Ensuring universal accessibility to information cannot be overstated. Unlike visual readers, however, screen reader users are given inefficient and restricted channels to acquire the given information. In particular, we focus on the initial step of information acquisition - quickly scanning the overall structure of a textual document so that the reader makes an informed decision about where to jump and read the details. While this step is inherently quick for visual users, screen reader users passively listen to the slow, sequential list of items read aloud. To close this gap, we call for a technique that accelerates screen reader users' scanning process. Our system,VoiceCogs takes multi-itemed text sources and synthesizes audio that concurrently plays multiple text-to-speech from a respective text source while facilitating the discernibility of individual sources. To this end, we devise and implement two interlocking techniques to minimize phonetic interferences between concurrent speeches. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121646
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Pohang",
              "institution": "POSTECH",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 121688
        }
      ]
    },
    {
      "id": 121837,
      "typeId": 13124,
      "title": "Departure Time Prediction Using Smartphone Data for Delayed-Full Charging BMS Algorithm",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1113",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Battery degradation, a gradual loss of capacity and performance brought on over time by frequent charging and discharging cycles, is a significant challenge to the widespread adoption of electric vehicles (EVs). This study proposes a \"delayed-full charging (DFC)\" battery management system (BMS) algorithm that delays full charging under selective conditions and fully charges the battery just before the end of charging to reduce battery degradation rate caused by fully charged state time. Our goal is to predict the charging end time based on an individual's departure time by capturing digital behavioral markers extracted from smartphone data, while minimizing reduction in driving range due to undesired predictions. Preliminary experiment was conducted with 41 subjects utilizing physical activity and smartphone activity data to assess the feasibility of the proposed approach. Our results demonstrate that the predictive model is capable of learning the departure behavior pattern using mobile passive features, achieving an average mean absolute error (MAE) of 0.2336.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanamdo",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121561
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121724
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121689
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Jeollanam-do",
              "city": "Naju-si",
              "institution": "KENTECH",
              "dsl": ""
            }
          ],
          "personId": 121576
        }
      ]
    },
    {
      "id": 121838,
      "typeId": 12924,
      "durationOverride": 4,
      "title": "TouchLog: Finger Micro Gesture Recognition Using Photo-Reflective Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23e-5758",
      "source": "PCS",
      "trackId": 12287,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "Fingertip input allows for interactions that are natural, easy to perform, and socially acceptable. It also has advantages in terms of low physical demand, confidentiality, and haptic feedback. In this study, we propose TouchLog, a fingernail-type device that uses skin deformation of the fingertip to identify finger micro gestures written with the thumb on the index finger. TouchLog is attached to the index fingernail and allows for one-handed fingertip input without compromising the haptic feedback on the finger.\r\nTo evaluate the accuracy of 11 types of finger micro gesture recognition, we conducted a user study (N = 10) and obtained an average identification accuracy of 91.5\\% (SD = 3.1\\%). A continuous input method using skin deformation and contact pressure was also examined, and its usefulness as a wearable device was discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 121543
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 121568
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 121635
        }
      ]
    },
    {
      "id": 121839,
      "typeId": 13124,
      "title": "Inspire creativity with ORIBA: Transform Artists' Original Characters into Chatbots through Large Language Model",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1079",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "This research delves into the intersection of illustration art and artificial intelligence (AI), focusing on illustrators' engagement with AI agents that embody their original characters (OCs). We introduce 'ORIBA', a customizable AI chatbot that facilitates illustrators to converse with their OCs. This approach allows artists to not only receive responses from their OCs but also observe how they think and behave. Despite the existing tension between artists and AI, our study explores innovative collaboration methods that are inspirational to illustrators. By examining the impact of AI on the creative process and the boundaries of authorship, we aim to enhance human-AI interactions in creative fields, with potential applications extending beyond illustration to interactive storytelling and more.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Royal College of Art",
              "dsl": "Computer Science Research Centre"
            }
          ],
          "personId": 121520
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology,",
              "dsl": "Digital Media, School of Literature, Media, and Communication"
            }
          ],
          "personId": 121519
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Emerging Interdisciplinary Areas"
            }
          ],
          "personId": 121698
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "rct.ai",
              "dsl": ""
            }
          ],
          "personId": 121658
        }
      ]
    },
    {
      "id": 121840,
      "typeId": 13124,
      "title": "Automatic, Manual, or Hybrid? A Preliminary Investigation of Users’ Perception of Features for Supporting Notification Management",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23b-1118",
      "source": "PCS",
      "trackId": 12286,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127796
      ],
      "eventIds": [],
      "abstract": "Mobile notifications, crucial to our daily activities and informational needs, are often undermined by insufficient management features. Previous research has suggested enhancements like automatic sorting, filtering, and categorization, but empirical evidence supporting these strategies is yet to be seen. This study bridges the gap, developing an Android application to assess these proposed features' efficacy in improving notification management efficiency and user experience. We utilized the Experience Sampling Method (ESM) for in-depth user insights, and our preliminary findings indicate a perceived superiority for a hybrid system combining automatic and manual functionalities, over systems solely dependent on either approach. This research paves the way for an optimized notification system, better equipped to assist users in managing mobile notifications effectively.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121615
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": " Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Institute of Multimedia Engineering"
            }
          ],
          "personId": 121524
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121719
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121673
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121625
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 121702
        }
      ]
    },
    {
      "id": 127853,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster presentation",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "a0b6ec06-59d1-4087-93c0-135c9ded6369",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 127859,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "7ba12561-1ae8-4b74-a29e-460f08d6a1a7",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128269,
      "typeId": 12924,
      "title": "ViSig: Automatic Interpretation of Visual Body Signals Using On-Body Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580797",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "Visual body signals are designated body poses that deliver an application-specific message. Such signals are widely used for fast message communication in sports (signaling by umpires and referees), transportation (naval officers and aircraft marshallers), and construction (signaling by riggers and crane operators), to list a few examples. Automatic interpretation of such signals can help maintaining safer operations in these industries, help in record-keeping for auditing or accident investigation purposes, and function as a score-keeper in sports. When automation of these signals is desired, it is traditionally performed from a viewer's perspective by running computer vision algorithms on camera feeds. However, computer vision based approaches suffer from performance deterioration in scenarios such as lighting variations, occlusions, etc., might face resolution limitations, and can be challenging to install. Our work, ViSig, breaks with tradition by instead deploying on-body sensors for signal interpretation. Our key innovation is the fusion of ultra-wideband (UWB) sensors for capturing on-body distance measurements, inertial sensors (IMU) for capturing orientation of a few body segments, and photodiodes for finger signal recognition, enabling a robust interpretation of signals. By deploying only a small number of sensors, we show that body signals can be interpreted unambiguously in many different settings, including in games of Cricket, Baseball, and Football, and in operational safety use-cases such as crane operations and flag semaphores for maritime navigation, with > 90% accuracy. Overall, we have seen substantial promise in this approach and expect a large body of future follow-on work to start using UWB and IMU fused modalities for the more general human pose estimation problems.\nhttps://dl.acm.org/doi/10.1145/3580797",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129065
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129063
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129061
        }
      ]
    },
    {
      "id": 128270,
      "typeId": 12924,
      "title": "Auditable XR: The Practicalities of Mobile System Transparency",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3495001",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "\"Virtual, Augmented and Mixed Reality (XR) technologies are becoming increasingly pervasive. However, the contextual nature of XR, and its tight coupling of the digital and physical environments, brings real propensity for loss and harm.\nThis means that auditability---the ability to inspect how a system operates---will be crucial for dealing with incidents as they occur, by providing the information enabling rectification, repair and recourse. However, supporting audit in XR brings considerations, as the process of capturing audit data itself has implications and challenges, both for the application (e.g., overheads) and more broadly.\nThis paper explores the practicalities of auditing XR systems, characterises the tensions between audit and other considerations, and argues the need for flexible tools enabling the management of such. In doing so, we introduce Droiditor, a configurable open-source Android toolkit that enables the runtime capture of audit-relevant data from mobile applications. We use Droiditor as a means to indicate some potential implications of audit data capture, demonstrate how greater configurability can assist in managing audit-related concerns, and discuss the potential considerations that result. Given the societal demands for more transparent and accountable systems, our broader aim is to draw attention to auditability, highlighting tangible ways forward and areas for future work.\nhttps://doi.org/10.1145/3495001\"",
      "authors": [
        {
          "affiliations": [],
          "personId": 129270
        },
        {
          "affiliations": [],
          "personId": 129269
        },
        {
          "affiliations": [],
          "personId": 129268
        }
      ]
    },
    {
      "id": 128271,
      "typeId": 12924,
      "title": "EarSpiro: Earphone-based Spirometry for Lung Function Assessment",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569480",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "\"Spirometry is the gold standard for evaluating lung functions. Recent research has proposed that mobile devices can measure lung function indices cost-efficiently. However, these designs fall short in two aspects. First, they cannot provide the flow-volume (F-V) curve, which is more informative than lung function indices. Secondly, these solutions lack inspiratory measurement, which is sensitive to lung diseases such as variable extrathoracic obstruction. In this paper, we present EarSpiro, an earphone-based solution that interprets the recorded airflow sound during a spirometry test into an F-V curve, including both the expiratory and inspiratory measurements. EarSpiro leverages a convolutional neural network (CNN) and a recurrent neural network (RNN) to capture the complex correlation between airflow sound and airflow speed. Meanwhile, EarSpiro adopts a clustering-based segmentation algorithm to track the weak inspiratory signals from the raw audio recording to enable inspiratory measurement. We also enable EarSpiro with daily mouthpiece-like objects such as a funnel using transfer learning and a decoder network with the help of only a few true lung function indices from the user. Extensive experiments with 60 subjects show that EarSpiro achieves mean errors of 0.20L/s and 0.42L/s for expiratory and inspiratory flow rate estimation, and 0.61L/s and 0.83L/s for expiratory and inspiratory F-V curve estimation. The mean correlation coefficient between the estimated F-V curve and the true one is 0.94. The mean estimation error for four common lung function indices is 7.3%.\nhttps://dl.acm.org/doi/10.1145/3569480\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology"
            }
          ],
          "personId": 128939
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology"
            }
          ],
          "personId": 128938
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 128936
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 128934
        }
      ]
    },
    {
      "id": 128272,
      "typeId": 12924,
      "title": "Generalization and Personalization of Mobile Sensing-Based Mood Inference Models: An Analysis of College Students in Eight Countries",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569483",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "Mood inference with mobile sensing data has been studied in ubicomp literature over the last decade. This inference enables context-aware and personalized user experiences in general mobile apps and valuable feedback and interventions in mobile health apps. However, even though model generalization issues have been highlighted in many studies, the focus has always been on improving the accuracies of models using different sensing modalities and machine learning techniques, with datasets collected in homogeneous populations. In contrast, less attention has been given to studying the performance of mood inference models to assess whether models generalize to new countries. In this study, we collected a mobile sensing dataset with 329K self-reports from 678 participants in eight countries (China, Denmark, India, Italy, Mexico, Mongolia, Paraguay, UK) to assess the effect of geographical diversity on mood inference models. We define and evaluate country-specific (trained and tested within a country), continent-specific (trained and tested within a continent), country-agnostic (tested on a country not seen on training data), and multi-country (trained and tested with multiple countries) approaches trained on sensor data for two mood inference tasks with population-level (non-personalized) and hybrid (partially personalized) models. We show that partially personalized country-specific models perform the best yielding area under the receiver operating characteristic curve (AUROC) scores of the range 0.78--0.98 for two-class (negative vs. positive valence) and 0.76--0.94 for three-class (negative vs. neutral vs. positive valence) inference. Further, with the country-agnostic approach, we show that models do not perform well compared to country-specific settings, even when models are partially personalized. We also show that continent-specific models outperform multi-country models in the case of Europe. Overall, we uncover generalization issues of mood inference models to new countries and how the geographical similarity of countries might impact mood inference.\nhttps://dl.acm.org/doi/10.1145/3569483",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Martigny",
              "institution": "Idiap Research Institute"
            }
          ],
          "personId": 129083
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Martigny",
              "institution": "Idiap Research Institute"
            }
          ],
          "personId": 129315
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "city": "Copenhagen",
              "institution": "Aalborg University"
            }
          ],
          "personId": 129313
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "city": "Copenhagen",
              "institution": "Aalborg University"
            }
          ],
          "personId": 129311
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Kollam",
              "institution": "Amrita Vishwa Vidyapeetham"
            }
          ],
          "personId": 129329
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kerala",
              "city": "Kollam",
              "institution": "Amrita Vishwa Vidyapeetham"
            }
          ],
          "personId": 129327
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "city": "San Luis Potosí",
              "institution": "Instituto Potosino de Investigación Científica y Tecnológica"
            }
          ],
          "personId": 129325
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Changchun",
              "institution": "Jilin University"
            }
          ],
          "personId": 129323
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Changchun",
              "institution": "Jilin University"
            }
          ],
          "personId": 129321
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "London School of Economics and Political Science (LSE)"
            }
          ],
          "personId": 129320
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "London School of Economics and Political Science (LSE)"
            }
          ],
          "personId": 129319
        },
        {
          "affiliations": [
            {
              "country": "Mongolia",
              "city": "Ulaanbaatar",
              "institution": "National University of Mongolia"
            }
          ],
          "personId": 129337
        },
        {
          "affiliations": [
            {
              "country": "Mongolia",
              "city": "Ulaanbaatar",
              "institution": "National University of Mongolia"
            }
          ],
          "personId": 129335
        },
        {
          "affiliations": [],
          "personId": 129333
        },
        {
          "affiliations": [],
          "personId": 129331
        },
        {
          "affiliations": [],
          "personId": 129349
        },
        {
          "affiliations": [],
          "personId": 129347
        },
        {
          "affiliations": [],
          "personId": 129345
        },
        {
          "affiliations": [],
          "personId": 129343
        },
        {
          "affiliations": [],
          "personId": 129342
        },
        {
          "affiliations": [],
          "personId": 129341
        },
        {
          "affiliations": [],
          "personId": 129359
        },
        {
          "affiliations": [],
          "personId": 129357
        },
        {
          "affiliations": [],
          "personId": 129355
        },
        {
          "affiliations": [],
          "personId": 129353
        },
        {
          "affiliations": [],
          "personId": 129351
        },
        {
          "affiliations": [],
          "personId": 129369
        }
      ]
    },
    {
      "id": 128273,
      "typeId": 12924,
      "title": "Affective Touch as Immediate and Passive Wearable Intervention",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569484",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "We investigated affective touch as a new pathway to passively mitigate in-the-moment anxiety. While existing mobile interventions offer great promises for health and well-being, they typically focus on achieving long-term effects such as shifting behaviors. As such, most mobile interventions are not applicable to provide immediate help in acute conditions -- when a user experiences a high anxiety level during ongoing events (e.g., completing high-stake tasks or mitigating interpersonal conflicts). A few works have developed passive interventions that are effective in-the-moment by leveraging breathing regulations and biofeedback. In this paper, we drew on neuroscientific findings on affective touch, the slow stroking on hairy skin that can elicit innate pleasantness and evaluated affective touch as a mobile health intervention. To induce affective touch, we first engineered a wearable device that renders a soft stroking sensation on the user's forearm. Then, we conducted a between-group experiment, in which participants underwent high-stress situations with/without receiving affective touch and post-experiment interviews, with 24 participants. Our results showed that participants who received affective touch experienced lower state anxiety and the same physiological stress response level compared to the control group participants. We also found that affective touch facilitated emotion regulation by rendering pleasantness, providing emotional support, and shifting attention. Finally, we discussed the immediate effect of affective touch on anxiety and physiological stress, the benefits of affective touch as a passive intervention, and the implementation considerations to use affective touch in just-in-time systems.\nhttps://dl.acm.org/doi/10.1145/3569484",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell University"
            }
          ],
          "personId": 129463
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129461
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell University"
            }
          ],
          "personId": 129459
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech"
            }
          ],
          "personId": 129457
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129455
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129472
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129453
        }
      ]
    },
    {
      "id": 128274,
      "typeId": 12924,
      "title": "Investigating In-Situ Personal Health Data Qeries on Smartwatches",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569481",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "\"Smartwatches enable not only the continuous collection of but also ubiquitous access to personal health data. However, exploring this data in-situ on a smartwatch is often reserved for singular and generic metrics, without the capacity for further insight. To address our limited knowledge surrounding smartwatch data exploration needs, we collect and characterize desired personal health data queries from smartwatch users. We conducted a week-long study (N = 18), providing participants with an application for recording responses that contain their query and current activity related information, throughout their daily lives. From the responses, we curated a dataset of 205 natural language queries. Upon analysis, we highlight a new preemptive and proactive data insight category, an activity-based lens for data exploration, and see the desired use of a smartwatch for data exploration throughout daily life. To aid in future research and the development of smartwatch health applications, we contribute the dataset and discuss implications of our findings.\nhttps://dl.acm.org/doi/10.1145/3569481\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia"
            }
          ],
          "personId": 129452
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129492
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland"
            }
          ],
          "personId": 129490
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Kelowna",
              "institution": "University of British Columbia (Okanagan)"
            }
          ],
          "personId": 129488
        }
      ]
    },
    {
      "id": 128275,
      "typeId": 12924,
      "title": "BLEselect: Gestural IoT Device Selection via Bluetooth Angle of Arrival Estimation from Smart Glasses",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569482",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "\"Spontaneous selection of IoT devices from the head-mounted device is key for user-centered pervasive interaction. BLEselect enables users to select an unmodified Bluetooth 5.1 compatible IoT device by nodding at, pointing at, or drawing a circle in the air around it. We designed a compact antenna array that fits on a pair of smart glasses to estimate the Angle of Arrival (AoA) of IoT and wrist-worn devices' advertising signals. We then developed a sensing pipeline that supports all three selection gestures with lightweight machine learning models, which are trained in real-time for both hand gestures. Extensive characterizations and evaluations show that our system is accurate, natural, low-power, and privacy-preserving. Despite the small effective size of the antenna array, our system achieves a higher than 90% selection accuracy within a 3 meters distance in front of the user. In a user study that mimics real-life usage cases, the overall selection accuracy is 96.7% for a diverse set of 22 participants in terms of age, technology savviness, and body structures.\nhttps://dl.acm.org/doi/10.1145/3569482\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences"
            }
          ],
          "personId": 128915
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Southeast University"
            }
          ],
          "personId": 128914
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 128912
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences"
            }
          ],
          "personId": 128910
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "-NONE-",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences"
            }
          ],
          "personId": 128908
        }
      ]
    },
    {
      "id": 128276,
      "typeId": 12924,
      "title": "Embracing Consumer-level UWB-equipped Devices for Fine-grained Wireless Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569487",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "RF sensing has been actively exploited in the past few years to enable novel IoT applications. Among different wireless technologies, WiFi-based sensing is most popular owing to the pervasiveness of WiFi infrastructure. However, one critical issue associated with WiFi sensing is that the information required for sensing can not be obtained from consumer-level devices such as smartphones or smart watches. The commonly-seen WiFi devices in our everyday lives actually can not be utilized for sensing. Instead, dedicated hardware with a specific WiFi card (e.g., Intel 5300) needs to be used for WiFi sensing. This paper involves Ultra-Wideband (UWB) into the ecosystem of RF sensing and makes RF sensing work on consumer-level hardware such as smartphones and smart watches for the first time. We propose a series of methods to realize UWB sensing on consumer-level electronics without any hardware modification. By leveraging fine-grained human respiration monitoring as the application example, we demonstrate that the achieved performance on consumer-level electronics is comparable to that achieved using dedicated UWB hardware. We show that UWB sensing hosted on consumer-level electronics is able to achieve fine granularity, robustness against interference and also multi-target sensing, pushing RF sensing one step towards real-life adoption.\nhttps://dl.acm.org/doi/10.1145/3569487",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Chinese Academy of Sciences"
            }
          ],
          "personId": 128990
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "Institut Polytechnique de Paris"
            }
          ],
          "personId": 128987
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst"
            }
          ],
          "personId": 128984
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences"
            }
          ],
          "personId": 128981
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tencent Inc"
            }
          ],
          "personId": 128979
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences"
            }
          ],
          "personId": 129003
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences"
            }
          ],
          "personId": 128977
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "Institut Polytechnique de Paris"
            }
          ],
          "personId": 128976
        }
      ]
    },
    {
      "id": 128277,
      "typeId": 12924,
      "title": "The Tale of a Complicated Relationship: Insights from Users’ Love/Breakup Letters to Their Smartphones before and during the COVID-19 Pandemic",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580792",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Smartphones have gotten under public scrutiny due to their ostensible negative impact on users' well-being. Nonetheless, users and related work report positive aspects of smartphones, too. We investigated this discrepancy through the prism of the emotional user-smartphone relationship by having people write love/breakup letters to their smartphones. We gathered 82 letters - 42 before and 40 during the COVID-19 pandemic. We found a mixed nature regarding the distribution of love and breakup letters and associated emotions based on the revisited OCC-model of emotions - with a slight shift towards the negative emotional spectrum during the COVID-19 pandemic. Furthermore, we performed an extensive qualitative analysis of 819 user statements extracted from the letters, resulting in a connection of emotions to 17 smartphone features and eight themes of real-life consequences of smartphone use. We then identified eight common patterns of this connection, classified as smartphone roles. The collected letters mostly model a complex user-smartphone relationship, comprising different roles depending on users' inner and outer context. We discuss how HCI could help in shaping the complex user-smartphone relationship in future research and suggest supporting a healthy balance between users' daily life and smartphone use.\nhttps://doi.org/10.1145/3580792\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129486
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129484
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129482
        }
      ]
    },
    {
      "id": 128278,
      "typeId": 12924,
      "title": "ForceSticker: Wireless, Batteryless, Thin & Flexible Force Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580793",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "\"Any two objects in contact with each other exert a force that could be simply due to gravity or mechanical contact, such as any ubiquitous object exerting weight on a platform or the contact between two bones at our knee joints. The most ideal way of capturing these contact forces is to have a flexible force sensor which can conform well to the contact surface. Further, the sensor should be thin enough to not affect the contact physics between the two objects. In this paper, we showcase the design of such thin, flexible sticker-like force sensors dubbed as 'ForceStickers', ushering into a new era of miniaturized force sensors. ForceSticker achieves this miniaturization by creating new class of capacitive force sensors which avoid both batteries, as well as wires. The wireless and batteryless readout is enabled via hybrid analog-digital backscatter, by piggybacking analog sensor data onto a digitally identified RFID link. Hence, ForceSticker finds natural applications in space and battery-constraint in-vivo usecases, like force-sensor backed orthopaedic implants, surgical robots. Further, ForceSticker finds applications in ubiquiti-constraint scenarios. For example, these force-stickers enable cheap, digitally readable barcodes that can provide weight information, with possible usecases in warehouse integrity checks. To meet these varied application scenarios, we showcase the general framework behind design of ForceSticker. With ForceSticker framework, we design 4mm*2mm sensor prototypes, with two different polymer layers of ecoflex and neoprene rubber, having force ranges of 0-6N and 0-40N respectively, with readout errors of 0.25, 1.6 N error each (<5% of max. force). Further, we stress test ForceSticker by >10,000 force applications without significant error degradation. We also showcase two case-studies onto the possible applications of ForceSticker: sensing forces from a toy knee-joint model and integrity checks of warehouse packaging.\nhttps://dl.acm.org/doi/10.1145/3580793\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129409
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129407
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129404
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129401
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129398
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Pleasanton",
              "institution": "Foothill High School"
            }
          ],
          "personId": 129395
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129421
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129419
        }
      ]
    },
    {
      "id": 128279,
      "typeId": 12924,
      "title": "ODSearch: Fast and Resource Efficient On-device Natural Language Search for Fitness Trackers' Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569488",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "Mobile and wearable technologies have promised significant changes to the healthcare industry. Although cutting-edge communication and cloud-based technologies have allowed for these upgrades, their implementation and popularization in low-income countries have been challenging. We propose ODSearch, an On-device Search framework equipped with a natural language interface for mobile and wearable devices. To implement search, ODSearch employs compression and Bloom filter, it provides near real-time search query responses without network dependency. In particular, the Bloom filter reduces the temporal scope of the search and compression reduces the size of the data to be searched. Our experiments were conducted on a mobile phone and smartwatch. We compared ODSearch with current state-of-the-art search mechanisms, and it outperformed them on average by 53 times in execution time, 26 times in energy usage, and 2.3% in memory utilization.\nhttps://dl.acm.org/doi/10.1145/3569488",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Boston University"
            }
          ],
          "personId": 128953
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Boston University"
            }
          ],
          "personId": 128950
        }
      ]
    },
    {
      "id": 128280,
      "typeId": 12924,
      "title": "GLOBEM: Cross-Dataset Generalization of Longitudinal Human Behavior Modeling",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569485",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "There is a growing body of research revealing that longitudinal passive sensing data from smartphones and wearable devices can capture daily behavior signals for human behavior modeling, such as depression detection. Most prior studies build and evaluate machine learning models using data collected from a single population. However, to ensure that a behavior model can work for a larger group of users, its generalizability needs to be verified on multiple datasets from different populations. We present the first work evaluating cross-dataset generalizability of longitudinal behavior models, using depression detection as an application. We collect multiple longitudinal passive mobile sensing datasets with over 500 users from two institutes over a two-year span, leading to four institute-year datasets. Using the datasets, we closely re-implement and evaluated nine prior depression detection algorithms. Our experiment reveals the lack of model generalizability of these methods. We also implement eight recently popular domain generalization algorithms from the machine learning community. Our results indicate that these methods also do not generalize well on our datasets, with barely any advantage over the naive baseline of guessing the majority. We then present two new algorithms with better generalizability. Our new algorithm, Reorder, significantly and consistently outperforms existing methods on most cross-dataset generalization setups. However, the overall advantage is incremental and still has great room for improvement. Our analysis reveals that the individual differences (both within and between populations) may play the most important role in the cross-dataset generalization challenge. Finally, we provide an open-source benchmark platform GLOBEM- short for Generalization of Longitudinal BEhavior Modeling - to consolidate all 19 algorithms. GLOBEM can support researchers in using, developing, and evaluating different longitudinal behavior modeling methods. We call for researchers' attention to model generalizability evaluation for future longitudinal human behavior modeling studies.\nhttps://dl.acm.org/doi/10.1145/3569485",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128791
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128790
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128808
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 128806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 128804
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 128820
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington, Seattle"
            }
          ],
          "personId": 128818
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128816
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128814
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128813
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128812
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 128830
        },
        {
          "affiliations": [],
          "personId": 128828
        },
        {
          "affiliations": [],
          "personId": 128826
        }
      ]
    },
    {
      "id": 128281,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "SleepMore: Inferring Sleep Duration at Scale via Multi-Device WiFi Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569489",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "\"The availability of commercial wearable trackers equipped with features to monitor sleep duration and quality has enabled more useful sleep health monitoring applications and analyses. However, much research has reported the challenge of long-term user retention in sleep monitoring through these modalities. Since modern Internet users own multiple mobile devices, our work explores the possibility of employing ubiquitous mobile devices and passive WiFi sensing techniques to predict sleep duration as the fundamental measure for complementing long-term sleep monitoring initiatives. In this paper, we propose SleepMore, an accurate and easy-to-deploy sleep-tracking approach based on machine learning over the user's WiFi network activity. It first employs a semi-personalized random forest model with an infinitesimal jackknife variance estimation method to classify a user's network activity behavior into sleep and awake states per minute granularity. Through a moving average technique, the system uses these state sequences to estimate the user's nocturnal sleep period and its uncertainty rate. Uncertainty quantification enables SleepMore to overcome the impact of noisy WiFi data that can yield large prediction errors. We validate SleepMore using data from a month-long user study involving 46 college students and draw comparisons with the Oura Ring wearable. Beyond the college campus, we evaluate SleepMore on non-student users of different housing profiles. Our results demonstrate that SleepMore produces statistically indistinguishable sleep statistics from the Oura ring baseline for predictions made within a 5% uncertainty rate. These errors range between 15-28 minutes for determining sleep time and 7-29 minutes for determining wake time, proving statistically significant improvements over prior work. Our in-depth analysis explains the sources of errors.\nhttps://dl.acm.org/doi/10.1145/3569489\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst"
            }
          ],
          "personId": 128836
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128835
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts, Amherst"
            }
          ],
          "personId": 128834
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "Duke-NUS Medical School"
            }
          ],
          "personId": 128857
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst"
            }
          ],
          "personId": 128855
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "Singapore Management University"
            }
          ],
          "personId": 128853
        }
      ]
    },
    {
      "id": 128282,
      "typeId": 12924,
      "title": "From 2D to 3D: Facilitating Single-Finger Mid-Air Typing on QWERTY Keyboards with Probabilistic Touch Modeling",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580829",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "Mid-air text entry on virtual keyboards suffers from the lack of tactile feedback, which brings challenges to both tap detection and input prediction. In this paper, we explored the feasibility of single-finger typing on virtual QWERTY keyboards in mid-air. We first conducted a study to examine users' 3D typing behavior on different sizes of virtual keyboards. Results showed that the participants perceived the vertical projection of the lowest point on the keyboard during a tap as the target location and inferring taps based on the intersection between the finger and the keyboard was not applicable. Aiming at this challenge, we derived a novel input prediction algorithm that took the uncertainty in tap detection into a calculation as probability, and performed probabilistic decoding that could tolerate false detection. We analyzed the performance of the algorithm through a full-factorial simulation. Results showed that the SVM-based probabilistic touch detection together with a 2D elastic probabilistic decoding algorithm (elasticity = 2) could achieve the optimal top-5 accuracy of 94.2%. In the evaluation user study, the participants reached a single-finger typing speed of 26.1 WPM with 3.2% uncorrected word-level error rate, which was significantly better than both tap-based and gesture-based baseline techniques. Also, the proposed technique received the highest preference score from the users, proving its usability in real text entry tasks.\nhttps://dl.acm.org/doi/10.1145/3580829",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128892
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128891
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara"
            }
          ],
          "personId": 128887
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128884
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128881
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128878
        }
      ]
    },
    {
      "id": 128283,
      "typeId": 12924,
      "title": "Narrative-Based Visual Feedback to Encourage Sustained Physical Activity: A Field Trial of the WhoIsZuki Mobile Health Platform",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580786",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "\"Stories are a core way human beings make meaning and sense of the world and our lived experiences, including our behaviors, desires, and goals. Narrative structures, both visual and textual, help us understand and act on information, while also evoking strong emotions. Focusing on the health context, this research examines the effectiveness of narrative-based feedback in motivating physical activity behaviors and underlying attitudes over longitudinal periods. After collecting two weeks of baseline physical activity levels, N=39 participants installed our smartphone application, WhoIsZuki. The WhoIsZuki app supports goal setting and semi-automated activity tracking, and it provides an ambient display that visually encodes these tracked activities as well as progress toward goals. Half of participants received a version of the interface that supplied behavioral feedback in the form of a multi-chapter episodic narrative, while the other half received a control condition version that provided an aesthetically-similar visualization but without any characterization, episodic structure, dramatic effect, or other narrative elements. After interacting with these versions for four months, our analysis showed that participants receiving the multi-chapter narrative feedback performed more physical activity, achieved more goals, experienced more positive psychological shifts, and overall engaged more meaningfully with the digital intervention.\nhttps://dl.acm.org/doi/10.1145/3580786\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 128864
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford"
            }
          ],
          "personId": 128890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford"
            }
          ],
          "personId": 128888
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 128886
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 128883
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 128880
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 128877
        }
      ]
    },
    {
      "id": 128284,
      "typeId": 12924,
      "title": "eat2pic: An Eating-Painting Interactive System to Nudge Users into Making Healthier Diet Choices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580784",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "\"Given the complexity of human eating behaviors, developing interactions to change the way users eat or their choice of meals is challenging. In this study, we propose an interactive system called eat2pic designed to encourage healthy eating habits such as adopting a balanced diet and eating more slowly, by refraining the task of selecting meals into that of adding color to landscape pictures. The eat2pic system comprises a sensor-equipped chopstick (one of a pair) and two types of digital canvases. It provides fast feedback by recognizing a user's eating behavior in real time and displaying the result on a small canvas called \"\"one-meal eat2pic.\"\" Moreover, it also provides slow feedback by displaying the number of colors of foods that the user consumed on a large canvas called \"\"one-week eat2pic.\"\" The former was designed and implemented as a guide to help people eat more slowly, and the latter to encourage people to select more balanced menus. Through two user studies, we explored the experience of interaction with eat2pic, in which users' daily eating behavior was reflected in a series of \"\"paintings,\"\" that is, images produced by the automated system. The experimental results suggest that eat2pic may provide an opportunity for reflection in meal selection and while eating, as well as assist users in becoming more aware of how they are eating and how balanced their daily meals are. We expect this system to inspire users' curiosity about different diets and ways of eating. This research also contributes to expanding the design space for products and services related to dietary support.\nhttps://doi.org/10.1145/3580784\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Fukuoka",
              "city": "Fukuoka",
              "institution": "Kyushu University"
            }
          ],
          "personId": 128824
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 128822
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 128840
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Nara",
              "city": "Ikoma",
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 128838
        }
      ]
    },
    {
      "id": 128285,
      "typeId": 12924,
      "title": "A User-Centric Evaluation of Smart Home Resolution Approaches for Conflicts Between Routines",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3581997",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "abstract": "\"With the increasing adoption of smart home devices, users rely on device automation to control their homes. This automation commonly comes in the form of smart home routines, an abstraction available via major vendors. Yet, questions remain about how a system should best handle conflicts in which different routines access the same devices simultaneously. In particular---among the myriad ways a smart home system could handle conflicts, which of them are currently utilized by existing systems, and which ones result in the highest user satisfaction? We investigate the first question via a survey of existing literature and find a set of conditions, modifications, and system strategies related to handling conflicts. We answer the second question via a scenario-based Mechanical-Turk survey of users interested in owning smart home devices and current smart home device owners (N=197). We find that: (i) there is no context-agnostic strategy that always results in high user satisfaction, and (ii) users' personal values frequently form the basis for shaping their expectations of how routines should execute.\nhttps://dl.acm.org/doi/10.1145/3581997\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois, Urbana-Champaign"
            }
          ],
          "personId": 128642
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 128640
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "University of Illinois at Urbana Champaign"
            }
          ],
          "personId": 128638
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "UIUC"
            }
          ],
          "personId": 128637
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "UIUC"
            }
          ],
          "personId": 128636
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois"
            }
          ],
          "personId": 128632
        }
      ]
    },
    {
      "id": 128286,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "Feeling the Temperature of the Room: Unobtrusive Thermal Display of Engagement during Group Communication",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580820",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "\"Thermal signals have been explored in HCI for emotion-elicitation and enhancing two-person communication, showing that temperature invokes social and emotional signals in individuals. Yet, extending these findings to group communication is missing. We investigated how thermal signals can be used to communicate group affective states in a hybrid meeting scenario to help people feel connected over a distance. We conducted a lab study (N=20 participants) and explored wrist-worn thermal feedback to communicate audience emotions. Our results show that thermal feedback is an effective method of conveying audience engagement without increasing workload and can help a presenter feel more in tune with the audience. We outline design implications for real-world wearable social thermal feedback systems for both virtual and in-person communication that support group affect communication and social connectedness. Thermal feedback has the potential to connect people across distances and facilitate more effective and dynamic communication in multiple contexts.\nhttps://dl.acm.org/doi/10.1145/3580820\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129500
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129499
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129498
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "Aalto University"
            }
          ],
          "personId": 129496
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128603
        }
      ]
    },
    {
      "id": 128287,
      "typeId": 12924,
      "title": "Voicify Your UI: Towards Android App Control with Voice Commands",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3581998",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "Nowadays, voice assistants help users complete tasks on the smartphone with voice commands, replacing traditional touchscreen interactions when such interactions are inhibited. However, the usability of those tools remains moderate due to the problems in understanding rich language variations in human commands, along with efficiency and comprehensibility issues. Therefore, we introduce Voicify, an Android virtual assistant that allows users to interact with on-screen elements in mobile apps through voice commands. Using a novel deep learning command parser, Voicify interprets human verbal input and performs matching with UI elements. In addition, the tool can directly open a specific feature from installed applications by fetching application code information to explore the set of in-app components. Our command parser achieved 90% accuracy on the human command dataset. Furthermore, the direct feature invocation module achieves better feature coverage in comparison to Google Assistant. The user study demonstrates the usefulness of Voicify in real-world scenarios.\nhttps://dl.acm.org/doi/10.1145/3581998",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University"
            }
          ],
          "personId": 129190
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University"
            }
          ],
          "personId": 129188
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Melbourne",
              "institution": "Monash University"
            }
          ],
          "personId": 129230
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Melbourne",
              "institution": "Monash University"
            }
          ],
          "personId": 129228
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "CSIRO's Data61"
            }
          ],
          "personId": 129226
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University"
            }
          ],
          "personId": 129224
        }
      ]
    },
    {
      "id": 128288,
      "typeId": 12924,
      "title": "WristAcoustic: Through-Wrist Acoustic Response Based Authentication for Smartwatches",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569473",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "PIN and pattern lock are difficult to accurately enter on small watch screens, and are vulnerable against guessing attacks. To address these problems, this paper proposes a novel implicit biometric scheme based on through-wrist acoustic responses. A cue signal is played on a surface transducer mounted on the dorsal wrist and the acoustic response recorded by a contact microphone on the volar wrist. We build classifiers using these recordings for each of three simple hand poses (relax, fist and open), and use an ensemble approach to make final authentication decisions. In an initial single session study (N=25), we achieve an Equal Error Rate (EER) of 0.01%, substantially outperforming prior on-wrist biometric solutions. A subsequent five recall-session study (N=20) shows reduced performance with 5.06% EER. We attribute this to increased variability in how participants perform hand poses over time. However, after retraining classifiers performance improved substantially, ultimately achieving 0.79% EER. We observed most variability with the relax pose. Consequently, we achieve the most reliable multi-session performance by combining the fist and open poses: 0.51% EER. Further studies elaborate on these basic results. A usability evaluation reveals users experience low workload as well as reporting high SUS scores and fluctuating levels of perceived exertion: moderate during initial enrollment dropping to slight during authentication. A final study examining performance in various poses and in the presence of noise demonstrates the system is robust to such disturbances and likely to work well in wide range of real-world contexts.\nhttps://dl.acm.org/doi/10.1145/3569473",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128784
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128782
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "UNIST"
            }
          ],
          "personId": 128800
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "UNIST"
            }
          ],
          "personId": 128798
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "Ulsan National Institute of Science and Technology"
            }
          ],
          "personId": 128796
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128794
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "UNIST"
            }
          ],
          "personId": 128792
        }
      ]
    },
    {
      "id": 128289,
      "typeId": 12924,
      "title": "PACT: Scalable, Long-Range Communication for Monitoring and Tracking Systems Using Battery-less Tags",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569471",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "\"The food and drug industry is facing the need to monitor the quality and safety of their products. This has made them turn to low-cost solutions that can enable smart sensing and tracking without adding much overhead. One such popular low-power solution is backscatter-based sensing and communication system. While it offers the promise of battery-less tags, it does so at the cost of a reduced communication range. In this work, we propose PACT - a scalable communication system that leverages the knowledge asymmetry in the network to improve the communication range of the tags. Borrowing from the backscatter principles, we design custom PACT Tags that are battery-less but use an active radio to extend the communication range beyond standard passive tags. They operate using the energy harvested from the PACT Source. A wide-band Reader is used to receive multiple Tag responses concurrently and upload them to a cloud server, enabling real-time monitoring and tracking at a longer range. We identify and address the challenges in the practical design of battery-less PACT Tags using an active radio and prototype them using off-the-shelf components. We show experimentally that our Tag consumes only 23μJ energy, which is harvested from an excitation Source that is up to 24 meters away from the Tag. We show that in outdoor deployments, the responses from an estimated 520 Tags can be received by a Reader concurrently while being 400 meters away from the Tags.\nhttps://doi.org/10.1145/3569471\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison"
            }
          ],
          "personId": 128874
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin, Madison"
            }
          ],
          "personId": 128900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin - Madison"
            }
          ],
          "personId": 128898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft"
            }
          ],
          "personId": 128896
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Wisconsin",
              "city": "Madison",
              "institution": "University of Wisconsin- Madison"
            }
          ],
          "personId": 128916
        }
      ]
    },
    {
      "id": 128290,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "Using Wearable Sensors to Measure Interpersonal Synchrony in Actors and Audience Members During a Live Theatre Performance",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580781",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "\"Studying social interaction in real-world settings is of increasing importance to social cognitive researchers. Theatre provides an ideal opportunity to study rich face-to-face interactions in a controlled, yet natural setting. Here we collaborated with Flute Theatre to investigate interpersonal synchrony between actors-actors, actors-audience and audience-audience within a live theatrical setting. Our 28 participants consisted of 6 actors and 22 audience members, with 5 of these audience members being audience participants in the show. The performance was a compilation of acting, popular science talks and demonstrations, and an audience participation period. Interpersonal synchrony was measured using inertial measurement unit (IMU) wearable accelerometers worn on the heads of participants, whilst audio-visual data recorded everything that occurred on the stage. Participants also completed post-show self-report questionnaires on their engagement with the overall scientists and actors performance. Cross Wavelet Transform (XWT) and Wavelet Coherence Transform (WCT) analysis were conducted to extract synchrony at different frequencies, pairing with audio-visual data. Findings revealed that XWT and WCT analysis are useful methods in extracting the multiple types of synchronous activity that occurs when people perform or watch a live performance together. We also found that audience members with higher ratings on questionnaire items such as the strength of their emotional response to the performance, or how empowered they felt by the performance, showed a high degree of interpersonal synchrony with actors during the acting segments of performance. We further found that audience members rated the scientists performance higher than the actors performance on questions related to their emotional response to the performance as well as, how uplifted, empowered, and connected to social issues they felt. This shows the types of potent connections audience members can have with live performances. Additionally, our findings highlight the importance of the performance context for audience engagement, in our case a theatre performance as part of public engagement with science rather than a stand-alone theatre performance. In sum we conclude that interdisciplinary real-world paradigms are an important and understudied route to understanding in-person social interactions.\nhttps://dl.acm.org/doi/10.1145/3580781\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "University College London"
            }
          ],
          "personId": 129442
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Goldsmiths University of London"
            }
          ],
          "personId": 129440
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Goldsmiths University of London"
            }
          ],
          "personId": 129437
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "University College London"
            }
          ],
          "personId": 129434
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "University College London"
            }
          ],
          "personId": 129431
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Goldsmiths University of London"
            }
          ],
          "personId": 129428
        }
      ]
    },
    {
      "id": 128291,
      "typeId": 12924,
      "title": "NeuralGait: Assessing Brain Health Using Your Smartphone",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569476",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"Brain health attracts more recent attention as the population ages. Smartphone-based gait sensing and analysis can help identify the risks of brain diseases in daily life for prevention. Existing gait analysis approaches mainly hand-craft temporal gait features or developing CNN-based feature extractors, but they are either prone to lose some inconspicuous pathological information or are only dedicated to a single brain disease screening. We discover that the relationship between gait segments can be used as a principle and generic indicator to quantify multiple pathological patterns. In this paper, we propose NeuralGait, a pervasive smartphone-cloud system that passively captures and analyzes principle gait segments relationship for brain health assessment. On the smartphone end, inertial gait data are collected while putting the smartphone in the pants pocket. We then craft local temporal-frequent gait domain features and develop a self-attention-based gait segment relationship encoder. Afterward, the domain features and relation features are fed to a scalable RiskNet in the cloud for brain health assessment. We also design a pathological hot update protocol to efficiently add new brain diseases in the RiskNet. NeuralGait is practical as it provides brain health assessment with no burden in daily life. In the experiment, we recruit 988 healthy people and 417 patients with a single or combination of PD, TBI, and stroke, and evaluate the brain health assessment using a set of specifically designed metrics including global accuracy, exact accuracy, sensitivity, and false alarm rate. We also demonstrate the generalization (e.g., analysis of feature effectiveness and model efficiency) and inclusiveness of NeuralGait.\nhttps://dl.acm.org/doi/10.1145/3569476\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "SUNY Buffalo"
            }
          ],
          "personId": 128932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "CASE WESTERN RESERVE"
            }
          ],
          "personId": 128930
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at buffalo"
            }
          ],
          "personId": 128928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Colorado Denver"
            }
          ],
          "personId": 128926
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "amherst",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128943
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "CASE WESTERN RESERVE"
            }
          ],
          "personId": 128924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester Medical Center"
            }
          ],
          "personId": 128923
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Kunshan",
              "institution": "Duke Kunshan University"
            }
          ],
          "personId": 128969
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at buffalo"
            }
          ],
          "personId": 128967
        }
      ]
    },
    {
      "id": 128292,
      "typeId": 12924,
      "title": "ThumbAir: In-Air Typing for Head Mounted Displays",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569474",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "Typing while wearing a standalone Head Mounted Display (HMD)---systems without external input devices or sensors to support text entry---is hard. To address this issue, prior work has used external trackers to monitor finger movements to support in-air typing on virtual keyboards. While performance has been promising, current systems are practically infeasible: finger movements may be visually occluded from inside-out HMD based tracking systems or, otherwise, awkward and uncomfortable to perform. To address these issues, this paper explores an alternative approach. Taking inspiration from the prevalence of thumb-typing on mobile phones, we describe four studies exploring, defining and validating the performance of ThumbAir, an in-air thumb-typing system implemented on a commercial HMD. The first study explores viable target locations, ultimately recommending eight targets sites. The second study collects performance data for taps on pairs of these targets to both inform the design of a target selection procedure and also support a computational design process to select a keyboard layout. The final two studies validate the selected keyboard layout in word repetition and phrase entry tasks, ultimately achieving final WPMs of 27.1 and 13.73. Qualitative data captured in the final study indicate that the discreet movements required to operate ThumbAir, in comparison to the larger scale finger and hand motions used in a baseline design from prior work, lead to reduced levels of perceived exertion and physical demand and are rated as acceptable for use in a wider range of social situations.\nhttps://dl.acm.org/doi/10.1145/3569474",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "UNIST"
            }
          ],
          "personId": 129059
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Ulsan",
              "institution": "UNIST"
            }
          ],
          "personId": 129055
        }
      ]
    },
    {
      "id": 128293,
      "typeId": 12924,
      "title": "Designing Reflective Derived Metrics for Fitness Trackers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569475",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "Personal tracking devices are equipped with more and more sensors and offer an ever-increasing level of accuracy. Yet, this comes at the cost of increased complexity. To deal with that problem, fitness trackers use derived metrics---scores calculated based on sensor data, e.g. a stress score. This means that part of the agency in interpreting health data is transferred from the user to the tracker. In this paper, we investigate the consequences of that transition and study how derived metrics can be designed to offer an optimal personal informatics experience. We conducted an online survey and a series of interviews which examined a health score (a hypothetical derived metric) at three levels of abstraction. We found that the medium abstraction level led to the highest level of reflection. Further, we determined that presenting the metric without contextual information led to decreased transparency and meaning. Our work contributes guidelines for designing effective derived metrics.\nhttps://dl.acm.org/doi/10.1145/3569475",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "city": "Utrecht",
              "institution": "Utrecht University"
            }
          ],
          "personId": 129027
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "St. Gallen",
              "institution": "University of St. Gallen"
            }
          ],
          "personId": 129069
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "city": "Gothenburg",
              "institution": "Chalmers University of Technology"
            }
          ],
          "personId": 129067
        }
      ]
    },
    {
      "id": 128294,
      "typeId": 12924,
      "title": "Combining Smart Speaker and Smart Meter to Infer Your Residential Power Usage by Self-supervised Cross-modal Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610905",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "\"Energy disaggregation is a key enabling technology for residential power usage monitoring, which benefits various applications such as carbon emission monitoring and human activity recognition. However, existing methods are difficult to balance the accuracy and usage burden (device costs, data labeling and prior knowledge). As the high penetration of smart speakers offers a low-cost way for sound-assisted residential power usage monitoring, this work aims to combine a smart speaker and a smart meter in a house to liberate the system from a high usage burden. However, it is still challenging to extract and leverage the consistent/complementary information (two types of relationships between acoustic and power features) from acoustic and power data without data labeling or prior knowledge. To this end, we design COMFORT, a cross-modality system for self-supervised power usage monitoring, including (i) a cross-modality learning component to automatically learn the consistent and complementary information, and (ii) a cross-modality inference component to utilize the consistent and complementary information. We implement and evaluate COMFORT with a self-collected dataset from six houses in 14 days, demonstrating that COMFORT finds the most appliances (98%), improves the appliance recognition performance in F-measure by at least 41.1%, and reduces the Mean Absolute Error (MAE) of energy disaggregation by at least 30.4% over other alternative solutions.\" https://doi.org/10.1145/3610905",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129043
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129041
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129039
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129037
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129035
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129033
        }
      ]
    },
    {
      "id": 128295,
      "typeId": 12924,
      "title": "Not Only for Contact Tracing: Use of Belgium’s Contact Tracing App among Young Adults",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570348",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Many countries developed and deployed contact tracing apps to reduce the spread of the COVID-19 coronavirus. Prior research explored people's intent to install these apps, which is necessary to ensure effectiveness. However, adopting contact tracing apps is not enough on its own, and much less is known about how people actually use these apps. Exploring app use can help us identify additional failures or risk points in the app life cycle. In this study, we conducted 13 semi-structured interviews with young adult users of Belgium's contact-tracing app, Coronalert. The interviews were conducted approximately a year after the onset of the COVID-19 pandemic. Our findings offer potential design directions for addressing issues identified in prior work - such as methods for maintaining long-term use and better integrating with the local health systems - and offer insight into existing design tensions such as the trade-off between maintaining users' privacy (by minimizing the personal data collected) and users' desire to have more information about an exposure incident. We distill from our results and the results of prior work a framework of people's decision points in contact-tracing app use that can serve to motivate careful design of future contact tracing technology.\nhttps://doi.org/10.1145/3570348\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Saarbrücken",
              "institution": "Max Planck Institute for Software Systems"
            }
          ],
          "personId": 128965
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Saarbrücken",
              "institution": "Max Planck Institute for Software Systems"
            }
          ],
          "personId": 128962
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "city": "Leuven",
              "institution": "University of Leuven"
            }
          ],
          "personId": 128959
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Saarbrücken",
              "institution": "Max Planck Institute for Software Systems"
            }
          ],
          "personId": 128956
        }
      ]
    },
    {
      "id": 128296,
      "typeId": 12924,
      "title": "I Know Your Intent: Graph-enhanced Intent-aware User Device Interaction Prediction via Contrastive Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610906",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "\"With the booming of smart home market, intelligent Internet of Things (IoT) devices have been increasingly involved in home life. To improve the user experience of smart homes, some prior works have explored how to use machine learning for predicting interactions between users and devices. However, the existing solutions have inferior User Device Interaction (UDI) prediction accuracy, as they ignore three key factors: routine, intent and multi-level periodicity of human behaviors. In this paper, we present SmartUDI, a novel accurate UDI prediction approach for smart homes. First, we propose a Message-Passing-based Routine Extraction (MPRE) algorithm to mine routine behaviors, then the contrastive loss is applied to narrow representations among behaviors from the same routines and alienate representations among behaviors from different routines. Second, we propose an Intent-aware Capsule Graph Attention Network (ICGAT) to encode multiple intents of users while considering complex transitions between different behaviors. Third, we design a Cluster-based Historical Attention Mechanism (CHAM) to capture the multi-level periodicity by aggregating the current sequence and the semantically nearest historical sequence representations through the attention mechanism. SmartUDI can be seamlessly deployed on cloud infrastructures of IoT device vendors and edge nodes, enabling the delivery of personalized device service recommendations to users. Comprehensive experiments on four real-world datasets show that SmartUDI consistently outperforms the state-of-the-art baselines with more accurate and highly interpretable results.\" https://doi.org/10.1145/3610906",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shen Zhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129119
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129117
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Peng Cheng Laboratory"
            }
          ],
          "personId": 129161
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Peng Cheng Laboratory"
            }
          ],
          "personId": 129159
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Ji Lin",
              "city": "Chang Chun",
              "institution": "Jilin University"
            }
          ],
          "personId": 129157
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Bei Jing",
              "institution": "Beijing Jiaotong University"
            }
          ],
          "personId": 129155
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129151
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shen Zhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129150
        }
      ]
    },
    {
      "id": 128297,
      "typeId": 12924,
      "title": "PhysiQ: Off-site Quality Assessment of Exercise in Physical Therapy",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570349",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "\"Physical therapy (PT) is crucial for patients to restore and maintain mobility, function, and well-being. Many on-site activities and body exercises are performed under the supervision of therapists or clinicians. However, the postures of some exercises at home cannot be performed accurately due to the lack of supervision, quality assessment, and self-correction. Therefore, in this paper, we design a new framework, PhysiQ, that continuously tracks and quantitatively measures people's off-site exercise activity through passive sensory detection. In the framework, we create a novel multi-task spatio-temporal Siamese Neural Network that measures the absolute quality through classification and relative quality based on an individual's PT progress through similarity comparison. PhysiQ digitizes and evaluates exercises in three different metrics: range of motions, stability, and repetition. We collect and annotate 31 participants' motion data with different levels of quality. Evaluation results show that PhysiQ recognizes the nuances in exercises, works with different numbers of repetitions, and achieves an accuracy of 89.67% in detecting levels of exercise quality and an average R-squared correlation of 0.949 in similarity comparison.\nhttps://doi.org/10.1145/3570349\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University"
            }
          ],
          "personId": 128997
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University"
            }
          ],
          "personId": 128994
        }
      ]
    },
    {
      "id": 128298,
      "typeId": 12924,
      "title": "VAX: Using Existing Video and Audio-based Activity Recognition Models to Bootstrap Privacy-Sensitive Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610907",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"The use of audio and video modalities for Human Activity Recognition (HAR) is common, given the richness of the data and the availability of pre-trained ML models using a large corpus of labeled training data. However, audio and video sensors also lead to significant consumer privacy concerns. Researchers have thus explored alternate modalities that are less privacy-invasive such as mmWave doppler radars, IMUs, motion sensors. However, the key limitation of these approaches is that most of them do not readily generalize across environments and require significant in-situ training data. Recent work has proposed cross-modality transfer learning approaches to alleviate the lack of trained labeled data with some success. In this paper, we generalize this concept to create a novel system called VAX (Video/Audio to 'X'), where training labels acquired from existing Video/Audio ML models are used to train ML models for a wide range of 'X' privacy-sensitive sensors. Notably, in VAX, once the ML models for the privacy-sensitive sensors are trained, with little to no user involvement, the Audio/Video sensors can be removed altogether to protect the user's privacy better. We built and deployed VAX in ten participants' homes while they performed 17 common activities of daily living. Our evaluation results show that after training, VAX can use its onboard camera and microphone to detect approximately 15 out of 17 activities with an average accuracy of 90%. For these activities that can be detected using a camera and a microphone, VAX trains a per-home model for the privacy-preserving sensors. These models (average accuracy = 84%) require no in-situ user input. In addition, when VAX is augmented with just one labeled instance for the activities not detected by the VAX A/V pipeline (~2 out of 17), it can detect all 17 activities with an average accuracy of 84%. Our results show that VAX is significantly better than a baseline supervised-learning approach of using one labeled instance per activity in each home (average accuracy of 79%) since VAX reduces the user burden of providing activity labels by 8x (~2 labels vs. 17 labels).\" https://doi.org/10.1145/3610907",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129462
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129460
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129458
        }
      ]
    },
    {
      "id": 128299,
      "typeId": 12924,
      "title": "TwinkleTwinkle: Interacting with Your Smart Devices by Eye Blink",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596238",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "Recent years have witnessed the rapid boom of mobile devices interweaving with changes the epidemic has made to people's lives. Though a tremendous amount of novel human-device interaction techniques have been put forward to facilitate various audiences and scenarios, limitations and inconveniences still occur to people having difficulty speaking or using their fingers/hands/arms or wearing masks/glasses/gloves. To fill the gap of such interaction contexts beyond using hands, voice, face, or mouth, in this work, we take the first step to propose a novel Human-Computer Interaction (HCI) system, TwinkleTwinkle, which senses and recognizes eye blink patterns in a contact-free and training-free manner leveraging ultrasound signals on commercial devices. TwinkleTwinkle first applies a phase difference based approach to depicting candidate eye blink motion profiles without removing any noises, followed by modeling intrinsic characteristics of blink motions through adaptive constraints to separate tiny patterns from interferences in conditions where blink habits and involuntary movements vary between individuals. We propose a vote-based approach to get final patterns designed to map with number combinations either self-defined or based on carriers like ASCII code and Morse code to make interaction seamlessly embedded with normal and well-known language systems. We implement TwinkleTwinkle on smartphones with all methods realized in the time domain and conduct extensive evaluations in various settings. Results show that TwinkleTwinkle achieves about 91% accuracy in recognizing 23 blink patterns among different people.\nhttps://dl.acm.org/doi/10.1145/3596238",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 128630
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 128628
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Qingdao",
              "institution": "Shandong University"
            }
          ],
          "personId": 128670
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 128668
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 128666
        }
      ]
    },
    {
      "id": 128300,
      "typeId": 12924,
      "title": "RetroSphere: Self-Contained Passive 3D Controller Tracking for Augmented Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569479",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "Advanced AR/VR headsets often have a dedicated depth sensor or multiple cameras, high processing power, and a high-capacity battery to track hands or controllers. However, these approaches are not compatible with the small form factor and limited thermal capacity of lightweight AR devices. In this paper, we present RetroSphere, a self-contained 6 degree of freedom (6DoF) controller tracker that can be integrated with almost any device. RetroSphere tracks a passive controller with just 3 retroreflective spheres using a stereo pair of mass-produced infrared blob trackers, each with its own infrared LED emitters. As the sphere is completely passive, no electronics or recharging is required. Each object tracking camera provides a tiny Arduino-compatible ESP32 microcontroller with the 2D position of the spheres. A lightweight stereo depth estimation algorithm that runs on the ESP32 performs 6DoF tracking of the passive controller. Also, RetroSphere provides an auto-calibration procedure to calibrate the stereo IR tracker setup. Our work builds upon Johnny Lee's Wii remote hacks and aims to enable a community of researchers, designers, and makers to use 3D input in their projects with affordable off-the-shelf components. RetroSphere achieves a tracking accuracy of about 96.5% with errors as low as ~3.5 cm over a 100 cm tracking range, validated with ground truth 3D data obtained using a LIDAR camera while consuming around 400 mW. We provide implementation details, evaluate the accuracy of our system, and demonstrate example applications, such as mobile AR drawing, 3D measurement, etc. with our Retrosphere-enabled AR glass prototype.\nhttps://dl.acm.org/doi/10.1145/3569479",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128850
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain view",
              "institution": "Google"
            }
          ],
          "personId": 128847
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University Of Maryland"
            }
          ],
          "personId": 128844
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google"
            }
          ],
          "personId": 128871
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Google Labs"
            }
          ],
          "personId": 128868
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Zurich",
              "institution": "Google"
            }
          ],
          "personId": 128866
        }
      ]
    },
    {
      "id": 128301,
      "typeId": 12924,
      "title": "HIPPO: Persuasive Hand-Grip Estimation from Everyday Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570344",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "Hand-grip strength is widely used to estimate muscle strength and it serves as a general indicator of the overall health of a person, particularly in aging adults. Hand-grip strength is typically estimated using dynamometers or specialized force resistant pressure sensors embedded onto objects. Both of these solutions require the user to interact with a dedicated measurement device which unnecessarily restricts the contexts where estimates are acquired. We contribute HIPPO, a novel non-intrusive and opportunistic method for estimating hand-grip strength from everyday interactions with objects. HIPPO re-purposes light sensors available in wearables (e.g., rings or gloves) to capture changes in light reflectivity when people interact with objects. This allows HIPPO to non-intrusively piggyback everyday interactions for health information without affecting the user's everyday routines. We present two prototypes integrating HIPPO, an early smart glove proof-of-concept, and a further optimized solution that uses sensors integrated onto a ring. We validate HIPPO through extensive experiments and compare HIPPO against three baselines, including a clinical dynamometer. Our results show that HIPPO operates robustly across a wide range of everyday objects, and participants. The force strength estimates correlate with estimates produced by pressure-based devices, and can also determine the correct hand grip strength category with up to 86% accuracy. Our findings also suggest that users prefer our approach to existing solutions as HIPPO blends the estimation with everyday interactions.\nhttps://dl.acm.org/doi/10.1145/3570344",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "University of Tartu"
            }
          ],
          "personId": 129012
        },
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "Univerisity of Tartu"
            }
          ],
          "personId": 129010
        },
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "University of Tartu"
            }
          ],
          "personId": 129008
        },
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "University of Tartu"
            }
          ],
          "personId": 129025
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "University of Helsinki"
            }
          ],
          "personId": 129024
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "University of Helsinki"
            }
          ],
          "personId": 129004
        },
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "University of Tartu"
            }
          ],
          "personId": 129045
        }
      ]
    },
    {
      "id": 128302,
      "typeId": 12924,
      "title": "“It’s Not an Issue of Malice, but of Ignorance”: Towards Inclusive Video Conferencing for Presenters Who are d/Deaf or Hard of Hearing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610901",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "\"As video conferencing (VC) has become necessary for many professional, educational, and social tasks, people who are d/Deaf and hard of hearing (DHH) face distinct accessibility barriers. We conducted studies to understand the challenges faced by DHH people during VCs and found that they struggled to easily present or communicate effectively due to accessibility limitations of VC platforms. These limitations include the lack of tools for DHH speakers to discreetly communicate their accommodation needs to the group. Based on these findings, we prototyped a suite of tools, called Erato that enables DHH speakers to be aware of their performance while speaking and remind participants of proper etiquette. We evaluated Erato by running a mock classroom case study over VC for three sessions. All participants felt more confident in their speaking ability and paid closer attention to making the classroom more inclusive while using our tool. We share implications of these results for the design of VC interfaces and human-the-the-loop assistive systems that can support users who are DHH to communicate effectively and advocate for their accessibility needs.\" https://doi.org/10.1145/3610901",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 129519
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta"
            }
          ],
          "personId": 129516
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University"
            }
          ],
          "personId": 129513
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Burnaby",
              "institution": "Simon Fraser University"
            }
          ],
          "personId": 129510
        }
      ]
    },
    {
      "id": 128303,
      "typeId": 12924,
      "title": "Naturalistic E-Scooter Maneuver Recognition with Federated Contrastive Rider Interaction Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570345",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "\"Smart micromobility, particularly the electric (e)-scooters, has emerged as an important ubiquitous mobility option that has proliferated within and across many cities in North America and Europe. Due to the fast speed (say, ~15km/h) and ease of maneuvering, understanding how the micromobility rider interacts with the scooter becomes essential for the e-scooter manufacturers, e-scooter sharing operators, and rider communities in promoting riding safety and relevant policy or regulations. In this paper, we propose FCRIL, a novel Federated maneuver identification and Contrastive e-scooter Rider Interaction Learning system. FCRIL aims at: (i) understanding, learning, and identifying the e-scooter rider interaction behaviors during naturalistic riding (NR) experience (without constraints on the data collection settings); and (ii) providing a novel federated maneuver learning model training and contrastive identification design for our proposed rider interaction learning (RIL). Towards the prototype and case studies of FCRIL, we have harvested an NR behavior dataset based on the inertial measurement units (IMUs), e.g., accelerometer and gyroscope, from the ubiquitous smartphones/embedded IoT devices attached to the e-scooters. Based on the harvested IMU sensor data, we have conducted extensive data analytics to derive the relevant rider maneuver patterns, including time series, spectrogram, and other statistical features, for the RIL model designs. We have designed a contrastive RIL network which takes in these maneuver features with class-to-class differentiation for comprehensive RIL and enhanced identification accuracy. Furthermore, to enhance the dynamic model training efficiency and coping with the emerging micromobility rider data privacy concerns, we have designed a novel asynchronous federated maneuver learning module, which asynchronously takes in multiple sets of model gradients (e.g., based on the IMU data from the riders' smartphones) for dynamic RIL model training and communication overhead reduction. We have conducted extensive experimental studies with different smartphone models and stand-alone IMU sensors on the e-scooters. Our experimental results have demonstrated the accuracy and effectiveness of FCRIL in learning and recognizing the e-scooter rider maneuvers.\nhttps://dl.acm.org/doi/10.1145/3570345\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128940
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "The University of Connecticut"
            }
          ],
          "personId": 128968
        }
      ]
    },
    {
      "id": 128304,
      "typeId": 12924,
      "title": "Exploring the Opportunities of AR for Enriching Storytelling with Family Photos between Grandparents and Grandchildren",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610903",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "\"Storytelling with family photos, as an important mode of reminiscence-based activities, can be instrumental in promoting intergenerational communication between grandparents and grandchildren by strengthening generation bonds and shared family values. Motivated by challenges that existing technology approaches encountered for improving intergenerational storytelling (e.g., the need to hold the tablet, the potential view detachment from the physical world in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality (AR) to support intergenerational storytelling, which offers new capabilities (e.g., 3D models, new interactivity) to enhance the expression for the storyteller. We conducted a two-part exploratory study, where pairs of grandparents and grandchildren 1) participated in an in-person storytelling activity with a semi-structured interview 2) and then a participatory design session with AR technology probes that we designed to inspire their exploration. Our findings revealed insights into the possible ways of intergenerational storytelling, the feasibility and usages of AR in facilitating it, and the key design implications for leveraging AR in intergenerational storytelling.\" https://doi.org/10.1145/3610903",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong SAR",
              "institution": "The Hong Kong University of Science and Technology"
            }
          ],
          "personId": 128865
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)"
            }
          ],
          "personId": 129556
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128863
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangdong",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)"
            }
          ],
          "personId": 128862
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "The Hong Kong University of Science and Technology (Guangzhou)"
            }
          ],
          "personId": 128861
        }
      ]
    },
    {
      "id": 128305,
      "typeId": 12924,
      "title": "Sounds of Health: Using Personalized Sonification Models to Communicate Health Information",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570346",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "This paper explores the feasibility of using sonification in delivering and communicating health and wellness status on personal devices. Ambient displays have proven to inform users of their health and wellness and help them to make healthier decisions, yet, little technology provides health assessments through sounds, which can be even more pervasive than visual displays. We developed a method to generate music from user preferences and evaluated it in a two-step user study. In the first step, we acquired general healthiness impressions from each user. In the second step, we generated customized melodies from music preferences in the first step to capture participants' perceived healthiness of those melodies. We deployed our surveys for 55 participants to complete on their own over 31 days. We analyzed the data to understand commonalities and differences in users' perceptions of music as an expression of health. Our findings show the existence of clear associations between perceived healthiness and different music features. We provide useful insights into how different musical features impact the perceived healthiness of music, how perceptions of healthiness vary between users, what trends exist between users' impressions, and what influences (or does not influence) a user's perception of healthiness in a melody. Overall, our results indicate validity in presenting health data through personalized music models. The findings can inform the design of behavior management applications on personal and ubiquitous devices.\nhttps://dl.acm.org/doi/10.1145/3570346",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 129417
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 129444
        }
      ]
    },
    {
      "id": 128306,
      "typeId": 12924,
      "title": "Cross-Modality Graph-based Language and Sensor Data Co-Learning of Human-Mobility Interaction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610904",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"Learning the human--mobility interaction (HMI) on interactive scenes (e.g., how a vehicle turns at an intersection in response to traffic lights and other oncoming vehicles) can enhance the safety, efficiency, and resilience of smart mobility systems (e.g., autonomous vehicles) and many other ubiquitous computing applications. Towards the ubiquitous and understandable HMI learning, this paper considers both \"spoken language\" (e.g., human textual annotations) and \"unspoken language\" (e.g., visual and sensor-based behavioral mobility information related to the HMI scenes) in terms of information modalities from the real-world HMI scenarios. We aim to extract the important but possibly implicit HMI concepts (as the named entities) from the textual annotations (provided by human annotators) through a novel human language and sensor data co-learning design.\n\nTo this end, we propose CG-HMI, a novel Cross-modality Graph fusion approach for extracting important Human-Mobility Interaction concepts from co-learning of textual annotations as well as the visual and behavioral sensor data. In order to fuse both unspoken and spoken \"languages\", we have designed a unified representation called the human--mobility interaction graph (HMIG) for each modality related to the HMI scenes, i.e., textual annotations, visual video frames, and behavioral sensor time-series (e.g., from the on-board or smartphone inertial measurement units). The nodes of the HMIG in these modalities correspond to the textual words (tokenized for ease of processing) related to HMI concepts, the detected traffic participant/environment categories, and the vehicle maneuver behavior types determined from the behavioral sensor time-series. To extract the inter- and intra-modality semantic correspondences and interactions in the HMIG, we have designed a novel graph interaction fusion approach with differentiable pooling-based graph attention. The resulting graph embeddings are then processed to identify and retrieve the HMI concepts within the annotations, which can benefit the downstream human-computer interaction and ubiquitous computing applications. We have developed and implemented CG-HMI into a system prototype, and performed extensive studies upon three real-world HMI datasets (two on car driving and the third one on e-scooter riding). We have corroborated the excellent performance (on average 13.11% higher accuracy than the other baselines in terms of precision, recall, and F1 measure) and effectiveness of CG-HMI in recognizing and extracting the important HMI concepts through cross-modality learning. Our CG-HMI studies also provide real-world implications (e.g., road safety and driving behaviors) about the interactions between the drivers and other traffic participants.\" https://doi.org/10.1145/3610904",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 129082
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "The University of Connecticut"
            }
          ],
          "personId": 129080
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "The University of Michigan Ann Arbor"
            }
          ],
          "personId": 129078
        }
      ]
    },
    {
      "id": 128307,
      "typeId": 12924,
      "title": "Genie in the Model: Automatic Generation of Human-in-the-Loop Deep Neural Networks for Mobile Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580815",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "\"Advances in deep neural networks (DNNs) have fostered a wide spectrum of intelligent mobile applications ranging from voice assistants on smartphones to augmented reality with smart-glasses. To deliver high-quality services, these DNNs should operate on resource-constrained mobile platforms and yield consistent performance in open environments. However, DNNs are notoriously resource-intensive, and often suffer from performance degradation in real-world deployments. Existing research strives to optimize the resource-performance trade-off of DNNs by compressing the model without notably compromising its inference accuracy. Accordingly, the accuracy of these compressed DNNs is bounded by the original ones, leading to more severe accuracy drop in challenging yet common scenarios such as low-resolution, small-size, and motion-blur. In this paper, we propose to push forward the frontiers of the DNN performance-resource trade-off by introducing human intelligence as a new design dimension. To this end, we explore human-in-the-loop DNNs (H-DNNs) and their automatic performance-resource optimization. We present H-Gen, an automatic H-DNN compression framework that incorporates human participation as a new hyperparameter for accurate and efficient DNN generation. It involves novel hyperparameter formulation, metric calculation, and search strategy in the context of automatic H-DNN generation. We also propose human participation mechanisms for three common DNN architectures to showcase the feasibility of H-Gen. Extensive experiments on twelve categories of challenging samples with three common DNN structures demonstrate the superiority of H-Gen in terms of the overall trade-off between performance (accuracy, latency), and resource (storage, energy, human labour).\nhttps://dl.acm.org/doi/10.1145/3580815\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128955
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128952
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "School of Computer Science"
            }
          ],
          "personId": 128949
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong kong",
              "institution": "City University of Hong Kong"
            }
          ],
          "personId": 128975
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "xian",
              "institution": "northwestern polytechnical univ."
            }
          ],
          "personId": 128973
        }
      ]
    },
    {
      "id": 128308,
      "typeId": 12924,
      "title": "StructureSense: Inferring Constructive Assembly Structures from User Behaviors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3570343",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "Recent advancements in object-tracking technologies can turn mundane constructive assemblies into Tangible User Interfaces (TUI) media. Users rely on instructions or their own creativity to build both permanent and temporary structures out of such objects. However, most existing object-tracking technologies focus on tracking structures as monoliths, making it impossible to infer and track the user's assembly process and the resulting structures. Technologies that can track the assembly process often rely on specially fabricated assemblies, limiting the types of objects and structures they can track. Here, we present StructureSense, a tracking system based on passive UHF-RFID sensing that infers constructive assembly structures from object motion. We illustrated StructureSense in two use cases (as guided instructions and authoring tool) on two different constructive sets (wooden lamp and Jumbo Blocks), and evaluated system performance and usability. Our results showed the feasibility of using StructureSense to track mundane constructive assembly structures.\nhttps://dl.acm.org/doi/10.1145/3570343",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 128710
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 128709
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "The University of Michigan"
            }
          ],
          "personId": 128721
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 128720
        }
      ]
    },
    {
      "id": 128309,
      "typeId": 12924,
      "title": "Headar: Sensing Head Gestures for Confirmation Dialogs on Smartwatches with Wearable Millimeter-Wave Radar",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610900",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"Mobile sensing is a ubiquitous and useful tool to make inferences about individuals' mental health based on physiology and behavior patterns. Along with sensing features directly associated with mental health, it can be valuable to detect different features of social contexts to learn about social interaction patterns over time and across different environments. This can provide insight into diverse communities' academic, work and social lives, and their social networks. We posit that passively detecting social contexts can be particularly useful for social anxiety research, as it may ultimately help identify changes in social anxiety status and patterns of social avoidance and withdrawal. To this end, we recruited a sample of highly socially anxious undergraduate students (N=46) to examine whether we could detect the presence of experimentally manipulated virtual social contexts via wristband sensors. Using a multitask machine learning pipeline, we leveraged passively sensed biobehavioral streams to detect contexts relevant to social anxiety, including (1) whether people were in a social situation, (2) size of the social group, (3) degree of social evaluation, and (4) phase of social situation (anticipating, actively experiencing, or had just participated in an experience). Results demonstrated the feasibility of detecting most virtual social contexts, with stronger predictive accuracy when detecting whether individuals were in a social situation or not and the phase of the situation, and weaker predictive accuracy when detecting the level of social evaluation. They also indicated that sensing streams are differentially important to prediction based on the context being predicted. Our findings also provide useful information regarding design elements relevant to passive context detection, including optimal sensing duration, the utility of different sensing modalities, and the need for personalization. We discuss implications of these findings for future work on context detection (e.g., just-in-time adaptive intervention development).\" https://doi.org/10.1145/3610900",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab"
            }
          ],
          "personId": 128933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128931
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google"
            }
          ],
          "personId": 128929
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128927
        }
      ]
    },
    {
      "id": 128310,
      "typeId": 12924,
      "title": "Exergy: A Toolkit to Simplify Creative Applications of Wind Energy Harvesting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580814",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "Energy harvesting reduces the burden of power source maintenance and promises to make computing systems genuinely ubiquitous. Researchers have made inroads in this area, but their novel energy harvesting materials and fabrication techniques remain inaccessible to the general maker communities. Therefore, this paper aims to provide a toolkit that makes energy harvesting accessible to novices. In Study 1, we investigate the challenges and opportunities associated with devising energy harvesting technology with experienced researchers and makers (N=9). Using the lessons learned from this investigation, we design a wind energy harvesting toolkit, Exergy, in Study 2. It consists of a simulator, hardware tools, a software example, and ideation cards. We apply it to vehicle environments, which have yet to be explored despite their potential. In Study 3, we conduct a two-phase workshop: hands-on experience and ideation sessions. The results show that novices (N=23) could use Exergy confidently and invent self-sustainable energy harvesting applications creatively.\nhttps://dl.acm.org/doi/10.1145/3580814",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129054
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129053
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Interactive Computing"
            }
          ],
          "personId": 129051
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129049
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129092
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129090
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University"
            }
          ],
          "personId": 129088
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129086
        }
      ]
    },
    {
      "id": 128311,
      "typeId": 12924,
      "title": "On the Long-Term Effects of Continuous Keystroke Authentication: Keeping User Frustration Low through Behavior Adaptation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596236",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "\"One of the main challenges in deploying a keystroke dynamics-based continuous authentication scheme on smartphones is ensuring low error rates over time. Unstable false rejection rates (FRRs) would lead to frequent phone locks during long-term use, and deteriorating attack detection rates would jeopardize its security benefits. The fact that it is undesirable to train complex deep learning models directly on smartphones or send private sensor data to servers for training present unique deployment constraints, requiring on-device solutions that can be trained fully on smartphones.\nTo improve authentication accuracy while satisfying such real-world deployment constraints, we propose two novel feature engineering techniques: (1) computation of pair-wise correlations between accelerometer and gyroscope sensor values, and (2) on-device feature extraction technique to compute dynamic time warping (DTW) distance measurements between autoencoder inputs and outputs via transfer-learning. Using those two feature sets in an ensemble blender, we achieved 6.4 percent equal error rate (EER) in a public dataset. In comparison, blending two state-of-the-art solutions achieved 14.1 percent EER in the same test settings. Our real-world dataset evaluation showed increasing FRRs (user frustration) over two months; however, through periodic model retraining, we were able to maintain average FRRs around 2.5 percent while keeping attack detection rates around 89 percent. The proposed solution has been deployed in the latest Samsung Galaxy smartphone series to protect secure workspace through continuous authentication.\nhttps://dl.acm.org/doi/10.1145/3596236\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128695
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128694
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128693
        },
        {
          "affiliations": [
            {
              "country": "Ukraine",
              "city": "Kiev",
              "institution": "Samsung R&D Institute Ukraine"
            }
          ],
          "personId": 128692
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Redwood City",
              "institution": "Moloco inc."
            }
          ],
          "personId": 128691
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Sungkyunkwan University"
            }
          ],
          "personId": 128690
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128689
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Sungkyunkwan University"
            }
          ],
          "personId": 128688
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Samsung Research"
            }
          ],
          "personId": 128687
        }
      ]
    },
    {
      "id": 128312,
      "typeId": 12924,
      "title": "HyWay: Enabling Mingling in the Hybrid World",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596235",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "\"We present HyWay, short for \"\"Hybrid Hallway\"\", to enable mingling and informal interactions among physical and virtual users, in casual spaces and settings, such as office water cooler areas, conference hallways, trade show floors, and more. We call out how the hybrid and unstructured (or semi-structured) nature of such settings set these apart from the all-virtual and/or structured settings considered in prior work. Key to the design of HyWay is bridging the awareness gap between physical and virtual users, and providing the virtual users the same agency as physical users.\nTo this end, we have designed HyWay to incorporate reciprocity (users can see and hear others only if they can be seen and heard), porosity (conversations in physical space are porous and not within airtight compartments), and agency (the ability for users to seamlessly move between conversations). We present our implementation of HyWay and the user survey findings from multiple deployments in unstructured settings (e.g., social gatherings), and semi-structured ones (e.g., a poster event). Results from these deployments show that HyWay enables effective mingling between physical and virtual users.\nhttps://doi.org/10.1145/3596235\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129079
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129077
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129076
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia"
            }
          ],
          "personId": 129093
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129072
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bengaluru",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129070
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129114
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129112
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129110
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 129108
        },
        {
          "affiliations": [
            {
              "country": "India",
              "city": "Bangalore",
              "institution": "Microsoft Reserach"
            }
          ],
          "personId": 129106
        }
      ]
    },
    {
      "id": 128313,
      "typeId": 12924,
      "title": "ConvBoost: Boosting ConvNets for Sensor-based Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596234",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "\"Human activity recognition (HAR) is one of the core research themes in ubiquitous and wearable computing. With the shift to deep learning (DL) based analysis approaches, it has become possible to extract high-level features and perform classification in an end-to-end manner. Despite their promising overall capabilities, DL-based HAR may suffer from overfitting due to the notoriously small, often inadequate, amounts of labeled sample data that are available for typical HAR applications. In response to such challenges, we propose ConvBoost -- a novel, three-layer, structured model architecture and boosting framework for convolutional network based HAR. Our framework generates additional training data from three different perspectives for improved HAR, aiming to alleviate the shortness of labeled training data in the field. Specifically, with the introduction of three conceptual layers--Sampling Layer, Data Augmentation Layer, and Resilient Layer--we develop three \"\"boosters\"\"--R-Frame, Mix-up, and C-Drop--to enrich the per-epoch training data by dense-sampling, synthesizing, and simulating, respectively. These new conceptual layers and boosters, that are universally applicable for any kind of convolutional network, have been designed based on the characteristics of the sensor data and the concept of frame-wise HAR. In our experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, GOTOV) we demonstrate the effectiveness of our ConvBoost framework for HAR applications based on variants of convolutional networks: vanilla CNN, ConvLSTM, and Attention Models. We achieved substantial performance gains for all of them, which suggests that the proposed approach is generic and can serve as a practical solution for boosting the performance of existing ConvNet-based HAR models. This is an open-source project, and the code can be found at https://github.com/sshao2013/ConvBoost\nhttps://dl.acm.org/doi/10.1145/3596234\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Coventry",
              "institution": "University of Warwick"
            }
          ],
          "personId": 129232
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Coventry",
              "institution": "University of Warwick"
            }
          ],
          "personId": 129263
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Tyne and Wear",
              "city": "Newcastle Upon Tyne",
              "institution": "Northumbria University"
            }
          ],
          "personId": 129518
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Tyne and Wear",
              "city": "Newcastle",
              "institution": "Newcastle University"
            }
          ],
          "personId": 129515
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129512
        }
      ]
    },
    {
      "id": 128314,
      "typeId": 12924,
      "title": "AMIR: Active Multimodal Interaction Recognition from Video and Network Traffic in Connected Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580818",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "Activity recognition using video data is widely adopted for elder care, monitoring for safety and security, and home automation. Unfortunately, using video data as the basis for activity recognition can be brittle, since models trained on video are often not robust to certain environmental changes, such as camera angle and lighting changes. There has been a proliferation of network-connected devices in home environments. Interactions with these smart devices are associated with network activity, making network data a potential source for recognizing these device interactions. This paper advocates for the synthesis of video and network data for robust interaction recognition in connected environments. We consider machine learning-based approaches for activity recognition, where each labeled activity is associated with both a video capture and an accompanying network traffic trace. We develop a simple but effective framework AMIR (Active Multimodal Interaction Recognition)1 that trains independent models for video and network activity recognition respectively, and subsequently combines the predictions from these models using a meta-learning framework. Whether in lab or at home, this approach reduces the amount of \"paired\" demonstrations needed to perform accurate activity recognition, where both network and video data are collected simultaneously. Specifically, the method we have developed requires up to 70.83% fewer samples to achieve 85% F1 score than random data collection, and improves accuracy by 17.76% given the same number of samples.\nhttps://dl.acm.org/doi/10.1145/3580818",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129425
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129451
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129449
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129423
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "Ohio State University"
            }
          ],
          "personId": 129468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129467
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129465
        }
      ]
    },
    {
      "id": 128315,
      "typeId": 12924,
      "title": "AI-to-Human Actuation: Boosting Unmodified AI’s Robustness by Proactively Inducing Favorable Human Sensing Conditions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580812",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "abstract": "\"Imagine a near-future smart home. Home-embedded visual AI sensors continuously monitor the resident, inferring her activities and internal states that enable higher-level services. Here, as home-embedded sensors passively monitor a free person, good inferences happen randomly. The inferences' confidence highly depends on how congruent her momentary conditions are to the conditions favored by the AI models, e.g., front-facing or unobstructed. We envision new strategies of AI-to-Human Actuation (AHA) that empower the sensory AIs with proactive actuation so that they induce the person's conditions to be more favorable to the AIs. In this light, we explore the initial feasibility and efficacy of AHA in the context of home-embedded visual AIs. We build a taxonomy of actuations that could be issued to home residents to benefit visual AIs. We deploy AHA in an actual home rich in sensors and interactive devices. With 20 participants, we comprehensively study their experiences with proactive actuation blended with their usual home routines. We also demonstrate the substantially improved inferences of the actuation-empowered AIs over the passive sensing baseline. This paper sets forth an initial step towards interweaving human-targeted AIs and proactive actuation to yield more chances for high-confidence inferences without sophisticating the model, in order to improve robustness against unfavorable conditions.\nhttps://dl.acm.org/doi/10.1145/3580812\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 128947
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128946
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 128945
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 128999
        }
      ]
    },
    {
      "id": 128316,
      "typeId": 12924,
      "title": "Efficient Adaptive Beacon Deployment Optimization for Indoor Crowd Monitoring Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569462",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "abstract": "\"The indoor crowd density monitoring system using BLE beacons is one of the effective ways to prevent overcrowded indoor situations. The indoor crowd density monitoring system consists of a mobile application at the user's side and the beacon sensor network as the infrastructure. Since the performance of crowd density monitoring highly depends on how BLE beacons are placed, BLE beacon placement optimization is fundamental research work. This research proposes a beacon deployment method EABeD to incrementally place the beacons adaptively to the latest signal propagation status. Also, EABeD reduces most walking and measurement labor costs by applying Bayesian optimization and the walking distance optimization algorithm. We conducted the placement optimization experiment in the wild environment and compared the results with placements derived by the simulation-based method and people. The result shows that our proposed method can achieve 26.4% higher detection coverage than the simulation-based approach, 23.2% and 5.2% higher detection coverage than the inexperienced person's solution and the expert's solution. As for the labor cost reduction, our proposed method can reduce 90.2% of the walking distance and 74.4% of the optimization time compared with optimization by the dense data gathering method.\nhttps://dl.acm.org/doi/10.1145/3569462\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 128992
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 128989
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 128986
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "Yahoo Japan Corporation"
            }
          ],
          "personId": 128983
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 128980
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 128978
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 129002
        }
      ]
    },
    {
      "id": 128317,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "SmokeMon: Unobtrusive Extraction of Smoking Topography Using Wearable Energy-Efficient Thermal",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569460",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "Smoking is the leading cause of preventable death worldwide. Cigarette smoke includes thousands of chemicals that are harmful and cause tobacco-related diseases. To date, the causality between human exposure to specific compounds and the harmful effects is unknown. A first step in closing the gap in knowledge has been measuring smoking topography, or how the smoker smokes the cigarette (puffs, puff volume, and duration). However, current gold-standard approaches to smoking topography involve expensive, bulky, and obtrusive sensor devices, creating unnatural smoking behavior and preventing their potential for real-time interventions in the wild. Although motion-based wearable sensors and their corresponding machine-learned models have shown promise in unobtrusively tracking smoking gestures, they are notorious for confounding smoking with other similar hand-to-mouth gestures such as eating and drinking. In this paper, we present SmokeMon, a chest-worn thermal-sensing wearable system that can capture spatial, temporal, and thermal information around the wearer and cigarette all day to unobtrusively and passively detect smoking events. We also developed a deep learning--based framework to extract puffs and smoking topography. We evaluate SmokeMon in both controlled and free-living experiments with a total of 19 participants, more than 110 hours of data, and 115 smoking sessions achieving an F1-score of 0.9 for puff detection in the laboratory and 0.8 in the wild. By providing SmokeMon as an open platform, we provide measurement of smoking topography in free-living settings to enable testing of smoking topography in the real world, with potential to facilitate timely smoking cessation interventions.\nhttps://dl.acm.org/doi/10.1145/3569460",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128708
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128707
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128706
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128705
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Goa",
              "city": "Zuarinagar",
              "institution": "BITS Pilani, Goa"
            }
          ],
          "personId": 128704
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128703
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128702
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 128701
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128700
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128699
        }
      ]
    },
    {
      "id": 128318,
      "typeId": 12924,
      "title": "Don’t “Weight” to Board: Augmenting Vision-based Passenger Weight Prediction via Viscoelastic Mat",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569465",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "\"Airlines overestimate the weight of their passengers by simply assigning a constant weight for everyone, causing each plane to burn more fuel than needed to carry the extra weight. Accurately estimating the passenger weights is a difficult problem for airlines as naively weighing all passengers with scales is impractical in already busy airports. Hence, we propose CamScale, a novel vision-based weight inference system that is augmented by an off-the-shelf viscoelastic mat (e.g., memory foam mat). CamScale takes the video feed of the mat placed on the floor as the passengers walk over it. It utilizes the inherent strain, or deformation of the mat due to the passengers' footsteps to infer their weights. CamScale is advantageous because it does not incur additional weighing time, while being cost-effective and accurate. We evaluate CamScale through real-world experiments by deploying RGB and infrared cameras and inviting 36 participants to walk a total of more than 17,000 steps over viscoelastic mats, equivalent to walking approximately 13.1 km. We demonstrate that CamScale is able to accurately estimate an individual's weight with an average error of 1.12 kg.\nhttps://doi.org/10.1145/3569465\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daegu",
              "institution": "Kyungpook National University"
            }
          ],
          "personId": 128972
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "Defence Science and Technology Agency"
            }
          ],
          "personId": 128971
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Merced",
              "institution": "University of California Merced"
            }
          ],
          "personId": 128970
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Yonsei University"
            }
          ],
          "personId": 128995
        }
      ]
    },
    {
      "id": 128319,
      "typeId": 12924,
      "title": "Technical Design Space Analysis for Unobtrusive Driver Emotion Assessment Using Multi-Domain Context",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569466",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "Driver emotions play a vital role in driving safety and performance. Consequently, regulating driver emotions through empathic interfaces have been investigated thoroughly. However, the prerequisite - driver emotion sensing - is a challenging endeavor: Body-worn physiological sensors are intrusive, while facial and speech recognition only capture overt emotions. In a user study (N=27), we investigate how emotions can be unobtrusively predicted by analyzing a rich set of contextual features captured by a smartphone, including road and traffic conditions, visual scene analysis, audio, weather information, and car speed. We derive a technical design space to inform practitioners and researchers about the most indicative sensing modalities, the corresponding impact on users' privacy, and the computational cost associated with processing this data. Our analysis shows that contextual emotion recognition is significantly more robust than facial recognition, leading to an overall improvement of 7% using a leave-one-participant-out cross-validation.\nhttps://dl.acm.org/doi/10.1145/3569466",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Stuttgart",
              "institution": "Porsche AG"
            }
          ],
          "personId": 129001
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Stuttgart",
              "institution": "Porsche AG"
            }
          ],
          "personId": 129000
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Berlin",
              "institution": "HU Berlin"
            }
          ],
          "personId": 129022
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Stuttgart",
              "institution": "Porsche AG"
            }
          ],
          "personId": 129020
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Berlin",
              "institution": "CODE University of Applied Sciences"
            }
          ],
          "personId": 129018
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 129016
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Stuttgart",
              "institution": "Porsche AG"
            }
          ],
          "personId": 129014
        }
      ]
    },
    {
      "id": 128320,
      "typeId": 12924,
      "title": "DRG-Keyboard: Enabling Subtle Gesture Typing on the Fingertip with Dual IMU Rings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569463",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"We present DRG-Keyboard, a gesture keyboard enabled by dual IMU rings, allowing the user to swipe the thumb on the index fingertip to perform word gesture typing as if typing on a miniature QWERTY keyboard. With dual IMUs attached to the user's thumb and index finger, DRG-Keyboard can 1) measure the relative attitude while mapping it to the 2D fingertip coordinates and 2) detect the thumb's touch-down and touch-up events combining the relative attitude data and the synchronous frequency domain data, based on which a fingertip gesture keyboard can be implemented. To understand users typing behavior on the index fingertip with DRG-Keyboard, we collected and analyzed user data in two typing manners. Based on the statistics of the gesture data, we enhanced the elastic matching algorithm with rigid pruning and distance measurement transform. The user study showed DRG-Keyboard achieved an input speed of 12.9 WPM (68.3% of their gesture typing speed on the smartphone) for all participants. The appending study also demonstrated the superiority of DRG-Keyboard for better form factors and wider usage scenarios. To sum up, DRG-Keyboard not only achieves good text entry speed merely on a tiny fingertip input surface, but is also well accepted by the participants for the input subtleness, accuracy, good haptic feedback, and availability.\nhttps://doi.org/10.1145/3569463\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128718
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128717
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128716
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128715
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128714
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128713
        }
      ]
    },
    {
      "id": 128321,
      "typeId": 12924,
      "title": "IoTBeholder: A Privacy Snooping Attack on User Habitual Behaviors from Smart Home Wi-Fi Traffic",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580890",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"With the deployment of a growing number of smart home IoT devices, privacy leakage has become a growing concern. Prior work on privacy-invasive device localization, classification, and activity identification have proven the existence of various privacy leakage risks in smart home environments. However, they only demonstrate limited threats in real world due to many impractical assumptions, such as having privileged access to the user's home network. In this paper, we identify a new end-to-end attack surface using IoTBeholder, a system that performs device localization, classification, and user activity identification. IoTBeholder can be easily run and replicated on commercial off-the-shelf (COTS) devices such as mobile phones or personal computers, enabling attackers to infer user's habitual behaviors from smart home Wi-Fi traffic alone. We set up a testbed with 23 IoT devices for evaluation in the real world. The result shows that IoTBeholder has good device classification and device activity identification performance. In addition, IoTBeholder can infer the users' habitual behaviors and automation rules with high accuracy and interpretability. It can even accurately predict the users' future actions, highlighting a significant threat to user privacy that IoT vendors and users should highly concern.\nhttps://doi.org/10.1145/3580890\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129148
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Peng Cheng Laboratory"
            }
          ],
          "personId": 129146
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129144
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129141
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "Hong Kong University of Science and Technology (GZ)"
            }
          ],
          "personId": 129138
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shen Zhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129184
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Tsinghua Shenzhen International Graduate School"
            }
          ],
          "personId": 129181
        }
      ]
    },
    {
      "id": 128322,
      "typeId": 12924,
      "title": "AdaEnlight: Energy-aware Low-light Video Stream Enhancement on Mobile Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569464",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"The ubiquity of camera-embedded devices and the advances in deep learning have stimulated various intelligent mobile video applications. These applications often demand on-device processing of video streams to deliver real-time, high-quality services for privacy and robustness concerns. However, the performance of these applications is constrained by the raw video streams, which tend to be taken with small-aperture cameras of ubiquitous mobile platforms in dim light. Despite extensive low-light video enhancement solutions, they are unfit for deployment to mobile devices due to their complex models and and ignorance of system dynamics like energy budgets. In this paper, we propose AdaEnlight, an energy-aware low-light video stream enhancement system on mobile devices. It achieves real-time video enhancement with competitive visual quality while allowing runtime behavior adaptation to the platform-imposed dynamic energy budgets. We report extensive experiments on diverse datasets, scenarios, and platforms and demonstrate the superiority of AdaEnlight compared with state-of-the-art low-light image and video enhancement solutions.\nhttps://doi.org/10.1145/3569464\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "School of Computer Science"
            }
          ],
          "personId": 128963
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "School of Computer Science"
            }
          ],
          "personId": 128960
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong kong",
              "institution": "School of Data Science"
            }
          ],
          "personId": 128957
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "xian",
              "institution": "northwestern polytechnical univ."
            }
          ],
          "personId": 128954
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "shaanxi",
              "city": "xi'an",
              "institution": "School of Computer Science"
            }
          ],
          "personId": 128951
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128948
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128974
        }
      ]
    },
    {
      "id": 128323,
      "typeId": 12924,
      "title": "Lumos: An Open-Source Device for Wearable Spectroscopy Research",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569502",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "\"Spectroscopy, the study of the interaction between electromagnetic radiation and matter, is a vital technique in many disciplines. This technique is limited to lab settings, and, as such, sensing is isolated and infrequent. Thus, it can only provide a brief snapshot of the monitored parameter. Wearable technology brings sensing and tracking technologies out into everyday life, creating longitudinal datasets that provide more insight into the monitored parameter. In this paper, we describe Lumos, an open-source device for wearable spectroscopy research. Lumos can facilitate on-body spectroscopy research in health monitoring, athletics, rehabilitation, and more. We developed an algorithm to determine the spectral response of a medium with a mean absolute error of 13nm. From this, researchers can determine the optimal spectrum and create customized sensors for their target application. We show the utility of Lumos in a pilot study, sensing of prediabetes, where we determine the relevant spectrum for glucose and create and evaluate a targeted tracking device.\nhttps://dl.acm.org/doi/10.1145/3569502\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania"
            }
          ],
          "personId": 128674
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania"
            }
          ],
          "personId": 128673
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "Drexel University"
            }
          ],
          "personId": 128672
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Philadelphia",
              "institution": "University of Pennsylvania"
            }
          ],
          "personId": 128697
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt"
            }
          ],
          "personId": 128696
        }
      ]
    },
    {
      "id": 128324,
      "typeId": 12924,
      "title": "PackquID: In-packet Liquid Identification Using RF Signals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569469",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "There are many scenarios where the liquid is occluded by other items (e.g. books in a packet), in which existing RF-based liquid identification methods are generally not suitable. Moreover, status methods are not applicable when the height of the liquid to be tested changes. This paper proposes PackquID, an RF-based in-packet liquid identification system, which can identify liquid without prior knowledge. In dealing with the obstruction of other items and the unknown container, we utilize a dual-antenna model and craft a relative frequency response factor, exploring the diversity of the permittivity in the frequency domain. In tackling the variable liquid height, we extend our model to 3D scope by analyzing the electric field distribution and solving the height effect via spatial-differential model. With 500 pages of printer paper obscured, PackquID can identify 9 common liquids, including Coca-Cola and Pepsi, with an accuracy of over 86% for 4 different packets (canvas bag, paper bag, backpack, and box) and 4 different containers. Nevertheless, PackquID can still identify liquids with an accuracy rate of over 87%, even when the liquid height changes from 4 cm to 12 cm.\nhttps://dl.acm.org/doi/10.1145/3569469",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Hefei",
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 128966
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Hefei",
              "institution": "School of Computer Science and Technology of USTC"
            }
          ],
          "personId": 128964
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hefei",
              "institution": "University of Science and Technology of China"
            }
          ],
          "personId": 128961
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Anhui",
              "city": "Hefei",
              "institution": "USTC"
            }
          ],
          "personId": 128958
        }
      ]
    },
    {
      "id": 128325,
      "typeId": 12924,
      "title": "GPS-assisted Indoor Pedestrian Dead Reckoning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569467",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "Indoor pedestrian dead reckoning (PDR) using embedded inertial sensors in smartphones has been actively studied in the ubicomp community. However, PDR relying only on inertial sensors suffers from the accumulation of errors from the sensors. Researchers have employed various indoor landmarks detectable by smartphone sensors such as magnetic fingerprints caused by elevators and Bluetooth signals from beacons with known coordinates to compensate for the errors. This study proposes a new type of indoor landmark that does not require additional device installation, e.g., beacons, and training data collection in a target environment, e.g., magnetic fingerprints, unlike existing landmarks. This study proposes the use of GPS signals received by a smartphone to correct the accumulated errors of the PDR. While it is impossible to locate the smartphone indoors using GPS satellites, the smartphone can receive signals at a window-side area through windows from satellites aligned with the orientation of the window normal. Based on this idea, we design a machine-learning-based module for detecting the proximity of a user to a window and the orientation of the window, which enables us to roughly determine the absolute coordinates of the smartphone and to correct the accumulated errors by referring to positions of window-side areas found in the floor plan of the environment. A key technical contribution of this study is designing the module, such that it can be trained based on data from environments other than the target environment yet work in any environment by extracting GPS-related information independent of wall orientation. We evaluated the effectiveness of the proposed method using sensor data collected in real environments.\nhttps://dl.acm.org/doi/10.1145/3569467",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Suita",
              "institution": "Osaka university"
            }
          ],
          "personId": 128698
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Suita",
              "institution": "Osaka University"
            }
          ],
          "personId": 128719
        }
      ]
    },
    {
      "id": 128326,
      "typeId": 12924,
      "title": "HearFire: Indoor Fire Detection via Inaudible Acoustic Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569500",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "\"Indoor conflagration causes a large number of casualties and property losses worldwide every year. Yet existing indoor fire detection systems either suffer from short sensing range (e.g., ≤ 0.5m using a thermometer), susceptible to interferences (e.g., smoke detector) or high computational and deployment overhead (e.g., cameras, Wi-Fi). This paper proposes HearFire, a cost-effective, easy-to-use and timely room-scale fire detection system via acoustic sensing. HearFire consists of a collocated commodity speaker and microphone pair, which remotely senses fire by emitting inaudible sound waves. Unlike existing works that use signal reflection effect to fulfill acoustic sensing tasks, HearFire leverages sound absorption and sound speed variations to sense the fire due to unique physical properties of flame. Through a deep analysis of sound transmission, HearFire effectively achieves room-scale sensing by correlating the relationship between the transmission signal length and sensing distance. The transmission frame is carefully selected to expand sensing range and balance a series of practical factors that impact the system's performance. We further design a simple yet effective approach to remove the environmental interference caused by signal reflection by conducting a deep investigation into channel differences between sound reflection and sound absorption. Specifically, sound reflection results in a much more stable pattern in terms of signal energy than sound absorption, which can be exploited to differentiate the channel measurements caused by fire from other interferences. Extensive experiments demonstrate that HireFire enables a maximum 7m sensing range and achieves timely fire detection in indoor environments with up to 99.2% accuracy under different experiment configurations.\nhttps://doi.org/10.1145/3569500\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Changsha",
              "institution": "Hunan University"
            }
          ],
          "personId": 129480
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hunan",
              "city": "ChangSha",
              "institution": "Hunan University"
            }
          ],
          "personId": 129478
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Changsha",
              "institution": "Hunan University"
            }
          ],
          "personId": 129476
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "Lingnan University"
            }
          ],
          "personId": 129475
        }
      ]
    },
    {
      "id": 128327,
      "typeId": 12924,
      "title": "Privacy-Enhancing Technology and Everyday Augmented Reality: Understanding Bystanders’ Varying Needs for Awareness and Consent",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569501",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "Fundamental to Augmented Reality (AR) headsets is their capacity to visually and aurally sense the world around them, necessary to drive the positional tracking that makes rendering 3D spatial content possible. This requisite sensing also opens the door for more advanced AR-driven activities, such as augmented perception, volumetric capture and biometric identification - activities with the potential to expose bystanders to significant privacy risks. Existing Privacy-Enhancing Technologies (PETs) often safeguard against these risks at a low level e.g., instituting camera access controls. However, we argue that such PETs are incompatible with the need for always-on sensing given AR headsets' intended everyday use. Through an online survey (N=102), we examine bystanders' awareness of, and concerns regarding, potentially privacy infringing AR activities; the extent to which bystanders' consent should be sought; and the level of granularity of information necessary to provide awareness of AR activities to bystanders. Our findings suggest that PETs should take into account the AR activity type, and relationship to bystanders, selectively facilitating awareness and consent. In this way, we can ensure bystanders feel their privacy is respected by everyday AR headsets, and avoid unnecessary rejection of these powerful devices by society. https://doi.org/10.1145/3569501",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 129211
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 129209
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Darmstadt",
              "institution": "TU-Darmstadt"
            }
          ],
          "personId": 129251
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 129249
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Hannover",
              "institution": "Leibniz University Hannover"
            }
          ],
          "personId": 129231
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 129245
        }
      ]
    },
    {
      "id": 128328,
      "typeId": 12924,
      "title": "Differentiable Neural Network Pruning to Enable Smart Applications on Microcontrollers",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569468",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"Wearable, embedded, and IoT devices are a centrepiece of many ubiquitous computing applications, such as fitness tracking, health monitoring, home security and voice assistants. By gathering user data through a variety of sensors and leveraging machine learning (ML), applications can adapt their behaviour: in other words, devices become \"\"smart\"\". Such devices are typically powered by microcontroller units (MCUs). As MCUs continue to improve, smart devices become capable of performing a non-trivial amount of sensing and data processing, including machine learning inference, which results in a greater degree of user data privacy and autonomy, compared to offloading the execution of ML models to another device.\nAdvanced predictive capabilities across many tasks make neural networks an attractive ML model for ubiquitous computing applications; however, on-device inference on MCUs remains extremely challenging. Orders of magnitude less storage, memory and computational ability, compared to what is typically required to execute neural networks, impose strict structural constraints on the network architecture and call for specialist model compression methodology. In this work, we present a differentiable structured pruning method for convolutional neural networks, which integrates a model's MCU-specific resource usage and parameter importance feedback to obtain highly compressed yet accurate models. Compared to related network pruning work, compressed models are more accurate due to better use of MCU resource budget, and compared to MCU specialist work, compressed models are produced faster. The user only needs to specify the amount of available computational resources and the pruning algorithm will automatically compress the network during training to satisfy them.\nWe evaluate our methodology using benchmark image and audio classification tasks and find that it (a) improves key resource usage of neural networks up to 80x; (b) has little to no overhead or even improves model training time; (c) produces compressed models with matching or improved resource usage up to 1.4x in less time compared to prior MCU-specific model compression methods.\nhttps://doi.org/10.1145/3569468\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "University of Cambridge"
            }
          ],
          "personId": 128712
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "University of Cambridge"
            }
          ],
          "personId": 128711
        }
      ]
    },
    {
      "id": 128329,
      "typeId": 12924,
      "title": "Come Fly With Me - Investigating the Effects of Path Visualizations in Automated Urban Air Mobility",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596249",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Automated Urban Air Mobility will enhance passenger transportation in metropolitan areas in the near future. Potential passengers, however, have little knowledge about this mobility form. Therefore, there could be concerns about safety and low trust. As trajectories are essential information to address these concerns, we evaluated seven path visualizations in an online video-based study (N=99). We found that a path line visualization was rated highest for trust and perceived safety. In a follow-up virtual reality study (N=24), we evaluated the effects of this visualization and of other air traffic flying by. We found that the participants looked at the path line more often when other air traffic was present and that the path line increased trust and predictability of the air taxi's future path.\nhttps://doi.org/10.1145/3596249\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "Ulm University"
            }
          ],
          "personId": 129223
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "Institute of Media Informatics"
            }
          ],
          "personId": 129222
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Ulm",
              "institution": "Institute of Media Informatics"
            }
          ],
          "personId": 129218
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "Institute of Mediainformatics"
            }
          ],
          "personId": 129216
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "University of Ulm"
            }
          ],
          "personId": 129214
        }
      ]
    },
    {
      "id": 128330,
      "typeId": 12924,
      "title": "Helping Users Debug Trigger-Action Programs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569506",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "abstract": "Trigger-action programming (TAP) empowers a wide array of users to automate Internet of Things (IoT) devices. However, it can be challenging for users to create completely correct trigger-action programs (TAPs) on the first try, necessitating debugging. While TAP has received substantial research attention, TAP debugging has not. In this paper, we present the first empirical study of users' end-to-end TAP debugging process, focusing on obstacles users face in debugging TAPs and how well users ultimately fix incorrect automations. To enable this study, we added TAP capabilities to an existing 3-D smart home simulator. Thirty remote participants spent a total of 84 hours debugging TAPs using this simulator. Without additional support, participants were often unable to fix buggy TAPs due to a series of obstacles we document. However, we also found that two novel tools we developed helped participants overcome many of these obstacles and more successfully debug TAPs. These tools collect either implicit or explicit feedback from users about automations that should or should not have happened in the past, using a SAT-solving-based algorithm we developed to automatically modify the TAPs to account for this feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 128268
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 128267
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University"
            }
          ],
          "personId": 128266
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129084
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 129081
        }
      ]
    },
    {
      "id": 128331,
      "typeId": 12924,
      "title": "PrISM-Tracker: A Framework for Multimodal Procedure Tracking Using Wearable Sensors and State Transition Information with User-Driven Handling of Errors and Uncertainty",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569504",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "A user often needs training and guidance while performing several daily life procedures, e.g., cooking, setting up a new appliance, or doing a COVID test. Watch-based human activity recognition (HAR) can track users' actions during these procedures. However, out of the box, state-of-the-art HAR struggles from noisy data and less-expressive actions that are often part of daily life tasks. This paper proposes PrISM-Tracker, a procedure-tracking framework that augments existing HAR models with (1) graph-based procedure representation and (2) a user-interaction module to handle model uncertainty. Specifically, PrISM-Tracker extends a Viterbi algorithm to update state probabilities based on time-series HAR outputs by leveraging the graph representation that embeds time information as prior. Moreover, the model identifies moments or classes of uncertainty and asks the user for guidance to improve tracking accuracy. We tested PrISM-Tracker in two procedures: latte-making in an engineering lab study and wound care for skin cancer patients at a clinic. The results showed the effectiveness of the proposed algorithm utilizing transition graphs in tracking steps and the efficacy of using simulated human input to enhance performance. This work is the first step toward human-in-the-loop intelligent systems for guiding users while performing new and complicated procedural tasks.\nhttps://dl.acm.org/doi/10.1145/3569504",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128578
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tsukuba",
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 128625
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128623
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128621
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "Case Western Reserve University School of Medicine"
            }
          ],
          "personId": 128619
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "Case Western Reserve University"
            }
          ],
          "personId": 128617
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "Case Western Reserve University"
            }
          ],
          "personId": 128615
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Boston University"
            }
          ],
          "personId": 128613
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Cleveland",
              "institution": "University Hospitals of Cleveland"
            }
          ],
          "personId": 128612
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128627
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128608
        }
      ]
    },
    {
      "id": 128332,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "LemurDx: Using Unconstrained Passive Sensing for an Objective Measurement of Hyperactivity in Children with no Parent Input",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596244",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "Hyperactivity is the most dominant presentation of Attention-Deficit/Hyperactivity Disorder in young children. Currently, measuring hyperactivity involves parents' or teachers' reports. These reports are vulnerable to subjectivity and can lead to misdiagnosis. LemurDx provides an objective measure of hyperactivity using passive mobile sensing. We collected data from 61 children (25 with hyperactivity) who wore a smartwatch for up to 7 days without changing their daily routine. The participants' parents maintained a log of the child's activities at a half-hour granularity (e.g., sitting, exercising) as contextual information. Our ML models achieved 85.2% accuracy in detecting hyperactivity in children (using parent-provided activity labels). We also built models that estimated children's context from the sensor data and did not rely on activity labels to reduce parent burden. These models achieved 82.0% accuracy in detecting hyperactivity. In addition, we interviewed five clinicians who suggested a need for a tractable risk score that enables analysis of a child's behavior across contexts. Our results show the feasibility of supporting the diagnosis of hyperactivity by providing clinicians with an interpretable and objective score of hyperactivity using off-the-shelf watches and adding no constraints to children or their guardians.\nhttps://dl.acm.org/doi/10.1145/3596244",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128875
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh"
            }
          ],
          "personId": 129143
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh"
            }
          ],
          "personId": 129140
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "NuRelm"
            }
          ],
          "personId": 129186
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh School of Medicine"
            }
          ],
          "personId": 129183
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129180
        }
      ]
    },
    {
      "id": 128333,
      "typeId": 12924,
      "title": "SkinLink: On-body Construction and Prototyping of Reconfigurable Epidermal Interfaces",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596241",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "Applying customized epidermal electronics closely onto the human skin offers the potential for biometric sensing and unique, always-available on-skin interactions. However, iterating designs of an on-skin interface from schematics to physical circuit wiring can be time-consuming, even with tiny modifications; it is also challenging to preserve skin wearability after repeated alteration. We present SkinLink, a reconfigurable on-skin fabrication approach that allows users to intuitively explore and experiment with the circuitry adjustment on the body. We demonstrate SkinLink with a customized on-skin prototyping toolkit comprising tiny distributed circuit modules and a variety of streamlined trace modules that adapt to diverse body surfaces. To evaluate SkinLink's performance, we conducted a 14-participant usability study to compare and contrast the workflows with a benchmark on-skin construction toolkit. Four case studies targeting a film makeup artist, two beauty makeup artists, and a wearable computing designer further demonstrate different application scenarios and usages.\nhttps://dl.acm.org/doi/10.1145/3596241",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128811
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128807
        }
      ]
    },
    {
      "id": 128334,
      "typeId": 12924,
      "title": "Scalability in External Communication of Automated Vehicles: Evaluation and Recommendations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596248",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Automated vehicles will alter traffic fundamentally. While users can engage in non-driving-related tasks such as reading or even sleeping, the possibility to interact with other \nroad users such as pedestrians via, for example, eye contact vanishes. Therefore, external communication of automated vehicles is currently researched with various concepts spanning dimensions such as anthropomorphism, technology, viewpoint, locus, message type, and others. However, the proposed concepts are mostly evaluated in simple scenarios, such as one person trying to cross in front of one automated vehicle. Therefore, we implemented a WebGL application of a four-lane road and conducted a within-subject study (N=46) to study the effects of nine concepts with and without the presence of other pedestrians and altering the yielding target of the automated vehicle. We found that all concepts were rated better than having no external communication. However, the effects were not uniform across the concepts.\nhttps://doi.org/10.1145/3596248\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "Ulm University"
            }
          ],
          "personId": 129370
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "Ulm University"
            }
          ],
          "personId": 129368
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Ulm",
              "institution": "University of Ulm"
            }
          ],
          "personId": 129366
        }
      ]
    },
    {
      "id": 128335,
      "typeId": 12924,
      "title": "Fingerprinting IoT Devices Using Latent Physical Side-Channels",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596247",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "The proliferation of low-end low-power internet-of-things (IoT) devices in \"smart\" environments necessitates secure identification and authentication of these devices via low-overhead fingerprinting methods. Previous work typically utilizes characteristics of the device's wireless modulation (WiFi, BLE, etc.) in the spectrum, or more recently, electromagnetic emanations from the device's DRAM to perform fingerprinting. The problem is that many devices, especially low-end IoT/embedded systems, may not have transmitter modules, DRAM, or other complex components, therefore making fingerprinting infeasible or challenging. To address this concern, we utilize electromagnetic emanations derived from the processor's clock to fingerprint. We present Digitus, an emanations-based fingerprinting system that can authenticate IoT devices at range. The advantage of Digitus is that we can authenticate low-power IoT devices using features intrinsic to their normal operation without the need for additional transmitters and/or other complex components such as DRAM. Our experiments demonstrate that we achieve ≥ 95% accuracy on average, applicability in a wide range of IoT scenarios (range ≥ 5m, non-line-of-sight, etc.), as well as support for IoT applications such as finding hidden devices. Digitus represents a low-overhead solution for the authentication of low-end IoT devices.\nhttps://dl.acm.org/doi/10.1145/3596247",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128748
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128747
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128744
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128754
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA"
            }
          ],
          "personId": 128753
        }
      ]
    },
    {
      "id": 128336,
      "typeId": 12924,
      "title": "ThermoFit: Thermoforning Smart Orthoses via Metamaterial Structures for Body-Fitting and Component-Adjusting",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580806",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "\"Smart orthoses hold great potential for intelligent rehabilitation monitoring and training. However, most of these electronic assistive devices are typically too difficult for daily use and challenging to modify to accommodate variations in body shape and medical needs. For existing clinicians, the customization pipeline of these smart devices imposes significant learning costs. This paper introduces ThermoFit, an end-to-end design and fabrication pipeline for thermoforming smart orthoses that adheres to the clinically accepted procedure. ThermoFit enables the shapes and electronics positions of smart orthoses to conform to bodies and allows rapid iteration by integrating low-cost Low-Temperature Thermoplastics (LTTPs) with custom metamaterial structures and electronic components. Specifically, three types of metamaterial structures are used in LTTPs to reduce the wrinkles caused by the thermoforming process and to permit component position adjustment and joint movement. A design tool prototype aids in generating metamaterial patterns and optimizing component placement and circuit routing. Three applications show that ThermoFit can be shaped on bodies to different wearables. Finally, a hands-on study with a clinician verifies the user-friendliness of thermoforming smart orthosis, and technical evaluations demonstrate fabrication efficiency and electronic continuity. \nhttps://doi.org/10.1145/3580806\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129367
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129365
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129364
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129363
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab"
            }
          ],
          "personId": 129382
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "BJ",
              "city": "beijing",
              "institution": "Tsinghua university"
            }
          ],
          "personId": 129380
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "International Design Institute of Zhejiang University"
            }
          ],
          "personId": 129378
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129376
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129374
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129372
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129390
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University"
            }
          ],
          "personId": 129388
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Software, Chinese Academy of Sciences"
            }
          ],
          "personId": 129387
        },
        {
          "affiliations": [],
          "personId": 129386
        },
        {
          "affiliations": [],
          "personId": 129411
        }
      ]
    },
    {
      "id": 128337,
      "typeId": 12924,
      "title": "Semi-Supervised Learning forWearable-based Momentary Stress Detection in the Wild",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596246",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "\"Physiological and behavioral data collected from wearable or mobile sensors have been used to estimate self-reported stress levels. Since stress annotation usually relies on self-reports during the study, a limited amount of labeled data can be an obstacle to developing accurate and generalized stress-predicting models. On the other hand, the sensors can continuously capture signals without annotations. This work investigates leveraging unlabeled wearable sensor data for stress detection in the wild. We propose a two-stage semi-supervised learning framework that leverages wearable sensor data to help with stress detection. The proposed structure consists of an auto-encoder pre-training method for learning information from unlabeled data and the consistency regularization approach to enhance the robustness of the model. Besides, we propose a novel active sampling method for selecting unlabeled samples to avoid introducing redundant information to the model. We validate these methods using two datasets with physiological signals and stress labels collected in the wild, as well as four human activity recognition (HAR) datasets to evaluate the generality of the proposed method. Our approach demonstrated competitive results for stress detection, improving stress classification performance by approximately 7% to 10% on the stress detection datasets compared to the baseline supervised learning models. Furthermore, the ablation study we conducted for the HAR tasks supported the effectiveness of our methods. Our approach showed comparable performance to state-of-the-art semi-supervised learning methods for both stress detection and HAR tasks.\nhttps://dl.acm.org/doi/10.1145/3596246\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University"
            }
          ],
          "personId": 128869
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University"
            }
          ],
          "personId": 128842
        }
      ]
    },
    {
      "id": 128338,
      "typeId": 12924,
      "title": "Lost in the Deep? Performance Evaluation of Dead Reckoning Techniques in Underwater Environments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596245",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"Computing research is increasingly addressing underwater environments and examining how computing can support diving and other activities. Unlike on land, where well-established positioning methods are widely available, underwater environments lack a common positioning mechanism, which is a prerequisite for many applications. Dead reckoning, the use of angle and distance estimates to track position changes from a known point of origin, is a promising candidate for underwater positioning as it does not rely on wireless signals (which decay rapidly in underwater environments) and as there is a wide range of literature and algorithms freely available. Yet, currently it is unclear whether the existing techniques can be adopted in underwater environments or whether the differences in medium and environment affect the performance of the dead reckoning techniques. We contribute by evaluating and systematically analyzing the performance and trade-offs associated with dead reckoning techniques in underwater environments. We present AEOLUS, a prototype unit comprising of a low-cost microcontroller and inertial measurement unit, to perform experiments on the ground and in underwater environments to assess how well the performance of different techniques translates from ground-based use cases to underwater environments. We benchmark 15 different algorithms and compare their performance in such environments to identify common patterns and dissimilarities, and identify root causes for these differences. The results show that displacement and turn errors can be estimated to within 5% error but that the best performing methods vary between land and underwater environments. We also show that the performance depends on the shape of the motion patterns with some algorithms performing better for hard turns whereas others perform better for gradual, more continuous turns.\nhttps://dl.acm.org/doi/10.1145/3596245\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Madeira",
              "city": "Funchal",
              "institution": "MARE, ARDITI, University of Madeira, University of Belgrade"
            }
          ],
          "personId": 129422
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Funchal",
              "institution": "Wave Labs, MARE, ARDITI, University of Madeira"
            }
          ],
          "personId": 129393
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Funchal",
              "institution": "Wave Labs, MARE, ARDITI, University of Madeira"
            }
          ],
          "personId": 129392
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Funchal",
              "institution": "Wave Labs, MARE, ARDITI, University of Madeira"
            }
          ],
          "personId": 129445
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "city": "Funchal",
              "institution": "MARE, ARDITI, University of Madeira"
            }
          ],
          "personId": 129443
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "University of Helsinki"
            }
          ],
          "personId": 129441
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "University of Helsinki"
            }
          ],
          "personId": 129438
        },
        {
          "affiliations": [
            {
              "country": "Estonia",
              "city": "Tartu",
              "institution": "University of Tartu"
            }
          ],
          "personId": 129435
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "University of Helsinki"
            }
          ],
          "personId": 129432
        }
      ]
    },
    {
      "id": 128339,
      "typeId": 12924,
      "title": "MoCaPose: Motion Capturing with Textile-integrated Capacitive Sensors in Loose-fitting Smart Garments",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580883",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"We present MoCaPose, a novel wearable motion capturing (MoCap) approach to continuously track the wearer's upper body's dynamic poses through multi-channel capacitive sensing integrated in fashionable, loose-fitting jackets. Unlike conventional wearable IMU MoCap based on inverse dynamics, MoCaPose decouples the sensor position from the pose system. MoCaPose uses a deep regressor to continuously predict the 3D upper body joints coordinates from 16-channel textile capacitive sensors, unbound by specific applications. The concept is implemented through two prototyping iterations to first solve the technical challenges, then establish the textile integration through fashion-technology co-design towards a design-centric smart garment. A 38-hour dataset of synchronized video and capacitive data from 21 participants was recorded for validation. The motion tracking result was validated on multiple levels from statistics (R2 ~ 0.91) and motion tracking metrics (MP JPE ~ 86mm) to the usability in pose and motion recognition (0.9 F1 for 10-class classification with unsupervised class discovery). The design guidelines impose few technical constraints, allowing the wearable system to be design-centric and usecase-specific. Overall, MoCaPose demonstrates that textile-based capacitive sensing with its unique advantages, can be a promising alternative for wearable motion tracking and other relevant wearable motion recognition applications.\nhttps://doi.org/10.1145/3580883\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129205
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129203
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129201
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129197
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Berlin",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129196
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129195
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129193
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129191
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129189
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Zürich",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 129187
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Berlin",
              "institution": "German Research Centerfor Artificial Intelligence(DFKI)"
            }
          ],
          "personId": 129229
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 129227
        }
      ]
    },
    {
      "id": 128340,
      "typeId": 12924,
      "title": "A Data-Driven Context-Aware Health Inference System for Children during School Closures",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580800",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "\"Many countries have implemented school closures due to the outbreak of the COVID-19 pandemic, which has inevitably affected children's physical and mental health. It is vital for parents to pay special attention to their children's health status during school closures. However, it is difficult for parents to recognize the changes in their children's health, especially without visible symptoms, such as psychosocial functioning in mental health. Moreover, healthcare resources and understanding of the health and societal impact of COVID-19 are quite limited during the pandemic. Against this background, we collected real-world datasets from 1,172 children in Hong Kong during four time periods under different pandemic and school closure conditions from September 2019 to January 2022. Based on these data, we first perform exploratory data analysis to explore the impact of school closures on six health indicators, including physical activity intensity, physical functioning, self-rated health, psychosocial functioning, resilience, and connectedness. We further study the correlation between children's contextual characteristics (i.e., demographics, socioeconomic status, electronic device usage patterns, financial satisfaction, academic performance, sleep pattern, exercise habits, and dietary patterns) and the six health indicators. Subsequently, a health inference system is designed and developed to infer children's health status based on their contextual features to derive the risk factors of the six health indicators. The evaluation and case studies on real-world datasets show that this health inference system can help parents and authorities better understand key factors correlated with children's health status during school closures.\nhttps://doi.org/10.1145/3580800\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129006
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129005
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "Faculty of Engineering, The University of Hong Kong"
            }
          ],
          "personId": 129046
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "Faculty of Science, The University of Hong Kong"
            }
          ],
          "personId": 129044
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "Faculty of Engineering, The University of Hong Kong"
            }
          ],
          "personId": 129042
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129040
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "Faculty of Engineering, The University of Hong Kong"
            }
          ],
          "personId": 129038
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "LKS Faculty of Medicine, The University of Hong Kong"
            }
          ],
          "personId": 129036
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "LKS Faculty of Medicine, The University of Hong Kong"
            }
          ],
          "personId": 129034
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "LKS Faculty of Medicine, The University of Hong Kong"
            }
          ],
          "personId": 129031
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "LKS Faculty of Medicine, The University of Hong Kong"
            }
          ],
          "personId": 129030
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129029
        }
      ]
    },
    {
      "id": 128341,
      "typeId": 12924,
      "title": "Eggly: Designing Mobile Augmented Reality Neurofeedback Training Games for Children with Autism Spectrum Disorder",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596251",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder that affects how children communicate and relate to other people and the world around them. Emerging studies have shown that neurofeedback training (NFT) games are an effective and playful intervention to enhance social and attentional capabilities for autistic children. However, NFT is primarily available in a clinical setting that is hard to scale. Also, the intervention demands deliberately-designed gamified feedback with fun and enjoyment, where little knowledge has been acquired in the HCI community. Through a ten-month iterative design process with four domain experts, we developed Eggly, a mobile NFT game based on a consumer-grade EEG headband and a tablet. Eggly uses novel augmented reality (AR) techniques to offer engagement and personalization, enhancing their training experience. We conducted two field studies (a single-session study and a three-week multi-session study) with a total of five autistic children to assess Eggly in practice at a special education center. Both quantitative and qualitative results indicate the effectiveness of the approach as well as contribute to the design knowledge of creating mobile AR NFT games.\nhttps://dl.acm.org/doi/10.1145/3596251",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo"
            }
          ],
          "personId": 129336
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 129334
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 129332
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 129350
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo"
            }
          ],
          "personId": 129348
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "National Research Council"
            }
          ],
          "personId": 129346
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo"
            }
          ],
          "personId": 129344
        }
      ]
    },
    {
      "id": 128342,
      "typeId": 12924,
      "title": "MagSound: Magnetic Field Assisted Wireless Earphone Tracking",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580889",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "\"Wireless earphones are pervasive acoustic sensing platforms that can be used for many applications such as motion tracking and handwriting input. However, wireless earphones suffer clock offset between the connected smart devices, which would accumulate error rapidly over time. Moreover, compared with smartphone and voice assistants, the acoustic signal transmitted by wireless earphone is much weaker due to the poor frequency response. In this paper, we propose MagSound, which uses the built-in magnets to improve the tracking and acoustic sensing performance of Commercial-Off-The-Shelf (COTS) earphones. Leveraging magnetic field strength, MagSound can predict the position of wireless earphones free from clock offset, which can be used to re-calibrate the acoustic tracking. Further, the fusion of the two modalities mitigates the accumulated clock offset and multipath effect. Besides, to increase the robustness to noise, MagSound employs finely designed Orthogonal Frequency-Division Multiplexing (OFDM) ranging signals. We implement a prototype of MagSound on COTS and perform experiments for tracking and handwriting input. Results demonstrate that MagSound maintains millimeter-level error in 2D tracking, and improves the handwriting recognition accuracy by 49.81%. We believe that MagSound can contribute to practical applications of wireless earphones-based sensing.\nhttps://dl.acm.org/doi/10.1145/3580889\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 129104
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 129103
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 129102
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 129098
        }
      ]
    },
    {
      "id": 128343,
      "typeId": 12924,
      "title": "WiMeasure: Millimeter-level Object Size Measurement with Commodity WiFi Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596250",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "In the past few years, a large range of wireless signals such as WiFi, RFID, UWB and Millimeter Wave were utilized for sensing purposes. Among these wireless sensing modalities, WiFi sensing attracts a lot of attention owing to the pervasiveness of WiFi infrastructure in our surrounding environments. While WiFi sensing has achieved a great success in capturing the target's motion information ranging from coarse-grained activities and gestures to fine-grained vital signs, it still has difficulties in precisely obtaining the target size owing to the low frequency and small bandwidth of WiFi signals. Even Millimeter Wave radar can only achieve a very coarse-grained size measurement. High precision object size sensing requires using RF signals in the extremely high-frequency band (e.g., Terahertz band). In this paper, we utilize low-frequency WiFi signals to achieve accurate object size measurement without requiring any learning or training. The key insight is that when an object moves between a pair of WiFi transceivers, the WiFi CSI variations contain singular points (i.e., singularities) and we observe an exciting opportunity of employing the number of singularities to measure the object size. In this work, we model the relationship between the object size and the number of singularities when an object moves near the LoS path, which lays the theoretical foundation for the proposed system to work. By addressing multiple challenges, for the first time, we make WiFi-based object size measurement work on commodity WiFi cards and achieve a surprisingly low median error of 2.6 mm. We believe this work is an important missing piece of WiFi sensing and opens the door to size measurement using low-cost low-frequency RF signals.\nhttps://dl.acm.org/doi/10.1145/3596250",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129413
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129412
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129410
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst"
            }
          ],
          "personId": 129408
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129405
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "School of Electronics Engineering and Computer Science"
            }
          ],
          "personId": 129402
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129399
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129396
        }
      ]
    },
    {
      "id": 128344,
      "typeId": 12924,
      "title": "Behavior Modeling Approach for Forecasting Physical Functioning of People with Multiple Sclerosis",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580887",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"Forecasting physical functioning of people with Multiple Sclerosis (MS) can inform timely clinical interventions and accurate \"\"day planning\"\" to improve their well-being. However, people's physical functioning often remains unchecked in between infrequent clinical visits, leading to numerous negative healthcare outcomes. Existing Machine Learning (ML) models trained on in-situ data collected outside of clinical settings (e.g., in people's homes) predict which people are currently experiencing low functioning. However, they do not forecast if and when people's symptoms and behaviors will negatively impact their functioning in the future. Here, we present a computational behavior model that formalizes clinical knowledge about MS to forecast people's end-of-day physical functioning in advance to support timely interventions. Our model outperformed existing ML baselines in a series of quantitative validation experiments. We showed that our model captured clinical knowledge about MS using qualitative visual model exploration in different \"\"what-if\"\" scenarios. Our work enables future behavior-aware interfaces that deliver just-in-time clinical interventions and aid in \"\"day planning\"\" and \"\"activity pacing\"\".\nhttps://dl.acm.org/doi/10.1145/3580887\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129178
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129176
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129174
        }
      ]
    },
    {
      "id": 128345,
      "typeId": 12924,
      "title": "DeepBrain: Enabling Fine-grain Brain-Robot Interaction through Human-Centered Learning of Coarse EEG Signals from Low-cost Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3550334",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"With the recent advancements of electroencephalograph (EEG) techniques, some brain-computer interface (BCI) solutions have been explored to assist individuals performing various tasks with their minds. One promising application is to combine BCI with robotic systems so that the mobility-impaired people can control robots to take care of themselves. Towards this ultimate goal to design BCIs for mobility-impaired, we firstly conducted an online survey with 54 mobility-impaired participants who barely had previous experience with BCI to identify the challenges they face in life for the purpose of designing a personalized BCI system in need. The results revealed these challenges including small daily tasks (such as feeding and cleaning), which weigh on the financial burdens of hiring a caregiver. Meanwhile, the off-the-shelf high-fidelity BCIs are often expensive, whereas the cheaper devices only collect coarse-grained signals, preventing practical application in care aids due to lack of temporal resolution and accuracy. Based on the survey findings, we then designed DeepBrain, a human-centered learning augmented BCI system, that requires only coarse-grained brain signals with low-cost BCI equipment, but supports fine-grained brain-robot interaction and scalable multi-robot collaboration for domestic multi-task operations. A follow-up system comparison with other approaches show that the proposed human-centered solution is a promising step towards the ultimate goal, as it achieves satisfactory accuracy with less low computation resources. Also the practical brain to multi-robot interaction system validates the feasibility of our framework and model used in DeepBrain.\nhttps://dl.acm.org/doi/10.1145/3550334\"",
      "authors": [
        {
          "affiliations": [],
          "personId": 129253
        }
      ]
    },
    {
      "id": 128346,
      "typeId": 12924,
      "title": "PARROT: Interactive Privacy-Aware Internet of Things Application Design Tool",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580880",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"Internet of Things (IoT) applications typically collect and analyse personal data that is categorised as sensitive or special category of personal data. These data are subject to a higher degree of protection under data privacy laws. Regardless of legal requirements to support privacy practices, such as in Privacy by Design (PbD) schemes, these practices are not yet commonly followed by software developers. The difficulty of developing privacy-preserving applications emphasises the importance of exploring the problems developers face to embed privacy techniques, suggesting the need for a supporting tool. An interactive IoT application design tool - PARROT (PrivAcy by design tool foR inteRnet Of Things) - is presented. This tool helps developers to design privacy-aware IoT applications, taking account of privacy compliance during the design process and providing real-time feedback on potential privacy violations. A user study with 18 developers was conducted, comprising a semi-structured interview and a design exercise to understand how developers typically handle privacy within the design process. Collaboration with a privacy lawyer was used to review designs produced by developers to uncover privacy limitations that could be addressed by developing a software tool. Based on the findings, a proof-of-concept prototype of PARROT was implemented and evaluated in two controlled lab studies. The outcome of the study indicates that IoT applications designed with PARROT addressed privacy concerns better and managed to reduce several of the limitations identified. From a privacy compliance perspective, PARROT helps developers to address compliance requirements throughout the design and testing process. This is achieved by incorporating privacy specific design features into the IoT application from the beginning rather than retrospectively. (Demo Video).\nhttps://doi.org/10.1145/3580880\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Walse",
              "city": "Cardiff",
              "institution": "Cardiff University"
            }
          ],
          "personId": 129244
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "My Data Fix Ltd"
            }
          ],
          "personId": 129242
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "University College London"
            }
          ],
          "personId": 129240
        },
        {
          "affiliations": [
            {
              "country": "Sri Lanka",
              "city": "Moratuwa",
              "institution": "University of Moratuwa"
            }
          ],
          "personId": 129238
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cardiff",
              "institution": "Cardiff University"
            }
          ],
          "personId": 129236
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cardiff",
              "institution": "Cardiff University"
            }
          ],
          "personId": 129234
        }
      ]
    },
    {
      "id": 128347,
      "typeId": 12924,
      "title": "SmartASL: “Point-of-Care” Comprehensive ASL Interpreter Using Wearables",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596255",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "Sign language builds up an important bridge between the d/Deaf and hard-of-hearing (DHH) and hearing people. Regrettably, most hearing people face challenges in comprehending sign language, necessitating sign language translation. However, state-of-the-art wearable-based techniques mainly concentrate on recognizing manual markers (e.g., hand gestures), while frequently overlooking non-manual markers, such as negative head shaking, question markers, and mouthing. This oversight results in the loss of substantial grammatical and semantic information in sign language. To address this limitation, we introduce SmartASL, a novel proof-of-concept system that can 1) recognize both manual and non-manual markers simultaneously using a combination of earbuds and a wrist-worn IMU, and 2) translate the recognized American Sign Language (ASL) glosses into spoken language. Our experiments demonstrate the SmartASL system's significant potential to accurately recognize the manual and non-manual markers in ASL, effectively bridging the communication gaps between ASL signers and hearing people using commercially available devices.\nhttps://dl.acm.org/doi/10.1145/3596255",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "BUFFALO",
              "institution": "Univerisity at Buffalo, the State University of New York"
            }
          ],
          "personId": 129429
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "HP Inc."
            }
          ],
          "personId": 129426
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "East China Normal University"
            }
          ],
          "personId": 128731
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128730
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, State University of New York"
            }
          ],
          "personId": 128729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Denver",
              "institution": "University of Colorado Denver"
            }
          ],
          "personId": 128728
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128727
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128726
        }
      ]
    },
    {
      "id": 128349,
      "typeId": 12924,
      "title": "Space-Mag: An Automatic, Scalable, and Rapid Space Compactor for Optimizing Smartphone App Interfaces for Low-Vision Users",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596253",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "Low-vision users interact with smartphones via screen magnifiers, which uniformly magnify raw screen pixels, including whitespace and user interface (UI) elements. Screen magnifiers thus occlude important contextual information, such as visual cues, from the user's viewport. This requires low-vision users to pan over the occluded portions and mentally reconstruct the context, which is cumbersome, tiring, and mentally demanding. Prior work aimed to address these usability issues with screen magnifiers by optimizing the representation of UI elements suitable for low-vision users or by magnifying whitespace and non-whitespace content (e.g., text, graphics, borders) differently. This paper combines both techniques and presents SpaceXMag, an optimization framework that automatically reduces whitespace within a smartphone app, thereby packing more information within the current magnification viewport. A study with 11 low-vision users indicates that, with a traditional screen magnifier, the space-optimized UI is more usable and saves at least 28.13% time for overview tasks and 42.89% time for target acquisition tasks, compared to the original, unoptimized UI of the same app. Furthermore, our framework is scalable, fast, and automatable. For example, on a public dataset containing 16, 566 screenshots of different Android apps, it saves approximately 47.17% of the space (area) on average, with a mean runtime of around 1.44 seconds, without requiring any human input. All are indicative of the promise and potential of SpaceXMag for low-vision screen magnifier users.  \nhttps://doi.org/10.1145/3596253",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University"
            }
          ],
          "personId": 129177
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University"
            }
          ],
          "personId": 129175
        }
      ]
    },
    {
      "id": 128350,
      "typeId": 12924,
      "title": "SeRaNDiP - Leveraging Inherent Sensor Random Noise for Differential Privacy Preservation in Wearable Community Sensing Applications",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596252",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"Personal data collected from today's wearable sensors contain a rich amount of information that can reveal a user's identity. Differential privacy (DP) is a well-known technique for protecting the privacy of the sensor data being sent to community sensing applications while preserving its statistical properties. However, differential privacy algorithms are computationally expensive, requiring user-level random noise generation which incurs high overheads on wearables with constrained hardware resources. In this paper, we propose SeRaNDiP -- which utilizes the inherent random noise existing in wearable sensors for distributed differential privacy. We show how various hardware configuration parameters available in wearable sensors can enable different amounts of inherent sensor noise and ensure distributed differential privacy guarantee for various community sensing applications with varying sizes of populations. Our evaluations of SeRaNDiP on five wearable sensors that are widely used in today's commercial wearables -- MPU-9250 accelerometer, ADXL345 accelerometer, BMP 388 barometer, MLP 3115A2 barometer, and MLX90632 body temperature sensor show a 1.4X-1.8X computation/communication speedup and 1.2X-1.5X energy savings against state-of-the-art DP implementation. To the best of our knowledge, SeRaNDiP is the first framework to leverage the inherent random sensor noise for differential privacy preservation in community sensing without any hardware modification.\nhttps://doi.org/10.1145/3596252\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "West",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 129373
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 129391
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 129389
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 129371
        }
      ]
    },
    {
      "id": 128351,
      "typeId": 12924,
      "title": "PrintShear: Shear Input Based on Fingerprint Deformation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596257",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "Most touch-based input devices, such as touchscreens and touchpads, capture low-resolution capacitive images when a finger touches the device's surface. These devices only output the two-dimensional (2D) positions of contacting points, which are insufficient for complex control tasks, such as the manipulation of 3D objects. To expand the modalities of touch inputs, researchers have proposed a variety of techniques, including finger poses, chording gestures, touch pressure, etc. With the rapid development of fingerprint sensing technology, especially under-screen fingerprint sensors, it has become possible to generate input commands to control multiple degrees of freedom (DOF) at a time using fingerprint images. In this paper, we propose PrintShear, a shear input technique based on fingerprint deformation. Lateral, longitudinal and rotational deformations are extracted from fingerprint images and mapped to 3DOF control commands. Further DOF expansion can be achieved through recognition of the contact region of the touching finger. We conducted a 12-person user study to evaluate the performance of PrintShear on 3D docking tasks. Comparisons with other input methods demonstrated the superiority of our approach. Specifically, a 19.79% reduction in completion time was achieved compared with conventional touch input in a full 6DOF 3D object manipulation task.\nhttps://dl.acm.org/doi/10.1145/3596257",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Department of Automation, BNRist, Tsinghua University"
            }
          ],
          "personId": 129379
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Department of Automation, BNRist, Tsinghua University"
            }
          ],
          "personId": 129377
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Department of Automation, BNRist, Tsinghua University"
            }
          ],
          "personId": 129375
        }
      ]
    },
    {
      "id": 128352,
      "typeId": 12924,
      "title": "DAPPER: Label-Free Performance Estimation after Personalization for Heterogeneous Mobile Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596256",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "Many applications utilize sensors in mobile devices and machine learning to provide novel services. However, various factors such as different users, devices, and environments impact the performance of such applications, thus making the domain shift (i.e., distributional shift between the training domain and the target domain) a critical issue in mobile sensing. Despite attempts in domain adaptation to solve this challenging problem, their performance is unreliable due to the complex interplay among diverse factors. In principle, the performance uncertainty can be identified and redeemed by performance validation with ground-truth labels. However, it is infeasible for every user to collect high-quality, sufficient labeled data. To address the issue, we present DAPPER (Domain AdaPtation Performance EstimatoR) that estimates the adaptation performance in a target domain with only unlabeled target data. Our key idea is to approximate the model performance based on the mutual information between the model inputs and corresponding outputs. Our evaluation with four real-world sensing datasets compared against six baselines shows that on average, DAPPER outperforms the state-of-the-art baseline by 39.8% in estimation accuracy. Moreover, our on-device experiment shows that DAPPER achieves up to 396x less computation overhead compared with the baselines.\nhttps://dl.acm.org/doi/10.1145/3596256",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128831
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128829
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128827
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute for AI Industry Research (AIR), Tsinghua University"
            }
          ],
          "personId": 128825
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128823
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128841
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Daejeon",
              "institution": "KAIST"
            }
          ],
          "personId": 128839
        }
      ]
    },
    {
      "id": 128353,
      "typeId": 12924,
      "title": "LoEar: Push the Range Limit of Acoustic Sensing for Vital Sign Monitoring",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3550293",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "\"Acoustic sensing has been explored in numerous applications leveraging the wide deployment of acoustic-enabled devices. However, most of the existing acoustic sensing systems work in a very short range only due to fast attenuation of ultrasonic signals, hindering their real-world deployment. In this paper, we present a novel acoustic sensing system using only a single microphone and speaker, named LoEar, to detect vital signs (respiration and heartbeat) with a significantly increased sensing range. We first develop a model, namely Carrierforming, to enhance the signal-to-noise ratio (SNR) via coherent superposition across multiple subcarriers on the target path. We then propose a novel technique called Continuous-MUSIC (Continuous-MUltiple SIgnal Classification) to detect a dynamic reflections, containing subtle motion, and further identify the target user based on the frequency distribution to enable Carrierforming. Finally, we adopt an adaptive Infinite Impulse Response (IIR) comb notch filter to recover the heartbeat pattern from the Channel Frequency Response (CFR) measurements which are dominated by respiration and further develop a peak-based scheme to estimate respiration rate and heart rate. We conduct extensive experiments to evaluate our system, and results show that our system outperforms the state-of-the-art using commercial devices, i.e., the range of respiration sensing is increased from 2 m to 7 m, and the range of heartbeat sensing is increased from 1.2 m to 6.5 m.\nhttps://dl.acm.org/doi/10.1145/3550293\"",
      "authors": [
        {
          "affiliations": [],
          "personId": 129254
        }
      ]
    },
    {
      "id": 128354,
      "typeId": 12924,
      "title": "The Power of Speech in the Wild: Discriminative Power of Daily Voice Diaries in Understanding Auditory Verbal Hallucinations Using Deep Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610890",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"Mobile phone sensing is increasingly being used in clinical research studies to assess a variety of mental health conditions (e.g., depression, psychosis). However, in-the-wild speech analysis -- beyond conversation detecting -- is a missing component of these mobile sensing platforms and studies. We augment an existing mobile sensing platform with a daily voice diary to assess and predict the severity of auditory verbal hallucinations (i.e., hearing sounds or voices in the absence of any speaker), a condition that affects people with and without psychiatric or neurological diagnoses. We collect 4809 audio diaries from N=384 subjects over a one-month-long study period. We investigate the performance of various deep-learning architectures using different combinations of sensor behavioral streams (e.g., voice, sleep, mobility, phone usage, etc.) and show the discriminative power of solely using audio recordings of speech as well as automatically generated transcripts of the recordings; specifically, our deep learning model achieves a weighted f-1 score of 0.78 solely from daily voice diaries. Our results surprisingly indicate that a simple periodic voice diary combined with deep learning is sufficient enough of a signal to assess complex psychiatric symptoms (e.g., auditory verbal hallucinations) collected from people in the wild as they go about their daily lives.\" https://doi.org/10.1145/3610890",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 129097
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129095
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129137
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 129135
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129133
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota"
            }
          ],
          "personId": 129131
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129129
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129125
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College"
            }
          ],
          "personId": 129124
        }
      ]
    },
    {
      "id": 128355,
      "typeId": 12924,
      "title": "MI-Poser: Human Body Pose Tracking Using Magnetic and Inertial Sensor Fusion with Metal Interference Mitigation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610891",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"Inside-out tracking of human body poses using wearable sensors holds significant potential for AR/VR applications, such as remote communication through 3D avatars with expressive body language. Current inside-out systems often rely on vision-based methods utilizing handheld controllers or incorporating densely distributed body-worn IMU sensors. The former limits hands-free and occlusion-robust interactions, while the latter is plagued by inadequate accuracy and jittering. We introduce a novel body tracking system, MI-Poser, which employs AR glasses and two wrist-worn electromagnetic field (EMF) sensors to achieve high-fidelity upper-body pose estimation while mitigating metal interference. Our lightweight system demonstrates a minimal error (6.6 cm mean joint position error) with real-world data collected from 10 participants. It remains robust against various upper-body movements and operates efficiently at 60 Hz. Furthermore, by incorporating an IMU sensor co-located with the EMF sensor, MI-Poser presents solutions to counteract the effects of metal interference, which inherently disrupts the EMF signal during tracking. Our evaluation effectively showcases the successful detection and correction of interference using our EMF-IMU fusion approach across environments with diverse metal profiles. Ultimately, MI-Poser offers a practical pose tracking system, particularly suited for body-centric AR applications.\" https://doi.org/10.1145/3610891",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129448
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Snap Research"
            }
          ],
          "personId": 129447
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Snap Research"
            }
          ],
          "personId": 129446
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129466
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Snap"
            }
          ],
          "personId": 129464
        }
      ]
    },
    {
      "id": 128356,
      "typeId": 12924,
      "title": "Midas: Generating mmWave Radar Data from Videos for Training Pervasive and Privacy-preserving Human Sensing Tasks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580872",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"Millimeter wave radar is a promising sensing modality for enabling pervasive and privacy-preserving human sensing. However, the lack of large-scale radar datasets limits the potential of training deep learning models to achieve generalization and robustness. To close this gap, we resort to designing a software pipeline that leverages wealthy video repositories to generate synthetic radar data, but it confronts key challenges including i) multipath reflection and attenuation of radar signals among multiple humans, ii) unconvertible generated data leading to poor generality for various applications, and iii) the class-imbalance issue of videos leading to low model stability. To this end, we design Midas to generate realistic, convertible radar data from videos via two components: (i) a data generation network (DG-Net) combines several key modules, depth prediction, human mesh fitting and multi-human reflection model, to simulate the multipath reflection and attenuation of radar signals to output convertible coarse radar data, followed by a Transformer model to generate realistic radar data; (ii) a variant Siamese network (VS-Net) selects key video clips to eliminate data redundancy for addressing the class-imbalance issue. We implement and evaluate Midas with video data from various external data sources and real-world radar data, demonstrating its great advantages over the state-of-the-art approach for both activity recognition and object detection tasks.\nhttps://dl.acm.org/doi/10.1145/3580872\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129225
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129221
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129220
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129219
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129217
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129215
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129213
        }
      ]
    },
    {
      "id": 128357,
      "typeId": 12924,
      "title": "LAUREATE: A Dataset for Supporting Research in Affective Computing and Human Memory Augmentation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610892",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "\"The latest developments in wearable sensors have resulted in a wide range of devices available to consumers, allowing users to monitor and improve their physical activity, sleep patterns, cognitive load, and stress levels. However, the lack of out-of-the-lab labelled data hinders the development of advanced machine learning models for predicting affective states. Furthermore, to the best of our knowledge, there are no publicly available datasets in the area of Human Memory Augmentation. This paper presents a dataset we collected during a 13-week study in a university setting. The dataset, named LAUREATE, contains the physiological data of 42 students during 26 classes (including exams), daily self-reports asking the students about their lifestyle habits (e.g. studying hours, physical activity, and sleep quality) and their performance across multiple examinations. In addition to the raw data, we provide expert features from the physiological data, and baseline machine learning models for estimating self-reported affect, models for recognising classes vs breaks, and models for user identification. Besides the use cases presented in this paper, among which Human Memory Augmentation, the dataset represents a rich resource for the UbiComp community in various domains, including affect recognition, behaviour modelling, user privacy, and activity and context recognition.\" https://doi.org/10.1145/3610892",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Lugano",
              "institution": "Faculty of Informatics, Università della Svizzera italiana (USI)"
            }
          ],
          "personId": 128843
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Lugano",
              "institution": "Faculty of Informatics, Università della Svizzera italiana (USI)"
            }
          ],
          "personId": 128870
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Lugano",
              "institution": "Faculty of Informatics, Università della Svizzera italiana (USI)"
            }
          ],
          "personId": 128867
        }
      ]
    },
    {
      "id": 128358,
      "typeId": 12924,
      "title": "TelecomTM: A Fine-Grained and Ubiquitous Traffic Monitoring System Using Pre-Existing Telecommunication Fiber-Optic Cables as Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596262",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "\"We introduce the TelecomTM system that uses pre-existing telecommunication fiber-optic cables as virtual strain sensors to sense vehicle-induced ground vibrations for fine-grained and ubiquitous traffic monitoring and characterization. Here we call it a virtual sensor because it is a software-based representation of a physical sensor. Due to the extensively installed telecommunication fiber-optic cables at the roadside, our system using redundant dark fibers enables to monitor traffic at low cost with low maintenance. Many existing traffic monitoring approaches use cameras, piezoelectric sensors, and smartphones, but they are limited due to privacy concerns and/or deployment requirements. Previous studies attempted to use telecommunication cables for traffic monitoring, but they were only exploratory and limited to simple tasks at a coarse granularity, e.g., vehicle detection, due to their hardware constraints and real-world challenges. In particular, those challenges are 1) unknown and heterogeneous properties of virtual sensors and 2) large and complex noise conditions. To this end, our TelecomTM system first characterizes the geographic location and analyzes the signal pattern of each virtual sensor through driving tests. We then develop a spatial-domain Bayesian filtering and smoothing algorithm to detect, track, and characterize each vehicle. Our approach uses the spatial dependency of multiple virtual sensors and Newton's laws of motion to combine the distributed sensor data to reduce uncertainties in vehicle detection and tracking. In our real-world evaluation on a two-way traffic road with 1120 virtual sensors, TelecomTM achieved 90.18% vehicle detection accuracy, 27x and 5x error reduction for vehicle position and speed tracking compared to a baseline method, and ±3.92% and ±11.98% percent error for vehicle wheelbase and weight estimation, respectively.\nhttps://doi.org/10.1145/3596262\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129165
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129163
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129208
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University"
            }
          ],
          "personId": 129204
        }
      ]
    },
    {
      "id": 128359,
      "typeId": 12924,
      "title": "Spectral-Loc: Indoor Localization Using Light Spectral Information",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580878",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"For indoor settings, we investigate the impact of location on the spectral distribution of the received light, i.e., the intensity of light for different wavelengths. Our investigations confirm that even under the same light source, different locations exhibit slightly different spectral distribution due to reflections from their localised environment containing different materials or colours. By exploiting this observation, we propose Spectral-Loc, a novel indoor localization system that uses light spectral information to identify the location of the device. With spectral sensors finding their way into the latest products and applications, such as white balancing in smartphone photography, Spectral-Loc can be readily deployed without requiring any additional hardware or infrastructure. We prototype Spectral-Loc using a commercial-off-the-shelf light spectral sensor, AS7265x, which can measure light intensity over 18 different wavelength sub-bands. We benchmark the localization accuracy of Spectral-Loc against the conventional light intensity sensors that provide only a single intensity value. Our evaluations over two different indoor spaces, a meeting room, and a large office space, demonstrate that the use of light spectral information significantly reduces the localization error for the different percentiles.\nhttps://dl.acm.org/doi/10.1145/3580878\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "UNSW"
            }
          ],
          "personId": 129509
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 129506
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "University of Cambridge"
            }
          ],
          "personId": 129503
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW"
            }
          ],
          "personId": 129501
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "UNSW"
            }
          ],
          "personId": 129525
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 129524
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "brisbane",
              "institution": "csiro"
            }
          ],
          "personId": 129523
        },
        {
          "affiliations": [
            {
              "country": "Egypt",
              "city": "Alexandria",
              "institution": "American University in Cairo and Alexandria University"
            }
          ],
          "personId": 128601
        }
      ]
    },
    {
      "id": 128360,
      "typeId": 12924,
      "title": "Synthetic Smartwatch IMU Data Generation from In-the-wild ASL Videos",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596261",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "\"The scarcity of training data available for IMUs in wearables poses a serious challenge for IMU-based American Sign Language (ASL) recognition. In this paper, we ask the following question: can we \"\"translate\"\" the large number of publicly available, in-the-wild ASL videos to their corresponding IMU data? We answer this question by presenting a video to IMU translation framework (Vi2IMU) that takes as input user videos and estimates the IMU acceleration and gyro from the perspective of user's wrist. Vi2IMU consists of two modules, a wrist orientation estimation module that accounts for wrist rotations by carefully incorporating hand joint positions, and an acceleration and gyro prediction module, that leverages the orientation for transformation while capturing the contributions of hand movements and shape to produce realistic wrist acceleration and gyro data. We evaluate Vi2IMU by translating publicly available ASL videos to their corresponding wrist IMU data and train a gesture recognition model purely using the translated data. Our results show that the model using translated data performs reasonably well compared to the same model trained using measured IMU data.\nhttps://dl.acm.org/doi/10.1145/3596261\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University"
            }
          ],
          "personId": 129173
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University"
            }
          ],
          "personId": 129172
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University"
            }
          ],
          "personId": 129171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University"
            }
          ],
          "personId": 129167
        }
      ]
    },
    {
      "id": 128361,
      "typeId": 12924,
      "title": "Exploring Smart Standing Desks to Foster a Healthier Workplace",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596260",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "\"Sedentary behavior is endemic in modern workplaces, contributing to negative physical and mental health outcomes. Although adjustable standing desks are increasing in popularity, people still avoid standing. We developed an open-source plug-and-play system to remotely control standing desks and investigated three system modes with a three-week in-the-wild user study (N=15). Interval mode forces users to stand once per hour, causing frustration. Adaptive mode nudges users to stand every hour unless the user has stood already. Smart mode, which raises the desk during breaks, was the best rated, contributing to increased standing time with the most positive qualitative feedback. However, non-computer activities need to be accounted for in the future. Therefore, our results indicate that a smart standing desk that shifts modes at opportune times has the most potential to reduce sedentary behavior in the workplace. We contribute our open-source system and insights for future intelligent workplace well-being systems.\nhttps://doi.org/10.1145/3596260\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128851
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "Northern Ostrobothnia",
              "city": "Oulu",
              "institution": "University of Oulu"
            }
          ],
          "personId": 128848
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128845
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128872
        }
      ]
    },
    {
      "id": 128362,
      "typeId": 12924,
      "title": "Quali-Mat: Evaluating the Quality of Execution in Body-Weight Exercises with a Pressure Sensitive Sports Mat",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3534610",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "While sports activity recognition is a well studied subject in mobile, wearable and ubiquitous computing, work to date mostly focuses on recognition and counting of specific exercise types. Quality assessment is a much more difficult problem with significantly less published results. In this work, we present Quali-Mat: a method for evaluating the quality of execution (QoE) in exercises using a smart sports mat that can measure the dynamic pressure profiles during full-body, body-weight exercises. As an example, our system not only recognizes that the user is doing push-ups, but also distinguishes 5 subtly different types of push-ups, each of which (according to sports science literature and professional trainers) has a different effect on different muscle groups. We have investigated various machine learning algorithms targeting the specific type of spatio-temporal data produced by the pressure mat system. We demonstrate that computationally efficient, yet effective Conv3D model outperforms more complex state-of-the-art options such as transfer learning from the image domain. The approach is validated through an experiment designed to cover 47 quantifiable variants of 9 basic exercises with 12 participants. Overall, the model can categorize 9 exercises with 98.6% accuracy / 98.6% F1 score, and 47 QoE variants with 67.3% accuracy / 68.1% F1 score. Through extensive discussions with both the experiment results and practical sports considerations, our approach can be used for not only precisely recognizing the type of exercises, but also quantifying the workout quality of execution on a fine time granularity. We also make the Quali-Mat data set available to the community to encourage further research in the area.\nhttps://doi.org/10.1145/3534610",
      "authors": [
        {
          "affiliations": [],
          "personId": 129259
        },
        {
          "affiliations": [],
          "personId": 129258
        },
        {
          "affiliations": [],
          "personId": 129257
        },
        {
          "affiliations": [],
          "personId": 129256
        },
        {
          "affiliations": [],
          "personId": 129255
        }
      ]
    },
    {
      "id": 128363,
      "typeId": 12924,
      "title": "Twin Meander Coil: Sensitive Readout of Battery-free On-body Wireless Sensors using Body-scale Meander Coils",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3494996",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "\"Energy-efficient and unconstrained wearable sensing platforms are essential for ubiquitous healthcare and activity monitoring applications. This paper presents Twin Meander Coil for wirelessly connecting battery-free on-body sensors to a textile-based reader knitted into clothing. This connection is based on passive inductive telemetry (PIT), wherein an external reader coil collects data from passive sensor coils via the magnetic field. In contrast to standard active sensing techniques, PIT does not require the reader to power up the sensors. Thus, the reader can be fabricated using a lossy conductive thread and industrial knitting machines. Furthermore, the sensors can superimpose information such as ID, touch, rotation, and pressure on its frequency response. However, conventional PIT technology needs a strong coupling between the reader and the sensor, requiring the reader to be small to the same extent as the sensors' size. Thus, applying this technology to body-scale sensing systems is challenging. To enable body-scale readout, Twin Meander Coil enhances the sensitivity of PIT technology by dividing the body-scale meander-shaped reader coils into two parts and integrating them so that they support the readout of each other. To demonstrate its feasibility, we built a prototype with a knitting machine, evaluated its sensing ability, and demonstrated several applications.\nhttps://dl.acm.org/doi/10.1145/3494996\"",
      "authors": [
        {
          "affiliations": [],
          "personId": 129246
        },
        {
          "affiliations": [],
          "personId": 129243
        },
        {
          "affiliations": [],
          "personId": 129241
        },
        {
          "affiliations": [],
          "personId": 129239
        },
        {
          "affiliations": [],
          "personId": 129237
        },
        {
          "affiliations": [],
          "personId": 129235
        }
      ]
    },
    {
      "id": 128364,
      "typeId": 12924,
      "title": "E3D: Harvesting Energy from Everyday Kinetic Interactions Using 3D Printed Attachment Mechanisms",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610897",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "\"The increase of distributed embedded systems has enabled pervasive sensing, actuation, and information displays across buildings and surrounding environments, yet also entreats huge cost expenditure for energy and human labor for maintenance. Our daily interactions, from opening a window to closing a drawer to twisting a doorknob, are great potential sources of energy but are often neglected. Existing commercial devices to harvest energy from these ambient sources are unaffordable, and DIY solutions are left with inaccessibility for non-experts preventing fully imbuing daily innovations in end-users. We present E3D, an end-to-end fabrication toolkit to customize self-powered smart devices at low cost. We contribute to a taxonomy of everyday kinetic activities that are potential sources of energy, a library of parametric mechanisms to harvest energy from manual operations of kinetic objects, and a holistic design system for end-user developers to capture design requirements by demonstrations then customize augmentation devices to harvest energy that meets unique lifestyle.\" https://doi.org/10.1145/3610897",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University"
            }
          ],
          "personId": 129470
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 129469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 129491
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University"
            }
          ],
          "personId": 129489
        }
      ]
    },
    {
      "id": 128365,
      "typeId": 12924,
      "title": "Abacus Gestures: A Large Set of Math-Based Usable Finger-Counting Gestures for Mid-Air Interactions",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610898",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"Designing an extensive set of mid-air gestures that are both easy to learn and perform quickly presents a significant challenge. Further complicating this challenge is achieving high-accuracy detection of such gestures using commonly available hardware, like a 2D commodity camera. Previous work often proposed smaller, application-specific gesture sets, requiring specialized hardware and struggling with adaptability across diverse environments. Addressing these limitations, this paper introduces Abacus Gestures, a comprehensive collection of 100 mid-air gestures. Drawing on the metaphor of Finger Abacus counting, gestures are formed from various combinations of open and closed fingers, each assigned different values. We developed an algorithm using an off-the-shelf computer vision library capable of detecting these gestures from a 2D commodity camera feed with an accuracy exceeding 98% for palms facing the camera and 95% for palms facing the body. We assessed the detection accuracy, ease of learning, and usability of these gestures in a user study involving 20 participants. The study found that participants could learn Abacus Gestures within five minutes after executing just 15 gestures and could recall them after a four-month interval. Additionally, most participants developed motor memory for these gestures after performing 100 gestures. Most of the gestures were easy to execute with the designated finger combinations, and the flexibility in executing the gestures using multiple finger combinations further enhanced the usability. Based on these findings, we created a taxonomy that categorizes Abacus Gestures into five groups based on motor memory development and three difficulty levels according to their ease of execution. Finally, we provided design guidelines and proposed potential use cases for Abacus Gestures in the realm of mid-air interaction\" https://doi.org/10.1145/3610898",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University"
            }
          ],
          "personId": 129087
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University"
            }
          ],
          "personId": 129085
        }
      ]
    },
    {
      "id": 128366,
      "typeId": 12924,
      "title": "CircuitGlue: A Software Configurable Converter for Interconnecting Multiple Heterogeneous Electronic Components",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596265",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"We present CircuitGlue, an electronic converter board that allows heterogeneous electronic components to be readily interconnected. Electronic components are plugged into an eight-pin programmable header on the board, and the assignment of each pin in the header is configured in software. CircuitGlue supports a variety of connections, including power, ground, analog signals, and various digital protocols at different voltages. As such, off-the-shelf electronic components and modules are instantly compatible no matter what voltage levels, interface types, communication protocols, and pinouts they use. In this paper, we demonstrate the use of CircuitGlue to ease and expedite prototyping with electronics and we explore new opportunities enabled by CircuitGlue. Finally, we reflect on the results of a preliminary user study evaluating the usability of CircuitGlue for people new to electronics.\nhttps://doi.org/10.1145/3596265\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "city": "Diepenbeek",
              "institution": "Flanders Make - Expertise Center for Digital Media"
            }
          ],
          "personId": 128684
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "city": "Hasselt",
              "institution": "Flanders Make - Expertise Centre for Digital Media"
            }
          ],
          "personId": 128682
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "Microsoft Research"
            }
          ],
          "personId": 128680
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Microsoft"
            }
          ],
          "personId": 128852
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Lancaster",
              "institution": "Lancaster University"
            }
          ],
          "personId": 128849
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Lancaster",
              "institution": "Lancaster University"
            }
          ],
          "personId": 128846
        }
      ]
    },
    {
      "id": 128367,
      "typeId": 12924,
      "title": "GlassMessaging: Towards Ubiquitous Messaging Using OHMDs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610931",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "https://doi.org/10.1145/3610931",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128736
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "Singapore University of Technology and Design"
            }
          ],
          "personId": 128735
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 128734
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128732
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128743
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "CNRS@CREATE"
            }
          ],
          "personId": 128742
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Chapel Hill",
              "institution": "University of North Carolina at Chapel Hill"
            }
          ],
          "personId": 128751
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128750
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "city": "Singapore",
              "institution": "National University of Singapore"
            }
          ],
          "personId": 128749
        }
      ]
    },
    {
      "id": 128368,
      "typeId": 12924,
      "title": "Investigating Passive Haptic Learning of Piano Songs Using Three Tactile Sensations of Vibration, Stroking and Tapping",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610899",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "\"Passive Haptic Learning (PHL) is a method by which users are able to learn motor skills without paying active attention. In past research, vibration is widely applied in PHL as the signal delivered on the participant's skin. The human somatosensory system provides not only discriminative input (the perception of pressure, vibration, slip, and texture, etc.) to the brain but also an affective input (sliding, tapping and stroking, etc.). The former is often described as being mediated by low-threshold mechanosensitive (LTM) units with rapidly conducting large myelinated (Aᵬ) afferents, while the latter is mediated by a class of LTM afferents called C-tactile afferents (CTs). We investigated whether different tactile sensations (tapping, light stroking, and vibration) influence the learning effect of PHL in this work. We built three wearable systems corresponding to the three sensations respectively. 17 participants were invited to learn to play three different note sequences passively via three different systems. The subjects were then tested on their remembered note sequences after each learning session. Our results indicate that the sensations of tapping or stroking are as effective as the vibration system in passive haptic learning of piano songs, providing viable alternatives to the vibration sensations that have been used so far. We also found that participants on average made up to 1.06 errors less when using affective inputs, namely tapping or stroking. As the first work exploring the differences in multiple types of tactile sensations in PHL, we offer our design to the readers and hope they may employ our works for further research of PHL.\" https://doi.org/10.1145/3610899",
      "authors": [
        {
          "affiliations": [],
          "personId": 129436
        },
        {
          "affiliations": [],
          "personId": 129433
        },
        {
          "affiliations": [],
          "personId": 129430
        },
        {
          "affiliations": [],
          "personId": 129427
        },
        {
          "affiliations": [],
          "personId": 129424
        },
        {
          "affiliations": [],
          "personId": 129450
        }
      ]
    },
    {
      "id": 128369,
      "typeId": 12924,
      "title": "Predicting Symptom Improvement During Depression Treatment Using Sleep Sensory Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610932",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"Depression is a serious mental illness. The current best guideline in depression treatment is closely monitoring patients and adjusting treatment as needed. Close monitoring of patients through physician-administered follow-ups or self-administered questionnaires, however, is difficult in clinical settings due to high cost, lack of trained professionals, and burden to the patients. Sensory data collected from mobile devices has been shown to provide a promising direction for long-term monitoring of depression symptoms. Most existing studies in this direction, however, focus on depression detection; the few studies that are on predicting changes in depression are not in clinical settings. In this paper, we investigate using one type of sensory data, sleep data, collected from wearables to predict improvement of depression symptoms over time after a patient initiates a new pharmacological treatment. We apply sleep trend filtering to noisy sleep sensory data to extract high-level sleep characteristics and develop a family of machine learning models that use simple sleep features (mean and variation of sleep duration) to predict symptom improvement. Our results show that using such simple sleep features can already lead to validation F1 score up to 0.68, indicating that using sensory data for predicting depression improvement during treatment is a promising direction.\" https://doi.org/10.1145/3610932",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128755
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128766
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Farmington",
              "institution": "UConn Health"
            }
          ],
          "personId": 128765
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128763
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Richmond",
              "institution": "University of Richmond"
            }
          ],
          "personId": 128779
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128777
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Farmington",
              "institution": "UConn Health"
            }
          ],
          "personId": 128775
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128773
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128771
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "Storrs",
              "institution": "University of Connecticut"
            }
          ],
          "personId": 128789
        }
      ]
    },
    {
      "id": 128370,
      "typeId": 12924,
      "title": "Single Packet, Single Channel, Switched Antenna Array for RF Localization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596263",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "Cost-effective and accurate means of localizing radio transmitters has the potential to enable a wide range of applications in the consumer electronics, IoT, and healthcare domains. However, existing multi-antenna localization methods require high-cost synchronized receivers, long integration times, and/or specialized packet structures. This paper proposes using a high-speed RF mux that sequentially connects antennas to a single 2MHz radio receiver and sub-packet switching to determine the Angle of Arrival of individual packets. Importantly, this approach does not need synchronization between the mux and the receiver, reducing cost and system complexity. Our signal processing pipeline recovers both switch timing and the antenna number from the received RF signal. The sub-packet waveforms are used to generate a synthetic reference packet, and our customized Multi-Resolution Beaming and MUSIC algorithms are used to determine the Angle of Arrival. Results show that our real-time system is highly accurate even when the target is moving, with a mean AoA accuracy of 3.4 degrees and a 2D localization accuracy of 36.4 cm. Furthermore, the system is capable of tracking multiple users carrying smartphones in either their hands or pockets. Ultimately this approach enables a single low-cost, low bandwidth commodity RF receiver to be used to create an N-element phased array receiver.\nhttps://dl.acm.org/doi/10.1145/3596263",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129385
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129384
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology"
            }
          ],
          "personId": 129383
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129381
        }
      ]
    },
    {
      "id": 128371,
      "typeId": 12924,
      "title": "A Meta-Synthesis of the Barriers and Facilitators for Personal Informatics Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610893",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "\"Personal informatics (PI) systems are designed for diverse users in the real world. Even when these systems are usable, people encounter barriers while engaging with them in ways designers cannot anticipate, which impacts the system's effectiveness. Although PI literature extensively reports such barriers, the volume of this information can be overwhelming. Researchers and practitioners often find themselves repeatedly addressing the same challenges since sifting through this enormous volume of knowledge looking for relevant insights is often infeasible. We contribute to alleviating this issue by conducting a meta-synthesis of the PI literature and categorizing people's barriers and facilitators to engagement with PI systems into eight themes. Based on the synthesized knowledge, we discuss specific generalizable barriers and paths for further investigations. This synthesis can serve as an index to identify barriers pertinent to each application domain and possibly to identify barriers from one domain that might apply to a different domain. Finally, to ensure the sustainability of the syntheses, we propose a Design Statements (DS) block for research articles.\" https://doi.org/10.1145/3610893",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah"
            }
          ],
          "personId": 129123
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Salt Lake City",
              "institution": "University of Utah"
            }
          ],
          "personId": 129121
        }
      ]
    },
    {
      "id": 128372,
      "typeId": 12924,
      "title": "VibPath: Two-Factor Authentication with Your Hand’s Vibration Response to Unlock Your Phone",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610894",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "\"Technical advances in the smart device market have fixated smartphones at the heart of our lives, warranting an ever more secure means of authentication. Although most smartphones have adopted biometrics-based authentication, after a couple of failed attempts, most users are given the option to quickly bypass the system with passcodes. To add a layer of security, two-factor authentication (2FA) has been implemented but has proven to be vulnerable to various attacks. In this paper, we introduce VibPath, a simultaneous 2FA scheme that can understand the user's hand neuromuscular system through touch behavior. VibPath captures the individual's vibration path responses between the hand and the wrist with the attention-based encoder-decoder network, authenticating the genuine users from the imposters unobtrusively. In a user study with 30 participants, VibPath achieved an average performance of 0.98 accuracy, 0.99 precision, 0.98 recall, 0.98 f1-score for user verification, and 94.3% accuracy for user identification across five passcodes. Furthermore, we also conducted several extensive studies, including in-the-wile, permanence, vulnerability, usability, and system overhead studies, to assess the practicability and viability of the VibPath from multiple aspects.\" https://doi.org/10.1145/3610894",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, State University of New York"
            }
          ],
          "personId": 128885
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, The State University of New York"
            }
          ],
          "personId": 128882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128879
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "BUFFALO",
              "institution": "Univerisity at Buffalo, the State University of New York"
            }
          ],
          "personId": 128876
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hunan",
              "city": "Changsha",
              "institution": "Hunan University"
            }
          ],
          "personId": 128873
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128899
        }
      ]
    },
    {
      "id": 128373,
      "typeId": 12924,
      "title": "BMAR: Barometric and Motion-based Alignment and Refinement for Offline Signal Synchronization across Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3596268",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "\"A requirement of cross-modal signal processing is accurate signal alignment. Though simple on a single device, accurate signal synchronization becomes challenging as soon as multiple devices are involved, such as during activity monitoring, health tracking, or motion capture---particularly outside controlled scenarios where data collection must be standalone, low-power, and support long runtimes. In this paper, we present BMAR, a novel synchronization method that operates purely based on recorded signals and is thus suitable for offline processing. BMAR needs no wireless communication between devices during runtime and does not require any specific user input, action, or behavior. BMAR operates on the data from devices worn by the same person that record barometric pressure and acceleration---inexpensive, low-power, and thus commonly included sensors in today's wearable devices. In its first stage, BMAR verifies that two recordings were acquired simultaneously and pre-aligns all data traces. In a second stage, BMAR refines the alignment using acceleration measurements while accounting for clock skew between devices. In our evaluation, three to five body-worn devices recorded signals from the wearer for up to ten hours during a series of activities. BMAR synchronized all signal recordings with a median error of 33.4 ms and reliably rejected non-overlapping signal traces. The worst-case activity was sleeping, where BMAR's second stage could not exploit motion for refinement and, thus, aligned traces with a median error of 3.06 s.\nhttps://doi.org/10.1145/3596268\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Zürich",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 129066
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "Zurich",
              "institution": "ETH Zürich"
            }
          ],
          "personId": 129064
        }
      ]
    },
    {
      "id": 128374,
      "typeId": 12924,
      "title": "PoseSonic: 3D Upper Body Pose Estimation Through Egocentric Acoustic Sensing on Smartglasses",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610895",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "abstract": "\"In this paper, we introduce PoseSonic, an intelligent acoustic sensing solution for smartglasses that estimates upper body poses. Our system only requires two pairs of microphones and speakers on the hinges of the eyeglasses to emit FMCW-encoded inaudible acoustic signals and receive reflected signals for body pose estimation. Using a customized deep learning model, PoseSonic estimates the 3D positions of 9 body joints including the shoulders, elbows, wrists, hips, and nose. We adopt a cross-modal supervision strategy to train our model using synchronized RGB video frames as ground truth. We conducted in-lab and semi-in-the-wild user studies with 22 participants to evaluate PoseSonic, and our user-independent model achieved a mean per joint position error of 6.17 cm in the lab setting and 14.12 cm in semi-in-the-wild setting when predicting the 9 body joint positions in 3D. Our further studies show that the performance was not significantly impacted by different surroundings or when the devices were remounted or by real-world environmental noise. Finally, we discuss the opportunities, challenges, and limitations of deploying PoseSonic in real-world applications.\" https://doi.org/10.1145/3610895",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129507
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129504
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129502
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129526
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129497
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 129495
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University"
            }
          ],
          "personId": 128602
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITHACA",
              "institution": "Cornell University"
            }
          ],
          "personId": 128599
        }
      ]
    },
    {
      "id": 128375,
      "typeId": 12924,
      "title": "TAO: Context Detection from Daily Activity Patterns Using Temporal Analysis and Ontology",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610896",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "abstract": "\"Translating fine-grained activity detection (e.g., phone ring, talking interspersed with silence and walking) into semantically meaningful and richer contextual information (e.g., on a phone call for 20 minutes while exercising) is essential towards enabling a range of healthcare and human-computer interaction applications. Prior work has proposed building ontologies or temporal analysis of activity patterns with limited success in capturing complex real-world context patterns. We present TAO, a hybrid system that leverages OWL-based ontologies and temporal clustering approaches to detect high-level contexts from human activities. TAO can characterize sequential activities that happen one after the other and activities that are interleaved or occur in parallel to detect a richer set of contexts more accurately than prior work. We evaluate TAO on real-world activity datasets (Casas and Extrasensory) and show that our system achieves, on average, 87% and 80% accuracy for context detection, respectively. We deploy and evaluate TAO in a real-world setting with eight participants using our system for three hours each, demonstrating TAO's ability to capture semantically meaningful contexts in the real world. Finally, to showcase the usefulness of contexts, we prototype wellness applications that assess productivity and stress and show that the wellness metrics calculated using contexts provided by TAO are much closer to the ground truth (on average within 1.1%), as compared to the baseline approach (on average within 30%).\" https://doi.org/10.1145/3610896",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129149
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129147
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129145
        }
      ]
    },
    {
      "id": 128377,
      "typeId": 12924,
      "title": "Mood Measurement on Smartphones: Which Measure, Which Design?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580864",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "Mood, often studied using smartphones, influences human perception, judgment, thought, and behavior. Mood measurements on smartphones face challenges concerning the selection of a proper mood measure and its transfer, or translation, into a digital application (app) that is user-engaging. Addressing these challenges, researchers sometimes end up developing a new interaction design and modifying the classic mood measure for an app. However, the extent to which such design alterations can impact user compliance, user experience, and the accuracy of mood measurements throughout a mood self-tracking study is unclear. In this paper, we explore and investigate how the selection of a mood measure (from two widely used measures) and its design alteration (from three options of classic, chatbot, and interactive designs) impact the (i) validity, (ii) user compliance, and (iii) user experience of mood measurement apps. For this purpose, we conducted a hybrid study with a mixed design in three parts. The first part suggests that a measure's validity can be susceptible to design modifications and introduces the concept of measure's resilience which can be essential when modifying the interaction design of a measurement tool. The second part discovers that both the type and design of the chosen measure can impact user compliance. This part also portrays a more complete picture of user compliance by demonstrating the use of several variables to investigate compliance. This investigation reveals that user compliance is not just about the response duration or length of a measurement tool. The final part finds that a measure or its design does not significantly influence the user experience for a well-designed app. In this part, we also discover which user experience criteria are more impactful for improving user compliance when designing mood tracking (or mood self-tracking) tools. Our results further suggest that, for a resilient measure, the interactive design is more likely to attract users and have higher user compliance and satisfaction as a whole. Ultimately, choosing a measure or design alternative would be a three-way trade-off between the measure's validity (or accuracy), user compliance, and user satisfaction, which researchers have to prioritize. A successful mood measurement with a smartphone needs to balance both concepts of app quality and assessment quality.\nhttps://dl.acm.org/doi/10.1145/3580864",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "city": "Delft",
              "institution": "Delft University of Technology"
            }
          ],
          "personId": 128664
        }
      ]
    },
    {
      "id": 128378,
      "typeId": 12924,
      "title": "Mites: Design and Deployment of a General-Purpose Sensing Infrastructure for Buildings",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580865",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "abstract": "\"There is increasing interest in deploying building-scale, general-purpose, and high-fidelity sensing to drive emerging smart building applications. However, the real-world deployment of such systems is challenging due to the lack of system and architectural support. Most existing sensing systems are purpose-built, consisting of hardware that senses a limited set of environmental facets, typically at low fidelity and for short-term deployment. Furthermore, prior systems with high-fidelity sensing and machine learning fail to scale effectively and have fewer primitives, if any, for privacy and security. For these reasons, IoT deployments in buildings are generally short-lived or done as a proof of concept. We present the design of Mites, a scalable end-to-end hardware-software system for supporting and managing distributed general-purpose sensors in buildings. Our design includes robust primitives for privacy and security, essential features for scalable data management, as well as machine learning to support diverse applications in buildings. We deployed our Mites system and 314 Mites devices in Tata Consultancy Services (TCS) Hall at Carnegie Mellon University (CMU), a fully occupied, five-story university building. We present a set of comprehensive evaluations of our system using a series of microbenchmarks and end-to-end evaluations to show how we achieved our stated design goals. We include five proof-of-concept applications to demonstrate the extensibility of the Mites system to support compelling IoT applications. Finally, we discuss the real-world challenges we faced and the lessons we learned over the five-year journey of our stack's iterative design, development, and deployment.\nhttps://dl.acm.org/doi/10.1145/3580865\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "La Jolla",
              "institution": "University of California San Diego"
            }
          ],
          "personId": 129169
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129168
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129166
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 129164
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129162
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 129207
        }
      ]
    },
    {
      "id": 128379,
      "typeId": 12924,
      "title": "BabyNutri: A Cost-Effective Baby Food Macronutrients Analyzer Based on Spectral Reconstruction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580858",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "\"The physical and physiological development of infants and toddlers requires the proper amount of macronutrient intake, making it an essential problem to estimate the macronutrient in baby food. Nevertheless, existing solutions are either too expensive or poor performing, preventing the widespread use of automatic baby nutrient intake logging. To narrow this gap, this paper proposes a cost-effective and portable baby food macronutrient estimation system, BabyNutri. BabyNutri exploits a novel spectral reconstruction algorithm to reconstruct high-dimensional informative spectra from low-dimensional spectra, which are available from low-cost spectrometers. We propose a denoising autoencoder for the reconstruction process, by which BabyNutri can reconstruct a 160-dimensional spectrum from a 5-dimensional spectrum. Since the high-dimensional spectrum is rich in light absorption features of macronutrients, it can achieve more accurate macronutrient estimation. In addition, considering that baby food contains complex ingredients, we also design a CNN nutrition estimation model with good generalization performance over various types of baby food. Our extensive experiments over 88 types of baby food show that the spectral reconstruction error of BabyNutri is only 5.91%, reducing 33% than the state-of-the-art baseline with the same time complexity. In addition, the nutrient estimation performance of BabyNutri not only obviously outperforms state-of-the-art and cost-effective solutions but also is highly correlated with the professional spectrometer, with the correlation coefficients of 0.81, 0.88, 0.82 for protein, fat, and carbohydrate, respectively. However the price of our system is only one percent of the commercial solution. We also validate that BabyNutri is robust regarding various factors, e.g., ambient light, food volume, and even unseen baby food samples.\nhttps://dl.acm.org/doi/10.1145/3580858\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology"
            }
          ],
          "personId": 128582
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128605
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 128604
        }
      ]
    },
    {
      "id": 128380,
      "typeId": 12924,
      "title": "NF-Heart: A Near-field Non-contact Continuous User Authentication System via Ballistocardiogram",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580851",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "abstract": "The increasingly remote workforce resulting from the global coronavirus pandemic has caused unprecedented cybersecurity concerns to organizations. Considerable evidence has shown that one-pass authentication fails to meet security needs when the workforce work from home. The recent advent of continuous authentication (CA) has shown the potential to solve this predicament. In this paper, we propose NF-Heart, a physiological-based CA system utilizing a ballistocardiogram (BCG). The key insight is that the BCG measures the body's micro-movements produced by the recoil force of the body in reaction to the cardiac ejection of blood, and we can infer cardiac biometrics from BCG signals. To measure BCG, we deploy a lightweight accelerometer on an office chair, turning the common chair into a smart continuous identity \"scanner\". We design multiple stages of signal processing to decompose and transform the distorted BCG signals so that the effects of motion artifacts and dynamic variations are eliminated. User-specific fiducial features are then extracted from the processed BCG signals for authentication. We conduct comprehensive experiments on 105 subjects in terms of verification accuracy, security, robustness, and long-term availability. The results demonstrate that NF-Heart achieves a mean balanced accuracy of 96.45% and a median equal error rate of 3.83% for CA. The proposed signal processing pipeline is effective in addressing various practical disturbances.\nhttps://dl.acm.org/doi/10.1145/3580851",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 129158
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "HKUST (Guangzhou)"
            }
          ],
          "personId": 129156
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Shenzhen University"
            }
          ],
          "personId": 129154
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 129153
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "HKUST (Guangzhou)"
            }
          ],
          "personId": 129152
        }
      ]
    },
    {
      "id": 128381,
      "typeId": 12924,
      "title": "FeverPhone: Accessible Core-Body Temperature Sensing for Fever Monitoring Using Commodity Smartphones",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580850",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "Smartphones contain thermistors that ordinarily monitor the temperature of the device's internal components; however, these sensors are also sensitive to warm entities in contact with the device, presenting opportunities for measuring human body temperature and detecting fevers. We present FeverPhone --- a smartphone app that estimates a person's core body temperature by having the user place the capacitive touchscreen of the phone against their forehead. During the assessment, the phone logs the temperature sensed by a thermistor and the raw capacitance sensed by the touchscreen to capture features describing the rate of heat transfer from the body to the device. These features are then used in a machine learning model to infer the user's core body temperature. We validate FeverPhone through both a lab simulation with a skin-like controllable heat source and a clinical study with real patients. We found that FeverPhone's temperature estimates are comparable to commercial off-the-shelf peripheral and tympanic thermometers. In a clinical study with 37 participants, FeverPhone readings achieved a mean absolute error of 0.229 °C, a limit of agreement of ±0.731 °C, and a Pearson's correlation coefficient of 0.763. Using these results for fever classification results in a sensitivity of 0.813 and a specificity of 0.904.\nhttps://dl.acm.org/doi/10.1145/3580850",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128606
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington School of Medicine"
            }
          ],
          "personId": 128648
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 128646
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128644
        }
      ]
    },
    {
      "id": 128382,
      "typeId": 12924,
      "title": "GlucoScreen: A Smartphone-based Readerless Glucose Test Strip for Prediabetes Screening",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580855",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "Blood glucose measurement is commonly used to screen for and monitor diabetes, a chronic condition characterized by the inability to effectively modulate blood glucose that can lead to heart disease, vision loss, and kidney failure. Early detection of prediabetes can forestall or reverse more serious illness if healthy lifestyle adjustments or medical interventions are made in a timely manner. Current diabetes screening methods require visits to a healthcare facility and use of over-the-counter glucose-testing devices (glucometers), both of which are costly or inaccessible for many populations, reducing the chances of early disease detection. We therefore developed GlucoScreen, a readerless glucose test strip that enables affordable, single-use, at-home glucose testing, leveraging the user's touchscreen cellphone for reading and displaying results. By integrating minimal, low-cost electronics with commercially available blood glucose testing strips, the GlucoScreen prototype introduces a new type of low-cost, battery-free glucose testing tool that works with any smartphone, obviating the need to purchase a separate dedicated reader. Our key innovation is using the phone's capacitive touchscreen for the readout of the minimally modified commercially available glucose test strips. In an in vitro evaluation with artificial glucose solutions, we tested GlucoScreen with five different phones and compared the findings to two common glucometers, AccuChek and True Metrix. The mean absolute error (MAE) for our GlucoScreen prototype was 4.52 mg/dl (Accu-Chek test strips) and 3.7 mg/dl (True Metrix test strips), compared to 4.98 mg/dl and 5.44 mg/dl for the AccuChek glucometer and True Metrix glucometer, respectively. In a clinical investigation with 75 patients, GlucoScreen had a MAE of 10.47 mg/dl, while the AccuChek glucometer had a 9.88 mg/dl MAE. These results indicate that GlucoScreen's performance is comparable to that of commonly available over-the-counter blood glucose testing devices. With further development and validation, GlucoScreen has the potential to facilitate large-scale and lower cost diabetes screening. This work employs GlucoScreen's smartphone-based technology for glucose testing, but it could be extended to build other readerless electrochemical assays in the future.\nhttps://dl.acm.org/doi/10.1145/3580855",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128598
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128595
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128592
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128589
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128586
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 128584
        }
      ]
    },
    {
      "id": 128383,
      "typeId": 12924,
      "title": "FocalPoint: Adaptive Direct Manipulation for Selecting Small 3D Virtual Objects",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580856",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "We propose FocalPoint, a direct manipulation technique in smartphone augmented reality (AR) for selecting small densely-packed objects within reach, a fundamental yet challenging task in AR due to the required accuracy and precision. FocalPoint adaptively and continuously updates a cylindrical geometry for selection disambiguation based on the user's selection history and hand movements. This design is informed by a preliminary study which revealed that participants preferred selecting objects appearing in particular regions of the screen. We evaluate FocalPoint against a baseline direct manipulation technique in a 12-participant study with two tasks: selecting a 3 mm wide target from a pile of cubes and virtually decorating a house with LEGO pieces. FocalPoint was three times as accurate for selecting the correct object and 5.5 seconds faster on average; participants using FocalPoint decorated their houses more and were more satisfied with the result. We further demonstrate the finer control enabled by FocalPoint in example applications of robot repair, 3D modeling, and neural network visualizations.\nhttps://dl.acm.org/doi/10.1145/3580856",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 128662
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University"
            }
          ],
          "personId": 128661
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University"
            }
          ],
          "personId": 128660
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Rhode Island",
              "city": "Providence",
              "institution": "Brown University"
            }
          ],
          "personId": 128656
        }
      ]
    },
    {
      "id": 128384,
      "typeId": 12924,
      "title": "Wet-Ra: Monitoring DiapersWetness with Wireless Signals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3534599",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "\"Diaper wetness monitoring is essential in various situations (e.g., babies and patients) to guarantee hygiene and avoid embarrassment. Existing diaper wetness monitoring methods include indicator lines, special sensors, and RFID, which require modifications on every diaper piece and cannot be easily checked under visual occlusions (e.g., trousers). In this paper, we introduce Wet-Ra, a contactless, ubiquitous, and user-friendly diaper wetness monitoring system based on RF signals. To extract informative features for wetness detection from RF signals, we construct Continuous-Radio-Snapshot and build corresponding signal representations that capture the distinct patterns of diapers of different wetness levels. We refine the signal representation by eliminating multi-path interference from the environment and mitigating the smearing effect with wavelet multisynchrosqueezing transform. To expand the usability of Wet-Ra, we build a transferable model that yields robust detection results in diversified environments and for new users. We conduct extensive experiments to evaluate Wet-Ra with 47 volunteers in 7 different rooms with three off-the-shelf diaper brands. Experiment results confirm that Wet-Ra can accurately identify diaper wetness in the real environment.\nhttps://dl.acm.org/doi/10.1145/3534599\"",
      "authors": [
        {
          "affiliations": [],
          "personId": 129233
        },
        {
          "affiliations": [],
          "personId": 129264
        },
        {
          "affiliations": [],
          "personId": 129262
        },
        {
          "affiliations": [],
          "personId": 129261
        },
        {
          "affiliations": [],
          "personId": 129260
        }
      ]
    },
    {
      "id": 128385,
      "typeId": 12924,
      "title": "Detecting Social Contexts from Mobile Sensing Indicators in Virtual Interactions with Socially Anxious Individuals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610916",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "\"Mobile sensing is a ubiquitous and useful tool to make inferences about individuals' mental health based on physiology and behavior patterns. Along with sensing features directly associated with mental health, it can be valuable to detect different features of social contexts to learn about social interaction patterns over time and across different environments. This can provide insight into diverse communities' academic, work and social lives, and their social networks. We posit that passively detecting social contexts can be particularly useful for social anxiety research, as it may ultimately help identify changes in social anxiety status and patterns of social avoidance and withdrawal. To this end, we recruited a sample of highly socially anxious undergraduate students (N=46) to examine whether we could detect the presence of experimentally manipulated virtual social contexts via wristband sensors. Using a multitask machine learning pipeline, we leveraged passively sensed biobehavioral streams to detect contexts relevant to social anxiety, including (1) whether people were in a social situation, (2) size of the social group, (3) degree of social evaluation, and (4) phase of social situation (anticipating, actively experiencing, or had just participated in an experience). Results demonstrated the feasibility of detecting most virtual social contexts, with stronger predictive accuracy when detecting whether individuals were in a social situation or not and the phase of the situation, and weaker predictive accuracy when detecting the level of social evaluation. They also indicated that sensing streams are differentially important to prediction based on the context being predicted. Our findings also provide useful information regarding design elements relevant to passive context detection, including optimal sensing duration, the utility of different sensing modalities, and the need for personalization. We discuss implications of these findings for future work on context detection (e.g., just-in-time adaptive intervention development).\" https://doi.org/10.1145/3610916",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "Department of Systems and Information Engineering, University of Virginia"
            }
          ],
          "personId": 128913
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 128911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 128909
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 128907
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 128905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville, VA",
              "institution": "Department of Systems and Information Engineering, University of Virginia"
            }
          ],
          "personId": 128903
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Titusville",
              "institution": "Janssen Pharmaceutical Companies of Johnson & Johnson"
            }
          ],
          "personId": 128921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charolottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 128919
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "Department of Systems and Information Engineering, University of Virginia"
            }
          ],
          "personId": 128918
        }
      ]
    },
    {
      "id": 128386,
      "typeId": 12924,
      "title": "AttFL: A Personalized Federated Learning Framework for Time-series Mobile and Embedded Sensor Data Processing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610917",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "\"This work presents AttFL, a federated learning framework designed to continuously improve a personalized deep neural network for efficiently analyzing time-series data generated from mobile and embedded sensing applications. To better characterize time-series data features and efficiently abstract model parameters, AttFL appends a set of attention modules to the baseline deep learning model and exchanges their feature map information to gather collective knowledge across distributed local devices at the server. The server groups devices with similar contextual goals using cosine similarity, and redistributes updated model parameters for improved inference performance at each local device. Specifically, unlike previously proposed federated learning frameworks, AttFL is designed specifically to perform well for various recurrent neural network (RNN) baseline models, making it suitable for many mobile and embedded sensing applications producing time-series sensing data. We evaluate the performance of AttFL and compare with five state-of-the-art federated learning frameworks using three popular mobile/embedded sensing applications (e.g., physiological signal analysis, human activity recognition, and audio processing). Our results obtained from CPU core-based emulations and a 12-node embedded platform testbed shows that AttFL outperforms all alternative approaches in terms of model accuracy and communication/computational overhead, and is flexible enough to be applied in various application scenarios exploiting different baseline deep learning model architectures.\" https://doi.org/10.1145/3610917",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Yonsei University"
            }
          ],
          "personId": 129032
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Yonsei University"
            }
          ],
          "personId": 129047
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Yonsei University"
            }
          ],
          "personId": 129028
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Ohio",
              "city": "Columbus",
              "institution": "Ohio State University"
            }
          ],
          "personId": 129026
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Seoul",
              "institution": "Yonsei Univeristy"
            }
          ],
          "personId": 129068
        }
      ]
    },
    {
      "id": 128387,
      "typeId": 12924,
      "title": "FewShotBP: Towards Personalized Ubiquitous Continuous Blood Pressure Measurement",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610918",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "\"Deep learning-based methods demonstrate promising results in continuous non-invasive blood pressure measurement, whereas those models trained on large public datasets suffer from severe performance degradation in predicting from real-world user data collected in home settings. Transfer learning has been recently introduced to personalize the pre-trained model with unseen users' data to solve the problem. However, the existing methods based on network fine-tuning for model personalization require a large amount of labeled data, lacking practicality due to labeling using a cuff-based blood pressure monitor is extremely tedious and laborious for home users. In this paper, we propose a novel few-shot transfer learning approach named FewShotBP, which addresses the above-mentioned challenges by introducing a personalization adapter at the personalization stage (i.e., the transfer learning stage), and a multi-modal spectro-temporal neural network at the pre-train stage, to bridge the gap between data-hungry models and limited labeled data in realistic scenarios. To evaluate the approach's significance, we conducted experiments using both a publicly available dataset and a real-world user experiment. The results demonstrated that the proposed approach achieves similar accuracy of blood pressure prediction with 10× less data for personalization compared with the state-of-the-art method in the public dataset and achieves a mean absolute error of 6.68 mmHg (systolic blood pressure) and 3.91 mmHg (diastolic blood pressure) with only 10 personal data samples in the real-world user experiment.\" https://doi.org/10.1145/3610918",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Computing Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 128634
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Computing Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 128633
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Computing Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 128631
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Institute of Computing Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 128629
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Institute of Computing Technology, Chinese Academy of Sciences"
            }
          ],
          "personId": 128671
        }
      ]
    },
    {
      "id": 128388,
      "typeId": 12924,
      "title": "Diagnosing Medical Score Calculator Apps",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610912",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "\"Mobile medical score calculator apps are widely used among practitioners to help make decisions regarding patient treatment and diagnosis. Errors in score definition, input, or calculations can result in severe and potentially life-threatening situations. Despite these high stakes, there has been no systematic or rigorous effort to examine and verify score calculator apps. We address these issues via a novel, interval-based score checking approach. Based on our observation that medical reference tables themselves may contain errors (which can propagate to apps) we first introduce automated correctness checking of reference tables. Specifically, we reduce score correctness checking to partition checking (coverage and non-overlap) over score parameters' ranges. We checked 12 scoring systems used in emergency, intensive, and acute care. Surprisingly, though some of these scores have been used for decades, we found errors in 5 score specifications: 8 coverage violations and 3 non-overlap violations. Second, we design and implement an automatic, dynamic analysis-based approach for verifying score correctness in a given Android app; the approach combines efficient, automatic GUI extraction and app exploration with partition/consistency checking to expose app errors. We applied the approach to 90 Android apps that implement medical score calculators. We found 23 coverage violations in 11 apps; 32 non-overlap violations in 12 apps, and 16 incorrect score calculations in 16 apps. We reported all findings to developers, which so far has led to fixes in 6 apps.\" https://doi.org/10.1145/3610912",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Newark",
              "institution": "New Jersey Institute Of Technology"
            }
          ],
          "personId": 129493
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Newark",
              "institution": "New Jersey Institute of Technology"
            }
          ],
          "personId": 129473
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Newark",
              "institution": "New Jersey Institute of Technology"
            }
          ],
          "personId": 129521
        }
      ]
    },
    {
      "id": 128389,
      "typeId": 12924,
      "title": "MR Object Identification and Interaction: Fusing Object Situation Information from Heterogeneous Sources",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610879",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"The increasing number of objects in ubiquitous computing environments creates a need for effective object detection and identification mechanisms that permit users to intuitively initiate interactions with these objects. While multiple approaches to such object detection -- including through visual object detection, fiducial markers, relative localization, or absolute spatial referencing -- are available, each of these suffers from drawbacks that limit their applicability. In this paper, we propose ODIF, an architecture that permits the fusion of object situation information from such heterogeneous sources and that remains vertically and horizontally modular to allow extending and upgrading systems that are constructed accordingly. We furthermore present BLEARVIS, a prototype system that builds on the proposed architecture and integrates computer-vision (CV) based object detection with radio-frequency (RF) angle of arrival (AoA) estimation to identify BLE-tagged objects. In our system, the front camera of a Mixed Reality (MR) head-mounted display (HMD) provides a live image stream to a vision-based object detection module, while an antenna array that is mounted on the HMD collects AoA information from ambient devices. In this way, BLEARVIS is able to differentiate between visually identical objects in the same environment and can provide an MR overlay of information (data and controls) that relates to them. We include experimental evaluations of both, the CV-based object detection and the RF-based AoA estimation, and discuss the applicability of the combined RF and CV pipelines in different ubiquitous computing scenarios. This research can form a starting point to spawn the integration of diverse object detection, identification, and interaction approaches that function across the electromagnetic spectrum, and beyond.\" https://doi.org/10.1145/3610879",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "St.Gallen",
              "institution": "University of St.Gallen"
            }
          ],
          "personId": 129304
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "TN",
              "city": "Trento",
              "institution": "University of Trento"
            }
          ],
          "personId": 129303
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "city": "Trento",
              "institution": "University of Trento"
            }
          ],
          "personId": 129302
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "St. Gallen",
              "city": "St.Gallen",
              "institution": "University of St.Gallen"
            }
          ],
          "personId": 129301
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "St. Gallen",
              "institution": "Institute of Computer Science"
            }
          ],
          "personId": 129310
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Braunschweig",
              "institution": "TU Braunschweig"
            }
          ],
          "personId": 129309
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "city": "St. Gallen",
              "institution": "University of St. Gallen"
            }
          ],
          "personId": 129308
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "Trento",
              "city": "Trento",
              "institution": "University of Trento"
            }
          ],
          "personId": 129307
        }
      ]
    },
    {
      "id": 128390,
      "typeId": 12924,
      "title": "Iris: Passive Visible Light Positioning Using Light Spectral Information",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610913",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "\"We propose a novel Visible Light Positioning (VLP) method, called Iris, that leverages light spectral information (LSI) to localize individuals in a completely passive manner. This means that the user does not need to carry any device, and the existing lighting infrastructure remains unchanged. Our method uses a background subtraction approach to accurately detect changes in ambient LSI caused by human movement. Furthermore, we design a Convolutional Neural Network (CNN) capable of learning and predicting user locations from the LSI change data.\n\nTo validate our approach, we implemented a prototype of Iris using a commercial-off-the-shelf light spectral sensor and conducted experiments in two typical real-world indoor environments: a 25 m2 one-bedroom apartment and a 13.3m × 8.4m office space. Our results demonstrate that Iris performs effectively in both artificial lighting at night and in highly dynamic natural lighting conditions during the day. Moreover, Iris outperforms the state-of-the-art passive VLP techniques significantly in terms of localization accuracy and the required density of light sensors.\n\nTo reduce the overhead associated with multi-channel spectral sensing, we develop and validate an algorithm that can minimize the required number of spectral channels for a given environment. Finally, we propose a conditional Generative Adversarial Network (cGAN) that can artificially generate LSI and reduce data collection effort by 50% without sacrificing localization accuracy.\"https://doi.org/10.1145/3610913",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 129397
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "UNSW"
            }
          ],
          "personId": 129394
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "University of Cambridge"
            }
          ],
          "personId": 129420
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW"
            }
          ],
          "personId": 129418
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "UNSW"
            }
          ],
          "personId": 129416
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "brisbane",
              "institution": "csiro"
            }
          ],
          "personId": 129415
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 129414
        },
        {
          "affiliations": [
            {
              "country": "Egypt",
              "city": "Alexandria",
              "institution": "American University in Cairo and Alexandria University"
            }
          ],
          "personId": 129439
        }
      ]
    },
    {
      "id": 128391,
      "typeId": 12924,
      "title": "UQRCom: Underwater Wireless Communication Based on QR Code",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3571588",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "\"While communication in the air has been a norm with the pervasiveness of WiFi and LTE infrastructure, underwater communication still faces a lot of challenges. Even nowadays, the main communication method for divers in underwater environment is hand gesture. There are multiple issues associated with gesture-based communication including limited amount of information and ambiguity. On the other hand, traditional RF-based wireless communication technologies which have achieved great success in the air can hardly work in underwater environment due to the extremely severe attenuation. In this paper, we propose UQRCom, an underwater wireless communication system designed for divers. We design a UQR code which stems from QR code and address the unique challenges in underwater environment such as color cast, contrast reduction and light interfere. With both real-world experiments and simulation, we show that the proposed system can achieve robust real-time communication in underwater environment. For UQR codes with a size of 19.8 cm x 19.8 cm, the communication distance can be 11.2 m and the achieved data rate (6.9 kbps ~ 13.6 kbps) is high enough for voice communication between divers.\nhttps://dl.acm.org/doi/10.1145/3571588\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129019
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129017
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst"
            }
          ],
          "personId": 129015
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129013
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129011
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129009
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Dalian",
              "institution": "Dalian University of Technology"
            }
          ],
          "personId": 129007
        }
      ]
    },
    {
      "id": 128392,
      "typeId": 12924,
      "title": "Uncovering Bias in Personal Informatics",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610914",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "abstract": "\"Personal informatics (PI) systems, powered by smartphones and wearables, enable people to lead healthier lifestyles by providing meaningful and actionable insights that break down barriers between users and their health information. Today, such systems are used by billions of users for monitoring not only physical activity and sleep but also vital signs and women's and heart health, among others. Despite their widespread usage, the processing of sensitive PI data may suffer from biases, which may entail practical and ethical implications. In this work, we present the first comprehensive empirical and analytical study of bias in PI systems, including biases in raw data and in the entire machine learning life cycle. We use the most detailed framework to date for exploring the different sources of bias and find that biases exist both in the data generation and the model learning and implementation streams. According to our results, the most affected minority groups are users with health issues, such as diabetes, joint issues, and hypertension, and female users, whose data biases are propagated or even amplified by learning models, while intersectional biases can also be observed.\" https://doi.org/10.1145/3610914",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "city": "Thessaloniki",
              "institution": "Aristotle University of Thessaloniki"
            }
          ],
          "personId": 128622
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "city": "Thessaloniki",
              "institution": "Aristotle University of Thessaloniki"
            }
          ],
          "personId": 128620
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "city": "Thessaloniki",
              "institution": "Aristotle University of Thessaloniki"
            }
          ],
          "personId": 128618
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "Northeastern University"
            }
          ],
          "personId": 128616
        }
      ]
    },
    {
      "id": 128393,
      "typeId": 12924,
      "title": "Society’s Attitudes Towards Human Augmentation and Performance Enhancement Technologies (SHAPE) Scale",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610915",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Human augmentation technologies (ATs) are a subset of ubiquitous on-body devices designed to improve cognitive, sensory, and motor capacities. Although there is a large corpus of knowledge concerning ATs, less is known about societal attitudes towards them and how they shift over time. To that end, we developed The Society's Attitudes Towards Human Augmentation and Performance Enhancement Technologies (SHAPE) Scale, which measures how users of ATs are perceived. To develop the scale, we first created a list of possible scale items based on past work on how people respond to new technologies. The items were then reviewed by experts. Next, we performed exploratory factor analysis to reduce the scale to its final length of thirteen items. Subsequently, we confirmed test-retest validity of our instrument, as well as its construct validity. The SHAPE scale enables researchers and practitioners to understand elements contributing to attitudes toward augmentation technology users. The SHAPE scale assists designers of ATs in designing artifacts that will be more universally accepted.\" https://doi.org/10.1145/3610915",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128897
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "city": "Oslo",
              "institution": "University of Oslo"
            }
          ],
          "personId": 128895
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Munich",
              "institution": "LMU Munich"
            }
          ],
          "personId": 128894
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "city": "Helsinki",
              "institution": "Aalto University"
            }
          ],
          "personId": 128893
        }
      ]
    },
    {
      "id": 128394,
      "typeId": 12924,
      "title": "CrowdQ: Predicting the Queue State of Hospital Emergency Department Using Crowdsensing Mobility Data-Driven Models",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610875",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "\"Hospital Emergency Departments (EDs) are essential for providing emergency medical services, yet often overwhelmed due to increasing healthcare demand. Current methods for monitoring ED queue states, such as manual monitoring, video surveillance, and front-desk registration are inefficient, invasive, and delayed to provide real-time updates. To address these challenges, this paper proposes a novel framework, CrowdQ, which harnesses spatiotemporal crowdsensing data for real-time ED demand sensing, queue state modeling, and prediction. By utilizing vehicle trajectory and urban geographic environment data, CrowdQ can accurately estimate emergency visits from noisy traffic flows. Furthermore, it employs queueing theory to model the complex emergency service process with medical service data, effectively considering spatiotemporal dependencies and event context impact on ED queue states. Experiments conducted on large-scale crowdsensing urban traffic datasets and hospital information system datasets from Xiamen City demonstrate the framework's effectiveness. It achieves an F1 score of 0.93 in ED demand identification, effectively models the ED queue state of key hospitals, and reduces the error in queue state prediction by 18.5%-71.3% compared to baseline methods. CrowdQ, therefore, offers valuable alternatives for public emergency treatment information disclosure and maximized medical resource allocation.\" https://doi.org/10.1145/3610875",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129274
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129273
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129271
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "University of Virginia"
            }
          ],
          "personId": 129282
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129281
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129290
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Macau",
              "institution": "University of Macau"
            }
          ],
          "personId": 129289
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hangzhou",
              "institution": "Zhejiang University City College"
            }
          ],
          "personId": 129288
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Fujian",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129287
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xiamen",
              "institution": "Xiamen University"
            }
          ],
          "personId": 129286
        }
      ]
    },
    {
      "id": 128395,
      "typeId": 12924,
      "title": "Integrating Gaze and Mouse Via Joint Cross-Attention Fusion Net for Students’ Activity Recognition in E-learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610876",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "\"E-learning has emerged as an indispensable educational mode in the post-epidemic era. However, this mode makes it difficult for students to stay engaged in learning without appropriate activity monitoring. Our work explores a promising solution that combines gaze and mouse data to recognize students' activities, thereby facilitating activity monitoring and analysis during e-learning. We initially surveyed 200 students from a local university, finding more acceptance for eye trackers and mouse loggers compared to video surveillance. We then designed eight students' routine digital activities to collect a multimodal dataset and analyze the patterns and correlations between gaze and mouse across various activities. Our proposed Joint Cross-Attention Fusion Net, a multimodal activity recognition framework, leverages the gaze-mouse relationship to yield improved classification performance by integrating cross-modal representations through a cross-attention mechanism and integrating the joint features that characterize gaze-mouse coordination. Evaluation results show that our method can achieve up to 94.87% F1-score in predicting 8-classes activities, with an improvement of at least 7.44% over using gaze or mouse data independently. This research illuminates new possibilities for monitoring student engagement in intelligent education systems, also suggesting a promising strategy for melding perception and action modalities in behavioral analysis across a range of ubiquitous computing environments.\" https://doi.org/10.1145/3610876",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Xi'an Jiaotong University"
            }
          ],
          "personId": 129300
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Xi'an Jiaotong University"
            }
          ],
          "personId": 129299
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Xi'an Jiaotong University"
            }
          ],
          "personId": 129298
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi’an",
              "institution": "Xi’an Jiaotong University"
            }
          ],
          "personId": 129297
        }
      ]
    },
    {
      "id": 128396,
      "typeId": 12924,
      "title": "CASES: A Cognition-Aware Smart Eyewear System for Understanding How People Read",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610910",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "abstract": "\"The process of reading has attracted decades of scientific research. Work in this field primarily focuses on using eye gaze patterns to reveal cognitive processes while reading. However, eye gaze patterns suffer from limited resolution, jitter noise, and cognitive biases, resulting in limited accuracy in tracking cognitive reading states. Moreover, using sequential eye gaze data alone neglects the linguistic structure of text, undermining attempts to provide semantic explanations for cognitive states during reading. Motivated by the impact of the semantic context of text on the human cognitive reading process, this work uses both the semantic context of text and visual attention during reading to more accurately predict the temporal sequence of cognitive states. To this end, we present a Cognition-Aware Smart Eyewear System (CASES), which fuses semantic context and visual attention patterns during reading. The two feature modalities are time-aligned and fed to a temporal convolutional network based multi-task classification deep model to automatically estimate and further semantically explain the reading state timeseries. CASES is implemented in eyewear and its use does not interrupt the reading process, thus reducing subjective bias. Furthermore, the real-time association between visual and semantic information enables the interactions between visual attention and semantic context to be better interpreted and explained. Ablation studies with 25 subjects demonstrate that CASES improves multi-label reading state estimation accuracy by 20.90% for sentence compared to eye tracking alone. Using CASES, we develop an interactive reading assistance system. Three and a half months of deployment with 13 in-field studies enables several observations relevant to the study of reading. In particular, observed how individual visual history interacts with the semantic context at different text granularities. Furthermore, CASES enables just-in-time intervention when readers encounter processing difficulties, thus promoting self-awareness of the cognitive process involved in reading and helping to develop more effective reading habits.\" https://doi.org/10.1145/3610910",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129075
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129074
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129073
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129071
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "University of London"
            }
          ],
          "personId": 129115
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129113
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129111
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 129109
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129107
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129105
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129101
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129100
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 129099
        }
      ]
    },
    {
      "id": 128397,
      "typeId": 12924,
      "title": "sUrban: Stable Prediction for Unseen Urban Data from Location-based Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610877",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"Recent machine learning research on smart cities has achieved great success in predicting future trends, under the key assumption that the test data follows the same distribution of the training data. The rapid urbanization, however, makes this assumption challenging to hold in practice. Because new data is emerging from new environments (e.g., an emerging city or region), which may follow different distributions from data in existing environments. Different from transfer-learning methods accessing target data during training, we often do not have any prior knowledge about the new environment. Therefore, it is critical to explore a predictive model that can be effectively adapted to unseen new environments. This work aims to address this Out-of-Distribution (OOD) challenge for sustainable cities. We propose to identify two kinds of features that are useful for OOD prediction in each environment: (1) the environment-invariant features to capture the shared commonalities for predictions across different environments; and (2) the environment-aware features to characterize the unique information of each environment. Take bike riding as an example. The bike demands of different cities often follow the same pattern that they significantly increase during the rush hour on workdays. Meanwhile, there are also some local patterns in each city because of different cultures and citizens' travel preferences. We introduce a principled framework -- sUrban -- that consists of an environment-invariant optimization module for learning invariant representation and an environment-aware optimization module for learning environment-aware representation. Evaluation on real-world datasets from various urban application domains corroborates the generalizability of sUrban. This work opens up new avenues to smart city development.\" https://doi.org/10.1145/3610877",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "School of Computer Science and Technology"
            }
          ],
          "personId": 129278
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "xian",
              "institution": "northwestern polytechnical univ."
            }
          ],
          "personId": 129277
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Illinois Chicago"
            }
          ],
          "personId": 129276
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 129275
        }
      ]
    },
    {
      "id": 128398,
      "typeId": 12924,
      "title": "AeroTraj: Trajectory Planning for Fast, and Accurate 3D Reconstruction Using a Drone-based LiDAR",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610911",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"As video conferencing (VC) has become necessary for many professional, educational, and social tasks, people who are d/Deaf and hard of hearing (DHH) face distinct accessibility barriers. We conducted studies to understand the challenges faced by DHH people during VCs and found that they struggled to easily present or communicate effectively due to accessibility limitations of VC platforms. These limitations include the lack of tools for DHH speakers to discreetly communicate their accommodation needs to the group. Based on these findings, we prototyped a suite of tools, called Erato that enables DHH speakers to be aware of their performance while speaking and remind participants of proper etiquette. We evaluated Erato by running a mock classroom case study over VC for three sessions. All participants felt more confident in their speaking ability and paid closer attention to making the classroom more inclusive while using our tool. We share implications of these results for the design of VC interfaces and human-the-the-loop assistive systems that can support users who are DHH to communicate effectively and advocate for their accessibility needs.\" https://doi.org/10.1145/3610911",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "Rochester Institute of Technology"
            }
          ],
          "personId": 129487
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California"
            }
          ],
          "personId": 129485
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California"
            }
          ],
          "personId": 129483
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California"
            }
          ],
          "personId": 129481
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Murray Hill",
              "institution": "Nokia Bell Labs"
            }
          ],
          "personId": 129479
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 129477
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California"
            }
          ],
          "personId": 129494
        }
      ]
    },
    {
      "id": 128399,
      "typeId": 12924,
      "title": "LapTouch: Using the Lap for Seated Touch Interaction with HMDs",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610878",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "abstract": "\"Use of virtual reality while seated is common, but studies on seated interaction beyond the use of controllers or hand gestures have been sparse. This work present LapTouch, which makes use of the lap as a touch interface and includes two user studies to inform the design of direct and indirect touch interaction using the lap with visual feedback that guides the user touch, as well as eye-free interaction in which users are not provided with such visual feedback. The first study suggests that direct interaction can provide effective layouts with 95% accuracy with up to a 4×4 layout and a shorter completion time, while indirect interaction can provide effective layouts with up to a 4×5 layout but a longer completion time. Considering user experience, which revealed that 4-row and 5-column layouts are not preferred, it is recommended to use both direct and indirect interaction with a maximum of a 3×4 layout. According to the second study, increasing the eye-free interaction with support vector machine (SVM) allows for a 2×2 layout with a generalized model and 2×2, 2×3 and 3×2 layouts with personalized models.\" https://doi.org/10.1145/3610878",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "886",
              "city": "Hsinchu",
              "institution": "National Yang Ming Chiao Tung University"
            }
          ],
          "personId": 129296
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "city": "Hsinchu City",
              "institution": "National Yang Ming Chiao Tung University"
            }
          ],
          "personId": 129295
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University"
            }
          ],
          "personId": 129294
        }
      ]
    },
    {
      "id": 128400,
      "typeId": 12924,
      "title": "MMTSA: Multi-Modal Temporal Segment Attention Network for Efficient Human Activity Recognition",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610872",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "abstract": "\"Multimodal sensors provide complementary information to develop accurate machine-learning methods for human activity recognition (HAR), but introduce significantly higher computational load, which reduces efficiency. This paper proposes an efficient multimodal neural architecture for HAR using an RGB camera and inertial measurement units (IMUs) called Multimodal Temporal Segment Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a temporal and structure-preserving gray-scale image using the Gramian Angular Field (GAF), representing the inherent properties of human activities. MMTSA then applies a multimodal sparse sampling method to reduce data redundancy. Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal fusion. Using three well-established public datasets, we evaluated MMTSA's effectiveness and efficiency in HAR. Results show that our method achieves superior performance improvements (11.13% of cross-subject F1-score on the MMAct dataset) than the previous state-of-the-art (SOTA) methods. The ablation study and analysis suggest that MMTSA's effectiveness in fusing multimodal data for accurate HAR. The efficiency evaluation on an edge device showed that MMTSA achieved significantly better accuracy, lower computational load, and lower inference latency than SOTA methods.\" https://doi.org/10.1145/3610872",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129306
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129305
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Charlottesville",
              "institution": "university of virginia"
            }
          ],
          "personId": 129318
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129317
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129316
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington"
            }
          ],
          "personId": 129314
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129312
        }
      ]
    },
    {
      "id": 128401,
      "typeId": 12924,
      "title": "Radio2Text: Streaming Speech Recognition Using mmWave Radio Signals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610873",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "\"Millimeter wave (mmWave) based speech recognition provides more possibility for audio-related applications, such as conference speech transcription and eavesdropping. However, considering the practicality in real scenarios, latency and recognizable vocabulary size are two critical factors that cannot be overlooked. In this paper, we propose Radio2Text, the first mmWave-based system for streaming automatic speech recognition (ASR) with a vocabulary size exceeding 13,000 words. Radio2Text is based on a tailored streaming Transformer that is capable of effectively learning representations of speech-related features, paving the way for streaming ASR with a large vocabulary. To alleviate the deficiency of streaming networks unable to access entire future inputs, we propose the Guidance Initialization that facilitates the transfer of feature knowledge related to the global context from the non-streaming Transformer to the tailored streaming Transformer through weight inheritance. Further, we propose a cross-modal structure based on knowledge distillation (KD), named cross-modal KD, to mitigate the negative effect of low quality mmWave signals on recognition performance. In the cross-modal KD, the audio streaming Transformer provides feature and response guidance that inherit fruitful and accurate speech information to supervise the training of the tailored radio streaming Transformer. The experimental results show that our Radio2Text can achieve a character error rate of 5.7% and a word error rate of 9.4% for the recognition of a vocabulary consisting of over 13,000 words.\" https://doi.org/10.1145/3610873",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong SAR",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129330
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Shanghai Qi Zhi Institute"
            }
          ],
          "personId": 129328
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Tsinghua University"
            }
          ],
          "personId": 129326
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong SAR",
              "institution": "The University of Hong Kong"
            }
          ],
          "personId": 129324
        }
      ]
    },
    {
      "id": 128402,
      "typeId": 12924,
      "title": "Echo: Reverberation-based Fast Black-Box Adversarial Attacks on Intelligent Audio Systems",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610874",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"Intelligent audio systems are ubiquitous in our lives, such as speech command recognition and speaker recognition. However, it is shown that deep learning-based intelligent audio systems are vulnerable to adversarial attacks. In this paper, we propose a physical adversarial attack that exploits reverberation, a natural indoor acoustic effect, to realize imperceptible, fast, and targeted black-box attacks. Unlike existing attacks that constrain the magnitude of adversarial perturbations within a fixed radius, we generate reverberation-alike perturbations that blend naturally with the original voice sample 1. Additionally, we can generate more robust adversarial examples even under over-the-air propagation by considering distortions in the physical environment. Extensive experiments are conducted using two popular intelligent audio systems in various situations, such as different room sizes, distance, and ambient noises. The results show that Echo can invade into intelligent audio systems in both digital and physical over-the-air environment.\" https://doi.org/10.1145/3610874",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 129285
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Wuhan",
              "institution": "Wuhan University"
            }
          ],
          "personId": 129284
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hubei",
              "city": "Wuhan",
              "institution": "School of Computer Science"
            }
          ],
          "personId": 129283
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 129293
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Wuhan",
              "institution": "Wuhan University"
            }
          ],
          "personId": 129292
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Wuhan",
              "institution": "Huazhong University of Science and Technology"
            }
          ],
          "personId": 129291
        }
      ]
    },
    {
      "id": 128403,
      "typeId": 12924,
      "title": "Interaction Harvesting: A Design Probe of User-Powered Widgets",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610880",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "\"Whenever a user interacts with a device, mechanical work is performed to actuate the user interface elements; the resulting energy is typically wasted, dissipated as sound and heat. Previous work has shown that many devices can be powered entirely from this otherwise wasted user interface energy. For these devices, wires and batteries, along with the related hassles of replacement and charging, become unnecessary and onerous. So far, these works have been restricted to proof-of-concept demonstrations; a specific bespoke harvesting and sensing circuit is constructed for the application at hand. The challenge of harvesting energy while simultaneously sensing fine-grained input signals from diverse modalities makes prototyping new devices difficult. To fill this gap, we present a hardware toolkit which provides a common electrical interface for harvesting energy from user interface elements. This facilitates exploring the composability, utility, and breadth of enabled applications of interaction-powered smart devices. We design a set of \"energy as input\" harvesting circuits, a standard connective interface with 3D printed enclosures, and software libraries to enable the exploration of devices where the user action generates the energy needed to perform the device's primary function. This exploration culminated in a demonstration campaign where we prototype several exemplar popular toys and gadgets, including battery-free Bop-It--- a popular 90s rhythm game, an electronic Etch-a-sketch, a \"Simon-Says\"-style memory game, and a service rating device. We run exploratory user studies to understand how generativity, creativity, and composability are hampered or facilitated by these devices. These demonstrations, user study takeaways, and the toolkit itself provide a foundation for building interactive and user-focused gadgets whose usability is not affected by battery charge and whose service lifetime is not limited by battery wear.\" https://doi.org/10.1145/3610880",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 128647
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128645
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128643
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University"
            }
          ],
          "personId": 128641
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles"
            }
          ],
          "personId": 128639
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology"
            }
          ],
          "personId": 128635
        }
      ]
    },
    {
      "id": 128404,
      "typeId": 12924,
      "title": "SignRing: Continuous American Sign Language Recognition Using IMU Rings and Virtual IMU Data",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610881",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "abstract": "\"Sign language is a natural language widely used by Deaf and hard of hearing (DHH) individuals. Advanced wearables are developed to recognize sign language automatically. However, they are limited by the lack of labeled data, which leads to a small vocabulary and unsatisfactory performance even though laborious efforts are put into data collection. Here we propose SignRing, an IMU-based system that breaks through the traditional data augmentation method, makes use of online videos to generate the virtual IMU (v-IMU) data, and pushes the boundary of wearable-based systems by reaching the vocabulary size of 934 with sentences up to 16 glosses. The v-IMU data is generated by reconstructing 3D hand movements from two-view videos and calculating 3-axis acceleration data, by which we are able to achieve a word error rate (WER) of 6.3% with a mix of half v-IMU and half IMU training data (2339 samples for each), and a WER of 14.7% with 100% v-IMU training data (6048 samples), compared with the baseline performance of the 8.3% WER (trained with 2339 samples of IMU data). We have conducted comparisons between v-IMU and IMU data to demonstrate the reliability and generalizability of the v-IMU data. This interdisciplinary work covers various areas such as wearable sensor development, computer vision techniques, deep learning, and linguistics, which can provide valuable insights to researchers with similar research objectives.\" https://doi.org/10.1145/3610881",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, The State University of New York"
            }
          ],
          "personId": 128596
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128593
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Goa",
              "city": "Ponda",
              "institution": "Indian Institute of Technology Goa"
            }
          ],
          "personId": 128590
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo (SUNY)"
            }
          ],
          "personId": 128587
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Binghamton",
              "institution": "Binghamton University"
            }
          ],
          "personId": 128585
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Schaumburg",
              "institution": "Motorola Solutions Inc."
            }
          ],
          "personId": 128583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128581
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, State University of New York"
            }
          ],
          "personId": 128580
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo, the State University of New York"
            }
          ],
          "personId": 128579
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128626
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 128624
        }
      ]
    },
    {
      "id": 128405,
      "typeId": 12924,
      "title": "Unveiling Causal Attention in Dogs’ Eyes with Smart Eyewear",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569490",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "abstract": "\"Our goals are to better understand dog cognition, and to support others who share this interest. Existing investigation methods predominantly rely on human-manipulated experiments to examine dogs' behavioral responses to visual stimuli such as human gestures. As a result, existing experimental paradigms are usually constrained to in-lab environments and may not reveal the dog's responses to real-world visual scenes. Moreover, visual signals pertaining to dog behavioral responses are empirically derived from observational evidence, which can be prone to subjective bias and may lead to controversies. We aim to overcome or reduce the existing limitations of dog cognition studies by investigating a challenging issue: identifying the visual signal(s) from dog eye motion that can be utilized to infer causal explanations of its behaviors, namely estimating causal attention. To this end, we design a deep learning framework named Causal AtteNtIon NEtwork (CANINE) to unveil the dogs' causal attention mechanism, inspired by the recent advance in causality analysis with deep learning. Equipped with CANINE, we developed the first eyewear device to enable inference on the vision-related behavioral causality of canine wearers. We demonstrate the technical feasibility of the proposed CANINE glasses through their application in multiple representative experimental scenarios of dog cognitive study. Various in-field trials are also performed to demonstrate the generality of the CANINE eyewear in real-world scenarios. With the proposed CANINE glasses, we collect the first large-scale dataset, named DogsView, which consists of automatically generated annotations on the canine wearer's causal attention across a wide range of representative scenarios. The DogsView dataset is available online to facilitate research.\nhttps://doi.org/10.1145/3569490\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128591
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128588
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128764
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Oxford",
              "institution": "University of Oxford"
            }
          ],
          "personId": 128762
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128780
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 128778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Boulder",
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 128776
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 128774
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Microsoft Research Asia"
            }
          ],
          "personId": 128772
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128770
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128769
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shanghai",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128768
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shanghai",
              "institution": "Fudan University"
            }
          ],
          "personId": 128786
        }
      ]
    },
    {
      "id": 128406,
      "typeId": 12924,
      "title": "Auto-Gait: Automatic Ataxia Risk Assessment with Computer Vision from Gait Task Videos",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580845",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "abstract": "Many patients with neurological disorders, such as Ataxia, do not have easy access to neurologists, -especially those living in remote localities and developing/underdeveloped countries. Ataxia is a degenerative disease of the nervous system that surfaces as difficulty with motor control, such as walking imbalance. Previous studies have attempted automatic diagnosis of Ataxia with the help of wearable biomarkers, Kinect, and other sensors. These sensors, while accurate, do not scale efficiently well to naturalistic deployment settings. In this study, we propose a method for identifying ataxic symptoms by analyzing videos of participants walking down a hallway, captured with a standard monocular camera. In a collaboration with 11 medical sites located in 8 different states across the United States, we collected a dataset of 155 videos along with their severity rating from 89 participants (24 controls and 65 diagnosed with or are pre-manifest spinocerebellar ataxias). The participants performed the gait task of the Scale for the Assessment and Rating of Ataxia (SARA). We develop a computer vision pipeline to detect, track, and separate the participants from their surroundings and construct several features from their body pose coordinates to capture gait characteristics such as step width, step length, swing, stability, speed, etc. Our system is able to identify and track a patient in complex scenarios. For example, if there are multiple people present in the video or an interruption from a passerby. Our Ataxia risk-prediction model achieves 83.06% accuracy and an 80.23% F1 score. Similarly, our Ataxia severity-assessment model achieves a mean absolute error (MAE) score of 0.6225 and a Pearson's correlation coefficient score of 0.7268. Our model competitively performed when evaluated on data from medical sites not used during training. Through feature importance analysis, we found that our models associate wider steps, decreased walking speed, and increased instability with greater Ataxia severity, which is consistent with previously established clinical knowledge. Furthermore, we are releasing the models and the body-pose coordinate dataset to the research community - the largest dataset on ataxic gait (to our knowledge). Our models could contribute to improving health access by enabling remote Ataxia assessment in non-clinical settings without requiring any sensors or special cameras. Our dataset will help the computer science community to analyze different characteristics of Ataxia and to develop better algorithms for diagnosing other movement disorders.\nhttps://dl.acm.org/doi/10.1145/3580845",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester"
            }
          ],
          "personId": 129096
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester"
            }
          ],
          "personId": 129094
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester"
            }
          ],
          "personId": 129136
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Houston Methodist"
            }
          ],
          "personId": 129134
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester"
            }
          ],
          "personId": 129132
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester"
            }
          ],
          "personId": 129130
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Rochester",
              "institution": "University of Rochester Medical Center"
            }
          ],
          "personId": 129128
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan"
            }
          ],
          "personId": 129127
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota"
            }
          ],
          "personId": 129126
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "Hôpital de la Pitié-Salpêtrière"
            }
          ],
          "personId": 129122
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Bonn",
              "institution": "University Hospital Bonn"
            }
          ],
          "personId": 129120
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Readisca.org"
            }
          ],
          "personId": 129118
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Readisca.org"
            }
          ],
          "personId": 129116
        },
        {
          "affiliations": [],
          "personId": 129160
        }
      ]
    },
    {
      "id": 128407,
      "typeId": 12924,
      "title": "Understanding the Mechanism of Through-Wall Wireless Sensing: A Model-based Perspective",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569494",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"During the last few years, there is a growing interest on the usage of Wi-Fi signals for human activity detection. A large number of Wi-Fi based sensing systems have been developed, including respiration detection, gesture classification, identity recognition, etc. However, the usability and robustness of such systems are still limited, due to the complexity of practical environments. Various pioneering approaches have been proposed to solve this problem, among which the model-based approach is attracting more and more attention, due to the advantage that it does not require a huge dataset for model training. Existing models are usually developed for Line-of-Sight (LoS) scenarios, and can not be applied to facilitating the design of wireless sensing systems in Non-Line-of-Sight (NLoS) scenarios (e.g., through-wall sensing). To fill this gap, we propose a through-wall wireless sensing model, aiming to characterize the propagation laws and sensing mechanisms of Wi-Fi signals in through-wall scenarios. Specifically, based on the insight that Wi-Fi signals will be refracted while there is a wall between the transceivers, we develop a refraction-aware Fresnel model, and prove theoretically that the original Fresnel model can be seen as a special case of the proposed model. We find that the presence of a wall will change the distribution of Fresnel zones, which we called the \"\"squeeze effect\"\" of Fresnel zones. Moreover, our theoretical analysis indicates that the \"\"squeeze effect\"\" can help improve the sensing capability (i.e., spatial resolution) of Wi-Fi signals. To validate the proposed model, we implement a through-wall respiration sensing system with a pair of transceivers. Extensive experiments in typical through-wall environments show that the respiration detection error is lower than 0.5 bpm, while the subject's vertical distance to the connection line of the transceivers is less than 200 cm. To the best of our knowledge, this is the first theoretical model that reveals the Wi-Fi based wireless sensing mechanism in through-wall scenarios.\nhttps://dl.acm.org/doi/10.1145/3569494\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128991
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128988
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shaanxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128985
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shannxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 128982
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shannxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 129272
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shannxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 129280
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Shannxi",
              "city": "Xi'an",
              "institution": "Northwestern Polytechnical University"
            }
          ],
          "personId": 129279
        }
      ]
    },
    {
      "id": 128408,
      "typeId": 12924,
      "title": "GC-Loc: A Graph Attention Based Framework for Collaborative Indoor Localization Using Infrastructure-free Signals",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569495",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"Indoor localization techniques play a fundamental role in empowering plenty of indoor location-based services (LBS) and exhibit great social and commercial values. The widespread fingerprint-based indoor localization methods usually suffer from the low feature discriminability with discrete signal fingerprint or high time overhead for continuous signal fingerprint collection. To address this, we introduce the collaboration mechanism and propose a graph attention based collaborative indoor localization framework, termed GC-Loc, which provides another perspective for efficient indoor localization. GC-Loc utilizes multiple discrete signal fingerprints collected by several users as input for collaborative localization. Specifically, we first construct an adaptive graph representation to efficiently model the relationships among the collaborative fingerprints. Then taking state-of-the-art GAT model as basic unit, we design a deep network with the residual structure and the hierarchical attention mechanism to extract and aggregate the features from the constructed graph for collaborative localization. Finally, we further employ ensemble learning mechanism in GC-Loc and devise a location refinement strategy based on model consensus for enhancing the robustness of GC-Loc. We have conducted extensive experiments in three different trial sites, and the experimental results demonstrate the superiority of GC-Loc, outperforming the comparison schemes by a wide margin (reducing the mean localization error by more than 42%).\nhttps://dl.acm.org/doi/10.1145/3569495\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128600
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128597
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128594
        }
      ]
    },
    {
      "id": 128409,
      "typeId": 12924,
      "title": "Robust Federated Learning for Ubiquitous Computing through Mitigation of Edge-Case Backdoor Attacks",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569492",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"Federated Learning (FL) allows several data owners to train a joint model without sharing their training data. Such a paradigm is useful for better privacy in many ubiquitous computing systems. However, FL is vulnerable to poisoning attacks, where malicious participants attempt to inject a backdoor task in the model at training time, along with the main task that the model was initially trained for. Recent works show that FL is particularly vulnerable to edge-case backdoors introduced by data points with unusual out-of-distribution features. Such attacks are among the most difficult to counter, and today's FL defense mechanisms usually fail to tackle them. In this paper, we present ARMOR, a defense mechanism that leverages adversarial learning to uncover edge-case backdoors. In contrast to most of existing FL defenses, ARMOR does not require real data samples and is compatible with secure aggregation, thus, providing better FL privacy protection. ARMOR relies on GANs (Generative Adversarial Networks) to extract data features from model updates, and uses the generated samples to test the activation of potential edge-case backdoors in the model. Our experimental evaluations with three widely used datasets and neural networks show that ARMOR can tackle edge-case backdoors with 95% resilience against attacks, and without hurting model quality.\nhttps://doi.org/10.1145/3569492\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Lyon",
              "institution": "INSA Lyon -- LIRIS"
            }
          ],
          "personId": 128944
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Lyon",
              "institution": "INSA Lyon -- LIRIS"
            }
          ],
          "personId": 128998
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Lyon",
              "institution": "INSA Lyon"
            }
          ],
          "personId": 128996
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Lyon",
              "institution": "CNRS - LIRIS"
            }
          ],
          "personId": 128993
        }
      ]
    },
    {
      "id": 128410,
      "typeId": 12924,
      "title": "Side-lobe Can Know More: Towards Simultaneous Communication and Sensing for mmWave",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569498",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"Thanks to the wide bandwidth, large antenna array, and short wavelength, millimeter wave (mmWave) has superior performance in both communication and sensing. Thus, the integration of sensing and communication is a developing trend for the mmWave band. However, the directional transmission characteristics of the mmWave limits the sensing scope to a narrow sector. Existing works coordinate sensing and communication in a time-division manner, which takes advantage of the sector level sweep during the beam training interval for sensing and the data transmission interval for communication. Beam training is a low frequency (e.g., 10Hz) and low duty-cycle event, which makes it hard to track fast movement or perform continuous sensing. Such time-division designs imply that we need to strike a balance between sensing and communication, and it is hard to get the best of both worlds. In this paper, we try to solve this dilemma by exploiting side lobes for sensing. We design Sidense, where the main lobe of the transmitter is directed towards the receiver, while in the meantime, the side lobes can sense the ongoing activities in the surrounding. In this way, sensing and downlink communication work simultaneously and will not compete for hardware and radio resources. In order to compensate for the low antenna gain of side lobes, Sidense performs integration to boost the quality of sensing signals. Due to the uneven side-lobe energy, Sidense also designs a target separation scheme to tackle the mutual interference in multi-target scenarios. We implement Sidense with Sivers mmWave module. Results show that Sidense can achieve millimeter motion tracking accuracy at 6m. We also demonstrate a multi-person respiration monitoring application. As Sidense does not modify the communication procedure or the beamforming strategy, the downlink communication performance will not be sacrificed due to concurrent sensing. We believe that more fascinating applications can be implemented on this concurrent sensing and communication platform.\nhttps://dl.acm.org/doi/10.1145/3569498\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 129474
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 129522
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 129520
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Shenzhen",
              "institution": "Southern University of Science and Technology"
            }
          ],
          "personId": 129517
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Peng Cheng Laboratory"
            }
          ],
          "personId": 129514
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Shenzhen",
              "institution": "Peng Cheng Laboratory"
            }
          ],
          "personId": 129511
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129508
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "HKUST"
            }
          ],
          "personId": 129505
        }
      ]
    },
    {
      "id": 128411,
      "typeId": 12924,
      "title": "Multi-Vib: Precise Multi-point Vibration Monitoring Using mmWave Radar",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569496",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"Vibration measurement is vital for fault diagnosis of structures (e.g., machines and civil structures). Different structure components undergo distinct vibration patterns, which jointly determine the structure's health condition, thus demanding simultaneous multi-point vibration monitoring. Existing solutions deploy multiple accelerometers along with their power supplies or laser vibrometers on the monitored object to measure multi-point vibration, which is inconvenient and costly. Cameras provide a less expensive solution while heavily relying on good lighting conditions. To overcome these limitations, we propose a cost-effective and passive system, called Multi-Vib, for precise multi-point vibration monitoring. Multi-Vib is implemented using a single mmWave radar to remotely and separately sense the vibration displacement of multiple points via signal reflection. However, simultaneously detecting and monitoring multiple points on a single object is a daunting task. This is because most radar signals are scattered away from vibration points due to their tilted locations and shapes by nature, causing an extremely weak reflected signal to the radar. To solve this issue, we dedicatedly design a physical marker placed on the target point, which can force the direction of the reflected signal towards the radar and significantly increase the reflected signal strength. Another practical issue is that the reflected signal from each point endures interferences and noises from the surroundings. Thus, we develop a series of effective signal processing methods to denoise the signal for accurate vibration frequency and displacement estimation. Extensive experimental results show that the average errors in multi-point vibration frequency and displacement estimation are around 0.16Hz and 14μm, respectively.\nhttps://dl.acm.org/doi/10.1145/3569496\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Qingdao",
              "institution": "Shandong University"
            }
          ],
          "personId": 128906
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hong Kong",
              "city": "Hong Kong",
              "institution": "Comp"
            }
          ],
          "personId": 128904
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 128922
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 128920
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Hunan",
              "city": "ChangSha",
              "institution": "Hunan University"
            }
          ],
          "personId": 128902
        }
      ]
    },
    {
      "id": 128412,
      "typeId": 12924,
      "title": "MilliPCD: Beyond Traditional Vision Indoor Point Cloud Generation via Handheld Millimeter-Wave Devices",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3569497",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"3D Point Cloud Data (PCD) has been used in many research and commercial applications widely, such as autonomous driving, robotics, and VR/AR. But existing PCD generation systems based on RGB-D and LiDARs require robust lighting and an unobstructed field of view of the target scenes. So, they may not work properly under challenging environmental conditions. Recently, millimeter-wave (mmWave) based imaging systems have raised considerable interest due to their ability to work in dark environments. But the resolution and quality of the PCD from these mmWave imaging systems are very poor. To improve the quality of PCD, we design and implement MilliPCD, a \"\"beyond traditional vision\"\" PCD generation system for handheld mmWave devices, by integrating traditional signal processing with advanced deep learning based algorithms. We evaluate MilliPCD with real mmWave reflected signals collected from large, diverse indoor environments, and the results show improvements in the quality w.r.t. the existing algorithms, both quantitatively and qualitatively.\nhttps://dl.acm.org/doi/10.1145/3569497\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Columbia",
              "institution": "University of South Carolina"
            }
          ],
          "personId": 129023
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Columbia",
              "institution": "University of South Carolina"
            }
          ],
          "personId": 129021
        }
      ]
    },
    {
      "id": 128413,
      "typeId": 12924,
      "title": "SmarCyPad: A Smart Seat Pad for Cycling Fitness Tracking Leveraging Low-cost Conductive Fabric Sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610927",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "\"Cycling is an efficient and effective way to improve one's overall fitness level, such as cardiovascular fitness, stamina, lower body strength, and body fat percentage. To improve fitness performance, real-time cycling fitness tracking can not only allow cyclists to better control their energy outputs but also help push workout intensity and keep users accountable for their fitness progress. However, existing bike sensors (e.g., the ones mounted to bike's wheel hub or crank arm) are only limited to measuring cycling cadence and speed. Although several recent studies relying on on-body sensors or cameras can provide more fine-grained information (e.g., riding position and knee joint angle), they would either require inconvenient setups or raise serious privacy concerns. To circumvent these limitations, in this paper, we propose SmarCyPad, an innovative smart seat pad that can continuously and unobtrusively track five cycling-specific metrics, including cadence, per-leg stability, leg strength balance, riding position, and knee joint angle of the cyclist. Specifically, we embed conductive fabric sensors in the seat pad to sense the pressure applied to the bike's seat exerted by the cyclist's gluteal muscles. A series of signal processing algorithms are developed to estimate the pedaling period from the sensed pressure signal and further derive the cycling cadence, per-leg stability, and leg strength balance. Additionally, we leverage a deep learning model to detect the cyclist's riding position and reconstruct the cyclist's knee joint angles via linear regression. The sensors and the system prototype are manufactured from scratch leveraging off-the-shelf materials, and the total cost is less than $50. Extensive experiments involving 15 participants demonstrate that SmarCyPad can accurately estimate the cycling cadence with an average error of 1.13 rounds per minute, quantify the cycling stability for each leg, detect cycling imbalance, distinguish five riding positions with an accuracy of 96.60%, and continuously track the knee joint angle with an average mean error as low as 9.58 degrees.\" https://doi.org/10.1145/3610927",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee, Knoxville"
            }
          ],
          "personId": 129212
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee"
            }
          ],
          "personId": 129210
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee, Knoxville"
            }
          ],
          "personId": 129252
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee"
            }
          ],
          "personId": 129250
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Microsoft"
            }
          ],
          "personId": 129248
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Knoxville",
              "institution": "University of Tennessee"
            }
          ],
          "personId": 129247
        }
      ]
    },
    {
      "id": 128414,
      "typeId": 12924,
      "title": "Surveying the Social Comfort of Body, Device, and Environment-Based Augmented Reality Interactions in Confined Passenger Spaces Using Mixed Reality Composite Videos",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610923",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "\"Augmented Reality (AR) headsets could significantly improve the passenger experience, freeing users from the restrictions of physical smartphones, tablets and seatback displays. However, the confined space of public transport and the varying proximity to other passengers may restrict what interaction techniques are deemed socially acceptable for AR users - particularly considering current reliance on mid-air interactions in consumer headsets. We contribute and utilize a novel approach to social acceptability video surveys, employing mixed reality composited videos to present a real user performing interactions across different virtual transport environments. This approach allows for controlled evaluation of perceived social acceptability whilst freeing researchers to present interactions in any simulated context. Our resulting survey (N=131) explores the social comfort of body, device, and environment-based interactions across seven transit seating arrangements. We reflect on the advantages of discreet inputs over mid-air and the unique challenges of face-to-face seating for passenger AR.\" https://doi.org/10.1145/3610923",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Paris",
              "institution": "Telecom Paris/Institut Polytechnique de Paris"
            }
          ],
          "personId": 128837
        },
        {
          "affiliations": [
            {
              "country": "France",
              "city": "Toulouse",
              "institution": "Ecole Nationale de l'Aviation Civile"
            }
          ],
          "personId": 128860
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 128859
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 128858
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 128856
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow"
            }
          ],
          "personId": 128854
        }
      ]
    },
    {
      "id": 128415,
      "typeId": 12924,
      "title": "Contact Tracing for HealthcareWorkers in an Intensive Care Unit",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610924",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "abstract": "\"The earlobe is a well-known location for wearing jewelry, but might also be promising for electronic output, such as presenting notifications. This work elaborates the pros and cons of different notification channels for the earlobe. Notifications on the earlobe can be private (only noticeable by the wearer) as well as public (noticeable in the immediate vicinity in a given social situation). A user study with 18 participants showed that the reaction times for the private channels (Poke, Vibration, Private Sound, Electrotactile) were on average less than 1 s with an error rate (missed notifications) of less than 1 %. Thermal Warm and Cold took significantly longer and Cold was least reliable (26 % error rate). The participants preferred Electrotactile and Vibration. Among the public channels the recognition time did not differ significantly between Sound (738 ms) and LED (828 ms), but Display took much longer (3175 ms). At 22 % the error rate of Display was highest. The participants generally felt comfortable wearing notification devices on their earlobe. The results show that the earlobe indeed is a suitable location for wearable technology, if properly miniaturized, which is possible for Electrotactile and LED. We present application scenarios and discuss design considerations. A small field study in a fitness center demonstrates the suitability of the earlobe notification concept in a sports context.\" https://doi.org/10.1145/3610924",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128725
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128724
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128723
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128722
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128733
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128740
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128739
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Missouri",
              "city": "St. Louis",
              "institution": "Washington University in St. Louis"
            }
          ],
          "personId": 128737
        }
      ]
    },
    {
      "id": 128416,
      "typeId": 12924,
      "title": "Can You Ear Me? — A Comparison of Different Private and Public Notification Channels for the Earlobe",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610925",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127792
      ],
      "eventIds": [],
      "abstract": "\"The earlobe is a well-known location for wearing jewelry, but might also be promising for electronic output, such as presenting notifications. This work elaborates the pros and cons of different notification channels for the earlobe. Notifications on the earlobe can be private (only noticeable by the wearer) as well as public (noticeable in the immediate vicinity in a given social situation). A user study with 18 participants showed that the reaction times for the private channels (Poke, Vibration, Private Sound, Electrotactile) were on average less than 1 s with an error rate (missed notifications) of less than 1 %. Thermal Warm and Cold took significantly longer and Cold was least reliable (26 % error rate). The participants preferred Electrotactile and Vibration. Among the public channels the recognition time did not differ significantly between Sound (738 ms) and LED (828 ms), but Display took much longer (3175 ms). At 22 % the error rate of Display was highest. The participants generally felt comfortable wearing notification devices on their earlobe. The results show that the earlobe indeed is a suitable location for wearable technology, if properly miniaturized, which is possible for Electrotactile and LED. We present application scenarios and discuss design considerations. A small field study in a fitness center demonstrates the suitability of the earlobe notification concept in a sports context.\" https://doi.org/10.1145/3610925",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Hannover",
              "institution": "Leibniz University Hannover"
            }
          ],
          "personId": 129322
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Hannover",
              "institution": "Leibniz University Hannover"
            }
          ],
          "personId": 129340
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Hannover",
              "institution": "Leibniz University Hannover"
            }
          ],
          "personId": 129339
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "city": "Hannover",
              "institution": "Leibniz University Hannover"
            }
          ],
          "personId": 129338
        }
      ]
    },
    {
      "id": 128417,
      "typeId": 12924,
      "title": "mmStress: Distilling Human Stress from Daily Activities via Contact-less Millimeter-wave Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610926",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "abstract": "\"Long-term exposure to stress hurts human's mental and even physical health,and stress monitoring is of increasing significance in the prevention, diagnosis, and management of mental illness and chronic disease. However, current stress monitoring methods are either burdensome or intrusive, which hinders their widespread usage in practice. In this paper, we propose mmStress, a contact-less and non-intrusive solution, which adopts a millimeter-wave radar to sense a subject's activities of daily living, from which it distills human stress. mmStress is built upon the psychologically-validated relationship between human stress and \"displacement activities\", i.e., subjects under stress unconsciously perform fidgeting behaviors like scratching, wandering around, tapping foot, etc. Despite the conceptual simplicity, to realize mmStress, the key challenge lies in how to identify and quantify the latent displacement activities autonomously, as they are usually transitory and submerged in normal daily activities, and also exhibit high variation across different subjects. To address these challenges, we custom-design a neural network that learns human activities from both macro and micro timescales and exploits the continuity of human activities to extract features of abnormal displacement activities accurately. Moreover, we also address the unbalance stress distribution issue by incorporating a post-hoc logit adjustment procedure during model training. We prototype, deploy and evaluate mmStress in ten volunteers' apartments for over four weeks, and the results show that mmStress achieves a promising accuracy of ~80% in classifying low, medium and high stress. In particular, mmStress manifests advantages, particularly under free human movement scenarios, which advances the state-of-the-art that focuses on stress monitoring in quasi-static scenarios.\" https://doi.org/10.1145/3610926",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129202
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129200
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Beijing University Of Posts and Telecommunications"
            }
          ],
          "personId": 129199
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129198
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Beijing University of Posts and Telecommunications"
            }
          ],
          "personId": 129194
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Hong Kong",
              "institution": "University of Hong Kong"
            }
          ],
          "personId": 129192
        }
      ]
    },
    {
      "id": 128418,
      "typeId": 12924,
      "title": "What and When to Explain? On-road Evaluation of Explanations in Highly Automated Vehicles",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610886",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "abstract": "\"Explanations in automated vehicles help passengers understand the vehicle's state and capabilities, leading to increased trust in the technology. Specifically, for passengers of SAE Level 4 and 5 vehicles who are not engaged in the driving process, the enhanced sense of control provided by explanations reduces potential anxieties, enabling them to fully leverage the benefits of automation. To construct explanations that enhance trust and situational awareness without disturbing passengers, we suggest testing with people who ultimately employ such explanations, ideally under real-world driving conditions. In this study, we examined the impact of various visual explanation types (perception, attention, perception+attention) and timing mechanisms (constantly provided or only under risky scenarios) on passenger experience under naturalistic driving scenarios using actual vehicles with mixed-reality support. Our findings indicate that visualizing the vehicle's perception state improves the perceived usability, trust, safety, and situational awareness without adding cognitive burden, even without explaining the underlying causes. We also demonstrate that the traffic risk probability could be used to control the timing of an explanation delivery, particularly when passengers are overwhelmed with information. Our study's on-road evaluation method offers a safe and reliable testing environment and can be easily customized for other AI models and explanation modalities.\" https://doi.org/10.1145/3610886",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology"
            }
          ],
          "personId": 129062
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology"
            }
          ],
          "personId": 129060
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology"
            }
          ],
          "personId": 129058
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 129057
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Gwangju",
              "institution": "Gwangju Institute of Science and Technology"
            }
          ],
          "personId": 129056
        }
      ]
    },
    {
      "id": 128419,
      "typeId": 12924,
      "title": "ProxiFit: Proximity Magnetic Sensing Using a Single Commodity Mobile toward Holistic Weight Exercise Monitoring",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610920",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "abstract": "\"Although many works bring exercise monitoring to smartphone and smartwatch, inertial sensors used in such systems require device to be in motion to detect exercises. We introduce ProxiFit, a highly practical on-device exercise monitoring system capable of classifying and counting exercises even if the device stays still. Utilizing novel proximity sensing of natural magnetism in exercise equipment, ProxiFit brings (1) a new category of exercise not involving device motion such as lower-body machine exercise, and (2) a new off-body exercise monitoring mode where a smartphone can be conveniently viewed in front of the user during workouts. ProxiFit addresses common issues of faint magnetic sensing by choosing appropriate preprocessing, negating adversarial motion artifacts, and designing a lightweight yet noise-tolerant classifier. Also, application-specific challenges such as a wide variety of equipment and the impracticality of obtaining large datasets are overcome by devising a unique yet challenging training policy. We evaluate ProxiFit on up to 10 weight machines (5 lower- and 5 upper-body) and 4 free-weight exercises, on both wearable and signage mode, with 19 users, at 3 gyms, over 14 months, and verify robustness against user and weather variations, spatial and rotational device location deviations, and neighboring machine interference.\" https://doi.org/10.1145/3610920",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 129052
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 129050
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 129048
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 129091
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "city": "Pohang",
              "institution": "POSTECH"
            }
          ],
          "personId": 129089
        }
      ]
    },
    {
      "id": 128420,
      "typeId": 12924,
      "title": "Privacy against Real-Time Speech Emotion Detection via Acoustic Adversarial Evasion of Machine Learning",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610887",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "abstract": "\"Smart speaker voice assistants (VAs) such as Amazon Echo and Google Home have been widely adopted due to their seamless integration with smart home devices and the Internet of Things (IoT) technologies. These VA services raise privacy concerns, especially due to their access to our speech. This work considers one such use case: the unaccountable and unauthorized surveillance of a user's emotion via speech emotion recognition (SER). This paper presents DARE-GP, a solution that creates additive noise to mask users' emotional information while preserving the transcription-relevant portions of their speech. DARE-GP does this by using a constrained genetic programming approach to learn the spectral frequency traits that depict target users' emotional content, and then generating a universal adversarial audio perturbation that provides this privacy protection. Unlike existing works, DARE-GP provides: a) real-time protection of previously unheard utterances, b) against previously unseen black-box SER classifiers, c) while protecting speech transcription, and d) does so in a realistic, acoustic environment. Further, this evasion is robust against defenses employed by a knowledgeable adversary. The evaluations in this work culminate with acoustic evaluations against two off-the-shelf commercial smart speakers using a small-form-factor (raspberry pi) integrated with a wake-word system to evaluate the efficacy of its real-world, real-time deployment.\" https://doi.org/10.1145/3610887",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University"
            }
          ],
          "personId": 128669
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University"
            }
          ],
          "personId": 128667
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University"
            }
          ],
          "personId": 128665
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University"
            }
          ],
          "personId": 128663
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University"
            }
          ],
          "personId": 128659
        }
      ]
    },
    {
      "id": 128421,
      "typeId": 12924,
      "title": "mSilent: Towards General Corpus Silent Speech Recognition Using COTS mmWave Radar",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580838",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "abstract": "\"Silent speech recognition (SSR) allows users to speak to the device without making a sound, avoiding being overheard or disturbing others. Compared to the video-based approach, wireless signal-based SSR can work when the user is wearing a mask and has fewer privacy concerns. However, previous wireless-based systems are still far from well-studied, e.g. they are only evaluated in corpus with highly limited size, making them only feasible for interaction with dozens of deterministic commands. In this paper, we present mSilent, a millimeter-wave (mmWave) based SSR system that can work in the general corpus containing thousands of daily conversation sentences. With the strong recognition capability, mSilent not only supports the more complex interaction with assistants, but also enables more general applications in daily life such as communication and input. To extract fine-grained articulatory features, we build a signal processing pipeline that uses a clustering-selection algorithm to separate articulatory gestures and generates a multi-scale detrended spectrogram (MSDS). To handle the complexity of the general corpus, we design an end-to-end deep neural network that consists of a multi-branch convolutional front-end and a Transformer-based sequence-to-sequence back-end. We collect a general corpus dataset of 1,000 daily conversation sentences that contains 21K samples of bi-modality data (mmWave and video). Our evaluation shows that mSilent achieves a 9.5% average word error rate (WER) at a distance of 1.5m, which is comparable to the performance of the state-of-the-art video-based approach. We also explore deploying mSilent in two typical scenarios of text entry and in-car assistant, and the less than 6% average WER demonstrates the potential of mSilent in general daily applications.\nhttps://dl.acm.org/doi/10.1145/3580838\"",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 128817
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Jiangsu",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 128815
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 128833
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Nanjing",
              "institution": "Nanjing University"
            }
          ],
          "personId": 128832
        }
      ]
    },
    {
      "id": 128422,
      "typeId": 12924,
      "title": "RealityReplay: Detecting and Replaying Temporal Changes In Situ Using Mixed Reality",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610888",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "abstract": "\"Humans easily miss events in their surroundings due to limited short-term memory and field of view. This happens, for example, while watching an instructor's machine repair demonstration or conversing during a sports game. We present RealityReplay, a novel Mixed Reality (MR) approach that tracks and visualizes these significant events using in-situ MR visualizations without modifying the physical space. It requires only a head-mounted MR display and a 360-degree camera. We contribute a method for egocentric tracking of important motion events in users' surroundings based on a combination of semantic segmentation and saliency prediction, and generating in-situ MR visual summaries of temporal changes. These summary visualizations are overlaid onto the physical world to reveal which objects moved, in what order, and their trajectory, enabling users to observe previously hidden events. The visualizations are informed by a formative study comparing different styles on their effects on users' perception of temporal changes. Our evaluation shows that RealityReplay significantly enhances sensemaking of temporal motion events compared to memory-based recall. We demonstrate application scenarios in guidance, education, and observation, and discuss implications for extending human spatiotemporal capabilities through technological augmentation.\" https://doi.org/10.1145/3610888",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128925
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128942
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 128941
        }
      ]
    },
    {
      "id": 128423,
      "typeId": 12924,
      "title": "LT-Fall: The Design and Implementation of a Life-threatening Fall Detection and Alarming System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3580835",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "abstract": "Falls are the leading cause of fatal injuries to elders in modern society, which has motivated researchers to propose various fall detection technologies. We observe that most of the existing fall detection solutions are diverging from the purpose of fall detection: timely alarming the family members, medical staff or first responders to save the life of the human with severe injury caused by fall. Instead, they focus on detecting the behavior of human falls, which does not necessarily mean a human is in real danger. The real critical situation is when a human cannot get up without assistance and is thus lying on the ground after the fall because of losing consciousness or becoming incapacitated due to severe injury. In this paper, we define a life-threatening fall as a behavior that involves a falling down followed by a long-lie of humans on the ground, and for the first time point out that a fall detection system should focus on detecting life-threatening falls instead of detecting any random falls. Accordingly, we design and implement LT-Fall, a mmWave-based life-threatening fall detection and alarming system. LT-Fall detects and reports both fall and fall-like behaviors in the first stage and then identifies life-threatening falls by continuously monitoring the human status after fall in the second stage. We propose a joint spatio-temporal localization technique to detect and locate the micro-motions of the human, which solves the challenge of mmWave's insufficient spatial resolution when the human is static, i.e., lying on the ground. Extensive evaluation on 15 volunteers demonstrates that compared to the state-of-the-art work (92% precision and 94% recall), LT-Fall achieves zero false alarms as well as a precision of 100% and a recall of 98.8%.\nhttps://dl.acm.org/doi/10.1145/3580835",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129362
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129361
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "JD.com"
            }
          ],
          "personId": 129360
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Buffalo",
              "institution": "University at Buffalo"
            }
          ],
          "personId": 129358
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129356
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129354
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Beijing",
              "institution": "Peking University"
            }
          ],
          "personId": 129352
        }
      ]
    },
    {
      "id": 128424,
      "typeId": 12924,
      "title": "MicroCam: Leveraging Smartphone Microscope Camera for Context-Aware Contact Surface Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610921",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "abstract": "\"The primary focus of this research is the discreet and subtle everyday contact interactions between mobile phones and their surrounding surfaces. Such interactions are anticipated to facilitate mobile context awareness, encompassing aspects such as dispensing medication updates, intelligently switching modes (e.g., silent mode), or initiating commands (e.g., deactivating an alarm). We introduce MicroCam, a contact-based sensing system that employs smartphone IMU data to detect the routine state of phone placement and utilizes a built-in microscope camera to capture intricate surface details. In particular, a natural dataset is collected to acquire authentic surface textures in situ for training and testing. Moreover, we optimize the deep neural network component of the algorithm, based on continual learning, to accurately discriminate between object categories (e.g., tables) and material constituents (e.g., wood). Experimental results highlight the superior accuracy, robustness and generalization of the proposed method. Lastly, we conducted a comprehensive discussion centered on our prototype, encompassing topics such as system performance and potential applications and scenarios.\" https://doi.org/10.1145/3610921",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 128752
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "Fife",
              "institution": "University of St Andrews"
            }
          ],
          "personId": 128761
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "sydney",
              "institution": "UNSW"
            }
          ],
          "personId": 128760
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "CSE"
            }
          ],
          "personId": 128759
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "city": "Sydney",
              "institution": "University of New South Wales"
            }
          ],
          "personId": 128758
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Syndey",
              "institution": "UNSW"
            }
          ],
          "personId": 128757
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Sydney",
              "institution": "CSIRO’s Data61"
            }
          ],
          "personId": 128756
        }
      ]
    },
    {
      "id": 128425,
      "typeId": 12924,
      "title": "PulmoListener: Continuous Acoustic Monitoring of Chronic Obstructive Pulmonary Disease in the Wild",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610889",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "abstract": "\"Prior work has shown the utility of acoustic analysis in controlled settings for assessing chronic obstructive pulmonary disease (COPD) --- one of the most common respiratory diseases that impacts millions of people worldwide. However, such assessments require active user input and may not represent the true characteristics of a patient's voice. We propose PulmoListener, an end-to-end speech processing pipeline that identifies segments of the patient's speech from smartwatch audio collected during daily living and analyzes them to classify COPD symptom severity. To evaluate our approach, we conducted a study with 8 COPD patients over 164 ± 92 days on average. We found that PulmoListener achieved an average sensitivity of 0.79 ± 0.03 and a specificity of 0.83 ± 0.05 per patient when classifying their symptom severity on the same day. PulmoListener can also predict the severity level up to 4 days in advance with an average sensitivity of 0.75 ± 0.02 and a specificity of 0.74 ± 0.07. The results of our study demonstrate the feasibility of leveraging natural speech for monitoring COPD in real-world settings, offering a promising solution for disease management and even diagnosis.\" https://doi.org/10.1145/3610889",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 128614
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 128611
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University Health Network"
            }
          ],
          "personId": 128610
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "Sunnybrook Health Sciences Centre"
            }
          ],
          "personId": 128609
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 128607
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto"
            }
          ],
          "personId": 128649
        }
      ]
    },
    {
      "id": 128426,
      "typeId": 12924,
      "title": "Fast Radio Map Construction with Domain Disentangled Learning for Wireless Localization",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610922",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "abstract": "https://doi.org/10.1145/3610922",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "city": "Guangzhou",
              "institution": "Sun Yat-Sen University"
            }
          ],
          "personId": 128805
        },
        {
          "affiliations": [
            {
              "country": "China",
              "city": "GuangZhou",
              "institution": "School of Computer Science and Engineering, Sun Yat-Sen University"
            }
          ],
          "personId": 128803
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Zhuhai",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128821
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Guangdong",
              "city": "Guangzhou",
              "institution": "Sun Yat-sen University"
            }
          ],
          "personId": 128819
        }
      ]
    },
    {
      "id": 128427,
      "typeId": 12924,
      "durationOverride": 9,
      "title": "Automated Face-To-Face Conversation Detection on a Commodity Smartwatch with Acoustic Sensing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610882",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "abstract": "\"Understanding social interactions is relevant across many domains and applications, including psychology, behavioral sciences, human computer interaction, and healthcare. In this paper, we present a practical approach for automatically detecting face-to-face conversations by leveraging the acoustic sensing capabilities of an off-the-shelf, unmodified smartwatch. Our proposed framework incorporates feature representations extracted from different neural network setups and shows the benefits of feature fusion. The framework does not require an acoustic model specifically trained to the speech of the individual wearing the watch or of those nearby. We evaluate our framework with 39 participants in 18 homes in a semi-naturalistic study and with four participants in free living, obtaining an F1 score of 83.2% and 83.3% respectively for detecting user's conversations with the watch. Additionally, we study the real-time capability of our framework by deploying a system on an actual smartwatch and discuss several strategies to improve its practicality in real life. To support further work in this area by the research community, we also release our annotated dataset of conversations.\" https://doi.org/10.1145/3610882",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin"
            }
          ],
          "personId": 129456
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin"
            }
          ],
          "personId": 129454
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "The University of Texas at Austin"
            }
          ],
          "personId": 129471
        }
      ]
    },
    {
      "id": 128428,
      "typeId": 12924,
      "title": "N-euro Predictor: A Neural Network Approach for Smoothing and Predicting Motion Trajectory",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610884",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "abstract": "\"Jitter and lag severely impact the smoothness and responsiveness of user experience on vision-based human-display interactive systems such as phones, TVs, and VR/AR. Current manually-tuned filters for smoothing and predicting motion trajectory struggle to effectively address both issues, especially for applications that have a large range of movement speed. To overcome this, we introduce N-euro, a residual-learning-based neural network predictor that can simultaneously reduce jitter and lag while maintaining low computational overhead. Compared to the fine-tuned existing filters, N-euro improves prediction performance by 36% and smoothing performance by 42%. We fabricated a Fish Tank VR system and an AR mirror system and conducted a user experience study (n=34) with the real-time implementation of N-euro. Our results indicate that the N-euro predictor brings a statistically significant improvement in user experience. With its validated effectiveness and usability, we expect this approach to bring a better user experience to various vision-based interactive systems.\"https://doi.org/10.1145/3610884",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University"
            }
          ],
          "personId": 128658
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Snap Inc."
            }
          ],
          "personId": 128657
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Snap Research"
            }
          ],
          "personId": 128655
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Snap Research"
            }
          ],
          "personId": 128653
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Snap Research"
            }
          ],
          "personId": 128651
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Snap"
            }
          ],
          "personId": 128686
        }
      ]
    },
    {
      "id": 128429,
      "typeId": 12924,
      "title": "PATCH: A Plug-in Framework of Non-blocking Inference for Distributed Multimodal System",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3610885",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "abstract": "\"Recent advancements in deep learning have shown that multimodal inference can be particularly useful in tasks like autonomous driving, human health, and production line monitoring. However, deploying state-of-the-art multimodal models in distributed IoT systems poses unique challenges since the sensor data from low-cost edge devices can get corrupted, lost, or delayed before reaching the cloud. These problems are magnified in the presence of asymmetric data generation rates from different sensor modalities, wireless network dynamics, or unpredictable sensor behavior, leading to either increased latency or degradation in inference accuracy, which could affect the normal operation of the system with severe consequences like human injury or car accident. In this paper, we propose PATCH, a framework of speculative inference to adapt to these complex scenarios. PATCH serves as a plug-in module in the existing multimodal models, and it enables speculative inference of these off-the-shelf deep learning models. PATCH consists of 1) a Masked-AutoEncoder-based cross-modality imputation module to impute missing data using partially-available sensor data, 2) a lightweight feature pair ranking module that effectively limits the searching space for the optimal imputation configuration with low computation overhead, and 3) a data alignment module that aligns multimodal heterogeneous data streams without using accurate timestamp or external synchronization mechanisms. We implement PATCH in nine popular multimodal models using five public datasets and one self-collected dataset. The experimental results show that PATCH achieves up to 13% mean accuracy improvement over the state-of-art method while only using 10% of training data and reducing the training overhead by 73% compared to the original cost of retraining the model.\" https://doi.org/10.1145/3610885",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129142
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129139
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129185
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129182
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129179
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129406
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Tokyo",
              "institution": "RIKEN AIP"
            }
          ],
          "personId": 129403
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "East Lansing",
              "institution": "Michigan State University"
            }
          ],
          "personId": 129400
        }
      ]
    },
    {
      "id": 128430,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Discussion Panel",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "58cf8205-bef7-4606-8d69-a0a5702edf8e",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128431,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "a534a6bd-7c4d-4534-9a19-e81bce237113",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127784
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128433,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "2677fb07-6d71-470b-bd2f-db6458fc3b79",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128434,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "eee38fa2-15e5-4085-bf35-4eea209cd4c5",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127785
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128435,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "7da8fdef-dc86-41ed-a57b-4ee462bff03c",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128436,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "23c77652-8aba-4b0b-a9ed-b80114c690f7",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127786
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128437,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "508aeaba-0018-4dc5-b349-d789388951f6",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128438,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "bfd9e210-a8ec-41bc-8f4a-afecb81c8921",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127788
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128439,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "17ba9d35-6098-4790-b107-d3720819649a",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128440,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "433b9f3c-0df6-440b-b739-5da20c21ff8c",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127789
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128442,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "e90ce122-dd9f-4a35-980e-c42aabee66bb",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128443,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "d7d3a50e-7842-4604-8f74-fe393ec4206d",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127790
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128444,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "3a09e997-58cb-453d-ba14-39f4c961a612",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128445,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "948f013c-b673-4f96-8863-ff08abd2b667",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127793
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128446,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "64e789ff-c7ce-4e91-ab9a-52f8c625ef32",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128447,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "993f1ef7-69ed-4775-800d-f0f940f1100d",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127794
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128448,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "b0f47020-5dec-4013-a0e2-5515ea4a245d",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128449,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "a1f6c751-f52e-41a7-809a-75577d019ac3",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127802
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128450,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "afeea863-622f-4f9d-ae91-0703356ec2ba",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128451,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "3f93369d-c617-4b4e-8a76-e6db1332a53e",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127803
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128452,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "ef8719be-3794-47b9-b31f-436e5537e6f5",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128453,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "da81a04c-08cd-4e67-a6ad-c0ce30e9a169",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127804
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128454,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "f58d2315-a43f-40d3-bc58-9da2fc4cc616",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128455,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "ba270ae4-653d-427b-a110-1531d9444148",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127806
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128456,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "427739d5-01a3-4f3e-8e28-e6ba2fa13c63",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128457,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "341400a7-2409-4110-b576-d245317067c6",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127807
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128458,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "99a910c1-df37-4f47-93ca-0dd78fe0602c",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128459,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "7649be0f-ee4c-4b97-a1df-46f30a7dccb2",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127808
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128460,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "4088eac0-8071-4c53-8f6c-fded765bed5b",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128461,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Panel Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "55c532dd-f86e-4d0a-9f5d-7e1b47fcb844",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127810
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128462,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "3d9ebe38-2ddb-48d0-924e-19b98ffbb4db",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128463,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "0ec3452f-379d-46ca-a1b5-d8ca7bf1e550",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127811
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128464,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "05491277-9c17-48a2-8ec2-e538af667e76",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128465,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "98cbc1f3-481e-4ed3-a9ee-36397c4aa57f",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127812
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128466,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "b5eeeb9e-afcf-4eb7-b694-66db4448add4",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128467,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "8ac4384b-4c07-4bda-ae73-c502f343ca83",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127814
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128468,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "954b65f6-6963-49c3-b3aa-0c401a1730d4",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128469,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "769597bb-e64b-4c65-80cc-221e150a85f8",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127815
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128470,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "e282e8af-6b39-4c88-a62f-55d8e08e11c7",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128471,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "35ed2416-c43f-4121-8919-927fc9888a7f",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127816
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128472,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "a035bae2-7370-449b-80ea-5502785ce6d5",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128473,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "8637bec7-a20c-4a1e-907a-b6671232e618",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127818
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128474,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "f9ddf08d-ee24-4ff8-be4b-83e21ec3cd05",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128475,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "7340e069-f2cf-43d2-9dfc-64879d277c7e",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127819
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128476,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "f7948e24-31ba-4c11-9379-21e299a833f1",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128477,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "79573a49-68bc-474c-bddc-7afc807246b0",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127820
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128478,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "a97503cd-d720-4632-95a9-2899b1feb049",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128479,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "b3719443-e8ec-4814-a649-d468acab0e42",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127822
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128480,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "0fbca65b-0fca-468a-8ff4-e8fd85186e04",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128481,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "54cd8d7c-4a17-4ef4-a39a-b931022cfd36",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128482,
      "typeId": 12928,
      "durationOverride": 20,
      "title": "Panel Discussion",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "912febcd-eec1-4a53-8e4d-b2f07aa1d651",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128483,
      "typeId": 12928,
      "durationOverride": 10,
      "title": "Poster Presentations",
      "recognitionIds": [],
      "isBreak": true,
      "importedId": "02402e6f-d964-4286-bc53-d9a95668e64e",
      "source": "SYS",
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 128562,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "SOlving the sensor-based Activity Recognition problem (SOAR): self-supervised, multi-modal recognition of activities from wearable sensors",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-8870",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121929
      ],
      "eventIds": [],
      "abstract": "Feature extraction lies at the core of Human Activity Recognition (HAR): the automated inference of what activity is being performed. Traditionally, the HAR community used statistical metrics and distribution-based representations to summarize the movement present in windows of sensor data into feature vectors. More recently, learned representations have been used successfully in lieu of such handcrafted and manually engineered features. In particular, the community has shown substantial interest in self-supervised methods, which leverage large-scale unlabeled data to first learn useful representations that are subsequently fine-tuned to the target applications. In this tutorial, we focus on representations for single-sensor and multi-modal setups, and go beyond the current de facto of learning representations. We also discuss the economic use of existing representations, specifically via transfer learning and domain adaptation. The proposed tutorial will introduce state-of-the-art methods for representation learning in HAR, and provide a forum for researchers from mobile and ubiquitous computing to not only discuss the current state of the field but to also chart future directions for the field itself, including answering what it would take to finally solve the activity recognition problem.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Electrical and Computer Engineering"
            }
          ],
          "personId": 121604
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 128558
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technical Univeristy of Kaiserslautern",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Embedded Intelligence"
            }
          ],
          "personId": 121506
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 121494
        }
      ]
    },
    {
      "id": 128563,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "FairComp: Workshop on Fairness and Robustness in Machine Learning for Ubiquitous Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-7781",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121925
      ],
      "eventIds": [],
      "abstract": "How can we ensure that Ubiquitous Computing (UbiComp) research outcomes are both ethical and fair? While fairness in machine learning (ML) has gained traction in recent years, fairness in UbiComp remains unexplored. This workshop aims to discuss fairness in UbiComp research and its social, technical, and legal implications. From a social perspective, we will examine the relationship between fairness and UbiComp research and identify pathways to ensure that ubiquitous technologies do not cause harm or infringe on individual rights. From a technical perspective, we will initiate a discussion on data collection and modeling practices to develop bias mitigation approaches tailored to UbiComp research. From a legal perspective, we will examine how new policies and regulations shape our community's work and future research. We aim to foster a vibrant community centered around the topic of responsible UbiComp, while also charting a clear path for future research endeavours in this field.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Thessaloniki",
              "institution": "Aristotle University of Thessaloniki",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 128489
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": ""
            }
          ],
          "personId": 128485
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Computer Science"
            }
          ],
          "personId": 128522
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Computer Lab"
            }
          ],
          "personId": 128551
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Aalborg",
              "institution": "Aalborg University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 128534
        }
      ]
    },
    {
      "id": 128564,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "CPD 2023: The 6th International Workshop on Combining Physical and Data-Driven Knowledge in Ubiquitous Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-7884",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121846
      ],
      "eventIds": [],
      "abstract": "With the proliferation of connected devices with advanced sensing, computing, and communication capabilities, ubiquitous computing systems have become prevalent nowadays. They have the potential to revolutionize various industries by enabling new applications and services (e.g., patient monitoring, personalized recommendations, traffic control, home energy management). However, in real-world ubiquitous computing systems, data collection can be expensive or impossible. Due to the limited quantity and quality of data available, pure data-driven methods may not perform well. A promising approach to overcome these limitations is to utilize physical knowledge, including domain knowledge from experts, heuristics based on experience, and analytic models of physical phenomena.\r\n\r\nThe theme of this workshop is to advance the theoretical understanding, algorithmic development and system implementations of ubiquitous computing systems that integrate physical knowledge with data-driven methods. The workshop will provide an inclusive gathering for researchers and practitioners from various fields and facilitate future collaborations. The workshop welcomes research papers as well as position papers. The overall goal is to grow the community who are dedicated to improving ubiquitous computing systems by fusing physical knowledge into data-driven methods.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Hawaii",
              "city": "Honolulu",
              "institution": "University of Hawaii at Manoa",
              "dsl": ""
            }
          ],
          "personId": 128491
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Coral Gables",
              "institution": "University of Miami",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 128502
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 128538
        }
      ]
    },
    {
      "id": 128565,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "The Third Workshop on Multiple Input Modalities and Sensations for VR/AR Interactions (MIMSVAI)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-7256",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121927
      ],
      "eventIds": [],
      "abstract": "Rapid technological advances are expanding the practical applicability of virtual reality and/or augmented reality (VR/AR); however, the degree to which new users are able to interact with these technologies is limited by current input modalities. Gaining an intuitive grasp of VR/AR applications requires that users be immersed in the virtual environment, which in turn depends on the integration of multiple realistic sensory feedback mechanisms. This workshop will bring together researchers from the fields of UbiComp and VR/AR to explore alternative input modalities and sensory feedback systems to facilitate the design of coherent and engaging VR/AR experiences comparable to those in the real world. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": "Graduate Institute of Art and Technology"
            }
          ],
          "personId": 128508
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai Jiao Tong University",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 128546
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 128494
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Tsing Hua University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 128490
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin ",
              "institution": "University of Texas at Austin",
              "dsl": "Computer Science department"
            }
          ],
          "personId": 128521
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Corma New Media",
              "dsl": ""
            }
          ],
          "personId": 128524
        }
      ]
    },
    {
      "id": 128566,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "11th InternationalWorkshop on Human Activity Sensing Corpus and Applications (HASCA)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-5125",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121844
      ],
      "eventIds": [],
      "abstract": "The recognition of complex and subtle human behaviors from wearable sensors will enable next-generation human-oriented computing in scenarios of high societal value (e.g., dementia care). This will require a large-scale human activity corpus and much-improved methods to recognize activities and the context in which they occur. This workshop deals with the challenges of designing reproducible experimental setups, running large-scale dataset collection campaigns, designing activity and context recognition methods that are robust and adaptive, and evaluating systems in the real world. We wish to reflect on future methods, such as lifelong learning approaches that allow open-ended activity recognition. This year HASCA will welcome papers from participants to the Fifth Sussex-Huawei Locomotion and Transportation Recognition Challenge in a special session.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Shiga",
              "institution": "Ritsumeikan University",
              "dsl": ""
            }
          ],
          "personId": 121569
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": ""
            }
          ],
          "personId": 128509
        },
        {
          "affiliations": [
            {
              "country": "Macedonia, The Former Yugoslav Republic of",
              "state": "",
              "city": "Skopje",
              "institution": "Ss. Cyril and Methodius University in Skopje",
              "dsl": ""
            }
          ],
          "personId": 128500
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Concordia University",
              "dsl": ""
            }
          ],
          "personId": 128526
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kitakyushu city",
              "institution": "Kyushu Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 128499
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Oulu",
              "institution": "University of Oulu",
              "dsl": "University of Oulu"
            }
          ],
          "personId": 128510
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 128519
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Freiburg",
              "institution": "University of Freiburg",
              "dsl": ""
            }
          ],
          "personId": 128493
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "East Sussex",
              "city": "Brighton",
              "institution": "University of Sussex",
              "dsl": "Wearable Technologies Lab, Sensor Technology Research Centre"
            }
          ],
          "personId": 128495
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Aichi",
              "city": "Nagoya",
              "institution": "Nagoya University",
              "dsl": "Graduate School of Engineering"
            }
          ],
          "personId": 128539
        }
      ]
    },
    {
      "id": 128567,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "EarComp 2023: Fourth International Workshop on Earable Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-8435",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121928
      ],
      "eventIds": [],
      "abstract": "The objective of the 4th ACM International Workshop on Earable Computing (EarComp 2023) is to provide an academic forum and bring together researchers, practitioners, and design experts to discuss how sensory earables technologies have and can complement human sensing research. It also aims to provide a launchpad for bold and visionary ideas and serve as a catalyst for advancements in this emerging new Earable Computing research space.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 128552
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 128535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland College Park",
              "dsl": "Computer Science"
            }
          ],
          "personId": 128486
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Southampton",
              "city": "Southampton",
              "institution": "University of Southampton",
              "dsl": ""
            }
          ],
          "personId": 128559
        }
      ]
    },
    {
      "id": 128568,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "Ubicomp Tutorial - UbiCHAI -  Experimental Methodologies for Cognitive Human Augmentation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-5699",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121930
      ],
      "eventIds": [],
      "abstract": "A central research goal of Ubicomp has always been the devel-\r\nopment of systems and methods that seamlessly support humans\r\nin accomplishing complex tasks in everyday life. In the wake of\r\nrapid advances in artificial intelligence (AI), topics such as \"Human-\r\nCentered AI\" and \"Hybrid Human AI\" are showing a growing in-\r\nterest in this very research that puts us humans and our needs at\r\nthe center of artificial intelligence. While methods for augmenting\r\nthe human body and the impact of these augmentations on human\r\nphysical life are being extensively researched, there has been very\r\nlimited progress in evaluating the impact on human cognitive per-\r\nception and its impact on the overall outcome of augmentations\r\nto the human body. In this tutorial, we will address the question\r\nof how to evaluate the cognitive impact of human augmentation.\r\nWe will address the different levels of cognitive effects, how to\r\nmeasure which methods of augmentation have the best effect, and\r\nwhich cognitive measures have the greatest impact on augmen-\r\ntation, and we will give the audience the opportunity to test and\r\nevaluate cognitive human augmentation systems themselves",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Embedded Intelligence",
              "dsl": "DKFI"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Embedded Intelligence",
              "dsl": "DKFI"
            }
          ],
          "personId": 121641
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 128561
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technische Universität Kaiserslautern",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "Technische Universität Kaiserslautern",
              "dsl": ""
            }
          ],
          "personId": 128515
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Goldsmiths University of London",
              "dsl": "Computing"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Institute of Cognitive Neuroscience"
            }
          ],
          "personId": 121594
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Kaiserslautern",
              "institution": "DFKI",
              "dsl": ""
            }
          ],
          "personId": 121468
        }
      ]
    },
    {
      "id": 128569,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "ARDUOUS: Tutorial on Annotation of useR Data for UbiquitOUs Systems - Developing a Data Annotation Protocol",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-9868",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121856
      ],
      "eventIds": [],
      "abstract": "Data annotation is key to a large number of fields, including ubiquitous computing. Documenting the quality and extent of annotation is increasingly recognised as an important aspect of understanding the validity, biases and limitations of systems built using this data: hence, it is also relevant to regulatory and compliance needs and outcomes. However, the process of annotation often receives little attention, and is characterised in the literature as ‘under-described’ and ‘invisible work’. In this tutorial, we bring together existing resources and methods to present a framework for the iterative development and evaluation of an annotation protocol, from requirements gathering, setting scope, development, documentation, piloting and evaluation, through to scaling-up annotation processes for a production annotation process. We also explore the potential of semi-supervised approaches and state-of-the-art methods such as the use of generative AI in supporting annotation workflows, and how such approaches are validated and their strengths and weaknesses characterised. This tutorial is designed to be suitable for people from a wide range of backgrounds, as annotation can be understood as a highly interdisciplinary task and often requires collaboration with subject matter experts from relevant fields. Participants will trial and evaluate a selection of annotation interfaces and walk through the process of evaluating the outcomes. By the end of the tutorial, participants will develop a deeper understanding of the task of developing an annotation protocol and aspects of the requirements and context which should be taken into account. Presentations and code from this event will be shared openly on a Github repository.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            }
          ],
          "personId": 128550
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Bristol",
              "institution": "University of Bristol",
              "dsl": ""
            }
          ],
          "personId": 128513
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Greifswald",
              "institution": "University of Greifswald",
              "dsl": "Data Science"
            }
          ],
          "personId": 128496
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Greifswald",
              "institution": "University of Greifswald",
              "dsl": "Institute for Data Science"
            }
          ],
          "personId": 128523
        }
      ]
    },
    {
      "id": 128570,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "First International Symposium on Inclusive and Equitable Ubiquitous Computing (UbiComp4All)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-8827",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121841
      ],
      "eventIds": [],
      "abstract": "We propose a symposium to foster research on inclusive and equitable ubiquitous computing in Latin America (LATAM). The meeting is inspired by the 1st CSCW@LatAm Research Catalyst Workshop, the ACM SIGCHI Across Borders Initiative. We aim to: 1) discuss opportunities and challenges of defining and accomplishing inclusive and equitable ubiquitous computing centered on LATAM; 2) collaboratively mentor emerging related projects focused on LATAM; 3) create a space that encourages cross-country collaborations to address together common research challenges, whilst also strengthening the links among all the members, regardless of their level of expertise in the field; and, 4) provide a forum for dialogue between local communities in Latin America (e.g., educational and health workers, NGOs) and UbiComp researchers, with the aim of identifying pathways for codesign-focused collaborations between diverse groups of stakeholders on problems of public interest.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Colima",
              "institution": "Universidad de Colima",
              "dsl": "School of Mechanical and Electrical Engineering"
            }
          ],
          "personId": 128514
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "Colima",
              "city": "Colima",
              "institution": "University of Colima",
              "dsl": "School of Telematics"
            }
          ],
          "personId": 128512
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "Rondonopolis",
              "institution": "Universidade Federal de Mato Grosso",
              "dsl": "Sistemas de Informação"
            }
          ],
          "personId": 128543
        },
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "University of Chile",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 128511
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Puebla",
              "institution": "LANIA",
              "dsl": ""
            }
          ],
          "personId": 128503
        },
        {
          "affiliations": [
            {
              "country": "Mexico",
              "state": "",
              "city": "Morelia",
              "institution": "Universidad Michoacana",
              "dsl": ""
            }
          ],
          "personId": 128492
        }
      ]
    },
    {
      "id": 128571,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "8th International Workshop on Mental Health and Well-being: Sensing and Intervention",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-6802",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121845
      ],
      "eventIds": [],
      "abstract": "Mental health and well-being are critical components of overall health: suffering from a mental illness can be both debilitating and life-threatening for individuals experiencing symptoms. Detecting symptoms of mental illness early-on and delivering interventions to prevent and/or manage symptoms can improve health and well-being outcomes. Ubiquitous systems are increasingly playing a central role in uncovering clinically relevant contextual information on mental health. Research shows that these systems can passively measure symptoms and enable opportunities to deliver intervention. However, despite this potential, the uptake of ubiquitous technologies into mental healthcare has been slow, and a number of challenges need to be addressed towards the effective implementation of these tools. The goal of this workshop is to bring together researchers, practitioners, and industry professionals interested in identifying, articulating, and addressing such issues and opportunities. Following the success of this workshop for the last seven years, we aim to continue facilitating the UbiComp community in both the conceptualization, translation, and implementation of novel approaches for sensing and intervention in the context of mental health.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Information Science"
            }
          ],
          "personId": 128504
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Information School"
            }
          ],
          "personId": 128507
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 128497
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": "Department of Electrical and Computer Engineering"
            }
          ],
          "personId": 128548
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "State College",
              "institution": "The Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 128556
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 128532
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Kgs. Lyngby",
              "institution": "Technical University of Denmark",
              "dsl": "Department of Health Technology"
            }
          ],
          "personId": 128545
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Hampshire",
              "city": "Hanover",
              "institution": "Dartmouth College",
              "dsl": "Thayer School of Engineering"
            }
          ],
          "personId": 128531
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 128488
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 128537
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell University",
              "dsl": "Information Science"
            }
          ],
          "personId": 128554
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "Cornell Tech",
              "dsl": ""
            }
          ],
          "personId": 128536
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California San Diego",
              "dsl": "Halıcıoğlu Data Science Institute"
            }
          ],
          "personId": 128518
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Houston",
              "institution": "Rice University",
              "dsl": ""
            }
          ],
          "personId": 128557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts Amherst",
              "dsl": "Manning College of Information and Computer Sciences"
            }
          ],
          "personId": 128560
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Syracuse",
              "institution": "Syracuse University ",
              "dsl": "Electrical Engineering and Computer Science "
            }
          ],
          "personId": 128549
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minnetonka",
              "institution": "Optum Labs",
              "dsl": "Digital Signals Group"
            }
          ],
          "personId": 128533
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Allen School of Computer Science and Engineering"
            }
          ],
          "personId": 128498
        }
      ]
    },
    {
      "id": 128572,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "Research Methodologies across the Physical - Virtual Reality Spectrum",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-4726",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121857
      ],
      "eventIds": [],
      "abstract": "Over the last couple of years, there has been a big push toward making immersive and mixed technologies available to the general public. Yet, designing for these new technologies is challenging, as users need to position virtual objects in 3D space. \r\nThe current state-of-the art technologies used to access these virtual environments (e.g., Head-mount displays (HMD)s also presents additional challenges for designers when considering depth perception issues that affect user precision. Moreover, these challenges are exacerbated when designers consider accessibility needs of special populations. \r\nTo make new immersive and mixed technologies more accessible, we propose a tutorial at UbiComp / ISWC 2023 to discuss design strategies, research methodologies, and implementation practices with special populations using technologies across the physical-virtual reality spectrum. In this tutorial participants will learn how to make these technologies more accessible by (1) teaching students of the tutorial how to design, prototype, and evaluate these technologies using empirical research. We aim to (2) bring together researchers, practitioners, and students who are interested in making immersive and mixed technologies more accessible and (3) identify common problems when designing new user interfaces. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "HCI4Good, Faculty of Computer Science"
            }
          ],
          "personId": 128528
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 128530
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 128525
        }
      ]
    },
    {
      "id": 128574,
      "typeId": 12927,
      "durationOverride": 210,
      "title": "Digital Therapeutics Evolution  What kind of research will make the difference in this area?",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-2017",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121847
      ],
      "eventIds": [],
      "abstract": "As in prescribable medical drug-based therapies, Digital Therapeutics (DTx) solutions introduce the use of software as 1) an active ingredient implemented as digital interventions to improve patients’ condition; and 2) as the excipient through which the intervention is conveyed to the patient. The most common DTx solutions implemented until today deal with the use of software-based solutions for dealing with mental health conditions. However, the potential of DTx is envisioned to grow to treat other health conditions with a number of technologies that still have not been duly explored. The objective of this workshop is to discuss the opportunities, barriers, and challenges of future DTx, the envisioned future use-cases, and the technologies with high potential to drive the uptake of these technologies, and this way to position this field as a solid complement of today’s therapeutic approaches for dealing with different health conditions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Trento",
              "institution": "Fondazione Bruno Kessler Research Institute",
              "dsl": ""
            }
          ],
          "personId": 128553
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Potsdam",
              "institution": "Hasso Plattner Institute",
              "dsl": "Digital Health - Connected Healthcare"
            }
          ],
          "personId": 128505
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Universidade de Lisboa",
              "dsl": "LASIGE, Faculdade de Ciências"
            }
          ],
          "personId": 128506
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculty of Sciences, University of Lisbon",
              "dsl": "LASIGE Lab"
            }
          ],
          "personId": 128501
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Ljubljana",
              "institution": "Jožef Stefan Institute",
              "dsl": "Department of Intelligent Systems"
            }
          ],
          "personId": 128517
        },
        {
          "affiliations": [
            {
              "country": "Macedonia, The Former Yugoslav Republic of",
              "state": "",
              "city": "Skopje",
              "institution": "Ss. Cyril and Methodius University in Skopje",
              "dsl": ""
            }
          ],
          "personId": 128500
        }
      ]
    },
    {
      "id": 128575,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "WellComp 2023: Sixth International Workshop on Computing for Well-Being",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-3655",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121926
      ],
      "eventIds": [],
      "abstract": "With the advancements in ubiquitous computing, ubicomp technology has deeply spread into our daily lives, including office work, home, and house-keeping, health management, transportation, and even urban living environments. Furthermore, beyond the initial metrics commonly applied in computing, such as “efficiency” and “productivity”, the benefits that people (users) get from well-being-aware ubiquitous technology have been greatly emphasized in recent years. Through the sixth “WellComp” (Computing for Well-being) workshop, we will discuss and debate the contribution of ubiquitous computing towards users’ well-being covering physical, mental, and social wellness (and the combinations thereof), from the viewpoints of various different layers of computing. Organized by a diverse international team of ubicomp researchers, WellComp 2023 will bring together researchers and practitioners from both academia and industry to explore versatile topics related to well-being and ubiquitous computing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zürich",
              "institution": "ETH Zürich",
              "dsl": "ETH AI Center"
            }
          ],
          "personId": 128527
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridgeshire",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 128485
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Nokia Bell Labs",
              "dsl": ""
            }
          ],
          "personId": 128542
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Ubiquitous Computing"
            }
          ],
          "personId": 128541
        }
      ]
    },
    {
      "id": 128576,
      "typeId": 12927,
      "durationOverride": 510,
      "title": "ASSET 2023 - Americas Student Symposium on Emerging Technologies at UbiComp/ISWC 2023",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "ubicomp23a-3681",
      "source": "PCS",
      "trackId": 12439,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121842
      ],
      "eventIds": [],
      "abstract": "UbiComp/ISWC 2023 is featuring a special program called ASSET (Americas Student Symposium on Emerging Technologies) as part of SIGMOBILE’s student outreach program. Student participating to ASSET will be divided into small groups and each group will be provided with an excellent and dynamic research mentor who will help them hone their research skills.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": "School of Information Systems"
            }
          ],
          "personId": 128540
        }
      ]
    },
    {
      "id": 128577,
      "typeId": 13119,
      "durationOverride": 390,
      "title": "GenAI4PC: Generative AI for Pervasive Computing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500000",
      "source": "CSV",
      "trackId": 12440,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121924
      ],
      "eventIds": [],
      "abstract": "Generative models – most prominently Large language Models (LLMs) for images and text – have been a major computing advance of 2023. For Ubiquitous Computing, these models offer tantalizing opportunities for personalization, data interpretation and more complex and meaningful interactions with humans.In this new era of AI there are many open problems, including how we assess them and how we deploy them both efficiently and safely. The Generative AI for Pervasive Computing Symposium (GenAI4PC) aims to bring together both industrial and academic Ubiquitous Computing, researchers to explore these issues in a format that is focused on encouraging discussion and debate.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Google"
            },
            {
              "country": "United Kingdom",
              "city": "Cambridge",
              "institution": "University of Cambridge"
            }
          ],
          "personId": 129267
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Google"
            }
          ],
          "personId": 129266
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "city": "London",
              "institution": "Google"
            }
          ],
          "personId": 129265
        }
      ]
    },
    {
      "id": 129557,
      "typeId": 12928,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500010",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127809
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129558,
      "typeId": 13126,
      "title": "Award Ceremony",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500001",
      "source": "CSV",
      "trackId": 12448,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129535
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129559,
      "typeId": 12928,
      "durationOverride": 90,
      "title": "Lunch Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500012",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129528
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129560,
      "typeId": 12928,
      "durationOverride": 90,
      "title": "Lunch Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500011",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129529
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129561,
      "typeId": 13122,
      "title": "Panel on Resilience in an Era of Destruction",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500003",
      "source": "CSV",
      "trackId": 12443,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127800
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129562,
      "typeId": 13125,
      "durationOverride": 180,
      "title": "Gala Dinner",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500014",
      "source": "CSV",
      "trackId": 12446,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127813
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129563,
      "typeId": 13122,
      "durationOverride": 75,
      "title": "Closing Keynote -  Conference Closing",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500002",
      "source": "CSV",
      "trackId": 12443,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127826
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129564,
      "typeId": 12928,
      "durationOverride": 90,
      "title": "Lunch Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500013",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        129530
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129565,
      "typeId": 13122,
      "title": "Welcome and Opening Keynote",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500005",
      "source": "CSV",
      "trackId": 12443,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127782
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129566,
      "typeId": 13124,
      "durationOverride": 120,
      "title": "Gadget Show",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500016",
      "source": "CSV",
      "trackId": 12449,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127799
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129567,
      "typeId": 13122,
      "durationOverride": 90,
      "title": "UbiComp/ISWC TownHall",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500004",
      "source": "CSV",
      "trackId": 12443,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127821
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129568,
      "typeId": 13125,
      "durationOverride": 180,
      "title": "Opening Reception",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500015",
      "source": "CSV",
      "trackId": 12446,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127795
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129569,
      "typeId": 12928,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500007",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127817
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129570,
      "typeId": 13123,
      "durationOverride": 90,
      "title": "N2Women Luncheon (only registered guests)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500018",
      "source": "CSV",
      "trackId": 12447,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127787
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129571,
      "typeId": 12928,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500006",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127791
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129572,
      "typeId": 13121,
      "durationOverride": 510,
      "title": "Doctoral Colloquium",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500017",
      "source": "CSV",
      "trackId": 12445,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        121859
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129573,
      "typeId": 12928,
      "durationOverride": 45,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500009",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127783
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129574,
      "typeId": 12928,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500008",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127801
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129575,
      "typeId": 13123,
      "durationOverride": 90,
      "title": "Diversity Luncheon (only registered guests)",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500019",
      "source": "CSV",
      "trackId": 12447,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127805
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129576,
      "typeId": 12928,
      "durationOverride": 45,
      "title": "Coffee Break",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "fc3f53a0-1771-4dff-a318-ade64e4d0ebf",
      "source": "CSV",
      "trackId": 12444,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127825
      ],
      "eventIds": [],
      "authors": []
    },
    {
      "id": 129690,
      "typeId": 12924,
      "title": "A Design Space for Human Sensor and Actuator Focused In-Vehicle Interaction Based on a Systematic Literature Review",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500021",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 129682
        },
        {
          "affiliations": [],
          "personId": 129681
        },
        {
          "affiliations": [],
          "personId": 129680
        }
      ]
    },
    {
      "id": 129691,
      "typeId": 12924,
      "title": "I Want to Know Your Hand: Authentication on Commodity Mobile Phones Based on Your Hand's Vibrations",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500020",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127824
      ],
      "eventIds": [],
      "abstract": "We present HoldPass, the first system that can authenticate a user while they simply hold their phone. It uses the heart activity as biometric trait sensed via the hand vibrations in response to the cardiac cycle - a process known as ballistocardiography (BCG). While heart activity has been used for biometric authentication, sensing it through hand-based ballistocardiography (Hand-BCG) using standard sensors found on commodity mobile phones is an uncharted territory. Using a combination of in-depth qualitative analysis and large-scale quantitative analysis involving over 100 volunteers, we paint a detailed picture of opportunities and challenges. Authentication based on Hand-BCG is shown to be feasible but the signal is weak, uniquely prone to motion artifacts and does not land itself to the common approach of alignment-based authentication. HoldPass addresses these challenges by introducing a novel alignment-free authentication scheme that builds on asynchronous signal slicing and a data-driven algorithm for identifying a reduced set of features for characterizing a user. We implement HoldPass and evaluate it using a multi-modal approach: a large-case study involving 112 volunteers and targeted studies with a smaller set of volunteers over a period of several months. The data shows that HoldPass provides an authentication accuracy and user experience on par with or better than state-of-the-art systems with stronger requirements on hardware and/or user participation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 129685
        },
        {
          "affiliations": [],
          "personId": 129684
        },
        {
          "affiliations": [],
          "personId": 129683
        }
      ]
    },
    {
      "id": 129692,
      "typeId": 12924,
      "title": "Automatic Update for Wi-Fi Fingerprinting Indoor Localization via Multi-Target Domain Adaptation",
      "recognitionIds": [],
      "isBreak": false,
      "importedId": "3500022",
      "source": "CSV",
      "trackId": 12438,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        127823
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 129679
        },
        {
          "affiliations": [],
          "personId": 129689
        },
        {
          "affiliations": [],
          "personId": 129688
        },
        {
          "affiliations": [],
          "personId": 129687
        },
        {
          "affiliations": [],
          "personId": 129686
        }
      ]
    }
  ],
  "people": [
    {
      "id": 121426,
      "firstName": "Esther",
      "lastName": "Zahn",
      "middleInitial": "Friederike",
      "importedId": "b8fTNQKrGHLJIMLVEx4-MQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121427,
      "firstName": "Abigail",
      "lastName": "Dalton",
      "middleInitial": "",
      "importedId": "79p_A6bS-HminsUpa2H-9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121428,
      "firstName": "Lucy E.",
      "lastName": "Dunne",
      "middleInitial": "",
      "importedId": "A9c9QlnbKx8O6OBevdkGfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121429,
      "firstName": "Bo",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "QLPz5ycPULDpm2i0tBppXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121430,
      "firstName": "Katia",
      "lastName": "Vega",
      "middleInitial": "",
      "importedId": "nUSPclWZt8TNmJtnExIPYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121431,
      "firstName": "Lily",
      "lastName": "Gabriel",
      "middleInitial": "M",
      "importedId": "Qg2pVjXPatRvpg4kNnuxKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121432,
      "firstName": "Emil",
      "lastName": "Woop",
      "middleInitial": "",
      "importedId": "lyUWDlryGL5rqzGQ9t6VKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121433,
      "firstName": "Chongzhou",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "4rBdizMbxC4xq8tFPLHEtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121434,
      "firstName": "Gozde",
      "lastName": "Goncu Berk",
      "middleInitial": "",
      "importedId": "BqUAkWhAwm8QESln2coCpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121435,
      "firstName": "Niharikha",
      "lastName": "Subash",
      "middleInitial": "",
      "importedId": "dEtDAWZY2R_fC4h6Ob1ASA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121436,
      "firstName": "Daniel",
      "lastName": "Geißler",
      "middleInitial": "",
      "importedId": "HKCOuXd8_wbcRs081VKV2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121437,
      "firstName": "Alireza",
      "lastName": "Golgouneh",
      "middleInitial": "",
      "importedId": "B8XvwYwVkKjbwCDSDrd4Dw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121438,
      "firstName": "Brad",
      "lastName": "Holschuh",
      "middleInitial": "",
      "importedId": "TsUKTnCpyPI8_KVhjf5OMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121439,
      "firstName": "Hymalai",
      "lastName": "Bello",
      "middleInitial": "",
      "importedId": "cc0EWcF1ABMuu__WbHrJKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121440,
      "firstName": "Catherine",
      "lastName": "Caudwell",
      "middleInitial": "",
      "importedId": "dSBDRvq_Pwpc6AjivYPs1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121441,
      "firstName": "Cindy Hsin-Liu",
      "lastName": "Kao",
      "middleInitial": "",
      "importedId": "0MMz0gYWT7ZslB5DbhBWLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121442,
      "firstName": "Shanel",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "pgOjNajAss6z6Wt242cfEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121443,
      "firstName": "Aisha F.",
      "lastName": "Iskanderani",
      "middleInitial": "",
      "importedId": "pfEaKVd_M9uWLUGrZQSIvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121444,
      "firstName": "Mirela",
      "lastName": "Alistar",
      "middleInitial": "",
      "importedId": "t5QI817pMPUqkR93C2vR-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121445,
      "firstName": "Houman",
      "lastName": "Homayoun",
      "middleInitial": "",
      "importedId": "KcMQPguGpA-WQpQB_QUUgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121446,
      "firstName": "Zoe",
      "lastName": "Kaputa",
      "middleInitial": "",
      "importedId": "GaS5uQIEFhD5m8e6xuEPbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121447,
      "firstName": "Robert",
      "lastName": "Pettys-Baker",
      "middleInitial": "",
      "importedId": "5e9kC6tS_zRvbb2tyQXpwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121448,
      "firstName": "Hannah",
      "lastName": "Curran",
      "middleInitial": "",
      "importedId": "QJoZ56E8c3XkXrmuHxVdzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121449,
      "firstName": "Gesche",
      "lastName": "Joost",
      "middleInitial": "",
      "importedId": "kfrdk6UQy_KHfzoXFwOFNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121450,
      "firstName": "galina",
      "lastName": "Mihaleva",
      "middleInitial": "",
      "importedId": "4DwaKq-zLvkZ2kySGCVSjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121451,
      "firstName": "Edgar",
      "lastName": "Rodríguez Ramírez",
      "middleInitial": "R",
      "importedId": "-2D9WoypGpJeBnTJ1GWINQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121452,
      "firstName": "Jingwen",
      "lastName": "Zhu",
      "middleInitial": "",
      "importedId": "J81OwzHjnRk3raOO1ut4xA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121453,
      "firstName": "Ruijie",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "xQbvQR41Sft5zPjpGkHh5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121454,
      "firstName": "Olaitan",
      "lastName": "Adeleke",
      "middleInitial": "",
      "importedId": "3mzAPdGzATdYth-Q4ZXpdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121455,
      "firstName": "Sasha",
      "lastName": "de Koninck",
      "middleInitial": "",
      "importedId": "LKYknp2P7brPkcw4bGmKFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121456,
      "firstName": "Afroditi",
      "lastName": "Psarra",
      "middleInitial": "",
      "importedId": "2hCRKQQPAdlS-ENNzCCA2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121457,
      "firstName": "Morgan",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "a5gCeZ7FizGl87TOZp-S_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121458,
      "firstName": "Shuyi",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "ObYpTimAU_7UAofjnvz5jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121459,
      "firstName": "Alejandra",
      "lastName": "Ruiz",
      "middleInitial": "",
      "importedId": "pSQNxZG6hfkFbuOtgnRcEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121460,
      "firstName": "Lala Shakti Swarup",
      "lastName": "Ray",
      "middleInitial": "",
      "importedId": "zCiooR8j8nxL8sbNsSVpxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121461,
      "firstName": "Eldy",
      "lastName": "Lazaro Vasquez",
      "middleInitial": "S.",
      "importedId": "oV3IQ2mhzdt7avTxyMEXBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121462,
      "firstName": "Laura",
      "lastName": "Devendorf",
      "middleInitial": "",
      "importedId": "sofI06kOCDEPVNeDDKNMkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121463,
      "firstName": "Sima",
      "lastName": "Pirmoradi",
      "middleInitial": "",
      "importedId": "d3CaPniFY_T13AsnDiRxIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121464,
      "firstName": "Nikita Menon",
      "lastName": "K. P.",
      "middleInitial": "",
      "importedId": "2jgnJJfzRUFIugVuXkZS4w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121465,
      "firstName": "Mikhaila",
      "lastName": "Friske",
      "middleInitial": "",
      "importedId": "WQn_WncGH4kEvtj2OMv3Gw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121466,
      "firstName": "Mary",
      "lastName": "Fitzgerald",
      "middleInitial": "",
      "importedId": "_fQ9lYV4gU4jVoAxm6aFBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121467,
      "firstName": "Ruoyu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "g3ISAZKGdyjKdEXqfoOzcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121468,
      "firstName": "Paul",
      "lastName": "Lukowicz",
      "middleInitial": "",
      "importedId": "y8L6P8t-U44z-L7oNzzNkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121469,
      "firstName": "Heidi",
      "lastName": "Woelfle",
      "middleInitial": "",
      "importedId": "mripsCW-tzxFspuCH0OJEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121470,
      "firstName": "Abhik",
      "lastName": "Chowdhury",
      "middleInitial": "",
      "importedId": "12yEqzyrItOHqoms_pnveQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121471,
      "firstName": "Annette",
      "lastName": "Henderson",
      "middleInitial": "M. E.",
      "importedId": "fBCRbFbQkKPPVuoxlo6ygQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121493,
      "firstName": "Hinase",
      "lastName": "Kawano",
      "middleInitial": "",
      "importedId": "uM1LGop7qwcEoTTh0oFl6Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121494,
      "firstName": "Thomas",
      "lastName": "Ploetz",
      "middleInitial": "",
      "importedId": "F2dmw_5HVVXLnV439Fcmdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121495,
      "firstName": "Melody Moore",
      "lastName": "Jackson",
      "middleInitial": "",
      "importedId": "d9cWCoqfs0fPHFRFqt4I5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121496,
      "firstName": "Jaewoong",
      "lastName": "Jang",
      "middleInitial": "",
      "importedId": "YDJF8Kl_nH3rJEo4SUTwyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121497,
      "firstName": "Sohei",
      "lastName": "Wakisaka",
      "middleInitial": "",
      "importedId": "kDOKVZT1_Z0xbP5yfpBKVA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121498,
      "firstName": "Markus",
      "lastName": "Ikeda",
      "middleInitial": "",
      "importedId": "d7JDRnNbw1RL_RSv1REXqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121499,
      "firstName": "Luis A.",
      "lastName": "Castro",
      "middleInitial": "",
      "importedId": "CMhDul8NWyGuv_M9nhkReg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121500,
      "firstName": "Tonja-Katrin",
      "lastName": "Machulla",
      "middleInitial": "",
      "importedId": "ojPBA-tF0kThWQ4BFKv7CQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121501,
      "firstName": "Kimberley",
      "lastName": "Lakes",
      "middleInitial": "D",
      "importedId": "93e1jp93E4le_zx5uX-WhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121502,
      "firstName": "Hyunkook",
      "lastName": "Cho",
      "middleInitial": "",
      "importedId": "o7JZZsmW7sKnxVpU2EJ3AQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121503,
      "firstName": "Charles",
      "lastName": "Ramey",
      "middleInitial": "",
      "importedId": "xw685KUWyhiKubdB4zLUoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121504,
      "firstName": "Yigao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "GyTnsurb7RdStXZQjHhefQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121505,
      "firstName": "Julian",
      "lastName": "Geheeb",
      "middleInitial": "",
      "importedId": "md7rEPyWMXhcJ_QTZdEsKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121506,
      "firstName": "Sungho",
      "lastName": "Suh",
      "middleInitial": "",
      "importedId": "EcYyiZ_SRlZkpCC_vY7Q4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121507,
      "firstName": "Florian",
      "lastName": "Kirchbuchner",
      "middleInitial": "",
      "importedId": "LkaqAmAUaeum-7y84-nRaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121508,
      "firstName": "Yukun",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "h00Qvcz4RTPZ24DFsKiU-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121509,
      "firstName": "Jiha",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "TLwtBlJKdVff-0eIg3izpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121510,
      "firstName": "Hong",
      "lastName": "Leung",
      "middleInitial": "",
      "importedId": "OYbbXUrURDibVRGv-axGhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121511,
      "firstName": "Andreas",
      "lastName": "Pichler",
      "middleInitial": "",
      "importedId": "pbQ1-vEDo5If0bgv9KWEjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121512,
      "firstName": "Li-Lun",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "b77ngVhGBrZaHntrcdPPJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121513,
      "firstName": "Catherine E.",
      "lastName": "Draper",
      "middleInitial": "",
      "importedId": "ze3kCFa8lD_I-5lo3M2vIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121514,
      "firstName": "Masamichi",
      "lastName": "Shimosaka",
      "middleInitial": "",
      "importedId": "fTvSz_oiH4fVrzwP5ZvXuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121515,
      "firstName": "Tomomi",
      "lastName": "Taniguchi",
      "middleInitial": "",
      "importedId": "QbNhkGewU-XN10I6UPAAvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121516,
      "firstName": "Insup",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "AIHKzS6dz_UGBSX4uXDqkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121517,
      "firstName": "Krzysztof",
      "lastName": "Grudzień",
      "middleInitial": "",
      "importedId": "bMf8BGBWKRF69nYNgFSbww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121518,
      "firstName": "Marina",
      "lastName": "Okamoto",
      "middleInitial": "",
      "importedId": "K4zTootdgYh8fr41ITQn2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121519,
      "firstName": "Xingyu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "lok3Cha4FDtwrG3pPrrFhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121520,
      "firstName": "Yuqian",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "HPEvyzxu1dHK2fExgLnskQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121521,
      "firstName": "Santhosh",
      "lastName": "Kumar S",
      "middleInitial": "",
      "importedId": "O3A7bdLyxWxSC4fxytkkVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121522,
      "firstName": "Masaharu",
      "lastName": "Hirose",
      "middleInitial": "",
      "importedId": "BLzvq1-D2B5iug0Blgza9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121523,
      "firstName": "Ian",
      "lastName": "Oakley",
      "middleInitial": "",
      "importedId": "b2tvt-MyhdyGByDL4JTqtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121524,
      "firstName": "Li-Ting",
      "lastName": "Su",
      "middleInitial": "",
      "importedId": "TkW9fGy_XFCbNUhnobHmpg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121525,
      "firstName": "Gernot",
      "lastName": "Stübl",
      "middleInitial": "",
      "importedId": "q835LrqwExlVr3w92L8-1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121526,
      "firstName": "Kosei",
      "lastName": "Miyazaki",
      "middleInitial": "",
      "importedId": "TuEzV7k0CKkdAq9rukpPEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121527,
      "firstName": "Saranya Arun",
      "lastName": "Menon",
      "middleInitial": "",
      "importedId": "iUZ3JoSLMWvY0XY-xde-gA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121528,
      "firstName": "Wenting",
      "lastName": "Zeng",
      "middleInitial": "",
      "importedId": "lK2LF7SbGOVRSWSjHzT8OQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121529,
      "firstName": "Fengzhen",
      "lastName": "Cui",
      "middleInitial": "",
      "importedId": "KvJVVCsGAfzNK5LvcB2GEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121530,
      "firstName": "Kobiljon",
      "lastName": "Toshnazarov",
      "middleInitial": "E.",
      "importedId": "TnDQmJwHPvrLydJFjRzK6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121531,
      "firstName": "Dayoung",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "eheyKnFbbsYTpN8_LmFchA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121532,
      "firstName": "Sailin",
      "lastName": "Zhong",
      "middleInitial": "",
      "importedId": "rdwM4_ejVGMjACEc2C2uVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121533,
      "firstName": "Dong Young",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "RanhN40pUpWzpz5Yd1xkJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121534,
      "firstName": "Lidia",
      "lastName": "Alecci",
      "middleInitial": "",
      "importedId": "ZgwnrcdTcTxpuSrvxubdoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121535,
      "firstName": "Stephan",
      "lastName": "Sigg",
      "middleInitial": "",
      "importedId": "LxmGvu7yqgBwRItD6bLZuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121536,
      "firstName": "Hyunkyu",
      "lastName": "Jang",
      "middleInitial": "",
      "importedId": "V3Tw56ltljt7KEUDEbo6IA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121537,
      "firstName": "yuxuan",
      "lastName": "he",
      "middleInitial": "",
      "importedId": "TLpoCJWhWwZo0iyAMP2zXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121538,
      "firstName": "Yueyao",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "5GWITOVCoyDd6gsATbrVLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121539,
      "firstName": "Seungwan",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "dp9PrHx6UBoi9_7ZuZv7Cw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121540,
      "firstName": "Sourish Gunesh",
      "lastName": "Dhekane",
      "middleInitial": "",
      "importedId": "uny-ss1RPjXAwqGTLwPNRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121541,
      "firstName": "Yuyang",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "hlMhhtPEN4qSU0N3Q54XFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121542,
      "firstName": "Nouran",
      "lastName": "Abdalazim",
      "middleInitial": "",
      "importedId": "4b9FS1UOzpwK-4EgIN6r8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121543,
      "firstName": "Riku",
      "lastName": "Kitamura",
      "middleInitial": "",
      "importedId": "7fB__stJsMGuy4wFr-dRug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121544,
      "firstName": "Kohsuke",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "P7vwGov6W9FJpXco9W7yYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121545,
      "firstName": "Parth",
      "lastName": "Arora",
      "middleInitial": "",
      "importedId": "JZlAHpz8bQi5hcqi3xj_lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121546,
      "firstName": "Masahiko",
      "lastName": "Inami",
      "middleInitial": "",
      "importedId": "FXW_xm0-1iPoWcy7t5lCIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121547,
      "firstName": "Kunihiro",
      "lastName": "Kato",
      "middleInitial": "",
      "importedId": "G8FYHR0NdQQx5WIpETSFKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121548,
      "firstName": "Chloe",
      "lastName": "Eghtebas",
      "middleInitial": "",
      "importedId": "y_eRbneURuY9aTG6O8Eaiw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121549,
      "firstName": "Alex Q.",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "E67o9aMTCGk4qY8vn7WS3Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121550,
      "firstName": "Runhua",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "lrP5BKpYqUFCwQkubMlxQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121551,
      "firstName": "Franceli",
      "lastName": "Cibrian",
      "middleInitial": "L.",
      "importedId": "YIjHcbgWU8OIrO-6oJMmLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121552,
      "firstName": "Steeven",
      "lastName": "Villa",
      "middleInitial": "",
      "importedId": "H9eoI_KhiFXgd9zP2Qn9EA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121553,
      "firstName": "Magdalena",
      "lastName": "Wróbel-Lachowska",
      "middleInitial": "",
      "importedId": "A26vRVQJwj0sm8hJkTNyGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121554,
      "firstName": "Yuanda",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "9LwFMC-TN5fE3p_tel6QNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121555,
      "firstName": "Mengxi",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "mtvHiZ3M_qRUi-hPYVLtOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121556,
      "firstName": "Vu",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "IsmNK8Z7cyFIL_ghDc9R1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121557,
      "firstName": "Kiljong",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "ZS6lcYvkwSLWuGtmS4sNuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121558,
      "firstName": "Sakiko",
      "lastName": "Kawai",
      "middleInitial": "",
      "importedId": "WFXX3qCC_jobpoyxU_3p4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121559,
      "firstName": "Maria",
      "lastName": "Pospelova",
      "middleInitial": "",
      "importedId": "PKhZwbm4UzQMDZGWBuQPEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121560,
      "firstName": "Jen-Chu",
      "lastName": "Hsu",
      "middleInitial": "",
      "importedId": "gLCYzQis0NEALPsKQrXN9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121561,
      "firstName": "Yonggeon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "7qHCvnsxXrzf4WIAReleDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121562,
      "firstName": "Andrzej",
      "lastName": "Romanowski",
      "middleInitial": "",
      "importedId": "u0G5Isp24c_skNNUiXXFmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121563,
      "firstName": "Younho",
      "lastName": "Nam",
      "middleInitial": "",
      "importedId": "TEZJHzFYJXW91jPIoWY1bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121564,
      "firstName": "Vitor",
      "lastName": "Fortes Rey",
      "middleInitial": "",
      "importedId": "TT1lF4AV_68ly2JJIXBHpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121565,
      "firstName": "Jongwon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "KdGLLKV4fNoKowWmrgru9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121566,
      "firstName": "Guilin",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "DLDi2GgLy8uZb56JBo9xUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121567,
      "firstName": "Yunhyeong",
      "lastName": "Jang",
      "middleInitial": "",
      "importedId": "jsSXaDuxao7bh6SlF95T3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121568,
      "firstName": "Takumi",
      "lastName": "Yamamoto",
      "middleInitial": "",
      "importedId": "Q_lto2fPS0fLNeRr49IZ-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121569,
      "firstName": "Kazuya",
      "lastName": "Murao",
      "middleInitial": "",
      "importedId": "5SgkG9ZvesqANi6uKOlnhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121570,
      "firstName": "Hiraki",
      "lastName": "Yasuda",
      "middleInitial": "",
      "importedId": "KX68ZNkhem2kViGQIFXiMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121571,
      "firstName": "Cheng-Hsun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "N_yqzuc1HXuQpnq7cgzUSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121572,
      "firstName": "Sizhen",
      "lastName": "Bian",
      "middleInitial": "",
      "importedId": "6HbHUmQ4bEPjGj6j0NZHZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121573,
      "firstName": "Silvia",
      "lastName": "Faquiri",
      "middleInitial": "",
      "importedId": "cwPp_KQ6AZmktczFrJ0_gA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121574,
      "firstName": "Hoyoung",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "pqIOpTjeF2Z6bKw4LGrxvQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121575,
      "firstName": "Malcolm",
      "lastName": "Haynes",
      "middleInitial": "",
      "importedId": "JA2v78KoiGEZ5cmEhsC2jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121576,
      "firstName": "Youngtae",
      "lastName": "Noh",
      "middleInitial": "",
      "importedId": "j7GWpyRMhZr-Bz0uCfJ3sQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121577,
      "firstName": "Michele",
      "lastName": "Magno",
      "middleInitial": "",
      "importedId": "osT_PRD1zRLCrT9B5hGvFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121578,
      "firstName": "Keiichiro",
      "lastName": "Kashiwagi",
      "middleInitial": "",
      "importedId": "pqHFtUF-gqz6smTA89D6-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121579,
      "firstName": "Sam",
      "lastName": "Tilsen",
      "middleInitial": "",
      "importedId": "XKxhijnvO3NPSnZm1199Zw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121580,
      "firstName": "Eiji",
      "lastName": "Kumakawa",
      "middleInitial": "",
      "importedId": "llxxgrKFTvKQY1qnmULzUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121581,
      "firstName": "Shiwen",
      "lastName": "Fang",
      "middleInitial": "",
      "importedId": "diTdBCMX-N3L9lSKjiJqjw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121582,
      "firstName": "Yunkai",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "7O8yoeZwZofsW9h9zmHZSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121583,
      "firstName": "Jesus",
      "lastName": "Beltran",
      "middleInitial": "Armando",
      "importedId": "7CJFicqybIso-T78hV99tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121584,
      "firstName": "Shenshen",
      "lastName": "Lei",
      "middleInitial": "",
      "importedId": "pL6x8vzPmjeS1qk9ngtGew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121585,
      "firstName": "Thad",
      "lastName": "Starner",
      "middleInitial": "",
      "importedId": "sWKS6wUrUI9sUgYhmC3G8g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121586,
      "firstName": "Xuanzhi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "kwRjnfSwRrWopb2e_jCxbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121587,
      "firstName": "SALMAN",
      "lastName": "MUHAMMAD",
      "middleInitial": "",
      "importedId": "xlbMzwnrVBtlI7cAbTOvYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121588,
      "firstName": "Hyungsook",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "4HzXe8TlWuPTedrpucFv9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121589,
      "firstName": "Mikołaj",
      "lastName": "Woźniak",
      "middleInitial": "P.",
      "importedId": "SOVeT4ZsldKXrmrxfwUAQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121590,
      "firstName": "Kazuki",
      "lastName": "Iwahana",
      "middleInitial": "",
      "importedId": "YYM3wBq1Cx1R7JDp4h07hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121591,
      "firstName": "Qi",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "Gb_u7wSqq774w-KIItzvbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121592,
      "firstName": "Frank H. P.",
      "lastName": "Fitzek",
      "middleInitial": "",
      "importedId": "wH1DIObb4EUivNQUEBPiaQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121593,
      "firstName": "Soto",
      "lastName": "Anno",
      "middleInitial": "",
      "importedId": "8Tmkc4iWX0CGUj-WSJlLtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121594,
      "firstName": "Jamie",
      "lastName": "Ward",
      "middleInitial": "A",
      "importedId": "M7UZiMVpfl8Uu0l80q0QeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121595,
      "firstName": "Akira",
      "lastName": "Tsujimoto",
      "middleInitial": "",
      "importedId": "UZSbI0E8GorSnXod3Ti2hg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121596,
      "firstName": "Susanne",
      "lastName": "Boll",
      "middleInitial": "",
      "importedId": "h8Gk_22hVpBlCzdhLPLxZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121597,
      "firstName": "Arturo",
      "lastName": "Morales-Tellez",
      "middleInitial": "",
      "importedId": "JC736u3AKEeJO02tvjA-Yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121598,
      "firstName": "Sungjae",
      "lastName": "Cho",
      "middleInitial": "",
      "importedId": "uU5pMLgrlczDShllpS4YHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121599,
      "firstName": "Aarohi",
      "lastName": "Vaidya",
      "middleInitial": "",
      "importedId": "YWNWk53fGWbAJqcGZbFPmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121600,
      "firstName": "Nianchong",
      "lastName": "Qu",
      "middleInitial": "",
      "importedId": "JXIO0Atpj2UBkhWkjjprxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121601,
      "firstName": "Patrick",
      "lastName": "Chwalek",
      "middleInitial": "",
      "importedId": "vPa93Q1XvexC04wyPPSFwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121602,
      "firstName": "Alexander",
      "lastName": "Liebald",
      "middleInitial": "",
      "importedId": "AGaJdQkyikia97zU1Cee5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121603,
      "firstName": "Koki",
      "lastName": "Iguma",
      "middleInitial": "",
      "importedId": "n1xyVLYUZTyqUui5u_FMIg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121604,
      "firstName": "Harish",
      "lastName": "Haresamudram",
      "middleInitial": "",
      "importedId": "QLMVoLujmCGj7Vx1LP7neg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121605,
      "firstName": "Damiano",
      "lastName": "Spina",
      "middleInitial": "",
      "importedId": "h_YuIGgSXTYV1QekmnRFjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121606,
      "firstName": "Heiko",
      "lastName": "Müller",
      "middleInitial": "",
      "importedId": "sqVNgvf9-9Gdn40KvjX0ig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121607,
      "firstName": "Yash",
      "lastName": "Jain",
      "middleInitial": "",
      "importedId": "H4Mxm_7PAxWDIXUlsoYQLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121608,
      "firstName": "Hyeokhyen",
      "lastName": "Kwon",
      "middleInitial": "",
      "importedId": "CFhwSzGA6GJeoE5h-mPE5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121609,
      "firstName": "Uichin",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "pqQx9PFJboSwansrrWi2PA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121610,
      "firstName": "Dohyeon",
      "lastName": "Yeo",
      "middleInitial": "",
      "importedId": "Gr-QtfQ0ap0zu2e5MYIDPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121611,
      "firstName": "Sofia",
      "lastName": "Vempala",
      "middleInitial": "Anandi",
      "importedId": "62pRVkLZfQTVeBv_Qa-JTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121612,
      "firstName": "Ye",
      "lastName": "Tao",
      "middleInitial": "",
      "importedId": "-Z8K7gpdIwpjDVQKoqDbag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121613,
      "firstName": "Kai",
      "lastName": "Niu",
      "middleInitial": "",
      "importedId": "dkgHY8jzgr-FLb9sk__vRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121614,
      "firstName": "Lucretia",
      "lastName": "Williams",
      "middleInitial": "",
      "importedId": "xCiO4NafUvE82P1vR9JaGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121615,
      "firstName": "Yong-Han",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "xwjOtWOz0P_WhV3GAwqLkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121616,
      "firstName": "Setareh",
      "lastName": "Zafari",
      "middleInitial": "",
      "importedId": "KLSScp_R3bFPQyAU79FWZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121617,
      "firstName": "John",
      "lastName": "Raiti",
      "middleInitial": "",
      "importedId": "GxpLwdtcaWze-2T6sLIYmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121618,
      "firstName": "Ryan",
      "lastName": "Mao",
      "middleInitial": "",
      "importedId": "2RePIyinnPLUabJwE3tsGw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121619,
      "firstName": "Falk",
      "lastName": "Scholer",
      "middleInitial": "",
      "importedId": "SW5-ELz0EQloNqmza9n3og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121620,
      "firstName": "Hiroto",
      "lastName": "Saito",
      "middleInitial": "",
      "importedId": "4PQZZcAaD4LentExfaZ5NQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121621,
      "firstName": "Shengzhang",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "Uv8Ofk2DDfP7J46zVSas2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121622,
      "firstName": "Kyungsik",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "oVcowPaM8UEB0TFHi7Yztg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121623,
      "firstName": "Yu",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "67VHigyxrXXJ3ZtYGgFc9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121624,
      "firstName": "Jongjin",
      "lastName": "Park",
      "middleInitial": "",
      "importedId": "YrFNL2-bl_s2SC58NI6AQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121625,
      "firstName": "Yi-Chi",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "0BixCOUgtaN5hWPpA5Ul6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121626,
      "firstName": "Jiwan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "moaLtaqn5MffvuzNuFd0Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121627,
      "firstName": "Longbiao",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "4_953N5siskCMZ9U2Hq7xA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121628,
      "firstName": "Ashika",
      "lastName": "Manjunath",
      "middleInitial": "",
      "importedId": "oa0Wor7MndF04ZcOpXt3-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121629,
      "firstName": "Tarek",
      "lastName": "Hamid",
      "middleInitial": "",
      "importedId": "zADyQOrQxIEYfaJZGWOvxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121630,
      "firstName": "G S Rajshekar",
      "lastName": "Reddy",
      "middleInitial": "",
      "importedId": "mP_bQrXMu_yWCAyAkgY_fA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121631,
      "firstName": "Kota",
      "lastName": "Tsubouchi",
      "middleInitial": "",
      "importedId": "aSjHwXyRdwndAcyF_VlYkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121632,
      "firstName": "Amanda",
      "lastName": "Watson",
      "middleInitial": "",
      "importedId": "tC-MDx-8l2SmTtR9D76gqg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121633,
      "firstName": "Yuki",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "0hnaa5ERyHIq5J6b4tPqdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121634,
      "firstName": "Arinobu",
      "lastName": "Niijima",
      "middleInitial": "",
      "importedId": "1hXdXCGzoscaaHtE_sgpbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121635,
      "firstName": "Yuta",
      "lastName": "Sugiura",
      "middleInitial": "",
      "importedId": "TeKRGK2C776DV4tpr6ldBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121636,
      "firstName": "Takayuki",
      "lastName": "Sakamoto",
      "middleInitial": "",
      "importedId": "15UCBLB4GdBsDrlJh-VKtw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121637,
      "firstName": "Takaaki",
      "lastName": "Ishikawa",
      "middleInitial": "",
      "importedId": "08PlkSB-Jzwv8BOPsTjPkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121638,
      "firstName": "Fabian",
      "lastName": "Widmoser",
      "middleInitial": "",
      "importedId": "IyYuF0bpn0AFe1d-UAQnXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121639,
      "firstName": "Tsutomu",
      "lastName": "Terada",
      "middleInitial": "",
      "importedId": "KVd-qRc6nRI-nIJCYetDtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121640,
      "firstName": "Daqing",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "fuXUkQg0SLSdBITnwxEpnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121641,
      "firstName": "Agnes",
      "lastName": "Gruenerbl",
      "middleInitial": "",
      "importedId": "CDLkbWni3L1RHex0agOCHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121642,
      "firstName": "Andreas",
      "lastName": "Sackl",
      "middleInitial": "",
      "importedId": "SwxIEAhCwd54Q2Lix-XBaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121643,
      "firstName": "Wernhuar",
      "lastName": "Tarng",
      "middleInitial": "",
      "importedId": "Sajp0dIMHTypWsgHMiUxjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121644,
      "firstName": "Wei",
      "lastName": "Xiang",
      "middleInitial": "",
      "importedId": "G5qePvF9n-s_Q6H659j7vQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121645,
      "firstName": "Anna",
      "lastName": "Walczak",
      "middleInitial": "",
      "importedId": "inn9b_DQzPCNLf3vteTRZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121646,
      "firstName": "Jeongwon",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "Fwm1aaP-maQuWZH_I994kA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121647,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "aNj3BsCl2b0b7FACxYJdKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121648,
      "firstName": "Joe",
      "lastName": "Paradiso",
      "middleInitial": "",
      "importedId": "mPgn9zhAqjthCQ5b5psNcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121649,
      "firstName": "Changzhen",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "Z4H90UbD4jKLngsh8AzMcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121650,
      "firstName": "Byung Hyung",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "FNBaezf67Tco02UXidsNVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121651,
      "firstName": "Masanori",
      "lastName": "Sugimoto",
      "middleInitial": "",
      "importedId": "tvL9JzzZeAGUqQbrB4nbig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121652,
      "firstName": "David",
      "lastName": "Ramsay",
      "middleInitial": "",
      "importedId": "3wPvvQe3xQWyeqAYgjVnYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121653,
      "firstName": "Benjamin",
      "lastName": "Tag",
      "middleInitial": "",
      "importedId": "DEa3EvJq4ssEnGWw_FCDUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121654,
      "firstName": "Berit",
      "lastName": "Greinke",
      "middleInitial": "",
      "importedId": "L6LPSHqr6mxYA3EXjTJfOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121655,
      "firstName": "Jasmin",
      "lastName": "Niess",
      "middleInitial": "",
      "importedId": "7cCtnrBqZkw8ngUiXKa6tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121656,
      "firstName": "Kuo-Liang",
      "lastName": "Ou",
      "middleInitial": "",
      "importedId": "odkrpyAgdOX3pBYmKFgH8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121657,
      "firstName": "Vincent",
      "lastName": "Abt",
      "middleInitial": "",
      "importedId": "ioV7xKJoy4CmIdgDoPOjdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121658,
      "firstName": "Jun",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "hOKAPSCIi84JSGMLBLAMRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121659,
      "firstName": "Lismer Andres",
      "lastName": "Caceres Najarro",
      "middleInitial": "",
      "importedId": "9nzIKVOpxCxmEXXCRRaFLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121660,
      "firstName": "Mingyeol",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "kkV4NZr-cqwPILTts9Pucg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121661,
      "firstName": "Gillian",
      "lastName": "Hayes",
      "middleInitial": "R",
      "importedId": "EyYcjfdES3PoDfkeAzaNwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121662,
      "firstName": "Silvia",
      "lastName": "Santini",
      "middleInitial": "",
      "importedId": "_IVFxLwSJgC_EbL6vgrJMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121663,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "pTW1Zvq8601esh3qGSR0qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121664,
      "firstName": "Jazette",
      "lastName": "Johnson",
      "middleInitial": "",
      "importedId": "ky7hXI-LUIh76-zhbaOuUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121665,
      "firstName": "Keiichi",
      "lastName": "Ochiai",
      "middleInitial": "",
      "importedId": "ft96FySMjHTPPWWjt95Y3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121666,
      "firstName": "Zikang",
      "lastName": "Leng",
      "middleInitial": "",
      "importedId": "JNjSVF0dEOYcb9nk_2mRMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121667,
      "firstName": "David",
      "lastName": "Martin",
      "middleInitial": "",
      "importedId": "OE83RHAm6JlMjas4tJQbcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121668,
      "firstName": "Ramesh",
      "lastName": "Makam",
      "middleInitial": "",
      "importedId": "89emREkyDkHoieV79TVeQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121669,
      "firstName": "Mudra",
      "lastName": "Nagda",
      "middleInitial": "",
      "importedId": "QYg0023qbaTk-URSzUhXeQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121670,
      "firstName": "Junya",
      "lastName": "Hotta",
      "middleInitial": "",
      "importedId": "94anK8n52Q9zrsYc6zQSCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121671,
      "firstName": "Francois",
      "lastName": "Guimbretiere",
      "middleInitial": "",
      "importedId": "haFD2OgOdA3IY14UrV_GbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121672,
      "firstName": "Norma",
      "lastName": "Puspitasari",
      "middleInitial": "",
      "importedId": "dxyZzoYTz7m6itLWGCY2cQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121673,
      "firstName": "Peng-Jui",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "t9rQ0D2f01T-j4S8OIZ9vA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121674,
      "firstName": "Arianna",
      "lastName": "Mastali",
      "middleInitial": "",
      "importedId": "fOjqHnYquxQ9R5NZMCmTsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121675,
      "firstName": "Ruidong",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "4gOWCTyy2zZpqRblr9GeDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121676,
      "firstName": "Richard",
      "lastName": "Jin",
      "middleInitial": "",
      "importedId": "l-0gvDuDbZ74h9WMv85Qxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121677,
      "firstName": "Vitor",
      "lastName": "Fortes",
      "middleInitial": "",
      "importedId": "ypi64tUXynAZzNzxtlkkAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121678,
      "firstName": "Hyunchul",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "04ufQKl9FDl6Y6qP8ififQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121679,
      "firstName": "Megha",
      "lastName": "Thukral",
      "middleInitial": "",
      "importedId": "Y5tGOGZwTdMTP7luqmPw2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121680,
      "firstName": "Devansh",
      "lastName": "Agarwal",
      "middleInitial": "",
      "importedId": "3mu9XEHJwXV9RduC7JTrew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121681,
      "firstName": "Jieun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "PRWINiV02oXbAxfkJ1waRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121682,
      "firstName": "Patrick",
      "lastName": "Seeling",
      "middleInitial": "",
      "importedId": "S4pFNGTj7h7KR3f36NAdCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121683,
      "firstName": "Benjamin",
      "lastName": "Steeper",
      "middleInitial": "",
      "importedId": "SdWXzxAwYBY9ydv_Aiipnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121684,
      "firstName": "Caylee",
      "lastName": "Cook",
      "middleInitial": "",
      "importedId": "b3wgrB0NlFljVz4gxX129A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121685,
      "firstName": "﻿Slim",
      "lastName": "Abdennadher",
      "middleInitial": "",
      "importedId": "Bf6q-W81_Qgg__9eHiMxDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121686,
      "firstName": "Cheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "1DDZdGdZa_bi_mwYItE6eA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121687,
      "firstName": "SeungJun",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "UzsGew0kYu72FK5pPTU3jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121688,
      "firstName": "Inseok",
      "lastName": "Hwang",
      "middleInitial": "",
      "importedId": "zfzoqAuOJsvTmBGkKyShKA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121689,
      "firstName": "Juhyun",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "ruQ9CYuiIk0wGIpGJpOqDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121690,
      "firstName": "Xiaoguang",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "Q2n_FVDRmlmABcnoi1mDng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121691,
      "firstName": "Flora",
      "lastName": "Salim",
      "middleInitial": "D.",
      "importedId": "DWrgPXRnHzslllOsqlYwqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121692,
      "firstName": "Zhi",
      "lastName": "Tian",
      "middleInitial": "",
      "importedId": "IDDnSfm3umO92cIjkaJIWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121693,
      "firstName": "Jungeun",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "RiwySLG8PNCy12-V7KAKMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121694,
      "firstName": "Adithya",
      "lastName": "Muralikrishna",
      "middleInitial": "",
      "importedId": "D8v_uS2rADYo-ok6ncXVPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121695,
      "firstName": "Wataru",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "tjDXE7R_FfUs2xbzVmpkbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121696,
      "firstName": "Atsushi",
      "lastName": "Izumihara",
      "middleInitial": "",
      "importedId": "N1O9OyZOFGPpr7cXoL3ULA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121697,
      "firstName": "Seokwoo",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "jXZPOVyP5nI3QbajQ5fSnA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121698,
      "firstName": "Ze",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "URqtoe-37bS946Nm-qD-UQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121699,
      "firstName": "Yoonsu",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "TozbMnA_aeB9lG3Rgnb15Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121700,
      "firstName": "Kaixin",
      "lastName": "Ji",
      "middleInitial": "",
      "importedId": "vX69Qx60toNxWesMdQt4lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121701,
      "firstName": "Ke",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "LcGDnYiyLhXxc1imDxPw7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121702,
      "firstName": "Yung-Ju",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "4fqA22UA7fOw_djjgasuxw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121703,
      "firstName": "Buntarou",
      "lastName": "Shizuki",
      "middleInitial": "",
      "importedId": "TJAWIctVjvJEKegv8Zj6KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121704,
      "firstName": "Leonardo",
      "lastName": "Alchieri",
      "middleInitial": "",
      "importedId": "1ehPJBlPTBMsGZUxAOYJVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121705,
      "firstName": "Xiaohe",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "3qPOfxnDQ0S1tbWgOYJ7FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121706,
      "firstName": "Shunsuke",
      "lastName": "Iwakiri",
      "middleInitial": "",
      "importedId": "hay1l2f2iffFwyG76yC6bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121707,
      "firstName": "Hisao",
      "lastName": "Katsumi",
      "middleInitial": "",
      "importedId": "VdA9ZOL8BliL3ln2JXSjLg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121708,
      "firstName": "Naoki",
      "lastName": "Okamoto",
      "middleInitial": "",
      "importedId": "YqH4yoqrNC8aYfpZPbu0QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121709,
      "firstName": "Hiroki",
      "lastName": "Watanabe",
      "middleInitial": "",
      "importedId": "j9uVhI0O6nKFZu_32F-wkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121710,
      "firstName": "Meghna",
      "lastName": "Bhatnagar",
      "middleInitial": "",
      "importedId": "lvp29h0YGmKWPqiiI2sCbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121711,
      "firstName": "Monica",
      "lastName": "Tentori",
      "middleInitial": "",
      "importedId": "hOZBmsOULUvn8pllFILqLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121712,
      "firstName": "Srikanth",
      "lastName": "T. Varadharajan",
      "middleInitial": "",
      "importedId": "dSI10qAMkU9DQXJRQQx51A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121713,
      "firstName": "Yu-Rou",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "rulkmcKQE8Uf9LEyLyz4KQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121714,
      "firstName": "Hyangmi",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "UEBGkPkEVdSc7Hoie6nJrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121715,
      "firstName": "Stephen",
      "lastName": "Voida",
      "middleInitial": "",
      "importedId": "q1am3-h2AZMNYrqvJ_MDcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121716,
      "firstName": "Koki",
      "lastName": "Mitani",
      "middleInitial": "",
      "importedId": "j2KZZgSmIcQzPGCQ28kD3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121717,
      "firstName": "Quinn",
      "lastName": "Burns",
      "middleInitial": "",
      "importedId": "tQEbfjKpkVN8qIKef7BnQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121718,
      "firstName": "Elizabeth",
      "lastName": "Ankrah",
      "middleInitial": "",
      "importedId": "X-v8YJhYiLpx77KmRjNBGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121719,
      "firstName": "Uei-Dar",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "OXV7hY7DkVnPNXKW1cVYEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121720,
      "firstName": "Adam",
      "lastName": "Sałata",
      "middleInitial": "Jan",
      "importedId": "XDor7Pr8X9R5lQLbDAVRbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121721,
      "firstName": "Hao",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "fueejEGDrxsMLdjxaJOpGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121722,
      "firstName": "Zhaoqu",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "yneWw5xRZ8Ic_suh758nMA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121723,
      "firstName": "Nathan",
      "lastName": "Perry",
      "middleInitial": "",
      "importedId": "Ok2CmVZSmLPY7CIRaoNpfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121724,
      "firstName": "Woojin",
      "lastName": "Song",
      "middleInitial": "",
      "importedId": "Q4zNhzBt6pJqCRa30rMbzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121725,
      "firstName": "Giang T.",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "vvuGUeGDQU-LYy01UnROXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121726,
      "firstName": "María Concepción",
      "lastName": "Valdez Gastelum",
      "middleInitial": "",
      "importedId": "j62XZ-Mlaby76f-o9U7FGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121727,
      "firstName": "Kaori",
      "lastName": "Ikematsu",
      "middleInitial": "",
      "importedId": "ydhDL2ZEmScGGp3dMOVqOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121728,
      "firstName": "Yufei",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "-Ga7sR0THAIrq1rjS7DRuQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121729,
      "firstName": "Gwangbin",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "N8WAz28gH2_8uGSCQ6Aa7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121730,
      "firstName": "Danula",
      "lastName": "Hettiachchi",
      "middleInitial": "",
      "importedId": "SlayOPpkJgxHbffvhD9zrA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121731,
      "firstName": "Hanna",
      "lastName": "Mofid",
      "middleInitial": "",
      "importedId": "adPHVXd7shk9o_8f7RQ1HA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121732,
      "firstName": "Zhuyu",
      "lastName": "Teng",
      "middleInitial": "",
      "importedId": "Hui3ZefZd_g8BMrs936zgw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121733,
      "firstName": "Hongbo",
      "lastName": "Ni",
      "middleInitial": "",
      "importedId": "X9AyycLuMvh4kU7yHXWhAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121734,
      "firstName": "Young-Joo",
      "lastName": "Suh",
      "middleInitial": "",
      "importedId": "QgvRRrfmJGYn7oH0XlrCzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121735,
      "firstName": "Amal",
      "lastName": "Yassien",
      "middleInitial": "",
      "importedId": "1cbKaa_8nL7DElSLcMITZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121736,
      "firstName": "Hong Yoon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "EuIhprEYDr4ZdMOYlS9zeA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121737,
      "firstName": "Bin",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "1vLuckVZFJgUB7He7aptRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121738,
      "firstName": "Julian",
      "lastName": "von Wilmsdorff",
      "middleInitial": "",
      "importedId": "vOdPOdI0oMcwKQuI2ehKew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121739,
      "firstName": "Wanying",
      "lastName": "Mo",
      "middleInitial": "",
      "importedId": "APhoMZwcBMWF5wlYmuFoqA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121740,
      "firstName": "Anam",
      "lastName": "Ahmad",
      "middleInitial": "",
      "importedId": "F0JziQCSAby9O8xxt2onIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121741,
      "firstName": "Rujia",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "Drk7e7rRPac5W1jM0VXKNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121742,
      "firstName": "Jonas",
      "lastName": "Schulz",
      "middleInitial": "",
      "importedId": "CMFZFhZqiiKojKG_M3M9aQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121743,
      "firstName": "Cheng",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "weP3Wh7QJtcBIViAyxpCQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121744,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "middleInitial": "",
      "importedId": "r8sh9GdFO7dA9XL50gpazA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121745,
      "firstName": "Sicheng",
      "lastName": "Yin",
      "middleInitial": "",
      "importedId": "zU7JVn5V_NRHwkxXUpV_Hw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121746,
      "firstName": "Rajandeep",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "TjHxqwJmAp9x1wmbCGm1Jg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121747,
      "firstName": "Weiyan",
      "lastName": "Shi",
      "middleInitial": "",
      "importedId": "tc_BZTG7khV0IYFK7cYLRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121748,
      "firstName": "Shengjie",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "m4_mBVJxn0snws6A-WysCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121749,
      "firstName": "Naishi",
      "lastName": "Shah",
      "middleInitial": "Lokeshbhai",
      "importedId": "RBRL-hPLbfbdFozDQ-cBMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121750,
      "firstName": "Dominique",
      "lastName": "Nshimyimana",
      "middleInitial": "",
      "importedId": "a8iZxLNUroqV-FOTJs9v9g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121751,
      "firstName": "Si",
      "lastName": "Zuo",
      "middleInitial": "",
      "importedId": "lpmOwaZIB3gKOXMDCNdBFg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121752,
      "firstName": "Leye",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "ix1uZBWvp31n1XJPd5BXaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121753,
      "firstName": "Peter",
      "lastName": "Feng",
      "middleInitial": "",
      "importedId": "BSPGkqHK1TmiQRYpypg-uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121754,
      "firstName": "Junxiang",
      "lastName": "Ji",
      "middleInitial": "",
      "importedId": "S2qh01Ok6vaHS_9dQIWE5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121755,
      "firstName": "Yunjo",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "HU8ktMBf_59GX0vzR3sMmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121756,
      "firstName": "Masami",
      "lastName": "Takahashi",
      "middleInitial": "",
      "importedId": "4fvbAvdpO-fFKlptw-dLXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121757,
      "firstName": "Gudrun",
      "lastName": "Klinker",
      "middleInitial": "",
      "importedId": "1oboBXzU7IY3q9R3HN97wQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121758,
      "firstName": "Takashi",
      "lastName": "Amesaka",
      "middleInitial": "",
      "importedId": "Ee3-4cSiiA6mPM8hL7KvRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121759,
      "firstName": "Eunsol",
      "lastName": "An",
      "middleInitial": "",
      "importedId": "sBZkYCaj2vK9iFEZcA0o9A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121760,
      "firstName": "Osamu",
      "lastName": "Saisho",
      "middleInitial": "",
      "importedId": "7jtZym1e-q6zPZguI5BYBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121761,
      "firstName": "Thuy Duong",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "dvx5gtkWazNR0-1GToZ5_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 121762,
      "firstName": "Xiaohua",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "jjiroOrUswWmt8g3AKnUwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128266,
      "firstName": "Michael",
      "lastName": "Littman",
      "middleInitial": "L",
      "importedId": "1113",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128267,
      "firstName": "Cyrus",
      "lastName": "Zhou",
      "importedId": "1112",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128268,
      "firstName": "Lefan",
      "lastName": "Zhang",
      "importedId": "1111",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128485,
      "firstName": "Dimitris",
      "lastName": "Spathis",
      "middleInitial": "",
      "importedId": "mlAnjtZ2PgKekH98xMgb4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128486,
      "firstName": "Nirupam",
      "lastName": "Roy",
      "middleInitial": "",
      "importedId": "4McBx34_F7c0pAgtm049qQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128488,
      "firstName": "Tanzeem",
      "lastName": "Choudhury",
      "middleInitial": "",
      "importedId": "eUoVJaRjEK-ygW4VV7kfhQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128489,
      "firstName": "Sofia",
      "lastName": "Yfantidou",
      "middleInitial": "",
      "importedId": "13lwLqQ6v8z7NMfMBpnrKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128490,
      "firstName": "Min-Chun",
      "lastName": "Hu",
      "middleInitial": "",
      "importedId": "4YUbVXXILqZeS-BeHgkYNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128491,
      "firstName": "Yuanzhang",
      "lastName": "Xiao",
      "middleInitial": "",
      "importedId": "TzlJzIr41uWcRJsBXIy5OA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128492,
      "firstName": "Cuauhtémoc",
      "lastName": "Rivera-Loaiza",
      "middleInitial": "",
      "importedId": "7jX9sR2oq5jsoBSK1tVWCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128493,
      "firstName": "Philipp",
      "lastName": "Scholl",
      "middleInitial": "M.",
      "importedId": "xWVQ6ZilR9myx9HRLjrHVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128494,
      "firstName": "Liwei",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "KB4Ew3efN16WCB23RZP3hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128495,
      "firstName": "Mathias",
      "lastName": "Ciliberto",
      "middleInitial": "",
      "importedId": "IwuCPN0RpzZGHq9SIImM_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128496,
      "firstName": "Teodor",
      "lastName": "Stoev",
      "middleInitial": "",
      "importedId": "MJU637FQdn7_tFSI8SWIzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128497,
      "firstName": "Varun",
      "lastName": "Mishra",
      "middleInitial": "",
      "importedId": "3KBsVQv4PaO7nXcQA7fIrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128498,
      "firstName": "Han",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "XMJn2Iia57rUn3UaeZAnCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128499,
      "firstName": "Tsuyoshi",
      "lastName": "Okita",
      "middleInitial": "",
      "importedId": "qCBqtw2-b9uCODcY-1hqjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128500,
      "firstName": "Hristijan",
      "lastName": "Gjoreski",
      "middleInitial": "",
      "importedId": "VnHdA3OfX7U6CB0m6wRz5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128501,
      "firstName": "Filipa",
      "lastName": "Ferreira-Brito",
      "middleInitial": "",
      "importedId": "dTklNzY3TMFrG8YkkJUA-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128502,
      "firstName": "Jie",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "hf-NVt723FAGzEpimyRPJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128503,
      "firstName": "Alfredo",
      "lastName": "Sanchez",
      "middleInitial": "",
      "importedId": "gZZgSpU3NRs93MNo-a9LwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128504,
      "firstName": "Daniel",
      "lastName": "Adler",
      "middleInitial": "A.",
      "importedId": "ZCt86YNEK25xhHAshmieZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128505,
      "firstName": "Bert",
      "lastName": "Arnrich",
      "middleInitial": "",
      "importedId": "F_zE-2JwcEKPpOYohWiLzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128506,
      "firstName": "Tiago",
      "lastName": "Guerreiro",
      "middleInitial": "",
      "importedId": "JJKBzLc6Yvr-SWSx6ZUm1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128507,
      "firstName": "Xuhai",
      "lastName": "Xu",
      "middleInitial": "",
      "importedId": "kZTgMM28kznZj_RjAAperg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128508,
      "firstName": "Chuang-Wen",
      "lastName": "You",
      "middleInitial": "",
      "importedId": "Lhg9wJUskHC2kfRmfey4LA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128509,
      "firstName": "Yu",
      "lastName": "Enokibori",
      "middleInitial": "",
      "importedId": "DrPsZuMlmCEqLaYjoEoczA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128510,
      "firstName": "Pekka",
      "lastName": "Siirtola",
      "middleInitial": "",
      "importedId": "yhkoDBFm30PG5ZreOGbrlg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128511,
      "firstName": "Francisco",
      "lastName": "Gutierrez",
      "middleInitial": "J.",
      "importedId": "jUvuAdmxjfQJg8vNeuLNwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128512,
      "firstName": "Pedro",
      "lastName": "Santana-Mancilla",
      "middleInitial": "C.",
      "importedId": "hGYE5YrQWhgK0he0iQa8vw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128513,
      "firstName": "Gregory",
      "lastName": "Tourte",
      "middleInitial": "J. L.",
      "importedId": "wY9gmcprcBCyNkgbDu91gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128514,
      "firstName": "Laura Sanely",
      "lastName": "Gaytán-Lugo",
      "middleInitial": "",
      "importedId": "Dc_8c4tbCJMcb0YgHsS-JA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128515,
      "firstName": "Thomas",
      "lastName": "Lachmann",
      "middleInitial": "",
      "importedId": "2VOh_yQkYsaVOeM3bMwYCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128517,
      "firstName": "Mitja",
      "lastName": "Luštrek",
      "middleInitial": "",
      "importedId": "3BTfTOghbTjRg7_MpFyxyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128518,
      "firstName": "Tauhidur",
      "lastName": "Rahman",
      "middleInitial": "",
      "importedId": "tmk5ZwV1Z9s2tuNTvxE0SA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128519,
      "firstName": "Kei",
      "lastName": "Hiroi",
      "middleInitial": "",
      "importedId": "_JUU5RGxjac4jdKBZm0cJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128521,
      "firstName": "Wei",
      "lastName": "Sun",
      "middleInitial": "",
      "importedId": "emnyZhKL7b_HLU7BY60G3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128522,
      "firstName": "Marios",
      "lastName": "Constantinides",
      "middleInitial": "",
      "importedId": "nkGVw2ca5PUqN-KjzkeoTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128523,
      "firstName": "Kristina",
      "lastName": "Yordanova",
      "middleInitial": "Y.",
      "importedId": "vOkEgpfCCCXDdc3ScHYgFA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128524,
      "firstName": "Yun-Jui",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "FZzxvKvvI7VP3mSgW1RphQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128525,
      "firstName": "Lizbeth",
      "lastName": "Escobedo",
      "middleInitial": "",
      "importedId": "OAriweduAzlFBLmAjFo6Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128526,
      "firstName": "Paula",
      "lastName": "Lago",
      "middleInitial": "",
      "importedId": "7dlLHo5q4SoXQdAHucfKPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128527,
      "firstName": "Shkurta",
      "lastName": "Gashi",
      "middleInitial": "",
      "importedId": "DOSeGlsZPn-nX8vYghYZDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128528,
      "firstName": "Rina",
      "lastName": "Wehbe",
      "middleInitial": "R.",
      "importedId": "jsKmTGp27WbxsEYkNv__LQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128530,
      "firstName": "Mayra",
      "lastName": "Barrera Machuca",
      "middleInitial": "Donaji",
      "importedId": "kelrth_S2MBODQVWUWbO2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128531,
      "firstName": "Elizabeth",
      "lastName": "Murnane",
      "middleInitial": "L",
      "importedId": "K9VwVSv-EXq5pYrpdawx7A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128532,
      "firstName": "Saeed",
      "lastName": "Abdullah",
      "middleInitial": "",
      "importedId": "N8VMF2-8IOsxKv3vpMlYOw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128533,
      "firstName": "Rony",
      "lastName": "Krell",
      "middleInitial": "",
      "importedId": "Z43nfkWHdFYEkUIQA5ghcA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128534,
      "firstName": "Niels",
      "lastName": "van Berkel",
      "middleInitial": "",
      "importedId": "25YjfE3J_mWOlw4h57KXSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128535,
      "firstName": "Alessandro",
      "lastName": "Montanari",
      "middleInitial": "",
      "importedId": "Kyf8zLeko0-MQcMDpYtQUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128536,
      "firstName": "Rajalakshmi",
      "lastName": "Nandakumar",
      "middleInitial": "",
      "importedId": "dV_gGz5wB58eJfRFK8IraA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128537,
      "firstName": "Mirco",
      "lastName": "Musolesi",
      "middleInitial": "",
      "importedId": "LQAMoY8HV4wG3kegm8Mbvw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128538,
      "firstName": "Weiwei",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "N93URo_XzhmOYtRsVZIVKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128539,
      "firstName": "Kenta",
      "lastName": "Urano",
      "middleInitial": "",
      "importedId": "haB-DocMVc08pBMNVqYYFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128540,
      "firstName": "Rajesh",
      "lastName": "Balan",
      "middleInitial": "",
      "importedId": "VvIassImxVR7pywru9I1Qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128541,
      "firstName": "Alexander",
      "lastName": "Hoelzemann",
      "middleInitial": "",
      "importedId": "kWcUU857SZjnf0LVI28Dyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128542,
      "firstName": "Ting",
      "lastName": "Dang",
      "middleInitial": "",
      "importedId": "fTPb1VauW60m85RLXJjt2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128543,
      "firstName": "Soraia",
      "lastName": "Prietch",
      "middleInitial": "Silva",
      "importedId": "iDG7Ywoqq8Sn2L6wEK9cyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128545,
      "firstName": "Jakob",
      "lastName": "Bardram",
      "middleInitial": "E.",
      "importedId": "9EdAIvuaOJXZxUptY-OGBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128546,
      "firstName": "Yi-Chao",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "l_6j4sISVEM08FhPBVQV7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128548,
      "firstName": "Akane",
      "lastName": "Sano",
      "middleInitial": "",
      "importedId": "H1rjWf6HYJ1WItUmGz3STQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128549,
      "firstName": "Asif",
      "lastName": "Salekin",
      "middleInitial": "",
      "importedId": "SjaW3cs4jZhYYI9Lblhssw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128550,
      "firstName": "Emma",
      "lastName": "Tonkin",
      "middleInitial": "",
      "importedId": "6u97fIkhSEf568vFlIi4Uw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128551,
      "firstName": "Tong",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "RIxjp11evZqSRxXpFR3wGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128552,
      "firstName": "Andrea",
      "lastName": "Ferlini",
      "middleInitial": "",
      "importedId": "Em7yJvvCjSDeGNpckr94PA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128553,
      "firstName": "Oscar",
      "lastName": "Mayora",
      "middleInitial": "",
      "importedId": "c35nEyiiTF6dt47dNV93kA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128554,
      "firstName": "Yiran",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "JKJ4mdPJBt-BYuGjLDajtg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128556,
      "firstName": "Sahiti",
      "lastName": "Kunchay",
      "middleInitial": "",
      "importedId": "nBVLrbAClq4n0YimpXK4lA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128557,
      "firstName": "Zachary",
      "lastName": "King",
      "middleInitial": "D",
      "importedId": "zu0En_YDJYySER2gKg7RbA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128558,
      "firstName": "Chi Ian",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "dM9LmD31cHPg_lEb9qc9FQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128559,
      "firstName": "Katayoun",
      "lastName": "Farrahi",
      "middleInitial": "",
      "importedId": "V_dqovOJGPmo8MOIEPXtoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128560,
      "firstName": "Manasa",
      "lastName": "Kalanadhabhatta",
      "middleInitial": "",
      "importedId": "ARg5XtWQNoAkMO7zV6NILw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128561,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "importedId": "PZO_YeskIPmyLzFr_vpQHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 128578,
      "firstName": "Riku",
      "lastName": "Arakawa",
      "importedId": "1220",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128579,
      "firstName": "Yang",
      "lastName": "Gao",
      "importedId": "1462",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128580,
      "firstName": "Seokmin",
      "lastName": "Choi",
      "importedId": "1461",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128581,
      "firstName": "Adam",
      "lastName": "Russell",
      "importedId": "1460",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128582,
      "firstName": "Haiyun",
      "lastName": "Hu",
      "importedId": "1217",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128583,
      "firstName": "Dingran",
      "lastName": "Wang",
      "importedId": "1459",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128584,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "importedId": "1216",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128585,
      "firstName": "Yengcheng",
      "lastName": "Jin",
      "importedId": "1458",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128586,
      "firstName": "Matthew",
      "lastName": "Thompson",
      "importedId": "1215",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128587,
      "firstName": "Sean",
      "lastName": "Jones",
      "middleInitial": "J",
      "importedId": "1457",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128588,
      "firstName": "Ning",
      "lastName": "Li",
      "importedId": "1699",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128589,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "importedId": "1214",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128590,
      "firstName": "Siddharth",
      "lastName": "Shah",
      "importedId": "1456",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128591,
      "firstName": "Yingying",
      "lastName": "Zhao",
      "importedId": "1698",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128592,
      "firstName": "Jason",
      "lastName": "Hoffman",
      "importedId": "1213",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128593,
      "firstName": "Lin",
      "lastName": "Huang",
      "importedId": "1455",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128594,
      "firstName": "Ning",
      "lastName": "Liu",
      "importedId": "1697",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128595,
      "firstName": "Farshid",
      "lastName": "Parizi",
      "middleInitial": "Salemi",
      "importedId": "1212",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128596,
      "firstName": "Jiyang",
      "lastName": "Li",
      "importedId": "1454",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128597,
      "firstName": "Qun",
      "lastName": "Niu",
      "importedId": "1696",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128598,
      "firstName": "Anandghan",
      "lastName": "Waghmare",
      "importedId": "1211",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128599,
      "firstName": "Cheng",
      "lastName": "Zhang",
      "importedId": "1453",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128600,
      "firstName": "Tao",
      "lastName": "He",
      "importedId": "1695",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128601,
      "firstName": "Moustafa",
      "lastName": "Youssef",
      "importedId": "1210",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128602,
      "firstName": "François",
      "lastName": "Guimbretière",
      "importedId": "1452",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128603,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "importedId": "1694",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128604,
      "firstName": "Qian",
      "lastName": "Zhang",
      "importedId": "1219",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128605,
      "firstName": "Qianyi",
      "lastName": "Huang",
      "importedId": "1218",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128606,
      "firstName": "Joseph",
      "lastName": "Breda",
      "importedId": "1231",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128607,
      "firstName": "Eyal",
      "lastName": "Lara",
      "middleInitial": "De",
      "importedId": "1473",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128608,
      "firstName": "Mayank",
      "lastName": "Goel",
      "importedId": "1230",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128609,
      "firstName": "Andrea",
      "lastName": "Gershon",
      "middleInitial": "S",
      "importedId": "1472",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128610,
      "firstName": "Robert",
      "lastName": "Wu",
      "importedId": "1471",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128611,
      "firstName": "Salaar",
      "lastName": "Liaqat",
      "importedId": "1470",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128612,
      "firstName": "Bryan",
      "lastName": "Carroll",
      "middleInitial": "T",
      "importedId": "1228",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128613,
      "firstName": "Alexander",
      "lastName": "Maytin",
      "middleInitial": "K",
      "importedId": "1227",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128614,
      "firstName": "Sejal",
      "lastName": "Bhalla",
      "importedId": "1469",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128615,
      "firstName": "Haarika",
      "lastName": "Reddy",
      "middleInitial": "A",
      "importedId": "1226",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128616,
      "firstName": "Ricardo",
      "lastName": "Baeza-Yates",
      "importedId": "1468",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128617,
      "firstName": "Dustin",
      "lastName": "Demeo",
      "middleInitial": "P",
      "importedId": "1225",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128618,
      "firstName": "Athena",
      "lastName": "Vakali",
      "importedId": "1467",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128619,
      "firstName": "Emma",
      "lastName": "Russell",
      "importedId": "1224",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128620,
      "firstName": "Pavlos",
      "lastName": "Sermpezis",
      "importedId": "1466",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128621,
      "firstName": "Suzanne",
      "lastName": "Nie",
      "importedId": "1223",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128622,
      "firstName": "Sofia",
      "lastName": "Yfantidou",
      "importedId": "1465",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128623,
      "firstName": "Vimal",
      "lastName": "Mollyn",
      "importedId": "1222",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128624,
      "firstName": "Zhanpeng",
      "lastName": "Jin",
      "importedId": "1464",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128625,
      "firstName": "Hiromu",
      "lastName": "Yakura",
      "importedId": "1221",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128626,
      "firstName": "Junsong",
      "lastName": "Yuan",
      "importedId": "1463",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128627,
      "firstName": "Jill",
      "lastName": "Lehman",
      "middleInitial": "Fain",
      "importedId": "1229",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128628,
      "firstName": "WEI",
      "lastName": "LOU",
      "importedId": "1242",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128629,
      "firstName": "Fan",
      "lastName": "Dong",
      "importedId": "1484",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128630,
      "firstName": "HAIMING",
      "lastName": "CHENG",
      "importedId": "1241",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128631,
      "firstName": "Jianfei",
      "lastName": "Shen",
      "importedId": "1483",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128632,
      "firstName": "Karrie",
      "lastName": "Karahalios",
      "importedId": "1240",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128633,
      "firstName": "Yang",
      "lastName": "Gu",
      "importedId": "1482",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128634,
      "firstName": "Feiyi",
      "lastName": "Fan",
      "importedId": "1481",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128635,
      "firstName": "Josiah",
      "lastName": "Hester",
      "importedId": "1480",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128636,
      "firstName": "Indranil",
      "lastName": "Gupta",
      "importedId": "1239",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128637,
      "firstName": "Camille",
      "lastName": "Cobb",
      "importedId": "1238",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128638,
      "firstName": "Vinay",
      "lastName": "Koshy",
      "importedId": "1237",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128639,
      "firstName": "Yang",
      "lastName": "Zhang",
      "importedId": "1479",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128640,
      "firstName": "Rui",
      "lastName": "Yang",
      "importedId": "1236",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128641,
      "firstName": "Julian",
      "lastName": "Richey",
      "importedId": "1478",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128642,
      "firstName": "Ali",
      "lastName": "Zaidi",
      "importedId": "1235",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128643,
      "firstName": "Thomas",
      "lastName": "Cohen",
      "importedId": "1477",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128644,
      "firstName": "Shwetek",
      "lastName": "Patel",
      "importedId": "1234",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128645,
      "firstName": "Amy",
      "lastName": "Guo",
      "importedId": "1476",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128646,
      "firstName": "Alex",
      "lastName": "Mariakakis",
      "importedId": "1233",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128647,
      "firstName": "John",
      "lastName": "Mamish",
      "importedId": "1475",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128648,
      "firstName": "Mastafa",
      "lastName": "Springston",
      "importedId": "1232",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128649,
      "firstName": "Alex",
      "lastName": "Mariakakis",
      "importedId": "1474",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128651,
      "firstName": "Gurunandan",
      "lastName": "Krishnan",
      "importedId": "1495",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128653,
      "firstName": "Vu",
      "lastName": "Tran",
      "middleInitial": "An",
      "importedId": "1494",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128655,
      "firstName": "Bing",
      "lastName": "Zhou",
      "importedId": "1493",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128656,
      "firstName": "Jeff",
      "lastName": "Huang",
      "importedId": "1250",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128657,
      "firstName": "Jian",
      "lastName": "Wang",
      "importedId": "1492",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128658,
      "firstName": "Qijia",
      "lastName": "Shao",
      "importedId": "1491",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128659,
      "firstName": "Asif",
      "lastName": "Salekin",
      "importedId": "1490",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128660,
      "firstName": "Tongyu",
      "lastName": "Zhou",
      "importedId": "1249",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128661,
      "firstName": "Jing",
      "lastName": "Qian",
      "importedId": "1248",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128662,
      "firstName": "Jiaju",
      "lastName": "Ma",
      "importedId": "1247",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128663,
      "firstName": "Avery",
      "lastName": "Gump",
      "importedId": "1489",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128664,
      "firstName": "Helma",
      "lastName": "Torkamaan",
      "importedId": "1246",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128665,
      "firstName": "Harshit",
      "lastName": "Sharma",
      "importedId": "1488",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128666,
      "firstName": "XINYU",
      "lastName": "ZHANG",
      "importedId": "1245",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128667,
      "firstName": "Yi",
      "lastName": "Xiao",
      "importedId": "1487",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128668,
      "firstName": "YI-PU",
      "lastName": "CHEN",
      "importedId": "1244",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128669,
      "firstName": "Brian",
      "lastName": "Testa",
      "importedId": "1486",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128670,
      "firstName": "YANNI",
      "lastName": "YANG",
      "importedId": "1243",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128671,
      "firstName": "Yiqiang",
      "lastName": "Chen",
      "importedId": "1485",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128672,
      "firstName": "Anush",
      "lastName": "Lingamoorthy",
      "importedId": "1264",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128673,
      "firstName": "Claire",
      "lastName": "Kendell",
      "importedId": "1263",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128674,
      "firstName": "Amanda",
      "lastName": "Watson",
      "importedId": "1262",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128680,
      "firstName": "STEVE",
      "lastName": "HODGES",
      "importedId": "1499",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128682,
      "firstName": "RAF",
      "lastName": "RAMAKERS",
      "importedId": "1498",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128684,
      "firstName": "MANNU",
      "lastName": "LAMBRICHTS",
      "importedId": "1497",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128686,
      "firstName": "Shree",
      "lastName": "Nayar",
      "importedId": "1496",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128687,
      "firstName": "CHOONG-HOON",
      "lastName": "LEE",
      "importedId": "1275",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128688,
      "firstName": "HYOUNGSHICK",
      "lastName": "KIM",
      "importedId": "1274",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128689,
      "firstName": "JUWON",
      "lastName": "LEE",
      "importedId": "1273",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128690,
      "firstName": "GEUMHWAN",
      "lastName": "CHO",
      "importedId": "1272",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128691,
      "firstName": "YOUNGHAN",
      "lastName": "PARK",
      "importedId": "1271",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128692,
      "firstName": "ALEXANDR",
      "lastName": "POPOV",
      "importedId": "1270",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128693,
      "firstName": "ILJOO",
      "lastName": "KIM",
      "importedId": "1269",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128694,
      "firstName": "SUNGSU",
      "lastName": "KWAG",
      "importedId": "1268",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128695,
      "firstName": "JUN",
      "lastName": "HUH",
      "middleInitial": "HO",
      "importedId": "1267",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128696,
      "firstName": "James",
      "lastName": "Weimer",
      "importedId": "1266",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128697,
      "firstName": "Insup",
      "lastName": "Lee",
      "importedId": "1265",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128698,
      "firstName": "Heng",
      "lastName": "Zhou",
      "importedId": "1286",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128699,
      "firstName": "Nabil",
      "lastName": "Alshurafa",
      "importedId": "1285",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128700,
      "firstName": "Aggelos",
      "lastName": "Katsaggelos",
      "middleInitial": "K",
      "importedId": "1284",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128701,
      "firstName": "Josiah",
      "lastName": "Hester",
      "importedId": "1283",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128702,
      "firstName": "Christopher",
      "lastName": "Romano",
      "importedId": "1282",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128703,
      "firstName": "Mahdi",
      "lastName": "Pedram",
      "importedId": "1281",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128704,
      "firstName": "Sougata",
      "lastName": "Sen",
      "importedId": "1280",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128705,
      "firstName": "Lingfeng",
      "lastName": "Li",
      "importedId": "1279",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128706,
      "firstName": "Stefany",
      "lastName": "Cruz",
      "importedId": "1278",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128707,
      "firstName": "Soroush",
      "lastName": "Shahi",
      "importedId": "1277",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128708,
      "firstName": "Rawan",
      "lastName": "Alharbi",
      "importedId": "1276",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128709,
      "firstName": "Keylonnie",
      "lastName": "Miller",
      "middleInitial": "L",
      "importedId": "1297",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128710,
      "firstName": "Xincheng",
      "lastName": "Huang",
      "importedId": "1296",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128711,
      "firstName": "Nicholas",
      "lastName": "Lane",
      "middleInitial": "D",
      "importedId": "1295",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128712,
      "firstName": "Edgar",
      "lastName": "Liberis",
      "importedId": "1294",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128713,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "importedId": "1293",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128714,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "importedId": "1292",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128715,
      "firstName": "Yukang",
      "lastName": "Yan",
      "importedId": "1291",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128716,
      "firstName": "Chun",
      "lastName": "Yu",
      "importedId": "1290",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128717,
      "firstName": "Chi",
      "lastName": "Hsia",
      "importedId": "1289",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128718,
      "firstName": "Chen",
      "lastName": "Liang",
      "importedId": "1288",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128719,
      "firstName": "Takuya",
      "lastName": "Maekawa",
      "importedId": "1287",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128720,
      "firstName": "Nikola",
      "lastName": "Banovic",
      "importedId": "1299",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128721,
      "firstName": "Alanson",
      "lastName": "Sample",
      "middleInitial": "P",
      "importedId": "1298",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128722,
      "firstName": "Ruiqi",
      "lastName": "Wang",
      "importedId": "1909",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128723,
      "firstName": "Ashraf",
      "lastName": "Rjob",
      "importedId": "1908",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128724,
      "firstName": "Ruixuan",
      "lastName": "Dai",
      "importedId": "1907",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128725,
      "firstName": "Jingwen",
      "lastName": "Zhang",
      "importedId": "1906",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128726,
      "firstName": "ZHANPENG",
      "lastName": "JIN",
      "importedId": "1905",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128727,
      "firstName": "HENRY",
      "lastName": "ADLER",
      "middleInitial": "J",
      "importedId": "1904",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128728,
      "firstName": "ZHENGXIONG",
      "lastName": "LI",
      "importedId": "1903",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128729,
      "firstName": "SEOKMIN",
      "lastName": "CHOI",
      "importedId": "1902",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128730,
      "firstName": "XUHAI",
      "lastName": "XU",
      "importedId": "1901",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128731,
      "firstName": "YANG",
      "lastName": "GAO",
      "importedId": "1900",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128732,
      "firstName": "Shengdong",
      "lastName": "Zhao",
      "importedId": "1919",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128733,
      "firstName": "Reshad",
      "lastName": "Hamauon",
      "importedId": "1910",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128734,
      "firstName": "Lin",
      "lastName": "Zhu",
      "importedId": "1918",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128735,
      "firstName": "Jie",
      "lastName": "Gao",
      "importedId": "1917",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128736,
      "firstName": "Nuwan",
      "lastName": "Janaka",
      "importedId": "1916",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128737,
      "firstName": "Chenyang",
      "lastName": "Lu",
      "importedId": "1915",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128738,
      "firstName": "Maria",
      "lastName": "Vazquez Guillamet",
      "middleInitial": "Cristina",
      "importedId": "1914",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128739,
      "firstName": "Victoria",
      "lastName": "Fraser",
      "middleInitial": "J",
      "importedId": "1913",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128740,
      "firstName": "Thomas",
      "lastName": "Bailey",
      "importedId": "1912",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128741,
      "firstName": "Jeffrey",
      "lastName": "Candell",
      "importedId": "1911",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128742,
      "firstName": "Peisen",
      "lastName": "Xu",
      "importedId": "1921",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128743,
      "firstName": "Lan",
      "lastName": "Lyu",
      "importedId": "1920",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128744,
      "firstName": "TIMOTHY",
      "lastName": "JACQUES",
      "importedId": "1929",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128745,
      "firstName": "DOMINIC",
      "lastName": "KONRAD",
      "importedId": "1928",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128746,
      "firstName": "SHAMIK",
      "lastName": "SARKAR",
      "importedId": "1927",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128747,
      "firstName": "TIANYI",
      "lastName": "ZHAO",
      "importedId": "1926",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128748,
      "firstName": "JUSTIN",
      "lastName": "FENG",
      "importedId": "1925",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128749,
      "firstName": "Yanch",
      "lastName": "Ong",
      "importedId": "1924",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128750,
      "firstName": "Silang",
      "lastName": "Wang",
      "importedId": "1923",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128751,
      "firstName": "Maximilian",
      "lastName": "Nabokow",
      "importedId": "1922",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128752,
      "firstName": "Yongquan",
      "lastName": "Hu",
      "importedId": "1932",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128753,
      "firstName": "NADER",
      "lastName": "SEHATBAKHSH",
      "importedId": "1931",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128754,
      "firstName": "DANIJELA",
      "lastName": "CABRIC",
      "importedId": "1930",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128755,
      "firstName": "Chinmaey",
      "lastName": "Shende",
      "importedId": "1939",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128756,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "importedId": "1938",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128757,
      "firstName": "Wen",
      "lastName": "Hu",
      "importedId": "1937",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128758,
      "firstName": "Don",
      "lastName": "Elvitigala",
      "middleInitial": "Samitha",
      "importedId": "1936",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128759,
      "firstName": "Haoran",
      "lastName": "Fan",
      "importedId": "1935",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128760,
      "firstName": "Mingyue",
      "lastName": "Yuan",
      "importedId": "1934",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128761,
      "firstName": "Hui-Shyong",
      "lastName": "Yeo",
      "importedId": "1933",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128762,
      "firstName": "Yujiang",
      "lastName": "Wang",
      "importedId": "1701",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128763,
      "firstName": "Reynaldo",
      "lastName": "Morillo",
      "importedId": "1943",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128764,
      "firstName": "Wentao",
      "lastName": "Pan",
      "importedId": "1700",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128765,
      "firstName": "Parit",
      "lastName": "Patel",
      "importedId": "1942",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128766,
      "firstName": "Stephen",
      "lastName": "Sam",
      "importedId": "1941",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128767,
      "firstName": "Soumyashree",
      "lastName": "Sahoo",
      "importedId": "1940",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128768,
      "firstName": "Ning",
      "lastName": "Gu",
      "importedId": "1709",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128769,
      "firstName": "Tun",
      "lastName": "Lu",
      "importedId": "1708",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128770,
      "firstName": "Fan",
      "lastName": "Yang",
      "importedId": "1707",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128771,
      "firstName": "Dongjin",
      "lastName": "Song",
      "importedId": "1949",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128772,
      "firstName": "Dongsheng",
      "lastName": "Li",
      "importedId": "1706",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128773,
      "firstName": "Alexander",
      "lastName": "Russell",
      "importedId": "1948",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128774,
      "firstName": "Robert",
      "lastName": "Dick",
      "middleInitial": "P",
      "importedId": "1705",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128775,
      "firstName": "Jayesh",
      "lastName": "Kamath",
      "importedId": "1947",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128776,
      "firstName": "Qin",
      "lastName": "Lv",
      "importedId": "1704",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128777,
      "firstName": "Jinbo",
      "lastName": "Bi",
      "importedId": "1946",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128778,
      "firstName": "Xianghua",
      "lastName": "Ding",
      "middleInitial": "(Sharon)",
      "importedId": "1703",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128779,
      "firstName": "Shweta",
      "lastName": "Ware",
      "importedId": "1945",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128780,
      "firstName": "Mingzhi",
      "lastName": "Dong",
      "importedId": "1702",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128781,
      "firstName": "Xinyu",
      "lastName": "Wang",
      "importedId": "1944",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128782,
      "firstName": "Hyejin",
      "lastName": "Shin",
      "importedId": "1712",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128784,
      "firstName": "Jun",
      "lastName": "Huh",
      "middleInitial": "Ho",
      "importedId": "1711",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128786,
      "firstName": "Li",
      "lastName": "Shang",
      "importedId": "1710",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128789,
      "firstName": "Bing",
      "lastName": "Wang",
      "importedId": "1950",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128790,
      "firstName": "Xin",
      "lastName": "Liu",
      "importedId": "1719",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128791,
      "firstName": "Xuhai",
      "lastName": "Xu",
      "importedId": "1718",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128792,
      "firstName": "Ian",
      "lastName": "Oakley",
      "importedId": "1717",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128793,
      "firstName": "KUNPENG",
      "lastName": "HUANG",
      "importedId": "1959",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128794,
      "firstName": "Choong-Hoon",
      "lastName": "Lee",
      "importedId": "1716",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128795,
      "firstName": "PIN-SUNG",
      "lastName": "KU",
      "importedId": "1958",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128796,
      "firstName": "Youngeun",
      "lastName": "Song",
      "importedId": "1715",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128798,
      "firstName": "Eunyong",
      "lastName": "Cheon",
      "importedId": "1714",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128800,
      "firstName": "Hongmin",
      "lastName": "Kim",
      "importedId": "1713",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128802,
      "firstName": "Kevin",
      "lastName": "Kuehn",
      "middleInitial": "S",
      "importedId": "1723",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128803,
      "firstName": "Lin",
      "lastName": "Shi",
      "importedId": "1965",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128804,
      "firstName": "Subigya",
      "lastName": "Nepal",
      "importedId": "1722",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128805,
      "firstName": "Weina",
      "lastName": "Jiang",
      "importedId": "1964",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128806,
      "firstName": "Weichen",
      "lastName": "Wang",
      "importedId": "1721",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128807,
      "firstName": "HSIN-LIU",
      "lastName": "KAO",
      "middleInitial": "(CINDY)",
      "importedId": "1963",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128808,
      "firstName": "Han",
      "lastName": "Zhang",
      "importedId": "1720",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128809,
      "firstName": "ALICIA",
      "lastName": "CHU",
      "importedId": "1962",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128810,
      "firstName": "BOAZ",
      "lastName": "NG",
      "importedId": "1961",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128811,
      "firstName": "NANCY",
      "lastName": "WANG",
      "importedId": "1960",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128812,
      "firstName": "Tim",
      "lastName": "Althoff",
      "importedId": "1729",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128813,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "importedId": "1728",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128814,
      "firstName": "Eve",
      "lastName": "Riskin",
      "middleInitial": "A",
      "importedId": "1727",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128815,
      "firstName": "Haoran",
      "lastName": "Wan",
      "importedId": "1969",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128816,
      "firstName": "Paula",
      "lastName": "Nurius",
      "middleInitial": "S",
      "importedId": "1726",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128817,
      "firstName": "Shang",
      "lastName": "Zeng",
      "importedId": "1968",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128818,
      "firstName": "Margaret",
      "lastName": "Morris",
      "middleInitial": "E",
      "importedId": "1725",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128819,
      "firstName": "Ning",
      "lastName": "Liu",
      "importedId": "1967",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128820,
      "firstName": "Jeremy",
      "lastName": "Huckins",
      "middleInitial": "F",
      "importedId": "1724",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128821,
      "firstName": "Qun",
      "lastName": "Niu",
      "importedId": "1966",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128822,
      "firstName": "Rei",
      "lastName": "Nakaoka",
      "importedId": "1734",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128823,
      "firstName": "SUNG",
      "lastName": "HWANG",
      "middleInitial": "JU",
      "importedId": "1976",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128824,
      "firstName": "Yugo",
      "lastName": "Nakamura",
      "importedId": "1733",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128825,
      "firstName": "YUNXIN",
      "lastName": "LIU",
      "importedId": "1975",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128826,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "importedId": "1732",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128827,
      "firstName": "ADIBA",
      "lastName": "ORZIKULOVA",
      "importedId": "1974",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128828,
      "firstName": "Anind",
      "lastName": "Dey",
      "middleInitial": "K",
      "importedId": "1731",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128829,
      "firstName": "YEWON",
      "lastName": "KIM",
      "importedId": "1973",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128830,
      "firstName": "Andrew",
      "lastName": "Campbell",
      "importedId": "1730",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128831,
      "firstName": "TAESIK",
      "lastName": "GONG",
      "importedId": "1972",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128832,
      "firstName": "Wei",
      "lastName": "Wang",
      "importedId": "1971",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128833,
      "firstName": "Shuyu",
      "lastName": "Shi",
      "importedId": "1970",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128834,
      "firstName": "Priyanka",
      "lastName": "Mammen",
      "middleInitial": "Mary",
      "importedId": "1739",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128835,
      "firstName": "Gizem",
      "lastName": "Yilmaz",
      "importedId": "1738",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128836,
      "firstName": "Camellia",
      "lastName": "Zakaria",
      "importedId": "1737",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128837,
      "firstName": "Daniel",
      "lastName": "Medeiros",
      "importedId": "1979",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128838,
      "firstName": "Keiichi",
      "lastName": "Yasumoto",
      "importedId": "1736",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128839,
      "firstName": "SUNG-JU",
      "lastName": "LEE",
      "importedId": "1978",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128840,
      "firstName": "Yuki",
      "lastName": "Matsuda",
      "importedId": "1735",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128841,
      "firstName": "JINWOO",
      "lastName": "SHIN",
      "importedId": "1977",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128842,
      "firstName": "AKANE",
      "lastName": "SANO",
      "importedId": "1990",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128843,
      "firstName": "Matias",
      "lastName": "Laporte",
      "importedId": "1503",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128844,
      "firstName": "David",
      "lastName": "Li",
      "importedId": "1745",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128845,
      "firstName": "ALBRECHT",
      "lastName": "SCHMIDT",
      "importedId": "1987",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128846,
      "firstName": "JOE",
      "lastName": "FINNEY",
      "importedId": "1502",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128847,
      "firstName": "Clayton",
      "lastName": "Kimber",
      "importedId": "1744",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128848,
      "firstName": "SABA",
      "lastName": "KHEIRINEJAD",
      "importedId": "1986",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128849,
      "firstName": "LORRAINE",
      "lastName": "UNDERWOOD",
      "importedId": "1501",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128850,
      "firstName": "Ananta",
      "lastName": "Balaji",
      "middleInitial": "Narayanan",
      "importedId": "1743",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128851,
      "firstName": "LUKE",
      "lastName": "HALIBURTON",
      "importedId": "1985",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128852,
      "firstName": "JAMES",
      "lastName": "DEVINE",
      "importedId": "1500",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128853,
      "firstName": "Rajesh",
      "lastName": "Balan",
      "importedId": "1742",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128854,
      "firstName": "Mark",
      "lastName": "Mcgill",
      "importedId": "1984",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128855,
      "firstName": "Prashant",
      "lastName": "Shenoy",
      "importedId": "1741",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128856,
      "firstName": "Katharina",
      "lastName": "Pöhlmann",
      "importedId": "1983",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128857,
      "firstName": "Michael",
      "lastName": "Chee",
      "importedId": "1740",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128858,
      "firstName": "Graham",
      "lastName": "Wilson",
      "importedId": "1982",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128859,
      "firstName": "Julie",
      "lastName": "Williamson",
      "importedId": "1981",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128860,
      "firstName": "Romane",
      "lastName": "Dubus",
      "importedId": "1980",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128861,
      "firstName": "Mingming",
      "lastName": "Fan",
      "importedId": "1509",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128862,
      "firstName": "Yuru",
      "lastName": "Huang",
      "importedId": "1508",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128863,
      "firstName": "Chen",
      "lastName": "Liang",
      "importedId": "1507",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128864,
      "firstName": "Elizabeth",
      "lastName": "Murnane",
      "middleInitial": "L",
      "importedId": "1749",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128865,
      "firstName": "Zisu",
      "lastName": "Li",
      "importedId": "1506",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128866,
      "firstName": "David",
      "lastName": "Kim",
      "importedId": "1748",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128867,
      "firstName": "Marc",
      "lastName": "Langheinrich",
      "importedId": "1505",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128868,
      "firstName": "Ruofei",
      "lastName": "Du",
      "importedId": "1747",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128869,
      "firstName": "HAN",
      "lastName": "YU",
      "importedId": "1989",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128870,
      "firstName": "Martin",
      "lastName": "Gjoreski",
      "importedId": "1504",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128871,
      "firstName": "Shengzhi",
      "lastName": "Wu",
      "importedId": "1746",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128872,
      "firstName": "SVEN",
      "lastName": "MAYER",
      "importedId": "1988",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128873,
      "firstName": "Di",
      "lastName": "Wu",
      "importedId": "1514",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128874,
      "firstName": "Yaman",
      "lastName": "Sangar",
      "importedId": "1756",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128875,
      "firstName": "RIKU",
      "lastName": "ARAKAWA",
      "importedId": "1998",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128876,
      "firstName": "Yincheng",
      "lastName": "Jin",
      "importedId": "1513",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128877,
      "firstName": "James",
      "lastName": "Landay",
      "middleInitial": "A",
      "importedId": "1755",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128878,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "importedId": "1997",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128879,
      "firstName": "Se",
      "lastName": "Kim",
      "middleInitial": "Jun",
      "importedId": "1512",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128880,
      "firstName": "Paula",
      "lastName": "L. Moya",
      "middleInitial": "M",
      "importedId": "1754",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128881,
      "firstName": "Hewu",
      "lastName": "Li",
      "importedId": "1996",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128882,
      "firstName": "Junghwan",
      "lastName": "Yim",
      "importedId": "1511",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128883,
      "firstName": "Grace",
      "lastName": "Zhao",
      "importedId": "1753",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128884,
      "firstName": "Chun",
      "lastName": "Yu",
      "importedId": "1995",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128885,
      "firstName": "Seokmin",
      "lastName": "Choi",
      "importedId": "1510",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128886,
      "firstName": "Raymond",
      "lastName": "Yao",
      "importedId": "1752",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128887,
      "firstName": "Jiuxu",
      "lastName": "Song",
      "importedId": "1994",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128888,
      "firstName": "Jean",
      "lastName": "Costa",
      "importedId": "1751",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128889,
      "firstName": "Haozhan",
      "lastName": "Chen",
      "importedId": "1993",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128890,
      "firstName": "Yekaterina",
      "lastName": "Glazko",
      "middleInitial": "S",
      "importedId": "1750",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128891,
      "firstName": "Chen",
      "lastName": "Liang",
      "importedId": "1992",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128892,
      "firstName": "Xin",
      "lastName": "Yi",
      "importedId": "1991",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128893,
      "firstName": "Robin",
      "lastName": "Welsch",
      "importedId": "1519",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128894,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "importedId": "1518",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128895,
      "firstName": "Jasmin",
      "lastName": "Niess",
      "importedId": "1517",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128896,
      "firstName": "Vaishnavi",
      "lastName": "Ranganathan",
      "importedId": "1759",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128897,
      "firstName": "Steeven",
      "lastName": "Villa",
      "importedId": "1516",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128898,
      "firstName": "Kai",
      "lastName": "Pederson",
      "importedId": "1758",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128899,
      "firstName": "Zhanpeng",
      "lastName": "Jin",
      "importedId": "1515",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128900,
      "firstName": "Yoganand",
      "lastName": "Biradavolu",
      "importedId": "1757",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128901,
      "firstName": "KARAN",
      "lastName": "AHUJA",
      "importedId": "1999",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128902,
      "firstName": "Yanwen",
      "lastName": "Wang",
      "importedId": "1770",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128903,
      "firstName": "Shashwat",
      "lastName": "Kumar",
      "importedId": "1525",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128904,
      "firstName": "Huafeng",
      "lastName": "Xu",
      "importedId": "1767",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128905,
      "firstName": "Katharine",
      "lastName": "Daniel",
      "middleInitial": "E",
      "importedId": "1524",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128906,
      "firstName": "Yanni",
      "lastName": "Yang",
      "importedId": "1766",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128907,
      "firstName": "Emma",
      "lastName": "Toner",
      "middleInitial": "R",
      "importedId": "1523",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128908,
      "firstName": "Yiqiang",
      "lastName": "Chen",
      "importedId": "1765",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128909,
      "firstName": "Mark",
      "lastName": "Rucker",
      "importedId": "1522",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128910,
      "firstName": "Yanrong",
      "lastName": "Li",
      "importedId": "1764",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128911,
      "firstName": "Maria",
      "lastName": "Larrazabal",
      "middleInitial": "A",
      "importedId": "1521",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128912,
      "firstName": "Chenren",
      "lastName": "Xu",
      "importedId": "1763",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128913,
      "firstName": "Zhiyuan",
      "lastName": "Wang",
      "importedId": "1520",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128914,
      "firstName": "Zitong",
      "lastName": "Lan",
      "importedId": "1762",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128915,
      "firstName": "Tengxiang",
      "lastName": "Zhang",
      "importedId": "1761",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128916,
      "firstName": "Bhuvana",
      "lastName": "Krishnaswamy",
      "importedId": "1760",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128917,
      "firstName": "Xiaoying",
      "lastName": "Yang",
      "importedId": "1529",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128918,
      "firstName": "Laura",
      "lastName": "Barnes",
      "middleInitial": "A",
      "importedId": "1528",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128919,
      "firstName": "Bethany",
      "lastName": "Teachman",
      "middleInitial": "A",
      "importedId": "1527",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128920,
      "firstName": "Jiannong",
      "lastName": "Cao",
      "importedId": "1769",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128921,
      "firstName": "Mehdi",
      "lastName": "Boukhechba",
      "importedId": "1526",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128922,
      "firstName": "Qianyi",
      "lastName": "Chen",
      "importedId": "1768",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128923,
      "firstName": "Dongmei",
      "lastName": "Li",
      "importedId": "1781",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128924,
      "firstName": "Xiaoue",
      "lastName": "Qian",
      "importedId": "1780",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128925,
      "firstName": "Hyunsung",
      "lastName": "Cho",
      "importedId": "1536",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128926,
      "firstName": "Zhengxiong",
      "lastName": "Li",
      "importedId": "1778",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128927,
      "firstName": "Yang",
      "lastName": "Zhang",
      "importedId": "1535",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128928,
      "firstName": "Chenhan",
      "lastName": "Xu",
      "importedId": "1777",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128929,
      "firstName": "Eiji",
      "lastName": "Hayashi",
      "importedId": "1534",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128930,
      "firstName": "Huan",
      "lastName": "Chen",
      "importedId": "1776",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128931,
      "firstName": "Mani",
      "lastName": "Srivastava",
      "importedId": "1533",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128932,
      "firstName": "Huining",
      "lastName": "Li",
      "importedId": "1775",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128933,
      "firstName": "Zihan",
      "lastName": "Yan",
      "importedId": "1532",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128934,
      "firstName": "Qian",
      "lastName": "Zhang",
      "importedId": "1774",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128935,
      "firstName": "Gaofeng",
      "lastName": "Dong",
      "importedId": "1531",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128936,
      "firstName": "Jin",
      "lastName": "Zhang",
      "importedId": "1773",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128937,
      "firstName": "Xue",
      "lastName": "Wang",
      "importedId": "1530",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128938,
      "firstName": "Qingyong",
      "lastName": "Hu",
      "importedId": "1772",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128939,
      "firstName": "Wentao",
      "lastName": "Xie",
      "importedId": "1771",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128940,
      "firstName": "Mahan",
      "lastName": "Tabatabaie",
      "importedId": "1539",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128941,
      "firstName": "David",
      "lastName": "Lindlbauer",
      "importedId": "1538",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128942,
      "firstName": "Matthew",
      "lastName": "Komar",
      "middleInitial": "L",
      "importedId": "1537",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128943,
      "firstName": "Hanbin",
      "lastName": "Zhang",
      "importedId": "1779",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128944,
      "firstName": "Fatima",
      "lastName": "Elhattab",
      "importedId": "1550",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128945,
      "firstName": "Jaewoong",
      "lastName": "Jang",
      "importedId": "1792",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128946,
      "firstName": "Yoomsu",
      "lastName": "Kim",
      "importedId": "1791",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128947,
      "firstName": "Sungjae",
      "lastName": "Cho",
      "importedId": "1790",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128948,
      "firstName": "Haocheng",
      "lastName": "Shen",
      "importedId": "1305",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128949,
      "firstName": "Sicong",
      "lastName": "Liu",
      "importedId": "1547",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128950,
      "firstName": "Yi",
      "lastName": "Rong",
      "importedId": "1789",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128951,
      "firstName": "Meng",
      "lastName": "Zhang",
      "importedId": "1304",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128952,
      "firstName": "Zhiwen",
      "lastName": "Yu",
      "importedId": "1546",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128953,
      "firstName": "Reza",
      "lastName": "Rawassizadeh",
      "importedId": "1788",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128954,
      "firstName": "Bin",
      "lastName": "Guo",
      "importedId": "1303",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128955,
      "firstName": "Yanfei",
      "lastName": "Wang",
      "importedId": "1545",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128956,
      "firstName": "Elissa",
      "lastName": "Redmiles",
      "middleInitial": "M",
      "importedId": "1787",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128957,
      "firstName": "Zimu",
      "lastName": "Zhou",
      "importedId": "1302",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128958,
      "firstName": "Xiang-Yang",
      "lastName": "Li",
      "importedId": "1544",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128959,
      "firstName": "Bart",
      "lastName": "Preneel",
      "importedId": "1786",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128960,
      "firstName": "Xiaochen",
      "lastName": "Li",
      "importedId": "1301",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128961,
      "firstName": "Yubo",
      "lastName": "Yan",
      "importedId": "1543",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128962,
      "firstName": "Sophie",
      "lastName": "Li",
      "importedId": "1785",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128963,
      "firstName": "Sicong",
      "lastName": "Liu",
      "importedId": "1300",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128964,
      "firstName": "Panlong",
      "lastName": "Yang",
      "importedId": "1542",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128965,
      "firstName": "Oshrat",
      "lastName": "Ayalon",
      "importedId": "1784",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128966,
      "firstName": "Fei",
      "lastName": "Shang",
      "importedId": "1541",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128967,
      "firstName": "Wenyao",
      "lastName": "Xu",
      "importedId": "1783",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128968,
      "firstName": "Suining",
      "lastName": "He",
      "importedId": "1540",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128969,
      "firstName": "Ming-Chun",
      "lastName": "Huang",
      "importedId": "1782",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128970,
      "firstName": "Shijia",
      "lastName": "Pan",
      "importedId": "1309",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128971,
      "firstName": "Yu",
      "lastName": "Lim",
      "middleInitial": "Kai",
      "importedId": "1308",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128972,
      "firstName": "Jong",
      "lastName": "Lee",
      "middleInitial": "Taek",
      "importedId": "1307",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128973,
      "firstName": "Bin",
      "lastName": "Guo",
      "importedId": "1549",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128974,
      "firstName": "Zhiwen",
      "lastName": "Yu",
      "importedId": "1306",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128975,
      "firstName": "Zimu",
      "lastName": "Zhou",
      "importedId": "1548",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128976,
      "firstName": "Daqing",
      "lastName": "Zhang",
      "importedId": "1561",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128977,
      "firstName": "Beihong",
      "lastName": "Jin",
      "importedId": "1560",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128978,
      "firstName": "Hiroaki",
      "lastName": "Murakami",
      "importedId": "1316",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128979,
      "firstName": "Jiazhi",
      "lastName": "Ni",
      "importedId": "1558",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128980,
      "firstName": "Matthew",
      "lastName": "Ishige",
      "importedId": "1315",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128981,
      "firstName": "Junqi",
      "lastName": "Ma",
      "importedId": "1557",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128982,
      "firstName": "Wenchao",
      "lastName": "Song",
      "importedId": "1799",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128983,
      "firstName": "Kota",
      "lastName": "Tsubouchi",
      "importedId": "1314",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128984,
      "firstName": "Jie",
      "lastName": "Xiong",
      "importedId": "1556",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128985,
      "firstName": "Zhuo",
      "lastName": "Sun",
      "importedId": "1798",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128986,
      "firstName": "Yoshiihiro",
      "lastName": "Kawahara",
      "importedId": "1313",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128987,
      "firstName": "Zhaoxin",
      "lastName": "Chang",
      "importedId": "1555",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128988,
      "firstName": "Zhu",
      "lastName": "Wang∗",
      "importedId": "1797",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128989,
      "firstName": "Masato",
      "lastName": "Sugasaki",
      "importedId": "1312",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128990,
      "firstName": "Fusang",
      "lastName": "Zhang",
      "importedId": "1554",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128991,
      "firstName": "Hualei",
      "lastName": "Zhang",
      "importedId": "1796",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128992,
      "firstName": "Yang",
      "lastName": "Zhen",
      "importedId": "1311",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128993,
      "firstName": "Vlad",
      "lastName": "Nitu",
      "importedId": "1553",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128994,
      "firstName": "Meiyi",
      "lastName": "Ma",
      "importedId": "1795",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128995,
      "firstName": "Jun",
      "lastName": "Han",
      "importedId": "1310",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128996,
      "firstName": "Rania",
      "lastName": "Talbi",
      "importedId": "1552",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128997,
      "firstName": "Hanchen",
      "lastName": "Wang",
      "middleInitial": "David",
      "importedId": "1794",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128998,
      "firstName": "Sara",
      "lastName": "Bouchenak",
      "importedId": "1551",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 128999,
      "firstName": "Inseok",
      "lastName": "Hwang",
      "importedId": "1793",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129000,
      "firstName": "Luis",
      "lastName": "Coelho",
      "middleInitial": "Falconeri",
      "importedId": "1319",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129001,
      "firstName": "David",
      "lastName": "Bethge",
      "importedId": "1318",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129002,
      "firstName": "Masamichi",
      "lastName": "Shimosaka",
      "importedId": "1317",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129003,
      "firstName": "Wenbo",
      "lastName": "Zhang",
      "importedId": "1559",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129004,
      "firstName": "Petteri",
      "lastName": "Nurmi",
      "importedId": "1330",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129005,
      "firstName": "Lin",
      "lastName": "Lin",
      "importedId": "1572",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129006,
      "firstName": "Zhihan",
      "lastName": "Jiang",
      "importedId": "1571",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129007,
      "firstName": "Yibo",
      "lastName": "Wang",
      "importedId": "1570",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129008,
      "firstName": "Abdul-Rasheed",
      "lastName": "Ottun",
      "importedId": "1327",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129009,
      "firstName": "Jiale",
      "lastName": "Li",
      "importedId": "1569",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129010,
      "firstName": "Mohan",
      "lastName": "Liyanage",
      "importedId": "1326",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129011,
      "firstName": "Xinhua",
      "lastName": "Gao",
      "importedId": "1568",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129012,
      "firstName": "Zhigang",
      "lastName": "Yin",
      "importedId": "1325",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129013,
      "firstName": "Chi",
      "lastName": "Lin",
      "importedId": "1567",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129014,
      "firstName": "Tobias",
      "lastName": "Grosse-Puppendahl",
      "importedId": "1324",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129015,
      "firstName": "Jie",
      "lastName": "Xiong",
      "importedId": "1566",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129016,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "importedId": "1323",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129017,
      "firstName": "Lei",
      "lastName": "Wang",
      "importedId": "1565",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129018,
      "firstName": "Ulrich",
      "lastName": "Zadow",
      "middleInitial": "Von",
      "importedId": "1322",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129019,
      "firstName": "Xinyang",
      "lastName": "Liu",
      "importedId": "1564",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129020,
      "firstName": "Satiyabooshan",
      "lastName": "Murugaboopathy",
      "importedId": "1321",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129021,
      "firstName": "Sanjib",
      "lastName": "Sur",
      "importedId": "1563",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129022,
      "firstName": "Thomas",
      "lastName": "Kosch",
      "importedId": "1320",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129023,
      "firstName": "Pingping",
      "lastName": "Cai",
      "importedId": "1562",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129024,
      "firstName": "Agustin",
      "lastName": "Zuniga",
      "importedId": "1329",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129025,
      "firstName": "Souvik",
      "lastName": "Paul",
      "importedId": "1328",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129026,
      "firstName": "Mi",
      "lastName": "Zhang",
      "importedId": "1341",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129027,
      "firstName": "Marit",
      "lastName": "Bentvelzen",
      "importedId": "1583",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129028,
      "firstName": "Sungmin",
      "lastName": "Lee",
      "importedId": "1340",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129029,
      "firstName": "Edith",
      "lastName": "Ngai",
      "middleInitial": "C.H",
      "importedId": "1582",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129030,
      "firstName": "Patrick",
      "lastName": "Ip",
      "importedId": "1581",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129031,
      "firstName": "Wilfred",
      "lastName": "Wong",
      "middleInitial": "H.S",
      "importedId": "1580",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129032,
      "firstName": "Jaeyeon",
      "lastName": "Park",
      "importedId": "1338",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129033,
      "firstName": "Huadong",
      "lastName": "Ma",
      "importedId": "1337",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129034,
      "firstName": "Hung-Kwan",
      "lastName": "So",
      "importedId": "1579",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129035,
      "firstName": "Rui",
      "lastName": "Yuan",
      "importedId": "1336",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129036,
      "firstName": "Ka-Man",
      "lastName": "Yip",
      "importedId": "1578",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129037,
      "firstName": "Zhengyuan",
      "lastName": "Zhang",
      "importedId": "1335",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129038,
      "firstName": "James",
      "lastName": "Lam",
      "importedId": "1577",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129039,
      "firstName": "Kuo",
      "lastName": "Tian",
      "importedId": "1334",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129040,
      "firstName": "Longbiao",
      "lastName": "Chen",
      "importedId": "1576",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129041,
      "firstName": "Dong",
      "lastName": "Zhao",
      "importedId": "1333",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129042,
      "firstName": "Running",
      "lastName": "Zhao",
      "importedId": "1575",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129043,
      "firstName": "Guanzhou",
      "lastName": "Zhu",
      "importedId": "1332",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129044,
      "firstName": "Jianduo",
      "lastName": "Luan",
      "importedId": "1574",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129045,
      "firstName": "Huber",
      "lastName": "Flores",
      "importedId": "1331",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129046,
      "firstName": "Xinchen",
      "lastName": "Zhang",
      "importedId": "1573",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129047,
      "firstName": "Kichang",
      "lastName": "Lee",
      "importedId": "1339",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129048,
      "firstName": "Jungeun",
      "lastName": "Lee",
      "importedId": "1352",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129049,
      "firstName": "Dong",
      "lastName": "Yoo",
      "middleInitial": "Whi",
      "importedId": "1594",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129050,
      "firstName": "Youngho",
      "lastName": "Nam",
      "importedId": "1351",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129051,
      "firstName": "Tingyu",
      "lastName": "Cheng",
      "importedId": "1593",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129052,
      "firstName": "Jiha",
      "lastName": "Kim",
      "importedId": "1350",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129053,
      "firstName": "Sienna",
      "lastName": "Sun",
      "middleInitial": "Xin",
      "importedId": "1592",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129054,
      "firstName": "Jung",
      "lastName": "Park",
      "middleInitial": "Wook",
      "importedId": "1591",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129055,
      "firstName": "Ian",
      "lastName": "Oakley",
      "importedId": "1590",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129056,
      "firstName": "Seungjun",
      "lastName": "Kim",
      "importedId": "1349",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129057,
      "firstName": "Daniela",
      "lastName": "Rus",
      "importedId": "1348",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129058,
      "firstName": "Taewoo",
      "lastName": "Jo",
      "importedId": "1347",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129059,
      "firstName": "Hyunjae",
      "lastName": "Gil",
      "importedId": "1589",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129060,
      "firstName": "Dohyeon",
      "lastName": "Yeo",
      "importedId": "1346",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129061,
      "firstName": "Mostafa",
      "lastName": "Ammar",
      "importedId": "1588",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129062,
      "firstName": "Gwangbin",
      "lastName": "Kim",
      "importedId": "1345",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129063,
      "firstName": "Ashutosh",
      "lastName": "Dhekne",
      "importedId": "1587",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129064,
      "firstName": "CHRISTIAN",
      "lastName": "HOLZ",
      "importedId": "1344",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129065,
      "firstName": "Yifeng",
      "lastName": "Cao",
      "importedId": "1586",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129066,
      "firstName": "MANUEL",
      "lastName": "MEIER",
      "importedId": "1343",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129067,
      "firstName": "Paweł",
      "lastName": "Woźniak",
      "middleInitial": "W",
      "importedId": "1585",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129068,
      "firstName": "Jeonggil",
      "lastName": "Ko",
      "importedId": "1342",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129069,
      "firstName": "Jasmin",
      "lastName": "Niess",
      "importedId": "1584",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129070,
      "firstName": "SIRISH",
      "lastName": "GAMBHIRA",
      "importedId": "1121",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129071,
      "firstName": "Yingying",
      "lastName": "Zhao",
      "importedId": "1363",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129072,
      "firstName": "MEGHNA",
      "lastName": "GUPTA",
      "importedId": "1120",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129073,
      "firstName": "Wentao",
      "lastName": "Pan",
      "importedId": "1362",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129074,
      "firstName": "Qi",
      "lastName": "Lu",
      "importedId": "1361",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129075,
      "firstName": "Xiangyao",
      "lastName": "Qi",
      "importedId": "1360",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129076,
      "firstName": "AMISH",
      "lastName": "MITTAL",
      "importedId": "1118",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129077,
      "firstName": "SAUMAY",
      "lastName": "PUSHP",
      "importedId": "1117",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129078,
      "firstName": "Kang",
      "lastName": "Shin",
      "middleInitial": "G",
      "importedId": "1359",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129079,
      "firstName": "HARSH",
      "lastName": "VIJAY",
      "importedId": "1116",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129080,
      "firstName": "Suining",
      "lastName": "He",
      "importedId": "1358",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129081,
      "firstName": "Shan",
      "lastName": "Lu",
      "importedId": "1115",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129082,
      "firstName": "Mahan",
      "lastName": "Tabatabaie",
      "importedId": "1357",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129083,
      "firstName": "Lakmal",
      "lastName": "Meegahapola",
      "importedId": "1599",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129084,
      "firstName": "Blase",
      "lastName": "Ur",
      "importedId": "1114",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129085,
      "firstName": "Syed",
      "lastName": "Billah",
      "middleInitial": "Masum",
      "importedId": "1356",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129086,
      "firstName": "Rosa",
      "lastName": "Arriaga",
      "middleInitial": "I",
      "importedId": "1598",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129087,
      "firstName": "Md",
      "lastName": "Ehtesham-Ul-Haque",
      "importedId": "1355",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129088,
      "firstName": "Gregory",
      "lastName": "Abowd",
      "middleInitial": "D",
      "importedId": "1597",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129089,
      "firstName": "Inseok",
      "lastName": "Hwang",
      "importedId": "1354",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129090,
      "firstName": "Youngwook",
      "lastName": "Do",
      "importedId": "1596",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129091,
      "firstName": "Young-Joo",
      "lastName": "Suh",
      "importedId": "1353",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129092,
      "firstName": "Jiawei",
      "lastName": "Zhou",
      "importedId": "1595",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129093,
      "firstName": "PRAVEEN",
      "lastName": "GUPTA",
      "importedId": "1119",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129094,
      "firstName": "Masum",
      "lastName": "Hasan",
      "importedId": "1132",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129095,
      "firstName": "Weizhe",
      "lastName": "Xu",
      "importedId": "1374",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129096,
      "firstName": "Wasifur",
      "lastName": "Rahman",
      "importedId": "1131",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129097,
      "firstName": "Weichen",
      "lastName": "Wang",
      "importedId": "1373",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129098,
      "firstName": "Shizhe",
      "lastName": "Liu",
      "importedId": "1130",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129099,
      "firstName": "Li",
      "lastName": "Shang",
      "importedId": "1372",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129100,
      "firstName": "Ning",
      "lastName": "Gu",
      "importedId": "1371",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129101,
      "firstName": "Tun",
      "lastName": "Lu",
      "importedId": "1370",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129102,
      "firstName": "Haipeng",
      "lastName": "Dai",
      "importedId": "1129",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129103,
      "firstName": "Wei",
      "lastName": "Wang",
      "importedId": "1128",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129104,
      "firstName": "Lihao",
      "lastName": "Wang",
      "importedId": "1127",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129105,
      "firstName": "Fan",
      "lastName": "Yang",
      "importedId": "1369",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129106,
      "firstName": "VENKATA",
      "lastName": "PADMANABHAN",
      "middleInitial": "N",
      "importedId": "1126",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129107,
      "firstName": "Robert",
      "lastName": "Dick",
      "middleInitial": "P",
      "importedId": "1368",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129108,
      "firstName": "AJAY",
      "lastName": "MANCHEPALLI",
      "importedId": "1125",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129109,
      "firstName": "Qin",
      "lastName": "Lv",
      "importedId": "1367",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129110,
      "firstName": "ARSHIA",
      "lastName": "ARYA",
      "importedId": "1124",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129111,
      "firstName": "Yuhu",
      "lastName": "Chang",
      "importedId": "1366",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129112,
      "firstName": "MAYANK",
      "lastName": "BARANWAL",
      "importedId": "1123",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129113,
      "firstName": "Mingzhi",
      "lastName": "Dong",
      "importedId": "1365",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129114,
      "firstName": "SHIVANG",
      "lastName": "CHOPRA",
      "importedId": "1122",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129115,
      "firstName": "Rui",
      "lastName": "Zhu",
      "importedId": "1364",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129116,
      "firstName": "Readisca",
      "lastName": "Investigators",
      "importedId": "1143",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129117,
      "firstName": "Qingsong",
      "lastName": "Zou",
      "importedId": "1385",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129118,
      "firstName": "Tetsuo",
      "lastName": "Ashizawa",
      "importedId": "1142",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129119,
      "firstName": "Jingyu",
      "lastName": "Xiao",
      "importedId": "1384",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129120,
      "firstName": "Thomas",
      "lastName": "Klockgether",
      "importedId": "1141",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129121,
      "firstName": "Jason",
      "lastName": "Wiese",
      "importedId": "1383",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129122,
      "firstName": "Alexandra",
      "lastName": "Durr",
      "importedId": "1140",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129123,
      "firstName": "Kazi",
      "lastName": "Kabir",
      "middleInitial": "Sinthia",
      "importedId": "1382",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129124,
      "firstName": "Andrew",
      "lastName": "Campbell",
      "importedId": "1381",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129125,
      "firstName": "Dror",
      "lastName": "Ben-Zeev",
      "importedId": "1380",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129126,
      "firstName": "Gulin",
      "lastName": "Oz",
      "importedId": "1139",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129127,
      "firstName": "Henry",
      "lastName": "Paulson",
      "importedId": "1138",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129128,
      "firstName": "Phillip",
      "lastName": "Yang",
      "importedId": "1137",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129129,
      "firstName": "Trevor",
      "lastName": "Cohen",
      "importedId": "1379",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129130,
      "firstName": "Abdelrahman",
      "lastName": "Abdelkader",
      "importedId": "1136",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129131,
      "firstName": "Serguei",
      "lastName": "Pakhomov",
      "importedId": "1378",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129132,
      "firstName": "Jeet",
      "lastName": "Thaker",
      "importedId": "1135",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129133,
      "firstName": "Benjamin",
      "lastName": "Buck",
      "importedId": "1377",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129134,
      "firstName": "Titilayo",
      "lastName": "Olubajo",
      "importedId": "1134",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129135,
      "firstName": "Subigya",
      "lastName": "Nepal",
      "importedId": "1376",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129136,
      "firstName": "Md",
      "lastName": "Islam",
      "middleInitial": "Saiful",
      "importedId": "1133",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129137,
      "firstName": "Ayesha",
      "lastName": "Chander",
      "importedId": "1375",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129138,
      "firstName": "Gareth",
      "lastName": "Tyson",
      "importedId": "1154",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129139,
      "firstName": "Guangjing",
      "lastName": "Wang",
      "importedId": "1396",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129140,
      "firstName": "GWENDOLYN",
      "lastName": "THOMPSON",
      "importedId": "2001",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129141,
      "firstName": "Yucheng",
      "lastName": "Huang",
      "importedId": "1153",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129142,
      "firstName": "Juexing",
      "lastName": "Wang",
      "importedId": "1395",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129143,
      "firstName": "KRISTIE",
      "lastName": "MAK",
      "importedId": "2000",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129144,
      "firstName": "Ruoyu",
      "lastName": "Li",
      "importedId": "1152",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129145,
      "firstName": "Yuvraj",
      "lastName": "Agarwal",
      "importedId": "1394",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129146,
      "firstName": "Qing",
      "lastName": "Li",
      "importedId": "1151",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129147,
      "firstName": "Prasoon",
      "lastName": "Patidar",
      "importedId": "1393",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129148,
      "firstName": "Qingsong",
      "lastName": "Zou",
      "importedId": "1150",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129149,
      "firstName": "Sudershan",
      "lastName": "Boovaraghavan",
      "importedId": "1392",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129150,
      "firstName": "Yong",
      "lastName": "Jiang",
      "importedId": "1391",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129151,
      "firstName": "Ruoyu",
      "lastName": "Li",
      "importedId": "1390",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129152,
      "firstName": "Kaishun",
      "lastName": "Wu",
      "importedId": "1149",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129153,
      "firstName": "Zhencan",
      "lastName": "Qian Zhang",
      "middleInitial": "Peng",
      "importedId": "1148",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129154,
      "firstName": "Lin",
      "lastName": "Chen",
      "importedId": "1147",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129155,
      "firstName": "Zixuan",
      "lastName": "Weng",
      "importedId": "1389",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129156,
      "firstName": "Minghui",
      "lastName": "Qiu",
      "importedId": "1146",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129157,
      "firstName": "Kang",
      "lastName": "Li",
      "importedId": "1388",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129158,
      "firstName": "Yandao",
      "lastName": "Huang",
      "importedId": "1145",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129159,
      "firstName": "Dan",
      "lastName": "Zhao",
      "importedId": "1387",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129160,
      "firstName": "Ehsan",
      "lastName": "Hoque",
      "importedId": "1144",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129161,
      "firstName": "Qing",
      "lastName": "Li",
      "importedId": "1386",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129162,
      "firstName": "Chris",
      "lastName": "Harrison",
      "importedId": "1165",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129163,
      "firstName": "SIYUAN",
      "lastName": "YUAN",
      "importedId": "2012",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129164,
      "firstName": "Yang",
      "lastName": "Zhang",
      "importedId": "1164",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129165,
      "firstName": "JINGXIAO",
      "lastName": "LIU",
      "importedId": "2011",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129166,
      "firstName": "Mike",
      "lastName": "Czapik",
      "importedId": "1163",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129167,
      "firstName": "JANA",
      "lastName": "KOSECKA",
      "importedId": "2010",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129168,
      "firstName": "Anurag",
      "lastName": "Maravi",
      "importedId": "1162",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129169,
      "firstName": "Chen",
      "lastName": "Chen",
      "importedId": "1161",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129170,
      "firstName": "Sudershan",
      "lastName": "Boovaraghavan",
      "importedId": "1160",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129171,
      "firstName": "HUZEFA",
      "lastName": "RANGWALA",
      "importedId": "2009",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129172,
      "firstName": "PARTH",
      "lastName": "PATHAK",
      "importedId": "2008",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129173,
      "firstName": "PANNEER",
      "lastName": "SANTHALINGAM",
      "middleInitial": "SELVAM",
      "importedId": "2007",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129174,
      "firstName": "Nikola",
      "lastName": "Banovic",
      "importedId": "1159",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129175,
      "firstName": "SYED",
      "lastName": "BILLAH",
      "middleInitial": "MASUM",
      "importedId": "2006",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129176,
      "firstName": "Anna",
      "lastName": "Kratz",
      "importedId": "1158",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129177,
      "firstName": "MD",
      "lastName": "ISLAM",
      "middleInitial": "TOUHIDUL",
      "importedId": "2005",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129178,
      "firstName": "Anindya",
      "lastName": "Antar",
      "middleInitial": "Das",
      "importedId": "1157",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129179,
      "firstName": "Huacheng",
      "lastName": "Zeng",
      "importedId": "1399",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129180,
      "firstName": "MAYANK",
      "lastName": "GOEL",
      "importedId": "2004",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129181,
      "firstName": "Yong",
      "lastName": "Jiang",
      "importedId": "1156",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129182,
      "firstName": "Li",
      "lastName": "Liu",
      "importedId": "1398",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129183,
      "firstName": "OLIVER",
      "lastName": "LINDHIEM",
      "importedId": "2003",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129184,
      "firstName": "Jingyu",
      "lastName": "Xiao",
      "importedId": "1155",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129185,
      "firstName": "Xiao",
      "lastName": "Zhang",
      "importedId": "1397",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129186,
      "firstName": "SAM",
      "lastName": "SHAABAN",
      "importedId": "2002",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129187,
      "firstName": "Sizhen",
      "lastName": "Bian",
      "importedId": "1176",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129188,
      "firstName": "Han",
      "lastName": "Wang",
      "importedId": "2023",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129189,
      "firstName": "Sungho",
      "lastName": "Suh",
      "importedId": "1175",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129190,
      "firstName": "Minh",
      "lastName": "Vu",
      "middleInitial": "Duc",
      "importedId": "2022",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129191,
      "firstName": "Vitor",
      "lastName": "Rey",
      "middleInitial": "Fortes",
      "importedId": "1174",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129192,
      "firstName": "Chenshu",
      "lastName": "Wu",
      "importedId": "2021",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129193,
      "firstName": "David",
      "lastName": "Gamarra",
      "importedId": "1173",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129194,
      "firstName": "Huadong",
      "lastName": "Ma",
      "importedId": "2020",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129195,
      "firstName": "Lala",
      "lastName": "Swarup Ray",
      "middleInitial": "Shakti",
      "importedId": "1172",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129196,
      "firstName": "Esther",
      "lastName": "Zahn",
      "middleInitial": "Friederike",
      "importedId": "1171",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129197,
      "firstName": "Clara",
      "lastName": "Gleiss",
      "middleInitial": "Elisabeth",
      "importedId": "1170",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129198,
      "firstName": "Hao",
      "lastName": "Zhou",
      "importedId": "2019",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129199,
      "firstName": "Zhan",
      "lastName": "Zhang",
      "importedId": "2018",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129200,
      "firstName": "Anfu",
      "lastName": "Zhou",
      "importedId": "2017",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129201,
      "firstName": "Marc",
      "lastName": "Faulhaber",
      "importedId": "1169",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129202,
      "firstName": "Kun",
      "lastName": "Liang",
      "importedId": "2016",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129203,
      "firstName": "Daniel",
      "lastName": "Geissler",
      "importedId": "1168",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129204,
      "firstName": "HAE",
      "lastName": "NOH",
      "middleInitial": "YOUNG",
      "importedId": "2015",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129205,
      "firstName": "Bo",
      "lastName": "Zhou",
      "importedId": "1167",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129206,
      "firstName": "BIONDO",
      "lastName": "BIONDI",
      "importedId": "2014",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129207,
      "firstName": "Yuvraj",
      "lastName": "Agarwal",
      "importedId": "1166",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129208,
      "firstName": "YIWEN",
      "lastName": "DONG",
      "importedId": "2013",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129209,
      "firstName": "Pejman",
      "lastName": "Saeghe",
      "importedId": "1187",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129210,
      "firstName": "Luis",
      "lastName": "González Villalobos",
      "middleInitial": "Alonso",
      "importedId": "2034",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129211,
      "firstName": "Joseph",
      "lastName": "O’Hagan",
      "importedId": "1186",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129212,
      "firstName": "Yi",
      "lastName": "Wu",
      "importedId": "2033",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129213,
      "firstName": "Huadong",
      "lastName": "Ma",
      "importedId": "1185",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129214,
      "firstName": "ENRICO",
      "lastName": "RUKZIO",
      "importedId": "2032",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129215,
      "firstName": "Anfu",
      "lastName": "Zhou",
      "importedId": "1184",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129216,
      "firstName": "MICHAEL",
      "lastName": "RIETZLER",
      "importedId": "2031",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129217,
      "firstName": "Shuyue",
      "lastName": "Wang",
      "importedId": "1183",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129218,
      "firstName": "ALEXANDER",
      "lastName": "FASSBENDER",
      "importedId": "2030",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129219,
      "firstName": "Zihan",
      "lastName": "Zhang",
      "importedId": "1182",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129220,
      "firstName": "Qiaoyue",
      "lastName": "Han",
      "importedId": "1181",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129221,
      "firstName": "Dong",
      "lastName": "Zhao",
      "importedId": "1180",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129222,
      "firstName": "LUCA-MAXIM",
      "lastName": "MEINHARDT",
      "importedId": "2029",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129223,
      "firstName": "MARK",
      "lastName": "COLLEY",
      "importedId": "2028",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129224,
      "firstName": "Chunyang",
      "lastName": "Chen",
      "importedId": "2027",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129225,
      "firstName": "Kaikai",
      "lastName": "Deng",
      "importedId": "1179",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129226,
      "firstName": "Zhenchang",
      "lastName": "Xing",
      "importedId": "2026",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129227,
      "firstName": "Paul",
      "lastName": "Lukowicz",
      "importedId": "1178",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129228,
      "firstName": "Gholamreza",
      "lastName": "Haffari",
      "importedId": "2025",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129229,
      "firstName": "Gesche",
      "lastName": "Joost",
      "importedId": "1177",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129230,
      "firstName": "Zhuang",
      "lastName": "Li",
      "importedId": "2024",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129231,
      "firstName": "Karola",
      "lastName": "Marky",
      "importedId": "1190",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129232,
      "firstName": "SHUAI",
      "lastName": "SHAO",
      "importedId": "1198",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129233,
      "firstName": "Meng",
      "lastName": "Xue",
      "importedId": "2045",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129234,
      "firstName": "Charith",
      "lastName": "Perera",
      "importedId": "1197",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129235,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "importedId": "2044",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129236,
      "firstName": "Omar",
      "lastName": "Rana",
      "importedId": "1196",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129237,
      "firstName": "Takao",
      "lastName": "Someya",
      "importedId": "2043",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129238,
      "firstName": "Dulani",
      "lastName": "Meedeniya",
      "importedId": "1195",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129239,
      "firstName": "Tomoyuki",
      "lastName": "Yokota",
      "importedId": "2042",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129240,
      "firstName": "Jose",
      "lastName": "Llanos",
      "middleInitial": "Tomas",
      "importedId": "1194",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129241,
      "firstName": "Takuya",
      "lastName": "Sasatani",
      "importedId": "2041",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129242,
      "firstName": "Stephanie",
      "lastName": "Beaumont",
      "importedId": "1193",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129243,
      "firstName": "Wakako",
      "lastName": "Yukita",
      "importedId": "2040",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129244,
      "firstName": "Nada",
      "lastName": "Alhirabi",
      "importedId": "1192",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129245,
      "firstName": "Mohamed",
      "lastName": "Mark Mcgill",
      "middleInitial": "Khamis",
      "importedId": "1191",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129246,
      "firstName": "Ryo",
      "lastName": "Takahashi",
      "importedId": "2039",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129247,
      "firstName": "Jian",
      "lastName": "Liu",
      "importedId": "2038",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129248,
      "firstName": "Çağdaş",
      "lastName": "Karataş",
      "importedId": "2037",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129249,
      "firstName": "Daniel",
      "lastName": "Medeiros",
      "importedId": "1189",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129250,
      "firstName": "Gregory",
      "lastName": "Croisdale",
      "middleInitial": "Thomas",
      "importedId": "2036",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129251,
      "firstName": "Jan",
      "lastName": "Gugenheimer",
      "importedId": "1188",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129252,
      "firstName": "Zhenning",
      "lastName": "Yang",
      "importedId": "2035",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129253,
      "firstName": "Di",
      "lastName": "Wu",
      "importedId": "2056",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129254,
      "firstName": "Lei",
      "lastName": "Wang",
      "importedId": "2055",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129255,
      "firstName": "Paul",
      "lastName": "Lukowicz",
      "importedId": "2054",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129256,
      "firstName": "Carlos",
      "lastName": "Velez Altamirano",
      "middleInitial": "Andres",
      "importedId": "2053",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129257,
      "firstName": "Vitor",
      "lastName": "Rey",
      "middleInitial": "Fortes",
      "importedId": "2052",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129258,
      "firstName": "Sungho",
      "lastName": "Suh",
      "importedId": "2051",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129259,
      "firstName": "Bo",
      "lastName": "Zhou",
      "importedId": "2050",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129260,
      "firstName": "Chunkai",
      "lastName": "Fan",
      "importedId": "2049",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129261,
      "firstName": "Jian",
      "lastName": "Zhang",
      "importedId": "2048",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129262,
      "firstName": "Xueluan",
      "lastName": "Gong",
      "importedId": "2047",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129263,
      "firstName": "YU",
      "lastName": "GUAN",
      "importedId": "1199",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129264,
      "firstName": "Yanjiao",
      "lastName": "Chen",
      "importedId": "2046",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129265,
      "firstName": "Yojan",
      "lastName": "Patel",
      "importedId": "2062",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129266,
      "firstName": "Shyam",
      "lastName": "Tailor",
      "importedId": "2061",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129267,
      "firstName": "Robert",
      "lastName": "Harle",
      "importedId": "2060",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129268,
      "firstName": "Jatinder",
      "lastName": "Singh",
      "importedId": "2059",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129269,
      "firstName": "Chris",
      "lastName": "Norval",
      "importedId": "2058",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129270,
      "firstName": "Richard",
      "lastName": "Cloete",
      "importedId": "2057",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129271,
      "firstName": "Yayao",
      "lastName": "Hong",
      "importedId": "1809",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129272,
      "firstName": "Zhihui",
      "lastName": "Ren",
      "importedId": "1800",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129273,
      "firstName": "Zhuohan",
      "lastName": "Ye",
      "importedId": "1808",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129274,
      "firstName": "Tieqi",
      "lastName": "Shou",
      "importedId": "1807",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129275,
      "firstName": "Zhiwen",
      "lastName": "Yu",
      "importedId": "1806",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129276,
      "firstName": "Lu",
      "lastName": "Cheng",
      "importedId": "1805",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129277,
      "firstName": "Bin",
      "lastName": "Guo",
      "importedId": "1804",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129278,
      "firstName": "Qianru",
      "lastName": "Wang",
      "importedId": "1803",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129279,
      "firstName": "Bin",
      "lastName": "Guo",
      "importedId": "1802",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129280,
      "firstName": "Zhiwen",
      "lastName": "Yu",
      "importedId": "1801",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129281,
      "firstName": "Hang",
      "lastName": "Zhu",
      "importedId": "1811",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129282,
      "firstName": "Zhiyuan",
      "lastName": "Wang",
      "importedId": "1810",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129283,
      "firstName": "Xueluan",
      "lastName": "Gong",
      "importedId": "1819",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129284,
      "firstName": "Kuang",
      "lastName": "Peng",
      "importedId": "1818",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129285,
      "firstName": "Meng",
      "lastName": "Xue",
      "importedId": "1817",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129286,
      "firstName": "Longbiao",
      "lastName": "Chen",
      "importedId": "1816",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129287,
      "firstName": "Cheng",
      "lastName": "Wang",
      "importedId": "1815",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129288,
      "firstName": "Binbin",
      "lastName": "Zhou",
      "importedId": "1814",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129289,
      "firstName": "Dingqi",
      "lastName": "Yang",
      "importedId": "1813",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129290,
      "firstName": "Zhihan",
      "lastName": "Jiang",
      "importedId": "1812",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129291,
      "firstName": "Routing",
      "lastName": "Li",
      "importedId": "1822",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129292,
      "firstName": "Yanjiao",
      "lastName": "Chen",
      "importedId": "1821",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129293,
      "firstName": "Qian",
      "lastName": "Zhang",
      "importedId": "1820",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129294,
      "firstName": "Liwei",
      "lastName": "Chan",
      "importedId": "1829",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129295,
      "firstName": "Jia-Jun",
      "lastName": "Wang",
      "importedId": "1828",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129296,
      "firstName": "Tzu-Wei",
      "lastName": "Mi",
      "importedId": "1827",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129297,
      "firstName": "Zhongmin",
      "lastName": "Cai",
      "importedId": "1826",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129298,
      "firstName": "Yunpeng",
      "lastName": "Song",
      "importedId": "1825",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129299,
      "firstName": "Liang",
      "lastName": "Shi",
      "importedId": "1824",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129300,
      "firstName": "Rongrong",
      "lastName": "Zhu",
      "importedId": "1823",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129301,
      "firstName": "Kimberly",
      "lastName": "García",
      "importedId": "1833",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129302,
      "firstName": "Federico",
      "lastName": "Carbone",
      "importedId": "1832",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129303,
      "firstName": "Khakim",
      "lastName": "Akhunov",
      "importedId": "1831",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129304,
      "firstName": "Jannis",
      "lastName": "Strecker",
      "importedId": "1830",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129305,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "importedId": "1839",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129306,
      "firstName": "Ziqi",
      "lastName": "Gao",
      "importedId": "1838",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129307,
      "firstName": "Kasim",
      "lastName": "Yildirim",
      "middleInitial": "Sinan",
      "importedId": "1837",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129308,
      "firstName": "Simon",
      "lastName": "Mayer",
      "importedId": "1836",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129309,
      "firstName": "Andres",
      "lastName": "Gomez",
      "importedId": "1835",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129310,
      "firstName": "Kenan",
      "lastName": "Bektas",
      "importedId": "1834",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129311,
      "firstName": "Amalia",
      "lastName": "Gamalia De Götzen",
      "middleInitial": "De",
      "importedId": "1602",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129312,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "importedId": "1844",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129313,
      "firstName": "Peter",
      "lastName": "Kun",
      "importedId": "1601",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129314,
      "firstName": "Xin",
      "lastName": "Liu",
      "importedId": "1843",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129315,
      "firstName": "William",
      "lastName": "Droz",
      "importedId": "1600",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129316,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "importedId": "1842",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129317,
      "firstName": "Junliang",
      "lastName": "Xing",
      "importedId": "1841",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129318,
      "firstName": "Jianguo",
      "lastName": "Chen",
      "importedId": "1840",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129319,
      "firstName": "George",
      "lastName": "Gaskell",
      "importedId": "1609",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129320,
      "firstName": "Miriam",
      "lastName": "Bidoglia",
      "importedId": "1608",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129321,
      "firstName": "Hao",
      "lastName": "Xu",
      "importedId": "1607",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129322,
      "firstName": "Dennis",
      "lastName": "Stanke",
      "importedId": "1849",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129323,
      "firstName": "Donglei",
      "lastName": "Song",
      "importedId": "1606",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129324,
      "firstName": "Edith",
      "lastName": "Ngai",
      "middleInitial": "C.H",
      "importedId": "1848",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129325,
      "firstName": "Salvador",
      "lastName": "Correa",
      "middleInitial": "Ruiz",
      "importedId": "1605",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129326,
      "firstName": "Hang",
      "lastName": "Zhao",
      "importedId": "1847",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129327,
      "firstName": "Shyam",
      "lastName": "Diwakar",
      "importedId": "1604",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129328,
      "firstName": "Jiangtao",
      "lastName": "Yu",
      "importedId": "1846",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129329,
      "firstName": "Chaitanya",
      "lastName": "Nutakki",
      "importedId": "1603",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129330,
      "firstName": "Running",
      "lastName": "Zhao",
      "importedId": "1845",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129331,
      "firstName": "Carlo",
      "lastName": "Caprini",
      "importedId": "1613",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129332,
      "firstName": "YAGE",
      "lastName": "XIAO",
      "importedId": "1855",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129333,
      "firstName": "Tsolmon",
      "lastName": "Zundui",
      "importedId": "1612",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129334,
      "firstName": "PENGCHENG",
      "lastName": "AN",
      "importedId": "1854",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129335,
      "firstName": "Amarsanaa",
      "lastName": "Ganbold",
      "importedId": "1611",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129336,
      "firstName": "YUE",
      "lastName": "LYU",
      "importedId": "1853",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129337,
      "firstName": "Altangerel",
      "lastName": "Chagnaa",
      "importedId": "1610",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129338,
      "firstName": "Michael",
      "lastName": "Rohs",
      "importedId": "1852",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129339,
      "firstName": "Kerem",
      "lastName": "Demir",
      "middleInitial": "Can",
      "importedId": "1851",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129340,
      "firstName": "Tim",
      "lastName": "Duente",
      "importedId": "1850",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129341,
      "firstName": "Marcelo",
      "lastName": "Britez",
      "middleInitial": "Rodas",
      "importedId": "1619",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129342,
      "firstName": "Ivano",
      "lastName": "Bison",
      "importedId": "1618",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129343,
      "firstName": "Luca",
      "lastName": "Cernuzzi",
      "importedId": "1617",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129344,
      "firstName": "JIAN",
      "lastName": "ZHAO",
      "importedId": "1859",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129345,
      "firstName": "Jose",
      "lastName": "Zarza",
      "middleInitial": "Luis",
      "importedId": "1616",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129346,
      "firstName": "KEIKO",
      "lastName": "KATSURAGAWA",
      "importedId": "1858",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129347,
      "firstName": "Alethia",
      "lastName": "Hume",
      "importedId": "1615",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129348,
      "firstName": "HUAN",
      "lastName": "ZHANG",
      "importedId": "1857",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129349,
      "firstName": "Daniele",
      "lastName": "Miorandi",
      "importedId": "1614",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129350,
      "firstName": "ZIBO",
      "lastName": "ZHANG",
      "importedId": "1856",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129351,
      "firstName": "Laura",
      "lastName": "Schelenz",
      "importedId": "1624",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129352,
      "firstName": "Daqing",
      "lastName": "Zhang",
      "importedId": "1866",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129353,
      "firstName": "And",
      "lastName": "Giunchiglia",
      "middleInitial": "Fausto",
      "importedId": "1623",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129354,
      "firstName": "Xuanzhi",
      "lastName": "Wang",
      "importedId": "1865",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129355,
      "firstName": "Can",
      "lastName": "Günel",
      "importedId": "1622",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129356,
      "firstName": "Yang",
      "lastName": "Li",
      "importedId": "1864",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129357,
      "firstName": "Ronald",
      "lastName": "Chenu-Abente",
      "importedId": "1621",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129358,
      "firstName": "Yaxiong",
      "lastName": "Xie",
      "importedId": "1863",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129359,
      "firstName": "Matteo",
      "lastName": "Busso",
      "importedId": "1620",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129360,
      "firstName": "Shengjie",
      "lastName": "Li",
      "importedId": "1862",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129361,
      "firstName": "Xusheng",
      "lastName": "Zhang",
      "importedId": "1861",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129362,
      "firstName": "Duo",
      "lastName": "Zhang",
      "importedId": "1860",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129363,
      "firstName": "Kuangqi",
      "lastName": "Zhu",
      "importedId": "1629",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129364,
      "firstName": "Mengyan",
      "lastName": "Guo",
      "importedId": "1628",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129365,
      "firstName": "Yue",
      "lastName": "Yang",
      "importedId": "1627",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129366,
      "firstName": "ENRICO",
      "lastName": "RUKZIO",
      "importedId": "1869",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129367,
      "firstName": "Guanyun",
      "lastName": "Wang",
      "importedId": "1626",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129368,
      "firstName": "JULIAN",
      "lastName": "BRITTEN",
      "importedId": "1868",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129369,
      "firstName": "Daniel",
      "lastName": "Gatica-Perez",
      "importedId": "1625",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129370,
      "firstName": "MARK",
      "lastName": "COLLEY",
      "importedId": "1867",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129371,
      "firstName": "LI-SHIUAN",
      "lastName": "PEH",
      "importedId": "1880",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129372,
      "firstName": "Danli",
      "lastName": "Luo",
      "importedId": "1635",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129373,
      "firstName": "AYANGA",
      "lastName": "KUMARI KALUPAHANA",
      "middleInitial": "IMESHA",
      "importedId": "1877",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129374,
      "firstName": "Jiaji",
      "lastName": "Li",
      "importedId": "1634",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129375,
      "firstName": "JIE",
      "lastName": "ZHOU",
      "importedId": "1876",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129376,
      "firstName": "Junzhe",
      "lastName": "Ji",
      "importedId": "1633",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129377,
      "firstName": "JIANJIANG",
      "lastName": "FENG",
      "importedId": "1875",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129378,
      "firstName": "Zihong",
      "lastName": "Zhou",
      "importedId": "1632",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129379,
      "firstName": "JINYANG",
      "lastName": "YU",
      "importedId": "1874",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129380,
      "firstName": "Qiang",
      "lastName": "Cui",
      "importedId": "1631",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129381,
      "firstName": "ALANSON",
      "lastName": "SAMPLE",
      "middleInitial": "P",
      "importedId": "1873",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129382,
      "firstName": "Zihan",
      "lastName": "Yan",
      "importedId": "1630",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129383,
      "firstName": "EUISEOK",
      "lastName": "HWANG",
      "importedId": "1872",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129384,
      "firstName": "CHOUCHANG",
      "lastName": "YANG",
      "middleInitial": "(JACK)",
      "importedId": "1871",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129385,
      "firstName": "YANG-HSI",
      "lastName": "SU",
      "importedId": "1870",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129386,
      "firstName": "Ye",
      "lastName": "Tao",
      "importedId": "1639",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129387,
      "firstName": "Teng",
      "lastName": "Han",
      "importedId": "1638",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129388,
      "firstName": "Yitao",
      "lastName": "Fan",
      "importedId": "1637",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129389,
      "firstName": "XIAOKUI",
      "lastName": "XIAO",
      "importedId": "1879",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129390,
      "firstName": "Deying",
      "lastName": "Pan",
      "importedId": "1636",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129391,
      "firstName": "ANANTA",
      "lastName": "BALAJI",
      "middleInitial": "NARAYANAN",
      "importedId": "1878",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129392,
      "firstName": "FRANCISCO",
      "lastName": "SILVA",
      "importedId": "1891",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129393,
      "firstName": "CLAUDIO",
      "lastName": "RODRIGUES",
      "importedId": "1890",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129394,
      "firstName": "Yanxiang",
      "lastName": "Wang",
      "importedId": "1404",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129395,
      "firstName": "Siddhi",
      "lastName": "Mundhra",
      "importedId": "1646",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129396,
      "firstName": "DAQING",
      "lastName": "ZHANG",
      "importedId": "1888",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129397,
      "firstName": "Jiawei",
      "lastName": "Hu",
      "importedId": "1403",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129398,
      "firstName": "Nagarjun",
      "lastName": "Bhat",
      "importedId": "1645",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129399,
      "firstName": "WENWEI",
      "lastName": "LI",
      "importedId": "1887",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129400,
      "firstName": "Tianxing",
      "lastName": "Li",
      "importedId": "1402",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129401,
      "firstName": "Cedric",
      "lastName": "Girerd",
      "importedId": "1644",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129402,
      "firstName": "JUNZHE",
      "lastName": "WANG",
      "importedId": "1886",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129403,
      "firstName": "Lin",
      "lastName": "Gu",
      "importedId": "1401",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129404,
      "firstName": "Shayaun",
      "lastName": "Bashar",
      "importedId": "1643",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129405,
      "firstName": "ZHIYUN",
      "lastName": "YAO",
      "importedId": "1885",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129406,
      "firstName": "Li",
      "lastName": "Zhichao Cao",
      "middleInitial": "Xiao",
      "importedId": "1400",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129407,
      "firstName": "Daegue",
      "lastName": "Park",
      "importedId": "1642",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129408,
      "firstName": "JIE",
      "lastName": "XIONG",
      "importedId": "1884",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129409,
      "firstName": "Agrim",
      "lastName": "Gupta",
      "importedId": "1641",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129410,
      "firstName": "ANLAN",
      "lastName": "YU",
      "importedId": "1883",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129411,
      "firstName": "Lingyun",
      "lastName": "Sun",
      "importedId": "1640",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129412,
      "firstName": "KAI",
      "lastName": "NIU",
      "importedId": "1882",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129413,
      "firstName": "XUANZHI",
      "lastName": "WANG",
      "importedId": "1881",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129414,
      "firstName": "Ashraf",
      "lastName": "Uddin",
      "importedId": "1409",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129415,
      "firstName": "Brano",
      "lastName": "Kusy",
      "importedId": "1408",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129416,
      "firstName": "Mahbub",
      "lastName": "Hassan",
      "importedId": "1407",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129417,
      "firstName": "Matthew",
      "lastName": "Clark",
      "importedId": "1649",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129418,
      "firstName": "Wen",
      "lastName": "Hu",
      "importedId": "1406",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129419,
      "firstName": "Dinesh",
      "lastName": "Bharadia",
      "importedId": "1648",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129420,
      "firstName": "Hong",
      "lastName": "Jia",
      "importedId": "1405",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129421,
      "firstName": "Tania",
      "lastName": "Morimoto",
      "middleInitial": "K",
      "importedId": "1647",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129422,
      "firstName": "MARKO",
      "lastName": "RADETA",
      "importedId": "1889",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129423,
      "firstName": "Jinjin",
      "lastName": "Zhao",
      "importedId": "1660",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129424,
      "firstName": "Yiran",
      "lastName": "Huang",
      "importedId": "1415",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129425,
      "firstName": "Shinan",
      "lastName": "Liu",
      "importedId": "1657",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129426,
      "firstName": "SHIBO",
      "lastName": "ZHANG",
      "importedId": "1899",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129427,
      "firstName": "Nikola",
      "lastName": "Fischer",
      "importedId": "1414",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129428,
      "firstName": "Jamie",
      "lastName": "Ward",
      "middleInitial": "A",
      "importedId": "1656",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129429,
      "firstName": "YINCHENG",
      "lastName": "JIN",
      "importedId": "1898",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129430,
      "firstName": "Erik",
      "lastName": "Pescara",
      "importedId": "1413",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129431,
      "firstName": "Sally",
      "lastName": "Day",
      "importedId": "1655",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129432,
      "firstName": "PETTERI",
      "lastName": "NURMI",
      "importedId": "1897",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129433,
      "firstName": "Timo",
      "lastName": "Müller",
      "importedId": "1412",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129434,
      "firstName": "Antonia",
      "lastName": "De C Hamilton",
      "middleInitial": "F",
      "importedId": "1654",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129435,
      "firstName": "HUBER",
      "lastName": "FLORES",
      "importedId": "1896",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129436,
      "firstName": "Likun",
      "lastName": "Fang",
      "importedId": "1411",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129437,
      "firstName": "Guido",
      "lastName": "Orgs",
      "importedId": "1653",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129438,
      "firstName": "AGUSTIN",
      "lastName": "ZUNIGA",
      "importedId": "1895",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129439,
      "firstName": "Moustafa",
      "lastName": "Youssef",
      "importedId": "1410",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129440,
      "firstName": "Dwaynica",
      "lastName": "Greaves",
      "middleInitial": "A",
      "importedId": "1652",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129441,
      "firstName": "NGOC",
      "lastName": "NGUYEN",
      "middleInitial": "THI",
      "importedId": "1894",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129442,
      "firstName": "Yanke",
      "lastName": "Sun",
      "importedId": "1651",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129443,
      "firstName": "JOÃO",
      "lastName": "PESTANA",
      "importedId": "1893",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129444,
      "firstName": "Afsaneh",
      "lastName": "Doryab",
      "importedId": "1650",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129445,
      "firstName": "PEDRO",
      "lastName": "ABREU",
      "importedId": "1892",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129446,
      "firstName": "Gurunandan",
      "lastName": "Krishnan",
      "importedId": "1419",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129447,
      "firstName": "Bing",
      "lastName": "Zhou",
      "importedId": "1418",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129448,
      "firstName": "Riku",
      "lastName": "Arakawa",
      "importedId": "1417",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129449,
      "firstName": "Ted",
      "lastName": "Shaowang",
      "importedId": "1659",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129450,
      "firstName": "Michael",
      "lastName": "Beigl",
      "importedId": "1416",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129451,
      "firstName": "Tarun",
      "lastName": "Mangla",
      "importedId": "1658",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129452,
      "firstName": "Bradley",
      "lastName": "Rey",
      "importedId": "1671",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129453,
      "firstName": "Tanzeem",
      "lastName": "Choudhury",
      "importedId": "1670",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129454,
      "firstName": "Alice",
      "lastName": "Zhang",
      "importedId": "1426",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129455,
      "firstName": "Alexander",
      "lastName": "Adams",
      "importedId": "1668",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129456,
      "firstName": "Dawei",
      "lastName": "Liang",
      "importedId": "1425",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129457,
      "firstName": "Rui",
      "lastName": "Maki",
      "importedId": "1667",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129458,
      "firstName": "Yuvraj",
      "lastName": "Agarwal",
      "importedId": "1424",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129459,
      "firstName": "Grace",
      "lastName": "Le",
      "importedId": "1666",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129460,
      "firstName": "Mayank",
      "lastName": "Goel",
      "importedId": "1423",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129461,
      "firstName": "Yujie",
      "lastName": "Tao",
      "importedId": "1665",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129462,
      "firstName": "Prasoon",
      "lastName": "Patidar",
      "importedId": "1422",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129463,
      "firstName": "Yiran",
      "lastName": "Zhao",
      "importedId": "1664",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129464,
      "firstName": "Shree",
      "lastName": "Nayar",
      "middleInitial": "K",
      "importedId": "1421",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129465,
      "firstName": "Nick",
      "lastName": "Feamester",
      "importedId": "1663",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129466,
      "firstName": "Mayank",
      "lastName": "Goel",
      "importedId": "1420",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129467,
      "firstName": "Sanjay",
      "lastName": "Krishnan",
      "importedId": "1662",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129468,
      "firstName": "John",
      "lastName": "Paparrizos",
      "importedId": "1661",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129469,
      "firstName": "Xue",
      "lastName": "Wang",
      "importedId": "1429",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129470,
      "firstName": "Abul",
      "lastName": "Arabi",
      "middleInitial": "Al",
      "importedId": "1428",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129471,
      "firstName": "Edison",
      "lastName": "Thomaz",
      "importedId": "1427",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129472,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "importedId": "1669",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129473,
      "firstName": "Raina",
      "lastName": "Samuel",
      "importedId": "1440",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129474,
      "firstName": "Qian",
      "lastName": "Yang",
      "importedId": "1682",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129475,
      "firstName": "Jiaxing",
      "lastName": "Shen",
      "importedId": "1681",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129476,
      "firstName": "Mi",
      "lastName": "Tian",
      "importedId": "1680",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129477,
      "firstName": "Karthikeyan",
      "lastName": "Sundaresan",
      "importedId": "1437",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129478,
      "firstName": "Yanwen",
      "lastName": "Wang",
      "importedId": "1679",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129479,
      "firstName": "Eugene",
      "lastName": "Chai",
      "importedId": "1436",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129480,
      "firstName": "Zheng",
      "lastName": "Wang",
      "importedId": "1678",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129481,
      "firstName": "John",
      "lastName": "D’Ambrosio",
      "importedId": "1435",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129482,
      "firstName": "Heinrich",
      "lastName": "Hussmann",
      "importedId": "1677",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129483,
      "firstName": "Rajrup",
      "lastName": "Ghosh",
      "importedId": "1434",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129484,
      "firstName": "Sarah",
      "lastName": "Aragon-Hahner",
      "importedId": "1676",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129485,
      "firstName": "Christina",
      "lastName": "Shin",
      "middleInitial": "Suyong",
      "importedId": "1433",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129486,
      "firstName": "Naða",
      "lastName": "Terzimehić",
      "importedId": "1675",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129487,
      "firstName": "Fawad",
      "lastName": "Ahmad",
      "importedId": "1432",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129488,
      "firstName": "Pourang",
      "lastName": "Irani",
      "importedId": "1674",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129489,
      "firstName": "Jeeeun",
      "lastName": "Kim",
      "importedId": "1431",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129490,
      "firstName": "Eun",
      "lastName": "Choe",
      "middleInitial": "Kyoung",
      "importedId": "1673",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129491,
      "firstName": "Yang",
      "lastName": "Zhang",
      "importedId": "1430",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129492,
      "firstName": "Bongshin",
      "lastName": "Lee",
      "importedId": "1672",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129493,
      "firstName": "Sydur",
      "lastName": "Rahaman",
      "importedId": "1439",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129494,
      "firstName": "Ramesh",
      "lastName": "Govindan",
      "importedId": "1438",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129495,
      "firstName": "Ruidong",
      "lastName": "Zhang",
      "importedId": "1451",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129496,
      "firstName": "Robin",
      "lastName": "Welsch",
      "importedId": "1693",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129497,
      "firstName": "Richard",
      "lastName": "Jin",
      "importedId": "1450",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129498,
      "firstName": "Linda",
      "lastName": "Hirsch",
      "importedId": "1692",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129499,
      "firstName": "Svenja",
      "lastName": "Schött",
      "middleInitial": "Yvonne",
      "importedId": "1691",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129500,
      "firstName": "Luke",
      "lastName": "Haliburton",
      "importedId": "1690",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129501,
      "firstName": "Wen",
      "lastName": "Hu",
      "importedId": "1206",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129502,
      "firstName": "Guilin",
      "lastName": "Hu",
      "importedId": "1448",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129503,
      "firstName": "Hong",
      "lastName": "Jia",
      "importedId": "1205",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129504,
      "firstName": "Ke",
      "lastName": "Li",
      "importedId": "1447",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129505,
      "firstName": "Qian",
      "lastName": "Zhang",
      "importedId": "1689",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129506,
      "firstName": "Jiawei",
      "lastName": "Hu",
      "importedId": "1204",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129507,
      "firstName": "Saif",
      "lastName": "Mahmud",
      "importedId": "1446",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129508,
      "firstName": "Xiaofeng",
      "lastName": "Tao",
      "importedId": "1688",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129509,
      "firstName": "Yanxiang",
      "lastName": "Wang",
      "importedId": "1203",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129510,
      "firstName": "Xing-Dong",
      "lastName": "Yang",
      "importedId": "1445",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129511,
      "firstName": "Weichao",
      "lastName": "Li",
      "importedId": "1687",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129512,
      "firstName": "THOMAS",
      "lastName": "PLÖTZ",
      "importedId": "1202",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129513,
      "firstName": "Parmit",
      "lastName": "Chilana",
      "middleInitial": "K",
      "importedId": "1444",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129514,
      "firstName": "Hao",
      "lastName": "Chen",
      "importedId": "1686",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129515,
      "firstName": "PAOLO",
      "lastName": "MISSIER",
      "importedId": "1201",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129516,
      "firstName": "Hongwei",
      "lastName": "Wang",
      "importedId": "1443",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129517,
      "firstName": "Jin",
      "lastName": "Zhang",
      "importedId": "1685",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129518,
      "firstName": "BING",
      "lastName": "ZHAI",
      "importedId": "1200",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129519,
      "firstName": "Josh",
      "lastName": "Davis",
      "middleInitial": "Urban",
      "importedId": "1442",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129520,
      "firstName": "Qianyi",
      "lastName": "Huang",
      "importedId": "1684",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129521,
      "firstName": "Iulian",
      "lastName": "Neamtiu",
      "importedId": "1441",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129522,
      "firstName": "Hengxin",
      "lastName": "Wu",
      "importedId": "1683",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129523,
      "firstName": "Brano",
      "lastName": "Kusy",
      "importedId": "1209",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129524,
      "firstName": "Ashraf",
      "lastName": "Uddin",
      "importedId": "1208",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129525,
      "firstName": "Mahbub",
      "lastName": "Hassan",
      "importedId": "1207",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129526,
      "firstName": "Hao",
      "lastName": "Chen",
      "importedId": "1449",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129556,
      "firstName": "Li",
      "lastName": "Feng",
      "importedId": "2063",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129679,
      "firstName": "JIANKUN",
      "lastName": "WANG",
      "importedId": "2070",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129680,
      "firstName": "Enrico",
      "lastName": "Rukzio",
      "importedId": "2069",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129681,
      "firstName": "Mark",
      "lastName": "Colley",
      "importedId": "2068",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129682,
      "firstName": "Pascal",
      "lastName": "Jansen",
      "importedId": "2067",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129683,
      "firstName": "André-Luc",
      "lastName": "Beylot",
      "importedId": "2066",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129684,
      "firstName": "Gentian",
      "lastName": "Jakllari",
      "importedId": "2065",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129685,
      "firstName": "Kevin",
      "lastName": "Jiokeng",
      "importedId": "2064",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129686,
      "firstName": "BIN",
      "lastName": "WU",
      "importedId": "2074",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129687,
      "firstName": "JIAYANG",
      "lastName": "CUI",
      "importedId": "2073",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129688,
      "firstName": "MENGLING",
      "lastName": "OU",
      "importedId": "2072",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129689,
      "firstName": "ZENGHUA",
      "lastName": "ZHAO",
      "importedId": "2071",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 129696,
      "firstName": "Max",
      "lastName": "Mühlhäuser",
      "importedId": "2078",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Technical University of Darmstadt"
        }
      ]
    },
    {
      "id": 129698,
      "firstName": "Kristof",
      "lastName": "Van Laerhoven",
      "importedId": "2099",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "University of Siegen"
        }
      ]
    },
    {
      "id": 129699,
      "firstName": "Judy",
      "lastName": "Kay",
      "importedId": "2076",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "The University of Sydney"
        }
      ]
    },
    {
      "id": 129708,
      "firstName": "Daniel",
      "lastName": "Gatica-Perez",
      "importedId": "2081",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Idiap-EPFL"
        }
      ]
    },
    {
      "id": 129709,
      "firstName": "Lama",
      "lastName": "Nachman",
      "importedId": "2080",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Intel Labs"
        }
      ]
    },
    {
      "id": 129712,
      "firstName": "Sozo",
      "lastName": "Inoue",
      "importedId": "2088",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "Kyushu Institute of Technology"
        }
      ]
    },
    {
      "id": 129719,
      "firstName": "Stephen",
      "lastName": "Vodia",
      "importedId": "2079",
      "source": "CSV",
      "affiliations": [
        {
          "institution": "University of Colorado Boulder"
        }
      ]
    }
  ],
  "recognitions": []
}