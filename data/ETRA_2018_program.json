{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10005,
    "startDate": 1528934400000,
    "endDate": 1529193600000,
    "shortName": "ETRA",
    "name": "ETRA 2018",
    "year": 2018,
    "fullName": "10th ACM Symposium on Eye Tracking Research & Applications",
    "url": "http://etra.acm.org/2018/",
    "location": "Warsaw, Poland",
    "timeZoneOffset": 60,
    "logoUrl": "https://files.sigchi.org/conference/logo/b6d2ccc2-13d2-0492-c8af-c0e6fef1ac32.png",
    "timeZoneName": "Europe/Warsaw"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10003,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10019,
      "name": "3rd floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/e9f84960-f705-2ef1-bf3d-1df979578c9b.png",
      "roomIds": [
        10088,
        10086,
        10085,
        10087
      ]
    },
    {
      "id": 10020,
      "name": "2nd floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/9367be46-ef7d-c4b8-a721-f6f35cff2197.png",
      "roomIds": [
        10084,
        10081,
        10080,
        10082,
        10083
      ]
    },
    {
      "id": 10021,
      "name": "Ground floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/5ba03253-9dfb-bb2f-18e5-d9e813ca905b.png"
    },
    {
      "id": 10022,
      "name": "Venue overview",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/55b4c876-b207-aa00-7316-4ef72b1449dd.png"
    }
  ],
  "rooms": [
    {
      "id": 10084,
      "name": "214",
      "typeId": 10328,
      "setup": "Special"
    },
    {
      "id": 10081,
      "name": "S202",
      "typeId": 10284,
      "setup": "Special"
    },
    {
      "id": 10080,
      "name": "S203",
      "typeId": 10284,
      "setup": "Special"
    },
    {
      "id": 10082,
      "name": "S226",
      "typeId": 10284,
      "setup": "Special"
    },
    {
      "id": 10083,
      "name": "S227",
      "typeId": 10284,
      "setup": "Special"
    },
    {
      "id": 10088,
      "name": "S302",
      "typeId": 10309,
      "setup": "Special"
    },
    {
      "id": 10086,
      "name": "S303",
      "typeId": 10304,
      "setup": "Special"
    },
    {
      "id": 10085,
      "name": "S304",
      "typeId": 10286,
      "setup": "Special"
    },
    {
      "id": 10087,
      "name": "S305",
      "typeId": 10328,
      "setup": "Special"
    }
  ],
  "tracks": [
    {
      "id": 10017,
      "typeId": 10047
    },
    {
      "id": 10018,
      "typeId": 10048
    },
    {
      "id": 10019,
      "typeId": 10284
    },
    {
      "id": 10020,
      "typeId": 10286
    },
    {
      "id": 10021,
      "typeId": 10307
    },
    {
      "id": 10022,
      "typeId": 10743
    },
    {
      "id": 10023,
      "typeId": 10779
    },
    {
      "id": 10024,
      "typeId": 10801
    },
    {
      "id": 10025,
      "typeId": 10928
    },
    {
      "id": 10026,
      "typeId": 10953
    },
    {
      "id": 10027,
      "typeId": 10970
    }
  ],
  "contentTypes": [
    {
      "id": 10041,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 10042,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 10043,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 10045,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 10046,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 10047,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 10048,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 10049,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 10050,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 10044,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 10289,
      "name": "Break",
      "duration": 0
    },
    {
      "id": 10305,
      "name": "Demos",
      "duration": 0
    },
    {
      "id": 10286,
      "name": "Doctoral Symposium",
      "duration": 0
    },
    {
      "id": 10333,
      "name": "Fairwell",
      "duration": 0
    },
    {
      "id": 10311,
      "name": "Keynote address",
      "duration": 0
    },
    {
      "id": 10293,
      "name": "Opening",
      "duration": 0
    },
    {
      "id": 10328,
      "name": "Papers Session",
      "duration": 0
    },
    {
      "id": 10304,
      "name": "Posters",
      "duration": 0
    },
    {
      "id": 10307,
      "name": "Sponsor Workshop",
      "duration": 0
    },
    {
      "id": 10284,
      "name": "Tutorial",
      "duration": 0,
      "displayName": "Tutorials"
    },
    {
      "id": 10309,
      "name": "Videos",
      "duration": 0
    },
    {
      "id": 10953,
      "name": "Demo",
      "duration": 0,
      "displayName": "Demos"
    },
    {
      "id": 10743,
      "name": "Keynote",
      "duration": 0
    },
    {
      "id": 10801,
      "name": "Long paper",
      "duration": 0
    },
    {
      "id": 10928,
      "name": "Note",
      "duration": 0
    },
    {
      "id": 10779,
      "name": "Short paper",
      "duration": 0
    },
    {
      "id": 10970,
      "name": "Video",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 10139,
      "type": "SESSION",
      "startDate": 1528966800000,
      "endDate": 1528979400000
    },
    {
      "id": 10141,
      "type": "SESSION",
      "startDate": 1528966800000,
      "endDate": 1528995600000
    },
    {
      "id": 10143,
      "type": "BREAK",
      "startDate": 1528970400000,
      "endDate": 1528972200000
    },
    {
      "id": 10144,
      "type": "BREAK",
      "startDate": 1528979400000,
      "endDate": 1528983000000
    },
    {
      "id": 10145,
      "type": "SESSION",
      "startDate": 1528983000000,
      "endDate": 1528995600000
    },
    {
      "id": 10146,
      "type": "BREAK",
      "startDate": 1528990200000,
      "endDate": 1528992000000
    },
    {
      "id": 10147,
      "type": "SESSION",
      "startDate": 1529053200000,
      "endDate": 1529054100000
    },
    {
      "id": 10148,
      "type": "SESSION",
      "startDate": 1529054100000,
      "endDate": 1529056800000
    },
    {
      "id": 10149,
      "type": "BREAK",
      "startDate": 1529056800000,
      "endDate": 1529058600000
    },
    {
      "id": 10151,
      "type": "SESSION",
      "startDate": 1529058600000,
      "endDate": 1529065800000
    },
    {
      "id": 10152,
      "type": "BREAK",
      "startDate": 1529065800000,
      "endDate": 1529069400000
    },
    {
      "id": 10155,
      "type": "SESSION",
      "startDate": 1529069400000,
      "endDate": 1529076600000
    },
    {
      "id": 10156,
      "type": "BREAK",
      "startDate": 1529076600000,
      "endDate": 1529078400000
    },
    {
      "id": 10158,
      "type": "SESSION",
      "startDate": 1529076600000,
      "endDate": 1529085600000
    },
    {
      "id": 10161,
      "type": "SESSION",
      "startDate": 1529078400000,
      "endDate": 1529085600000
    },
    {
      "id": 10162,
      "type": "SESSION",
      "startDate": 1529078700000,
      "endDate": 1529080500000
    },
    {
      "id": 10163,
      "type": "SESSION",
      "startDate": 1529080800000,
      "endDate": 1529082600000
    },
    {
      "id": 10164,
      "type": "SESSION",
      "startDate": 1529082900000,
      "endDate": 1529084700000
    },
    {
      "id": 10165,
      "type": "SESSION",
      "startDate": 1529139600000,
      "endDate": 1529143200000
    },
    {
      "id": 10166,
      "type": "BREAK",
      "startDate": 1529143200000,
      "endDate": 1529145000000
    },
    {
      "id": 10167,
      "type": "SESSION",
      "startDate": 1529145000000,
      "endDate": 1529152200000
    },
    {
      "id": 10170,
      "type": "BREAK",
      "startDate": 1529152200000,
      "endDate": 1529155800000
    },
    {
      "id": 10171,
      "type": "SESSION",
      "startDate": 1529155800000,
      "endDate": 1529159700000
    },
    {
      "id": 10172,
      "type": "SESSION",
      "startDate": 1529155800000,
      "endDate": 1529163000000
    },
    {
      "id": 10174,
      "type": "SESSION",
      "startDate": 1529160300000,
      "endDate": 1529163000000
    },
    {
      "id": 10176,
      "type": "BREAK",
      "startDate": 1529163000000,
      "endDate": 1529164800000
    },
    {
      "id": 10177,
      "type": "SESSION",
      "startDate": 1529164800000,
      "endDate": 1529169300000
    },
    {
      "id": 10178,
      "type": "SESSION",
      "startDate": 1529164800000,
      "endDate": 1529172000000
    },
    {
      "id": 10179,
      "type": "SESSION",
      "startDate": 1529169300000,
      "endDate": 1529171100000
    },
    {
      "id": 10180,
      "type": "SESSION",
      "startDate": 1529226000000,
      "endDate": 1529229600000
    },
    {
      "id": 10181,
      "type": "BREAK",
      "startDate": 1529229600000,
      "endDate": 1529231400000
    },
    {
      "id": 10183,
      "type": "SESSION",
      "startDate": 1529231400000,
      "endDate": 1529238600000
    },
    {
      "id": 10185,
      "type": "BREAK",
      "startDate": 1529238600000,
      "endDate": 1529242200000
    },
    {
      "id": 10187,
      "type": "SESSION",
      "startDate": 1529242200000,
      "endDate": 1529249400000
    },
    {
      "id": 10188,
      "type": "SESSION",
      "startDate": 1529251200000,
      "endDate": 1529258400000
    }
  ],
  "sessions": [
    {
      "id": 1933,
      "name": "Eye movement metrics: event detection",
      "typeId": 10284,
      "roomId": 10080,
      "chairIds": [
        8734,
        16510
      ],
      "contentIds": [
        7063
      ],
      "timeSlotId": 10139
    },
    {
      "id": 2472,
      "name": "Spatial Cognition in the Wild — Methods for Large-Scale Behavioural Research in Visuo-Locomotive Perception",
      "typeId": 10284,
      "roomId": 10081,
      "chairIds": [
        8734,
        16510
      ],
      "contentIds": [
        7537
      ],
      "timeSlotId": 10139
    },
    {
      "id": 1369,
      "name": "ETRA Doctoral Symposium",
      "typeId": 10286,
      "roomId": 10085,
      "chairIds": [
        9510,
        12096,
        15832
      ],
      "contentIds": [
        2855,
        6480,
        3939,
        4227,
        6259,
        6367,
        4116,
        5550,
        5353,
        5088,
        6301,
        3193,
        5912
      ],
      "timeSlotId": 10141
    },
    {
      "id": 1166,
      "name": "Eye-Tracking and Visual Analytics",
      "typeId": 10284,
      "roomId": 10082,
      "chairIds": [
        8734,
        16510
      ],
      "contentIds": [
        6899
      ],
      "timeSlotId": 10141
    },
    {
      "id": 2215,
      "name": "Eyetracking in Virtual and Augmented Reality",
      "typeId": 10284,
      "roomId": 10083,
      "chairIds": [
        8734,
        16510
      ],
      "contentIds": [
        7332
      ],
      "timeSlotId": 10141
    },
    {
      "id": 1186,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10143
    },
    {
      "id": 2288,
      "name": "Lunch break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10144
    },
    {
      "id": 1975,
      "name": "Gaze Analytics Pipeline",
      "typeId": 10284,
      "roomId": 10081,
      "chairIds": [
        8734,
        16510
      ],
      "contentIds": [
        6428
      ],
      "timeSlotId": 10145
    },
    {
      "id": 2226,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10146
    },
    {
      "id": 1897,
      "name": "Opening Session",
      "typeId": 10293,
      "roomId": 10084,
      "chairIds": [
        21825,
        17228
      ],
      "contentIds": [],
      "timeSlotId": 10147
    },
    {
      "id": 1756,
      "name": "Keynote address 1: Susana Martinez-Conde",
      "typeId": 10311,
      "roomId": 10084,
      "chairIds": [
        21825,
        17228
      ],
      "contentIds": [
        6827
      ],
      "timeSlotId": 10148
    },
    {
      "id": 1506,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10149
    },
    {
      "id": 2049,
      "name": "COGAIN Session 1",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        13443
      ],
      "contentIds": [
        5667,
        7052,
        2866,
        7010,
        7400,
        3492
      ],
      "timeSlotId": 10151
    },
    {
      "id": 1082,
      "name": "ETRA Session 1: Cognition",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        19122
      ],
      "contentIds": [
        4001,
        6911,
        5457,
        6997,
        6581,
        6535
      ],
      "timeSlotId": 10151
    },
    {
      "id": 2513,
      "name": "Lunch break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10152
    },
    {
      "id": 1783,
      "name": "COGAIN Session 2",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        21398
      ],
      "contentIds": [
        4530,
        5934,
        2881,
        4152
      ],
      "timeSlotId": 10155
    },
    {
      "id": 1953,
      "name": "ETRA Session 2: Fundamental Eye Tracking",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        15465
      ],
      "contentIds": [
        4896,
        3738,
        7989,
        3631,
        5385,
        7544
      ],
      "timeSlotId": 10155
    },
    {
      "id": 2086,
      "name": "Eye tracking philosophy from Tobii Pro",
      "typeId": 10307,
      "roomId": 10083,
      "chairIds": [
        14304
      ],
      "contentIds": [
        7624
      ],
      "timeSlotId": 10155
    },
    {
      "id": 1115,
      "name": "Coffee break & Reception",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10156
    },
    {
      "id": 1005,
      "name": "ETRA Video Session 1",
      "typeId": 10309,
      "roomId": 10088,
      "chairIds": [
        18463,
        19704
      ],
      "contentIds": [
        6606,
        8129,
        5972,
        7234
      ],
      "timeSlotId": 10156
    },
    {
      "id": 1301,
      "name": "ETRA Poster Session",
      "typeId": 10304,
      "roomId": 10086,
      "chairIds": [
        14339
      ],
      "contentIds": [
        5457,
        6581,
        3738,
        3631,
        6346,
        3936,
        2743,
        4890,
        4252,
        5827,
        5317,
        7077,
        5666,
        5143,
        6165,
        2868,
        4607,
        3981,
        2972,
        5508,
        7379,
        4071,
        4127,
        4344,
        7533,
        4716,
        7602,
        2784,
        7439,
        4391,
        3865,
        6650,
        8042,
        5220,
        7409,
        7181,
        6782,
        4997,
        3984
      ],
      "timeSlotId": 10158
    },
    {
      "id": 2175,
      "name": "ETRA Demo Session",
      "typeId": 10305,
      "roomId": 10085,
      "chairIds": [
        18463,
        19704
      ],
      "contentIds": [
        6118,
        3071,
        3263,
        6764,
        7489,
        6605,
        7375,
        6834,
        7948
      ],
      "timeSlotId": 10161
    },
    {
      "id": 2064,
      "name": "PETMEI Session 1",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        13130
      ],
      "contentIds": [
        5544,
        7161,
        6361
      ],
      "timeSlotId": 10161
    },
    {
      "id": 1278,
      "name": "Remote Eye Tracking - Challenges and Future",
      "typeId": 10307,
      "roomId": 10083,
      "chairIds": [
        22885,
        15817,
        10422
      ],
      "contentIds": [
        7507
      ],
      "timeSlotId": 10161
    },
    {
      "id": 1551,
      "name": "ETRA Video Session 2",
      "typeId": 10309,
      "roomId": 10088,
      "chairIds": [
        18463,
        19704
      ],
      "contentIds": [
        6456,
        4154,
        7454,
        4163
      ],
      "timeSlotId": 10162
    },
    {
      "id": 1940,
      "name": "ETRA Video Session 3",
      "typeId": 10309,
      "roomId": 10088,
      "chairIds": [
        18463,
        19704
      ],
      "contentIds": [
        5329,
        3248,
        6359,
        2750
      ],
      "timeSlotId": 10163
    },
    {
      "id": 1568,
      "name": "ETRA Video Session 4",
      "typeId": 10309,
      "roomId": 10088,
      "chairIds": [
        18463,
        19704
      ],
      "contentIds": [
        7727,
        2828,
        5904,
        3127
      ],
      "timeSlotId": 10164
    },
    {
      "id": 1907,
      "name": "Keynote address 2: Andreas Bulling",
      "typeId": 10311,
      "roomId": 10084,
      "chairIds": [
        21825,
        17228
      ],
      "contentIds": [
        6929
      ],
      "timeSlotId": 10165
    },
    {
      "id": 1670,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10166
    },
    {
      "id": 2188,
      "name": "ETRA Session 3: Digital Interactions",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        10369
      ],
      "contentIds": [
        8107,
        2975,
        8174,
        6707,
        6346,
        2777
      ],
      "timeSlotId": 10167
    },
    {
      "id": 1692,
      "name": "Hands-on with Pupil Labs - Current features and future directions",
      "typeId": 10307,
      "roomId": 10083,
      "chairIds": [
        16669,
        17518,
        14822,
        20172,
        13181,
        14640,
        21977
      ],
      "contentIds": [
        4693
      ],
      "timeSlotId": 10167
    },
    {
      "id": 1958,
      "name": "PETMEI Session 2",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        9199
      ],
      "contentIds": [
        8053,
        7960,
        4165
      ],
      "timeSlotId": 10167
    },
    {
      "id": 1960,
      "name": "Lunch break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10170
    },
    {
      "id": 1098,
      "name": "ETVIS Session 1: Visualization",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        21610
      ],
      "contentIds": [
        5131,
        3306,
        7371
      ],
      "timeSlotId": 10171
    },
    {
      "id": 1219,
      "name": "ETRA Session 4: Mobile Eye Tracking",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        9953
      ],
      "contentIds": [
        3134,
        7845,
        5352,
        3936,
        5398,
        4031
      ],
      "timeSlotId": 10172
    },
    {
      "id": 2458,
      "name": "Eye Tracking at Oculus Research : Revisiting the Fundamentals",
      "typeId": 10307,
      "roomId": 10083,
      "chairIds": [
        12786
      ],
      "contentIds": [
        5392
      ],
      "timeSlotId": 10172
    },
    {
      "id": 1345,
      "name": "ETVIS Session 2: Evaluation",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        11854
      ],
      "contentIds": [
        4488,
        6887,
        7212
      ],
      "timeSlotId": 10174
    },
    {
      "id": 1993,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10176
    },
    {
      "id": 1738,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10176
    },
    {
      "id": 1496,
      "name": "ETVIS Session 3: Applications",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        19432
      ],
      "contentIds": [
        3058,
        6104,
        5129,
        4771
      ],
      "timeSlotId": 10177
    },
    {
      "id": 2498,
      "name": "ETRA Session 5: Gaze-based Interaction",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        13804
      ],
      "contentIds": [
        7318,
        6117,
        4890,
        6063,
        6374,
        2743
      ],
      "timeSlotId": 10178
    },
    {
      "id": 2042,
      "name": "ETVIS: Panel Discussion",
      "typeId": 10047,
      "roomId": 10087,
      "chairIds": [],
      "contentIds": [
        6637
      ],
      "timeSlotId": 10179
    },
    {
      "id": 2449,
      "name": "Keynote address 3: Halszka Jarodzka",
      "typeId": 10311,
      "roomId": 10084,
      "chairIds": [
        21825,
        17228
      ],
      "contentIds": [
        7682
      ],
      "timeSlotId": 10180
    },
    {
      "id": 1009,
      "name": "Coffee break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10181
    },
    {
      "id": 2097,
      "name": "EMIP Session 1",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        21687,
        11248
      ],
      "contentIds": [
        6942,
        7072,
        4089
      ],
      "timeSlotId": 10183
    },
    {
      "id": 1178,
      "name": "ETRA Session 6: Social and Natural Behavior",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        19325
      ],
      "contentIds": [
        5285,
        5519,
        3778,
        4252,
        5827,
        3703
      ],
      "timeSlotId": 10183
    },
    {
      "id": 1107,
      "name": "iMotion Workshop",
      "typeId": 10307,
      "roomId": 10083,
      "chairIds": [
        9010,
        15251,
        14360
      ],
      "contentIds": [
        7370
      ],
      "timeSlotId": 10183
    },
    {
      "id": 1601,
      "name": "Lunch break",
      "typeId": 10289,
      "roomId": 10086,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10185
    },
    {
      "id": 2468,
      "name": "EMIP Session 2",
      "typeId": 10328,
      "roomId": 10087,
      "chairIds": [
        21687,
        11248
      ],
      "contentIds": [
        6659,
        4058,
        7994
      ],
      "timeSlotId": 10187
    },
    {
      "id": 1331,
      "name": "ETRA Session 7: Clinical and Emotional",
      "typeId": 10328,
      "roomId": 10084,
      "chairIds": [
        12096
      ],
      "contentIds": [
        7077,
        5143,
        6487,
        5666,
        5982,
        5317,
        8191
      ],
      "timeSlotId": 10187
    },
    {
      "id": 1669,
      "name": "Town Hall Meeting",
      "typeId": 10333,
      "roomId": 10084,
      "chairIds": [
        21825,
        17228
      ],
      "contentIds": [],
      "timeSlotId": 10188
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 7682,
      "typeId": 10743,
      "title": "Eye tracking in Education – bridging the gap from fundamental research to educational practice",
      "trackId": 10022,
      "tags": [],
      "keywords": [],
      "abstract": "Over the past decade, eye tracking has shed light onto aspects of learning and instruction that could not have been addressed otherwise before. However, few of these insights have found their way to educational practice. To bridge this gap between this fundamental research and educational practice, we now must take a step from controlled laboratory settings towards real-life scenarios accounting for their full complexity.\n\nIn my keynote, I would like to address this question from three perspectives, namely by using eye tracking to analyze the viewpoint of the teacher as well as that of the student, and by using eye tracking as direct instruction tool. From the teacher’s perspective, I will focus on how teachers develop visual expertise in managing a classroom full of students and how this can be investigated by eye tracking (and other triangulating measures). From the students’ perspective, I will discuss how eye tracking can help us to improve the instructional design of computer-based learning- and testing environments and what influence social presence has on students’ attentional processes and ultimately on their learning. Finally, I will present how we use eye tracking directly for teaching by means of so-called eye movement modeling examples, which is a form of video-based instruction, and what guidelines we can derive thus far for their design.\n\nFor all three aspects, I will draw connections to other areas of eye tracking research to show what we learned from them and described what theoretical development our research aims at.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13045
        }
      ],
      "sessionIds": [
        2449
      ],
      "eventIds": []
    },
    {
      "id": 6659,
      "typeId": 10048,
      "title": "Beyond gaze: preliminary analysis of pupil dilation and blink rates in an fMRI study of program comprehension",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Researchers have been employing psycho-physiological measures to better understand program comprehension, for example simultaneous fMRI and eye tracking to validate top-down comprehension models. In this paper, we argue that there is additional value in eye-tracking data beyond eye gaze: Pupil dilation and blink rates may offer insights into programmers' cognitive load. However, the fMRI environment may influence pupil dilation and blink rates, which would diminish their informative value. We conducted a preliminary analysis of pupil dilation and blink rates of an fMRI experiment with 22 student participants. We conclude from our preliminary analysis that the correction for our fMRI environment is challenging, but possible, such that we can use pupil dilation and blink rates to more reliably observe program comprehension.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8786
        },
        {
          "affiliations": [],
          "personId": 18022
        },
        {
          "affiliations": [],
          "personId": 16204
        },
        {
          "affiliations": [],
          "personId": 23399
        },
        {
          "affiliations": [],
          "personId": 19755
        }
      ],
      "sessionIds": [
        2468
      ],
      "eventIds": []
    },
    {
      "id": 5129,
      "typeId": 10048,
      "title": "GaRSIVis: improving the predicting of self-interruption during reading using gaze data",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze pattern data provides a promising opportunity to create a predictive model of self-interruption during reading that could support active interventions to keep a reader's attention at times when self-interruptions are predicted to occur. We present two systems designed to help analysts create and improve such a model. We present GaRSIVis, (Gaze Reading Self-Interruption Visualizer), that integrates a visualization front-end suitable for data cleansing and a prediction back-end that can be run repeatedly as the input data is iteratively improved. It allows analysts refining the predictive model to filter out unwanted parts of the gaze data that should not be used in the prediction. It relies on data gathered by GaRSILogger, which logs gaze data and activity associated with interruptions during on-screen reading. By integrating data cleansing and our prediction results in our visualization, we enable analysts using GaRSIVis to come up with a comprehensible way of understanding self-interruption from gaze related features.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15843
        },
        {
          "affiliations": [],
          "personId": 14880
        },
        {
          "affiliations": [],
          "personId": 13750
        },
        {
          "affiliations": [],
          "personId": 8530
        },
        {
          "affiliations": [],
          "personId": 13677
        }
      ],
      "sessionIds": [
        1496
      ],
      "eventIds": []
    },
    {
      "id": 5385,
      "typeId": 10048,
      "title": "Comparison of mapping algorithms for implicit calibration using probable fixation targets",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "With growing access to cheap low end eye trackers using simple web cameras, there is also a growing demand on easy and fast usage of this devices by untrained and unsupervised end users. For such users the necessity to calibrate the eye tracker prior to its first usage is often perceived as obtrusive and inconvenient. In the same time perfect accuracy is not necessary for many commercial applications. Therefore, the idea of implicit calibration attracts more and more attention. Algorithms for implicit calibration are able to calibrate the device without any active collaboration with users. Especially, a real time implicit calibration, that is able to calibrate a device on-the-fly, while a person uses an eye tracker, seems to be a reasonable solution to the aforementioned problems.\n\nThe paper presents examples of implicit calibration algorithms (including their real time versions) based on the idea of probable fixation targets (PFT). The algorithms were tested during a free viewing experiment and compared to the state of the art PFT based algorithm and explicit calibration results.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8734
        },
        {
          "affiliations": [],
          "personId": 8923
        }
      ],
      "sessionIds": [
        1953
      ],
      "eventIds": []
    },
    {
      "id": 5131,
      "typeId": 10048,
      "title": "Intuitive visualization technique to support eye tracking data analysis: a user-study",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "While fixation distribution is conventionally visualized using heat maps, there is still a lack of a commonly accepted technique to visualize saccade distributions. Inspired by wind maps and the Oriented Line Integral Convolution (OLIC) technique, we visualize saccades by drawing ink droplets which follow the direction indicated by a flow direction map. This direction map is computed using a kernel density estimation technique over the tangent directions to each saccade gaze point. The image is further blended with the corresponding heat map. It results in an animation or a static image showing main directions of the transitions between different areas of interest. We also present results from a web-based user study where naive non-expert users where asked to identify the direction of the flow and simple patterns. The results showed that these visualizations can successfully be used to support visual analysis of the eye-tracking data. It also showed that the use of animation allows to ease the task and to improve the performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10048
        },
        {
          "affiliations": [],
          "personId": 19480
        }
      ],
      "sessionIds": [
        1098
      ],
      "eventIds": []
    },
    {
      "id": 7948,
      "typeId": 10953,
      "title": "An Inconspicuous and Modular Head-Mounted Eye Tracke",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "State of the art head mounted eye trackers employ glasses like frames, making their usage uncomfortable or even impossible for prescription eyewear users. Nonetheless, these users represent a notable portion of the population (e.g. the Prevent Blindness America organization reports that about half of the USA population use corrective eyewear for refractive errors alone). Thus, making eye tracking accessible for eyewear users is paramount to not only improve usability, but is also key for the ecological validity of eye tracking studies. In this work, we report on a novel approach for eye tracker design in the form of a modular and inconspicuous device that can be easily attached to glasses; for users without glasses, we also provide a 3D printable frame blueprint. Our prototypes include both low cost Commerical Out of The Shelf (COTS) and more expensive Original Equipment manufacturer (OEM) cameras, with sampling rates ranging between 30 and 120 fps and multiple pixel resolutions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22070
        },
        {
          "affiliations": [],
          "personId": 21137
        },
        {
          "affiliations": [],
          "personId": 21691
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 2828,
      "typeId": 10970,
      "title": "Modeling Corneal Reflection for Eye-tracking Considering Eyelid Occlusion",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Capturing Purkinje images is essential for wide-range and accurate eye-tracking. The range of eye rotation over which the Purkinje image is observable has so far been modeled by a cone shape called a gaze cone. In this study, we extended the gaze cone model to include occlusion by an eyelid. First, we developed a measurement device that has eight spider-like arms. Then, we proposed a novel model that considers eyeball movement. Using the device, we measured the range of corneal reflection, and we fitted the proposed model to the results.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21149
        },
        {
          "affiliations": [],
          "personId": 13804
        },
        {
          "affiliations": [],
          "personId": 14600
        },
        {
          "affiliations": [],
          "personId": 15457
        }
      ],
      "sessionIds": [
        1568
      ],
      "eventIds": []
    },
    {
      "id": 7181,
      "typeId": 10928,
      "title": "An Eye Gaze Model for Seismic Interpretation Support",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Designing systems to o er support to experts during cognitive intensive tasks at the right time is still a challenging endeavor, despite years of research progress in the area. This paper proposes a gaze model based on eye tracking empirical data to identify when a system should proactively interact with the expert during visual inspection tasks. The gaze model derives from the analyses of a user study where 11 seismic interpreters were asked to perform the visual inspection task of seismic images from known and unknown basins. The eye tracking  fixation patterns were triangulated with pupil dilations and thinking-aloud data. Results show that cumulative saccadic distances allow identifying when additional information could be offered to support seismic interpreters, changing the visual search behavior from exploratory to goal-directed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23452
        },
        {
          "affiliations": [],
          "personId": 13494
        },
        {
          "affiliations": [],
          "personId": 20532
        },
        {
          "affiliations": [],
          "personId": 11073
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7439,
      "typeId": 10928,
      "title": "Development and Evaluation of a Gaze Feedback System Integrated into EyeTrace",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "A growing field of studies in eye-tracking is the use of gaze data for realtime feedback to the subject.\nIn this work, we present a software system for such experiments and validate it with a visual search task experiment. This system was integrated into an eye tracking analysis tool.\nOur aim was to improve subject performance in this task by employing saliency features for gaze guidance. This realtime feedback system can be applicable within many realms, such as learning interventions, computer entertainment, or virtual reality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20197
        },
        {
          "affiliations": [],
          "personId": 10433
        },
        {
          "affiliations": [],
          "personId": 8373
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 5904,
      "typeId": 10970,
      "title": "Enhanced Representation of Web Pages for Usability Analysis with Eye Tracking",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking as a tool to quantify user attention plays a major role in research and application design. For Web page usability, it has become a prominent measure to assess which sections of a Web page are read, glanced or skipped. Such assessments primarily depend on the mapping of gaze data to a Web page representation. However, current representation methods, a virtual screenshot of the Web page or a video recording of the complete interaction session, suffer either from accuracy or scalability issues. We present a method that identifies fixed elements on Web pages and combines user viewport screenshots in relation to fixed elements for an enhanced representation of the page. We conducted an experiment with 10 participants and the results signify that analysis with our method is more efficient than a video recording, which is an essential criterion for large scale Web studies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10135
        },
        {
          "affiliations": [],
          "personId": 21691
        },
        {
          "affiliations": [],
          "personId": 20190
        },
        {
          "affiliations": [],
          "personId": 8206
        },
        {
          "affiliations": [],
          "personId": 11875
        },
        {
          "affiliations": [],
          "personId": 11841
        }
      ],
      "sessionIds": [
        1568
      ],
      "eventIds": []
    },
    {
      "id": 5392,
      "typeId": 10307,
      "title": "Eye Tracking at Oculus Research : Revisiting the Fundamentals",
      "trackId": 10021,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking technology has been available in some form for decades. However, truly immersive VR and AR experiences require great eye tracking, all the time, for every user with virtually no allowance for weight, power, conspicuity, or compute. Traditional tracking technologies do not meet all of these stringent criteria, and it’s unclear if more engineering effort will get us there. The field requires fundamental research to challenge long-standing assumptions in eye tracking best practices. In this talk, we’ll provide an overview of the approach at Oculus Research, and dive deep on many of the challenges that face the community. We look forward to collaborating with researchers and industry partners to unlock the true potential of VR/AR though great eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12786
        }
      ],
      "sessionIds": [
        2458
      ],
      "eventIds": []
    },
    {
      "id": 6929,
      "typeId": 10743,
      "title": "Pervasive Gaze Sensing, Analysis, and Interaction: The New Frontier",
      "trackId": 10022,
      "tags": [],
      "keywords": [],
      "abstract": "The measurement, analysis, and use of human gaze has a long history in various academic disciplines and in industry but has long been limited to special application domains or user groups. Driven by the recent commercial breakthrough of virtual and augmented reality technology, as well as the advent of affordable stationary and head-mounted eye trackers, gaze interfaces are on the verge to finally become available in a wide range of consumer applications and to be used by millions of users on a daily basis. These latest advances pose an important question - what is next?\n\nIn my talk I will argue for a new frontier in eye tracking research: The development of pervasive attentive user interfaces that sense, analyse, and adapt to users' gaze in all explicit and implicit interactions that users perform with machines in everyday life. These new interfaces will go far beyond current interfaces that still require (partly) controlled environments, that assume gaze input to be deliberately triggered by users, that often consider the point of gaze as the only gaze characteristic, and that are geared to temporary interactions. Pervasive attentive user interfaces promise exciting new applications that, for example, sense gaze robustly, accurately, and seamlessly across arbitrary devices and systems, that analyse gaze behaviour continuously over long periods of time in daily life, and that combine gaze with other input modalities to enable truly natural, intuitive, and expressive interactions with machines.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1907
      ],
      "eventIds": []
    },
    {
      "id": 4116,
      "typeId": 10286,
      "title": "Investigating the multicausality of processing speed deficits across developmental disorders with eye tracking and EEG: extended abstract",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Neuropsychological tests inform about performance differences in cognitive functions but they typically tell little about the causes for these differences. Here, we propose a project which builds upon a recently developed novel multimodal neuroscientific approach of simultanous eye-tracking and EEG measurements to provide insights into diverse causes of performance differences in the Symbol Search Test (SST). Using a unique large dataset we plan to investigate the causes for performance differences in the SST in healthy and clinically diagnosed children and adolescents. Firstly, we aim to investigate how causes for differences in performance in the SST evolve over age in healthy, typically developing children. With this we plan to dissect aging effects from effects that are specific to developmental neuropsychiatric disorders. Secondly, we will include subjects with deficient performance to investigate different causes for bad performance to identify data-driven subgroups of poor performers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9179
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 6165,
      "typeId": 10928,
      "title": "EyeMSA: Exploring Eye Movement Data with Pairwise and Multiple Sequence Alignment",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movement data can be regarded as a set of scan paths, each corresponding to one of the visual scanning strategies of a certain study participant. Finding common subsequences in those scan paths is a challenging task since they are typically not equally temporally long, do not consist of the same number of fixations, or do not lead along similar stimulus regions. In this paper we describe a technique based on pairwise and multiple sequence alignment to support a data analyst to see the most important patterns in the data. To reach this goal the scan paths are first transformed into a sequence of characters based on metrics as well as spatial and temporal aggregations. The result of the algorithmic data transformation is used as input for an interactive consensus matrix visualization. We illustrate the usefulness of the concepts by applying it to formerly recorded eye movement data investigating route finding tasks in public transport maps.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19432
        },
        {
          "affiliations": [],
          "personId": 21610
        },
        {
          "affiliations": [],
          "personId": 15411
        },
        {
          "affiliations": [],
          "personId": 18716
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 5398,
      "typeId": 10048,
      "title": "Fixation Detection for Head-Mounted Eye Tracking Based on Visual Similarity of Gaze Targets",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Fixations are widely analysed in human vision, gaze-based interaction, and experimental psychology research. However, robust fixation detection in mobile settings is profoundly challenging given the prevalence of user and gaze target motion. These movements feign a shift in gaze estimates in the frame of reference defined by the eye tracker's scene camera. To address this challenge, we present a novel fixation detection method for head-mounted eye trackers. Our method exploits that, independent of user or gaze target motion, target appearance remains about the same during a fixation. It extracts image information from small regions around the current gaze position and analyses the appearance similarity of these gaze patches across video frames to detect fixations. We evaluate our method using fine-grained fixation annotations on a five-participant indoor dataset (MPIIEgoFixation) with more than 2,300 fixations in total. Our method outperforms commonly used velocity- and dispersion-based algorithms, which highlights its significant potential to analyse scene image information for eye movement detection.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11219
        },
        {
          "affiliations": [],
          "personId": 16025
        },
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1219
      ],
      "eventIds": []
    },
    {
      "id": 5143,
      "typeId": 10928,
      "title": "Towards Using the Spatio-temporal Properties of Eye Movements to Classify Visual Field Defects",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Perimetry - assessment of visual field defects (VFD) - requires patients to be able to maintain a prolonged stable fixation, as well as to provide feedback through motor response. These aspects limit the testable population and often lead to inaccurate results. We hypothesized that different VFD would alter the eye-movements in systematic ways, thus making it possible to infer the presence of VFD by quantifying the spatio-temporal properties of eye movements. We developed a tracking test to record participant's eye-movements while we simulated different gaze-contingent VFD. We tested 50 visually healthy participants and simulated three common scotomas: peripheral loss, central loss and hemifield loss. We quantified spatio-temporal features using cross-correlogram analysis, then applied cross-validation to train a decision tree algorithm to classify the conditions. Our test is faster and more comfortable than standard perimetry and it is able to achieve a classifying accuracy of ~90% (True Positive Rate = ~98%) with data acquired in less than 2 minutes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12882
        },
        {
          "affiliations": [],
          "personId": 21143
        },
        {
          "affiliations": [],
          "personId": 13810
        },
        {
          "affiliations": [],
          "personId": 11682
        }
      ],
      "sessionIds": [
        1301,
        1331
      ],
      "eventIds": []
    },
    {
      "id": 5912,
      "typeId": 10286,
      "title": "Virtual reality as a proxy for real-life social attention?",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Previous studies found large amounts of overt attention allocated towards human faces when they were presented as images or videos, but a relative avoidance of gaze at conspecifics' faces in real-world situations. We measured gaze behavior in a complex virtual scenario in which a human face and an object were similarily exposed to the participants' view. Gaze at the face was avoided compared to gaze at the object, providing support for the hypothesis that virtual reality scenarios are capable of eliciting modes of information processing comparable to real-world situations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10468
        },
        {
          "affiliations": [],
          "personId": 16034
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 7960,
      "typeId": 10048,
      "title": "The art of pervasive eye tracking: unconstrained eye tracking in the Austrian Gallery Belvedere",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Pervasive mobile eye tracking provides a rich data source to investigate human natural behavior, providing a high degree of ecological validity in natural environments. However, challenges and limitations intrinsic to unconstrained mobile eye tracking makes its development and usage to some extent an art. Nonetheless, researchers are pushing the boundaries of this technology to help assess museum visitors' attention not only between the exhibited works, but also within particular pieces, providing significantly more detailed insights than traditional timing-and-tracking or external observer approaches. In this paper, we present in detail the eye tracking system developed for a large scale fully-unconstrained study in the Austrian Gallery Belvedere, providing useful information for eye-tracking system designers. Furthermore, the study is described, and we report on usability and real-time performance metrics. Our results suggest that, although the system is comfortable enough, further eye tracker improvements are necessary to make it less conspicuous. Additionally, real-time accuracy already suffices for simple applications such as audio guides for the majority of users even in the absence of eye-tracker slippage compensation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13130
        },
        {
          "affiliations": [],
          "personId": 24370
        },
        {
          "affiliations": [],
          "personId": 8664
        },
        {
          "affiliations": [],
          "personId": 23520
        },
        {
          "affiliations": [],
          "personId": 18363
        },
        {
          "affiliations": [],
          "personId": 23370
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1958
      ],
      "eventIds": []
    },
    {
      "id": 3865,
      "typeId": 10928,
      "title": "Implicit user calibration for gaze-tracking systems using averaged saliency map around the optical axis of the eye",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "A 3D gaze-tracking method that uses two cameras and two light sources can measure the optical axis of the eye without user calibration. The visual axis of the eye (line of sight) is estimated by conducting a single-point user calibration. This single-point user calibration estimates the angle kappa that is offset between the optical and visual axes of the eye, which is a user-dependent parameter. We have proposed an implicit user calibration method for gaze-tracking systems using a saliency map around the optical axis of the eye. We assume that the peak of the average of the saliency maps indicates the visual axis of the eye in the eye coordinate system. We used both-eye restrictions effectively. The experimental result shows that the proposed system could estimate angle kappa without explicit personal calibration.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11918
        },
        {
          "affiliations": [],
          "personId": 21149
        },
        {
          "affiliations": [],
          "personId": 15457
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4890,
      "typeId": 10928,
      "title": "Hidden Pursuits: Evaluating Gaze-selection via Pursuits when the Stimulus Trajectory is Partially Hidden",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "The idea behind gaze interaction using Pursuits is to leverage the human’s smooth pursuit eye movements performed when following moving targets. However, humans can also anticipate where a moving target would reappear if it temporarily hides from their view. In this work, we investigate how well users can select targets using Pursuits in cases where the target’s trajectory is partially invisible (HiddenPursuits): e.g., can users select a moving target that temporarily hides behind another object? Although HiddenPursuitswas not studied in the context of interaction before, understanding how well users can perform HiddenPursuits presents numerous opportunities, particularly for small interfaces where a target’s trajectory can cover the area outside of the screen. We found that users can still select targets quickly via Pursuits even if their trajectory is up to 50% hidden, and at the expense of longer selection times when the hidden portion is larger. We discuss how gaze-based interfaces can leverage HiddenPursuits for an improved user experience.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14365
        },
        {
          "affiliations": [],
          "personId": 16400
        },
        {
          "affiliations": [],
          "personId": 22787
        },
        {
          "affiliations": [],
          "personId": 20391
        },
        {
          "affiliations": [],
          "personId": 12720
        }
      ],
      "sessionIds": [
        1301,
        2498
      ],
      "eventIds": []
    },
    {
      "id": 6428,
      "typeId": 10284,
      "title": "Gaze Analytics Pipeline",
      "trackId": 10019,
      "tags": [],
      "keywords": [],
      "abstract": "This tutorial presents details of a Python-based gaze analytics pipeline developed and used by Prof. Duchowski and Ms. Gehrer. The gaze analytics pipeline consists of Python scripts for extraction of raw eye movement data, analysis and event detection via velocity-based filtering, collation of events for statistical evaluation, analysis and visualization of results using R. The tutorial is couched in analysis of gaze data collected during discrimination of different emotional expressions while viewing faces. The tutorial covers basic eye movement analytics, e.g., fixation count and dwell time within AOIs, as well as advanced analysis using gaze transition entropy. Newer analytical tools and techniques such as microsaccade detection and the Index of Pupillary Activity will be covered with time permitting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10369
        },
        {
          "affiliations": [],
          "personId": 21699
        }
      ],
      "sessionIds": [
        1975
      ],
      "eventIds": []
    },
    {
      "id": 6942,
      "typeId": 10048,
      "title": "Toward conjoint analysis of simultaneous eye-tracking and fMRI data for program-comprehension studies",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "After decades of research, there is still no comprehensive, validated model of program comprehension. Recently, researchers have been applying psycho-physiological measures to expand our understanding of program comprehension. In this position paper, we argue that measuring program comprehension simultaneously with functional magnetic resonance imaging (fMRI) and eye tracking is promising. However, due to the different nature of both measures in terms of response delay and temporal resolution, we need to develop suitable tools. We describe the challenges of conjoint analysis of fMRI and eye-tracking data, and we also outline possible solution strategies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8786
        },
        {
          "affiliations": [],
          "personId": 18022
        },
        {
          "affiliations": [],
          "personId": 16204
        },
        {
          "affiliations": [],
          "personId": 23399
        },
        {
          "affiliations": [],
          "personId": 19755
        }
      ],
      "sessionIds": [
        2097
      ],
      "eventIds": []
    },
    {
      "id": 7454,
      "typeId": 10970,
      "title": "Hands-Free Web Browsing: Enriching the User Experience with Gaze and Voice Modality",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Hands-free browsers provide an effective tool for Web interaction and accessibility, overcoming the need for conventional input sources. Current approaches to hands-free interaction are primarily categorized in either voice or gaze-based modality. In this work, we investigate how these two modalities could be integrated to provide a better hands-free experience for end-users. We demonstrate a multimodal browsing approach combining eye gaze and voice inputs for optimized interaction, and to suffice user preferences with unimodal benefits. The initial assessment with five participants indicates improved performance for the multimodal prototype in comparison to single modalities for hands-free Web browsing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17046
        },
        {
          "affiliations": [],
          "personId": 20347
        },
        {
          "affiliations": [],
          "personId": 10135
        },
        {
          "affiliations": [],
          "personId": 20190
        },
        {
          "affiliations": [],
          "personId": 11841
        }
      ],
      "sessionIds": [
        1551
      ],
      "eventIds": []
    },
    {
      "id": 4127,
      "typeId": 10928,
      "title": "Pupil Size as an Indicator of Visual-motor Workload and Expertise in Microsurgical Training Tasks",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Pupillary responses have been for long linked to cognitive workload in numerous tasks. In this work, we investigate the role of pupil dilations in the context of microsurgical training, handling of microinstruments and the suturing act in particular. With an eye-tracker embedded on the surgical microscope oculars, eleven medical participants repeated 12 sutures of artificial skin under high magnification. A detailed analysis of pupillary dilations in suture segments revealed that pupillary responses indeed varied not only according to the main suture segments but also in relation to participants' expertise.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21687
        },
        {
          "affiliations": [],
          "personId": 10952
        },
        {
          "affiliations": [],
          "personId": 9510
        },
        {
          "affiliations": [],
          "personId": 12643
        },
        {
          "affiliations": [],
          "personId": 16722
        },
        {
          "affiliations": [],
          "personId": 17141
        },
        {
          "affiliations": [],
          "personId": 19216
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4896,
      "typeId": 10048,
      "title": "Supervised Descent Method (SDM) applied to accurate pupil detection in off-the-shelf eye tracking systems",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "The precise detection of pupil/iris center is key to estimate gaze accurately. This fact becomes specially challenging in low cost frameworks in which the algorithms employed for high performance systems fail. In the last years an outstanding effort has been made in order to apply training-based methods to low resolution images. In this paper, Supervised Descent Method (SDM) is applied to GI4E database. The 2D landmarks employed for training are the corners of the eyes and the pupil centers. In order to validate the algorithm proposed, a cross validation procedure is performed. The strategy employed for the training allows us to affirm that our method can potentially outperform the state of the art algorithms applied to the same dataset in terms of 2D accuracy. The promising results encourage to carry on in the study of training-based methods for eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19880
        },
        {
          "affiliations": [],
          "personId": 15465
        },
        {
          "affiliations": [],
          "personId": 14339
        }
      ],
      "sessionIds": [
        1953
      ],
      "eventIds": []
    },
    {
      "id": 5666,
      "typeId": 10928,
      "title": "Development of diagnostic performance & visual processing in different types of radiological expertise",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "The aim of this research was to compare visual patterns while examining radiographs in groups of people with different levels and different types of expertise. Introducing the latter comparative base is the original contribution of these studies. The residents and specialists were trained in medical diagnosing of X-Rays and for these two groups it was possible to compare visual patterns between observers with different level of the same expertise type. On the other hand, the radiographers who took part in the examination - due to specific of their daily work - had experience in reading and evaluating X-Rays quality and were not trained in diagnosing. Involving this group created in our research the new opportunity to explore eye movements obtained when examining X-Ray for both medical diagnosing and quality assessment purposes, which may be treated as different types of expertise.\n\nWe found that, despite the low diagnosing performance, the radiographers eye movement characteristics were more similar to the specialists than eye movement characteristics of the residents. It may be inferred that people with different type of expertise, yet after gaining a certain level of experience (or practise), may develop similar visual patterns which is the original conclusion of the research.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8734
        },
        {
          "affiliations": [],
          "personId": 8923
        },
        {
          "affiliations": [],
          "personId": 8244
        }
      ],
      "sessionIds": [
        1301,
        1331
      ],
      "eventIds": []
    },
    {
      "id": 5667,
      "typeId": 10801,
      "title": "Advantages of Eye-Gaze over Gaze-based Interaction in Virtual and Augmented Reality",
      "trackId": 10024,
      "tags": [],
      "keywords": [],
      "abstract": "The current best practice for hands-free selection using Virtual and Augmented Reality (VR/AR) head-mounted displays is to use head-gaze for aiming and dwell-time or clicking for triggering the selection. There is an observable trend for new VR and AR devices to come with integrated eye-tracking units to improve rendering, to provide means for attention analysis or for social interactions. Eye-gaze has been successfully used for human-computer interaction in other domains, primarily on desktop computers. In VR/AR systems, aiming via eye-gaze could be significantly faster and less exhausting than via head-gaze.\n\nTo evaluate benefits of eye-gaze-based interaction methods in VR and AR, we compared aiming via head-gaze and aiming via eye-gaze. We show that eye-gaze outperforms head-gaze in terms of speed, task load, required head movement and user preference. We furthermore show that the advantages of eye-gaze further increase with larger FOV sizes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16271
        },
        {
          "affiliations": [],
          "personId": 17157
        },
        {
          "affiliations": [],
          "personId": 22105
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 2855,
      "typeId": 10286,
      "title": "A text entry interface using smooth pursuit movements and language model",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, with the development of eye tracking technology, the gaze-interaction applications demonstrate great potential. Smooth pursuit based gaze typing is an intuitive text entry system with low learning effort. In this study, we provide a language-prediction function for a smooth-pursuit based gaze-typing system. Since the state-of-the-art neural network models have been successfully applied in language modeling, this study uses a pretrained model based on convolutional neural networks (CNNs) and develops a prediction function, which can predict both next possible letters and word. The results of a pilot experiment have shown that the next possible letters or word can be well predicted and selected. The mean typing speed can achieve 4.5 words per minute. The participants consider that the word prediction is helpful for reducing the visual search time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20974
        },
        {
          "affiliations": [],
          "personId": 19214
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 4391,
      "typeId": 10928,
      "title": "SLAM-based Localization of 3D Gaze using a Mobile Eye Tracker",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Past work in eye tracking has focused on estimating gaze targets in two dimensions (2D), e.g. on a computer screen or scene camera image. Three-dimensional (3D) gaze estimates would be extremely useful when humans are mobile and interacting with the real 3D environment. We describe a system for estimating the 3D locations of gaze using a mobile eye tracker. The system integrates estimates of the user's gaze vector from a mobile eye tracker, estimates of the eye tracker pose from a visual-inertial simultaneous localization and mapping (SLAM) algorithm, a 3D point cloud map of the environment from a RGB-D sensor. Experimental results indicate that our system produces accurate estimates of 3D gaze over a much larger range than remote eye trackers. Our system will enable applications, such as the analysis of 3D human attention and more anticipative human robot interfaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10005
        },
        {
          "affiliations": [],
          "personId": 15440
        },
        {
          "affiliations": [],
          "personId": 15496
        },
        {
          "affiliations": [],
          "personId": 22568
        },
        {
          "affiliations": [],
          "personId": 21198
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7212,
      "typeId": 10048,
      "title": "Visual analysis of eye gazes to assist strategic planning in computer games",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "This work studies the use of a conventional eye tracking system for analysis of an online game player's thinking processes. For this purpose, the eye gaze data of several users playing a simple online turn-based checkers game were recorded and made available in real-time to gaze-informed players. The motivation behind this work is to determine if making the eye-gaze data available can help these players to predict the gaze-tracked opponent player's further moves, and also how this can be most effectively done. We also tested different orientations of the screen on which the gaze data were displayed. By our visual and algorithmic analysis we validated (1) that prediction is possible and (2) that accuracy highly depends on the moves of players throughout the game as well as on the screen orientation. We believe that our study has implications on visual problem solving in general, especially in collaborative scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17574
        },
        {
          "affiliations": [],
          "personId": 19432
        },
        {
          "affiliations": [],
          "personId": 23341
        }
      ],
      "sessionIds": [
        1345
      ],
      "eventIds": []
    },
    {
      "id": 5934,
      "typeId": 10779,
      "title": "A Fitts' Law Evaluation of Gaze Input on Large Displays Compared to Touch and Mouse Inputs",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze-assisted interaction has commonly been used in a standard desktop setting. When interacting with large displays, as new scenarios like situationally-induced impairments emerge, it is more convenient to use the gaze-based multi-modal input than other inputs. However, it is unknown as to how the gaze-based multi-modal input compares to touch and mouse inputs. We compared gaze+foot multi-modal input to touch and mouse inputs on a large display in a Fitts' Law experiment that conforms to ISO 9241-9. From a study involving 23 participants, we found that the gaze input has the lowest throughput (2.33 bits/s), and the highest movement time (1.176 s) of the three inputs. In addition, though touch input involves maximum physical movements, it achieved the highest throughput (5.49 bits/s), the least movement time (0.623 s), and was the most preferred input.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14886
        },
        {
          "affiliations": [],
          "personId": 9360
        }
      ],
      "sessionIds": [
        1783
      ],
      "eventIds": []
    },
    {
      "id": 3631,
      "typeId": 10928,
      "title": "Smooth-i: Smart Re-Calibration Using Smooth Pursuit Eye Movements",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Eye gaze for interaction is dependent on calibration. However, gaze calibration can deteriorate over time affecting the usability of the system. We propose to use motion matching of smooth pursuit eye movements and known motion on the display to determine when there is a drift in accuracy and use it as input for re-calibration. To explore this idea we developed Smooth-i, an algorithm that stores calibration points and updates them incrementally when inaccuracies are identified. To validate the accuracy of Smooth-i, we conducted a study with five participants and a remote eye tracker. A baseline calibration profile was used by all participants to test the accuracy of the Smooth-i re-calibration following interaction with moving targets. Results show that Smooth-i is able to manage re-calibration efficiently, updating the calibration profile only when inaccurate data samples are detected.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19061
        },
        {
          "affiliations": [],
          "personId": 13804
        }
      ],
      "sessionIds": [
        1953,
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7727,
      "typeId": 10970,
      "title": "Real-time gaze transition entropy",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "In this video, we introduce a real-time algorithm that computes gaze transition entropy. This approach can be employed in detecting higher level cognitive states such as situation awareness. We first compute fixations using our real-time version of a well established velocity threshold based algorithm. We then compute the gaze transition entropy for a content independent grid of areas of interest in real-time using an update processing window approach. We test for Markov property after each update to test whether Markov assumption holds. Higher entropy corresponds to increased eye movement and more frequent monitoring of the visual field. In contrast, lower entropy corresponds to fewer eye movements and less frequent monitoring. Based on entropy levels, the system could then alert the user accordingly and plausibly offer an intervention. We developed an example application to demonstrate the use of the online calculation of gaze transition entropy in a practical scenario.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13804
        },
        {
          "affiliations": [],
          "personId": 22633
        }
      ],
      "sessionIds": [
        1568
      ],
      "eventIds": []
    },
    {
      "id": 2866,
      "typeId": 10801,
      "title": "3D gaze estimation in the scene volume with a head-mounted eye tracker",
      "trackId": 10024,
      "tags": [],
      "keywords": [],
      "abstract": "Most applications involving gaze-based interaction are supported by estimation techniques that find a mapping between gaze data and corresponding targets on a 2D surface. However, in Virtual and Augmented Reality (AR) environments, interaction occurs mostly in a volumetric space, which poses a challenge to such techniques. Accurate point-of-regard (PoR) estimation, in particular, is of great importance to AR applications, since most known setups are prone to parallax error and target ambiguity. In this work, we expose the limitations of widely used techniques for PoR estimation in 3D and propose a new calibration procedure using an uncalibrated head-mounted binocular eye tracker coupled with an RGB-D camera to track 3D gaze within the scene volume. We conducted a study to evaluate our setup with real-world data using a geometric and an appearance-based method. Our results show that accurate estimation in this setting still is a challenge, though some gaze-based interaction techniques in 3D should be possible.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10156
        },
        {
          "affiliations": [],
          "personId": 17775
        },
        {
          "affiliations": [],
          "personId": 12120
        },
        {
          "affiliations": [],
          "personId": 8239
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 6707,
      "typeId": 10048,
      "title": "The Eye of the Typer: A Benchmark and Analysis of Gaze Behavior during Typing",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "We examine the relationship between eye gaze and typing, focusing on the differences between touch and non-touch typists. To enable typing-based research, we created a 51-participant benchmark dataset for user input across multiple tasks, including user input data, screen recordings, webcam video of the participant's face, and eye tracking positions. There are patterns of eye movements that differ between the two types of typists, representing glances at the keyboard, which can be used to identify touch-typed strokes with 92% accuracy. Then, we relate eye gaze with cursor activity, aligning both pointing and typing to eye gaze. One demonstrative application of the work is in extending WebGazer, a real-time web-browser-based webcam eye tracker. We show that incorporating typing behavior as a secondary signal improves eye tracking accuracy by 16% for touch typists, and 8% for non-touch typists.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13194
        },
        {
          "affiliations": [],
          "personId": 21109
        },
        {
          "affiliations": [],
          "personId": 12483
        },
        {
          "affiliations": [],
          "personId": 13063
        },
        {
          "affiliations": [],
          "personId": 14760
        }
      ],
      "sessionIds": [
        2188
      ],
      "eventIds": []
    },
    {
      "id": 2868,
      "typeId": 10928,
      "title": "PuReST: Robust Pupil Tracking for Real-Time Pervasive Eye Tracking",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Pervasive eye-tracking applications such as gaze-based human computer interaction and advanced driver assistance require real-time, accurate, and robust pupil detection.  However, automated pupil detection has proved to be an intricate task in real-world scenarios due to a large mixture of challenges -- for instance, quickly changing illumination and occlusions.  In this work, we introduce the Pupil Reconstructor with Subsequent Tracking (PuReST), a novel method for fast and robust pupil tracking.  The proposed method was evaluated on over 266,000 realistic and challenging images acquired with three distinct head-mounted eye tracking devices, increasing pupil detection rate by 5.44 and 29.92 percentage points while reducing average run time by a factor of 2.74 and 1.1. w.r.t. state-of-the-art 1) pupil detectors and 2) vendor provided pupil trackers, respectively.  Overall,  outperformed other methods in 81.82% of use cases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13130
        },
        {
          "affiliations": [],
          "personId": 12726
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7989,
      "typeId": 10048,
      "title": "A novel approach to single camera, glint-free 3D eye model fitting including corneal refraction",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Model-based methods for glint-free gaze estimation typically infer eye pose using pupil contours extracted from eye images. Existing methods, however, either ignore or require complex hardware setups to deal with refraction effects occurring at the corneal interfaces. In this work we provide a detailed analysis of the effects of refraction in glint-free gaze estimation using a single near-eye camera, based on the method presented by [Świrski and Dodgson 2013]. We demonstrate systematic deviations in inferred eyeball positions and gaze directions with respect to synthetic ground-truth data and show that ignoring corneal refraction can result in angular errors of several degrees. Furthermore, we quantify gaze direction dependent errors in pupil radius estimates. We propose a novel approach to account for corneal refraction in 3D eye model fitting and by analyzing synthetic and real images show that our new method successfully captures refraction effects and helps to overcome the shortcomings of the state of the art approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16669
        },
        {
          "affiliations": [],
          "personId": 14822
        },
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1953
      ],
      "eventIds": []
    },
    {
      "id": 3127,
      "typeId": 10970,
      "title": "Self-made mobile gaze tracking for group studies",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile gaze tracking does not need to be expensive. We have a self-made mobile gaze tracking system, consisting of glasses-like frame and a software which computes the gaze point. As the total cost of the equipment is less than thousand euros, we have prepared five devices which we use in group studies, to simultaneously estimate multiple students' gaze in classroom. The inexpensive mobile gaze tracking technology opens new possibilities for studying the attentional processes on a group level.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23953
        },
        {
          "affiliations": [],
          "personId": 12534
        },
        {
          "affiliations": [],
          "personId": 24401
        }
      ],
      "sessionIds": [
        1568
      ],
      "eventIds": []
    },
    {
      "id": 4152,
      "typeId": 10779,
      "title": "Beyond Gaze Cursor - Exploring Information-Based Gaze Sharing In Chat",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze sharing is found to be beneficial for computer-mediated collaboration by several studies. Usually, a coordinate-based visualization like a gaze cursor is used which helps to reduce or replace deictic expressions and thus makes gaze sharing a useful addition for tasks where referencing is crucial. But the spatial-based visualization limits its use to What-You-See-Is-What-I-See (WYSIWIS) interfaces and puts a ceiling on group size, as multiple object tracking becomes near impossible beyond dyads. In this paper, we will explore and discuss the use of gaze sharing outside the realm of WYSIWIS interfaces and dyads by replacing the gaze cursor with information-based gaze sharing in form of a reading progress bar to a chat interface. Preliminary results with triads show that historic and present information on attention and reading behavior are a useful addition to increase group awareness.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16681
        },
        {
          "affiliations": [],
          "personId": 21870
        },
        {
          "affiliations": [],
          "personId": 19363
        },
        {
          "affiliations": [],
          "personId": 9479
        }
      ],
      "sessionIds": [
        1783
      ],
      "eventIds": []
    },
    {
      "id": 6456,
      "typeId": 10970,
      "title": "Tracing Gaze-Following Behavior in Virtual Reality Using Wiener-Granger Causality",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "We modelled gaze following behavior in a naturalistic virtual reality environment using Wiener-Granger causality. Using this method, gaze following was statistically tangible throughout the experiment, but could not easily be pinpointed to precise moments in time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10468
        },
        {
          "affiliations": [],
          "personId": 16034
        }
      ],
      "sessionIds": [
        1551
      ],
      "eventIds": []
    },
    {
      "id": 4154,
      "typeId": 10970,
      "title": "Semantic Fovea: Real-time annotation of ego-centric videos with gaze context",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Visual context plays a crucial role in understanding human visual attention in natural, unconstrained tasks - the objects we look at during everyday tasks provide an indicator of our ongoing attention. Collection, interpretation, and study of visual behaviour in unconstrained environments therefore is necessary, however presents many challenges, requiring painstaking hand-coding. Here we demonstrate a proof-of-concept system that enables real-time annotation of objects in an egocentric video stream from head-mounted eye-tracking glasses. We concurrently obtain a live stream of user gaze vectors with respect to their own visual field. Even during dynamic, fast-paced interactions, our system was able to recognise all objects in the user's field-of-view with moderate accuracy. To validate our concept, our system was used to annotate an in-lab breakfast scenario in real time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12270
        },
        {
          "affiliations": [],
          "personId": 15659
        },
        {
          "affiliations": [],
          "personId": 12454
        },
        {
          "affiliations": [],
          "personId": 21339
        },
        {
          "affiliations": [],
          "personId": 8592
        }
      ],
      "sessionIds": [
        1551
      ],
      "eventIds": []
    },
    {
      "id": 7994,
      "typeId": 10048,
      "title": "Gaze behaviour in computer programmers with dyslexia: considerations regarding code style, layout and crowding",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Previous work investigating the eye movements of computer programmers with dyslexia suggests that the gaze behaviour expected of dyslexic readers when processing natural text does not consistently manifest when programmers with dyslexia read program code. Instead, the observed eye movements of programmers with dyslexia appear to represent a complex hybrid of gaze behaviour both typical and atypical of dyslexic readers. Building on this work, this paper explores the possible impact of code style, layout and crowding on the reading behaviour of programmers with dyslexia. Related work on the phenomenon of crowding in the dyslexia literature is used to inform a possible experimental design to explore the effect of crowding in this context.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19233
        },
        {
          "affiliations": [],
          "personId": 21754
        }
      ],
      "sessionIds": [
        2468
      ],
      "eventIds": []
    },
    {
      "id": 3134,
      "typeId": 10048,
      "title": "Predicting the Gaze Depth in Head-mounted Displays using Multiple Feature Regression",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Head-mounted displays (HMDs) with integrated eye trackers have opened up a new realm for gaze-contingent rendering. The accurate estimation of gaze depth is essential when modeling the optical capabilities of the eye. Most recently multifocal displays are gaining importance, requiring focus estimates to control displays or lenses. Deriving the gaze depth solely by sampling the scene's depth at the point-of-regard fails for complex or thin objects as eye tracking is suffering from inaccuracies. Gaze depth measures using the eye's vergence only provide an accurate depth estimate for the first meter. In this work, we combine vergence measures and multiple depth measures into feature sets. This data is used to train a regression model to deliver improved estimates. We present a study showing that using multiple features allows for an accurate estimation of the focused depth (MSE<0.1m) over a wide range (first 6m).",
      "authors": [
        {
          "affiliations": [],
          "personId": 17449
        },
        {
          "affiliations": [],
          "personId": 11032
        },
        {
          "affiliations": [],
          "personId": 16833
        },
        {
          "affiliations": [],
          "personId": 17662
        }
      ],
      "sessionIds": [
        1219
      ],
      "eventIds": []
    },
    {
      "id": 7489,
      "typeId": 10953,
      "title": "A Gaze Gesture-based Paradigm for Situational Impairments, Accessibility, and Rich Interactions",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze gesture-based interactions on a computer are promising, but the existing systems are limited by the number of supported gestures, recognition accuracy, need to remember the stroke order, lack of extensibility, and so on. We present a gaze gesture-based interaction framework where a user can design gestures and associate them to appropriate commands like minimize, maximize, scroll, and so on. This allows the user to interact with a wide range of applications using a common set of gestures. Furthermore, our gesture recognition algorithm is independent of the screen size, resolution, and the user can draw the gesture anywhere on the target application. Results from a user study involving seven participants showed that the system recognizes a set of nine gestures with an accuracy of 93% and a F-measure of 0.96. We envision, this framework can be leveraged in developing solutions for situational impairments, accessibility, and also for implementing rich a interaction paradigm.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14886
        },
        {
          "affiliations": [],
          "personId": 9360
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 2881,
      "typeId": 10801,
      "title": "Content-based Image Retrieval Based On Eye-tracking",
      "trackId": 10024,
      "tags": [],
      "keywords": [],
      "abstract": "To improve the performance of an image retrieval system, a novel content-based image retrieval (CBIR) framework with eye tracking data based on an implicit relevance feedback mechanism is proposed in this paper. Our proposed framework consists of three components: feature extraction and selection, visual retrieval, and relevance feedback. First, by using the quantum genetic algorithm and the principle component analysis algorithm, optimal image features with 70 components are extracted. Second, a finer retrieving procedure based on multiclass support vector machine (SVM) and fuzzy c-mean (FCM) algorithm is implemented for retrieving most relevant images. Finally, a deep neural network is trained to exploit the information of the user regarding the relevance of the returned images. This information is then employed to update the retrieving point for a new round retrieval. Experiments on two databases (Corel and Caltech) show that the performance of CBIR can be significantly improved by using our proposed framework.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15494
        },
        {
          "affiliations": [],
          "personId": 20319
        },
        {
          "affiliations": [],
          "personId": 17198
        }
      ],
      "sessionIds": [
        1783
      ],
      "eventIds": []
    },
    {
      "id": 7234,
      "typeId": 10970,
      "title": "Substantiating Reading Teachers with Scanpaths",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "We present a tool that allows reading teachers to record and replay students' voice and gaze behavior during reading. The tool replays scanpaths to reading professionals without prior gaze data experience. On the basis of test experiences with 147 students, we share our initial observations on how teachers make use of the tool to create a dialog with their students.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17011
        },
        {
          "affiliations": [],
          "personId": 14896
        },
        {
          "affiliations": [],
          "personId": 17607
        },
        {
          "affiliations": [],
          "personId": 19325
        }
      ],
      "sessionIds": [
        1005
      ],
      "eventIds": []
    },
    {
      "id": 4163,
      "typeId": 10970,
      "title": "Mobile Consumer Shopping Journey in Fashion Retail: Eye Tracking Mobile Apps and Websites",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Despite the rapid adoption of smartphones among fashion consumers, their dissatisfaction with retailers' mobile apps and websites also increases. This suggests that understanding how mobile consumers use smartphones for shopping is important in developing digital shopping platforms fulfilling consumers' expectations. Research to date has not focused on eye tracking consumer shopping behavior using smartphones. For this research, we employed mobile eye tracking experiments in order to develop unique shopping journeys for each fashion consumer accounting for differences and similarities in their behavior. Based on scan path visualizations and shopping journeys we developed a precise account about the areas the majority of fashion consumers look at when browsing and inspecting product pages. Based on the findings, we identified mobile consumers' behaviour patterns, usability issues of the mobile channel and established what features the mobile retail channel needs to have to satisfy fashion consumers' needs by offering pleasing customer user experiences.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14140
        }
      ],
      "sessionIds": [
        1551
      ],
      "eventIds": []
    },
    {
      "id": 4165,
      "typeId": 10048,
      "title": "Eye tracking in naturalistic badminton play: comparing visual gaze pattern strategy in world-rank and amateur player",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "A professional player's expertise rests on the ability to predict action by optimally extracting the opponent's postural cues. Eye tracking (head-mounted system) data in a naturalistic singles badminton play was collected from one professional world-ranked player facing five amateur players (10 serves or 50 trials) and two amateurs playing against four other amateur players each (10 serves or 80 trials). The visual gaze on the opponent body, segregated into 3 areas-of-interest covering the feet, face/torso, and hand/racket of the opponent and the shuttle, was analysed for a) the period just before the serve, b) while receiving the serve and c) the entire rally. The comparative analysis shows the first area-of-interest for professional player as the opponent's feet while executing the serve and the hand/racket when receiving a serve. On the other hand, the amateur players show no particular strategy of fixation location either for the serve task or while facing a serve. The average fixation duration (just before serve) for the professional was 0.96s and for the amateurs it was 1.48s. The findings highlight the differences in the postural cue considered important and the preparatory time in professional and amateur players. We believe, analytical models from dynamic gaze behavior in naturalistic game conditions as applied in this study can be used for enhancing perceptual-cognitive skills during training.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17680
        },
        {
          "affiliations": [],
          "personId": 15290
        },
        {
          "affiliations": [],
          "personId": 18093
        }
      ],
      "sessionIds": [
        1958
      ],
      "eventIds": []
    },
    {
      "id": 6480,
      "typeId": 10286,
      "title": "Asynchronous gaze sharing: towards a dynamic help system to support learners during program comprehension",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "To participate in a society of a rapidly changing world, learning fundamentals of programming is important. However, learning to program is challenging for many novices and reading source code is one major obstacle in this challenge. The primary research objective of my dissertation is developing a help system based on historical and interactive eye tracking data to help novices master program comprehension. Helping novices requires detecting problematic situations while solving programming tasks using a classifier to split novices into successful/unsuccessful participants based on the answers given to program comprehension tasks. One set of features of this classifier is the story reading and execution reading order. The first step in my dissertation is creating a classifier for the reading order problem. The current status of this step is analyzing eye tracking datasets of novices and experts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21843
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 5457,
      "typeId": 10928,
      "title": "How many words is a picture worth? Attention allocation on thumbnails versus title text regions",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Cognitive scientists and psychologists have long noted the ”picture superiority effect”, that is, pictorial content is more likely to be remembered and more likely to lead to an increased understanding of the material.We investigated the relative importance of pictorial regions versus textual regions on a website where pictures and text co-occur in a very structured manner: video content sharing websites. In our study, we tracked participants’ eye movements as they performed a casual browsing task, that is, selecting a video to watch. The fixations were coded as falling on one of two areas of interest: thumbnail image or title text region. We found that participants allocated almost twice as much attention to thumbnails as to title text regions. They also tended to look at the thumbnail images before the title text, as predicted by the picture superiority effect. These results have implications for both user experience designers as well as video content creators.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9721
        },
        {
          "affiliations": [],
          "personId": 12402
        },
        {
          "affiliations": [],
          "personId": 10808
        },
        {
          "affiliations": [],
          "personId": 20432
        }
      ],
      "sessionIds": [
        1082,
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7507,
      "typeId": 10307,
      "title": "Remote Eye Tracking - Challenges and Future",
      "trackId": 10021,
      "tags": [],
      "keywords": [],
      "abstract": "Smart Eye AB is a Swedish company with a 20 year track record as for remote head- and eye tracking. Serving demanding industries such as the Automotive and Aviation means that R&D is a vital part of Smart Eye operations. To achieve benchmark performance and robustness within human factors is a challenge. The workshop will present and discuss some of the areas crucial for remote tracking. Such as illumination technologies, where Smart Eye tracking algorithm is invariant to changing lighting levels. Remote tracking of subjects wearing a face mask is another challenge. Also we will present our team work using neural networks. Smart Eye technology is operational within different industries. These have shaped the evolution of the Smart Eye PRO system. Different challenging setups. Such as control rooms and different simulators will be presented. Where being non-intrusive is the absolute requirement. The workshop will conclude with a Q&A session.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22885
        },
        {
          "affiliations": [],
          "personId": 15817
        },
        {
          "affiliations": [],
          "personId": 10422
        }
      ],
      "sessionIds": [
        1278
      ],
      "eventIds": []
    },
    {
      "id": 5972,
      "typeId": 10970,
      "title": "Automatic mapping of gaze position coordinates of eye-tracking glasses video on a common static reference image",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes a method for automatic semantic gaze mapping from video obtained by eye-tracking glasses to a common reference image. Image feature detection and description algorithms are utilized to find the position of subsequent video frames and map corresponding gaze coordinates on a common reference image. This process allows aggregate experiment results for further experiment analysis and provides an alternative for manual semantic gaze mapping methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18163
        },
        {
          "affiliations": [],
          "personId": 9188
        }
      ],
      "sessionIds": [
        1005
      ],
      "eventIds": []
    },
    {
      "id": 4693,
      "typeId": 10307,
      "title": "Hands-on with Pupil Labs - Current features and future directions",
      "trackId": 10021,
      "tags": [],
      "keywords": [],
      "abstract": "This workshop will provide a brief introduction to the mobile eye-tracking solutions and software ecosystem developed by Pupil Labs. In a hands-on demonstration and live-coding session, we will give a practical guide to leveraging the Pupil Labs open tool chain. Going forward, the team will present current development and research projects at Pupil Labs and in particular discuss how they utilise modern machine learning and computer vision methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16669
        },
        {
          "affiliations": [],
          "personId": 17518
        },
        {
          "affiliations": [],
          "personId": 14822
        },
        {
          "affiliations": [],
          "personId": 20172
        },
        {
          "affiliations": [],
          "personId": 13181
        },
        {
          "affiliations": [],
          "personId": 14640
        },
        {
          "affiliations": [],
          "personId": 21977
        }
      ],
      "sessionIds": [
        1692
      ],
      "eventIds": []
    },
    {
      "id": 6997,
      "typeId": 10048,
      "title": "Cross-subject workload classification using pupil-related measures",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Real-time evaluation of a person's cognitive load can be desirable in many situations. It can be employed to automatically assess or adjust the difficulty of a task, as a safety measure, or in psychological research. Eye-related measures, such as the pupil diameter or blink rate, provide a non-intrusive way to assess the cognitive load of a subject and have therefore been used in a variety of applications.\nUsually, workload classifiers trained on these measures are highly  subject-dependent and transfer poorly to other subjects. We present a novel method to generalize from a set of trained classifiers to new and unknown subjects. We use normalized features and a similarity function to match a new subject with similar subjects, for which classifiers have been previously trained. These classifiers are then used in a weighted voting system to detect workload for an unknown subject. For real-time workload classification, our methods performs at 70.4% accuracy. Higher accuracy of 76.8% can be achieved in an offline classification setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9858
        },
        {
          "affiliations": [],
          "personId": 10910
        },
        {
          "affiliations": [],
          "personId": 9199
        },
        {
          "affiliations": [],
          "personId": 11588
        }
      ],
      "sessionIds": [
        1082
      ],
      "eventIds": []
    },
    {
      "id": 6487,
      "typeId": 10048,
      "title": "Scanpath comparison in medical image reading skills of dental students: Distinguishing stages of expertise development",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "A popular topic in eye tracking is the difference between novices\nand experts and their domain-specific eye movement behaviors.\nHowever, very little is researched regarding how expertise develops,\nand more specifically, the developmental stages of eye movement\nbehaviors. Our work compares the scanpaths of five semesters of\ndental students viewing orthopantomograms (OPTs) with classifiers\nto distinguish sixth semester through tenth semester students. We\nused the analysis algorithm SubsMatch 2.0 and the Needleman-\nWunsch algorithm. Overall, both classifiers were able distinguish\nthe stages of expertise in medical image reading above chance level.\nSpecifically, it was able to accurately determine sixth semester\nstudents with no prior training as well as sixth semester students\nafter training. Ultimately, using scanpath models to recognize gaze\npatterns characteristic of learning stages, we can provide more\nadaptive, gaze-based training for students.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10433
        },
        {
          "affiliations": [],
          "personId": 21137
        },
        {
          "affiliations": [],
          "personId": 9199
        },
        {
          "affiliations": [],
          "personId": 12260
        },
        {
          "affiliations": [],
          "personId": 19535
        },
        {
          "affiliations": [],
          "personId": 13091
        },
        {
          "affiliations": [],
          "personId": 13744
        },
        {
          "affiliations": [],
          "personId": 8709
        }
      ],
      "sessionIds": [
        1331
      ],
      "eventIds": []
    },
    {
      "id": 5982,
      "typeId": 10048,
      "title": "Implementing Innovative Gaze Analytic Methods in Clinical Psychology",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "A variety of psychological disorders like antisocial personality disorder have been linked to impairments in facial emotion recognition. Exploring eye movements during categorization of emotional faces is a promising approach with the potential to reveal possible differences in cognitive processes underlying these deficits. Based on this premise we investigated whether antisocial violent offenders exhibit different scan patterns compared to a matched healthy control group while categorizing emotional faces. Group differences were analyzed in terms of attention to the eyes, extent of exploration behavior and structure of switching patterns between Areas of Interest. While we were not able to show clear group differences, the present study is one of the first that demonstrates the feasibility and utility of incorporating recently developed eye movement metrics such as gaze transition entropy into clinical psychology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21699
        },
        {
          "affiliations": [],
          "personId": 11489
        },
        {
          "affiliations": [],
          "personId": 10369
        },
        {
          "affiliations": [],
          "personId": 17228
        }
      ],
      "sessionIds": [
        1331
      ],
      "eventIds": []
    },
    {
      "id": 3936,
      "typeId": 10928,
      "title": "Wearable Eye Tracker Calibration at Your Fingertips",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Common calibration techniques for head-mounted eye trackers rely on markers or an additional person to assist with the procedure. This is a tedious process and may even hinder some practical applications. We propose a novel calibration technique which simplifies the initial calibration step for mobile scenarios. To collect the calibration samples, users only have to point with a finger to various locations in the scene. Our vision-based algorithm detects the users' hand and fingertips which indicate the users' point of interest. This eliminates the need for additional assistance or specialized markers. Our approach achieves comparable accuracy to similar marker-based calibration techniques and is the preferred method by users from our study. The implementation is openly available as a plugin for the open-source Pupil eye tracking platform.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16974
        },
        {
          "affiliations": [],
          "personId": 15711
        },
        {
          "affiliations": [],
          "personId": 19563
        }
      ],
      "sessionIds": [
        1301,
        1219
      ],
      "eventIds": []
    },
    {
      "id": 7010,
      "typeId": 10779,
      "title": "MovEye: Gaze Control of Video Playback",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "Several methods of gaze control of video playback were implemented in MovEye application. Two versions of MovEye are almost ready: for watching online movies from the YouTube service and for watching movies from the files stored on local drives. We have two goals: the social one is to help people with physical disabilities to control and enrich their immediate environment; the scientific one is to compare the usability of several gaze control methods for video playback in case of healthy and disabled users. This paper aims to our gaze control applications. Our next step will be conducting the accessibility and user experience (UX) tests for both healthy and disabled users. The long-time perspective of this research could lead to the implementation of gaze control in TV sets and other video playback devices.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13389
        },
        {
          "affiliations": [],
          "personId": 16153
        },
        {
          "affiliations": [],
          "personId": 8997
        },
        {
          "affiliations": [],
          "personId": 18734
        },
        {
          "affiliations": [],
          "personId": 9757
        },
        {
          "affiliations": [],
          "personId": 15556
        },
        {
          "affiliations": [],
          "personId": 16358
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 3939,
      "typeId": 10286,
      "title": "Audio-visual interaction in emotion perception for communication: doctoral symposium, extended abstract",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Information from multiple modalities contributes to recognizing emotions. While it is known interactions occur between modalities, it is unclear what characterizes these. These interactions, and changes in these interactions due to sensory impairments, are the main subject of this PhD project.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17166
        },
        {
          "affiliations": [],
          "personId": 9129
        },
        {
          "affiliations": [],
          "personId": 11682
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 5220,
      "typeId": 10928,
      "title": "Training facilitates cognitive control on pupil dilation",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Physiological responses are generally involuntary; however, real-time feedback enables, at least to a certain extent, to voluntary control automatic processes. Recently, it was demonstrated that even pupil dilation is subject to controlled interference. To address effects of training on the ability to exercise control on pupil dilation, the current study examines repeated exercise over seven successive days. Participants utilize self-induced changes in arousal to increase pupil diameter, real-time feedback was applied to evaluate and improve individual performance. We observe inter-individual differences with regard to responsiveness of the pupillary response: six of eight participants considerably increase pupil diameter already during the first session, two exhibit only slight changes, and all showed rather stable performance throughout training. There was a trend towards stronger peak amplitudes that tend to occur increasingly early across time. Hence, higher cognitive control on pupil dilations can be practiced by most users and may therefore provide an appropriate input mechanism in human-computer interaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8719
        },
        {
          "affiliations": [],
          "personId": 24166
        },
        {
          "affiliations": [],
          "personId": 21398
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 8042,
      "typeId": 10928,
      "title": "Suitability of calibration polynomials for eye-tracking data with simulated fixation inaccuracies",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Current video-based eye trackers are not suited for calibration of patients who cannot produce stable and accurate  xations. Reli- able calibration is crucial in order to make repeatable recordings, which in turn are important to accurately measure the e ects of a medical intervention. To test the suitability of di erent calibration polynomials for such patients, inaccurate calibration data were sim- ulated using a geometric model of the EyeLink 1000 Plus desktop mode setup. This model is used to map eye position features to screen coordinates, creating screen data with known eye tracker data. This allows for objective evaluation of gaze estimation per- formance over the entire computer screen. Results show that the choice of calibration polynomial is crucial in order to ensure a high repeatability across measurements from patients who are hard to calibrate. Higher order calibration polynomials resulted in poor gaze estimation even for small simulated  xation inaccuracies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11298
        },
        {
          "affiliations": [],
          "personId": 12096
        },
        {
          "affiliations": [],
          "personId": 20800
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4716,
      "typeId": 10928,
      "title": "GazeCode: open-source software for manual mapping of mobile eye-tracking data",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Purpose: Eye movements recorded with mobile eye trackers generally have to be mapped to the visual stimulus manually. Manufacturer software usually has sub-optimal user interfaces. Here, we compare our in-house developed open-source alternative to the manufacturer software, called GazeCode. Method: 330 seconds of eye movements were recorded with the Tobii Pro Glasses 2. Eight coders subsequently categorized fixations using both Tobii Pro Lab and GazeCode. Results: Average manual mapping speed was more than two times faster when using GazeCode (0.649 events/s) compared with Tobii Pro Lab (0.292 events/s). Inter-rater reliability (Cohen’s Kappa) was similar and satisfactory; 0.886 for Tobii Pro Lab and 0.871 for GazeCode. Conclusion: GazeCode is a faster alternative to Tobii Pro Lab for mapping eye movements to the visual stimulus. Moreover, it accepts eye-tracking data from manufacturers SMI, Positive Science, Tobii, and Pupil Labs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23722
        },
        {
          "affiliations": [],
          "personId": 13520
        },
        {
          "affiliations": [],
          "personId": 17047
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 6764,
      "typeId": 10953,
      "title": "Deep Learning vs. Manual Annotation of Eye Movements",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "Deep Learning models have revolutionized many research fields already. However, the raw eye movement data is still typically processed into discrete events via threshold-based algorithms or manual labelling. In this work, we describe a compact 1D CNN model, which we combined with BLSTM to achieve end-to-end sequence-to-sequence learning. We discuss the acquisition process for the ground truth that we use, as well as the performance of our approach, in comparison to various literature models and manual raters. Our deep method demonstrates superior performance, which brings us closer to human-level labelling quality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19202
        },
        {
          "affiliations": [],
          "personId": 23215
        },
        {
          "affiliations": [],
          "personId": 18184
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 7533,
      "typeId": 10928,
      "title": "Binocular model-based gaze estimation with a camera and a single infrared light source",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a binocular model-based method that only uses a single camera and an infrared light source. Most gaze estimation approaches are based on single eye models and with binocular models they are addressed by averaging the results from each eye. In this work, we propose a geometric model of both eyes for gaze estimation. The proposed model is implemented and evaluated in a simulated environment and is compared to a binocular model-based method and polynomial regression-based method with one camera and two infrared lights that average the results from both eyes. The method performs on par with methods using multiple light sources while maintaining robustness to head movements. The study shows that when using both eyes in gaze estimation models it is possible to reduce the hardware requirements while maintaining robustness.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16250
        },
        {
          "affiliations": [],
          "personId": 21082
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7537,
      "typeId": 10284,
      "title": "Spatial Cognition in the Wild - Methods for Large-Scale Behavioural Research in Visuo-Locomotive Perception",
      "trackId": 10019,
      "tags": [],
      "keywords": [],
      "abstract": "The tutorial on Spatial Cognition in the Wild presents an interdisciplinary perspective on conducting evidence-based human behaviour research from the viewpoints of spatial cognition and computation, environmental psychology, and visual perception. The tutorial emphasises the semantic interpretation of multimodal behavioural data, and the (empirically-driven) synthesis of embodied interactive experiences in real world settings. Of special focus are: visual (e.g., perception, attention based on eye-tracking), visuolocomotive (e.g., movement, indoor wayfinding), and visuo-auditory (e.g., moving images) cognitive experiences in the context of areas such as architecture & built environment design, narrative media design, product design, cognitive media studies (e.g., film, animation, immersive reality). The technical focus of the tutorial is on demonstrating general computational methods, tools, and cognitive assistive technologies that can be used for multi-modal human behaviour studies in visual, visuo-locomotive, and visuo-auditory perception. Presented methods are rooted in foundational research in artificial intelligence, spatial informatics, and human-computer interaction. The tutorial utilises case-studies from large-scale experiments in domains such as evidence-based architecture design, communication and media studies, and cognitive film studies to demonstrate the application of the foundational practical methods and tools.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9176
        },
        {
          "affiliations": [],
          "personId": 20514
        }
      ],
      "sessionIds": [
        2472
      ],
      "eventIds": []
    },
    {
      "id": 6259,
      "typeId": 10286,
      "title": "Eye-tracking measures in audiovisual stimuli in infants at high genetic risk for ASD: challenging issues",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Individuals with autism spectrum disorder (ASD) have shown difficulties to integrate auditory and visual sensory modalities. Here we aim to explore if very young infants at genetic risk of ASD show atypicalities in this ability early in development. We registered visual attention of 4-month-old infants in a task using audiovisual natural stimuli (speaking faces). The complexity of this information and the attentional features of this population, among others, involves a great amount of challenges regarding data quality obtained with an eye-tracker. Here we discuss some of them and draw possible solutions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20021
        },
        {
          "affiliations": [],
          "personId": 10817
        },
        {
          "affiliations": [],
          "personId": 8982
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 8053,
      "typeId": 10048,
      "title": "Gaze-based interest detection on newspaper articles",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking measures have been used to recognize cognitive states involving mental workload, comprehension, and self-confidence in the task of reading. In this paper, we present how these measures can be used to detect the interest of a reader. From the reading behavior of 13 university students on 18 newspaper articles, we have extracted features related to fixations, saccades, blinks and pupil diameters to detect which documents each participant finds interesting or uninteresting. We have classified their level of interests into four classes with an accuracy of 44% using eye movements, and it has increased to 62% if a survey about subjective comprehension is included. This research can be incorporated in the real-time prediction of a user's interest while reading, for the betterment of future designs of human-document interaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8560
        },
        {
          "affiliations": [],
          "personId": 18274
        },
        {
          "affiliations": [],
          "personId": 15254
        },
        {
          "affiliations": [],
          "personId": 24348
        }
      ],
      "sessionIds": [
        1958
      ],
      "eventIds": []
    },
    {
      "id": 3703,
      "typeId": 10048,
      "title": "Enabling unsupervised eye tracker calibration by school children through games",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "To use eye trackers in a school classroom, children need to be able to calibrate their own tracker unsupervised and on repeated occasions. A game designed specifically around the need to maintain their gaze in fixed locations was used to collect calibration and verification data. The data quality obtained was compared with a standard calibration procedure and another game, in two studies carried out in three elementary schools. One studied the effect on data quality over repeated occasions and the other studied the effect of age on data quality. The first showed that accuracy obtained from unsupervised calibration by children was twice as good after six occasions with the game requiring the fixed gaze location compared with the standard calibration, and as good as standard calibration by group of supervised adults. In the second study, age was found to have no effect on performance in the groups of children studied.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9488
        },
        {
          "affiliations": [],
          "personId": 13443
        },
        {
          "affiliations": [],
          "personId": 10441
        },
        {
          "affiliations": [],
          "personId": 18522
        },
        {
          "affiliations": [],
          "personId": 18591
        }
      ],
      "sessionIds": [
        1178
      ],
      "eventIds": []
    },
    {
      "id": 7544,
      "typeId": 10048,
      "title": "Revisiting Data Normalization for Appearance-Based Gaze Estimation",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Appearance-based gaze estimation is promising for unconstrained real-world settings, but the significant variability in head pose and user-camera distance poses significant challenges for training generic gaze estimators. Data normalization was proposed to cancel out this geometric variability by mapping input images and gaze labels to a normalized space. Although used successfully in prior works, the role and importance of data normalization remains unclear. To fill this gap, we study data normalization for the first time using principled evaluations on both simulated and real data. We propose a modification to the current data normalization formulation by removing the scaling factor and show that our new formulation performs significantly better (between 9.5% and 32.7%) in the different evaluation settings. Using images synthesized from a 3D face model, we demonstrate the benefit of data normalization for the efficiency of the model training. Experiments on real-world images confirm the advantages of data normalization in terms of gaze estimation performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18059
        },
        {
          "affiliations": [],
          "personId": 13110
        },
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1953
      ],
      "eventIds": []
    },
    {
      "id": 3193,
      "typeId": 10286,
      "title": "Using eye tracking to simplify screening for visual field defects and improve vision rehabilitation",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "My thesis will encompass two main research objectives: (1) Evaluation of eye tracking as a method to screen for visual field defects. (2) Investigating how vision rehabilitation therapy can be improved by employing eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17311
        },
        {
          "affiliations": [],
          "personId": 12882
        },
        {
          "affiliations": [],
          "personId": 9697
        },
        {
          "affiliations": [],
          "personId": 11682
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 6782,
      "typeId": 10928,
      "title": "Predicting observer's task from eye movement patterns during motion image analysis",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Predicting an observer's tasks from eye movements during several viewing tasks has been investigated by several authors.\nThis contribution adds task prediction from eye movements tasks occurring during motion image analysis: \nExplore, Observe, Search, and Track. For this purpose, gaze data was recorded from 30 human observers viewing a motion image sequence once under each task. For task decoding, the classification methods Random Forest, LDA, and QDA were used; features were fixation- or saccade-related measures. Best accuracy for prediction of the three tasks Observe, Search, Track from the 4-minute gaze data samples was 83.7%\n(chance level 33%) using Random Forest. Best accuracy for prediction of all four tasks from the gaze data samples containing the first 30 seconds of viewing was 59.3% (chance level 25%) using LDA. Accuracy decreased significantly for task prediction on small gaze data chunks of 5 and 3 seconds, being 45.3% and 38.0% (chance 25%) for the four tasks, and 52.3% and 47.7% (chance 33%) for the three tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15422
        },
        {
          "affiliations": [],
          "personId": 17943
        },
        {
          "affiliations": [],
          "personId": 15762
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4227,
      "typeId": 10286,
      "title": "Automatic detection and inhibition of neutral and emotional stimuli in post-traumatic stress disorder: an eye-tracking study: eye-tracking data of an original antisaccade task",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "This research project addresses the understanding of attentional biases post-traumatic stress disorder (PTSD). This psychiatric condition is mainly characterized by symptoms of intrusion (flashbacks), avoidance, alteration of arousal and reactivity (hypervigilance), and negative mood and cognitions persisting one month after the exposure of a traumatic event [American Psychiatric Association 2013]. Clinical observations as well as empirical research highlighted the symptom of hypervigilance as being central in the PTSD symptomatology, considering that other clinical features could be maintained by it [Ehlers and Clark 2000]. Attentional Control theory has described the hypervigilance in anxious disorders as the co-occurrence of two cognitive processes : an enhanced detection of threatening information followed by difficulties to inhibit their processing [Eysenck et al. 2007]. Nevertheless, attentional control theory has never been applied to PTSD. This project aims at providing cognitive evidence of hypervigilance symptoms in PTSD using eye-tracking during the realization of reliable Miyake tasks [Eysenck and Derakshan 2011]. Therefore, our first aim is to model the co-occurring processes of hypervigilance using eye-tracking technology. Indeed, behavioral measures (as reaction time) do not allow a clear representation of cognitive processes occurring subconsciously in a few milliseconds [Felmingham 2016]. Therefore, eye-tracking technology is essential in our studies. Secondly, we aim to analyze the differential impact of trauma-related stimulus vs negative stimuli on PTSD patients, by conducting scan paths following both of those stimuli presentation. This research project is divided into four studies. The first one will be described is this doctoral symposium.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15837
        },
        {
          "affiliations": [],
          "personId": 21535
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 5508,
      "typeId": 10928,
      "title": "Image-Based Scanpath Comparison with Slit-Scan Visualization",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "The comparison of scanpaths between multiple participants is an important analysis task in eye tracking research. Established methods typically inspect recorded gaze sequences based on geometrical trajectory properties or strings derived from annotated areas of interest (AOIs). We propose a new approach based on image similarities of gaze-guided slit-scans: For each time step, a vertical slice is extracted from the stimulus at the gaze position. Placing the slices next to each other over time creates a compact representation of a scanpath in the context of the stimulus. These visual representations can be compared based on their image similarity, providing a new measure for scanpath comparison without the need for annotation. We demonstrate how comparative slit-scan visualization can be integrated into a visual analytics approach to support the interpretation of scanpath similarities in general.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19414
        },
        {
          "affiliations": [],
          "personId": 21610
        },
        {
          "affiliations": [],
          "personId": 18716
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4997,
      "typeId": 10928,
      "title": "AutoPager: Exploiting Change Blindness for Gaze-Assisted Reading",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "A novel gaze-assisted reading technique uses the fact that in linear reading, the looking behavior of the reader is readily predicted. We introduce the AutoPager \"page turning\" technique, where the next bit of unread text is rendered in the periphery, ready to be read. This approach enables continuous gaze-assisted reading without requiring manual input to scroll: the reader merely saccades to the top of the page to read on. We demonstrate that when the new text is introduced with a gradual cross-fade effect, users are often unaware of the change: the user's impression is of reading the same page over and over again, yet the content changes. We present a user evaluation that compares AutoPager to previous gaze-assisted scrolling techniques. AutoPager may offer some advantages over previous gaze-assisted reading techniques, and is a rare example of exploiting \"change blindness\" in user interfaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21847
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 6535,
      "typeId": 10048,
      "title": "A system to determine if learners know the divisibility rules and apply them correctly",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Mathematics teachers may find it challenging to manage the learning that takes place in learners’ minds. Typical true/false or multiple choice assessments, whether in oral, written or electronic format, do not provide evidence that learners applied the correct principles. \nA system was developed to analyse learners’ gaze behaviour while they were determining whether a multi-digit dividend is divisible by a divisor. The system provides facilities for a teacher to set up tests and generate various types of quantitative and qualitative reports. \nThe system was tested with a group of 16 learners from Grade 7 to Grade 10 in a pre-post experiment to investigate the effect of revision on their performance. It was proven that, with tests that are carefully compiled according to a set of heuristics, eye tracking can be used to determine whether learners use the correct strategy when applying divisibility rules.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18039
        },
        {
          "affiliations": [],
          "personId": 22379
        }
      ],
      "sessionIds": [
        1082
      ],
      "eventIds": []
    },
    {
      "id": 4488,
      "typeId": 10048,
      "title": "The influence of anxiety on visual entropy of experienced drivers",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "This study tested the use of entropy to identify changes on behavior of drivers under pressure. Sixteen experienced drivers drove in a simulator wearing a head-mounted eye tracker under low-and high-anxiety conditions. Anxiety was induced by manipulating some psychological factors such as peer-pressure. Fixations transitions between AOIs (lane, speedometer and mirrors) were calculated through first-order transition matrix, transformed to Markov probability matrix and adjusted into the entropy equation. Drivers showed greater state-anxiety scores and higher mean heart rates following manipulation. Under anxiety, drivers showed higher visual entropy, indicating a more random scanning. The randomness implies into a poorer acquisition of information and may indicate an impaired top-down control of attention provoked by anxiety.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22822
        },
        {
          "affiliations": [],
          "personId": 16435
        },
        {
          "affiliations": [],
          "personId": 21412
        },
        {
          "affiliations": [],
          "personId": 15172
        },
        {
          "affiliations": [],
          "personId": 18997
        },
        {
          "affiliations": [],
          "personId": 10461
        },
        {
          "affiliations": [],
          "personId": 19267
        },
        {
          "affiliations": [],
          "personId": 8587
        },
        {
          "affiliations": [],
          "personId": 11304
        }
      ],
      "sessionIds": [
        1345
      ],
      "eventIds": []
    },
    {
      "id": 7052,
      "typeId": 10779,
      "title": "Eye movements and viewer's impressions in response to HMD-evoked head movements",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "The relationships between eye and head movements during the viewing of various visual stimuli using a head mounted display (HMD) and a large flat display were compared. The visual sizes of the images displayed were adjusted virtually, using an image processing technique. The negative correlations between head and eye movements in a horizontal direction were significant for some visual stimuli using an HMD and after some practice viewing images. Also, scores of two factors for subjective assessment of viewing stimuli positively correlated with horizontal head movements when certain visual stimuli were used. The result suggests that under certain conditions the viewing of tasks promotes head movements and stimulates correlational relationships between eye and head movements.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16597
        },
        {
          "affiliations": [],
          "personId": 14235
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 3981,
      "typeId": 10928,
      "title": "Robustness of metrics used for scanpath comparison",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "In every quantitative eye tracking research study, researchers need to compare eye movements between subjects or conditions. For both static and dynamic tasks, there is a variety of metrics that could serve this purpose. It is important to explore the robustness of the metrics with respect to artificial noise. For dynamic tasks, where eye movement data is represented as scanpaths, there are currently no studies regarding the robustness of the metrics.\nIn this study, we explored properties of five metrics (Levenshtein distance, correlation distance, Fréchet distance, mean and median distance) used for comparison of scanpaths. We systematically added noise by applying three transformations to the scanpaths: translation, rotation, and scaling. For each metric, we computed baseline similarity for two random scanpaths and explored the metrics' sensitivity. Our results allow other researchers to convert results between studies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11612
        },
        {
          "affiliations": [],
          "personId": 24199
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 5519,
      "typeId": 10048,
      "title": "I See What You See: Gaze Awareness in Mobile Remote Collaboration",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "An emerging use of mobile video telephony is to enable joint activities and collaboration on physical tasks. We conducted a controlled user study to understand if seeing the gaze of a remote instructor is benecial for mobile video collaboration and if it is valuable that the instructor is aware of the sharing of the gaze. We compared three gaze sharing congurations, (a) Gaze_Visible where the instructor is aware and can view own gaze point that is being shared, (b) Gaze_Invisible where the instructor is aware of the shared gaze, but cannot view her own gaze point and (c) Gaze_Unaware where the instructor is unaware about the gaze sharing, with a baseline of shared-mouse pointer. Our results suggests that naturally occurring gaze may not be as useful as explicitly produced eye movements. Further, instructors prefer using mouse rather than gaze for remote gesturing, while the workers also find value in transferring the gaze information.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15915
        },
        {
          "affiliations": [],
          "personId": 22905
        },
        {
          "affiliations": [],
          "personId": 10589
        }
      ],
      "sessionIds": [
        1178
      ],
      "eventIds": []
    },
    {
      "id": 3984,
      "typeId": 10928,
      "title": "Evaluating Similarity Measures for Gaze Patterns in the Context of Representational Competence in Physics Education",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "The competent handling of representations is required for understanding physics' concepts, developing problem-solving skills, and achieving scientific expertise. Using eye-tracking methodology, we present the contributions of this paper as follows: We first investigated the preferences of students with the different levels of knowledge; experts, intermediates, and novices, in representational competence in the domain of physics problem-solving. It reveals that experts more likely prefer to use vector than other representations. Besides, a similar tendency of table representation usage was observed in all groups.Also, diagram representation has been used less than others. Secondly, we evaluated three similarity measures; Levenshtein distance, transition entropy, and Jensen-Shannon divergence. Conducting Recursive Feature Elimination technique suggests Jensen-Shannon divergence is the best discriminating feature among the three. However, investigation on mutual dependency of the features implies transition entropy mutually links between two other features where it has mutual information with Levenshtein distance (Maximal Information Coefficient  = 0.44) and has a correlation with Jensen-Shannon divergence (r(18313)=0.70, p<.001).",
      "authors": [
        {
          "affiliations": [],
          "personId": 23514
        },
        {
          "affiliations": [],
          "personId": 13696
        },
        {
          "affiliations": [],
          "personId": 21708
        },
        {
          "affiliations": [],
          "personId": 20577
        },
        {
          "affiliations": [],
          "personId": 12941
        },
        {
          "affiliations": [],
          "personId": 24348
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7318,
      "typeId": 10048,
      "title": "Circular Orbits Detection for Gaze Interaction Using 2D Correlation and Profile Matching Algorithms",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, interaction techniques in which the user selects screen targets by matching their movement with the input device have been gaining popularity, particularly in the context of gaze interaction (e.g. Pursuits, Orbits, AmbiGaze, etc.). However, though many algorithms for enabling such interaction techniques have been proposed, we still lack an understanding of how they compare to each other. In this paper, we introduce two new algorithms for matching eye movements: Profile Matching and 2D Correlation, and present a systematic comparison of these algorithms with two other state-of-the-art algorithms: the Basic Correlation algorithm used in Pursuits and the Rotated Correlation algorithm used in PathSync. We also examine the effects of two thresholding techniques and post-hoc filtering. We evaluated the algorithms on a user dataset and found the 2D Correlation with one-level thresholding and post-hoc filtering to be the best performing algorithm.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12167
        },
        {
          "affiliations": [],
          "personId": 18639
        },
        {
          "affiliations": [],
          "personId": 23321
        },
        {
          "affiliations": [],
          "personId": 8239
        }
      ],
      "sessionIds": [
        2498
      ],
      "eventIds": []
    },
    {
      "id": 7063,
      "typeId": 10284,
      "title": "Eye movement metrics: event detection",
      "trackId": 10019,
      "tags": [],
      "keywords": [],
      "abstract": "Event detection, is an end point critical to eye movement applications. Be it gaze interaction or passive attention analysis, software needs to be aware of the user's instantaneous state. Typically, eye movement metrics such as fixation duration (dwell time) modulates a behavioral event; i.e. actuate a button press or similar response that indicate a decision has been made.\n\nI will describe numerous metrics, how they might be derived, and then used to score various behavioral events. Included in the tutorial will be a discussion on the significance of spatial calibration, and techniques to accommodate lack thereof. Also the significance of temporal frequency, i.e. sample rate to derive certain eye metrics and how interaction with temporal artifact filters can delay event detection and impede system performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16040
        }
      ],
      "sessionIds": [
        1933
      ],
      "eventIds": []
    },
    {
      "id": 3738,
      "typeId": 10928,
      "title": "CBF:Circular binary features for robust and real-time pupil center detection",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Modern eye tracking systems rely on fast and robust pupil detection,\nand several algorithms have been proposed for eye tracking under\nreal world conditions. In this work, we propose a novel binary feature\nselection approach that is trained by computing conditional\ndistributions. These features are scalable and rotatable, allowing\nfor distinct image resolutions, and consist of simple intensity comparisons,\nmaking the approach robust to different illumination conditions\nas well as rapid illumination changes. The proposed method\nwas evaluated on multiple publicly available data sets, considerably\noutperforming state-of-the-art methods, and being real-time capable\nfor very high frame rates. Moreover, our method is designed to\nbe able to sustain pupil center estimation even when typical edgedetection-\nbased approaches fail – e.g., when the pupil outline is not\nvisible due to occlusions from reflections or eye lids / lashes. As a\nconsequece, it does not attempt to provide an estimate for the pupil\noutline. Nevertheless, the pupil center suffices for gaze estimation –\ne.g., by regressing the relationship between pupil center and gaze\npoint during calibration.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12726
        },
        {
          "affiliations": [],
          "personId": 8373
        },
        {
          "affiliations": [],
          "personId": 13130
        },
        {
          "affiliations": [],
          "personId": 9858
        },
        {
          "affiliations": [],
          "personId": 23370
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1953,
        1301
      ],
      "eventIds": []
    },
    {
      "id": 2972,
      "typeId": 10928,
      "title": "AnyOrbit: Eye-tracking for orbital navigation in virtual environments",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze-based interactions promise to be fast, intuitive and effective in controlling virtual and augmented environments. Yet, there is still a lack of\nusable 3D navigation and observation techniques.  In this work: 1) We introduce a highly advantageous orbital navigation technique, AnyOrbit, providing an intuitive and hands-free method of observation in virtual environments that uses eye-tracking to control the orbital center of movement; 2) The versatility of the technique is demonstrated with several control schemes and use-cases in virtual/augmented reality head-mounted-display and desktop setups, including observation of 3D astronomical data and spectator sports.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17252
        },
        {
          "affiliations": [],
          "personId": 23510
        },
        {
          "affiliations": [],
          "personId": 13998
        },
        {
          "affiliations": [],
          "personId": 9990
        },
        {
          "affiliations": [],
          "personId": 22250
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 4252,
      "typeId": 10928,
      "title": "A Visual Comparison of Gaze Behavior from Pedestrians and Cyclists",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we contribute an eye tracking study conducted with pedestrians and cyclists. We apply a visual analytics-based method to inspect pedestrians' and cyclists' gaze behavior as well as video recordings and accelerometer data. This method using multi-modal data allows us to explore patterns and extract common eye movement strategies. Our results are that participants paid most attention to the path itself; advertisements do not distract participants; participants focus more on pedestrians than on cyclists; pedestrians perform more shoulder checks than cyclists do; and we extracted common gaze sequences. Such an experiment in a real-world traffic environment allows us to understand realistic behavior of pedestrians and cyclists better.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17643
        },
        {
          "affiliations": [],
          "personId": 18463
        },
        {
          "affiliations": [],
          "personId": 12725
        },
        {
          "affiliations": [],
          "personId": 20528
        },
        {
          "affiliations": [],
          "personId": 15061
        }
      ],
      "sessionIds": [
        1301,
        1178
      ],
      "eventIds": []
    },
    {
      "id": 6301,
      "typeId": 10286,
      "title": "Training operational monitoring in future ATCOs using eye tracking.",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Improved technological possibilities continue to increase the significance of operational monitoring in air traffic control (ATC). The role of the air traffic controller (ATCO) will change in that they will have to monitor the operations of an automated system for failures. In order to take over control when automation fails, future ATCOs will need to be trained. While current ATC training is mainly based on performance indicators, this study will focus on the benefit of using eye tracking in future ATC training. Utilizing a low-fidelity operational monitoring task, a model of how attention should be allocated in case of malfunction will be derived. Based on this model, one group of ATC novices will receive training on how to allocate their attention appropriately (treatment). The other group will receive no training (control). Eye movements will be recorded to investigate how attention is allocated and if the training is successful. Performance measures will be used to evaluate the effectiveness of the training.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22076
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 2975,
      "typeId": 10048,
      "title": "Gaze and head pointing for hands-free text entry: Applicability to ultra-small virtual keyboards",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "With the proliferation of small-screen computing devices, there has been a continuous trend in reducing the size of interface elements. In virtual keyboards, this allows for more characters in a layout and additional function widgets. However, vision-based interfaces (VBIs) have only been investigated with large (e.g., full-screen) keyboards. To understand how key size reduction affects the accuracy and speed performance of text entry VBIs, we evaluated gaze-controlled VBI (g-VBI) and head-controlled VBI (h-VBI) with unconventionally small (0.4°, 0.6°, 0.8° and 1°) keys. Novices (N = 26) yielded significantly more accurate and fast text production with h-VBI than with g-VBI, while the performance of experts (N = 12) for both VBIs was nearly equal when a 0.8-1° key size was used. We discuss advantages and limitations of the VBIs for typing with ultra-small keyboards and emphasize relevant factors for designing such systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17097
        },
        {
          "affiliations": [],
          "personId": 17971
        },
        {
          "affiliations": [],
          "personId": 17631
        },
        {
          "affiliations": [],
          "personId": 23128
        },
        {
          "affiliations": [],
          "personId": 12141
        }
      ],
      "sessionIds": [
        2188
      ],
      "eventIds": []
    },
    {
      "id": 7072,
      "typeId": 10048,
      "title": "Data flow metrics in program comprehension tasks",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Existing research in program comprehension has paid less attention to the coverage of programming concepts that were contained within the source codes used for studies. In this paper, we examine source codes covering four introductory programming concepts: branching, loops and arrays, sorting, and tail recursion. The diverse types of code fragments give rise to eye movement patterns more structured according to the control flow and data flow of the program. To facilitate analysis of this class of program comprehension strategies, we propose data flow-based metrics and describe automatic computation of the metrics. In evaluation of the proposal, we conducted a pilot study with novice and intermediate programmers. In the study with recordings from 26 programmers we compute basic fixation and saccade metrics along with a data flow-based metric.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11254
        },
        {
          "affiliations": [],
          "personId": 20060
        },
        {
          "affiliations": [],
          "personId": 14476
        },
        {
          "affiliations": [],
          "personId": 16654
        }
      ],
      "sessionIds": [
        2097
      ],
      "eventIds": []
    },
    {
      "id": 4001,
      "typeId": 10048,
      "title": "An investigation of the effects of n-gram length in scanpath analysis for eye-tracking research",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Scanpath analysis is a controversial and important topic in eye tracking research. Previous work has shown the value of scanpath analysis in perceptual tasks; little research has examined its utility for understanding human reasoning in complex tasks. Here, we analyze n-grams, which are continuous ordered subsequences of participants’ scanpaths. In particular we studied the length of n-grams that are most appropriate for this form of analysis. We re-use datasets from previous studies of human cognition, medical diagnosis and art, systematically analyzing the frequency of n-grams of increasing length, and compare this approach with a string alignment-based method. The results show that subsequences of four or more areas of interest may not be of value for finding patterns that distinguish between two groups. The study is the first to systematically define the parameters of the length of n-gram suitable for analysis, using an approach that holds across diverse domains.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20286
        },
        {
          "affiliations": [],
          "personId": 15165
        },
        {
          "affiliations": [],
          "personId": 17271
        }
      ],
      "sessionIds": [
        1082
      ],
      "eventIds": []
    },
    {
      "id": 4771,
      "typeId": 10048,
      "title": "Region of interest generation algorithms for eye tracking data",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Using human fixation behavior, we can interfere regions that require to be processed at high resolution and where stronger compression can be favored. Analyzing the visual scan path solely based on a predefined set of regions of interest (ROIs) limits the exploration room of the analysis. Insights can only be gained for those regions that the data analyst considered worthy of labeling. Furthermore, visual exploration is naturally time-dependent: A short initial overview phase may be followed by an in-depth analysis of regions that attracted the most attention. Therefore, the shape and size of regions of interest may change over time. Automatic ROI generation can help in automatically reshaping the ROIs to the data of a time slice. We developed three novel methods for automatic ROI generation and show their applicability to different eye tracking data sets. The methods are publicly available as part of the EyeTrace software at http://www.ti.uni-tuebingen.de/Eyetrace.175L0.html",
      "authors": [
        {
          "affiliations": [],
          "personId": 12726
        },
        {
          "affiliations": [],
          "personId": 20100
        },
        {
          "affiliations": [],
          "personId": 24370
        },
        {
          "affiliations": [],
          "personId": 18363
        },
        {
          "affiliations": [],
          "personId": 23370
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1496
      ],
      "eventIds": []
    },
    {
      "id": 7332,
      "typeId": 10284,
      "title": "Eyetracking in Virtual and Augmented Reality",
      "trackId": 10019,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual and augmented reality (VR/AR) are highly attractive application areas for eye tracking. Virtual environments have the potential to revolutionize life-sized interactive experimentation under highly-controlled conditions. VR simulations are thus attractive for many fields of research, such as marketing research, human-computer interaction, robotics or psychology. As virtual and augmented reality today primarily address the visual modality, they are well suited for gaze-based interaction, either for direct control or to realize attention aware interactions.\n\nThe tutorial provides hands-on experiences with several hardware combinations of VR/AR (e.g. HTC Vive, Microsoft HoloLens) and eye tracking systems (e.g. Pupil, Tobii). In the first half of the tutorial (morning session), an introduction about eye tracking in VR/AR, its potential applications and current solutions will be given. Participants will also get familiar with basic setups of an eye tracking project in Unity3D. In the afternoon session, the focus will be on how gaze can be mapped to 3D objects, how 3D attention can be analyzed and, how gaze can be used to interact with the virtual world. Finally, there will be some hands-on activities where participants can experiment with eyetracking VR/AR setups and do some example projects in Unity3D on both gaze interaction and attention analysis.\n\nAs software framework, Unity3D will be used to implement the virtual environments. Basic knowledge of Unity3D is required to get the most out of the hands-on part. The example project and assets will be provided to all participants, which allows for an easy implementation of VR/AR projects with eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13675
        },
        {
          "affiliations": [],
          "personId": 22105
        }
      ],
      "sessionIds": [
        2215
      ],
      "eventIds": []
    },
    {
      "id": 3492,
      "typeId": 10801,
      "title": "Context Switching Eye Typing Using Dynamic Expanding Targets",
      "trackId": 10024,
      "tags": [],
      "keywords": [],
      "abstract": "Text entry by gazing on a virtual keyboard (also known as eye typing) is an important component of any gaze communication system. One of the main challenges for efficient communication is how to avoid unintended key selections due to the Midas' touch problem. The most common selection technique by gaze is dwelling. Though easy to learn, long dwell-times slows down the communication, and short dwells are prone to error. Context switching (CS) is a faster and more comfortable alternative, but the duplication of contexts takes a lot of screen space. In this paper we introduce two new CS designs using dynamic expanding targets that are more appropriate when a reduced interaction window is required. We compare the performance of the two new designs with the original CS design using QWERTY layouts as contexts. Our results with 6 participants typing with the 3 keyboards show that the use of smaller size layouts with dynamic expanding targets are as accurate and comfortable as the larger QWERTY layout, though providing lower typing speeds.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8239
        },
        {
          "affiliations": [],
          "personId": 14212
        },
        {
          "affiliations": [],
          "personId": 12120
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 5285,
      "typeId": 10048,
      "title": "Robust Eye Contact Detection in Natural Multi-Person Interactions Using Gaze and Speaking Behaviour",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Eye contact is one of the most important non-verbal social cues and fundamental to human interactions. However, detecting eye contact without specialised eye tracking equipment poses significant challenges, particularly for multiple people in real-world settings. We present a novel method to robustly detect eye contact in natural three- and four-person interactions using off-the-shelf ambient cameras. Our method exploits that, during conversations, people tend to look at the person who is currently speaking. Harnessing the correlation between people's gaze and speaking behaviour therefore allows our method to automatically acquire training data during deployment and adaptively train eye contact detectors for each target user. We empirically evaluate the performance of our method on a recent dataset of natural group interactions and demonstrate that it achieves a relative improvement over the state-of-the-art method of more than 60%, and also improves over a head pose based baseline.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16059
        },
        {
          "affiliations": [],
          "personId": 16025
        },
        {
          "affiliations": [],
          "personId": 18059
        },
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1178
      ],
      "eventIds": []
    },
    {
      "id": 7077,
      "typeId": 10928,
      "title": "Systematic shifts of fixation disparity accompanying brightness changes",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Video-based gaze tracking is prone to brightness changes due to their effects on pupil size. Monocular observations indeed confirm variable fixation locations depending on brightness. In close viewing, pupil size is coupled with accommodation and vergence, the so-called near triad. Hence, systematic changes in fixation disparity might be expected to co-occur with varying pupil size. In the current experiment, fixation disparity was assessed. Calibration was conducted either on dark or on bright background, and text had to be read on both backgrounds, on a self-illuminating screen and on paper. When calibration background matches background during reading, mean fixation disparity did not differ from zero. In the non-calibrated conditions, however, a brighter stimulus went along with a dominance of crossed fixations and vice versa. The data demonstrate that systematic changes in fixation disparity occur as effect of brightness changes advising for careful setting calibration parameters.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21398
        }
      ],
      "sessionIds": [
        1301,
        1331
      ],
      "eventIds": []
    },
    {
      "id": 7845,
      "typeId": 10048,
      "title": "Capturing Real-World Gaze Behaviour: Live and Unplugged",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding human gaze behaviour has benefits from scientific understanding to many application domains. Current practices constrain possible use cases, requiring experimentation restricted to a lab setting or controlled environment. In this paper, we demonstrate a flexible unconstrained end-to-end solution that allows for collection and analysis of gaze data in real-world settings. To achieve these objectives, rich 3D models of the real world are derived along with strategies for associating experimental eye-tracking data with these models. In particular, we demonstrate the strength of photogrammetry in allowing these capabilities to be realized, and demonstrate the first complete solution for 3D gaze analysis in large-scale outdoor environments using standard camera technology without fiducial markers. The paper also presents techniques for quantitative analysis and visualization of 3D gaze data. As a whole, the body of techniques presented provides a foundation for future research, with new opportunities for experimental studies and computational modeling efforts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13839
        },
        {
          "affiliations": [],
          "personId": 24264
        },
        {
          "affiliations": [],
          "personId": 22131
        }
      ],
      "sessionIds": [
        1219
      ],
      "eventIds": []
    },
    {
      "id": 5544,
      "typeId": 10048,
      "title": "Introducing I2Head Database",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "I2Head database has been created with the aim to become an optimal reference for low cost gaze estimation. It exhibits the following outstanding characteristics: it takes into account key aspects of low resolution eye tracking technology; it combines images of users gazing at different grids of points from alternative positions with registers of user's head position and it provides calibration information of the camera and a simple 3D head model for each user. Hardware used to build the database includes a 6D magnetic sensor and a webcam. A careful calibration method between the sensor and the camera has been developed to guarantee the accuracy of the data. Different sessions have been recorded for each user including not only static head scenarios but also controlled displacements and even free head movements. The database is an outstanding framework to test both gaze estimation algorithms and head pose estimation methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17194
        },
        {
          "affiliations": [],
          "personId": 14339
        },
        {
          "affiliations": [],
          "personId": 15465
        },
        {
          "affiliations": [],
          "personId": 15852
        }
      ],
      "sessionIds": [
        2064
      ],
      "eventIds": []
    },
    {
      "id": 8107,
      "typeId": 10048,
      "title": "Leveraging Eye-gaze and Time-series Features to Predict User Interests and Build a Recommendation Model for Visual Analysis",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "We developed a new concept to improve the efficiency of visual analysis\nthrough visual recommendations. It uses a novel eye-gaze based\nrecommendation model that aids users in identifying interesting\ntime-series patterns. Our model combines time-series features and\neye-gaze interests, captured via an eye-tracker. Mouse selections are\nalso considered. The system provides an overlay visualization with\nrecommended patterns, and an eye-history graph, that supports\nthe users in the data exploration process. We conducted an experiment\nwith 5 tasks where 30 participants explored sensor data of a\nwind turbine. This work presents results on pre-attentive features,\nand discusses the precision/recall of our model in comparison to\nfinal selections made by users. Our model helps users to efficiently\nidentify interesting time-series patterns.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8633
        },
        {
          "affiliations": [],
          "personId": 17591
        },
        {
          "affiliations": [],
          "personId": 19534
        },
        {
          "affiliations": [],
          "personId": 18464
        },
        {
          "affiliations": [],
          "personId": 22113
        },
        {
          "affiliations": [],
          "personId": 18912
        }
      ],
      "sessionIds": [
        2188
      ],
      "eventIds": []
    },
    {
      "id": 6827,
      "typeId": 10743,
      "title": "From fixation to exploration: Towards an integrative view of oculomotor function",
      "trackId": 10022,
      "tags": [],
      "keywords": [],
      "abstract": "Vision depends on motion: we see things either because they move or because our eyes do. What may be more surprising is that large and miniature eye motions help us examine the world in similar ways - largely at the same time. In this presentation, I will discuss recent research from my lab and others suggesting that exploration and gaze-fixation are not all that different processes in the brain. Our eyes scan visual scenes with a same general strategy whether the images are huge or tiny, or even when we try to fix our gaze. These findings indicate that exploration and fixation are not fundamentally different behaviors, but rather two ends of the same visual scanning continuum. They also imply that the same brain systems control our eye movements when we explore and when we fixate - an insight that may ultimately offer clues to understanding both normal oculomotor function in the healthy brain, and oculomotor dysfunction in neurological disease.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21464
        }
      ],
      "sessionIds": [
        1756
      ],
      "eventIds": []
    },
    {
      "id": 5550,
      "typeId": 10286,
      "title": "Seeing in time: an investigation of entrainment and visual processing in toddlers",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Recent neurophysiological and behavioral studies have provided strong evidence of rhythmic entrainment in the perceptual level in adults. The present study examines if rhythmic auditory stimulation synchronized with visual stimuli and fast tempi could enhance the visual processing in toddlers. Two groups of participants with different musical experiences are recruited. A head-mounted camera will be used to investigate perceptual entrainment when participants perform visual search tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15008
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 6063,
      "typeId": 10048,
      "title": "Contour-Guided Gaze Gestures: Using Object Contours as Visual Guidance for Triggering Interactions",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "The eyes are an interesting modality for pervasive interactions, though their applicability for mobile scenarios is restricted by several issues so far. In this paper, we propose the idea of contour-guided gaze gestures, which overcome former constraints, like the need for calibration, by relying on unnatural and relative eye movements, as users trace the contours of objects in order to trigger an interaction. The interaction concept and the system design are described, along with two user studies, that demonstrate the method's applicability. It is shown that users were able to trace object contours to trigger actions from various positions on multiple different objects. It is further determined, that the proposed method is an easy to learn, hands-free interaction technique, that is robust against false positive activations. Results highlight low demand values and show that the method holds potential for further exploration, but also reveal areas for refinement.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8988
        },
        {
          "affiliations": [],
          "personId": 13787
        },
        {
          "affiliations": [],
          "personId": 14329
        }
      ],
      "sessionIds": [
        2498
      ],
      "eventIds": []
    },
    {
      "id": 3248,
      "typeId": 10970,
      "title": "A Gaze-Contingent Intention Decoding Engine for human augmentation",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Humans process high volumes of visual information to perform everyday tasks. In a reaching task, the brain estimates the distance and position of the object of interest, to reach for it. Having a grasp intention in mind, human eye-movements produce specific relevant patterns. Our Gaze-Contingent Intention Decoding Engine uses eye-movement data and gaze-point position to indicate the hidden intention. We detect the object of interest using deep convolution neural networks and estimate its position in a physical space using 3D gaze vectors. Then we trigger the possible actions from an action grammar database to perform an assistive movement of the robotic arm, improving action performance in physically disabled people. This document is a short report to accompany the Gaze-contingent Intention Decoding Engine demonstrator, providing details of the setup used and results obtained.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12454
        },
        {
          "affiliations": [],
          "personId": 21339
        },
        {
          "affiliations": [],
          "personId": 12270
        },
        {
          "affiliations": [],
          "personId": 18174
        },
        {
          "affiliations": [],
          "personId": 8592
        }
      ],
      "sessionIds": [
        1940
      ],
      "eventIds": []
    },
    {
      "id": 4530,
      "typeId": 10779,
      "title": "A Fitts' Law Study of Click and Dwell Interaction by Gaze, Head and Mouse with a Head-Mounted Display",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze and head tracking, or pointing, in head-mounted displays enables new input modalities for point-select tasks. We conducted a Fitts' law experiment with 41 subjects comparing head pointing and gaze pointing using a 300 ms dwell (n = 22) or click (n = 19) activation, with mouse input providing a baseline for both conditions. Gaze and head pointing were equally fast but slower than the mouse; dwell activation was faster than click activation. Throughput was highest for the mouse (2.75 bits/s), followed by head pointing (2.04 bits/s) and gaze pointing (1.85 bits/s). With dwell activation, however, throughput for gaze and head pointing were almost identical, as was the effective target width (≈ 55 pixels; about 2°) for all three input methods. Subjective feedback rated the physical workload less for gaze pointing than head pointing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19325
        },
        {
          "affiliations": [],
          "personId": 14886
        },
        {
          "affiliations": [],
          "personId": 13202
        },
        {
          "affiliations": [],
          "personId": 15334
        }
      ],
      "sessionIds": [
        1783
      ],
      "eventIds": []
    },
    {
      "id": 6834,
      "typeId": 10953,
      "title": "iTrace: Eye Tracking Infrastructure for Development Environments",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "The paper presents iTrace, an eye tracking infrastructure, that enables eye tracking in development environments such as Visual Studio and Eclipse. Software developers work with software that is comprised of numerous source code files. This requires frequent switching between project artifacts during program understanding or debugging activities. Additionally, the amount of content contained within each artifact can be quite large and require scrolling or navigation of the content. Current approaches to eye tracking are meant for fixed stimuli and struggle to capture context during these activities. iTrace overcomes these limitations allowing developers to work in realistic settings during an eye tracking study. The iTrace architecture is presented along with several use cases of where it can be used by researchers. A short video demonstration is available at https://youtu.be/AmrLWgw4OEs",
      "authors": [
        {
          "affiliations": [],
          "personId": 14940
        },
        {
          "affiliations": [],
          "personId": 21528
        },
        {
          "affiliations": [],
          "personId": 12831
        },
        {
          "affiliations": [],
          "personId": 10275
        },
        {
          "affiliations": [],
          "personId": 21825
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 7602,
      "typeId": 10928,
      "title": "Relating eye-tracking measures with changes in knowledge on search tasks",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "We conducted an eye-tracking study where 30 participants performed searches on the web. We measured their topical knowledge before and after each task. Their eye-fixations were labelled as  \"reading\" or \"scanning\". The series of reading fixations in a line, called \"reading-sequences\" were characterized by their length in pixels, fixation duration, and the number of fixations making up the sequence. We hypothesize that differences in knowledge-change of participants are reflected in their eye-tracking measures related to reading. Our results show that the participants with higher change in knowledge differ significantly in terms of their total reading-sequence-length,  reading-sequence-duration, and number of reading fixations, when compared to participants with lower knowledge-change.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20088
        },
        {
          "affiliations": [],
          "personId": 22633
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 6581,
      "typeId": 10928,
      "title": "Correlation Between Gaze and Hovers During Decision-Making Interaction",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Taps only consist of a small part of the manual input when interacting with touch-enabled surfaces. Indeed, how the hand behaves in the hovering space is informative of what the user intends to do. In this article, we present a data collection related to hand and eye motion. We tailored a kiosk-like system to record participants’ gaze and hand movements. We specifically designed a memory game to detect the decision-making process users may face. Our data collection comprises of 177 trials from 71 participants. Based on a hand movement classification, we extracted 16588 hovers. We study the gaze behaviour during hovers, and we found out that the distance between gaze and hand depends on the target’s location on the screen. We also showed how indecision can be deducted from this distance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16531
        },
        {
          "affiliations": [],
          "personId": 13804
        }
      ],
      "sessionIds": [
        1082,
        1301
      ],
      "eventIds": []
    },
    {
      "id": 2743,
      "typeId": 10928,
      "title": "Rapid Alternating Saccade Training",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "While individual eye movement characteristics are remarkably stable, experiments on saccadic spatial adaptation indicate that oculomotor learning is possible. To further investigate saccadic learning, participants received veridical feedback on saccade rate while making sequential saccades as quickly as possible between two horizontal targets. Over the course of five days, with just ten minutes of training per day, participants were able to significantly increase the rate of sequential saccades. This occurred through both a reduction in dwell duration and to changes in secondary saccade characteristics. There was no concomitant change in participant’s accuracy or precision. The learning was retained following the training and generalized to saccades of different directions, and to reaction time measures during a delayed saccade task. The study provides evidence for a novel form of saccadic learning with applicability in a number of domains.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11945
        },
        {
          "affiliations": [],
          "personId": 18882
        }
      ],
      "sessionIds": [
        1301,
        2498
      ],
      "eventIds": []
    },
    {
      "id": 2750,
      "typeId": 10970,
      "title": "EyeMic: an eye tracker for surgical microscope",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "The concept of hands free surgical microscope has become increasingly popular in the domain of microsurgery. The higher magnification, the smaller field of view, necessitates frequent interaction with the microscope during an operation. Researchers showed that manual (hand) interactions with a surgical microscope resulted in disruptive and hazardous situations. Previously, we proposed the idea of eye control microscope as a solution to this interaction problem. While gaze contingent applications have been widely studied in HCI and eye tracking domain the lack of ocular based eye trackers for microscope being an important concern in this domain. To solve this critical problem and provide opportunity to capture eye movements in microsurgery in real time we present EyeMic, a binocular eye tracker that can be attached on top of any microscope ocular. Our eye tracker has only 5mm height to grantee same field of view, and it supports up to 120 frame per",
      "authors": [
        {
          "affiliations": [],
          "personId": 22070
        },
        {
          "affiliations": [],
          "personId": 16844
        }
      ],
      "sessionIds": [
        1940
      ],
      "eventIds": []
    },
    {
      "id": 4031,
      "typeId": 10048,
      "title": "Error-Aware Gaze-Based Interfaces for Robust Mobile Gaze Interaction",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze estimation error can severely hamper usability and performance of mobile gaze-based interfaces given that the error varies constantly for different interaction positions. In this work, we explore error-aware gaze-based interfaces that estimate and adapt to gaze estimation error on-the-fly. We implement a sample error-aware user interface for gaze-based selection and different error compensation methods: a naïve approach that increases component size directly proportional to the absolute error, a recent model by Feit et al. that is based on the two-dimensional error distribution, and a novel predictive model that shifts gaze by a directional error estimate. We evaluate these models in a 12-participant user study and show that our predictive model significantly outperforms the others in terms of selection rate, particularly for small gaze targets. These results underline both the feasibility and potential of next generation error-aware gaze-based user interfaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22721
        },
        {
          "affiliations": [],
          "personId": 18177
        },
        {
          "affiliations": [],
          "personId": 23196
        },
        {
          "affiliations": [],
          "personId": 20391
        }
      ],
      "sessionIds": [
        1219
      ],
      "eventIds": []
    },
    {
      "id": 3263,
      "typeId": 10953,
      "title": "Robust Marker Tracking for Mapping Mobile Eye Tracking Data",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "One of the challenges of mobile eye tracking is mapping gaze data on a reference image of the stimulus. Here we present a marker-tracking system that relies on the scene-video, recorded by eye tracking glasses, to recognize and track markers and map gaze data on the reference image. Due to the simple nature of the markers employed, the current system works with low-quality videos and at long distances from the stimulus, allowing the use of mobile eye tracking in new situations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18920
        },
        {
          "affiliations": [],
          "personId": 11295
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 8129,
      "typeId": 10970,
      "title": "Developing Photo-Sensor Oculography (PS-OG) system for Virtual Reality headset",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual reality (VR) is employed in a variety of different applications. It is our belief that eye-tracking is going to be a part of the majority of VR devices that will reduce computational burden via a technique called foveated rendering and will increase the immersion of the VR environment. A promising technique to achieve low energy, fast, and accurate eye-tracking is photo-sensor oculography (PS-OG). PS-OG technology enables tracking a user's gaze location at very fast rates - 1000Hz or more, and is expected to consume several orders of magnitude less power compared to a traditional video-oculography approach. In this demo we present a prototype of a PS-OG system that we started to develop. The long-term aim of our project is to develop a PS-OG system that is robust to sensor shifts. As a first step we have built a prototype that allows us to test different sensors and their configurations, as well as record and analyze eye-movement data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22259
        },
        {
          "affiliations": [],
          "personId": 9488
        }
      ],
      "sessionIds": [
        1005
      ],
      "eventIds": []
    },
    {
      "id": 3778,
      "typeId": 10048,
      "title": "Gaze patterns during Remote Presentations while Listening and Speaking",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Managing an audience’s visual attention to presentation content is critical for effective communication in tele-conferences. This paper explores how audience and presenter coordinate visual and verbal information, and how consistent their gaze behavior is, to understand if their gaze behavior can be used for inferring and communicating attention in remote presentations. In a lab study, participants were asked first to view a short video presentation, and next, to rehearse and present to a remote viewer using the slides from the video presentation. We found that presenters coordinate their speech and gaze at visual regions of the slides in a timely manner (in 72% of all events analyzed), whereas audience only looked at what the presenter talked about in 53% of all events. Rehearsing aloud and presenting resulted in similar scanpaths. To further explore if it possible to infer if what a presenter is looking at is also talked about, we successfully trained models to detect an attention match between gaze and speech. These findings suggest that us-ing the presenter’s gaze has the potential to reliably communicate the presenter’s focus on essential parts of the visual presentation material to help the audience better follow the presenter.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9953
        },
        {
          "affiliations": [],
          "personId": 14030
        }
      ],
      "sessionIds": [
        1178
      ],
      "eventIds": []
    },
    {
      "id": 5827,
      "typeId": 10928,
      "title": "Scene perception while listening to music: an eye-tracking study",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Previous studies have observed longer fixations and fewer\nsaccades while viewing various outdoor scenes and listening to\nmusic compared to a no-music condition. There is also evidence\nthat musical tempo can modulate the speed of eye movements.\nHowever, recent investigations from environmental psychology\ndemonstrated differences in eye movement behavior while\nviewing natural and urban outdoor scenes. Viewing natural scenes\nis associated with longer fixation durations than viewing outdoor\nurban scenes. The first goal of this study was to replicate the\nobserved effect of music listening while viewing outdoor scenes\nwith different musical stimuli. Next, the effect of a fast and a slow\nmusical tempo on eye movement speed was investigated. Finally,\nthe effect of the type of outdoor scene (natural vs. urban scenes)\nwas explored. The results revealed shorter fixation durations in\nthe no-music condition compared to both music conditions, but\nthese differences were non-significant. Moreover, we did not find\ndifferences in eye movements between music conditions with fast\nand slow tempo. Although significantly shorter fixations were\nfound for viewing urban scenes compared with natural scenes, we\ndid not find a significant interaction between the type of scene and\nmusic conditions. It suggests that music processing does not\nrequire the same cognitive resources as the processing of visual\nscenes. It is supposed that the type of musical stimuli, the specific\ntempo, and the specific experimental procedure and the interest\nand engagement of participants in listening to background music\nwhile processing visual information are important factors that\ninfluence attentional processes, which are manifested in eyemovement\nbehavior.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16273
        },
        {
          "affiliations": [],
          "personId": 12959
        },
        {
          "affiliations": [],
          "personId": 20468
        },
        {
          "affiliations": [],
          "personId": 21482
        }
      ],
      "sessionIds": [
        1301,
        1178
      ],
      "eventIds": []
    },
    {
      "id": 5317,
      "typeId": 10928,
      "title": "Ocular reactions in response to impressions of emotion-evoking pictures",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Oculomotor indicies in response to emotional stimuli were analysed\n chronologically in order to investigate the relationships between eye\n behaviour and emotional activity in human visual perception.\nSeven participants classified visual stimuli into two emotional groups\n using subjective ratings of images, such as ``Pleasant'' and\n ``Unpleasant''. Changes in both eye movements and pupil diameters\n between the two groups of images were compared.  \nBoth the mean saccade lengths and the cross power spectra of eye\n movements for ``Unpleasant'' ratings were significantly higher than\n for other ratings of eye movements in regards to certain the duration of\n certain pictures shown. \nAlso, both mean pupil diameters and their power spectrum densities were\n significantly higher when the durations of pictures presented were\n lengthened. \nWhen comparing the response latencies, pupil reactions followed the \n appearance of changes in the direction of eye movements. \nThe results suggest that at specific latencies, ``Unpleasant'' images\n activate both eye movements and pupil dilations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14235
        }
      ],
      "sessionIds": [
        1301,
        1331
      ],
      "eventIds": []
    },
    {
      "id": 7624,
      "typeId": 10307,
      "title": "Eye tracking philosophy from Tobii Pro",
      "trackId": 10021,
      "tags": [],
      "keywords": [],
      "abstract": "This workshop will focus on the human eye movements and how to measure them from a physical and geometrical point of view, as opposed to the more common topic of the cognitive aspects of eye movements. There will be a walk through different parts of the human eye and muscles controlling it (including both optical and mechanical behavior). Followed by a brief discussion around different types of eye movements and gaze behavior and how these correlate to the eye mechanics (saccades, fixations, smooth pursuit, VOR, accomodation and vergence). A short introduction to different methods of measuring eye movements will be given, leading to a more detailed discussion about the design philosophy behind Tobii eye trackers and how they take into account the optical and geometrical aspects of eye movements, as described in the beginning. All parameters in the output data will be described in detail, with reference to the geometrical behavior of a real human eye.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14304
        }
      ],
      "sessionIds": [
        2086
      ],
      "eventIds": []
    },
    {
      "id": 7370,
      "typeId": 10307,
      "title": "iMotion Workshop",
      "trackId": 10021,
      "tags": [],
      "keywords": [],
      "abstract": "Join our iMotions workshop and learn how biometric techniques and our platform are being utilized in human behavior research. We will start with a short presentation about biometrics, continue with a 1-hour long interactive DEMO study, and finish with a discussion where our biometric research experts, Jeff Zornig, Kerstin Wolf, and Ole Baunbæk Jensen will answer any of your questions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9010
        },
        {
          "affiliations": [],
          "personId": 15251
        },
        {
          "affiliations": [],
          "personId": 14360
        }
      ],
      "sessionIds": [
        1107
      ],
      "eventIds": []
    },
    {
      "id": 6346,
      "typeId": 10928,
      "title": "Towards Gaze-Based Quantification of the Security of Graphical Authentication Schemes",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we introduce a two-step method for estimating the strength of user-created graphical passwords based on the eye-gaze behaviour during password composition. First, the individuals' gaze patterns, represented by the unique fixations on each area of interest (AOI) and the total fixation duration per AOI, are calculated.  Second, the gaze-based entropy of the individual is calculated. To investigate whether the proposed metric is a credible predictor of the password strength, we conducted two feasibility studies. Results revealed a strong positive correlation between the strength of the created passwords and the gaze-based entropy. Hence, we argue that the proposed gaze-based metric allows for unobtrusive prediction of the strength of the password a user is going to create and enables intervention to the password composition for helping users create stronger passwords.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10656
        },
        {
          "affiliations": [],
          "personId": 19912
        },
        {
          "affiliations": [],
          "personId": 11917
        }
      ],
      "sessionIds": [
        1301,
        2188
      ],
      "eventIds": []
    },
    {
      "id": 7371,
      "typeId": 10048,
      "title": "The hierarchical flow of eye movements",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements are composed of spatial and temporal aspects. Moreover, not only the eye movements of one subject are of interest, but a data analyst is more or less interested in the scanning strategies of a group of people in a condensed form. This data aggregation can provide useful insights into the visual attention over space and time leading to the detection of possible visual problems or design flaws in the presented stimulus. In this paper we present a way to visually explore the flow of eye movements, i.e., we try to bring a layered hierarchical structure into the spatio-temporal eye movements. To reach this goal, the stimulus is spatially divided into areas of interest (AOIs) and temporally or sequentially aggregated into time periods or subsequences. The weighted AOI transitions are used to model directed graph edges while the AOIs build the graph vertices. The flow of eye movements is naturally obtained by computing hierarchical layers for the AOIs while the downward edges indicate the hierarchical flow between the AOIs on the corresponding layers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19432
        },
        {
          "affiliations": [],
          "personId": 17574
        },
        {
          "affiliations": [],
          "personId": 23341
        }
      ],
      "sessionIds": [
        1098
      ],
      "eventIds": []
    },
    {
      "id": 6605,
      "typeId": 10953,
      "title": "New Features of ScanGraph - a Tool for Revealing Participants' Strategy from Eye-movement Data",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "The demo describes new features of ScanGraph, an application intended for a finding of participants with a similar stimulus reading strategy based on the sequences of visited Areas of Interest. The result is visualised using cliques of a simple graph. ScanGraph was initially introduced in 2016. Since the original publication, new features were added. First of them is the implementation of Damerau-Levenshtein algorithm for similarity calculation. A heuristic algorithm for cliques finding used in the original version was replaced by the Bron-Kerbosch algorithm. ScanGraph reads data from open-source application OGAMA, and with the use of conversion tool also data from SMI BeGaze, which allows analysing dynamic stimuli as well. The most prominent enhancement is the possibility of similarity calculation among participants not only for a single stimulus but for multiple files at once.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8674
        },
        {
          "affiliations": [],
          "personId": 9737
        },
        {
          "affiliations": [],
          "personId": 12104
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 6606,
      "typeId": 10970,
      "title": "Head and Gaze Control of a Telepresence Robot With an HMD",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze interaction with telerobots is a new opportunity for wheelchair users with severe motor disabilities. We present a video showing how head-mounted displays (HMD) with gaze tracking can be used to monitor a robot that carries a 360° video camera and a microphone. Our interface supports autonomous driving via way-points on a map, along with gaze-controlled steering and gaze typing. It is implemented with Unity, which communicates with the Robot Operating System (ROS).",
      "authors": [
        {
          "affiliations": [],
          "personId": 17011
        },
        {
          "affiliations": [],
          "personId": 14896
        },
        {
          "affiliations": [],
          "personId": 17607
        },
        {
          "affiliations": [],
          "personId": 19325
        }
      ],
      "sessionIds": [
        1005
      ],
      "eventIds": []
    },
    {
      "id": 7375,
      "typeId": 10953,
      "title": "A Visual Comparison of Gaze Behavior from Pedestrians and Cyclists",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we contribute an eye tracking study conducted with pedestrians and cyclists. We apply a visual analytics-based method to inspect pedestrians' and cyclists' gaze behavior as well as video recordings and accelerometer data. This method using multi-modal data allows us to explore patterns and extract common eye movement strategies. Our results are that participants paid most attention to the path itself; advertisements do not distract participants; participants focus more on pedestrians than on cyclists; pedestrians perform more shoulder checks than cyclists do; and we extracted common gaze sequences. Such an experiment in a real-world traffic environment allows us to understand realistic behavior of pedestrians and cyclists better.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17643
        },
        {
          "affiliations": [],
          "personId": 18463
        },
        {
          "affiliations": [],
          "personId": 12725
        },
        {
          "affiliations": [],
          "personId": 20528
        },
        {
          "affiliations": [],
          "personId": 15061
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 5329,
      "typeId": 10970,
      "title": "EyeMR - Low-cost Eye-Tracking for Rapid-prototyping in Head-mounted Mixed Reality",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Mixed Reality devices can either augment reality (AR) or create completely virtual realities (VR). Combined with head-mounted devices and eye-tracking, they enable users to interact with these systems in novel ways. However, current eye-tracking systems are expensive and limited in the interaction with virtual content. In this paper, we present EyeMR, a low-cost system (below 100$) that enables researchers to rapidly prototype new techniques for eye and gaze interactions. Our system supports mono- and binocular tracking (using Pupil Capture) and includes a Unity framework to support the fast development of new interaction techniques. We argue for the usefulness of EyeMR based on results of a user evaluation with HCI experts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15935
        },
        {
          "affiliations": [],
          "personId": 17605
        },
        {
          "affiliations": [],
          "personId": 9805
        }
      ],
      "sessionIds": [
        1940
      ],
      "eventIds": []
    },
    {
      "id": 7379,
      "typeId": 10928,
      "title": "Pupil Responses Signal Less Inhibition for Own Relative to Other Names",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Previous research suggests that self-relevant stimuli, such as one`s own name, attract more attention than stimuli that are not self-relevant. In two experiments, we examined to which extent the own name is also less prone to inhibition than other names using a Go/NoGo approach. The pupil diameter was employed as psychophysiological indicator of attention. A total of 36 subjects performed various categorization tasks, with their own name and other names.  Whereas in Go-trials, pupil dilation for own and other names did not differ, in NoGo-trials, significant larger pupil dilations were obtained for subjects' own names compared to other names. This difference was especially pronounced at larger intervals after stimulus onset, suggesting that inhibitory processing was less effective with one's own name.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16269
        },
        {
          "affiliations": [],
          "personId": 24166
        },
        {
          "affiliations": [],
          "personId": 21398
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 6359,
      "typeId": 10970,
      "title": "Use of Attentive Information Dashboards to Support Task Resumption in Working Environments",
      "trackId": 10027,
      "tags": [],
      "keywords": [],
      "abstract": "Interruptions are known as one of the big challenges in working environments. Due to improper resuming the primary task, such interruptions may result in task resumption failures and negatively influence the task performance. This phenomenon also occurs when users are working with information dashboards in working environments. To address this problem, an attentive dashboard issuing visual feedback is developed. This feedback supports the user in resuming the primary task after the interruption by guiding the visual attention. The attentive dashboard captures visual attention allocation of the user with a low-cost screen-based eye-tracker while they are monitoring the graphs. This dashboard is sensitive to the occurrence of external interruption by tracking the eye-movement data in real-time. Moreover, based on the collected eye-movement data, two types of visual feedback are designed which highlight the last fixated graph and unnoticed ones.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16576
        },
        {
          "affiliations": [],
          "personId": 14048
        },
        {
          "affiliations": [],
          "personId": 18389
        }
      ],
      "sessionIds": [
        1940
      ],
      "eventIds": []
    },
    {
      "id": 6104,
      "typeId": 10048,
      "title": "Improving the adaptive event detection algorithm of Nyström and Holmquist for noisy data",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Detecting eye tracking events such as fixations and saccades is one of the first important steps in eye tracking research. The adaptive algorithm by Nyström and Holmqvist [2010] estimates thresholds by computing a \"peak velocity detection threshold\" (PT) that depends on the data's noise level. However, too high thresholds might result in only few detected saccades. The present study investigated a solution with an upper bound for PT. Fixations and saccades were computed for N = 68 participants who performed a fixation task and a visual detection test. The original version of the algorithm was compared with five versions utilizing upper bounds for PT (ranging from 100deg/sec to 300deg/sec) according to three predefined criteria. These criteria suggest an optimal upper bound at 200deg/sec for the utilized static and simple structured testing materials.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22330
        }
      ],
      "sessionIds": [
        1496
      ],
      "eventIds": []
    },
    {
      "id": 2777,
      "typeId": 10048,
      "title": "Enhanced Representation of Web Pages for Usability Analysis with Eye Tracking",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking as a tool to quantify user attention plays a major role in research and application design. For Web page usability, it has become a prominent measure to assess which sections of a Web page are read, glanced or skipped. Such assessments primarily depend on the mapping of gaze data to a Web page representation. However, current representation methods, a virtual screenshot of the Web page or a video recording of the complete interaction session, suffer either from accuracy or scalability issues. We present a method that identifies fixed elements on Web pages and combines user viewport screenshots in relation to fixed elements for an enhanced representation of the page. We conducted an experiment with 10 participants and the results signify that analysis with our method is more efficient than a video recording, which is an essential criterion for large scale Web studies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10135
        },
        {
          "affiliations": [],
          "personId": 21691
        },
        {
          "affiliations": [],
          "personId": 20190
        },
        {
          "affiliations": [],
          "personId": 8206
        },
        {
          "affiliations": [],
          "personId": 11875
        },
        {
          "affiliations": [],
          "personId": 11841
        }
      ],
      "sessionIds": [
        2188
      ],
      "eventIds": []
    },
    {
      "id": 6361,
      "typeId": 10048,
      "title": "Crowdsourcing pupil annotation datasets: boundary vs. center, what performs better?",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Pupil-related feature detection is one of the most common approaches used in the eye-tracking literature and practice. Validation and benchmarking of the detection algorithms relies on accurate ground-truth datasets, but creating of these is costly. Many approaches have been used to obtain human based annotations. A recent proposal to obtain these work-intensive data is through a crowdsourced registration of the pupil center, in which a large number of users provide a single click to indicate the pupil center [Gil de Gómez Pérez and Bednarik 2018a]. In this paper we compare the existing approach to a method based on multiple clicks on the boundary of the pupil region, in order to determine which approach provides better results. To compare both methods, a new data collection was performed over the same image database. Several metrics were applied in order to evaluate the accuracy of the two methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17141
        },
        {
          "affiliations": [],
          "personId": 15365
        },
        {
          "affiliations": [],
          "personId": 21687
        }
      ],
      "sessionIds": [
        2064
      ],
      "eventIds": []
    },
    {
      "id": 4058,
      "typeId": 10048,
      "title": "Eye movements in code review",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "In order to ensure sufficient quality, software engineers conduct code reviews to read over one another's code looking for errors that should be fixed before committing to their source code repositories. Many kinds of errors are spotted, from simple spelling mistakes and syntax errors, to architectural flaws that may span several files. However, we know little about how software developers read code when looking for defects. What kinds of code trigger engineers to check more deeply into suspected defects? How long do they take to verify whether a defect is really there? We conducted a study of 35 software engineers performing 40 code reviews while capturing their gaze with an eye tracker. We classified each code defect the developers found and captured the patterns of eye gazes used to deliberate about each one. We report how long it took to confirm defect suspicions for each type of defect and the fraction of time spent skimming the code vs. carefully reading it. This work provides a starting point for automating code reviews that could help engineers spend more time focusing on the difficult task of defect confirmation rather than the tedious task of defect discovery.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20872
        },
        {
          "affiliations": [],
          "personId": 9510
        }
      ],
      "sessionIds": [
        2468
      ],
      "eventIds": []
    },
    {
      "id": 6367,
      "typeId": 10286,
      "title": "Intelligent cockpit: eye tracking integration to enhance the pilot-aircraft interaction",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "In this research, we use eye tracking to monitor the attentional behavior of pilots in the cockpit. We built a cockpit monitoring database that serves as a reference for real-time assessment of the pilot's monitoring strategies, based on numerous flight simulator sessions with eye-tracking recordings. Eye tracking may also be employed as a passive input for assistive system, future studies will also explore the possibility to adapt the notifications' modality using gaze.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23003
        },
        {
          "affiliations": [],
          "personId": 10048
        },
        {
          "affiliations": [],
          "personId": 11590
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 2784,
      "typeId": 10928,
      "title": "Fixation-Indices based Correlation between Text and Image Visual Features of Webpages",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Web elements associate with a set of visual features based on their data modality. For example, text associated with font-size and font-family whereas images associate with intensity and color. The unavailability of methods to relate these heterogeneous visual features limiting the attention-based analyses on webpages. In this paper, we propose a novel approach to establish the correlation between text and image visual features that influence users’ attention. We pair the visual features of text and images based on their associated fixation-indices obtained from eye-tracking. From paired data, a common subspace is learned using Canonical Correlation Analysis (CCA) to maximize the correlation between them. The performance of the proposed approach is analyzed through a controlled eye-tracking experiment conducted on 51 real-world webpages. A very high correlation of 99.48% is achieved between text and images with text related font families and image related color features influencing the correlation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12946
        },
        {
          "affiliations": [],
          "personId": 13670
        },
        {
          "affiliations": [],
          "personId": 21879
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 5088,
      "typeId": 10286,
      "title": "Towards concise gaze sharing",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Computer-supported collaboration changed the way we learn and work together, as co-location is no longer a necessity. While presence, pointing and actions belong to the established inventory of awareness functionality which aims to inform about peer activities, visual attention as a beneficial cue for successful collaboration does not. Several studies have shown that providing real-time gaze cues is advantageous, as it enables more efficient referencing by reducing deictic expressions and fosters joint attention by facilitating shared gaze. But the actual use is held back due to its inherent limitations: Real-time gaze display is often considered distracting, which is caused by its constant movement and an overall low signal-to-noise ratio. As a result, the transient nature makes it difficult to associate with a dynamic stimulus over time. While it is helpful when referencing or shared gaze is crucial, the application in common collaborative environments with a constant alternation between close and loose collaboration presents challenges. My dissertation work will explore a novelty gaze sharing approach, that aims to detect task-related gaze patterns which are displayed in concise representations. This work will contribute to our understanding of coordination in collaborative environments and propose algorithms and design recommendations for gaze sharing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16681
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 6117,
      "typeId": 10048,
      "title": "Dwell Time Reduction Technique using Fitts' Law for Gaze-Based Target Acquisition",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "We present a dwell time reduction technique for gaze-based target acquisition. We adopt Fitts' Law to achieve the dwell time reduction. Our technique uses both the eye movement time for target acquisition estimated using Fitts' Law (Te) and the actual eye movement time (Ta) for target acquisition; a target is acquired when the difference between Te and Ta is small. First, we investigated the relation between the eye movement for target acquisition and Fitts' Law; the result indicated a correlation of 0.90 after error correction. Then we designed and implemented our technique. Finally, we conducted a user study to investigate the performance of our technique; an average dwell time of 86.7 ms was achieved, with a 10.0% Midas-touch rate.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15306
        },
        {
          "affiliations": [],
          "personId": 14787
        },
        {
          "affiliations": [],
          "personId": 8228
        },
        {
          "affiliations": [],
          "personId": 19809
        }
      ],
      "sessionIds": [
        2498
      ],
      "eventIds": []
    },
    {
      "id": 6374,
      "typeId": 10048,
      "title": "Improving Map Reading with Gaze-Adaptive Legends",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Complex information visualizations, such as thematic maps, encode information using a particular symbology that often requires the use of a legend to explain its meaning.Traditional legends are placed at the edge of a visualization, which can be difficult to maintain visually while switching attention between content and legend.\nMoreover, an extensive search may be required to extract relevant information from the legend. In this paper we propose to consider the user’s visual attention to improve interaction with a map legend by adapting both the legend’s placement and content to the user’s gaze.\nIn a user study, we compared two novel adaptive legend behaviors to a traditional (non-adaptive) legend. We found that, with both of our approaches, participants spent significantly less task time looking at the legend than with the baseline approach. Furthermore, participants stated that they preferred the gaze-based approach of adapting the legend content (but not its placement).",
      "authors": [
        {
          "affiliations": [],
          "personId": 14831
        },
        {
          "affiliations": [],
          "personId": 13856
        },
        {
          "affiliations": [],
          "personId": 16798
        },
        {
          "affiliations": [],
          "personId": 10369
        },
        {
          "affiliations": [],
          "personId": 11303
        }
      ],
      "sessionIds": [
        2498
      ],
      "eventIds": []
    },
    {
      "id": 6118,
      "typeId": 10953,
      "title": "An Implementation of Eye Movement-Driven Biometrics in Virtual Reality",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "As eye tracking can reduce the computational burden of virtual reality devices through a technique known as foveated rendering, we believe not only that eye tracking will be implemented in all virtual reality devices, but that eye tracking biometrics will become the standard method of authentication in virtual reality. Thus, we have created a real-time eye movement-driven authentication system for virtual reality devices. In this work, we describe the architecture of the system and provide a specific implementation that is done using the FOVE head-mounted display. We end with an exploration into future topics of research to spur thought and discussion.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9488
        },
        {
          "affiliations": [],
          "personId": 8579
        },
        {
          "affiliations": [],
          "personId": 19339
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 4071,
      "typeId": 10928,
      "title": "Useful Approaches to Exploratory Analysis of Gaze Data: Enhanced Heatmaps, Cluster Maps, and Transition Maps",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Exploratory analysis of gaze data requires methods that make it possible to process large amounts of data while minimizing human labor. The conventional approach in exploring gaze data is to construct heatmap visualizations. While simple and intuitive, conventional heatmaps do not clearly indicate differences between groups of viewers or give estimates for the repeatability (i.e., which parts of the heatmap would look similar if the data were collected again). We discuss difference maps and significance maps that answer to these needs. In addition we describe methods based on automatic clustering that allow us to achieve similar results with cluster observation maps and transition maps. As demonstrated with our example data, these methods are effective in highlighting the strongest differences between groups more effectively than conventional heatmaps.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10589
        },
        {
          "affiliations": [],
          "personId": 14570
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 6887,
      "typeId": 10048,
      "title": "Eye-tracking evaluation of 3D thematic maps",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Although many 3D thematic cartography methods exist, the effectiveness of their use is not known. The described experiment comprised two parts focusing on the evaluation of two 3D thematic cartography methods (Prism Map and Illuminated Choropleth Map) compared to a simple choropleth map. The task in both parts of the experiment was to determine which of the marked areas showed a higher value of the displayed phenomenon. The correctness of answers, response time and selected eye-tracking metrics were analysed. In the first part of the experiment, a higher number of correct answers was found for Prism Maps than for simple choropleth maps, but it required more time to solve the task. The Illuminated Choropleth Map showed a higher proportion of correct answers than a simple choropleth map. During evaluation of the eye-tracking metrics, a statistically significant difference was not found in most cases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8674
        }
      ],
      "sessionIds": [
        1345
      ],
      "eventIds": []
    },
    {
      "id": 7400,
      "typeId": 10779,
      "title": "Playing Music with the Eyes Through an Isomorphic Interface",
      "trackId": 10023,
      "tags": [],
      "keywords": [],
      "abstract": "Playing music with the eyes is a challenging task. In this paper, we propose a virtual digital musical instrument, usable by both motor-impaired and able-bodied people, controlled through an eye tracker and a \"switch\". Musically speaking, the layout of the graphical interface is isomorphic, since the harmonic relations between notes have the same geometrical shape regardless of the key signature of the music piece. Four main design principles guided our choices, namely: (1) Minimization of eye movements, especially in case of large note intervals; (2) Use of a grid layout where \"nodes\" (keys) are connected each other through segments (employed as guides for the gaze); (3) No need for smoothing filters or time thresholds; and (4) Strategic use of color to facilitate gaze shifts. Preliminary tests, also involving another eye-controlled musical instrument, have shown that the developed system allows \"correct\" execution of music pieces even when characterized by complex melodies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17439
        },
        {
          "affiliations": [],
          "personId": 11354
        },
        {
          "affiliations": [],
          "personId": 9675
        },
        {
          "affiliations": [],
          "personId": 10931
        }
      ],
      "sessionIds": [
        2049
      ],
      "eventIds": []
    },
    {
      "id": 5352,
      "typeId": 10048,
      "title": "Learning to Find Eye Region Landmarks for Remote Gaze Estimation in Unconstrained Settings",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Conventional feature-based and model-based gaze estimation methods have proven to perform well in settings with controlled illumination and specialized cameras. In unconstrained real-world settings, however, such methods are surpassed by recent appearance-based methods due to difficulties in modeling factors such as illumination changes and other visual artifacts. We present a novel learning-based method for eye region landmark localization that enables conventional methods to be competitive to latest appearance-based methods. Despite having been trained exclusively on synthetic data, our method exceeds the state of the art for iris localization and eye shape registration on real-world imagery. We then use the detected landmarks as input to iterative model-fitting and lightweight learning-based gaze estimation methods. Our approach outperforms existing model-fitting and appearance-based methods in the context of person-independent and personalized gaze estimation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16318
        },
        {
          "affiliations": [],
          "personId": 18059
        },
        {
          "affiliations": [],
          "personId": 20391
        },
        {
          "affiliations": [],
          "personId": 23808
        }
      ],
      "sessionIds": [
        1219
      ],
      "eventIds": []
    },
    {
      "id": 5353,
      "typeId": 10286,
      "title": "Seeing into the music score: eye-tracking and sight-reading in a choral context",
      "trackId": 10020,
      "tags": [],
      "keywords": [],
      "abstract": "Musical sight-reading is a complex task which requires fluent use of multiple types of skills and knowledge. The ability to sight-read a score is typically described as one of the most challenging aims for beginners and finding ways of scaffolding their learning is, therefore, an important task for researchers in music education. The purpose of this study is to provide a deeper understanding of how an application of eye tracking technology can be utilized to improve choir singers' sight-reading ability. Collected data of novices' sight-reading patterns during choral rehearsal have helped identify problems that singers are facing. Analyzing corresponding patterns in sight-reading performed by expert singers may provide valuable information about helpful strategies developed with increasing experience. This project is expected to generate an approximate model, similar to the experts' eye movement path. The model will then be implemented in a training method for unskilled choral singers. Finally, as a summative result, we plan to evaluate how the training affects novices' competency in sight-reading and comprehension of the score.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14699
        }
      ],
      "sessionIds": [
        1369
      ],
      "eventIds": []
    },
    {
      "id": 3306,
      "typeId": 10048,
      "title": "Multiscale scanpath visualization and filtering",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "The analysis of eye-tracking data can be very useful when evaluating controlled user studies. To support the analysis in a fast and easy fashion, we have developed a web-based framework for a visual inspection of eye-tracking data and a comparison of scanpaths based on filtering of fixations and similarity measures. Concerning the first part, we introduce a multiscale aggregation of fixations and saccades based on a spatial partitioning that reduces visual clutter of overlaid scanpaths without changing the overall impression of large-scale eye movements. The multiscale technique abstracts the individual scanpaths and allows an analyst to visually identify clusters or patterns inherent to the gaze data without the need for lengthy precomputations. For the second part, we introduce an approach where analysts can remove fixations from a pair of scanpaths in order to increase the similarity between them. This can be useful to discover and understand reasons for dissimilarity between scanpaths, data cleansing, and outlier detection. Our implementation uses the MultiMatch algorithm to predict similarities after the removal of individual fixations. Finally, we demonstrate the usefulness of our techniques in a use case with scanpaths that were recorded in a study with metro maps.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18744
        },
        {
          "affiliations": [],
          "personId": 16784
        },
        {
          "affiliations": [],
          "personId": 16185
        },
        {
          "affiliations": [],
          "personId": 18716
        }
      ],
      "sessionIds": [
        1098
      ],
      "eventIds": []
    },
    {
      "id": 6637,
      "typeId": 10047,
      "title": "How can visualization make a larger contribution to ETRA?",
      "trackId": 10017,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 9199
        },
        {
          "affiliations": [],
          "personId": 13856
        },
        {
          "affiliations": [],
          "personId": 22105
        },
        {
          "affiliations": [],
          "personId": 12725
        },
        {
          "affiliations": [],
          "personId": 10369
        }
      ],
      "sessionIds": [
        2042
      ],
      "eventIds": []
    },
    {
      "id": 8174,
      "typeId": 10048,
      "title": "Gaze Typing in Virtual Reality: Impact of Keyboard Design, Selection Method, and Motion",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze tracking in virtual reality (VR) allows for hands-free text entry, but it has not yet been explored. We investigate how the keyboard design, selection method, and motion in the field of view may impact typing performance and user experience. We present two studies of people (n = 32) typing with gaze+dwell and gaze+click inputs in VR. In study 1, the typing keyboard was flat and within-view; in study 2, it was larger-than-view but curved. Both studies included a stationary and a dynamic motion conditions in the user's field of view.\n\nOur findings suggest that 1) gaze typing in VR is viable but constrained, 2) the users perform best (10.15 WPM) when the entire keyboard is within-view; the larger-than-view keyboard (9.15 WPM) induces physical strain due to increased head movements, 3) motion in the field of view impacts the user's performance: users perform better while stationary than when in motion, and 4) gaze+click is better than dwell only (fixed at 550 ms) interaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14886
        },
        {
          "affiliations": [],
          "personId": 19325
        }
      ],
      "sessionIds": [
        2188
      ],
      "eventIds": []
    },
    {
      "id": 7409,
      "typeId": 10928,
      "title": "Microsaccade detection using pupil and corneal reflection signals",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "In contemporary research, microsaccade detection is typically per-\nformed using the calibrated gaze-velocity signal acquired from\na video-based eye tracker. To generate this signal, the pupil and\ncorneal reflection (CR) signals are subtracted from each other and a\ndifferentiation filter is applied, both of which may prevent small mi-\ncrosaccades from being detected due to signal distortion and noise\namplification. We propose a new algorithm where microsaccades\nare detected directly from uncalibrated pupil-, and CR signals. It is\nbased on detrending followed by windowed correlation between\npupil and CR signals.The proposed algorithm outperforms the most\ncommonly used algorithm in the field (Engbert & Kliegl, 2003), in\nparticular for small amplitude microsaccades that are difficult to see\nin the velocity signal even with the naked eye. We argue that it is\nadvantageous to consider the most basic output of the eye tracker,\ni.e. pupil-, and CR signals, when detecting small microsaccades.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10414
        },
        {
          "affiliations": [],
          "personId": 12096
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 3058,
      "typeId": 10048,
      "title": "Visualizing pilot eye movements for flight instructors",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "The idea of using eye tracking technology in pilot training has been suggested and successfully applied in the past. At the same time, the possibilities of visualizing eye tracking data have strongly progressed. Nonetheless, little effort has been invested into exploring which type of eye tracking visualization flight instructors prefer for evaluating pilots' visual scanning strategies. This paper introduces ongoing research, which provides flight instructors with different eye tracking visualizations for assessing pilots' eye movements and evaluates those in an empirical study.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20276
        },
        {
          "affiliations": [],
          "personId": 13856
        },
        {
          "affiliations": [],
          "personId": 11303
        }
      ],
      "sessionIds": [
        1496
      ],
      "eventIds": []
    },
    {
      "id": 6899,
      "typeId": 10284,
      "title": "Eye-Tracking and Visual Analytics",
      "trackId": 10019,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking has become a widely used method to analyze human behavior in marketing, neuroscience, human-computer interaction, perception and cognition research, as well as visualization. Apart from measuring completion times and recording accuracy rates of correctly given answers during the performance of visual tasks in classical controlled experiments, eye tracking-based evaluations provide additional information on how visual attention is distributed and how it changes for a presented stimulus. Due to the wide field of applications of eye tracking and various kinds of research questions, different approaches have been developed to analyze eye movement data such as statistical algorithms (either descriptive or inferential), string editing algorithms, visualization-related techniques, and visual analytics techniques. Regardless of whether statistical or visual methods are used for eye movement data analysis, a large amount of data generated during eye tracking experiments has to be handled.\n\nWhere statistical analysis mainly provides quantitative results, visualization techniques allow researchers to analyze different levels and aspects of the recorded eye movement data in an explorative and qualitative way. When visualization techniques are not able to handle the large amount of eye movement data, the emerging discipline of visual analytics can be an option for exploratory data analysis. Machine-based analysis techniques such as methods from data mining or knowledge discovery in databases are combined with interactive visualizations and the perceptual abilities of a human viewer. Due to the increasing complexity of tasks and stimuli in eye tracking experiments, we believe that visual analytics approaches will play an increasingly important role in future eye tracking research. However, researchers are still missing sophisticated tools for an analysis of eye movement data using visual analytics approaches.\n\nIn this tutorial, we will present an overview of visual analytics approaches for eye movement data. We also demonstrate the analysis of eye movement data using Blickshift Analytics, a visual analytics software, which includes different eye tracking visualizations, pattern search techniques, and statistical method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18463
        },
        {
          "affiliations": [],
          "personId": 12725
        },
        {
          "affiliations": [],
          "personId": 19432
        }
      ],
      "sessionIds": [
        1166
      ],
      "eventIds": []
    },
    {
      "id": 4344,
      "typeId": 10928,
      "title": "DeepComics: Saliency estimation for comics",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "A key requirement for training deep learning saliency models is large training eye tracking datasets. Despite the fact that the accessibility of eye tracking technology has greatly increased, collecting eye tracking data on a large scale for very specific content types is cumbersome, such as comic images, which are different from natural images such as photographs because text and pictorial content is integrated. In this paper, we show that a deep network trained on visual categories where the gaze deployment is similar to comics outperforms existing models and models trained with visual categories for which the gaze deployment is dramatically different from comics. Further, we find that it is better to use a computationally generated dataset on visual category close to comics one than real eye tracking data of a visual category that has different gaze deployment. These findings hold implications for the transference of deep networks to different domains.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11951
        },
        {
          "affiliations": [],
          "personId": 20432
        },
        {
          "affiliations": [],
          "personId": 14970
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 7161,
      "typeId": 10048,
      "title": "Making stand-alone PS-OG technology tolerant to the equipment shifts",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Tracking users' gaze in virtual reality headsets allows natural and intuitive interaction with virtual avatars and virtual objects. Moreover, a technique known as foveated rendering can help save computational resources and enable hi-resolution but lightweight virtual reality technologies. Predominantly, eye-tracking hardware in modern VR headsets consist of infrared camera(s) and LEDs. Such hardware, together with image processing software consumes a substantial amount of energy, and, provided that hi-speed gaze detection is needed, might be very expensive. A promising technique to overcome these issues is photo-sensor oculography (PS-OG), which allows eye-tracking with high sampling rate and low power consumption. However, the main limitation of the previous PS-OG systems is their inability to compensate for the equipment shifts. In this study, we employ a simple multi-layer perceptron neural network to map raw sensor data to gaze locations and report its performance for shift compensation. Modeling and evaluation is done via a simulation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22259
        },
        {
          "affiliations": [],
          "personId": 9488
        }
      ],
      "sessionIds": [
        2064
      ],
      "eventIds": []
    },
    {
      "id": 4089,
      "typeId": 10048,
      "title": "Can we predict stressful technical interview settings through eye-tracking?",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, eye-tracking analysis for finding the cognitive load and stress while problem-solving on the whiteboard during a technical interview is finding its way in software engineering society. However, there is no empirical study on analyzing how much the interview setting characteristics affect the eye-movement measurements. Without knowing that, the results of a research on eye-movement measurements analysis for stress detection will not be reliable. In this paper, we analyzed the eye-movements of 11 participants in two interview settings, one on the whiteboard and the other on the paper, to find out if the characteristics of the interview settings affect the analysis of participants' stress. To this end, we applied 7 Machine Learning classification algorithms on three different labeling strategies of the data to suggest researchers of the domain a useful practice of checking the reliability of the eye-measurements before reporting any results.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13881
        },
        {
          "affiliations": [],
          "personId": 16204
        }
      ],
      "sessionIds": [
        2097
      ],
      "eventIds": []
    },
    {
      "id": 6650,
      "typeId": 10928,
      "title": "BORE: Boosted-oriented edge optimization for robust, real time remote pupil center detection",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "Undoubtedly, eye movements contain an immense amount of information,\nespecially when looking to fast eye movements, namely\ntime to the fixation, saccade, and micro-saccade events. While, modern\ncameras support recording of few thousand frames per second,\nto date, majority studies use eye trackers with the frame rates of\nabout 120 Hz for head-mounted and 250 Hz for remote-based trackers.\nIn this study, we aim to overcome the challenge of the pupil\ntracking algorithms to perform real time with high speed cameras\nfor remote eye tracking applications. We propose an iterative pupil\ncenter detection algorithm formulated as an optimization problem.\nWe evaluated our algorithm on more than 13,000 eye images, in\nwhich it outperforms earlier solutions both with regard to runtime\nand detection accuracy. Moreover, our system is capable of boosting\nits runtime in an unsupervised manner, thus we remove the need\nfor manual annotation of pupil images.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12726
        },
        {
          "affiliations": [],
          "personId": 22070
        },
        {
          "affiliations": [],
          "personId": 16174
        },
        {
          "affiliations": [],
          "personId": 22687
        },
        {
          "affiliations": [],
          "personId": 23370
        },
        {
          "affiliations": [],
          "personId": 9199
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    },
    {
      "id": 3071,
      "typeId": 10953,
      "title": "AnyOrbit: Orbital navigation in virtual environments with eye-tracking",
      "trackId": 10026,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze-based interactions promise to be fast, intuitive and effective in controlling virtual and augmented environments. Yet, there is still a lack of usable 3D navigation and observation techniques. In this work: 1) We introduce a highly advantageous orbital navigation technique, AnyOrbit, providing an intuitive and hands-free method of observation in virtual environments that uses eye-tracking to control the orbital center of movement; 2) The versatility of the technique is demonstrated with several control schemes and use-cases in virtual/augmented reality head-mounted-display and desktop setups, including observation of 3D astronomical data and spectator sports.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17252
        },
        {
          "affiliations": [],
          "personId": 23510
        },
        {
          "affiliations": [],
          "personId": 13998
        },
        {
          "affiliations": [],
          "personId": 9990
        },
        {
          "affiliations": [],
          "personId": 22250
        }
      ],
      "sessionIds": [
        2175
      ],
      "eventIds": []
    },
    {
      "id": 6911,
      "typeId": 10048,
      "title": "Evaluating gender difference on algorithmic problems using eye-tracker",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "Gender differences in programming comprehension has been a topic\nof discussion in recent years. We conducted an eye-tracking study\non 51(21 female, 30 male) computer science undergraduate university\nstudents to examine their cognitive processes in pseudocode\ncomprehension. We aim to identify their reading strategies and eye\ngaze behavior on the comprehension of pseudocodes in terms of\nperformance and visual effort when solving algorithmic problems\nof varying difficulty levels. Each student completed a series of tasks\nrequiring them to rearrange randomized pseudocode statements\nin a correct order for the problem presented. Our results indicated\nthat the speed of analyzing the problems were faster among male\nstudents, although female students fixated longer in understanding\nthe problem requirements. In addition, female students more\ncommonly fixated on indicative verbs (i.e., prompt, print), while\nmale students fixated more on operational statements (i.e., loops,\nvariables calculations, file handling).",
      "authors": [
        {
          "affiliations": [],
          "personId": 8229
        },
        {
          "affiliations": [],
          "personId": 18038
        }
      ],
      "sessionIds": [
        1082
      ],
      "eventIds": []
    },
    {
      "id": 8191,
      "typeId": 10048,
      "title": "Dynamics of emotional facial expressions recognition in individuals with social anxiety",
      "trackId": 10018,
      "tags": [],
      "keywords": [],
      "abstract": "This paper demonstrates the utility of ambient-focal attention and pupil dilation dynamics to describe visual processing of emotional facial expressions. Pupil dilation and focal eye movements reflect deeper cognitive processing and thus shed more light on the dy- namics of emotional expression recognition. Socially anxious in- dividuals (N = 24) and non-anxious controls (N = 24) were asked to recognize emotional facial expressions that gradually morphed from a neutral expression to one of happiness, sadness, or anger in 10-sec animations. Anxious cohorts exhibited more ambient face scanning than their non-anxious counterparts. We observed a positive relationship between focal fixations and pupil dilation, indi- cating deeper processing of viewed faces, but only by non-anxious participants, and only during the last phase of emotion recognition. Group differences in the dynamics of ambient-focal attention sup- port the hypothesis of vigilance to emotional expression processing by socially anxious individuals. We discuss the results by referring to current literature on cognitive psychopathology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17228
        },
        {
          "affiliations": [],
          "personId": 18745
        },
        {
          "affiliations": [],
          "personId": 10915
        },
        {
          "affiliations": [],
          "personId": 10703
        },
        {
          "affiliations": [],
          "personId": 9258
        },
        {
          "affiliations": [],
          "personId": 10369
        }
      ],
      "sessionIds": [
        1331
      ],
      "eventIds": []
    },
    {
      "id": 4607,
      "typeId": 10928,
      "title": "Sensitivity to Natural 3D Image Transformations during Eye Movements",
      "trackId": 10025,
      "tags": [],
      "keywords": [],
      "abstract": "The saccadic suppression effect, in which visual sensitivity is reduced significantly during saccades, has been suggested as a mechanism for masking graphic updates in a 3D virtual environment. In this study, we investigate whether the degree of saccadic suppression depends on the type of image change, particularly between different natural 3D scene transformations. The user observed 3D scenes and made a horizontal saccade in response to the displacement of a target object in the scene. During this saccade the entire scene translated or rotated. We studied six directions of transformation corresponding to the canonical directions for the six degrees of freedom. Following each trial, the user made a forced-choice indication of direction of the scene change. Results show that during horizontal saccades, the most recognizable changes were rotations along the roll axis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21711
        },
        {
          "affiliations": [],
          "personId": 12569
        }
      ],
      "sessionIds": [
        1301
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 14339,
      "firstName": "Rafael",
      "lastName": "Cabeza",
      "affiliations": []
    },
    {
      "id": 15365,
      "firstName": "Matti",
      "lastName": "Suokas",
      "affiliations": []
    },
    {
      "id": 8709,
      "firstName": "Constanze",
      "lastName": "Keutel",
      "affiliations": []
    },
    {
      "id": 9737,
      "firstName": "Jitka",
      "lastName": "Dolezalova",
      "affiliations": []
    },
    {
      "id": 8206,
      "firstName": "Tina",
      "lastName": "Walber",
      "affiliations": []
    },
    {
      "id": 8719,
      "firstName": "Jan",
      "lastName": "Ehlers",
      "affiliations": []
    },
    {
      "id": 13839,
      "firstName": "Karishma",
      "lastName": "Singh",
      "affiliations": []
    },
    {
      "id": 16400,
      "firstName": "Mahsa",
      "lastName": "Mirzamohammad",
      "affiliations": []
    },
    {
      "id": 17943,
      "firstName": "Christian",
      "lastName": "Kühnle",
      "affiliations": []
    },
    {
      "id": 14360,
      "firstName": "Ole",
      "lastName": "Jensen",
      "middleInitial": "B.",
      "affiliations": []
    },
    {
      "id": 21528,
      "firstName": "Corey",
      "lastName": "Bryant",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 19480,
      "firstName": "Christophe",
      "lastName": "Hurter",
      "affiliations": []
    },
    {
      "id": 9757,
      "firstName": "Dawid",
      "lastName": "Gruszczyński",
      "affiliations": []
    },
    {
      "id": 14365,
      "firstName": "Thomas",
      "lastName": "Mattusch",
      "affiliations": []
    },
    {
      "id": 8734,
      "firstName": "Paweł",
      "lastName": "Kasprowski",
      "affiliations": []
    },
    {
      "id": 12831,
      "firstName": "Ashwin",
      "lastName": "Mishra",
      "affiliations": []
    },
    {
      "id": 17439,
      "firstName": "Nicola",
      "lastName": "Davanzo",
      "affiliations": []
    },
    {
      "id": 11295,
      "firstName": "Roberto",
      "lastName": "Delfiore",
      "affiliations": []
    },
    {
      "id": 18463,
      "firstName": "Tanja",
      "lastName": "Blascheck",
      "affiliations": []
    },
    {
      "id": 21535,
      "firstName": "Mandy",
      "lastName": "Rossignol",
      "affiliations": []
    },
    {
      "id": 13856,
      "firstName": "Peter",
      "lastName": "Kiefer",
      "affiliations": []
    },
    {
      "id": 18464,
      "firstName": "Vedran",
      "lastName": "Sabol",
      "affiliations": []
    },
    {
      "id": 14880,
      "firstName": "Shareen",
      "lastName": "Mahmud",
      "affiliations": []
    },
    {
      "id": 11298,
      "firstName": "William",
      "lastName": "Rosengren",
      "affiliations": []
    },
    {
      "id": 20514,
      "firstName": "Jakob",
      "lastName": "Suchan",
      "affiliations": []
    },
    {
      "id": 10275,
      "firstName": "Jonathan",
      "lastName": "Maletic",
      "affiliations": []
    },
    {
      "id": 8228,
      "firstName": "Buntarou",
      "lastName": "Shizuki",
      "affiliations": []
    },
    {
      "id": 8229,
      "firstName": "Unaizah",
      "lastName": "Obaidellah",
      "affiliations": []
    },
    {
      "id": 14886,
      "firstName": "Vijay",
      "lastName": "Rajanna",
      "affiliations": []
    },
    {
      "id": 11303,
      "firstName": "Martin",
      "lastName": "Raubal",
      "affiliations": []
    },
    {
      "id": 11304,
      "firstName": "Sergio",
      "lastName": "Rodrigues",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 22568,
      "firstName": "Shaojie",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 17449,
      "firstName": "Martin",
      "lastName": "Weier",
      "affiliations": []
    },
    {
      "id": 9258,
      "firstName": "Michał",
      "lastName": "Olszanowski",
      "affiliations": []
    },
    {
      "id": 15915,
      "firstName": "Deepak",
      "lastName": "Akkil",
      "affiliations": []
    },
    {
      "id": 8239,
      "firstName": "Carlos",
      "lastName": "Morimoto",
      "middleInitial": "H.",
      "affiliations": []
    },
    {
      "id": 14896,
      "firstName": "Janus",
      "lastName": "Madsen",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 20528,
      "firstName": "Sarah",
      "lastName": "Hausmann",
      "affiliations": []
    },
    {
      "id": 15411,
      "firstName": "Niklas",
      "lastName": "Kleinhans",
      "affiliations": []
    },
    {
      "id": 16435,
      "firstName": "Paulo",
      "lastName": "Schor",
      "affiliations": []
    },
    {
      "id": 17971,
      "firstName": "Oleg",
      "lastName": "Spakov",
      "affiliations": []
    },
    {
      "id": 20532,
      "firstName": "Rogerio",
      "lastName": "de Paula",
      "affiliations": []
    },
    {
      "id": 8244,
      "firstName": "Sabina",
      "lastName": "Kasprowska",
      "affiliations": []
    },
    {
      "id": 20021,
      "firstName": "Itziar",
      "lastName": "Lozano",
      "affiliations": []
    },
    {
      "id": 18997,
      "firstName": "Dominic",
      "lastName": "Orth",
      "affiliations": []
    },
    {
      "id": 22070,
      "firstName": "Shahram",
      "lastName": "Eivazi",
      "affiliations": []
    },
    {
      "id": 10808,
      "firstName": "Madison",
      "lastName": "Le",
      "affiliations": []
    },
    {
      "id": 13881,
      "firstName": "Mahnaz",
      "lastName": "Behroozi",
      "affiliations": []
    },
    {
      "id": 22076,
      "firstName": "Carolina",
      "lastName": "Barzantny",
      "affiliations": []
    },
    {
      "id": 15422,
      "firstName": "Jutta",
      "lastName": "Hild",
      "affiliations": []
    },
    {
      "id": 15935,
      "firstName": "Tim",
      "lastName": "Stratmann",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 10817,
      "firstName": "Ruth",
      "lastName": "Campos",
      "affiliations": []
    },
    {
      "id": 11841,
      "firstName": "Steffen",
      "lastName": "Staab",
      "affiliations": []
    },
    {
      "id": 9805,
      "firstName": "Susanne",
      "lastName": "Boll",
      "affiliations": []
    },
    {
      "id": 13389,
      "firstName": "Jacek",
      "lastName": "Matulewski",
      "affiliations": []
    },
    {
      "id": 19534,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "affiliations": []
    },
    {
      "id": 16974,
      "firstName": "Mihai",
      "lastName": "Bâce",
      "affiliations": []
    },
    {
      "id": 11854,
      "firstName": "Lewis",
      "lastName": "Chuang",
      "affiliations": []
    },
    {
      "id": 19535,
      "firstName": "Juliane",
      "lastName": "Richter",
      "affiliations": []
    },
    {
      "id": 15440,
      "firstName": "Jimin",
      "lastName": "Pi",
      "affiliations": []
    },
    {
      "id": 8786,
      "firstName": "Norman",
      "lastName": "Peitek",
      "affiliations": []
    },
    {
      "id": 12882,
      "firstName": "Alessandro",
      "lastName": "Grillini",
      "affiliations": []
    },
    {
      "id": 23128,
      "firstName": "Matthew",
      "lastName": "Turk",
      "affiliations": []
    },
    {
      "id": 22105,
      "firstName": "Thies",
      "lastName": "Pfeiffer",
      "affiliations": []
    },
    {
      "id": 11354,
      "firstName": "Piercarlo",
      "lastName": "Dondi",
      "affiliations": []
    },
    {
      "id": 18522,
      "firstName": "Harri",
      "lastName": "Siirtola",
      "affiliations": []
    },
    {
      "id": 21082,
      "firstName": "Hansen",
      "lastName": "Dan",
      "affiliations": []
    },
    {
      "id": 14940,
      "firstName": "Drew",
      "lastName": "Guarnera",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 20060,
      "firstName": "Adam",
      "lastName": "Talian",
      "affiliations": []
    },
    {
      "id": 22113,
      "firstName": "Eva",
      "lastName": "Eggeling",
      "affiliations": []
    },
    {
      "id": 20577,
      "firstName": "Sheraz",
      "lastName": "Ahmed",
      "affiliations": []
    },
    {
      "id": 15457,
      "firstName": "Takashi",
      "lastName": "Nagamatsu",
      "affiliations": []
    },
    {
      "id": 11875,
      "firstName": "Christoph",
      "lastName": "Schaefer",
      "affiliations": []
    },
    {
      "id": 24166,
      "firstName": "Christoph",
      "lastName": "Strauch",
      "affiliations": []
    },
    {
      "id": 18022,
      "firstName": "Janet",
      "lastName": "Siegmund",
      "affiliations": []
    },
    {
      "id": 22633,
      "firstName": "Jacek",
      "lastName": "Gwizdka",
      "affiliations": []
    },
    {
      "id": 15465,
      "firstName": "Arantxa",
      "lastName": "Villanueva",
      "affiliations": []
    },
    {
      "id": 21610,
      "firstName": "Kuno",
      "lastName": "Kurzhals",
      "affiliations": []
    },
    {
      "id": 19563,
      "firstName": "Gábor",
      "lastName": "Sörös",
      "affiliations": []
    },
    {
      "id": 17518,
      "firstName": "Ching-Ting",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 12402,
      "firstName": "Chaitra",
      "lastName": "Yangandul",
      "affiliations": []
    },
    {
      "id": 17011,
      "firstName": "Sigrid",
      "lastName": "Klerke",
      "affiliations": []
    },
    {
      "id": 22131,
      "firstName": "Neil",
      "lastName": "Bruce",
      "affiliations": []
    },
    {
      "id": 19061,
      "firstName": "Argenis",
      "lastName": "Ramirez Gomez",
      "affiliations": []
    },
    {
      "id": 21109,
      "firstName": "Aaron",
      "lastName": "Gokaslan",
      "affiliations": []
    },
    {
      "id": 18038,
      "firstName": "Mohammed",
      "lastName": "Al Haek",
      "affiliations": []
    },
    {
      "id": 18039,
      "firstName": "Pieter",
      "lastName": "Potgieter",
      "middleInitial": "P.H.",
      "affiliations": []
    },
    {
      "id": 20088,
      "firstName": "Nilavra",
      "lastName": "Bhattacharya",
      "affiliations": []
    },
    {
      "id": 14970,
      "firstName": "Oliver",
      "lastName": "Le Meur",
      "affiliations": []
    },
    {
      "id": 16510,
      "firstName": "Preethi",
      "lastName": "Vaidyanathan",
      "affiliations": []
    },
    {
      "id": 10369,
      "firstName": "Andrew",
      "lastName": "Duchowski",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 9858,
      "firstName": "Tobias",
      "lastName": "Appel",
      "affiliations": []
    },
    {
      "id": 13443,
      "firstName": "Howell",
      "lastName": "Istance",
      "affiliations": []
    },
    {
      "id": 20100,
      "firstName": "Thomas",
      "lastName": "Kuebler",
      "affiliations": []
    },
    {
      "id": 15494,
      "firstName": "Ying",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 24199,
      "firstName": "Jiri",
      "lastName": "Lukavsky",
      "affiliations": []
    },
    {
      "id": 15496,
      "firstName": "Tong",
      "lastName": "Qin",
      "affiliations": []
    },
    {
      "id": 18059,
      "firstName": "Xucong",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 14476,
      "firstName": "Jozef",
      "lastName": "Tvarozek",
      "affiliations": []
    },
    {
      "id": 12941,
      "firstName": "Jochen",
      "lastName": "Kuhn",
      "affiliations": []
    },
    {
      "id": 11917,
      "firstName": "Nikolaos",
      "lastName": "Avouris",
      "affiliations": []
    },
    {
      "id": 11918,
      "firstName": "Mamoru",
      "lastName": "Hiroe",
      "affiliations": []
    },
    {
      "id": 9360,
      "firstName": "Tracy",
      "lastName": "Hammond",
      "affiliations": []
    },
    {
      "id": 21137,
      "firstName": "Thomas",
      "lastName": "Kübler",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 12946,
      "firstName": "Sandeep",
      "lastName": "Vidyapu",
      "affiliations": []
    },
    {
      "id": 16531,
      "firstName": "Pierre",
      "lastName": "Weill-Tessier",
      "affiliations": []
    },
    {
      "id": 17046,
      "firstName": "Korok",
      "lastName": "Sengupta",
      "affiliations": []
    },
    {
      "id": 17047,
      "firstName": "Ignace",
      "lastName": "Hooge",
      "middleInitial": "T.C.",
      "affiliations": []
    },
    {
      "id": 21143,
      "firstName": "Daniel",
      "lastName": "Ombelet",
      "affiliations": []
    },
    {
      "id": 16025,
      "firstName": "Michael Xuelin",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 23196,
      "firstName": "Daniel",
      "lastName": "Sonntag",
      "affiliations": []
    },
    {
      "id": 21149,
      "firstName": "Michiya",
      "lastName": "Yamamoto",
      "affiliations": []
    },
    {
      "id": 10910,
      "firstName": "Christian",
      "lastName": "Scharinger",
      "affiliations": []
    },
    {
      "id": 12959,
      "firstName": "Denis",
      "lastName": "Šefara",
      "affiliations": []
    },
    {
      "id": 22687,
      "firstName": "Anna",
      "lastName": "Eivazi",
      "affiliations": []
    },
    {
      "id": 18591,
      "firstName": "Kari-Jouko",
      "lastName": "Räihä",
      "affiliations": []
    },
    {
      "id": 15008,
      "firstName": "Hsing-fen",
      "lastName": "Tu",
      "affiliations": []
    },
    {
      "id": 16034,
      "firstName": "Matthias",
      "lastName": "Gamer",
      "affiliations": []
    },
    {
      "id": 10915,
      "firstName": "Izabela",
      "lastName": "Krejtz",
      "affiliations": []
    },
    {
      "id": 17574,
      "firstName": "Ayush",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 12454,
      "firstName": "Pavel",
      "lastName": "Orlov",
      "affiliations": []
    },
    {
      "id": 16040,
      "firstName": "Edward",
      "lastName": "Ryklin",
      "affiliations": []
    },
    {
      "id": 11945,
      "firstName": "Brent",
      "lastName": "Parsons",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 23722,
      "firstName": "Jeroen",
      "lastName": "Benjamins",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 18093,
      "firstName": "Kavita",
      "lastName": "Vemuri",
      "affiliations": []
    },
    {
      "id": 13998,
      "firstName": "Tanner",
      "lastName": "Person",
      "affiliations": []
    },
    {
      "id": 10414,
      "firstName": "Diederick",
      "lastName": "Niehorster",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 23215,
      "firstName": "Ioannis",
      "lastName": "Agtzidis",
      "affiliations": []
    },
    {
      "id": 11951,
      "firstName": "Kévin",
      "lastName": "BANNIER",
      "affiliations": []
    },
    {
      "id": 19122,
      "firstName": "Frederick",
      "lastName": "Shic",
      "affiliations": []
    },
    {
      "id": 10931,
      "firstName": "Marco",
      "lastName": "Porta",
      "affiliations": []
    },
    {
      "id": 8373,
      "firstName": "David",
      "lastName": "Geisler",
      "affiliations": []
    },
    {
      "id": 10422,
      "firstName": "Gabriel",
      "lastName": "Nyström",
      "affiliations": []
    },
    {
      "id": 13494,
      "firstName": "Juliana Jansen",
      "lastName": "Ferreira",
      "affiliations": []
    },
    {
      "id": 17591,
      "firstName": "Tobias",
      "lastName": "Schreck",
      "affiliations": []
    },
    {
      "id": 21687,
      "firstName": "Roman",
      "lastName": "Bednarik",
      "affiliations": []
    },
    {
      "id": 21691,
      "firstName": "Hanadi",
      "lastName": "Tamimi",
      "affiliations": []
    },
    {
      "id": 16059,
      "firstName": "Philipp",
      "lastName": "Müller",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 16576,
      "firstName": "Peyman",
      "lastName": "Toreini",
      "affiliations": []
    },
    {
      "id": 10433,
      "firstName": "Nora",
      "lastName": "Castner",
      "affiliations": []
    },
    {
      "id": 22721,
      "firstName": "Michael",
      "lastName": "Barz",
      "affiliations": []
    },
    {
      "id": 21699,
      "firstName": "Nina",
      "lastName": "Gehrer",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 12483,
      "firstName": "James",
      "lastName": "Tompkin",
      "affiliations": []
    },
    {
      "id": 15556,
      "firstName": "Mateusz",
      "lastName": "Kuchta",
      "affiliations": []
    },
    {
      "id": 17605,
      "firstName": "Uwe",
      "lastName": "Gruenefeld",
      "affiliations": []
    },
    {
      "id": 17607,
      "firstName": "Emil",
      "lastName": "Jacobsen",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 10952,
      "firstName": "Piotr",
      "lastName": "Bartczak",
      "affiliations": []
    },
    {
      "id": 24264,
      "firstName": "Mahmoud",
      "lastName": "Kalash",
      "affiliations": []
    },
    {
      "id": 17097,
      "firstName": "Julia",
      "lastName": "Kuosmanen",
      "affiliations": []
    },
    {
      "id": 10441,
      "firstName": "Tiia",
      "lastName": "Viitanen",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 20172,
      "firstName": "Will",
      "lastName": "Patera",
      "affiliations": []
    },
    {
      "id": 21708,
      "firstName": "Jouni",
      "lastName": "Viiri",
      "affiliations": []
    },
    {
      "id": 14030,
      "firstName": "Matthew",
      "lastName": "Lee",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 21198,
      "firstName": "Bertram",
      "lastName": "Shi",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 21711,
      "firstName": "Maryam",
      "lastName": "Keyvanara",
      "affiliations": []
    },
    {
      "id": 18639,
      "firstName": "Flavio",
      "lastName": "Coutinho",
      "middleInitial": "L.",
      "affiliations": []
    },
    {
      "id": 13520,
      "firstName": "Roy",
      "lastName": "Hessels",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 15061,
      "firstName": "Thomas",
      "lastName": "Schlegel",
      "affiliations": []
    },
    {
      "id": 16597,
      "firstName": "Taijirou",
      "lastName": "Shiraishi",
      "affiliations": []
    },
    {
      "id": 8923,
      "firstName": "Katarzyna",
      "lastName": "Harezlak",
      "affiliations": []
    },
    {
      "id": 10461,
      "firstName": "Geert",
      "lastName": "Savelsbergh",
      "affiliations": []
    },
    {
      "id": 20190,
      "firstName": "Chandan",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 17631,
      "firstName": "Outi",
      "lastName": "Tuisku",
      "affiliations": []
    },
    {
      "id": 14048,
      "firstName": "Moritz",
      "lastName": "Langner",
      "affiliations": []
    },
    {
      "id": 11489,
      "firstName": "Michael",
      "lastName": "Schönenberg",
      "affiliations": []
    },
    {
      "id": 9953,
      "firstName": "Pernilla",
      "lastName": "Qvarfordt",
      "affiliations": []
    },
    {
      "id": 10468,
      "firstName": "Marius",
      "lastName": "Rubo",
      "affiliations": []
    },
    {
      "id": 20197,
      "firstName": "Kai",
      "lastName": "Otto",
      "affiliations": []
    },
    {
      "id": 22250,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 14570,
      "firstName": "Jari",
      "lastName": "Kangas",
      "affiliations": []
    },
    {
      "id": 17643,
      "firstName": "Mathias",
      "lastName": "Trefzger",
      "affiliations": []
    },
    {
      "id": 22259,
      "firstName": "Raimondas",
      "lastName": "Zemblys",
      "affiliations": []
    },
    {
      "id": 18163,
      "firstName": "Adam",
      "lastName": "Bykowski",
      "affiliations": []
    },
    {
      "id": 17141,
      "firstName": "David",
      "lastName": "de Gómez Pérez",
      "middleInitial": "G.",
      "affiliations": []
    },
    {
      "id": 13045,
      "firstName": "Halszka",
      "lastName": "Jarodzka",
      "affiliations": []
    },
    {
      "id": 12534,
      "firstName": "Visajaani",
      "lastName": "Salonen",
      "affiliations": []
    },
    {
      "id": 19704,
      "firstName": "Rakshit",
      "lastName": "Kothari",
      "affiliations": []
    },
    {
      "id": 21754,
      "firstName": "Raymond",
      "lastName": "Bond",
      "affiliations": []
    },
    {
      "id": 18174,
      "firstName": "Noyan",
      "lastName": "Songur",
      "affiliations": []
    },
    {
      "id": 17662,
      "firstName": "Philipp",
      "lastName": "Slusallek",
      "affiliations": []
    },
    {
      "id": 23808,
      "firstName": "Otmar",
      "lastName": "Hilliges",
      "affiliations": []
    },
    {
      "id": 18177,
      "firstName": "Florian",
      "lastName": "Daiber",
      "affiliations": []
    },
    {
      "id": 19202,
      "firstName": "Mikhail",
      "lastName": "Startsev",
      "affiliations": []
    },
    {
      "id": 22787,
      "firstName": "Mohamed",
      "lastName": "Khamis",
      "affiliations": []
    },
    {
      "id": 17157,
      "firstName": "Patrick",
      "lastName": "Renner",
      "affiliations": []
    },
    {
      "id": 9990,
      "firstName": "Kouta",
      "lastName": "Minamizawa",
      "affiliations": []
    },
    {
      "id": 13063,
      "firstName": "Yuze",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 9479,
      "firstName": "Andrea",
      "lastName": "Kienle",
      "affiliations": []
    },
    {
      "id": 14600,
      "firstName": "Ryoma",
      "lastName": "Matsuo",
      "affiliations": []
    },
    {
      "id": 18184,
      "firstName": "Michael",
      "lastName": "Dorr",
      "affiliations": []
    },
    {
      "id": 17166,
      "firstName": "M.",
      "lastName": "de Boer",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 16654,
      "firstName": "Pavol",
      "lastName": "Navrat",
      "affiliations": []
    },
    {
      "id": 19214,
      "firstName": "Matthias",
      "lastName": "Roetting",
      "affiliations": []
    },
    {
      "id": 9488,
      "firstName": "Oleg",
      "lastName": "Komogortsev",
      "affiliations": []
    },
    {
      "id": 19216,
      "firstName": "Antti",
      "lastName": "Huotarinen",
      "affiliations": []
    },
    {
      "id": 17680,
      "firstName": "Nithiya",
      "lastName": "Uppara",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 10005,
      "firstName": "Haofei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 8982,
      "firstName": "Mercedes",
      "lastName": "Belinchón",
      "affiliations": []
    },
    {
      "id": 11032,
      "firstName": "Thorsten",
      "lastName": "Roth",
      "affiliations": []
    },
    {
      "id": 16153,
      "firstName": "Bibianna",
      "lastName": "Bałaj",
      "affiliations": []
    },
    {
      "id": 12569,
      "firstName": "Robert",
      "lastName": "Allison",
      "affiliations": []
    },
    {
      "id": 23321,
      "firstName": "Andrew",
      "lastName": "Kurauchi",
      "affiliations": []
    },
    {
      "id": 18716,
      "firstName": "Daniel",
      "lastName": "Weiskopf",
      "affiliations": []
    },
    {
      "id": 24348,
      "firstName": "Andreas",
      "lastName": "Dengel",
      "affiliations": []
    },
    {
      "id": 8988,
      "firstName": "Florian",
      "lastName": "Jungwirth",
      "affiliations": []
    },
    {
      "id": 16669,
      "firstName": "Kai",
      "lastName": "Dierkes",
      "affiliations": []
    },
    {
      "id": 19233,
      "firstName": "Ian",
      "lastName": "McChesney",
      "affiliations": []
    },
    {
      "id": 13091,
      "firstName": "Thérése",
      "lastName": "Eder",
      "affiliations": []
    },
    {
      "id": 8997,
      "firstName": "Ewelina",
      "lastName": "Marek",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 22822,
      "firstName": "Gisele",
      "lastName": "Gotardi",
      "affiliations": []
    },
    {
      "id": 9510,
      "firstName": "Hana",
      "lastName": "Vrzakova",
      "affiliations": []
    },
    {
      "id": 16681,
      "firstName": "Christian",
      "lastName": "Schlösser",
      "affiliations": []
    },
    {
      "id": 17194,
      "firstName": "Ion",
      "lastName": "Martinikorena",
      "affiliations": []
    },
    {
      "id": 19755,
      "firstName": "André",
      "lastName": "Brechmann",
      "affiliations": []
    },
    {
      "id": 15659,
      "firstName": "John",
      "lastName": "Harston",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 23341,
      "firstName": "Klaus",
      "lastName": "Mueller",
      "affiliations": []
    },
    {
      "id": 16174,
      "firstName": "Benedikt Werner",
      "lastName": "Hosp",
      "affiliations": []
    },
    {
      "id": 18734,
      "firstName": "Łukasz",
      "lastName": "Piasecki",
      "affiliations": []
    },
    {
      "id": 17198,
      "firstName": "Zheru",
      "lastName": "Chi",
      "affiliations": []
    },
    {
      "id": 14640,
      "firstName": "Pablo",
      "lastName": "Prietz",
      "affiliations": []
    },
    {
      "id": 9010,
      "firstName": "Jeff",
      "lastName": "Zornig",
      "affiliations": []
    },
    {
      "id": 24370,
      "firstName": "Hanna",
      "lastName": "Brinkmann",
      "affiliations": []
    },
    {
      "id": 20276,
      "firstName": "David",
      "lastName": "Rudi",
      "affiliations": []
    },
    {
      "id": 13110,
      "firstName": "Yusuke",
      "lastName": "Sugano",
      "affiliations": []
    },
    {
      "id": 18744,
      "firstName": "Nils",
      "lastName": "Rodrigues",
      "affiliations": []
    },
    {
      "id": 16185,
      "firstName": "Joachim",
      "lastName": "Spalink",
      "affiliations": []
    },
    {
      "id": 18745,
      "firstName": "Katarzyna",
      "lastName": "Wisiecka",
      "affiliations": []
    },
    {
      "id": 22330,
      "firstName": "Benedict",
      "lastName": "Fehringer",
      "middleInitial": "C. O. F.",
      "affiliations": []
    },
    {
      "id": 14140,
      "firstName": "Zofija",
      "lastName": "Tupikovskaja-Omovie",
      "affiliations": []
    },
    {
      "id": 15165,
      "firstName": "Niels",
      "lastName": "Peek",
      "affiliations": []
    },
    {
      "id": 20286,
      "firstName": "Manuele",
      "lastName": "Reani",
      "affiliations": []
    },
    {
      "id": 12096,
      "firstName": "Marcus",
      "lastName": "Nyström",
      "affiliations": []
    },
    {
      "id": 20800,
      "firstName": "Martin",
      "lastName": "Stridh",
      "affiliations": []
    },
    {
      "id": 10048,
      "firstName": "Vsevolod",
      "lastName": "Peysakhovich",
      "affiliations": []
    },
    {
      "id": 21825,
      "firstName": "Bonita",
      "lastName": "Sharif",
      "affiliations": []
    },
    {
      "id": 11073,
      "firstName": "Renato",
      "lastName": "Cerqueira",
      "affiliations": []
    },
    {
      "id": 19267,
      "firstName": "Paula",
      "lastName": "Polastri",
      "middleInitial": "F.",
      "affiliations": []
    },
    {
      "id": 15172,
      "firstName": "Martina",
      "lastName": "Navarro",
      "affiliations": []
    },
    {
      "id": 11588,
      "firstName": "Peter",
      "lastName": "Gerjets",
      "affiliations": []
    },
    {
      "id": 11590,
      "firstName": "Mickaël",
      "lastName": "Causse",
      "affiliations": []
    },
    {
      "id": 12104,
      "firstName": "Marketa",
      "lastName": "Beitlova",
      "affiliations": []
    },
    {
      "id": 13130,
      "firstName": "Thiago",
      "lastName": "Santini",
      "affiliations": []
    },
    {
      "id": 23370,
      "firstName": "Wolfgang",
      "lastName": "Rosenstiel",
      "affiliations": []
    },
    {
      "id": 16204,
      "firstName": "Chris",
      "lastName": "Parnin",
      "affiliations": []
    },
    {
      "id": 17228,
      "firstName": "Krzysztof",
      "lastName": "Krejtz",
      "affiliations": []
    },
    {
      "id": 24401,
      "firstName": "Markku",
      "lastName": "Hannula",
      "affiliations": []
    },
    {
      "id": 8530,
      "firstName": "Xinhong",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 16722,
      "firstName": "Jani",
      "lastName": "Koskinen",
      "affiliations": []
    },
    {
      "id": 21843,
      "firstName": "Fabian",
      "lastName": "Deitelhoff",
      "affiliations": []
    },
    {
      "id": 21847,
      "firstName": "Shane",
      "lastName": "Williams",
      "affiliations": []
    },
    {
      "id": 12120,
      "firstName": "Antonio",
      "lastName": "Tula",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 21339,
      "firstName": "Ali",
      "lastName": "Shafti",
      "affiliations": []
    },
    {
      "id": 11612,
      "firstName": "Filip",
      "lastName": "Dechterenko",
      "affiliations": []
    },
    {
      "id": 10589,
      "firstName": "Poika",
      "lastName": "Isokoski",
      "affiliations": []
    },
    {
      "id": 15711,
      "firstName": "Sander",
      "lastName": "Staal",
      "affiliations": []
    },
    {
      "id": 20319,
      "firstName": "Jiajun",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 19809,
      "firstName": "Shin",
      "lastName": "Takahashi",
      "affiliations": []
    },
    {
      "id": 18274,
      "firstName": "Syed",
      "lastName": "Bukhari",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 12643,
      "firstName": "Antti-Pekka",
      "lastName": "Elomaa",
      "affiliations": []
    },
    {
      "id": 17252,
      "firstName": "Benjamin",
      "lastName": "Outram",
      "affiliations": []
    },
    {
      "id": 22885,
      "firstName": "Jörgen",
      "lastName": "Thaung",
      "affiliations": []
    },
    {
      "id": 13670,
      "firstName": "V Vijaya",
      "lastName": "Saradhi",
      "affiliations": []
    },
    {
      "id": 23399,
      "firstName": "Sven",
      "lastName": "Apel",
      "affiliations": []
    },
    {
      "id": 22379,
      "firstName": "Pieter",
      "lastName": "Blignaut",
      "affiliations": []
    },
    {
      "id": 13675,
      "firstName": "Diako",
      "lastName": "Mardanbegi",
      "affiliations": []
    },
    {
      "id": 14699,
      "firstName": "Maria",
      "lastName": "Timoshenko",
      "affiliations": []
    },
    {
      "id": 13677,
      "firstName": "Tamara",
      "lastName": "Munzner",
      "affiliations": []
    },
    {
      "id": 12141,
      "firstName": "Veikko",
      "lastName": "Surakka",
      "affiliations": []
    },
    {
      "id": 21870,
      "firstName": "Linda",
      "lastName": "Cedli",
      "affiliations": []
    },
    {
      "id": 17775,
      "firstName": "Pushkar",
      "lastName": "Shukla",
      "affiliations": []
    },
    {
      "id": 8560,
      "firstName": "Soumy",
      "lastName": "Jacob",
      "affiliations": []
    },
    {
      "id": 21879,
      "firstName": "Samit",
      "lastName": "Bhattacharya",
      "affiliations": []
    },
    {
      "id": 17271,
      "firstName": "Caroline",
      "lastName": "Jay",
      "affiliations": []
    },
    {
      "id": 22905,
      "firstName": "Biju",
      "lastName": "Thankachan",
      "affiliations": []
    },
    {
      "id": 16250,
      "firstName": "Laura",
      "lastName": "Sesma-Sanchez",
      "affiliations": []
    },
    {
      "id": 20347,
      "firstName": "Min",
      "lastName": "Ke",
      "affiliations": []
    },
    {
      "id": 13181,
      "firstName": "Bernhard",
      "lastName": "Petersch",
      "affiliations": []
    },
    {
      "id": 19325,
      "firstName": "John",
      "lastName": "Hansen",
      "middleInitial": "P.",
      "affiliations": []
    },
    {
      "id": 13696,
      "firstName": "Pascal",
      "lastName": "Klein",
      "affiliations": []
    },
    {
      "id": 8579,
      "firstName": "Dillon",
      "lastName": "Lohr",
      "middleInitial": "J.",
      "affiliations": []
    },
    {
      "id": 14212,
      "firstName": "José",
      "lastName": "Leyva",
      "middleInitial": "A.T.",
      "affiliations": []
    },
    {
      "id": 12167,
      "firstName": "Eduardo",
      "lastName": "Velloso",
      "affiliations": []
    },
    {
      "id": 20872,
      "firstName": "Andrew",
      "lastName": "Begel",
      "affiliations": []
    },
    {
      "id": 13194,
      "firstName": "Alexandra",
      "lastName": "Papoutsaki",
      "affiliations": []
    },
    {
      "id": 8587,
      "firstName": "Raoul",
      "lastName": "Oudejans",
      "affiliations": []
    },
    {
      "id": 19339,
      "firstName": "Samuel-Hunter",
      "lastName": "Berndt",
      "affiliations": []
    },
    {
      "id": 16269,
      "firstName": "Lukas",
      "lastName": "Greiter",
      "affiliations": []
    },
    {
      "id": 16271,
      "firstName": "Jonas",
      "lastName": "Blattgerste",
      "affiliations": []
    },
    {
      "id": 16784,
      "firstName": "Rudolf",
      "lastName": "Netzel",
      "affiliations": []
    },
    {
      "id": 8592,
      "firstName": "Aldo",
      "lastName": "Faisal",
      "affiliations": []
    },
    {
      "id": 23953,
      "firstName": "Miika",
      "lastName": "Toivanen",
      "affiliations": []
    },
    {
      "id": 16273,
      "firstName": "Jan",
      "lastName": "Petruzalek",
      "affiliations": []
    },
    {
      "id": 13202,
      "firstName": "Scott",
      "lastName": "MacKenzie",
      "affiliations": []
    },
    {
      "id": 15762,
      "firstName": "Jürgen",
      "lastName": "Beyerer",
      "affiliations": []
    },
    {
      "id": 15251,
      "firstName": "Kerstin",
      "lastName": "Wolf",
      "affiliations": []
    },
    {
      "id": 21398,
      "firstName": "Anke",
      "lastName": "Huckauf",
      "affiliations": []
    },
    {
      "id": 15254,
      "firstName": "Shoya",
      "lastName": "Ishimaru",
      "affiliations": []
    },
    {
      "id": 10135,
      "firstName": "Raphael",
      "lastName": "Menges",
      "affiliations": []
    },
    {
      "id": 14235,
      "firstName": "Minoru",
      "lastName": "Nakayama",
      "affiliations": []
    },
    {
      "id": 23452,
      "firstName": "Vagner",
      "lastName": "Santana",
      "middleInitial": "F. de",
      "affiliations": []
    },
    {
      "id": 16798,
      "firstName": "Ioannis",
      "lastName": "Giannopoulos",
      "affiliations": []
    },
    {
      "id": 17311,
      "firstName": "Birte",
      "lastName": "Gestefeld",
      "affiliations": []
    },
    {
      "id": 10656,
      "firstName": "Christina",
      "lastName": "Katsini",
      "affiliations": []
    },
    {
      "id": 11682,
      "firstName": "Frans",
      "lastName": "Cornelissen",
      "middleInitial": "W.",
      "affiliations": []
    },
    {
      "id": 19363,
      "firstName": "Benedikt",
      "lastName": "Schröder",
      "affiliations": []
    },
    {
      "id": 21412,
      "firstName": "John",
      "lastName": "van der Kamp",
      "affiliations": []
    },
    {
      "id": 20391,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "affiliations": []
    },
    {
      "id": 19880,
      "firstName": "Andoni",
      "lastName": "Larumbe",
      "affiliations": []
    },
    {
      "id": 14760,
      "firstName": "Jeff",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 9129,
      "firstName": "D.",
      "lastName": "Başkent",
      "affiliations": []
    },
    {
      "id": 10156,
      "firstName": "Carlos",
      "lastName": "Elmadjian",
      "affiliations": []
    },
    {
      "id": 13744,
      "firstName": "Fabian",
      "lastName": "Huettig",
      "affiliations": []
    },
    {
      "id": 12720,
      "firstName": "Florian",
      "lastName": "Alt",
      "affiliations": []
    },
    {
      "id": 12725,
      "firstName": "Michael",
      "lastName": "Raschke",
      "affiliations": []
    },
    {
      "id": 13750,
      "firstName": "Vanessa",
      "lastName": "Putnam",
      "affiliations": []
    },
    {
      "id": 12726,
      "firstName": "Wolfgang",
      "lastName": "Fuhl",
      "affiliations": []
    },
    {
      "id": 8633,
      "firstName": "Nelson",
      "lastName": "Silva",
      "middleInitial": "J. S.",
      "affiliations": []
    },
    {
      "id": 15290,
      "firstName": "Aditi",
      "lastName": "Mavalankar",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 18363,
      "firstName": "Raphael",
      "lastName": "Rosenberg",
      "affiliations": []
    },
    {
      "id": 16318,
      "firstName": "Seonwook",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 16833,
      "firstName": "André",
      "lastName": "Hinkenjann",
      "affiliations": []
    },
    {
      "id": 18882,
      "firstName": "Richard",
      "lastName": "Ivry",
      "affiliations": []
    },
    {
      "id": 14787,
      "firstName": "Toshiyuki",
      "lastName": "Ando",
      "affiliations": []
    },
    {
      "id": 19912,
      "firstName": "Christos",
      "lastName": "Fidas",
      "affiliations": []
    },
    {
      "id": 15817,
      "firstName": "Ulf",
      "lastName": "Löfberg",
      "affiliations": []
    },
    {
      "id": 15306,
      "firstName": "Toshiya",
      "lastName": "Isomoto",
      "affiliations": []
    },
    {
      "id": 9675,
      "firstName": "Mauro",
      "lastName": "Mosconi",
      "affiliations": []
    },
    {
      "id": 16844,
      "firstName": "Maximilian",
      "lastName": "Maurer",
      "affiliations": []
    },
    {
      "id": 10703,
      "firstName": "Paweł",
      "lastName": "Holas",
      "affiliations": []
    },
    {
      "id": 20432,
      "firstName": "Eakta",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 11219,
      "firstName": "Julian",
      "lastName": "Steil",
      "affiliations": []
    },
    {
      "id": 18389,
      "firstName": "Alexander",
      "lastName": "Maedche",
      "affiliations": []
    },
    {
      "id": 23510,
      "firstName": "Yun Suen",
      "lastName": "Pai",
      "affiliations": []
    },
    {
      "id": 19414,
      "firstName": "Maurice",
      "lastName": "Koch",
      "affiliations": []
    },
    {
      "id": 21464,
      "firstName": "Susana",
      "lastName": "Martinez-Conde",
      "affiliations": []
    },
    {
      "id": 9176,
      "firstName": "Mehul",
      "lastName": "Bhatt",
      "affiliations": []
    },
    {
      "id": 8664,
      "firstName": "Luise",
      "lastName": "Reitstätter",
      "affiliations": []
    },
    {
      "id": 15832,
      "firstName": "Raynold",
      "lastName": "Bailey",
      "affiliations": []
    },
    {
      "id": 21977,
      "firstName": "Marc",
      "lastName": "Tonsen",
      "affiliations": []
    },
    {
      "id": 23514,
      "firstName": "Seyyed Saleh",
      "lastName": "Mozaffari Chanijani",
      "affiliations": []
    },
    {
      "id": 23003,
      "firstName": "Christophe",
      "lastName": "Lounis",
      "affiliations": []
    },
    {
      "id": 13787,
      "firstName": "Michael",
      "lastName": "Haslgrübler",
      "affiliations": []
    },
    {
      "id": 9179,
      "firstName": "Sabine",
      "lastName": "Dziemian",
      "affiliations": []
    },
    {
      "id": 15837,
      "firstName": "Wivine",
      "lastName": "Blekic",
      "affiliations": []
    },
    {
      "id": 18912,
      "firstName": "Dieter",
      "lastName": "W. Fellner",
      "affiliations": []
    },
    {
      "id": 14304,
      "firstName": "Anders",
      "lastName": "Kingbäck",
      "affiliations": []
    },
    {
      "id": 23520,
      "firstName": "Helmut",
      "lastName": "Leder",
      "affiliations": []
    },
    {
      "id": 9697,
      "firstName": "Jan-Bernard",
      "lastName": "Marsman",
      "affiliations": []
    },
    {
      "id": 8674,
      "firstName": "Stanislav",
      "lastName": "Popelka",
      "affiliations": []
    },
    {
      "id": 15843,
      "firstName": "Jan",
      "lastName": "Pilzer",
      "affiliations": []
    },
    {
      "id": 9188,
      "firstName": "Szymon",
      "lastName": "Kupiński",
      "affiliations": []
    },
    {
      "id": 12260,
      "firstName": "Katharina",
      "lastName": "Scheiter",
      "affiliations": []
    },
    {
      "id": 14822,
      "firstName": "Moritz",
      "lastName": "Kassner",
      "affiliations": []
    },
    {
      "id": 15334,
      "firstName": "Per",
      "lastName": "Bækgaard",
      "affiliations": []
    },
    {
      "id": 16358,
      "firstName": "Włodzisław",
      "lastName": "Duch",
      "affiliations": []
    },
    {
      "id": 19432,
      "firstName": "Michael",
      "lastName": "Burch",
      "affiliations": []
    },
    {
      "id": 18920,
      "firstName": "Iyad",
      "lastName": "Aldaqre",
      "affiliations": []
    },
    {
      "id": 21482,
      "firstName": "Martin",
      "lastName": "Kabeláč",
      "affiliations": []
    },
    {
      "id": 13804,
      "firstName": "Hans",
      "lastName": "Gellersen",
      "affiliations": []
    },
    {
      "id": 15852,
      "firstName": "Sonia",
      "lastName": "Porta",
      "affiliations": []
    },
    {
      "id": 12270,
      "firstName": "Chaiyawan",
      "lastName": "Auepanwiriyakul",
      "affiliations": []
    },
    {
      "id": 20974,
      "firstName": "Zhe",
      "lastName": "Zeng",
      "affiliations": []
    },
    {
      "id": 9199,
      "firstName": "Enkelejda",
      "lastName": "Kasneci",
      "affiliations": []
    },
    {
      "id": 14831,
      "firstName": "Fabian",
      "lastName": "Göbel",
      "affiliations": []
    },
    {
      "id": 11248,
      "firstName": "Carsten",
      "lastName": "Schulte",
      "affiliations": []
    },
    {
      "id": 12786,
      "firstName": "Robert",
      "lastName": "Cavin",
      "affiliations": []
    },
    {
      "id": 13810,
      "firstName": "Rijul",
      "lastName": "Soans",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 20468,
      "firstName": "Marek",
      "lastName": "Franek",
      "affiliations": []
    },
    {
      "id": 11254,
      "firstName": "Martin",
      "lastName": "Konopka",
      "affiliations": []
    },
    {
      "id": 9721,
      "firstName": "Sachin",
      "lastName": "Paryani",
      "affiliations": []
    },
    {
      "id": 14329,
      "firstName": "Alois",
      "lastName": "Ferscha",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 18,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 13:50:00+00"
  }
}