{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10033,
    "startDate": 1561420800000,
    "endDate": 1561680000000,
    "shortName": "ETRA",
    "name": "ETRA 2019",
    "year": 2019,
    "fullName": "ETRA '19: 2019 Symposium on Eye Tracking Research and Applications",
    "url": "https://etra.acm.org/2019/index.html",
    "location": "Denver,CO, USA",
    "timeZoneOffset": -360,
    "logoUrl": "https://files.sigchi.org/conference/logo/499b1f90-10df-36cf-b08f-441c60e28fbd.png",
    "timeZoneName": "America/Denver"
  },
  "sponsors": [
    {
      "id": 10025,
      "name": "SIGCHI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/a9179b35-d74e-b3d5-ad5c-2ec0a0ad6773.png",
      "levelId": 10032,
      "order": 4
    },
    {
      "id": 10026,
      "name": "ACMSIGGRAPH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/a9a7c2ed-fc08-72cd-4520-0901f36180fc.png",
      "levelId": 10032,
      "order": 2
    },
    {
      "id": 10028,
      "name": "IMOTIONS",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/a758e97b-f86b-0649-0feb-3c8f7c9e30a2.png",
      "levelId": 10033,
      "order": 6
    },
    {
      "id": 10029,
      "name": "PUPIL LABS",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/75731401-5b93-9ff1-d573-aede522b9d5d.png",
      "levelId": 10033,
      "order": 7
    },
    {
      "id": 10030,
      "name": "TOBII PRO",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/bb13c82f-5b3c-45ae-93b0-7c4c266d7bb2.png",
      "levelId": 10033,
      "order": 8
    },
    {
      "id": 10031,
      "name": "SR RESEARCH INC.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/4ea563cc-b95a-4149-16fb-6893a84e8726.png",
      "levelId": 10034,
      "order": 14
    },
    {
      "id": 10032,
      "name": "FOVE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/25f00d3b-6db8-7731-3385-286666f4818b.png",
      "levelId": 10034,
      "order": 11
    },
    {
      "id": 10033,
      "name": "FACEBOOK REALITY LABS",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/9e8fa536-1ea4-f79a-1059-39a6301cd0f3.png",
      "levelId": 10034,
      "order": 10
    },
    {
      "id": 10034,
      "name": "OCULUS",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/a6d8def6-8289-9afa-307b-05ae99f6d5a0.png",
      "levelId": 10034,
      "order": 12
    },
    {
      "id": 10035,
      "name": "VPixx Technologies Inc.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/c757f98e-194d-843c-01e4-ed99de9adcff.png",
      "levelId": 10034,
      "order": 16
    },
    {
      "id": 10036,
      "name": "AdHawk microsystem",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/aa6cdb40-aa14-deca-0a6d-084d0a964b17.png",
      "levelId": 10034,
      "order": 9
    },
    {
      "id": 10027,
      "name": "COLLEGE OF ENGINEERING UNIVERSITY OF FLORIDA",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/1e739470-2e52-200a-2d12-9c777dd29198.png",
      "levelId": 10032,
      "order": 3
    },
    {
      "id": 10037,
      "name": "SMART EYE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/d0091fbd-af16-4e2d-7db6-5486a7923d91.png",
      "levelId": 10034,
      "order": 13
    },
    {
      "id": 10038,
      "name": "University of Florida",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/b2e65a4b-02d2-adf7-9842-0793fdf9335b.png",
      "levelId": 10034,
      "order": 15
    },
    {
      "id": 10039,
      "name": "GAZEPOINT",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/5560b7d1-1077-7bb8-c7df-faf1deceaae6.png",
      "levelId": 10035,
      "order": 17
    },
    {
      "id": 10041,
      "name": "ACM Association for Computer Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/a285237b-ba89-a8e1-32af-bdb2023c508f.png",
      "levelId": 10032,
      "order": 1
    },
    {
      "id": 10042,
      "name": "University of Tubingen",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/8f8c592d-f3c6-512c-a17c-2584259e0ab2.png",
      "levelId": 10032,
      "order": 5
    }
  ],
  "sponsorLevels": [
    {
      "id": 10032,
      "name": "INSTITUTIONAL",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10033,
      "name": "PLATINUM",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10034,
      "name": "GOLD",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10035,
      "name": "SILVER",
      "rank": 4,
      "isDefault": false
    },
    {
      "id": 10031,
      "name": "Sponsors",
      "rank": 5,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10059,
      "name": "conference space",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/3d5af98b-913b-84ee-be66-a21712546843.png",
      "roomIds": [
        10249,
        10245,
        10244,
        10253,
        10251,
        10250,
        10242,
        10252,
        10243
      ]
    },
    {
      "id": 10060,
      "name": "Exhibitors' Space",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/3cea2582-ac99-534a-a271-03d9bbc906f6.png"
    }
  ],
  "rooms": [
    {
      "id": 10249,
      "name": "Ballroom Foyer",
      "typeId": 11375,
      "setup": "Theatre",
      "capacity": "200 people"
    },
    {
      "id": 10245,
      "name": "Ellingwood A&B",
      "typeId": 11416,
      "setup": "Special",
      "capacity": "200 people"
    },
    {
      "id": 10244,
      "name": "Humboldt",
      "typeId": 11372,
      "setup": "Rounds",
      "capacity": "70 people"
    },
    {
      "id": 10253,
      "name": "Lockwood",
      "typeId": 11375,
      "setup": "Special",
      "capacity": "200 people"
    },
    {
      "id": 10251,
      "name": "Oxford",
      "typeId": 11372,
      "setup": "Rounds",
      "capacity": "70 people"
    },
    {
      "id": 10250,
      "name": "Pikes Peak",
      "typeId": 11372,
      "setup": "Rounds",
      "capacity": "70 people"
    },
    {
      "id": 10242,
      "name": "Red Cloud\n",
      "typeId": 11374,
      "setup": "Classroom",
      "capacity": "36 people"
    },
    {
      "id": 10252,
      "name": "Torrey's",
      "typeId": 11372,
      "setup": "Rounds",
      "capacity": "40 people"
    },
    {
      "id": 10243,
      "name": "Oxford / Humboldt / Pikes Peak\n",
      "typeId": 11373,
      "setup": "Theatre",
      "capacity": "200 people"
    }
  ],
  "tracks": [
    {
      "id": 10553,
      "typeId": 11374
    },
    {
      "id": 10562,
      "typeId": 11402
    },
    {
      "id": 10559,
      "typeId": 11372
    },
    {
      "id": 10566,
      "typeId": 11411
    },
    {
      "id": 10565,
      "typeId": 11373
    },
    {
      "id": 10552,
      "typeId": 11407
    },
    {
      "id": 10550,
      "typeId": 11401
    },
    {
      "id": 10555,
      "typeId": 11372
    },
    {
      "id": 10557,
      "typeId": 11372
    },
    {
      "id": 10556,
      "typeId": 11372
    },
    {
      "id": 10558,
      "typeId": 11372
    },
    {
      "id": 10564,
      "typeId": 11372
    },
    {
      "id": 10551,
      "typeId": 11410
    },
    {
      "id": 10563,
      "typeId": 11369
    },
    {
      "id": 10560,
      "typeId": 11404
    },
    {
      "id": 10561,
      "typeId": 11371
    },
    {
      "id": 10554,
      "typeId": 11406
    }
  ],
  "contentTypes": [
    {
      "id": 11410,
      "name": "Fast Forward",
      "color": "#90979a",
      "duration": 2
    },
    {
      "id": 11411,
      "name": "General Discussion",
      "color": "#90979a",
      "duration": 20
    },
    {
      "id": 11416,
      "name": "Sponsors presentations",
      "color": "#90979a",
      "duration": 0
    },
    {
      "id": 11401,
      "name": "Doctoral Symposium",
      "color": "#90979a",
      "duration": 30
    },
    {
      "id": 11402,
      "name": "Tutorial",
      "color": "#90979a",
      "duration": 120,
      "displayName": "Tutorials"
    },
    {
      "id": 11404,
      "name": "Keynote Talk",
      "color": "#90979a",
      "duration": 60
    },
    {
      "id": 11412,
      "name": "Lunch",
      "color": "#90979a",
      "duration": 0
    },
    {
      "id": 11406,
      "name": "Poster",
      "color": "#90979a",
      "duration": 4,
      "displayName": "Posters"
    },
    {
      "id": 11407,
      "name": "Demo&Video",
      "color": "#90979a",
      "duration": 8
    },
    {
      "id": 11366,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11367,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11368,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11370,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11374,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 60,
      "displayName": "Workshops"
    },
    {
      "id": 11375,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 180,
      "displayName": "Events"
    },
    {
      "id": 11371,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 60,
      "displayName": "Panels"
    },
    {
      "id": 11369,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 60,
      "displayName": "Invited Talks"
    },
    {
      "id": 11373,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 120
    },
    {
      "id": 11372,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    }
  ],
  "timeSlots": [
    {
      "id": 10962,
      "type": "SESSION",
      "startDate": 1561449600000,
      "endDate": 1561456800000
    },
    {
      "id": 10963,
      "type": "BREAK",
      "startDate": 1561456800000,
      "endDate": 1561458600000
    },
    {
      "id": 10964,
      "type": "SESSION",
      "startDate": 1561458600000,
      "endDate": 1561465800000
    },
    {
      "id": 10965,
      "type": "LUNCH",
      "startDate": 1561465800000,
      "endDate": 1561469400000
    },
    {
      "id": 10966,
      "type": "SESSION",
      "startDate": 1561469400000,
      "endDate": 1561476600000
    },
    {
      "id": 10967,
      "type": "BREAK",
      "startDate": 1561476600000,
      "endDate": 1561478400000
    },
    {
      "id": 10968,
      "type": "SESSION",
      "startDate": 1561478400000,
      "endDate": 1561485600000
    },
    {
      "id": 10990,
      "type": "SESSION",
      "startDate": 1561660200000,
      "endDate": 1561671000000
    },
    {
      "id": 10976,
      "type": "SESSION",
      "startDate": 1561572000000,
      "endDate": 1561582800000
    },
    {
      "id": 10983,
      "type": "SESSION",
      "startDate": 1561624200000,
      "endDate": 1561627800000
    },
    {
      "id": 10984,
      "type": "BREAK",
      "startDate": 1561627800000,
      "endDate": 1561629600000
    },
    {
      "id": 10969,
      "type": "SESSION",
      "startDate": 1561537800000,
      "endDate": 1561541400000
    },
    {
      "id": 10970,
      "type": "BREAK",
      "startDate": 1561541400000,
      "endDate": 1561543200000
    },
    {
      "id": 10971,
      "type": "SESSION",
      "startDate": 1561543200000,
      "endDate": 1561550400000
    },
    {
      "id": 10972,
      "type": "LUNCH",
      "startDate": 1561550400000,
      "endDate": 1561554000000
    },
    {
      "id": 10973,
      "type": "SESSION",
      "startDate": 1561554000000,
      "endDate": 1561561200000
    },
    {
      "id": 10974,
      "type": "BREAK",
      "startDate": 1561561200000,
      "endDate": 1561563000000
    },
    {
      "id": 10975,
      "type": "SESSION",
      "startDate": 1561563000000,
      "endDate": 1561570200000
    },
    {
      "id": 10985,
      "type": "SESSION",
      "startDate": 1561629600000,
      "endDate": 1561636800000
    },
    {
      "id": 10986,
      "type": "LUNCH",
      "startDate": 1561636800000,
      "endDate": 1561640400000
    },
    {
      "id": 10987,
      "type": "SESSION",
      "startDate": 1561640400000,
      "endDate": 1561647600000
    },
    {
      "id": 10988,
      "type": "BREAK",
      "startDate": 1561647600000,
      "endDate": 1561649400000
    },
    {
      "id": 10989,
      "type": "SESSION",
      "startDate": 1561649400000,
      "endDate": 1561656600000
    },
    {
      "id": 10977,
      "type": "SESSION",
      "startDate": 1561710600000,
      "endDate": 1561714200000
    },
    {
      "id": 10978,
      "type": "BREAK",
      "startDate": 1561714200000,
      "endDate": 1561716000000
    },
    {
      "id": 10979,
      "type": "SESSION",
      "startDate": 1561716000000,
      "endDate": 1561723200000
    },
    {
      "id": 10980,
      "type": "LUNCH",
      "startDate": 1561723200000,
      "endDate": 1561726800000
    },
    {
      "id": 10981,
      "type": "SESSION",
      "startDate": 1561726800000,
      "endDate": 1561734000000
    },
    {
      "id": 10982,
      "type": "BREAK",
      "startDate": 1561734000000,
      "endDate": 1561735800000
    }
  ],
  "sessions": [
    {
      "id": 1373,
      "name": "ETRA Session 4: Head-Mounted Eye Tracking",
      "typeId": 11372,
      "roomId": 10243,
      "chairIds": [
        23963
      ],
      "contentIds": [
        3450,
        5496,
        4779,
        8108,
        2735
      ],
      "timeSlotId": 10987
    },
    {
      "id": 2220,
      "name": "ETRA Session 6: Privacy, Authentication, Fitts of Skill",
      "typeId": 11372,
      "roomId": 10243,
      "chairIds": [
        16851
      ],
      "contentIds": [
        7963,
        4068,
        4835,
        7418,
        3218
      ],
      "timeSlotId": 10979
    },
    {
      "id": 1667,
      "name": "ETVIS Session 1: Visualization Tools and Techniques",
      "typeId": 11372,
      "roomId": 10252,
      "chairIds": [
        10978
      ],
      "contentIds": [
        4651,
        7777,
        4560,
        6768
      ],
      "timeSlotId": 10987
    },
    {
      "id": 1807,
      "name": " Doctoral Symposium S3",
      "typeId": 11401,
      "roomId": 10252,
      "chairIds": [
        11179,
        13891,
        14373
      ],
      "contentIds": [
        5895,
        4656,
        7866
      ],
      "timeSlotId": 10966
    },
    {
      "id": 1974,
      "name": "Keynote Address by Enkelejda Kasneci",
      "typeId": 11404,
      "roomId": 10243,
      "chairIds": [
        16805,
        14118
      ],
      "contentIds": [
        4053
      ],
      "timeSlotId": 10983
    },
    {
      "id": 1062,
      "name": "Tutorial 3: part 1",
      "typeId": 11402,
      "roomId": 10242,
      "chairIds": [
        19079
      ],
      "contentIds": [
        6115
      ],
      "timeSlotId": 10962
    },
    {
      "id": 1798,
      "name": "Sponsor Workshop by Tobii Pro",
      "typeId": 11374,
      "roomId": 10251,
      "chairIds": [
        15160,
        9468
      ],
      "contentIds": [
        5593
      ],
      "timeSlotId": 10973
    },
    {
      "id": 1527,
      "name": "Fast Forward Session (16:15-17:30)",
      "typeId": 11410,
      "roomId": 10244,
      "chairIds": [
        9418,
        22026
      ],
      "contentIds": [
        3377,
        7765,
        5891,
        5880,
        7614,
        7105,
        5442,
        4265,
        6356,
        7102,
        4821,
        3994,
        5610,
        8001,
        3820,
        5291,
        4445,
        6307,
        3620,
        6267,
        5572,
        6886,
        6409,
        4517,
        7361,
        3524,
        7679,
        4783,
        4841,
        3551,
        4402,
        4687,
        6568,
        8009,
        5032,
        5775,
        5722,
        4747
      ],
      "timeSlotId": 10975
    },
    {
      "id": 1150,
      "name": "ETVIS Session 2: Visual Scanpath Comparison",
      "typeId": 11372,
      "roomId": 10252,
      "chairIds": [
        9746
      ],
      "contentIds": [
        5797,
        5689,
        6546,
        5545
      ],
      "timeSlotId": 10989
    },
    {
      "id": 1732,
      "name": "Opening Session  & Keynote Address by Oleg Komogortsev",
      "typeId": 11404,
      "roomId": 10243,
      "chairIds": [
        16805,
        14118
      ],
      "contentIds": [
        2699
      ],
      "timeSlotId": 10969
    },
    {
      "id": 1093,
      "name": "Privacy Issues of Ubiguity Eye Tracking",
      "typeId": 11371,
      "roomId": 10243,
      "chairIds": [
        15225,
        9418
      ],
      "contentIds": [
        6888
      ],
      "timeSlotId": 10977
    },
    {
      "id": 1516,
      "name": "Town Hall Meeting & ETRA 2020 Introduction",
      "typeId": 11373,
      "roomId": 10243,
      "chairIds": [
        20549
      ],
      "contentIds": [
        5284
      ],
      "timeSlotId": 10981
    },
    {
      "id": 1634,
      "name": "COGAIN Session 2",
      "typeId": 11372,
      "roomId": 10250,
      "chairIds": [
        22642
      ],
      "contentIds": [
        3371,
        2859,
        6654,
        3651,
        6635,
        4808
      ],
      "timeSlotId": 10973
    },
    {
      "id": 2239,
      "name": "ETRA Session 2: Calibration, Cognition, Smartphones, & Sequences",
      "typeId": 11372,
      "roomId": 10244,
      "chairIds": [
        11239
      ],
      "contentIds": [
        4341,
        6679,
        6107,
        4375,
        3802
      ],
      "timeSlotId": 10973
    },
    {
      "id": 1616,
      "name": "ETRA Challenge Session",
      "typeId": 11372,
      "roomId": 10252,
      "chairIds": [
        20595,
        20774
      ],
      "contentIds": [
        3498,
        6756,
        7397,
        8123
      ],
      "timeSlotId": 10979
    },
    {
      "id": 1661,
      "name": "Tutorial 3: part 2",
      "typeId": 11402,
      "roomId": 10242,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        7168
      ],
      "timeSlotId": 10964
    },
    {
      "id": 2230,
      "name": "Tutorial 4: part 2",
      "typeId": 11402,
      "roomId": 10242,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        4952
      ],
      "timeSlotId": 10968
    },
    {
      "id": 2450,
      "name": "Reception, Poster Session, Video & Demo Session",
      "typeId": 11406,
      "roomId": 10245,
      "chairIds": [
        22026,
        9418
      ],
      "contentIds": [
        7472,
        6263,
        5873,
        5058,
        8155,
        5199,
        6540,
        5580,
        4675,
        3728,
        3966,
        4382,
        5140,
        4395,
        7377,
        4976,
        6522,
        4605,
        4592,
        6426,
        6296,
        7324,
        5737,
        5053,
        7050,
        2897,
        3583,
        3387,
        3805,
        7143,
        7746,
        5820,
        6354,
        7824,
        5399,
        4908,
        6849,
        5034
      ],
      "timeSlotId": 10976
    },
    {
      "id": 2252,
      "name": "COGAIN Session 1",
      "typeId": 11372,
      "roomId": 10250,
      "chairIds": [
        22026
      ],
      "contentIds": [
        7031,
        6531,
        3143,
        4866
      ],
      "timeSlotId": 10971
    },
    {
      "id": 2184,
      "name": "ETRA Session 1: Deep Learning, Paths, Transitions & Pursuits",
      "typeId": 11372,
      "roomId": 10244,
      "chairIds": [
        15225
      ],
      "contentIds": [
        4864,
        2997,
        7876,
        7284,
        2845
      ],
      "timeSlotId": 10971
    },
    {
      "id": 2374,
      "name": "ETWEB Session 1: Eye Tracking for the Web",
      "typeId": 11372,
      "roomId": 10252,
      "chairIds": [
        18308
      ],
      "contentIds": [
        3267,
        6680,
        6829,
        4823,
        4949
      ],
      "timeSlotId": 10985
    },
    {
      "id": 1089,
      "name": "Sponsor Workshop by SR Research",
      "typeId": 11374,
      "roomId": 10242,
      "chairIds": [
        15160,
        9468
      ],
      "contentIds": [
        4975
      ],
      "timeSlotId": 10985
    },
    {
      "id": 1090,
      "name": "Tutorial 1: part 1",
      "typeId": 11402,
      "roomId": 10243,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        5698
      ],
      "timeSlotId": 10962
    },
    {
      "id": 1487,
      "name": "Tutorial 1: part 2",
      "typeId": 11402,
      "roomId": 10243,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        6068
      ],
      "timeSlotId": 10964
    },
    {
      "id": 1559,
      "name": "Tutorial 2: part 1",
      "typeId": 11402,
      "roomId": 10243,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        2937
      ],
      "timeSlotId": 10966
    },
    {
      "id": 2099,
      "name": "Tutorial 2: part 2",
      "typeId": 11402,
      "roomId": 10243,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        6284
      ],
      "timeSlotId": 10968
    },
    {
      "id": 2431,
      "name": "Tutorial 4: part 1",
      "typeId": 11402,
      "roomId": 10242,
      "chairIds": [
        19079,
        21249
      ],
      "contentIds": [
        6198
      ],
      "timeSlotId": 10966
    },
    {
      "id": 2060,
      "name": "ETRA Session 3: Vis & Programming",
      "typeId": 11372,
      "roomId": 10243,
      "chairIds": [
        20549
      ],
      "contentIds": [
        5844,
        6609,
        4219,
        6287,
        7193
      ],
      "timeSlotId": 10985
    },
    {
      "id": 1666,
      "name": "ETRA Session 5: Gaze Detection and Prediction",
      "typeId": 11372,
      "roomId": 10243,
      "chairIds": [
        19815
      ],
      "contentIds": [
        7692,
        6822,
        5408,
        3386,
        6485
      ],
      "timeSlotId": 10989
    },
    {
      "id": 1081,
      "name": "ET4S Session 1: Eye Tracking for Spatial Research",
      "typeId": 11372,
      "roomId": 10250,
      "chairIds": [
        22641
      ],
      "contentIds": [
        7257,
        6817,
        5148,
        4533,
        6757,
        3965
      ],
      "timeSlotId": 10975
    },
    {
      "id": 1431,
      "name": "Sponsor Workshop by Facebook Reality Labs",
      "typeId": 11374,
      "roomId": 10251,
      "chairIds": [
        15160,
        9468
      ],
      "contentIds": [
        7157
      ],
      "timeSlotId": 10971
    },
    {
      "id": 2079,
      "name": "Doctoral Symposium S1",
      "typeId": 11401,
      "roomId": 10252,
      "chairIds": [
        11179,
        13891,
        14373
      ],
      "contentIds": [
        7697,
        3702,
        6144,
        6871
      ],
      "timeSlotId": 10962
    },
    {
      "id": 1207,
      "name": "Doctoral Symposium S2",
      "typeId": 11401,
      "roomId": 10252,
      "chairIds": [
        11179,
        13891,
        14373
      ],
      "contentIds": [
        7600,
        3320,
        4913
      ],
      "timeSlotId": 10964
    },
    {
      "id": 1410,
      "name": " Doctoral Symposium S4",
      "typeId": 11401,
      "roomId": 10252,
      "chairIds": [
        11179,
        13891,
        14373
      ],
      "contentIds": [
        5787,
        5585,
        4566
      ],
      "timeSlotId": 10968
    }
  ],
  "events": [
    {
      "id": 2663,
      "name": "ACM ETRA'19 Sponsors' presentations",
      "typeId": 11375,
      "roomId": 10245,
      "chairIds": [
        15160,
        9468
      ],
      "contentIds": [],
      "startDate": 1561453200000,
      "endDate": 1561453200000,
      "description": "Visit ACM ETRA'19 sponsors' booths for the most contemporary developments in eye tracking software and hardware",
      "presenterIds": []
    },
    {
      "id": 2675,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561456800000,
      "endDate": 1561458600000,
      "presenterIds": []
    },
    {
      "id": 2581,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561541400000,
      "endDate": 1561543200000,
      "presenterIds": []
    },
    {
      "id": 2553,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561561200000,
      "endDate": 1561563000000,
      "presenterIds": []
    },
    {
      "id": 2637,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561627800000,
      "endDate": 1561629600000,
      "presenterIds": []
    },
    {
      "id": 2589,
      "name": "Lunch",
      "typeId": 11375,
      "roomId": 10253,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561550400000,
      "endDate": 1561554000000,
      "presenterIds": []
    },
    {
      "id": 2656,
      "name": "Banquet / Gala Dinner",
      "typeId": 11375,
      "roomId": 10243,
      "chairIds": [
        14118,
        16805
      ],
      "contentIds": [],
      "startDate": 1561660200000,
      "endDate": 1561671000000,
      "description": "ETRA'19 Banquet for all the attendees ",
      "presenterIds": []
    },
    {
      "id": 2646,
      "name": "Lunch",
      "typeId": 11375,
      "roomId": 10243,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561465800000,
      "endDate": 1561469400000,
      "presenterIds": []
    },
    {
      "id": 2627,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561476600000,
      "endDate": 1561478400000,
      "presenterIds": []
    },
    {
      "id": 2563,
      "name": "Lunch",
      "typeId": 11375,
      "roomId": 10243,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561636800000,
      "endDate": 1561640400000,
      "presenterIds": []
    },
    {
      "id": 2582,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561647600000,
      "endDate": 1561649400000,
      "presenterIds": []
    },
    {
      "id": 2676,
      "name": "Coffee Break",
      "typeId": 11375,
      "roomId": 10249,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561714200000,
      "endDate": 1561716000000,
      "presenterIds": []
    },
    {
      "id": 2597,
      "name": "Lunch",
      "typeId": 11375,
      "roomId": 10243,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1561723200000,
      "endDate": 1561726800000,
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 4864,
      "typeId": 11372,
      "title": "Deep learning investigation for chess player attention prediction using eye-tracking and game data",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "This article reports on an investigation of the use of convolutional neural networks to predict the visual attention of chess players. While the visual attention model described in this article has been created to create saliency maps that capture hierarchical and spatial features of chessboard, in order to predict the probability fixation for individual pixels. Using a skip-layer architecture of an autoencoder, with a unified decoder, we are able to use multiscale features to predict saliency of part of the board at different scales, showing multiple relations between pieces. We have used scan path and fixation data from players engaged in solving chess problems, to compute 6600 saliency maps associated to the corresponding chess piece configurations. This corpus is completed with synthetically generated data from actual games gathered from an online chess platform. Experimental realized using both scan-paths from chess players and the CAT2000 saliency dataset of natural images, highlights several results. Deep features, pretrained on natural images, were found to be helpful in training visual attention prediction for chess. The proposed neural network architecture is able to generate meaningful saliency maps on unseen chess configurations with good scores on standard metrics. This work provides a baseline for future work on visual attention prediction in similar contexts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10016
        },
        {
          "affiliations": [],
          "personId": 13917
        },
        {
          "affiliations": [],
          "personId": 19939
        },
        {
          "affiliations": [],
          "personId": 17555
        }
      ],
      "sessionIds": [
        2184
      ],
      "eventIds": []
    },
    {
      "id": 7168,
      "typeId": 11402,
      "title": "Gaze Analytics Pipeline",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "This tutorial gives a short introduction to experimental design in general and with regard to eye tracking studies in particular. Additionally, the design of three different eye tracking studies (using stationary as well as mobile eye trackers) will be presented and the strengths and limitations of their designs will be discussed. Further, the tutorial presents details of a Python-based gaze analytics pipeline developed and used by Prof. Duchowski and Ms. Gehrer. The gaze analytics pipeline consists of Python scripts for extraction of raw eye movement data, analysis and event detection via velocity-based filtering, collation of events for statistical evaluation, analysis and visualization of results using R. Attendees of the tutorial will have the opportunity to run the scripts of an analysis of gaze data collected during categorization of different emotional expressions while viewing faces. The tutorial covers basic eye movement analytics, e.g., fixation count and dwell time within AOIs, as well as advanced analysis using gaze transition entropy. Newer analytical tools and techniques such as microsaccade detection and the Index of Pupillary Activity will be covered with time permitting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 8800
        }
      ],
      "sessionIds": [
        1661
      ],
      "eventIds": []
    },
    {
      "id": 6144,
      "typeId": 11401,
      "title": "Pupil Diameter as a Measure of Emotion and Sickness in VR",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking is rapidly becoming popular in consumer technology, including virtual and augmented reality. Eye trackers commonly provide an estimate of gaze location, and pupil diameter. Pupil diameter is useful for interactive systems, as it provides means to estimate cognitive load, stress, and emotional state. However, there are several roadblocks that limit the use of pupil diameter. In VR HMDs there are a lack of models that account for stereoscopic viewing and the increased brightness of near eye displays. Existing work has shown correlations between pupil diameter and emotion, but have not been extended to VR environments. The scope of this work is to bridge the gap between existing research on emotion and pupil diameter to VR, while also attempting to use pupillary data to tackle the problem of simulator sickness in VR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16335
        }
      ],
      "sessionIds": [
        2079
      ],
      "eventIds": []
    },
    {
      "id": 4866,
      "typeId": 11372,
      "title": "Hand- and Gaze-Control of Telepresence Robots",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile robotic telepresence systems are increasingly used to promote social interaction between geographically dispersed people. People with severe motor disabilities may be included by eye-gaze control of the telepresence robots. However, use of gaze control for navigation of the telepresence robots needs to be explored. This paper presents an experimental comparison between gaze-controlled and hand-controlled telepresence robots with a head-mounted display. Participants (n = 16) had similar experience of presence and emotional changing in pleasure and arousal. Gaze control was 30.72% slower than hand control. The gaze-controlled robots had more collisions and deviation from optimal paths. The participants reported a higher workload, reduced feeling of dominance. Moreover, with gaze control, their situation awareness was significantly degraded. Their accuracy of post-trial reproduction of maze layout and trial duration were also significantly reduced.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22983
        },
        {
          "affiliations": [],
          "personId": 11387
        },
        {
          "affiliations": [],
          "personId": 9338
        }
      ],
      "sessionIds": [
        2252
      ],
      "eventIds": []
    },
    {
      "id": 5891,
      "typeId": 11410,
      "title": "Exploring Simple Neural Network Architectures for Eye Movement Classification",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and postsaccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We eliminate ad-hoc post-processing to create a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16172
        },
        {
          "affiliations": [],
          "personId": 8221
        },
        {
          "affiliations": [],
          "personId": 18521
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5895,
      "typeId": 11401,
      "title": "Accessible Control of Telepresence Robots based on Eye-Tracking",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze may be a good alternative input modality for people with limited hand mobility. This accessible control based on eye tracking can be implemented into telepresence robots, which are widely used to promote remote social interaction and providing the feeling of presence. This extended abstract introduces a PhD research project, which takes a two-phase approach towards investigating gaze-controlled telepresence robots. A system supporting gaze-controlled telepresence has been implemented. However, our current findings indicates that there were still serious challenges with regard to gaze-based driving. Potential improvement are discussed, and plans for future study are also presented.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22983
        },
        {
          "affiliations": [],
          "personId": 22533
        }
      ],
      "sessionIds": [
        1807
      ],
      "eventIds": []
    },
    {
      "id": 6409,
      "typeId": 11410,
      "title": "Eye-tracking based Fatigue and Cognitive Assessment",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Fatigue detection, monitoring and management is important and needs to be accommodated in the busy lifestyles that many people have these days. It may have an impact on the physical as well as the emotional health of the individuals. Detection of fatigue is the first step towards its management. With eye-tracking software using cameras, and being included in the laptops and smartphones, it now has the potential to become quite ubiquitous. This extended abstract describes my PhD project for fatigue detection using eye-tracking measures while gaze typing. The steps taken and experiments conducted upto now are presented, with an outline of the future plans. The principal use-case will be to provide the service of fatigue detection for people with neurological disorders, who use eye-tracking for alternative communications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23816
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7692,
      "typeId": 11372,
      "title": "Characterizing Joint Attention Behavior during Real World Interactions using Automated Object and Gaze Detection",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Joint attention is an essential part of the development process of children, and impairments in joint attention are among the early symptoms of autism. In this paper, we develop a novel technique to characterize, and potentially train joint attention in real-time, by studying the interaction of two human subjects with each other and with multiple objects present in the room. This is done by capturing the subjects’ gaze through eye-tracking glasses and detecting their looks on predefined indicator objects. A deep learning network is trained and deployed to detect the objects in the subjects’ field of vision by processing the video feed of the worldview camera mounted on the eye-tracking glasses. The subjects’ look patterns are thus determined and a real-time audio response is provided when a joint attention is detected, i.e. when their looks coincide. This system is made more robust using appropriate filtering and thresholding techniques.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16697
        },
        {
          "affiliations": [],
          "personId": 21077
        },
        {
          "affiliations": [],
          "personId": 15868
        },
        {
          "affiliations": [],
          "personId": 23708
        },
        {
          "affiliations": [],
          "personId": 9766
        },
        {
          "affiliations": [],
          "personId": 13941
        },
        {
          "affiliations": [],
          "personId": 10121
        }
      ],
      "sessionIds": [
        1666
      ],
      "eventIds": []
    },
    {
      "id": 7697,
      "typeId": 11401,
      "title": "Looks Can Mean Achieving: Understanding Eye Gaze Patterns of Proficiency in Code Comprehension",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "The research proposes four hypotheses that focus on deriving helpful insights from eye patterns, including hidden truths concerning programmer expertise, task context and difficulty. We present results from a study performed in a classroom setting with 17 students, in which we found that novice programmers visit output statements and declarations the same amount as the rest of the program they are presented other than control flow block headers.  This research builds upon insightful findings from our previous work, wherein we focus on gathering statistical eye-gaze effects between categories of various populations to drive the pursuit of new research. Ongoing and future work entails using the iTrace infrastructure to capture gaze as participants scroll to read code pages extending longer than what can fit on one screen. The focus will be on building various models that relate eye gaze to comprehension via methods that realistically capture activity in a development environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11902
        }
      ],
      "sessionIds": [
        2079
      ],
      "eventIds": []
    },
    {
      "id": 5140,
      "typeId": 11406,
      "title": "Eye-tracking based Fatigue and Cognitive Assessment",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Fatigue detection, monitoring and management is important and needs to be accommodated in the busy lifestyles that many people have these days. It may have an impact on the physical as well as the emotional health of the individuals. Detection of fatigue is the first step towards its management. With eye-tracking software using cameras, and being included in the laptops and smartphones, it now has the potential to become quite ubiquitous. This extended abstract describes my PhD project for fatigue detection using eye-tracking measures while gaze typing. The steps taken and experiments conducted upto now are presented, with an outline of the future plans. The principal use-case will be to provide the service of fatigue detection for people with neurological disorders, who use eye-tracking for alternative communications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23816
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5399,
      "typeId": 11407,
      "title": "Get a Grip: Slippage-Robust and Glint-Free Gaze Estimation for Real-Time Pervasive Head-Mounted Eye Tracking",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "A key assumption conventionally made by flexible head-mounted eye-tracking systems is often invalid: The eye center does not remain stationary w.r.t. the eye camera due to slippage. For instance, eye-tracker slippage might happen due to head acceleration or explicit adjustments by the user. As a result, gaze estimation accuracy can be significantly reduced. In this work, we propose Grip, a novel gaze estimation method capable of instantaneously compensating for eye-tracker slippage without additional hardware requirements such as glints or stereo eye camera setups. Grip was evaluated using previously collected data from a large scale unconstrained pervasive eye-tracking study. Our results indicate significant slippage compensation potential, decreasing average participant median angular offset by more than 43% w.r.t. a non-slippage-robust gaze estimation method. A reference implementation of Grip was integrated into EyeRecToo, an open-source hardware-agnostic eye-tracking software, thus making it readily accessible for multiple eye trackers (Available at: www.ti.uni-tuebingen.de/perception).",
      "authors": [
        {
          "affiliations": [],
          "personId": 17919
        },
        {
          "affiliations": [],
          "personId": 18642
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4375,
      "typeId": 11372,
      "title": "Reducing Calibration Drift in Mobile Eye Trackers by Exploiting Mobile Phone Usage",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Automatic saliency-based recalibration is promising for addressing calibration drift in mobile eye trackers but existing bottom-up saliency methods neglect user's goal-directed visual attention in natural behaviour. By inspecting real-life recordings of egocentric eye tracker cameras, we reveal that users are likely to look at their phones once these appear in view. We propose two novel automatic recalibration methods that exploit mobile phone usage: The first builds saliency maps using the phone location in the egocentric view to identify likely gaze locations. The second uses the occurrence of touch events to recalibrate the eye tracker, thereby enabling privacy-preserving recalibration. Through in-depth evaluations on a recent mobile eye tracking dataset (N=17, 65 hours) we show that our approaches outperform a state-of-the-art saliency approach for the automatic recalibration task. As such, our approach improves mobile eye tracking and gaze-based interaction, particularly for long-term use.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19118
        },
        {
          "affiliations": [],
          "personId": 23929
        },
        {
          "affiliations": [],
          "personId": 18631
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        2239
      ],
      "eventIds": []
    },
    {
      "id": 6679,
      "typeId": 11372,
      "title": "Time- and Space-efficient Eye Tracker Calibration",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "One of the obstacles to bring eye tracking technology to everyday human computer interactions is the time consuming calibration procedure. We introduce a novel calibration method based on smooth pursuit eye movement. The method uses linear regression to calculate the calibration mapping. The advantage is that users can perform the calibration quickly in a few seconds, and only use a small calibration area to cover a large tracking area. We first describe the theoretical background on establishing a calibration mapping and discuss differences of calibration methods used. We then present a user study comparing the new regression-based method with a classical nine-point and with other pursuit-based calibrations. The results show the proposed method to be fully functional, quick, and enables accurate tracking of a large area. The method has the potential to be integrated into current eye tracking systems to make them more usable in broader use cases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9946
        },
        {
          "affiliations": [],
          "personId": 19224
        },
        {
          "affiliations": [],
          "personId": 14505
        }
      ],
      "sessionIds": [
        2239
      ],
      "eventIds": []
    },
    {
      "id": 6680,
      "typeId": 11372,
      "title": "Quantitative Visual Attention Prediction on Webpage Images using Multiclass SVM",
      "trackId": 10558,
      "tags": [],
      "keywords": [],
      "abstract": "Webpage images—image elements on a webpage—are prominent to draw user attention. Modeling attention on webpage images helps in their synthesis and rendering. This paper presents a visual feature-based attention prediction model for webpage images. Firstly, fixated images were assigned quantitative visual attention based on users’ sequential attention allocation on webpages. Subsequently, fixated images’ intrinsic visual features were extracted along with position and size on respective webpages. A multiclass support vector machine (multiclass SVM) was learned using the visual features and associated attention. In tandem, a majority-voting-scheme was employed to predict the quantitative visual attention for test webpage images. The proposed approach was analyzed through an eye-tracking experiment conducted on 36 real-world webpages with 42 participants. Our model outperforms (average accuracy of 91.64% and micro F1-score of 79.1%) the existing position and size constrained regression model (average accuracy of 73.92% and micro F1-score of 34.80%).",
      "authors": [
        {
          "affiliations": [],
          "personId": 11665
        },
        {
          "affiliations": [],
          "personId": 10793
        },
        {
          "affiliations": [],
          "personId": 23351
        }
      ],
      "sessionIds": [
        2374
      ],
      "eventIds": []
    },
    {
      "id": 7193,
      "typeId": 11372,
      "title": "Classification of Strategies for Solving Programming Problems using AoI Sequence Analysis",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "This eye tracking study examines participants' visual attention when solving algorithmic problems in the form of programming problems. The stimuli consisted of a problem statement, example output, and a set of multiple-choice questions regarding variables, data types, and operations needed to solve the programming problems. We recorded eye movements of students and performed an Area of Interest (AoI) sequence analysis to identify reading strategies in terms of participants' performance and visual effort. Using classical eye tracking metrics and a visual AoI sequence analysis we identified two main groups of participants---effective and ineffective problem solvers. This indicates that diversity of participants' mental schemas leads to a difference in their performance. Therefore, identifying how participants' reading behavior varies at a finer level of granularity warrants further investigation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9315
        },
        {
          "affiliations": [],
          "personId": 19220
        },
        {
          "affiliations": [],
          "personId": 15225
        }
      ],
      "sessionIds": [
        2060
      ],
      "eventIds": []
    },
    {
      "id": 6426,
      "typeId": 11406,
      "title": "Microsaccadic and Pupillary Response to Tactile Task Difficulty",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The research goal is to explore the relationship between eye tracking measures and a tactile version of the n-back task. The n-back task is often used to evoke cognitive load, however this is the first study that incorporates tactile stimulus as input. The study follows a within-subject design with easy and difficult experimental conditions. In the tactile n-back task, each participant will be asked to identify the number of pins felt under the fingertips. In the easy condition, each participant will then be asked to respond if a number shown on the computer screen is congruent with the number of recognized pins. In the difficult condition, each participant will be asked to refer to the pin number in the current trial and the previous trial. Microsaccades and pupil dilation will be recorded during the top-down process of performing the n-back task.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9309
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7963,
      "typeId": 11372,
      "title": "Privacy-Aware Eye Tracking Using Differential Privacy",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "With eye tracking being increasingly integrated into virtual and augmented reality (VR/AR) head-mounted displays, preserving users' privacy is an ever more important, yet under-explored, topic in the eye tracking community. We report a large-scale online survey (N=124) on privacy aspects of eye tracking that provides the first comprehensive account of with whom, for which services, and to what extent users are willing to share their gaze data. Using these insights, we design a privacy-aware VR interface that uses differential privacy, which we evaluate on a new 20-participant dataset for two privacy sensitive tasks: We show that our method can prevent user re-identification and protect gender information while maintaining high performance for gaze-based document type classification. Our results highlight the privacy challenges particular to gaze data and demonstrate that differential privacy is a potential means to address them. Thus, this paper lays important foundations for future research on privacy-aware gaze interfaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20843
        },
        {
          "affiliations": [],
          "personId": 9728
        },
        {
          "affiliations": [],
          "personId": 18631
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        2220
      ],
      "eventIds": []
    },
    {
      "id": 5148,
      "typeId": 11372,
      "title": "Eye gaze and head gaze in collaborative games",
      "trackId": 10555,
      "tags": [],
      "keywords": [],
      "abstract": "We present an investigation of sharing the focus of visual attention between two players in a collaborative game, so that where one player was looking was visible to the other. Two experiments are reported, the first investigates the effect of a high immersion presentation of the game in VR compared with a lower immersion desktop presentation. The second examines the high immersion condition in more detail. The studies show that in spite of there being many factors that could affect the outcome of a relatively short period of game play, sharing eye-gaze in the high immersion condition produces shorter overall durations and better subjective ratings of team work than does sharing head-gaze. This difference is not apparent in the low immersion condition. The findings are a good argument for exploiting the opportunities for including and using eye tracking within head mounted displays in the context of collaborative games.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15119
        },
        {
          "affiliations": [],
          "personId": 17169
        },
        {
          "affiliations": [],
          "personId": 11857
        },
        {
          "affiliations": [],
          "personId": 17881
        },
        {
          "affiliations": [],
          "personId": 19727
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 2845,
      "typeId": 11372,
      "title": "Analyzing Gaze Transition Behavior Using Bayesian Mixed Effects Markov Models",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The complex stochastic nature of eye tracking data calls for exploring sophisticated statistical models to ensure reliable inference in multi-trial eye-tracking experiments. We employ a Bayesian semi-parametric mixed-effects Markov model to compare gaze transition matrices between different experimental factors accommodating individual random effects. The model not only allows us to assess global influences of the external factors on the gaze transition dynamics but also provides comprehension of these effects at a deeper local level. We experimented to explore the impact of recognizing distorted images of artwork and landmarks on the gaze transition patterns. Our dataset comprises sequences representing areas of interest visited when applying a content independent grid to the resulting scan paths in a multi-trial setting. Results suggest that recognition to some extent affects the dynamics of the transitions while image category played an essential role in the viewing behavior.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14763
        },
        {
          "affiliations": [],
          "personId": 23414
        },
        {
          "affiliations": [],
          "personId": 8340
        },
        {
          "affiliations": [],
          "personId": 14590
        },
        {
          "affiliations": [],
          "personId": 19857
        }
      ],
      "sessionIds": [
        2184
      ],
      "eventIds": []
    },
    {
      "id": 4382,
      "typeId": 11406,
      "title": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "authors": [
        {
          "affiliations": [],
          "personId": 23724
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5408,
      "typeId": 11372,
      "title": "A fast approach to refraction-aware 3D eye-model fitting and gaze prediction",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "By temporally integrating information about pupil contours extracted from eye images, model-based methods for glint-free gaze estimation can mitigate pupil detection noise. However, current approaches require time-consuming iterative solving of a nonlinear minimization problem to estimate key parameters, such as eyeball position. Based on the method presented by [Swirski and Dodgson 2013], we propose a novel approach to glint-free 3D eye-model fitting and gaze prediction using a single near-eye camera. By recasting model optimization as a least-squares intersection of lines, we make it amenable to a fast non-iterative solution. We further present a method for estimating deterministic refraction-correction functions from synthetic eye images and validate them on both synthetic and real eye images. We demonstrate the robustness of our method in the presence of pupil detection noise and show the benefit of temporal integration of pupil contour information on eyeball position and gaze estimation accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10525
        },
        {
          "affiliations": [],
          "personId": 21133
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        1666
      ],
      "eventIds": []
    },
    {
      "id": 3620,
      "typeId": 11410,
      "title": "PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric Scene Image and Eye Movement Features",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the \"privacy level\" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20843
        },
        {
          "affiliations": [],
          "personId": 20147
        },
        {
          "affiliations": [],
          "personId": 17638
        },
        {
          "affiliations": [],
          "personId": 22533
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4395,
      "typeId": 11406,
      "title": "EyeVEIL: Degrading Iris Authentication in Eye-Tracking Headsets",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Mixed reality headsets are being designed with integrated eye trackers: cameras that image the user's eye to infer gaze location and pupil diameter. While the intent is to improve the quality of experience, built-in eye trackers create a security vulnerability for hackers -- high resolution images of the user's iris. Anyone stealing an iris image has effectively captured a gold standard biometric, relied on for secure authentication in applications such as banking and voting. We present a low cost solution to degrade iris authentication while still permitting the utility of gaze tracking with acceptable accuracy. By demonstrating this solution on a commodity eye tracker, this paper urges the community to think about iris based authentication as a byproduct of eye tracking, and create solutions that empower a user to control this biometric.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16335
        },
        {
          "affiliations": [],
          "personId": 19562
        },
        {
          "affiliations": [],
          "personId": 9418
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4651,
      "typeId": 11372,
      "title": "Using Warped Time Distance Chart to Compare Scan-paths of Multiple Observers",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "The aim of the research is the introduction of a new technique that enables a visual comparison of scan-paths. Every eye tracking experiment produces many scan-paths, and one of the main challenges of eye tracking analysis is how two compare these scan-paths. The comparison techniques typically return a value (or several values) that may be used as scan-path similarity/distance measure. However, there is still a lack of widely adopted methods that offer not only the measure but enable a visual comparison of scan-paths. The paper introduces two possible options: the Mutual Distance Plot for two scan-paths and the Warped Time Distance Chart for the comparison of the theoretically unlimited number of scan-paths. It is shown that these visualizations may reveal information about relationships between two or more scan-paths on straightforward charts. The informativeness of the solution is analyzed using both artificial and real data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9746
        },
        {
          "affiliations": [],
          "personId": 17461
        }
      ],
      "sessionIds": [
        1667
      ],
      "eventIds": []
    },
    {
      "id": 3371,
      "typeId": 11372,
      "title": "SacCalib: Reducing Calibration Distortion for Stationary Eye Trackers Using Saccadic Eye Movements",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "Recent methods to automatically calibrate stationary eye trackers were shown to effectively reduce inherent calibration distortion. However, these methods require additional information, such as mouse clicks or on-screen content. We propose the first method that only requires users' eye movements to reduce calibration distortion in the background while users naturally look at an interface. Our method exploits that calibration distortion makes straight saccade trajectories appear curved between the saccadic start and end points. We show that this curving effect is systematic and the result of distorted gaze projection plane. To mitigate calibration distortion, our method undistorts this plane by straightening saccade trajectories using image warping. We show that this approach improves over the common six-point calibration and is promising for reducing calibration distortion. As such, it provides a non-intrusive solution to alleviating accuracy decrease of eye tracker during long-term use.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18631
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 2859,
      "typeId": 11372,
      "title": "SaccadeMachine: Software for Analyzing Saccade Tests (Anti-Saccade and Pro-saccade)",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "Various types of saccadic paradigms, in particular, Prosaccade and Antisaccade tests are widely used in Pathophysiology and Psychology. Despite been widely used, there has not been a standard tool for processing and analyzing the eye tracking data obtained from saccade tests. We describe an open-source software for extracting and analyzing the eye movement data of different types of saccade tests that can be used to extract and compare participants’ performance and various task-related measures across participants. We further demonstrate the utility of the software by using it to analyze the data from an antisaccade, and a recent distractor experiment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19079
        },
        {
          "affiliations": [],
          "personId": 10473
        },
        {
          "affiliations": [],
          "personId": 13753
        },
        {
          "affiliations": [],
          "personId": 19275
        },
        {
          "affiliations": [],
          "personId": 13809
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 4908,
      "typeId": 11407,
      "title": "PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric Scene Image and Eye Movement Features",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the \"privacy level\" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20843
        },
        {
          "affiliations": [],
          "personId": 20147
        },
        {
          "affiliations": [],
          "personId": 17638
        },
        {
          "affiliations": [],
          "personId": 22533
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4656,
      "typeId": 11401,
      "title": "Attentional orienting in real and virtual 360-degree environments: application to aeronautics",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the mechanisms of attentional orienting in a 360-degree virtual environments. Through the use of Posner’s paradigm,we study the effects of different attentional guidance techniquesdesigned to improve information processing. The most efficienttechnique will be applied to a procedure learning tool in virtualreality and a remote air traffic control tower. The eye-tracker allowsus to explore the differential effects of overt and covert orienting,to estimate the effectiveness of visual research and to use it as atechnique for interaction in virtual reality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24170
        },
        {
          "affiliations": [],
          "personId": 16606
        },
        {
          "affiliations": [],
          "personId": 24132
        }
      ],
      "sessionIds": [
        1807
      ],
      "eventIds": []
    },
    {
      "id": 7472,
      "typeId": 11406,
      "title": "A Deep Learning Approach for Robust Head Pose Independent Eye movements recognition from Videos",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The recognition of eye movements is an important task for gaze behavior understanding like in human communication analysis (human-human, computer or robot interactions) or diagnosis (medical, reading impairments). In this paper, we address this task using remote R-GBD sensors, which allows handling people in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 25/30Hz and provide low-resolution eye images. Hence the resulting gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous method less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade and blink classes, distinguishing irrelevant noise (illumination, inaccurate eye alignment, ...) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24177
        },
        {
          "affiliations": [],
          "personId": 15255
        },
        {
          "affiliations": [],
          "personId": 16865
        },
        {
          "affiliations": [],
          "personId": 11126
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4913,
      "typeId": 11401,
      "title": "When you don’t see what you expect: incongruence in music and source code reading",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Both musicians and programmers have expectations when they read music scores or source code due to prior knowledge. The goal of these studies is to get an insight into what will happen when these expectations are violated in familiar tasks. In music reading study we explored eye-movements of musically experienced participants singing and playing on a piano familiar melodies either containing or not containing a bar shifted down a tone in two different keys of C major and B major. First-pass fixation durations, mean pupil size during first-pass fixations and eye-time span parameters were analysed using linear mixed models. All three parameters can provide useful information on processing of incongruence in music. Furthermore, the pupil size parameter might be sensitive to modality of performance. In code reading study we plan to study incongruence in familiar code tasks and its reflection in eye movements of programmers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23192
        }
      ],
      "sessionIds": [
        1207
      ],
      "eventIds": []
    },
    {
      "id": 3377,
      "typeId": 11410,
      "title": "Improving Real Time CNN-Based Pupil Detection Through Domain-Specific Data Augmentation",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Deep learning is a promising technique for real-world pupil detection applications. However, the small amount of available accurately-annotated data poses a challenge when training such networks. Here, we utilize non-challenging eye videos where algorithmic approaches perform virtually without errors to automatically generate a foundational data set containing subpixel pupil annotations. Then, we propose multiple domain-specific data augmentation methods to create unique training sets containing controlled distributions of pupil-detection challenges. The feasibility, convenience, and advantage of this approach is demonstrated by training a CNN with these datasets. The resulting network outperformed current methods in multiple publicly-available, realistic, and challenging datasets, despite being trained solely with the augmented eye images. This network also exhibited better generalization w.r.t. the latest state-of-the-art CNN: Whereas on datasets similar to training data, the nets displayed similar performance, on datasets unseen to both networks, ours outperformed the state-of-the-art by ~27 in terms of detection rate.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11665
        },
        {
          "affiliations": [],
          "personId": 10793
        },
        {
          "affiliations": [],
          "personId": 23351
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4402,
      "typeId": 11410,
      "title": "Microsaccadic and Pupillary Response to Tactile Task Difficulty",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The research goal is to explore the relationship between eye tracking measures and a tactile version of the n-back task. The n-back task is often used to evoke cognitive load, however this is the first study that incorporates tactile stimulus as input. The study follows a within-subject design with easy and difficult experimental conditions. In the tactile n-back task, each participant will be asked to identify the number of pins felt under the fingertips. In the easy condition, each participant will then be asked to respond if a number shown on the computer screen is congruent with the number of recognized pins. In the difficult condition, each participant will be asked to refer to the pin number in the current trial and the previous trial. Microsaccades and pupil dilation will be recorded during the top-down process of performing the n-back task.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9309
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 6198,
      "typeId": 11402,
      "title": "Eye Tracking in the Study of Developmental Conditions: A Computer Scientists Primer",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "Children with developmental conditions, such as autism, genetic disorders, and fetal alcohol syndrome, present with complex etiologies and can incur significant challenges throughout their life. Especially in very young children, heterogeneity across and within diagnostic categories makes uniform application of standard assessment methods, that often rely on assumptions of communicative or other developmental abilities, difficult. Eye tracking has emerged as a powerful tool to study both the mechanistic underpinnings of atypical development as well as facets of cognitive and attentional development that may be of clinical and prognostic value. In this tutorial we discuss the challenges and approaches associated with studying developmental conditions using eye tracking. Using autism spectrum disorder (ASD) as a model, we discuss the interplay between clinical facets of conditions and studies and techniques used to probe neurodevelopment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16851
        }
      ],
      "sessionIds": [
        2431
      ],
      "eventIds": []
    },
    {
      "id": 5689,
      "typeId": 11372,
      "title": "Clustered Eye Movement Similarity Matrices",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements recorded for many study participants are difficult to interpret, in particular when the task is to identify similar scanning strategies over space, time, and participants. In this paper we describe an approach in which we first compare scanpaths, not only based on Jaccard (JD) and bounding box (BB) similarities, but also on more complex approaches like longest common subsequence (LCS), Frechet distance (FD), dynamic time warping (DTW), and edit distance (ED). The results of these algorithms generate a weighted comparison matrix while each entry encodes the pairwise participant scanpath comparison strength. To better identify participant groups of similar eye movement behavior we reorder this matrix by hierarchical clustering, optimal-leaf ordering, dimensionality reduction, or a spectral approach. The matrix visualization is linked to the original stimulus overplotted with visual attention maps and gaze plots on which typical interactions like temporal, spatial, or participant-based filtering can be applied.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21349
        },
        {
          "affiliations": [],
          "personId": 9283
        },
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 22761
        }
      ],
      "sessionIds": [
        1150
      ],
      "eventIds": []
    },
    {
      "id": 3386,
      "typeId": 11372,
      "title": "Screen Corner Detection using Polarization Camera for Cross-Ratio Based Gaze Estimation",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "In eye tracking, various methods have been studied such as regression-based and model-based approaches. These methods generally use near-infrared light emitting diodes(LEDs) and camera for estimating point-of-gaze with high accuracy. Recently, cross-ratio based method, which calculates the point-of-gaze using homography matrices, has attracted attention, because it does not require hardware calibration which determines the geometric relationship between a camera and a display. However, this method needs near-infrared LEDs attached to the display to detect the screen corners, and thus, the installation of LEDs around the display is required. Therefore, we propose to use a polarization camera for detecting the screen area which is reflected on corneal surface. The screen corners can be determined without using near-infrared LEDs, and the point-of-gaze is estimated using the detected corners on the cornea. We investigated the accuracy of the estimated point-of-gaze based on the proposed method under various illumination and display conditions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15889
        },
        {
          "affiliations": [],
          "personId": 23403
        },
        {
          "affiliations": [],
          "personId": 14689
        }
      ],
      "sessionIds": [
        1666
      ],
      "eventIds": []
    },
    {
      "id": 3387,
      "typeId": 11406,
      "title": "The vision and interpretation of paintings: bottom-up visual processes, top-down culturally informed attention, and aesthetic experience.",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "We aim to contribute to our knowledge about how we experience paintings, and how visual exploration, cognitive categorization and emotive evaluation contribute to its aesthetic dimension [1], [2], [3]. Visual exploration activates universal visio-mimetic capacities, however these capacities are implemented culturally and historically [4]. We combine psychological, cognitive, and historical studies to address the question of the historically variable “forms of intentionality” [5] of our ways of looking at paintings, investigting how the notion of a “period eye” [6], [7] can be accommodated with the constancy of human vision. For this purpose, we use eye-tracking data of 52 participants looking at the Isenheim altarpiece before and after restoration. First results allowed us to classify the zones of salience as well as the effects of participants’ backgrounds and emotions on fixation time and visual attention.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23565
        },
        {
          "affiliations": [],
          "personId": 19375
        },
        {
          "affiliations": [],
          "personId": 10061
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 8001,
      "typeId": 11410,
      "title": "A Gaze Model Improves Autonomous Driving",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "End-to-end behavioral cloning trained by human demonstration is now a popular approach for vision-based autonomous driving. A deep neural network maps drive-view images directly to steering commands. However, the images contain much task-irrelevant data. Humans attend to behaviorally relevant information using saccades that direct gaze towards important areas. We demonstrate that behavioral cloning also benefits from active control of gaze. We trained a generative deep neural network model that accurately predicts human gaze maps while driving in both familiar and unseen environments. We incorporated the predicted gaze maps into end-to-end networks for two behaviors: following and overtaking. Incorporating gaze information significantly improves generalization to unseen environments. We hypothesize that incorporating gaze information enables the network to focus on task critical objects, which vary little between environments, and ignore irrelevant elements in the background, which vary greatly.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17561
        },
        {
          "affiliations": [],
          "personId": 22647
        },
        {
          "affiliations": [],
          "personId": 21894
        },
        {
          "affiliations": [],
          "personId": 9130
        },
        {
          "affiliations": [],
          "personId": 14305
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5442,
      "typeId": 11410,
      "title": "SeTA: Semiautomatic Tool for Annotation of Eye Tracking Images",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Availability of large scale tagged datasets is a must in the field of deep learning applied to the eye tracking challenge. In this paper, the potential of Supervised-Descent-Method (SDM) as a semiautomatic labelling tool for eye tracking images is shown. The objective of the paper is to evidence how the human effort needed for manually labelling large eye tracking datasets can be radically reduced by the use of cascaded regressors. Different applications are provided in the fields of high and low resolution systems. An iris/pupil center labelling is shown as example for low resolution images while a pupil contour points detection is demonstrated in high resolution. In both cases manual annotation requirements are drastically reduced.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23348
        },
        {
          "affiliations": [],
          "personId": 10669
        },
        {
          "affiliations": [],
          "personId": 9078
        },
        {
          "affiliations": [],
          "personId": 22026
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7746,
      "typeId": 11406,
      "title": "When you don’t see what you expect: incongruence in music and source code reading",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Both musicians and programmers have expectations when they read music scores or source code due to prior knowledge. The goal of these studies is to get an insight into what will happen when these expectations are violated in familiar tasks. In music reading study we explored eye-movements of musically experienced participants singing and playing on a piano familiar melodies either containing or not containing a bar shifted down a tone in two different keys of C major and B major. First-pass fixation durations, mean pupil size during first-pass fixations and eye-time span parameters were analysed using linear mixed models. All three parameters can provide useful information on processing of incongruence in music. Furthermore, the pupil size parameter might be sensitive to modality of performance. In code reading study we plan to study incongruence in familiar code tasks and its reflection in eye movements of programmers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23192
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5698,
      "typeId": 11402,
      "title": "Deep Learning in the Eye Tracking World",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "Recently deep learning has become a hype word in computer science. Many problems, which till now could be solved only using sophisticated algorithms, can be now solved with specially developed neural networks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9746
        }
      ],
      "sessionIds": [
        1090
      ],
      "eventIds": []
    },
    {
      "id": 4675,
      "typeId": 11406,
      "title": "Calibration-free Text Entry using Smooth Pursuit Eye Movements",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a calibration-free gaze-based text entry system that uses smooth pursuit eye movements. We report on our implementation, which improves over prior work on smooth pursuit text entry by 1) eliminating the need of calibration using motion correlation, 2) increasing input rate from 3.34 to 3.41 words per minute, 3) featuring text suggestions that were trained on 10,000 lexicon sentences recommended in the literature. We report on a user study (N=26) which shows that users are able to eye type at 3.41 words per minutes without calibration and without user training. Qualitative feedback also indicates that users positively perceive the system. Our work is of particular benefit for disabled users and for situations when voice and tactile input are not feasible (e.g., in noisy environments or when the hands are occupied).",
      "authors": [
        {
          "affiliations": [],
          "personId": 13779
        },
        {
          "affiliations": [],
          "personId": 22295
        },
        {
          "affiliations": [],
          "personId": 16495
        },
        {
          "affiliations": [],
          "personId": 22005
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 3651,
      "typeId": 11372,
      "title": "Impact of Variable Position of Text Prediction in Gaze-based Text Entry",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "Text predictions play an important role in improving the performance of gaze-based text entry systems. However, visual search, scanning, and selection of text predictions require a shift in the user's attention from the keyboard layout. Hence the spatial positioning of predictions becomes an imperative aspect of the end-user experience. In this work, we investigate the role of spatial positioning by comparing the performance of three different keyboards entailing variable positions for text predictions. The experiment result shows no significant differences in the text entry performance, i.e., displaying suggestions closer to visual fovea did not enhance the text entry rate of participants, however, they used more keystrokes and backspace. This implies to the inessential usage of suggestions when it is in the constant visual attention of users, resulting in increased cost of correction. Furthermore, we argue that the fast saccadic eye movements undermine the spatial distance optimization in prediction positioning.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18170
        },
        {
          "affiliations": [],
          "personId": 12023
        },
        {
          "affiliations": [],
          "personId": 18308
        },
        {
          "affiliations": [],
          "personId": 10707
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 3143,
      "typeId": 11372,
      "title": "Pointing by Gaze, Head, and Foot in a Head-Mounted Display",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents two Fitts' law experiments and a clinical case study performed with a head-mounted display (HMD). The first baseline experiment compared hand and mouse pointing. Both yielded similar performance on time to activate, throughput, and effective target width. The second experiment compared gaze, feet, and head pointing. Gaze was slower than the other pointing methods, especially in the lower visual field. Throughputs for gaze and foot pointing were lower than mouse and head pointing and their effective target widths were also higher. A follow-up case study included seven participants with movement disorders that completed the task with the gaze and head pointing methods. Only two of the participants were able to calibrate for gaze tracking but all 7 could use head pointing, although with throughput less than one-third of the non-clinical participants.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11387
        },
        {
          "affiliations": [],
          "personId": 9338
        },
        {
          "affiliations": [],
          "personId": 22642
        },
        {
          "affiliations": [],
          "personId": 22176
        },
        {
          "affiliations": [],
          "personId": 18569
        }
      ],
      "sessionIds": [
        2252
      ],
      "eventIds": []
    },
    {
      "id": 8009,
      "typeId": 11410,
      "title": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "authors": [
        {
          "affiliations": [],
          "personId": 23724
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5199,
      "typeId": 11406,
      "title": "Attentional orienting in real and virtual 360-degree environments: application to aeronautics",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the mechanisms of attentional orienting in a 360-degree virtual environments. Through the use of Posner’s paradigm,we study the effects of different attentional guidance techniquesdesigned to improve information processing. The most efficienttechnique will be applied to a procedure learning tool in virtualreality and a remote air traffic control tower. The eye-tracker allowsus to explore the differential effects of overt and covert orienting,to estimate the effectiveness of visual research and to use it as atechnique for interaction in virtual reality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24170
        },
        {
          "affiliations": [],
          "personId": 16606
        },
        {
          "affiliations": [],
          "personId": 24132
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4687,
      "typeId": 11410,
      "title": "Looks Can Mean Achieving: Understanding Eye Gaze Patterns of Proficiency in Code Comprehension",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The research proposes four hypotheses that focus on deriving helpful insights from eye patterns, including hidden truths concerning programmer expertise, task context and difficulty. We present results from a study performed in a classroom setting with 17 students, in which we found that novice programmers visit output statements and declarations the same amount as the rest of the program they are presented other than control flow block headers.  This research builds upon insightful findings from our previous work, wherein we focus on gathering statistical eye-gaze effects between categories of various populations to drive the pursuit of new research. Ongoing and future work entails using the iTrace infrastructure to capture gaze as participants scroll to read code pages extending longer than what can fit on one screen. The focus will be on building various models that relate eye gaze to comprehension via methods that realistically capture activity in a development environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11902
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 2897,
      "typeId": 11406,
      "title": "Remote Corneal Imaging by Integrating a 3D Face Model and an Eyeball Model",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "In corneal imaging methods, it is essential to use a 3D eyeball model for generating an undistorted image. Thus, the relationship between the eye and eye camera is fixed by using a head-mounted device. Remote corneal imaging has several potential applications such as surveillance systems and driver monitoring. We integrated a 3D eyeball model with a 3D face model to facilitate remote corneal imaging. We conducted evaluation experiments and confirmed the feasibility of remote corneal imaging. We showed that the center of the eyeball can be estimated based on face tracking, and thus, corneal imaging can function as continuous remote eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20838
        },
        {
          "affiliations": [],
          "personId": 14689
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7765,
      "typeId": 11410,
      "title": "Reading Detection in Real-time",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a new reading detection pipeline that incorporates both global and local components of the existing reading detection paradigms. For our methodology, ground truth is determined globally, from a task driven user study. This allows us to avoid the tedium and bias of annotating individual gaze tracks as ground truth reading. From this set up we utilize a Region Ranking SVM (RRSVM), a specific genre of classifier that learns local importances while optimizing for the global inference goal, allowing us to learn which local fixation windows play a role in defining reading of skimming, without actually defining so explicitly. Doing so givers us a detector which can provide reading and skimming in both global and local contexts, using only coarse grained ground truth.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10666
        },
        {
          "affiliations": [],
          "personId": 16757
        },
        {
          "affiliations": [],
          "personId": 10496
        },
        {
          "affiliations": [],
          "personId": 21379
        },
        {
          "affiliations": [],
          "personId": 17397
        },
        {
          "affiliations": [],
          "personId": 23292
        },
        {
          "affiliations": [],
          "personId": 10555
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4949,
      "typeId": 11372,
      "title": "An Interactive Web-Based Visual Analytics Tool for Detecting Strategic Eye Movement Patterns",
      "trackId": 10558,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movement data contains spatial and temporal information, typically recorded by tracking the eyes of several people. The immense flood of data demands for a visual analytics tool that provides a condensed view on the data with the goal to identify strategic eye movement patterns. These patterns can be useful to detect visual problems or design flaws in the displayed stimuli with the goal to improve them. In this paper we describe an interactive and web-based visual analytics tool combining linked visualization techniques, algorithmic approaches, and the perceptual strengths of the human observer with a special focus on detecting hierarchical scanning behavior in eye movement data. To illustrate the usefulness of our concept we apply it to real-world eye movement data and finally, discuss scalability issues and limitations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 21349
        },
        {
          "affiliations": [],
          "personId": 9283
        }
      ],
      "sessionIds": [
        2374
      ],
      "eventIds": []
    },
    {
      "id": 6485,
      "typeId": 11372,
      "title": "Guiding Gaze: Expressive Models of Reading and Face Scanning",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "We evaluate subtle, emotionally-driven models of eye movement animation. Two models are tested, reading and face scanning, each based on recorded gaze transition probabilities. For reading, simulated emotional mood is governed by the probability density function that varies word advancement, i.e., re-fixations, forward, or backward skips. For face scanning, gaze behavior depends on task (gender or emotion discrimination) or the facial emotion portrayed. Probability density functions in both cases are derived from empirically observed transitions that significantly alter viewing behavior, captured either during mood-induced reading or during scanning faces expressing different emotions. A perceptual study shows that viewers can distinguish between reading and face scanning eye movements. However, viewers could not gauge the emotional valence of animated eye motion. For animation, our contribution shows that simulated emotionally-driven viewing behavior is too subtle to be discerned, or it needs to be exaggerated to be effective.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 16310
        },
        {
          "affiliations": [],
          "personId": 11257
        },
        {
          "affiliations": [],
          "personId": 8800
        },
        {
          "affiliations": [],
          "personId": 15617
        },
        {
          "affiliations": [],
          "personId": 16805
        }
      ],
      "sessionIds": [
        1666
      ],
      "eventIds": []
    },
    {
      "id": 4952,
      "typeId": 11402,
      "title": "Eye Tracking in the Study of Developmental Conditions: A Computer Scientists Primer",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "Children with developmental conditions, such as autism, genetic disorders, and fetal alcohol syndrome, present with complex etiologies and can incur significant challenges throughout their life. Especially in very young children, heterogeneity across and within diagnostic categories makes uniform application of standard assessment methods, that often rely on assumptions of communicative or other developmental abilities, difficult. Eye tracking has emerged as a powerful tool to study both the mechanistic underpinnings of atypical development as well as facets of cognitive and attentional development that may be of clinical and prognostic value. In this tutorial we discuss the challenges and approaches associated with studying developmental conditions using eye tracking. Using autism spectrum disorder (ASD) as a model, we discuss the interplay between clinical facets of conditions and studies and techniques used to probe neurodevelopment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16851
        }
      ],
      "sessionIds": [
        2230
      ],
      "eventIds": []
    },
    {
      "id": 7257,
      "typeId": 11369,
      "title": "Eye Tracking in Mixed Reality and its Promises for Spatial Research",
      "trackId": 10563,
      "tags": [],
      "keywords": [],
      "abstract": "Mixed Reality headsets such as “HoloLens 2” and “Magic Leap” blend the \nvirtual with the real world and provide tremendous new possibilities for\n users and researchers alike – especially given the integrated eye and \nhand tracking as well as spatial mapping capabilities. On the one hand, \nthis provides users with entirely new ways to engage with both their \nvirtual and real environment. On the other hand, it provides a rich and \npowerful toolset for researchers to, for example, investigate visual \nattention for spatial research in a much more complex space. Whether \nthis is to navigate more efficiently through a virtual 2D or 3D map that\n is placed in your real environment,  to find your way through an \nunknown building or to explore new ways to investigate shared visual \nattention in a complex 3D space. As part of the “HoloLens 2” team, I \nwill talk about the possibilities and challenges that I see for \nAugmented and Mixed Reality headsets – in particular with respect to how\n spatial research can benefit in various ways from this innovative \ntechnology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19539
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 5722,
      "typeId": 11410,
      "title": "Random ferns for area of interest free scan path classification",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "We propose to use random ferns in combination with saccade angle successions to compare scanpaths. One advantage of our method is that it does not require areas of interest to be computed or annotated. The conditional distribution in random ferns additionally allows for learning angle successions, which do not have to be entirely present in a scan path. We evaluated our approach on two publicly available datasets and improved the classification accuracy by ≈10 and ≈20 percent.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16460
        },
        {
          "affiliations": [],
          "personId": 20400
        },
        {
          "affiliations": [],
          "personId": 18504
        },
        {
          "affiliations": [],
          "personId": 24039
        },
        {
          "affiliations": [],
          "personId": 18095
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4445,
      "typeId": 11410,
      "title": "W!NCE: Eyewear Solution for Upper Face Action Units Monitoring",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to unobtrusively and continuously monitor one’s facial expressions has implications for a variety of application domains ranging from affective computing to health-care and the entertain- ment industry. The standard Facial Action Coding System (FACS) along with camera based methods have been shown to provide ob- jective indicators of facial expressions; however, these approaches can also be fairly limited for mobile applications due to privacy con- cerns and awkward positioning of the camera. To bridge this gap, W!NCE re-purposes a commercially available Electrooculography- based eyeglass (J!NS MEME) for continuously and unobtrusively sensing of upper facial action units with high fidelity. W!NCE de- tects facial gestures using a two-stage processing pipeline involving motion artifact removal and facial action detection. We validate our system’s applicability through extensive evaluation on data from 17 users under stationary and ambulatory settings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15473
        },
        {
          "affiliations": [],
          "personId": 23706
        },
        {
          "affiliations": [],
          "personId": 10188
        },
        {
          "affiliations": [],
          "personId": 11669
        },
        {
          "affiliations": [],
          "personId": 15235
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7777,
      "typeId": 11372,
      "title": "An Intuitive Visualization for Rapid Data Analysis - Using the DNA Metaphor for Eye Movement Patterns",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking has become a valuable approach in user centered studies, because it adds an additional source of information that can be recorded for later analysis, instead of just relying on the task completion and the time to completion. Without a proper analysis of eye tracking data, important information can remain untold. In this paper we present an intuitive visualization for rapid data analysis of eye movement patterns. Compared to heat maps or scan path visualizations, AOI-DNAs are easy to implement and take less effort to detect eye movement patterns. Therefore, they are viable for a visual data mining and offer capabilities for user interactions for extended visual analytics. In conclusion we implemented a first prototype to use AOI-DNAs and a first version of a fuzzy search to highlight eye movement patterns.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13865
        },
        {
          "affiliations": [],
          "personId": 12442
        },
        {
          "affiliations": [],
          "personId": 8280
        }
      ],
      "sessionIds": [
        1667
      ],
      "eventIds": []
    },
    {
      "id": 6756,
      "typeId": 11372,
      "title": "Encodji: Encoding Gaze Data Into Emoji Space for an amusing Scanpath Classification ;)",
      "trackId": 10564,
      "tags": [],
      "keywords": [],
      "abstract": "To this day, a variety of information has been obtained from human eye movements. This information aids in the understanding cognitive processes and classifying cognitive states. This paper deals with scanpath classification using unpaired image-to-image translation. We use generative adversarial networks (GANs) that learn the transformation between eye movements and emojis. Afterward, a convolutional neural network is trained to classify the stimulus image class based on the generated emoji. This work analyzes the effectiveness and usability of the emoji space for scanpath classification. In addition, we show how to train a generative adversarial network to generate novel emojis based on gaze data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16460
        },
        {
          "affiliations": [],
          "personId": 19606
        },
        {
          "affiliations": [],
          "personId": 17718
        },
        {
          "affiliations": [],
          "personId": 20400
        },
        {
          "affiliations": [],
          "personId": 18473
        },
        {
          "affiliations": [],
          "personId": 17919
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        1616
      ],
      "eventIds": []
    },
    {
      "id": 6757,
      "typeId": 11372,
      "title": "POI-Track: Improving Map-Based Planning with Implicit POI Tracking",
      "trackId": 10555,
      "tags": [],
      "keywords": [],
      "abstract": "Maps enable complex decision making, such as planning a day trip in a foreign city. This kind of task often requires combining information from different parts of the map leading to a sequence of visual searches and map extent changes. Hereby, the user can easily get lost, not being able to find back to relevant points of interest (POI). In this paper, we present POITrack, a novel gaze-adaptive map which supports a user in finding previously inspected POIs faster by providing highlights. Our approach allows filtering inspected POIs based on their category and automatically adapting the current map extent. Not only could participants find visited locations faster with our system, but they also rated the interaction as more pleasing. Our findings can contribute to improving the interaction with high-density visual information, which requires revisiting of previously seen objects whose relevance for the task may not have been clear initially.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18314
        },
        {
          "affiliations": [],
          "personId": 22641
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 5737,
      "typeId": 11406,
      "title": "Quantifying and Understanding the Differences in Visual Activities with Contrast Subsequences",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding differences and similarities between scanpaths has been one of the primary goals for eye tracking research. Sequences of areas of interest mapped from fixations are a major focus for many analytic techniques since these sequences directly relate to the semantic meaning of the visual input. Many studies analyze complete sequences while overlooking the micro-transitions in subsequences. In this paper, we propose a method which extracts subsequences as features and finds contrasting patterns between different viewer groups. The contrast patterns help domain experts to quantify variations between visual activities and understand reasoning processes for complex visual tasks. Experiments were conducted with 39 expert and novice radiographers using nine radiology images corresponding to nine levels of task complexity. Identified contrast patterns, validated by an expert, prove that the method effectively reveals visual reasoning processes that are otherwise hidden.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15596
        },
        {
          "affiliations": [],
          "personId": 20071
        },
        {
          "affiliations": [],
          "personId": 22683
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4975,
      "typeId": 11374,
      "title": "Recording and analyzing gaze during website interactions with EyeLink eye trackers",
      "trackId": 10553,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking can be a powerful tool in usability research and graphical interface design, providing important information concerning where users direct their attention to websites and applications they are interacting with. In website usability, for example, eye tracking can reveal important information about which areas of a web page are read, which areas are skipped, or even which areas increase cognitive workload. In traditional eye tracking, the researcher has tight control over what is shown, where it is shown and when it is shown. Analysis of the gaze data typically involves mapping gaze up with various areas of interest, and reporting measures such as fixation count and dwell time. Eye tracking for usability research, however, introduces a number of complications that traditional stimulus presentation and analysis software do not always deal with adequately. For example, the participant themselves determines what is shown, and when /where it is shown. As such, an accurate recording of the screen is critical. Web pages often contain dynamic (moving) content, and can themselves be scrolled, adding further complications to traditional analysis approaches, in which interest areas are typically static. This workshop will introduce new recording and analysis software from SR Research that allows researchers to record and quantify participants gaze whilst they interact with websites. Key features include screen and audio recording, keypress and mouse logging, the ability to provide a live preview of the gaze data during recording, automatic scroll compensation at the analysis stage, automatic data segmentation and navigation based on URLs, data aggregation from multiple participants, mouse event data visualization and extraction, and new report variables specific to web page tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10945
        }
      ],
      "sessionIds": [
        1089
      ],
      "eventIds": []
    },
    {
      "id": 6768,
      "typeId": 11372,
      "title": "Art Facing Science: Artistic Heuristics for Face Detection: Tracking Gaze When Looking at Faces",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "Automatic Area Of Interest (AOI) demarcation of facial regions is not yet commonplace in applied eye-tracking research, partially because automatic AOI labeling is prone to error. Most previous eye-tracking studies relied on manual frame-by-frame labeling of facial AOIs. We present a fully automatic approach for facial AOI labeling (i.e., eyes, nose, mouth) and gaze registration within those AOIs, based on modern computer vision techniques combined with artistic heuristics. We discuss details in computing gaze analytics, provide proof-of-concept, and a short validation against what we consider ground truth. Relative dwell time over expected AOIs exceeded 98% showing efficacy of the approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 8800
        },
        {
          "affiliations": [],
          "personId": 15617
        },
        {
          "affiliations": [],
          "personId": 16805
        }
      ],
      "sessionIds": [
        1667
      ],
      "eventIds": []
    },
    {
      "id": 4976,
      "typeId": 11406,
      "title": "High-Resolution Eye Tracking Using Scanning Laser Ophthalmoscopy",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Current eye-tracking techniques rely primarily on video-based tracking of components of the anterior surfaces of the eye. However, these trackers have several limitations. Their limited resolution precludes study of small fixational eye motion. Furthermore, many of these trackers rely on calibration procedures that do not offer a way to validate their eye motion traces. By comparison, retinal-image-based trackers can track the motion of the retinal image directly, at frequencies greater than 1kHz and with subarcminute accuracy. The retinal image provides a way to validate the eye position at any point in time, offering an unambiguous record of eye motion as a reference for the eye trace. The benefits of using scanning retinal imaging systems as eye trackers, however, comes at the price of different problems that are not present in video-based systems, and need to be solved to obtain robust eye traces. The current abstract provides an overview of retinal-image-based eye tracking methods, provides preliminary eye-tracking results from a tracking scanning-laser ophthalmoscope (TSLO), and proposes a new binocular line-scanning eye-tracking system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22816
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7284,
      "typeId": 11372,
      "title": "Exploring Simple Neural Network Architectures for Eye Movement Classification",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and postsaccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We eliminate ad-hoc post-processing to create a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16172
        },
        {
          "affiliations": [],
          "personId": 8221
        },
        {
          "affiliations": [],
          "personId": 18521
        }
      ],
      "sessionIds": [
        2184
      ],
      "eventIds": []
    },
    {
      "id": 3702,
      "typeId": 11401,
      "title": "Microsaccadic and Pupillary Response to Tactile Task Difficulty",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "The research goal is to explore the relationship between eye tracking measures and a tactile version of the n-back task. The n-back task is often used to evoke cognitive load, however this is the first study that incorporates tactile stimulus as input. The study follows a within-subject design with easy and difficult experimental conditions. In the tactile n-back task, each participant will be asked to identify the number of pins felt under the fingertips. In the easy condition, each participant will then be asked to respond if a number shown on the computer screen is congruent with the number of recognized pins. In the difficult condition, each participant will be asked to refer to the pin number in the current trial and the previous trial. Microsaccades and pupil dilation will be recorded during the top-down process of performing the n-back task.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9309
        }
      ],
      "sessionIds": [
        2079
      ],
      "eventIds": []
    },
    {
      "id": 7031,
      "typeId": 11369,
      "title": "Eye Tracking - From the Past to the Future",
      "trackId": 10563,
      "tags": [],
      "keywords": [],
      "abstract": "The talk starts emphasizing the social importance of human eye gaze \nfollowed by an overview of early eye tracking research. It continues \nwith some critical remarks on current eye tracking research done by the \nHCI community and finally gives an outlook on the future research topics\n of eye tracking for human computer interaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9946
        }
      ],
      "sessionIds": [
        2252
      ],
      "eventIds": []
    },
    {
      "id": 6263,
      "typeId": 11406,
      "title": "A Fitts' Law Study of Pupil Dilations in a Head-Mounted Display",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Head-mounted displays offer full control over lighting conditions. When equipped with eye tracking technology, they are well suited for experiments investigating pupil dilation in response to cognitive tasks, emotional stimuli, and motor task complexity, particularly for studies that would otherwise have required the use of a chinrest, since the eye cameras are fixed with respect to the head. This paper analyses pupil dilations for 13 out of 27 participants completing a Fitts' law task using a virtual reality headset with built-in eye tracking. The largest pupil dilation occurred for the condition subjectively rated as requiring the most physical and mental effort. Fitts' index of difficulty had no significant effect on pupil dilation, suggesting differences in motor task complexity may not affect pupil dilation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22176
        },
        {
          "affiliations": [],
          "personId": 11387
        },
        {
          "affiliations": [],
          "personId": 9338
        },
        {
          "affiliations": [],
          "personId": 22642
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5496,
      "typeId": 11372,
      "title": "Get a Grip: Slippage-Robust and Glint-Free Gaze Estimation for Real-Time Pervasive Head-Mounted Eye Tracking",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "A key assumption conventionally made by flexible head-mounted eye-tracking systems is often invalid: The eye center does not remain stationary w.r.t. the eye camera due to slippage. For instance, eye-tracker slippage might happen due to head acceleration or explicit adjustments by the user. As a result, gaze estimation accuracy can be significantly reduced. In this work, we propose Grip, a novel gaze estimation method capable of instantaneously compensating for eye-tracker slippage without additional hardware requirements such as glints or stereo eye camera setups. Grip was evaluated using previously collected data from a large scale unconstrained pervasive eye-tracking study. Our results indicate significant slippage compensation potential, decreasing average participant median angular offset by more than 43% w.r.t. a non-slippage-robust gaze estimation method. A reference implementation of Grip was integrated into EyeRecToo, an open-source hardware-agnostic eye-tracking software, thus making it readily accessible for multiple eye trackers (Available at: www.ti.uni-tuebingen.de/perception).",
      "authors": [
        {
          "affiliations": [],
          "personId": 17919
        },
        {
          "affiliations": [],
          "personId": 18642
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        1373
      ],
      "eventIds": []
    },
    {
      "id": 2937,
      "typeId": 11402,
      "title": "Discussion and standardisation of the metrics for eye movement detection",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "By now, a vast number of algorithms and approaches for detecting various eye movements (fixations, saccades, PSO, pursuit, OKN, etc.) have been proposed and evaluated by researchers in the field. The reported results are not always directly comparable and easily interpretable, even by experts. Part of this problem lies in the diversity of the metrics that are used to test the algorithms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12895
        },
        {
          "affiliations": [],
          "personId": 9133
        }
      ],
      "sessionIds": [
        1559
      ],
      "eventIds": []
    },
    {
      "id": 3450,
      "typeId": 11372,
      "title": "Towards a low cost and high speed mobile eye tracker",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present a stroboscopic catadioptric eye-tracking system, a novel approach for mobile ET based on rolling shutter cameras and stroboscopic structured infrared lighting. It uses a geometric model where the cornea acts as a spherical mirror in a catadioptric system, changing the projection as it moves. Calibration methods for the geometry of the system and for the gaze estimation are presented.By carefully adjusting the camera exposure and the lighting period, we show that multiple samples can be captured within a frame period. Instead of tracking common eye landmarks, such as the pupil center, we track multiple glints on the cornea. We assess the model in a simulated environment and also propose a prototype as a proof-of-concept. Our evaluations demonstrate the feasibility of our approach, which we envision as a step further in the direction of a mobile, robust, affordable, and high-speed eye tracker.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14044
        },
        {
          "affiliations": [],
          "personId": 16783
        }
      ],
      "sessionIds": [
        1373
      ],
      "eventIds": []
    },
    {
      "id": 6522,
      "typeId": 11406,
      "title": "Improving Real Time CNN-Based Pupil Detection Through Domain-Specific Data Augmentation",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Deep learning is a promising technique for real-world pupil detection applications. However, the small amount of available accurately-annotated data poses a challenge when training such networks. Here, we utilize non-challenging eye videos where algorithmic approaches perform virtually without errors to automatically generate a foundational data set containing subpixel pupil annotations. Then, we propose multiple domain-specific data augmentation methods to create unique training sets containing controlled distributions of pupil-detection challenges. The feasibility, convenience, and advantage of this approach is demonstrated by training a CNN with these datasets. The resulting network outperformed current methods in multiple publicly-available, realistic, and challenging datasets, despite being trained solely with the augmented eye images. This network also exhibited better generalization w.r.t. the latest state-of-the-art CNN: Whereas on datasets similar to training data, the nets displayed similar performance, on datasets unseen to both networks, ours outperformed the state-of-the-art by ~27 in terms of detection rate.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11665
        },
        {
          "affiliations": [],
          "personId": 10793
        },
        {
          "affiliations": [],
          "personId": 23351
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6267,
      "typeId": 11410,
      "title": "iLid: Eyewear Solution for Low-power Fatigue and Drowsiness Monitoring",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsi- ness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can contin- ually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percent- age of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline and implementation on a prototype computational eyeglass platform.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15473
        },
        {
          "affiliations": [],
          "personId": 11286
        },
        {
          "affiliations": [],
          "personId": 15235
        },
        {
          "affiliations": [],
          "personId": 23223
        },
        {
          "affiliations": [],
          "personId": 12339
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4219,
      "typeId": 11372,
      "title": "Using Developer Eye Movements to Externalize the Mental Model Used in Code Summarization Tasks",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements of developers are used to speculate the mental cognition model (i.e., bottom-up or top-down) applied during program comprehension tasks. The cognition models examine how programmers understand source code by describing the temporary information structures in the programmer's short term memory. The two types of models that we are interested in are top-down and bottom-up. The top-down model is normally applied as-needed (i.e., the domain of the system is familiar). The bottom-up model is typically applied when a developer is not familiar with the domain or the source code. An eye-tracking study of 18 developers reading and summarizing Java methods is used as our dataset for analyzing the mental cognition model. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. The results indicate that on average, experts and novices read the methods more closely (using the bottom-up mental model) than bouncing around (using top-down). However, on average novices spend longer gaze time performing bottom-up (66s.) compared to experts (43s.)",
      "authors": [
        {
          "affiliations": [],
          "personId": 11740
        },
        {
          "affiliations": [],
          "personId": 23193
        },
        {
          "affiliations": [],
          "personId": 14118
        }
      ],
      "sessionIds": [
        2060
      ],
      "eventIds": []
    },
    {
      "id": 3965,
      "typeId": 11372,
      "title": "Gaze awareness improves collaboration efficiency in a physical collaborative assembly task",
      "trackId": 10555,
      "tags": [],
      "keywords": [],
      "abstract": "In building human robot interaction systems, it would be helpful to understand how humans collaborate, and in particular, how humans use others' gaze behavior to estimate their intent. Here we studied the use of gaze in a collaborative assembly task, where a human user assembled an object with the assistance of a human helper. We found that the being aware of the partner's gaze significantly improved collaboration efficiency. Task completion times were much shorter when gaze communication was available, than when it was blocked. In addition, we found that the user's gaze was more likely to lie on the object of interest in the gaze awareness case than the gaze block case. In the context of human-robot collaboration systems, our results suggest that gaze data from the beginning of the verbal request to one second after the request is most informative and can be used to predict the target object.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20010
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 3966,
      "typeId": 11406,
      "title": "Exploring Simple Neural Network Architectures for Eye Movement Classification",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Analysis of eye-gaze is a critical tool for studying human-computer interaction and visualization. Yet eye tracking systems only report eye-gaze on the scene by producing large volumes of coordinate time series data. To be able to use this data, we must first extract salient events such as eye fixations, saccades, and postsaccadic oscillations (PSO). Manually extracting these events is time-consuming, labor-intensive and subject to variability. In this paper, we present and evaluate simple and fast automatic solutions for eye-gaze analysis based on supervised learning. Similar to some recent studies, we developed different simple neural networks demonstrating that feature learning produces superior results in identifying events from sequences of gaze coordinates. We eliminate ad-hoc post-processing to create a fully automated end-to-end algorithms that perform as good as current state-of-the-art architectures. Once trained they are fast enough to be run in a near real time setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16172
        },
        {
          "affiliations": [],
          "personId": 8221
        },
        {
          "affiliations": [],
          "personId": 18521
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6531,
      "typeId": 11372,
      "title": "A Comparative Study of Eye Tracking and Hand Controller for Aiming Tasks in Virtual Reality",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "Aiming is key for virtual reality (VR) interaction, and it is often done using VR controllers. Recent eye tracking integrations in commercial VR head-mounted displays (HMDs) call for further research on usability and performance aspects to determine possibilities and limitations better. This paper presents a user study exploring gaze aiming in VR compared to a traditional controller in an aim and shoot task. Different speeds of targets and trajectories were studied. Qualitative data was gathered using the system usability scale (SUS) and cognitive load (NASA TLX) questionnaires. Results show a lower perceived cognitive load using gaze aiming and on par usability scale. Gaze aiming produced on par task duration but lower accuracy on most conditions. Lastly, the trajectory of the target significantly affected the orientation of the HMD in relation to the target's location. The results show potential using gaze aiming in VR and motivate further research.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14443
        },
        {
          "affiliations": [],
          "personId": 20473
        }
      ],
      "sessionIds": [
        2252
      ],
      "eventIds": []
    },
    {
      "id": 7050,
      "typeId": 11406,
      "title": "Reading Detection in Real-time",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a new reading detection pipeline that incorporates both global and local components of the existing reading detection paradigms. For our methodology, ground truth is determined globally, from a task driven user study. This allows us to avoid the tedium and bias of annotating individual gaze tracks as ground truth reading. From this set up we utilize a Region Ranking SVM (RRSVM), a specific genre of classifier that learns local importances while optimizing for the global inference goal, allowing us to learn which local fixation windows play a role in defining reading of skimming, without actually defining so explicitly. Doing so givers us a detector which can provide reading and skimming in both global and local contexts, using only coarse grained ground truth.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10666
        },
        {
          "affiliations": [],
          "personId": 16757
        },
        {
          "affiliations": [],
          "personId": 10496
        },
        {
          "affiliations": [],
          "personId": 21379
        },
        {
          "affiliations": [],
          "personId": 17397
        },
        {
          "affiliations": [],
          "personId": 23292
        },
        {
          "affiliations": [],
          "personId": 10555
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4747,
      "typeId": 11410,
      "title": "GazeVR: A Toolkit for Developing Gaze Interactive Applications in VR/AR",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "For head mounted displays, like they are used in mixed reality applications, eye gaze seems to be a natural interaction modality. EyeMRTK provides building blocks for eye gaze interaction in virtual and augmented reality. Based on a hardware abstraction layer, it allows interaction researchers and developers to focus on their interaction concepts, while enabling them to evaluate their ideas on all supported systems. In addition to that, the toolkit provides a simulation layer for debugging purposes, which speeds up prototyping during development on the desktop.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19079
        },
        {
          "affiliations": [],
          "personId": 23535
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 2699,
      "typeId": 11404,
      "title": "Eye Tracking Sensors Past, Present, Future and Their Applications",
      "trackId": 10560,
      "tags": [],
      "keywords": [],
      "abstract": "The availability of eye tracking sensors is set to explode, with billions of units available in future Virtual Reality (VR) and Augmented Reality (AR) platforms. In my talk I will discuss the past and present status of eye tracking sensors, along with my vision for future development. I will also discuss applications that necessitate the presence of such sensors in VR/AR devices, along with applications that have the power to benefit society on a large scale when VR/AR solutions are widely adopted.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15160
        }
      ],
      "sessionIds": [
        1732
      ],
      "eventIds": []
    },
    {
      "id": 6540,
      "typeId": 11406,
      "title": "Automatic quick-phase detection in bedside recordings from patients with acute dizziness and nystagmus",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Benign Paroxysmal Positional Vertigo (BPPV) is the most common cause of vertigo. Vestibular experts can diagnose and treat this condition with simple maneuvers. However, there is a high rate of misdiagnosis that results in high medical costs when non-specialists use expensive, time-consuming neuroimaging techniques. Here we show how to improve saccade detection methods to detect quick-phases of nystagmus, a key sign of BPPV, in data recorded during the Dix-Hallpike maneuver.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17394
        },
        {
          "affiliations": [],
          "personId": 19945
        },
        {
          "affiliations": [],
          "personId": 20774
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6284,
      "typeId": 11402,
      "title": "Discussion and standardisation of the metrics for eye movement detection",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "By now, a vast number of algorithms and approaches for detecting various eye movements (fixations, saccades, PSO, pursuit, OKN, etc.) have been proposed and evaluated by researchers in the field. The reported results are not always directly comparable and easily interpretable, even by experts. Part of this problem lies in the diversity of the metrics that are used to test the algorithms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12895
        },
        {
          "affiliations": [],
          "personId": 9133
        }
      ],
      "sessionIds": [
        2099
      ],
      "eventIds": []
    },
    {
      "id": 6287,
      "typeId": 11372,
      "title": "Visually Analyzing Eye Movements on Natural Language Texts and Source Code Snippets",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we analyze eye movement data of 26 participants using a quantitative and qualitative approach to investigate how people read natural language text in comparison to source code. In particular, we use the radial transition graph visualization to explore strategies of participants during these reading tasks and extract common patterns amongst participants. We illustrate via examples how visualization can play a role at uncovering behavior of people while reading natural language text versus source code. Our results show that the linear reading order of natural text is only partially applicable to source code reading. We found patterns representing a linear order and also patterns that represent reading of the source code in execution order. Participants also focus more on those areas that are important to comprehend core functionality and we found that they skip unimportant constructs such as brackets.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15225
        },
        {
          "affiliations": [],
          "personId": 14118
        }
      ],
      "sessionIds": [
        2060
      ],
      "eventIds": []
    },
    {
      "id": 5775,
      "typeId": 11410,
      "title": "Detecting cognitive bias in a relevance assessment task using an eye tracker",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Cognitive biases, such as the bandwagon effect, occur when a participant places a disproportionate emphasis on external information when making decisions under uncertainty. These effects are challenging for humans to overcome – even when they are explicitly made aware of their own biases. One challenge for researchers is to detect if the information is used in decision making and to what degree. One can gain a better understanding of how this external information is used in decision making using an eye tracker. In this paper, we evaluate cognitive biases in the context of assessing the binary relevance of a set of documents in response to a given information need. We show that these cognitive biases can be observed by examining gaze time in Areas of Interest (AOI) that contain this pertinent external information.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11954
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 3728,
      "typeId": 11406,
      "title": "Detecting cognitive bias in a relevance assessment task using an eye tracker",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Cognitive biases, such as the bandwagon effect, occur when a participant places a disproportionate emphasis on external information when making decisions under uncertainty. These effects are challenging for humans to overcome – even when they are explicitly made aware of their own biases. One challenge for researchers is to detect if the information is used in decision making and to what degree. One can gain a better understanding of how this external information is used in decision making using an eye tracker. In this paper, we evaluate cognitive biases in the context of assessing the binary relevance of a set of documents in response to a given information need. We show that these cognitive biases can be observed by examining gaze time in Areas of Interest (AOI) that contain this pertinent external information.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11954
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7824,
      "typeId": 11407,
      "title": "Estimation of Situation Awareness Score and Performance  Using Eye and Head Gaze for Human-Robot Collaboration",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "Human attention processes play a major role in the optimization of human-robot collaboration (HRC) systems. This work describes a novel methodology to measure and predict situation awareness and from this overall performance from gaze features in real-time. The awareness about scene objects of interest is described by 3D gaze analysis using data from wearable eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRC were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict standard measures of situation awareness (SAGAT, SART) as well as performance in the HRI task in real-time and will open new opportunities for human factors based performance optimization in HRI applications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15867
        },
        {
          "affiliations": [],
          "personId": 14867
        },
        {
          "affiliations": [],
          "personId": 19187
        },
        {
          "affiliations": [],
          "personId": 13237
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6546,
      "typeId": 11372,
      "title": "Finding the Outliers in Scanpath Data",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we describe the design and implementation of an interactive visualization tool for the comparison of eye movement data with a special focus on the outliers. We explain the steps we took in order to design and create this tool, as well as the design goals. Furthermore, we will explain the implementation details and discuss further improvements. In order to make the tool usable and accessible to anyone with a data science background, we provide a web-based solution by using the Dash library based on the Python programming language and the Python library Plotly. Interactive visualization is very well supported by Dash, which makes the visualization tool easy to use, without any restrictions. The visualization tool supports multiple ways of comparing user scanpaths, as well as showing gaze plots and visual attention maps. Finally, we discuss scalability issues and limitations of our approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 21349
        },
        {
          "affiliations": [],
          "personId": 22761
        },
        {
          "affiliations": [],
          "personId": 15150
        },
        {
          "affiliations": [],
          "personId": 9408
        },
        {
          "affiliations": [],
          "personId": 10114
        },
        {
          "affiliations": [],
          "personId": 11163
        },
        {
          "affiliations": [],
          "personId": 23011
        }
      ],
      "sessionIds": [
        1150
      ],
      "eventIds": []
    },
    {
      "id": 3218,
      "typeId": 11372,
      "title": "Assessing Surgeons' Skill Level in Laparoscopic Cholecystectomy using Eye Metrics",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Laparoscopic surgery has revolutionised state of the art in surgical health care. However, its complexity puts a significant burden on the surgeon's cognitive resources resulting in major biliary injuries. With the increasing number of laparoscopic surgeries, it is crucial to identify surgeons' cognitive loads and levels of focus in real time to give them unobtrusive feedback when detecting the suboptimal level of attention. Assuming that the experts appear to be more focused on attention, we investigate how the skill level of surgeons during live surgery is reflected through eye metrics. Forty-two laparoscopic surgeries have been conducted with four surgeons who have different expertise levels. Concerning eye metrics, we have used six metrics which belong to fixation and pupillary based metrics. With the use of mean, standard deviation and ANOVA test we have proven three reliable metrics which we can use to differentiate the skill level during live surgeries.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10344
        },
        {
          "affiliations": [],
          "personId": 13931
        },
        {
          "affiliations": [],
          "personId": 18969
        },
        {
          "affiliations": [],
          "personId": 22110
        },
        {
          "affiliations": [],
          "personId": 16584
        },
        {
          "affiliations": [],
          "personId": 16009
        },
        {
          "affiliations": [],
          "personId": 19285
        },
        {
          "affiliations": [],
          "personId": 23138
        }
      ],
      "sessionIds": [
        2220
      ],
      "eventIds": []
    },
    {
      "id": 6296,
      "typeId": 11406,
      "title": "Motion Tracking of Iris Features for Eye tracking",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Current video-based eye trackers fail to acquire a high signal-to-noise (SNR) ratio which is crucial for specific applications like interactive systems, event detection, the study of various eye movements, and most importantly estimating the gaze position with high certainty. Specifically, current video-based eye trackers over-rely on precise localization of the pupil boundary and/or corneal reflection for gaze tracking, which often results in inaccuracies and large sample-to-sample root mean square (RMS-S2S). Therefore, it is crucial to address the shortcomings of these trackers, and we plan to study a new video-based eye tracking methodology focused on simultaneously tracking the motion of many iris features and investigate its implications for obtaining high accuracy and precision. In our preliminary work, the method has shown great potential for robust detection of microsaccades over 0.2 degrees with high confidence. We plan to explore and optimize this technique.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9750
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 3994,
      "typeId": 11410,
      "title": "Quantifying and Understanding the Differences in Visual Activities with Contrast Subsequences",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding differences and similarities between scanpaths has been one of the primary goals for eye tracking research. Sequences of areas of interest mapped from fixations are a major focus for many analytic techniques since these sequences directly relate to the semantic meaning of the visual input. Many studies analyze complete sequences while overlooking the micro-transitions in subsequences. In this paper, we propose a method which extracts subsequences as features and finds contrasting patterns between different viewer groups. The contrast patterns help domain experts to quantify variations between visual activities and understand reasoning processes for complex visual tasks. Experiments were conducted with 39 expert and novice radiographers using nine radiology images corresponding to nine levels of task complexity. Identified contrast patterns, validated by an expert, prove that the method effectively reveals visual reasoning processes that are otherwise hidden.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15596
        },
        {
          "affiliations": [],
          "personId": 20071
        },
        {
          "affiliations": [],
          "personId": 22683
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5787,
      "typeId": 11401,
      "title": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movements during reading and reading assessment in Swedish school children – a new window on reading difficulties",
      "authors": [
        {
          "affiliations": [],
          "personId": 23724
        }
      ],
      "sessionIds": [
        1410
      ],
      "eventIds": []
    },
    {
      "id": 7324,
      "typeId": 11406,
      "title": "Pupil Diameter as a Measure of Emotion and Sickness in VR",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking is rapidly becoming popular in consumer technology, including virtual and augmented reality. Eye trackers commonly provide an estimate of gaze location, and pupil diameter. Pupil diameter is useful for interactive systems, as it provides means to estimate cognitive load, stress, and emotional state. However, there are several roadblocks that limit the use of pupil diameter. In VR HMDs there are a lack of models that account for stereoscopic viewing and the increased brightness of near eye displays. Existing work has shown correlations between pupil diameter and emotion, but have not been extended to VR environments. The scope of this work is to bridge the gap between existing research on emotion and pupil diameter to VR, while also attempting to use pupillary data to tackle the problem of simulator sickness in VR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16335
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6817,
      "typeId": 11372,
      "title": "GeoGCD: Improved Visual Search via Gaze-Contingent Display",
      "trackId": 10555,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze-Contingent Displays (GCDs) can improve visual search performance on large displays. GCDs, a Level Of Detail (LOD) management technique, discards redundant peripheral detail using various human visual perception models. Models of depth and contrast perception (e.g., depth-of-field and foveation) have often been studied to address the trade-off between the computational and perceptual benefits of GCDs. However, color perception models and combinations of multiple models have not received as much attention. In this paper, we present GeoGCD which uses individual contrast, color, and depth-perception models, and their combination to render scenes without perceptible latency. As proof-of-concept, we present a three-stage user evaluation built upon geographic image interpretation tasks. GeoGCD does not impair users’ visual search performance or affect their display preferences. On the contrary, in some cases, it can significantly improve users’ performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9468
        },
        {
          "affiliations": [],
          "personId": 23359
        },
        {
          "affiliations": [],
          "personId": 13791
        },
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 17699
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 6307,
      "typeId": 11410,
      "title": "A Gaze-Based Experimenter Platform for Designing and Evaluating Adaptive Interventions in Information Visualizations",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "We present an experimenter platform for designing and evaluating user-adaptive support in information visualizations. Specifically, this platform leverages eye-tracking data in real time to deliver adaptive support in visualizations based on the user's intentional patterns over the visualization, and their individual traits and states. We describe the main functionalities of this platform, and show an application of this platform to support processing of textual documents with embedded bar charts, by dynamically providing highlighting in the charts to guide a user’s attention to the relevant information. We expect this platform to ease the evaluation of different forms of adaptation across visualizations and tasks, which is crucial to broaden our understanding of how user adaptation can improve the effectiveness of information visualizations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14958
        },
        {
          "affiliations": [],
          "personId": 21848
        },
        {
          "affiliations": [],
          "personId": 20719
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5284,
      "typeId": 11373,
      "title": "Town hall meeting & ETRA 2020 Introduction",
      "trackId": 10565,
      "tags": [],
      "keywords": [],
      "abstract": "Town hall meeting, ETRA 2020 Introduction",
      "authors": [
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 11308
        },
        {
          "affiliations": [],
          "personId": 23599
        },
        {
          "affiliations": [],
          "personId": 16805
        },
        {
          "affiliations": [],
          "personId": 14118
        },
        {
          "affiliations": [],
          "personId": 19815
        }
      ],
      "sessionIds": [
        1516
      ],
      "eventIds": []
    },
    {
      "id": 5797,
      "typeId": 11372,
      "title": "Visually Comparing Eye Movements over Space and Time",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "Analyzing and visualizing eye movement data can provide useful insights into the connectivities and linkings of points and areas of interest. Those typically time-varying relations can give hints about applied visual scanning strategies by either individual or many eye tracked people. However, the challenging issue with this kind of data is its spatio-temporal nature requiring a good visual encoding in order to ﬁrst, achieve a scalable overview-based diagram, and second, to derive static or dynamic patterns that might correspond to certain comparable visual scanning strategies. To reliably identify the dynamic strategies we describe a visualization technique that generates a more linear representation of the spatio-temporal scan paths. This is achieved by applying different visual encodings of the spatial dimensions that typically build a limitation for an eye movement data visualization causing visual clutter effects, overdraw, and occlusions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21349
        },
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 22761
        }
      ],
      "sessionIds": [
        1150
      ],
      "eventIds": []
    },
    {
      "id": 4517,
      "typeId": 11410,
      "title": "Pupil Diameter as a Measure of Emotion and Sickness in VR",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking is rapidly becoming popular in consumer technology, including virtual and augmented reality. Eye trackers commonly provide an estimate of gaze location, and pupil diameter. Pupil diameter is useful for interactive systems, as it provides means to estimate cognitive load, stress, and emotional state. However, there are several roadblocks that limit the use of pupil diameter. In VR HMDs there are a lack of models that account for stereoscopic viewing and the increased brightness of near eye displays. Existing work has shown correlations between pupil diameter and emotion, but have not been extended to VR environments. The scope of this work is to bridge the gap between existing research on emotion and pupil diameter to VR, while also attempting to use pupillary data to tackle the problem of simulator sickness in VR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16335
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 6822,
      "typeId": 11372,
      "title": "A Novel Gaze Event Detection Metric That Is Not Fooled by Gaze-independent Baselines",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Eye movement detectors are typically evaluated either in isolation or in comparison to previously introduced approaches. We, however, first introduce and thoroughly evaluate a set of both random and above-chance baselines that are completely independent of the eye tracking signal. Surprisingly, our baselines often show performance that is either comparable to, or even exceeds the scores of dedicated eye movement detectors, for smooth pursuit in particular. In these cases, it may be that (i) algorithm performance is poor, (ii) the dataset is overly simplistic with little variability of eye movements, or, alternatively, (iii) the currently used metrics are inappropriate. Based on these observations, we discuss the level of stimulus dependency of the eye movements in four datasets. Finally, we propose a novel measure of agreement between true and assigned eye movement events, which, unlike existing metrics, is able to reveal the expected performance gap between the baselines and dedicated algorithms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12895
        },
        {
          "affiliations": [],
          "personId": 19150
        },
        {
          "affiliations": [],
          "personId": 8300
        }
      ],
      "sessionIds": [
        1666
      ],
      "eventIds": []
    },
    {
      "id": 5032,
      "typeId": 11410,
      "title": "Estimation of Situation Awareness Score and Performance  Using Eye and Head Gaze for Human-Robot Collaboration",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Human attention processes play a major role in the optimization of human-robot collaboration (HRC) systems. This work describes a novel methodology to measure and predict situation awareness and from this overall performance from gaze features in real-time. The awareness about scene objects of interest is described by 3D gaze analysis using data from wearable eye tracking glasses and a precise optical tracking system. A probabilistic framework of uncertainty considers coping with measurement errors in eye and position estimation. Comprehensive experiments on HRC were conducted with typical tasks including handover in a lab based prototypical manufacturing environment. The methodology is proven to predict standard measures of situation awareness (SAGAT, SART) as well as performance in the HRI task in real-time and will open new opportunities for human factors based performance optimization in HRI applications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15867
        },
        {
          "affiliations": [],
          "personId": 14867
        },
        {
          "affiliations": [],
          "personId": 19187
        },
        {
          "affiliations": [],
          "personId": 13237
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 6568,
      "typeId": 11410,
      "title": "High-Resolution Eye Tracking Using Scanning Laser Ophthalmoscopy",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Current eye-tracking techniques rely primarily on video-based tracking of components of the anterior surfaces of the eye. However, these trackers have several limitations. Their limited resolution precludes study of small fixational eye motion. Furthermore, many of these trackers rely on calibration procedures that do not offer a way to validate their eye motion traces. By comparison, retinal-image-based trackers can track the motion of the retinal image directly, at frequencies greater than 1kHz and with subarcminute accuracy. The retinal image provides a way to validate the eye position at any point in time, offering an unambiguous record of eye motion as a reference for the eye trace. The benefits of using scanning retinal imaging systems as eye trackers, however, comes at the price of different problems that are not present in video-based systems, and need to be solved to obtain robust eye traces. The current abstract provides an overview of retinal-image-based eye tracking methods, provides preliminary eye-tracking results from a tracking scanning-laser ophthalmoscope (TSLO), and proposes a new binocular line-scanning eye-tracking system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22816
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5545,
      "typeId": 11411,
      "title": "General Discussion and ETVIS Best Papers Awards",
      "trackId": 10566,
      "tags": [],
      "keywords": [],
      "abstract": "General Discussion and ETVIS Best Papers Awards",
      "authors": [
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 9746
        },
        {
          "affiliations": [],
          "personId": 14099
        }
      ],
      "sessionIds": [
        1150
      ],
      "eventIds": []
    },
    {
      "id": 4265,
      "typeId": 11410,
      "title": "A Fitts' Law Study of Pupil Dilations in a Head-Mounted Display",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Head-mounted displays offer full control over lighting conditions. When equipped with eye tracking technology, they are well suited for experiments investigating pupil dilation in response to cognitive tasks, emotional stimuli, and motor task complexity, particularly for studies that would otherwise have required the use of a chinrest, since the eye cameras are fixed with respect to the head. This paper analyses pupil dilations for 13 out of 27 participants completing a Fitts' law task using a virtual reality headset with built-in eye tracking. The largest pupil dilation occurred for the condition subjectively rated as requiring the most physical and mental effort. Fitts' index of difficulty had no significant effect on pupil dilation, suggesting differences in motor task complexity may not affect pupil dilation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22176
        },
        {
          "affiliations": [],
          "personId": 11387
        },
        {
          "affiliations": [],
          "personId": 9338
        },
        {
          "affiliations": [],
          "personId": 22642
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 3498,
      "typeId": 11372,
      "title": "Task Classification Model for Visual Fixation, Exploration, and Search",
      "trackId": 10564,
      "tags": [],
      "keywords": [],
      "abstract": "Yarbus' claim to decode the observer's task from eye movements has received mixed reactions. In this paper, we have supported the hypothesis that it is possible to decode the task. We conducted an exploratory analysis on the dataset by projecting features and data points into a scatter plot to visualize the nuance properties for each task. Following this analysis, we eliminated highly correlated features before training an SVM and Ada Boosting classifier to predict the tasks from this filtered eye movements data. We achieve an accuracy of 95.4% on this task classification problem and hence, support the hypothesis that task classification is possible from a user's eye movement data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21349
        },
        {
          "affiliations": [],
          "personId": 9144
        },
        {
          "affiliations": [],
          "personId": 10978
        },
        {
          "affiliations": [],
          "personId": 14452
        },
        {
          "affiliations": [],
          "personId": 22761
        }
      ],
      "sessionIds": [
        1616
      ],
      "eventIds": []
    },
    {
      "id": 5034,
      "typeId": 11407,
      "title": "iLid: Eyewear Solution for Low-power Fatigue and Drowsiness Monitoring",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to monitor eye closures and blink patterns has long been known to enable accurate assessment of fatigue and drowsi- ness in individuals. Many measures of the eye are known to be correlated with fatigue including coarse-grained measures like the rate of blinks as well as fine-grained measures like the duration of blinks and the extent of eye closures. Despite a plethora of research validating these measures, we lack wearable devices that can contin- ually and reliably monitor them in the natural environment. In this work, we present a low-power system, iLid, that can continually sense fine-grained measures such as blink duration and Percent- age of Eye Closures (PERCLOS) at high frame rates of 100fps. We present a complete solution including design of the sensing, signal processing, and machine learning pipeline and implementation on a prototype computational eyeglass platform.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15473
        },
        {
          "affiliations": [],
          "personId": 11286
        },
        {
          "affiliations": [],
          "personId": 15235
        },
        {
          "affiliations": [],
          "personId": 23223
        },
        {
          "affiliations": [],
          "personId": 12339
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4779,
      "typeId": 11372,
      "title": "Getting (More) Real: Bringing Eye Movement Classification to HMD Experiments with Equirectangular Stimuli",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Over recent years, we have experienced an increasing shift towards more immersive experimental scenarios with the use of eye-tracking enabled glasses and head-mounted displays. In these new scenarios, however, most of the existing eye movement classification algorithms cannot be applied robustly anymore because they were developed for monitor-based experiments. In this paper, we first discuss how decision criteria have to change in the space of 360 videos, and use these criteria to modify five popular algorithms from the literature and make them publicly available. Then, for cases where an existing algorithm cannot be modified (e.g. closed-source) we present an approach that maps the data to an undistorted space. An empirical evaluation of both approaches shows that they significantly reduce the artifacts of the initial algorithm, especially in areas further from the horizontal midline.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12798
        },
        {
          "affiliations": [],
          "personId": 8300
        }
      ],
      "sessionIds": [
        1373
      ],
      "eventIds": []
    },
    {
      "id": 5291,
      "typeId": 11410,
      "title": "Boosting Speed- and Accuracy of Gradient based Dark Pupil Tracking using Vectorization and Differential Evolution",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Gradient based dark pupil tracking [Timm and Barth 2011] is a simple and robust algorithm for pupil center estimation. The algorithm's time complexity of O(n^4) can be tackled by applying a two-stage process (coarse center estimation followed by a windowed refinement), as well as by optimizing and parallelizing code using cache-friendly data structures, vector-extensions of modern CPU's and GPU acceleration. We could achieve a substantial speed up compared to a non-optimized implementation: 12x using vector extensions and 65x using a GPU. Further, the two-stage process combined with parameter optimization using differential evolution considerably increased the accuracy of the algorithm. We evaluated our implementation using the „Labelled pupils the wild“ dataset. The percentage of frames with a pixel error below 15px increased from 28% to 72%, surpassing algorithmically more complex algorithms like ExCuse (64%) and catching up with recent algorithms like PuRe (87%).",
      "authors": [
        {
          "affiliations": [],
          "personId": 12258
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 8108,
      "typeId": 11372,
      "title": "Power-efficient and shift-robust eye-tracking sensor for portable VR headsets",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15488
        },
        {
          "affiliations": [],
          "personId": 13563
        },
        {
          "affiliations": [],
          "personId": 15160
        }
      ],
      "sessionIds": [
        1373
      ],
      "eventIds": []
    },
    {
      "id": 6829,
      "typeId": 11372,
      "title": "Attention Towards Privacy Notifications on Web Pages",
      "trackId": 10558,
      "tags": [],
      "keywords": [],
      "abstract": "A significant number of Internet users is unaware of privacy threats and they might not pay much attention to privacy notifications. It is imperative to create effective but, at the same time, non-distracting notifications about privacy policy of different web services. The arti- cle presents an eye-tracking study (N = 16) testing the effectiveness of different common types of privacy notification in capturing and retaining users’ attention. Results showed that an non-intrusive no- tification presented at the top of the web page may be as effective as intrusive notifications in capturing attention. Results are discussed in terms of well-known effects of visual attention distribution on web pages. Present findings also offer practical conclusions for web service developers and publishers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13925
        },
        {
          "affiliations": [],
          "personId": 17160
        },
        {
          "affiliations": [],
          "personId": 16805
        },
        {
          "affiliations": [],
          "personId": 20549
        }
      ],
      "sessionIds": [
        2374
      ],
      "eventIds": []
    },
    {
      "id": 4783,
      "typeId": 11410,
      "title": "Motion Tracking of Iris Features for Eye tracking",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Current video-based eye trackers fail to acquire a high signal-to-noise (SNR) ratio which is crucial for specific applications like interactive systems, event detection, the study of various eye movements, and most importantly estimating the gaze position with high certainty. Specifically, current video-based eye trackers over-rely on precise localization of the pupil boundary and/or corneal reflection for gaze tracking, which often results in inaccuracies and large sample-to-sample root mean square (RMS-S2S). Therefore, it is crucial to address the shortcomings of these trackers, and we plan to study a new video-based eye tracking methodology focused on simultaneously tracking the motion of many iris features and investigate its implications for obtaining high accuracy and precision. In our preliminary work, the method has shown great potential for robust detection of microsaccades over 0.2 degrees with high confidence. We plan to explore and optimize this technique.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9750
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 2735,
      "typeId": 11372,
      "title": "Monocular Gaze Depth Estimation using the Vestibulo-Ocular Reflex",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze depth estimation presents a challenge for eye tracking in 3D. This work investigates a novel approach to the problem based on eye movement mediated by the vestibulo-ocular reflex (VOR). VOR stabilises gaze on a target during head movement, with eye movement in the opposite direction, and the VOR gain increases the closer the fixated target is to the viewer. We present a theoretical analysis of the relationship between VOR gain and depth which we investigate with empirical data collected in a user study (N=10). We show that VOR gain can be captured using pupil centres, and propose and evaluate a practical method for gaze depth estimation based on a generic function of VOR gain and two-point depth calibration. The results show that VOR gain is comparable with vergence in capturing depth while only requiring one eye, and provide insight into open challenges in harnessing VOR gain as a robust measure.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19079
        },
        {
          "affiliations": [],
          "personId": 13355
        },
        {
          "affiliations": [],
          "personId": 19275
        }
      ],
      "sessionIds": [
        1373
      ],
      "eventIds": []
    },
    {
      "id": 7600,
      "typeId": 11401,
      "title": "The vision and interpretation of paintings: bottom-up visual processes, top-down culturally informed attention, and aesthetic experience.",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "We aim to contribute to our knowledge about how we experience paintings, and how visual exploration, cognitive categorization and emotive evaluation contribute to its aesthetic dimension [1], [2], [3]. Visual exploration activates universal visio-mimetic capacities, however these capacities are implemented culturally and historically [4]. We combine psychological, cognitive, and historical studies to address the question of the historically variable “forms of intentionality” [5] of our ways of looking at paintings, investigting how the notion of a “period eye” [6], [7] can be accommodated with the constancy of human vision. For this purpose, we use eye-tracking data of 52 participants looking at the Isenheim altarpiece before and after restoration. First results allowed us to classify the zones of salience as well as the effects of participants’ backgrounds and emotions on fixation time and visual attention.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23565
        },
        {
          "affiliations": [],
          "personId": 19375
        },
        {
          "affiliations": [],
          "personId": 10061
        }
      ],
      "sessionIds": [
        1207
      ],
      "eventIds": []
    },
    {
      "id": 6068,
      "typeId": 11402,
      "title": "Deep Learning in the Eye Tracking World",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "Recently deep learning has become a hype word in computer science. Many problems, which till now could be solved only using sophisticated algorithms, can be now solved with specially developed neural networks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9746
        }
      ],
      "sessionIds": [
        1487
      ],
      "eventIds": []
    },
    {
      "id": 4533,
      "typeId": 11372,
      "title": "Attentional orienting in virtual reality using endogenous and exogenous cues in auditory and visual modalities",
      "trackId": 10555,
      "tags": [],
      "keywords": [],
      "abstract": "The virtual reality (VR) has numerous applications in training, education, and rehabilitation. To efficiently present the immersive 3D stimuli, we need to understand the attention orientation in VR. The efficiency of different cues can be compared using the Posner effect. In this study, we designed an ecological environment where participants were presented with the Posner cueing paradigm. 20 subjects equipped with an eye-tracking system and VR HMD performed a sandwich preparation task. They were asked to assemble the ingredients following endogenous and exogenous cues in both auditory and visual modalities. The results showed that all valid cues made participants react faster. While directional arrow (visual endogenous) and 3D sound (auditory exogenous) oriented attention globally to the cued hemifield, the vocal instruction (auditory endogenous) and object highlighting (visual exogenous) allowed more local orientation. No differences in gaze shift initiation nor time to fixate the target were found indicating the covert orienting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24170
        },
        {
          "affiliations": [],
          "personId": 8283
        },
        {
          "affiliations": [],
          "personId": 16606
        },
        {
          "affiliations": [],
          "personId": 24132
        }
      ],
      "sessionIds": [
        1081
      ],
      "eventIds": []
    },
    {
      "id": 2997,
      "typeId": 11372,
      "title": "Semantic Gaze Labeling for Human-Robot Shared Manipulation",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Human-robot collaboration systems benefit from recognizing people's intentions. This capability is especially useful for collaborative manipulation applications, in which users operate robot arms to manipulate objects. For collaborative manipulation, systems can determine intention by tracking eye gaze and identifying gaze fixations with particular objects in the scene (i.e., semantic gaze labeling). One approach is to assign each fixation to the object closest to it. However, calibration drift, head motion, and the extra dimension required for real-world interactions make this position matching approach inaccurate. In this work, we introduce velocity features that compare the relative motion between subsequent gaze fixations and a finite set of known points. We validate our approach on synthetic data to demonstrate that classifying using velocity features is more robust than a position matching approach. In addition, a classifier using velocity features improves semantic labeling on a real-world dataset of human-robot assistive manipulation interactions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19315
        },
        {
          "affiliations": [],
          "personId": 18923
        }
      ],
      "sessionIds": [
        2184
      ],
      "eventIds": []
    },
    {
      "id": 7866,
      "typeId": 11401,
      "title": "Automatic quick-phase detection in bedside recordings from patients with acute dizziness and nystagmus",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Benign Paroxysmal Positional Vertigo (BPPV) is the most common cause of vertigo. Vestibular experts can diagnose and treat this condition with simple maneuvers. However, there is a high rate of misdiagnosis that results in high medical costs when non-specialists use expensive, time-consuming neuroimaging techniques. Here we show how to improve saccade detection methods to detect quick-phases of nystagmus, a key sign of BPPV, in data recorded during the Dix-Hallpike maneuver.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17394
        },
        {
          "affiliations": [],
          "personId": 19945
        },
        {
          "affiliations": [],
          "personId": 20774
        }
      ],
      "sessionIds": [
        1807
      ],
      "eventIds": []
    },
    {
      "id": 8123,
      "typeId": 11372,
      "title": "Understanding the Relationship between Microsaccades and Pupil Dilation",
      "trackId": 10564,
      "tags": [],
      "keywords": [],
      "abstract": "Existing literature reveals little information about the relationship between the microsaccade rate and the change in pupil size. The current study is trying to investigate this relationship and how the microsaccade rate may be relevant to cognitive load. In our study, we compared the characteristics in microsaccade rate and the change in pupil size during eight different visual conditions. The data was analyzed by looking at the first part of the task as well as the entire task for all conditions. We discovered a significant correlation between microsaccade rate and pupil dilation during the early stage of visual tasks as well as comparable characteristics throughout the entire task. We hope that this finding will help us further understand the relative function of microsaccades and use it to support cognitive load response measurement.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23048
        },
        {
          "affiliations": [],
          "personId": 13859
        },
        {
          "affiliations": [],
          "personId": 22150
        },
        {
          "affiliations": [],
          "personId": 16507
        }
      ],
      "sessionIds": [
        1616
      ],
      "eventIds": []
    },
    {
      "id": 5820,
      "typeId": 11407,
      "title": "A Gaze-Based Experimenter Platform for Designing and Evaluating Adaptive Interventions in Information Visualizations",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "We present an experimenter platform for designing and evaluating user-adaptive support in information visualizations. Specifically, this platform leverages eye-tracking data in real time to deliver adaptive support in visualizations based on the user's intentional patterns over the visualization, and their individual traits and states. We describe the main functionalities of this platform, and show an application of this platform to support processing of textual documents with embedded bar charts, by dynamically providing highlighting in the charts to guide a user’s attention to the relevant information. We expect this platform to ease the evaluation of different forms of adaptation across visualizations and tasks, which is crucial to broaden our understanding of how user adaptation can improve the effectiveness of information visualizations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14958
        },
        {
          "affiliations": [],
          "personId": 21848
        },
        {
          "affiliations": [],
          "personId": 20719
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5053,
      "typeId": 11406,
      "title": "Random ferns for area of interest free scan path classification",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "We propose to use random ferns in combination with saccade angle successions to compare scanpaths. One advantage of our method is that it does not require areas of interest to be computed or annotated. The conditional distribution in random ferns additionally allows for learning angle successions, which do not have to be entirely present in a scan path. We evaluated our approach on two publicly available datasets and improved the classification accuracy by ≈10 and ≈20 percent.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16460
        },
        {
          "affiliations": [],
          "personId": 20400
        },
        {
          "affiliations": [],
          "personId": 18504
        },
        {
          "affiliations": [],
          "personId": 24039
        },
        {
          "affiliations": [],
          "personId": 18095
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7614,
      "typeId": 11410,
      "title": "Remote Corneal Imaging by Integrating a 3D Face Model and an Eyeball Model",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "In corneal imaging methods, it is essential to use a 3D eyeball model for generating an undistorted image. Thus, the relationship between the eye and eye camera is fixed by using a head-mounted device. Remote corneal imaging has several potential applications such as surveillance systems and driver monitoring. We integrated a 3D eyeball model with a 3D face model to facilitate remote corneal imaging. We conducted evaluation experiments and confirmed the feasibility of remote corneal imaging. We showed that the center of the eyeball can be estimated based on face tracking, and thus, corneal imaging can function as continuous remote eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20838
        },
        {
          "affiliations": [],
          "personId": 14689
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7102,
      "typeId": 11410,
      "title": "Calibration-free Text Entry using Smooth Pursuit Eye Movements",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a calibration-free gaze-based text entry system that uses smooth pursuit eye movements. We report on our implementation, which improves over prior work on smooth pursuit text entry by 1) eliminating the need of calibration using motion correlation, 2) increasing input rate from 3.34 to 3.41 words per minute, 3) featuring text suggestions that were trained on 10,000 lexicon sentences recommended in the literature. We report on a user study (N=26) which shows that users are able to eye type at 3.41 words per minutes without calibration and without user training. Qualitative feedback also indicates that users positively perceive the system. Our work is of particular benefit for disabled users and for situations when voice and tactile input are not feasible (e.g., in noisy environments or when the hands are occupied).",
      "authors": [
        {
          "affiliations": [],
          "personId": 13779
        },
        {
          "affiliations": [],
          "personId": 22295
        },
        {
          "affiliations": [],
          "personId": 16495
        },
        {
          "affiliations": [],
          "personId": 22005
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 6849,
      "typeId": 11407,
      "title": "W!NCE: Eyewear Solution for Upper Face Action Units Monitoring",
      "trackId": 10552,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to unobtrusively and continuously monitor one’s facial expressions has implications for a variety of application domains ranging from affective computing to health-care and the entertain- ment industry. The standard Facial Action Coding System (FACS) along with camera based methods have been shown to provide ob- jective indicators of facial expressions; however, these approaches can also be fairly limited for mobile applications due to privacy con- cerns and awkward positioning of the camera. To bridge this gap, W!NCE re-purposes a commercially available Electrooculography- based eyeglass (J!NS MEME) for continuously and unobtrusively sensing of upper facial action units with high fidelity. W!NCE de- tects facial gestures using a two-stage processing pipeline involving motion artifact removal and facial action detection. We validate our system’s applicability through extensive evaluation on data from 17 users under stationary and ambulatory settings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15473
        },
        {
          "affiliations": [],
          "personId": 23706
        },
        {
          "affiliations": [],
          "personId": 10188
        },
        {
          "affiliations": [],
          "personId": 11669
        },
        {
          "affiliations": [],
          "personId": 15235
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7361,
      "typeId": 11410,
      "title": "Accessible Control of Telepresence Robots based on Eye-Tracking",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze may be a good alternative input modality for people with limited hand mobility. This accessible control based on eye tracking can be implemented into telepresence robots, which are widely used to promote remote social interaction and providing the feeling of presence. This extended abstract introduces a PhD research project, which takes a two-phase approach towards investigating gaze-controlled telepresence robots. A system supporting gaze-controlled telepresence has been implemented. However, our current findings indicates that there were still serious challenges with regard to gaze-based driving. Potential improvement are discussed, and plans for future study are also presented.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22983
        },
        {
          "affiliations": [],
          "personId": 22533
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7105,
      "typeId": 11410,
      "title": "TobiiGlassesPySuite: An open-source suite for using the Tobii Pro Glasses 2 in eye-tracking studies",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present the TobiiGlassesPySuite, an open-source suite we implemented for using the Tobii Pro Glasses 2 wearable eye-tracker in custom eye-tracking studies. We provide a platform-independent solution for controlling the device and for managing the recordings. The software consists of Python modules, integrated into a single package, accompanied by sample scripts and recordings. The proposed solution aims at providing additional methods with respect to the manufacturer's software, for allowing the users to exploit more the device's capabilities and the existing software. Our suite is available for download from the repository indicated in the paper and usable according to the terms of the GNU GPL v3.0 license.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16313
        },
        {
          "affiliations": [],
          "personId": 20560
        },
        {
          "affiliations": [],
          "personId": 12798
        },
        {
          "affiliations": [],
          "personId": 8300
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5058,
      "typeId": 11406,
      "title": "Accessible Control of Telepresence Robots based on Eye-Tracking",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze may be a good alternative input modality for people with limited hand mobility. This accessible control based on eye tracking can be implemented into telepresence robots, which are widely used to promote remote social interaction and providing the feeling of presence. This extended abstract introduces a PhD research project, which takes a two-phase approach towards investigating gaze-controlled telepresence robots. A system supporting gaze-controlled telepresence has been implemented. However, our current findings indicates that there were still serious challenges with regard to gaze-based driving. Potential improvement are discussed, and plans for future study are also presented.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22983
        },
        {
          "affiliations": [],
          "personId": 22533
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 3267,
      "typeId": 11372,
      "title": "Interaction Graphs: Visual Analysis of Eye Movement Data from Interactive Stimuli",
      "trackId": 10558,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking studies have been conducted to understand the visual attention in different scenarios like, for example, how people read text, which graphical elements in a visualization are frequently attended, how they drive a car, or how they behave during a shopping task. All of these scenarios - either static or dynamic - show a visual stimulus in which the spectator is not able to change the visual content of what he sees. This is different if interaction is allowed like in (graphical) user interfaces (UIs), integrated development environments (IDEs), web pages, or interactive displays in general as in human-computer interaction, which gives a viewer the opportunity to actively change the stimulus content.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10978
        }
      ],
      "sessionIds": [
        2374
      ],
      "eventIds": []
    },
    {
      "id": 7876,
      "typeId": 11372,
      "title": "EyeFlow: Pursuit Interactions Using an Unmodified Camera",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the smooth pursuit eye movement-based interaction using an unmodified camera. We compute the indicative direction of the eye movement by analyzing low vectors obtained using the Lucas-Kanade optical low algorithm. To this end, we discuss how carefully selected low vectors could replace the traditional pupil centers detection in smooth pursuit interaction. This simple approach is easy to implement and abstains from many of the complexities of pupil based approaches. In particular, EyeFlow does not call for either a 3D pupil model or 2D pupil detection to track the pupil center location. We compare this method to state-of-the-art approaches and find that this can enable pursuit interactions with normal cameras. Results from the evaluation with 12 participants yield an accuracy that compares favorably to previous studies. In addition, the benefit of this work is that the approach does not necessitate highly matured computer vision algorithms and expensive IR-pass cameras.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8510
        },
        {
          "affiliations": [],
          "personId": 24132
        },
        {
          "affiliations": [],
          "personId": 16606
        }
      ],
      "sessionIds": [
        2184
      ],
      "eventIds": []
    },
    {
      "id": 5572,
      "typeId": 11410,
      "title": "Get a Grip: Slippage-Robust and Glint-Free Gaze Estimation for Real-Time Pervasive Head-Mounted Eye Tracking",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "A key assumption conventionally made by flexible head-mounted eye-tracking systems is often invalid: The eye center does not remain stationary w.r.t. the eye camera due to slippage. For instance, eye-tracker slippage might happen due to head acceleration or explicit adjustments by the user. As a result, gaze estimation accuracy can be significantly reduced. In this work, we propose Grip, a novel gaze estimation method capable of instantaneously compensating for eye-tracker slippage without additional hardware requirements such as glints or stereo eye camera setups. Grip was evaluated using previously collected data from a large scale unconstrained pervasive eye-tracking study. Our results indicate significant slippage compensation potential, decreasing average participant median angular offset by more than 43% w.r.t. a non-slippage-robust gaze estimation method. A reference implementation of Grip was integrated into EyeRecToo, an open-source hardware-agnostic eye-tracking software, thus making it readily accessible for multiple eye trackers (Available at: www.ti.uni-tuebingen.de/perception).",
      "authors": [
        {
          "affiliations": [],
          "personId": 17919
        },
        {
          "affiliations": [],
          "personId": 18642
        },
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 3524,
      "typeId": 11410,
      "title": "The vision and interpretation of paintings: bottom-up visual processes, top-down culturally informed attention, and aesthetic experience.",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "We aim to contribute to our knowledge about how we experience paintings, and how visual exploration, cognitive categorization and emotive evaluation contribute to its aesthetic dimension [1], [2], [3]. Visual exploration activates universal visio-mimetic capacities, however these capacities are implemented culturally and historically [4]. We combine psychological, cognitive, and historical studies to address the question of the historically variable “forms of intentionality” [5] of our ways of looking at paintings, investigting how the notion of a “period eye” [6], [7] can be accommodated with the constancy of human vision. For this purpose, we use eye-tracking data of 52 participants looking at the Isenheim altarpiece before and after restoration. First results allowed us to classify the zones of salience as well as the effects of participants’ backgrounds and emotions on fixation time and visual attention.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23565
        },
        {
          "affiliations": [],
          "personId": 19375
        },
        {
          "affiliations": [],
          "personId": 10061
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4808,
      "typeId": 11411,
      "title": "General Discussion and COGAIN Best Papers Awards",
      "trackId": 10566,
      "tags": [],
      "keywords": [],
      "abstract": "General Discussion and ETVIS Best Papers Awards",
      "authors": [
        {
          "affiliations": [],
          "personId": 11387
        },
        {
          "affiliations": [],
          "personId": 11308
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 5580,
      "typeId": 11406,
      "title": "Boosting Speed- and Accuracy of Gradient based Dark Pupil Tracking using Vectorization and Differential Evolution",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Gradient based dark pupil tracking [Timm and Barth 2011] is a simple and robust algorithm for pupil center estimation. The algorithm's time complexity of O(n^4) can be tackled by applying a two-stage process (coarse center estimation followed by a windowed refinement), as well as by optimizing and parallelizing code using cache-friendly data structures, vector-extensions of modern CPU's and GPU acceleration. We could achieve a substantial speed up compared to a non-optimized implementation: 12x using vector extensions and 65x using a GPU. Further, the two-stage process combined with parameter optimization using differential evolution considerably increased the accuracy of the algorithm. We evaluated our implementation using the „Labelled pupils the wild“ dataset. The percentage of frames with a pixel error below 15px increased from 28% to 72%, surpassing algorithmically more complex algorithms like ExCuse (64%) and catching up with recent algorithms like PuRe (87%).",
      "authors": [
        {
          "affiliations": [],
          "personId": 12258
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 4560,
      "typeId": 11372,
      "title": "Iris: A Tool for Designing Contextually Relevant Gaze Visualizations",
      "trackId": 10556,
      "tags": [],
      "keywords": [],
      "abstract": "Advances in eye tracking technology have enabled new interaction techniques and gaze-based applications. However, the techniques for visualizing gaze information have remained relatively unchanged. We developedIris, a tool to support the design of contextually relevant gaze visualizations. Iris allows users to explore displaying different features of gaze behavior including the current fixation point, duration, and saccades. Stylistic elements such as color, opacity, and smoothness can also be adjusted to give users creative and detailed control over the design of their gaze visualization. We present the Iris system and perform a user study to examine how participants can make use of the tool to devise contextually relevant gaze visualizations for a variety of collaborative tasks. We show that changes in color and opacity as well as variation in gaze trails can be adjusted to create meaningful gaze visualizations that fit the context of use.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9380
        },
        {
          "affiliations": [],
          "personId": 17629
        }
      ],
      "sessionIds": [
        1667
      ],
      "eventIds": []
    },
    {
      "id": 5585,
      "typeId": 11401,
      "title": "Eye-tracking based Fatigue and Cognitive Assessment",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Fatigue detection, monitoring and management is important and needs to be accommodated in the busy lifestyles that many people have these days. It may have an impact on the physical as well as the emotional health of the individuals. Detection of fatigue is the first step towards its management. With eye-tracking software using cameras, and being included in the laptops and smartphones, it now has the potential to become quite ubiquitous. This extended abstract describes my PhD project for fatigue detection using eye-tracking measures while gaze typing. The steps taken and experiments conducted upto now are presented, with an outline of the future plans. The principal use-case will be to provide the service of fatigue detection for people with neurological disorders, who use eye-tracking for alternative communications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23816
        }
      ],
      "sessionIds": [
        1410
      ],
      "eventIds": []
    },
    {
      "id": 7377,
      "typeId": 11406,
      "title": "Factors Influencing Dwell Time During Source Code Reading: A Large-Scale Replication Experiment",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The paper partially replicates and extends a previous study by Busjahn et al. on the factors influencing dwell time during source code reading, where source code element type and frequency of gaze visits are studied as factors. Unlike the previous study, this study focuses on analyzing eye movement data in large open source Java projects. Five experts and thirteen novices participated in the study where the main task is to summarize methods. The results examine semantic line-level information that developers view during summarization. We find no correlation between the line length and the total duration of time spent looking on the line even though it exists between a token's length and the total fixation time on the token reported in prior work. The first fixations inside a method are more likely to be on a method's signature, a variable declaration, or an assignment compared to the other fixations inside a method. In addition, it is found that smaller methods tend to have shorter overall fixation duration for the entire method, but have significantly longer duration per line in the method. The analysis provides insights into how source code's unique characteristics can help in building more robust methods for analyzing eye movements in source code and overall in building theories to support program comprehension on realistic tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16372
        },
        {
          "affiliations": [],
          "personId": 11740
        },
        {
          "affiliations": [],
          "personId": 23273
        },
        {
          "affiliations": [],
          "personId": 23193
        },
        {
          "affiliations": [],
          "personId": 14118
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6609,
      "typeId": 11372,
      "title": "Space-Time Volume Visualization of Gaze and Stimulus",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "We present a method for the spatio-temporal analysis of gaze data from multiple participants in the context of a video stimulus. For such data, an overview of the recorded patterns is important to identify common viewing behavior (such as attentional synchrony) and outliers. We adopt the approach of space-time cube visualization, which extends the spatial dimensions of the stimulus by time as the third dimension. Previous work mainly handled eye tracking data in the space-time cube as point cloud, providing no information about the stimulus context. This paper presents a novel visualization technique that combines gaze data, a dynamic stimulus, and optical flow with volume rendering to derive an overview of the data with contextual information. With specifically designed transfer functions, we emphasize different data aspects, making the visualization suitable for explorative analysis and for illustrative support of statistical findings alike.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18172
        },
        {
          "affiliations": [],
          "personId": 21102
        },
        {
          "affiliations": [],
          "personId": 20078
        },
        {
          "affiliations": [],
          "personId": 14452
        },
        {
          "affiliations": [],
          "personId": 12493
        }
      ],
      "sessionIds": [
        2060
      ],
      "eventIds": []
    },
    {
      "id": 6354,
      "typeId": 11406,
      "title": "GazeVR: A Toolkit for Developing Gaze Interactive Applications in VR/AR",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "For head mounted displays, like they are used in mixed reality applications, eye gaze seems to be a natural interaction modality. EyeMRTK provides building blocks for eye gaze interaction in virtual and augmented reality. Based on a hardware abstraction layer, it allows interaction researchers and developers to focus on their interaction concepts, while enabling them to evaluate their ideas on all supported systems. In addition to that, the toolkit provides a simulation layer for debugging purposes, which speeds up prototyping during development on the desktop.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19079
        },
        {
          "affiliations": [],
          "personId": 23535
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6356,
      "typeId": 11410,
      "title": "Factors Influencing Dwell Time During Source Code Reading: A Large-Scale Replication Experiment",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The paper partially replicates and extends a previous study by Busjahn et al. on the factors influencing dwell time during source code reading, where source code element type and frequency of gaze visits are studied as factors. Unlike the previous study, this study focuses on analyzing eye movement data in large open source Java projects. Five experts and thirteen novices participated in the study where the main task is to summarize methods. The results examine semantic line-level information that developers view during summarization. We find no correlation between the line length and the total duration of time spent looking on the line even though it exists between a token's length and the total fixation time on the token reported in prior work. The first fixations inside a method are more likely to be on a method's signature, a variable declaration, or an assignment compared to the other fixations inside a method. In addition, it is found that smaller methods tend to have shorter overall fixation duration for the entire method, but have significantly longer duration per line in the method. The analysis provides insights into how source code's unique characteristics can help in building more robust methods for analyzing eye movements in source code and overall in building theories to support program comprehension on realistic tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16372
        },
        {
          "affiliations": [],
          "personId": 11740
        },
        {
          "affiliations": [],
          "personId": 23273
        },
        {
          "affiliations": [],
          "personId": 23193
        },
        {
          "affiliations": [],
          "personId": 14118
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5844,
      "typeId": 11372,
      "title": "Eye Tracking Support for Visual Analytics Systems: Foundations, Current Applications, and Research Challenges",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Visual analytics (VA) research provides helpful solutions for interactive visual data analysis when exploring large and complex datasets. Due to recent advances in eye tracking technology, promising opportunities arise to extend these traditional VA approaches. Therefore, we discuss foundations for eye tracking support in VA systems. We first review and discuss the structure and range of typical VA systems. Based on a widely used VA model, we present five comprehensive examples that cover a wide range of usage scenarios. Then, we demonstrate that the VA model can be used to systematically explore how concrete VA systems could be extended with eye tracking, to create supportive and adaptive analytics systems. This allows us to identify general research and application opportunities, and classify them into research themes. In a call for action, we map the road for future research to broaden the use of eye tracking and advance visual analytics.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8904
        },
        {
          "affiliations": [],
          "personId": 15225
        },
        {
          "affiliations": [],
          "personId": 24314
        },
        {
          "affiliations": [],
          "personId": 14523
        },
        {
          "affiliations": [],
          "personId": 14452
        },
        {
          "affiliations": [],
          "personId": 21371
        },
        {
          "affiliations": [],
          "personId": 9575
        }
      ],
      "sessionIds": [
        2060
      ],
      "eventIds": []
    },
    {
      "id": 4821,
      "typeId": 11410,
      "title": "Analyzing Gaze Transition Behavior Using Bayesian Mixed Effects Markov Models",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The complex stochastic nature of eye tracking data calls for exploring sophisticated statistical models to ensure reliable inference in multi-trial eye-tracking experiments. We employ a Bayesian semi-parametric mixed-effects Markov model to compare gaze transition matrices between different experimental factors accommodating individual random effects. The model not only allows us to assess global influences of the external factors on the gaze transition dynamics but also provides comprehension of these effects at a deeper local level. We experimented to explore the impact of recognizing distorted images of artwork and landmarks on the gaze transition patterns. Our dataset comprises sequences representing areas of interest visited when applying a content independent grid to the resulting scan paths in a multi-trial setting. Results suggest that recognition to some extent affects the dynamics of the transitions while image category played an essential role in the viewing behavior.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14763
        },
        {
          "affiliations": [],
          "personId": 23414
        },
        {
          "affiliations": [],
          "personId": 8340
        },
        {
          "affiliations": [],
          "personId": 14590
        },
        {
          "affiliations": [],
          "personId": 19857
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4053,
      "typeId": 11404,
      "title": "From Gazing to Perceiving",
      "trackId": 10560,
      "tags": [],
      "keywords": [],
      "abstract": "Eye tracking technology is based on the assumption that our perception follows the fovea – a tiny region in our retina responsible for sharp central vision. In fact, what we usually refer to as the line of sight is nothing but the imaginary line connecting the fovea to the gazed location. However, our visual perception is far more complex than that: Gazing is not perceiving. As a tangible example, consider our retinal peripheral view. Whereas we cannot distinguish details in this region, movements are perceptible nonetheless. In this talk, I will go beyond the line of sight simplification by a) exploring requirements needed to shift our paradigm from foveal to retina-aware eye tracking, and b) discussing novel ways to employ this new paradigm to further our understanding of human perception.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8899
        }
      ],
      "sessionIds": [
        1974
      ],
      "eventIds": []
    },
    {
      "id": 4566,
      "typeId": 11401,
      "title": "High-Resolution Eye Tracking Using Scanning Laser Ophthalmoscopy",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Current eye-tracking techniques rely primarily on video-based tracking of components of the anterior surfaces of the eye. However, these trackers have several limitations. Their limited resolution precludes study of small fixational eye motion. Furthermore, many of these trackers rely on calibration procedures that do not offer a way to validate their eye motion traces. By comparison, retinal-image-based trackers can track the motion of the retinal image directly, at frequencies greater than 1kHz and with subarcminute accuracy. The retinal image provides a way to validate the eye position at any point in time, offering an unambiguous record of eye motion as a reference for the eye trace. The benefits of using scanning retinal imaging systems as eye trackers, however, comes at the price of different problems that are not present in video-based systems, and need to be solved to obtain robust eye traces. The current abstract provides an overview of retinal-image-based eye tracking methods, provides preliminary eye-tracking results from a tracking scanning-laser ophthalmoscope (TSLO), and proposes a new binocular line-scanning eye-tracking system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22816
        }
      ],
      "sessionIds": [
        1410
      ],
      "eventIds": []
    },
    {
      "id": 6871,
      "typeId": 11401,
      "title": "Motion Tracking of Iris Features for Eye tracking",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Current video-based eye trackers fail to acquire a high signal-to-noise (SNR) ratio which is crucial for specific applications like interactive systems, event detection, the study of various eye movements, and most importantly estimating the gaze position with high certainty. Specifically, current video-based eye trackers over-rely on precise localization of the pupil boundary and/or corneal reflection for gaze tracking, which often results in inaccuracies and large sample-to-sample root mean square (RMS-S2S). Therefore, it is crucial to address the shortcomings of these trackers, and we plan to study a new video-based eye tracking methodology focused on simultaneously tracking the motion of many iris features and investigate its implications for obtaining high accuracy and precision. In our preliminary work, the method has shown great potential for robust detection of microsaccades over 0.2 degrees with high confidence. We plan to explore and optimize this technique.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9750
        }
      ],
      "sessionIds": [
        2079
      ],
      "eventIds": []
    },
    {
      "id": 4823,
      "typeId": 11372,
      "title": "Image, Brand and Price Info: do they always matter the same?",
      "trackId": 10558,
      "tags": [],
      "keywords": [],
      "abstract": "We study attention processes to brand, price and visual information about products in online retailing websites, simultaneously considering the effects of consumers' goals, purchase category and consumers' statements. We use an intra-subject experimental design, simulated web stores and a combination of observational eye-tracking techniques and declarative measures. Image information about the product is the more important stimulus, regardless of the task at hand or the store involved. The roles of brand and price information are dependent on the product category and the purchase task involved. Declarative measures of relative brand importance are found to be positively related with its observed importance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24200
        },
        {
          "affiliations": [],
          "personId": 10159
        },
        {
          "affiliations": [],
          "personId": 22026
        }
      ],
      "sessionIds": [
        2374
      ],
      "eventIds": []
    },
    {
      "id": 5593,
      "typeId": 11374,
      "title": "Tobii Pro solutions for VR experiments",
      "trackId": 10553,
      "tags": [],
      "keywords": [],
      "abstract": "This workshop will present Tobii Pro’s solutions for conducting VR experiments, and will go through how areas of interests, trials, moving AOIs, fixation classification, and other concepts, are handled in the experiment workflow. It will provide an understanding of what parts are taken care of by the software, and what is expected of the researcher themselves. Workshop attendees will get a chance to try VR hardware and the software solutions themselves.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12590
        },
        {
          "affiliations": [],
          "personId": 18189
        }
      ],
      "sessionIds": [
        1798
      ],
      "eventIds": []
    },
    {
      "id": 3802,
      "typeId": 11372,
      "title": "Aiming for Quiet Eye in Biathlon",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "The duration of the so-called “Quiet Eye” (QE) the final fixation before the initiation of a critical movement seems to be linked to better perceptual-motor performances in various domains. The aim of this paper was to investigage QE in elite biathletes in an ecologically valid environment. We tested whether longer QE durations result in higher shooting accuracy. To this end, we developed a gun-mounted eye tracker as a means to obtain reliable gaze data without interfering with the athletes’ performance routines. During regular training protocols we collected gaze and performance data of 9 members of the German national junior team. The results did not show a significant effect of QE duration on shooting performance . Based on our findings, we critically discuss various conceptual as well as methodological issues with the QE literature that need to be aligned in future research to resolve current inconsistencies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23963
        },
        {
          "affiliations": [],
          "personId": 22041
        },
        {
          "affiliations": [],
          "personId": 20736
        }
      ],
      "sessionIds": [
        2239
      ],
      "eventIds": []
    },
    {
      "id": 6107,
      "typeId": 11372,
      "title": "Task-embedded online eye-tracker calibration for improving robustness to head motion",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Remote eye trackers are widely used for screen-based interactions. They are less intrusive than head mounted eye trackers, but are generally quite sensitive to head movement. This leads to the requirement for frequent recalibration, especially in applications requiring accurate eye tracking. We propose here an online calibration method to compensate for head movements if estimates of the true gaze targets are available. For example, in dwell-time based gaze typing it is reasonable to assume that for correct selections, the user's true gaze during the dwell-time was at the key center. We use this assumption to derive an eye-position dependent linear transformation matrix for correcting the measured gaze. Our experiments show that the proposed method significantly reduces errors over a large range of head movements.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11126
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        2239
      ],
      "eventIds": []
    },
    {
      "id": 8155,
      "typeId": 11406,
      "title": "Analyzing Gaze Transition Behavior Using Bayesian Mixed Effects Markov Models",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The complex stochastic nature of eye tracking data calls for exploring sophisticated statistical models to ensure reliable inference in multi-trial eye-tracking experiments. We employ a Bayesian semi-parametric mixed-effects Markov model to compare gaze transition matrices between different experimental factors accommodating individual random effects. The model not only allows us to assess global influences of the external factors on the gaze transition dynamics but also provides comprehension of these effects at a deeper local level. We experimented to explore the impact of recognizing distorted images of artwork and landmarks on the gaze transition patterns. Our dataset comprises sequences representing areas of interest visited when applying a content independent grid to the resulting scan paths in a multi-trial setting. Results suggest that recognition to some extent affects the dynamics of the transitions while image category played an essential role in the viewing behavior.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14763
        },
        {
          "affiliations": [],
          "personId": 23414
        },
        {
          "affiliations": [],
          "personId": 8340
        },
        {
          "affiliations": [],
          "personId": 14590
        },
        {
          "affiliations": [],
          "personId": 19857
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 3805,
      "typeId": 11406,
      "title": "TobiiGlassesPySuite: An open-source suite for using the Tobii Pro Glasses 2 in eye-tracking studies",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present the TobiiGlassesPySuite, an open-source suite we implemented for using the Tobii Pro Glasses 2 wearable eye-tracker in custom eye-tracking studies. We provide a platform-independent solution for controlling the device and for managing the recordings. The software consists of Python modules, integrated into a single package, accompanied by sample scripts and recordings. The proposed solution aims at providing additional methods with respect to the manufacturer's software, for allowing the users to exploit more the device's capabilities and the existing software. Our suite is available for download from the repository indicated in the paper and usable according to the terms of the GNU GPL v3.0 license.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16313
        },
        {
          "affiliations": [],
          "personId": 20560
        },
        {
          "affiliations": [],
          "personId": 12798
        },
        {
          "affiliations": [],
          "personId": 8300
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 3551,
      "typeId": 11410,
      "title": "Towards a Data-driven Framework for Realistic Self-Organized Virtual Humans: Coordinated Head and Eye movements",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17813
        },
        {
          "affiliations": [],
          "personId": 11179
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 4835,
      "typeId": 11372,
      "title": "Differential Privacy for Eye-Tracking Data",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "As large eye-tracking datasets are created, data privacy is a pressing concern for the eye-tracking community. De-identifying data does not guarantee privacy because multiple datasets can be linked for inferences. A common belief is that aggregating individuals' data into composite representations such as heatmaps protects the individual. However, we analytically examine the privacy of (noise-free) heatmaps and show that they do not guarantee privacy. We further propose two noise mechanisms that guarantee privacy and analyze their privacy-utility tradeoff. Analysis reveals that our Gaussian noise mechanism is an elegant solution to preserve privacy for heatmaps. Our results have implications for interdisciplinary research to create differentially private mechanisms for eye tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18237
        },
        {
          "affiliations": [],
          "personId": 22673
        },
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 11179
        },
        {
          "affiliations": [],
          "personId": 23856
        },
        {
          "affiliations": [],
          "personId": 9418
        }
      ],
      "sessionIds": [
        2220
      ],
      "eventIds": []
    },
    {
      "id": 6115,
      "typeId": 11402,
      "title": "Gaze Analytics Pipeline",
      "trackId": 10562,
      "tags": [],
      "keywords": [],
      "abstract": "This tutorial gives a short introduction to experimental design in general and with regard to eye tracking studies in particular. Additionally, the design of three different eye tracking studies (using stationary as well as mobile eye trackers) will be presented and the strengths and limitations of their designs will be discussed. Further, the tutorial presents details of a Python-based gaze analytics pipeline developed and used by Prof. Duchowski and Ms. Gehrer. The gaze analytics pipeline consists of Python scripts for extraction of raw eye movement data, analysis and event detection via velocity-based filtering, collation of events for statistical evaluation, analysis and visualization of results using R. Attendees of the tutorial will have the opportunity to run the scripts of an analysis of gaze data collected during categorization of different emotional expressions while viewing faces. The tutorial covers basic eye movement analytics, e.g., fixation count and dwell time within AOIs, as well as advanced analysis using gaze transition entropy. Newer analytical tools and techniques such as microsaccade detection and the Index of Pupillary Activity will be covered with time permitting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 8800
        }
      ],
      "sessionIds": [
        1062
      ],
      "eventIds": []
    },
    {
      "id": 4068,
      "typeId": 11372,
      "title": "PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric Scene Image and Eye Movement Features",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Eyewear devices, such as augmented reality displays, increasingly integrate eye tracking but the first-person camera required to map a user's gaze to the visual scene can pose a significant threat to user and bystander privacy. We present PrivacEye, a method to detect privacy-sensitive everyday situations and automatically enable and disable the eye tracker's first-person camera using a mechanical shutter. To close the shutter in privacy-sensitive situations, the method uses a deep representation of the first-person video combined with rich features that encode users' eye movements. To open the shutter without visual input, PrivacEye detects changes in users' eye movements alone to gauge changes in the \"privacy level\" of the current situation. We evaluate our method on a first-person video dataset recorded in daily life situations of 17 participants, annotated by themselves for privacy sensitivity, and show that our method is effective in preserving privacy in this challenging setting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20843
        },
        {
          "affiliations": [],
          "personId": 20147
        },
        {
          "affiliations": [],
          "personId": 17638
        },
        {
          "affiliations": [],
          "personId": 22533
        }
      ],
      "sessionIds": [
        2220
      ],
      "eventIds": []
    },
    {
      "id": 7397,
      "typeId": 11372,
      "title": "Towards a better description of visual exploration through temporal dynamic of ambient and focal modes",
      "trackId": 10564,
      "tags": [],
      "keywords": [],
      "abstract": "Human eye movements are far from being well described with current indicators. From the dataset provided by the ETRA 2019 challenge, we analyzed saccades and fixations during a free exploration of blank or natural scenes and during visual search. Based on the two modes of exploration, ambient and focal, we used the K coefficient (Kretz et al., 2016). We failed to find any differences between tasks but this indicator gives only the dominant mode over the entire recording. The stability of both modes, assesses with the switch frequency and the mode duration allowed to differentiate gaze behavior according to situations. Time course analyses of K coefficient and switch frequency corroborate that the latter is a useful indicator, describing a greater portion of the eye movement recording.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17890
        },
        {
          "affiliations": [],
          "personId": 16165
        },
        {
          "affiliations": [],
          "personId": 22245
        },
        {
          "affiliations": [],
          "personId": 20612
        },
        {
          "affiliations": [],
          "personId": 20659
        },
        {
          "affiliations": [],
          "personId": 14382
        }
      ],
      "sessionIds": [
        1616
      ],
      "eventIds": []
    },
    {
      "id": 6886,
      "typeId": 11410,
      "title": "When you don’t see what you expect: incongruence in music and source code reading",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Both musicians and programmers have expectations when they read music scores or source code due to prior knowledge. The goal of these studies is to get an insight into what will happen when these expectations are violated in familiar tasks. In music reading study we explored eye-movements of musically experienced participants singing and playing on a piano familiar melodies either containing or not containing a bar shifted down a tone in two different keys of C major and B major. First-pass fixation durations, mean pupil size during first-pass fixations and eye-time span parameters were analysed using linear mixed models. All three parameters can provide useful information on processing of incongruence in music. Furthermore, the pupil size parameter might be sensitive to modality of performance. In code reading study we plan to study incongruence in familiar code tasks and its reflection in eye movements of programmers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23192
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7143,
      "typeId": 11406,
      "title": "Towards a Data-driven Framework for Realistic Self-Organized Virtual Humans: Coordinated Head and Eye movements",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17813
        },
        {
          "affiliations": [],
          "personId": 11179
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6888,
      "typeId": 11371,
      "title": "Privacy Issues of Ubiguity Eye Tracking",
      "trackId": 10561,
      "tags": [],
      "keywords": [],
      "abstract": "Technological advances in computing and sensing devices from one side and the crucial role that eye tracking plays in near-eye displays (e.g., in VR/AR devices) and driver assistance systems from the other, are moving eye tracking into the mainstream. On the way towards a new human-machine interaction paradigm, fundamental questions have to be answered on how the users want to use this new technology in the future. Discussions about ethical implications and issues of data privacy will be important for the further positive development of eye-tracking technology and its acceptance by the society. Because eye-tracking will become a pervasive technology, possibly affecting millions of people, its misuse has to be avoided. This panel aims to discuss privacy questions in the eye tracking community to offer a forum for people from the eye tracking, human-computer interaction, other relevant communities, as well as the industry to gather, discuss, and address the privacy as well as confidentiality issues related to eye movement data before it becomes a part of everyday life.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18735
        },
        {
          "affiliations": [],
          "personId": 19815
        },
        {
          "affiliations": [],
          "personId": 20101
        },
        {
          "affiliations": [],
          "personId": 8899
        },
        {
          "affiliations": [],
          "personId": 13228
        },
        {
          "affiliations": [],
          "personId": 22673
        }
      ],
      "sessionIds": [
        1093
      ],
      "eventIds": []
    },
    {
      "id": 4841,
      "typeId": 11410,
      "title": "Automatic quick-phase detection in bedside recordings from patients with acute dizziness and nystagmus",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Benign Paroxysmal Positional Vertigo (BPPV) is the most common cause of vertigo. Vestibular experts can diagnose and treat this condition with simple maneuvers. However, there is a high rate of misdiagnosis that results in high medical costs when non-specialists use expensive, time-consuming neuroimaging techniques. Here we show how to improve saccade detection methods to detect quick-phases of nystagmus, a key sign of BPPV, in data recorded during the Dix-Hallpike maneuver.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17394
        },
        {
          "affiliations": [],
          "personId": 19945
        },
        {
          "affiliations": [],
          "personId": 20774
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 5610,
      "typeId": 11410,
      "title": "A Deep Learning Approach for Robust Head Pose Independent Eye movements recognition from Videos",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "The recognition of eye movements is an important task for gaze behavior understanding like in human communication analysis (human-human, computer or robot interactions) or diagnosis (medical, reading impairments). In this paper, we address this task using remote R-GBD sensors, which allows handling people in natural conditions. This is very challenging given that such sensors have a normal sampling rate of 25/30Hz and provide low-resolution eye images. Hence the resulting gaze signals one can extract in these conditions have lower precision compared to dedicated IR eye trackers, rendering previous method less appropriate for the task. To tackle these challenges, we propose a deep learning method that directly processes the eye image video streams to classify them into fixation, saccade and blink classes, distinguishing irrelevant noise (illumination, inaccurate eye alignment, ...) from true eye motion signals. Experiments on natural 4-party interactions demonstrate the benefit of our approach compared to previous methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24177
        },
        {
          "affiliations": [],
          "personId": 15255
        },
        {
          "affiliations": [],
          "personId": 16865
        },
        {
          "affiliations": [],
          "personId": 11126
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 6635,
      "typeId": 11372,
      "title": "Inducing Gaze Gestures by Static Illustrations",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "In gesture-based user interfaces, the effort needed for learning the gestures is a persistent problem that hinders their adoption in products. However, people’s natural gaze paths form shapes during viewing. For example, reading creates a recognizable pattern. These gaze patterns can be utilized in human-technology interaction. We experimented with the idea of inducing specific gaze patterns by static drawings. The drawings included visual hints to guide the gaze. By looking at the parts of the drawing, the user’s gaze composed a gaze gesture that activated a command. We organized a proof-of-concept trial to see how intuitive the idea is. Most participants understood the idea without specific instructions already on the first round of trials. We argue that with careful design the form of objects and especially their decorative details can serve as a gaze-based user interface in smart homes and other environments of ubiquitous computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11308
        },
        {
          "affiliations": [],
          "personId": 19475
        },
        {
          "affiliations": [],
          "personId": 23659
        },
        {
          "affiliations": [],
          "personId": 10097
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 3820,
      "typeId": 11410,
      "title": "Inferring target locations from gaze data: A smartphone study",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Although smartphones are widely used in everyday life, studies of viewing behavior mainly employ desktop computers. This study examines whether closely spaced target locations on a smartphone can be decoded from gaze.  Subjects wore a head-mounted eye tracker and fixated a target that successively appeared at 30 positions spaced by 1.1 x 0.9 cm. A “hand-held” (phone in subject’s hand) and a “mounted” (phone on surface) condition were conducted. Linear-mixed-models were fitted to examine whether gaze differed between targets. T-tests on root-mean-squared errors were calculated to evaluate the deviation between gaze and targets. To decode target positions from gaze data we trained a classifier and assessed its performance for every subject/condition.  While gaze positions differed between targets (main effect “target”), gaze deviated from the real positions. The classifier’s performance for the 30 locations ranged considerably between subjects (“mounted”: 30 to 93% accuracy; “hand-held”: 8 to 100 % accuracy).",
      "authors": [
        {
          "affiliations": [],
          "personId": 13517
        },
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 16310
        },
        {
          "affiliations": [],
          "personId": 11257
        },
        {
          "affiliations": [],
          "personId": 8800
        },
        {
          "affiliations": [],
          "personId": 15617
        },
        {
          "affiliations": [],
          "personId": 16805
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 3052,
      "typeId": 11374,
      "title": "iMotions workshop",
      "trackId": 10553,
      "tags": [],
      "keywords": [],
      "abstract": "to be announced at the conference",
      "authors": [],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4592,
      "typeId": 11406,
      "title": "Looks Can Mean Achieving: Understanding Eye Gaze Patterns of Proficiency in Code Comprehension",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "The research proposes four hypotheses that focus on deriving helpful insights from eye patterns, including hidden truths concerning programmer expertise, task context and difficulty. We present results from a study performed in a classroom setting with 17 students, in which we found that novice programmers visit output statements and declarations the same amount as the rest of the program they are presented other than control flow block headers.  This research builds upon insightful findings from our previous work, wherein we focus on gathering statistical eye-gaze effects between categories of various populations to drive the pursuit of new research. Ongoing and future work entails using the iTrace infrastructure to capture gaze as participants scroll to read code pages extending longer than what can fit on one screen. The focus will be on building various models that relate eye gaze to comprehension via methods that realistically capture activity in a development environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11902
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 5873,
      "typeId": 11406,
      "title": "A Gaze Model Improves Autonomous Driving",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "End-to-end behavioral cloning trained by human demonstration is now a popular approach for vision-based autonomous driving. A deep neural network maps drive-view images directly to steering commands. However, the images contain much task-irrelevant data. Humans attend to behaviorally relevant information using saccades that direct gaze towards important areas. We demonstrate that behavioral cloning also benefits from active control of gaze. We trained a generative deep neural network model that accurately predicts human gaze maps while driving in both familiar and unseen environments. We incorporated the predicted gaze maps into end-to-end networks for two behaviors: following and overtaking. Incorporating gaze information significantly improves generalization to unseen environments. We hypothesize that incorporating gaze information enables the network to focus on task critical objects, which vary little between environments, and ignore irrelevant elements in the background, which vary greatly.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17561
        },
        {
          "affiliations": [],
          "personId": 22647
        },
        {
          "affiliations": [],
          "personId": 21894
        },
        {
          "affiliations": [],
          "personId": 9130
        },
        {
          "affiliations": [],
          "personId": 14305
        },
        {
          "affiliations": [],
          "personId": 15665
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7157,
      "typeId": 11374,
      "title": "Establishing a Ground-Truth for Eye Tracking",
      "trackId": 10553,
      "tags": [],
      "keywords": [],
      "abstract": "Calibration and performance evaluation of current eye trackers typically rely on comparing known target positions to measured gaze directions while a participant is fixating on those targets. A mapping function or geometric eye model is then optimized based on this correspondence, essentially treating the calibration targets as the \"ground truth\" for each gaze direction. While this has worked reasonably well to achieve current calibration accuracies of around 0.5 degrees, trying to optimize beyond this point reveals that calibration targets are more a self-report measure than real ground truth: Participant compliance, fixational eye movements such as drifts and micro-saccades, as well as the accuracy of positioning the fovea or preferred viewing location itself all contribute to uncertainty in the “ground-truth” target location and thus form a lower bound for tracking accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17821
        },
        {
          "affiliations": [],
          "personId": 17751
        },
        {
          "affiliations": [],
          "personId": 23122
        },
        {
          "affiliations": [],
          "personId": 13025
        },
        {
          "affiliations": [],
          "personId": 20883
        }
      ],
      "sessionIds": [
        1431
      ],
      "eventIds": []
    },
    {
      "id": 4341,
      "typeId": 11372,
      "title": "Gaze Behaviour on Interacted Objects during Hand Interaction in Virtual Reality for Eye Tracking Re-calibration",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we investigate the probability and timing of attaining gaze fixations on interacted objects virtual reality during hand interaction, with the main purpose as a means for implicit and continuous eye tracking re-calibration. We conducted an evaluation with 15 participants in which their gaze was recorded while interacting with virtual objects. The data was analysed to find factors influencing the probability of fixations at different phases of interaction with different types of objects. The results indicate that 1) object size affect fixation dispersion on the object 2) interacting with stationary objects may be favourable to moving objects, 3) prolonged and precision-demanding interactions positively influences the probability to attain fixations, 4) performing multiple interactions simultaneously can negatively impact the probability of fixations, and 5) feedback can initiate and end fixations on objects.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17758
        },
        {
          "affiliations": [],
          "personId": 8367
        }
      ],
      "sessionIds": [
        2239
      ],
      "eventIds": []
    },
    {
      "id": 3320,
      "typeId": 11401,
      "title": "Towards a Data-driven Framework for Realistic Self-Organized Virtual Humans: Coordinated Head and Eye movements",
      "trackId": 10550,
      "tags": [],
      "keywords": [],
      "abstract": "Driven by significant investments from the gaming, film, advertising, and customer service industries among others, efforts across many different fields are converging to create realistic representations of humans that look like (computer graphics), sound like (natural language generation), move like (motion capture), and reason like (artificial intelligence) real humans. The ultimate goal of this work is to push the boundaries even further by exploring the development of realistic self-organized virtual humans that are capable of demonstrating coordinated behaviors across different modalities. Eye movements, for example, may be accompanied by changes in facial expression, head orientation, posture, gait properties, or speech. Traditionally however, these modalities are captured and modeled separately and this disconnect contributes to the well-known uncanny valley phenomenon. We focus initially on facial modalities, in particular, coordinated eye and head movements (and eventually facial expressions), but our proposed data-driven framework will be able to accommodate other modalities as well.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17813
        },
        {
          "affiliations": [],
          "personId": 11179
        }
      ],
      "sessionIds": [
        1207
      ],
      "eventIds": []
    },
    {
      "id": 5880,
      "typeId": 11410,
      "title": "EyeVEIL: Degrading Iris Authentication in Eye-Tracking Headsets",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "Mixed reality headsets are being designed with integrated eye trackers: cameras that image the user's eye to infer gaze location and pupil diameter. While the intent is to improve the quality of experience, built-in eye trackers create a security vulnerability for hackers -- high resolution images of the user's iris. Anyone stealing an iris image has effectively captured a gold standard biometric, relied on for secure authentication in applications such as banking and voting. We present a low cost solution to degrade iris authentication while still permitting the utility of gaze tracking with acceptable accuracy. By demonstrating this solution on a commodity eye tracker, this paper urges the community to think about iris based authentication as a byproduct of eye tracking, and create solutions that empower a user to control this biometric.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16335
        },
        {
          "affiliations": [],
          "personId": 19562
        },
        {
          "affiliations": [],
          "personId": 9418
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    },
    {
      "id": 7418,
      "typeId": 11372,
      "title": "Just Gaze and Wave: Exploring the Use of Gaze and Gestures for Shoulder-surfing Resilient Authentication",
      "trackId": 10557,
      "tags": [],
      "keywords": [],
      "abstract": "Eye-gaze and mid-air gestures are promising for resisting various types of side-channel attacks during authentication. However, to date, a comparison of the different authentication modalities is missing. We investigate multiple authentication mechanisms that leverage gestures, eye gaze, and a multimodal combination of them and study their resilience to shoulder surfing. To this end, we report on our implementation of three schemes and results from usability and security evaluations where we also experimented with fixed and randomized layouts. We found that our gaze-based approach outperforms the other schemes in terms of input time, error rate, perceived workload, and resistance to observation attacks, and that randomizing the layout does not improve observation resistance enough to warrant the reduced usability. Our work further underlines the significance of replicating previous studies with today’s sensors as we show significant improvement over similar gaze-based authentication systems that were introduced in prior work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13779
        },
        {
          "affiliations": [],
          "personId": 16495
        },
        {
          "affiliations": [],
          "personId": 17650
        },
        {
          "affiliations": [],
          "personId": 22384
        },
        {
          "affiliations": [],
          "personId": 17743
        }
      ],
      "sessionIds": [
        2220
      ],
      "eventIds": []
    },
    {
      "id": 4605,
      "typeId": 11406,
      "title": "Inferring target locations from gaze data: A smartphone study",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Although smartphones are widely used in everyday life, studies of viewing behavior mainly employ desktop computers. This study examines whether closely spaced target locations on a smartphone can be decoded from gaze.  Subjects wore a head-mounted eye tracker and fixated a target that successively appeared at 30 positions spaced by 1.1 x 0.9 cm. A “hand-held” (phone in subject’s hand) and a “mounted” (phone on surface) condition were conducted. Linear-mixed-models were fitted to examine whether gaze differed between targets. T-tests on root-mean-squared errors were calculated to evaluate the deviation between gaze and targets. To decode target positions from gaze data we trained a classifier and assessed its performance for every subject/condition.  While gaze positions differed between targets (main effect “target”), gaze deviated from the real positions. The classifier’s performance for the 30 locations ranged considerably between subjects (“mounted”: 30 to 93% accuracy; “hand-held”: 8 to 100 % accuracy).",
      "authors": [
        {
          "affiliations": [],
          "personId": 13517
        },
        {
          "affiliations": [],
          "personId": 20549
        },
        {
          "affiliations": [],
          "personId": 16310
        },
        {
          "affiliations": [],
          "personId": 11257
        },
        {
          "affiliations": [],
          "personId": 8800
        },
        {
          "affiliations": [],
          "personId": 15617
        },
        {
          "affiliations": [],
          "personId": 16805
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 6654,
      "typeId": 11372,
      "title": "GazeButton: Enhancing Buttons with Eye Gaze Interactions",
      "trackId": 10559,
      "tags": [],
      "keywords": [],
      "abstract": "The button is an element of a user interface to trigger an action. We introduce GazeButton, a novel concept extending the default button mode with advanced gaze-based interactions. During normal interaction, users can utilise this button as a universal hub for gaze-based UI shortcuts. The advantages are:1) easy to integrate in existing UIs, 2) complementary, as users choose either gaze or manual interaction, 3) straightforward, as all features are located in one button, 4) one button to interact with the whole screen. We explore GazeButtons for a text editing tool on a multitouch tablet device. For example, to set text cursor position, users look at the position and tap once on the GazeButton, avoiding costly physical movement. We present concrete design of such application examples based on a systematic design space, pointing to potential future buttons that become highly expressive by unifying the user's visual and manual inputs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20900
        },
        {
          "affiliations": [],
          "personId": 13779
        },
        {
          "affiliations": [],
          "personId": 22869
        },
        {
          "affiliations": [],
          "personId": 19224
        },
        {
          "affiliations": [],
          "personId": 14505
        }
      ],
      "sessionIds": [
        1634
      ],
      "eventIds": []
    },
    {
      "id": 3583,
      "typeId": 11406,
      "title": "SeTA: Semiautomatic Tool for Annotation of Eye Tracking Images",
      "trackId": 10554,
      "tags": [],
      "keywords": [],
      "abstract": "Availability of large scale tagged datasets is a must in the field of deep learning applied to the eye tracking challenge. In this paper, the potential of Supervised-Descent-Method (SDM) as a semiautomatic labelling tool for eye tracking images is shown. The objective of the paper is to evidence how the human effort needed for manually labelling large eye tracking datasets can be radically reduced by the use of cascaded regressors. Different applications are provided in the fields of high and low resolution systems. An iris/pupil center labelling is shown as example for low resolution images while a pupil contour points detection is demonstrated in high resolution. In both cases manual annotation requirements are drastically reduced.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23348
        },
        {
          "affiliations": [],
          "personId": 10669
        },
        {
          "affiliations": [],
          "personId": 9078
        },
        {
          "affiliations": [],
          "personId": 22026
        }
      ],
      "sessionIds": [
        2450
      ],
      "eventIds": []
    },
    {
      "id": 7679,
      "typeId": 11410,
      "title": "Attentional orienting in real and virtual 360-degree environments: application to aeronautics",
      "trackId": 10551,
      "tags": [],
      "keywords": [],
      "abstract": "We investigate the mechanisms of attentional orienting in a 360-degree virtual environments. Through the use of Posner’s paradigm,we study the effects of different attentional guidance techniquesdesigned to improve information processing. The most efficienttechnique will be applied to a procedure learning tool in virtualreality and a remote air traffic control tower. The eye-tracker allowsus to explore the differential effects of overt and covert orienting,to estimate the effectiveness of visual research and to use it as atechnique for interaction in virtual reality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24170
        },
        {
          "affiliations": [],
          "personId": 16606
        },
        {
          "affiliations": [],
          "personId": 24132
        }
      ],
      "sessionIds": [
        1527
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 9728,
      "firstName": "Inken",
      "lastName": "Hagestedt",
      "affiliations": []
    },
    {
      "id": 22533,
      "firstName": "Susanne",
      "lastName": "Boll",
      "affiliations": []
    },
    {
      "id": 23048,
      "firstName": "Sudeep",
      "lastName": "Raj",
      "affiliations": []
    },
    {
      "id": 22026,
      "firstName": "Arantxa",
      "lastName": "Villanueva",
      "affiliations": []
    },
    {
      "id": 23565,
      "firstName": "Pablo",
      "lastName": "Fontoura",
      "affiliations": []
    },
    {
      "id": 15889,
      "firstName": "Masato",
      "lastName": "Sasaki",
      "affiliations": []
    },
    {
      "id": 9746,
      "firstName": "Pawel",
      "lastName": "Kasprowski",
      "affiliations": []
    },
    {
      "id": 14867,
      "firstName": "Cornelia",
      "lastName": "Murko",
      "affiliations": []
    },
    {
      "id": 19475,
      "firstName": "Jari",
      "lastName": "Laitinen",
      "affiliations": []
    },
    {
      "id": 9750,
      "firstName": "Aayush",
      "lastName": "Chaudhary",
      "affiliations": []
    },
    {
      "id": 11286,
      "firstName": "Addison",
      "lastName": "Mayberry",
      "affiliations": []
    },
    {
      "id": 22041,
      "firstName": "Amelie",
      "lastName": "Heinrich",
      "affiliations": []
    },
    {
      "id": 18969,
      "firstName": "Bernhard",
      "lastName": "Anzengruber",
      "affiliations": []
    },
    {
      "id": 8221,
      "firstName": "Michael",
      "lastName": "Grossberg",
      "affiliations": []
    },
    {
      "id": 13859,
      "firstName": "Chia-Chien",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 14373,
      "firstName": "Ann",
      "lastName": "McNamara",
      "affiliations": []
    },
    {
      "id": 9766,
      "firstName": "Andrew",
      "lastName": "Gilman",
      "affiliations": []
    },
    {
      "id": 10793,
      "firstName": "Saradhi",
      "lastName": "Vijaya",
      "affiliations": []
    },
    {
      "id": 13865,
      "firstName": "Fabian",
      "lastName": "Deitelhoff",
      "affiliations": []
    },
    {
      "id": 18473,
      "firstName": "David",
      "lastName": "Geisler",
      "affiliations": []
    },
    {
      "id": 20010,
      "firstName": "Haofei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 13355,
      "firstName": "Christopher",
      "lastName": "Clarke",
      "affiliations": []
    },
    {
      "id": 11308,
      "firstName": "Päivi",
      "lastName": "Majaranta",
      "affiliations": []
    },
    {
      "id": 14382,
      "firstName": "Karine",
      "lastName": "Doré-Mazars",
      "affiliations": []
    },
    {
      "id": 23599,
      "firstName": "Pernilla",
      "lastName": "Qvarfordt",
      "affiliations": []
    },
    {
      "id": 12339,
      "firstName": "Jeremy",
      "lastName": "Gummeson",
      "affiliations": []
    },
    {
      "id": 17461,
      "firstName": "Katarzyna",
      "lastName": "Harezlak",
      "affiliations": []
    },
    {
      "id": 9283,
      "firstName": "Neil",
      "lastName": "Timmermans",
      "affiliations": []
    },
    {
      "id": 13891,
      "firstName": "Hana",
      "lastName": "Vrakova",
      "affiliations": []
    },
    {
      "id": 24132,
      "firstName": "Vsevolod",
      "lastName": "Peysakhovich",
      "affiliations": []
    },
    {
      "id": 20549,
      "firstName": "Andrew",
      "lastName": "Duchowski",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 18504,
      "firstName": "Thomas",
      "lastName": "Kübler",
      "affiliations": []
    },
    {
      "id": 16460,
      "firstName": "Wolfgang",
      "lastName": "Fuhl",
      "affiliations": []
    },
    {
      "id": 20560,
      "firstName": "Agnieszka",
      "lastName": "Wykowska",
      "affiliations": []
    },
    {
      "id": 11857,
      "firstName": "Kari-Jouko",
      "lastName": "Räihä",
      "affiliations": []
    },
    {
      "id": 23122,
      "firstName": "Kavitha",
      "lastName": "Ratnam",
      "affiliations": []
    },
    {
      "id": 19539,
      "firstName": "Sophie",
      "lastName": "Stelmach",
      "affiliations": []
    },
    {
      "id": 21077,
      "firstName": "Tushar",
      "lastName": "Dobhal",
      "affiliations": []
    },
    {
      "id": 8280,
      "firstName": "Andrea",
      "lastName": "Kienle",
      "affiliations": []
    },
    {
      "id": 18521,
      "firstName": "Ronak",
      "lastName": "Etemadpour",
      "affiliations": []
    },
    {
      "id": 8283,
      "firstName": "Pom",
      "lastName": "Charras",
      "affiliations": []
    },
    {
      "id": 9309,
      "firstName": "Justyna",
      "lastName": "Żurawska",
      "affiliations": []
    },
    {
      "id": 13917,
      "firstName": "Thomas",
      "lastName": "Guntz",
      "affiliations": []
    },
    {
      "id": 22110,
      "firstName": "Alois",
      "lastName": "Ferscha",
      "affiliations": []
    },
    {
      "id": 12895,
      "firstName": "Mikhail",
      "lastName": "Startsev",
      "affiliations": []
    },
    {
      "id": 8800,
      "firstName": "Nina",
      "lastName": "Gehrer",
      "affiliations": []
    },
    {
      "id": 23138,
      "firstName": "Peter",
      "lastName": "Solleder",
      "affiliations": []
    },
    {
      "id": 9315,
      "firstName": "Unaizah",
      "lastName": "Obaidellah",
      "affiliations": []
    },
    {
      "id": 13925,
      "firstName": "Agnieszka",
      "lastName": "Ozimek",
      "affiliations": []
    },
    {
      "id": 20071,
      "firstName": "Carla",
      "lastName": "Allen",
      "affiliations": []
    },
    {
      "id": 10344,
      "firstName": "Nishan",
      "lastName": "Gunawardena",
      "affiliations": []
    },
    {
      "id": 24170,
      "firstName": "Rébaï",
      "lastName": "Soret",
      "affiliations": []
    },
    {
      "id": 19562,
      "firstName": "Sanjeev",
      "lastName": "Koppal",
      "affiliations": []
    },
    {
      "id": 14443,
      "firstName": "Francisco",
      "lastName": "Luro",
      "middleInitial": "Lopez",
      "affiliations": []
    },
    {
      "id": 23659,
      "firstName": "Jari",
      "lastName": "Kangas",
      "affiliations": []
    },
    {
      "id": 13931,
      "firstName": "Michael",
      "lastName": "Matscheko",
      "affiliations": []
    },
    {
      "id": 8300,
      "firstName": "Michael",
      "lastName": "Dorr",
      "affiliations": []
    },
    {
      "id": 14958,
      "firstName": "Sébastien",
      "lastName": "Lallé",
      "affiliations": []
    },
    {
      "id": 21102,
      "firstName": "Kuno",
      "lastName": "Kurzhals",
      "affiliations": []
    },
    {
      "id": 20078,
      "firstName": "Steffen",
      "lastName": "Frey",
      "affiliations": []
    },
    {
      "id": 16495,
      "firstName": "Mohamed",
      "lastName": "Khamis",
      "affiliations": []
    },
    {
      "id": 15473,
      "firstName": "Soha",
      "lastName": "Rostaminia",
      "affiliations": []
    },
    {
      "id": 22641,
      "firstName": "Peter",
      "lastName": "Kiefer",
      "affiliations": []
    },
    {
      "id": 24177,
      "firstName": "Remy",
      "lastName": "Siegfried",
      "affiliations": []
    },
    {
      "id": 22642,
      "firstName": "I.",
      "lastName": "MacKenzie",
      "middleInitial": "Scott",
      "affiliations": []
    },
    {
      "id": 20595,
      "firstName": "Susana",
      "lastName": "Martinez-Conde",
      "affiliations": []
    },
    {
      "id": 14452,
      "firstName": "Daniel",
      "lastName": "Weiskopf",
      "affiliations": []
    },
    {
      "id": 13941,
      "firstName": "Pamela",
      "lastName": "Cosman",
      "affiliations": []
    },
    {
      "id": 22647,
      "firstName": "Yuying",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9338,
      "firstName": "Katsumi",
      "lastName": "Minakata",
      "affiliations": []
    },
    {
      "id": 11387,
      "firstName": "John",
      "lastName": "Hansen",
      "middleInitial": "Paulin",
      "affiliations": []
    },
    {
      "id": 16507,
      "firstName": "Nada",
      "lastName": "Attar",
      "affiliations": []
    },
    {
      "id": 11902,
      "firstName": "Jonathan",
      "lastName": "Saddler",
      "affiliations": []
    },
    {
      "id": 15488,
      "firstName": "Dmytro",
      "lastName": "Katrychuk",
      "affiliations": []
    },
    {
      "id": 20612,
      "firstName": "Coralie",
      "lastName": "Petermann",
      "affiliations": []
    },
    {
      "id": 20101,
      "firstName": "Apu",
      "lastName": "Kapadia",
      "affiliations": []
    },
    {
      "id": 22150,
      "firstName": "Shreya",
      "lastName": "Raj",
      "affiliations": []
    },
    {
      "id": 19079,
      "firstName": "Diako",
      "lastName": "Mardanbegi",
      "affiliations": []
    },
    {
      "id": 24200,
      "firstName": "Monica",
      "lastName": "Cortinas",
      "affiliations": []
    },
    {
      "id": 18569,
      "firstName": "Vijay",
      "lastName": "Rajanna",
      "affiliations": []
    },
    {
      "id": 16009,
      "firstName": "Andreas",
      "lastName": "Shamiyeh",
      "affiliations": []
    },
    {
      "id": 21133,
      "firstName": "Moritz",
      "lastName": "Kassner",
      "affiliations": []
    },
    {
      "id": 22673,
      "firstName": "Lirong",
      "lastName": "Xia",
      "affiliations": []
    },
    {
      "id": 17555,
      "firstName": "Dominique",
      "lastName": "Vaufreydaz",
      "affiliations": []
    },
    {
      "id": 8340,
      "firstName": "Jacek",
      "lastName": "Gwizdka",
      "affiliations": []
    },
    {
      "id": 19606,
      "firstName": "Efe",
      "lastName": "Bozkir",
      "affiliations": []
    },
    {
      "id": 23192,
      "firstName": "Natalia",
      "lastName": "Chitalkina",
      "affiliations": []
    },
    {
      "id": 17561,
      "firstName": "Congcong",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 23193,
      "firstName": "Jonathan",
      "lastName": "Maletic",
      "affiliations": []
    },
    {
      "id": 23706,
      "firstName": "Alexander",
      "lastName": "Lamson",
      "affiliations": []
    },
    {
      "id": 12442,
      "firstName": "Andreas",
      "lastName": "Harrer",
      "affiliations": []
    },
    {
      "id": 22683,
      "firstName": "Chi-Ren",
      "lastName": "Shyu",
      "affiliations": []
    },
    {
      "id": 23708,
      "firstName": "Tu",
      "lastName": "Nguyen",
      "middleInitial": "N.M.",
      "affiliations": []
    },
    {
      "id": 22176,
      "firstName": "Per",
      "lastName": "Bukgaard",
      "affiliations": []
    },
    {
      "id": 9380,
      "firstName": "Jeff",
      "lastName": "Brewer",
      "affiliations": []
    },
    {
      "id": 14505,
      "firstName": "Florian",
      "lastName": "Alt",
      "affiliations": []
    },
    {
      "id": 23724,
      "firstName": "Andrea",
      "lastName": "Strandberg",
      "affiliations": []
    },
    {
      "id": 19118,
      "firstName": "Philipp",
      "lastName": "Müller",
      "affiliations": []
    },
    {
      "id": 18095,
      "firstName": "Wolfgang",
      "lastName": "Rosenstiel",
      "affiliations": []
    },
    {
      "id": 8367,
      "firstName": "Anders",
      "lastName": "Lundström",
      "affiliations": []
    },
    {
      "id": 11954,
      "firstName": "Christopher",
      "lastName": "Harris",
      "middleInitial": "G.",
      "affiliations": []
    },
    {
      "id": 20147,
      "firstName": "Marion",
      "lastName": "Koelle",
      "affiliations": []
    },
    {
      "id": 20659,
      "firstName": "Bernard",
      "lastName": "Gosselin",
      "affiliations": []
    },
    {
      "id": 23223,
      "firstName": "Benjamin",
      "lastName": "Marlin",
      "affiliations": []
    },
    {
      "id": 14523,
      "firstName": "Nils",
      "lastName": "Rodrigues",
      "affiliations": []
    },
    {
      "id": 9408,
      "firstName": "Wouter",
      "lastName": "Nuijten",
      "affiliations": []
    },
    {
      "id": 10945,
      "firstName": "Sam",
      "lastName": "Hutton",
      "affiliations": []
    },
    {
      "id": 8899,
      "firstName": "Enkelejda",
      "lastName": "Kasneci",
      "affiliations": []
    },
    {
      "id": 18631,
      "firstName": "Michael",
      "lastName": "Huang",
      "middleInitial": "Xuelin",
      "affiliations": []
    },
    {
      "id": 16584,
      "firstName": "Martin",
      "lastName": "Schobesberger",
      "affiliations": []
    },
    {
      "id": 8904,
      "firstName": "Nelson",
      "lastName": "Silva",
      "affiliations": []
    },
    {
      "id": 9418,
      "firstName": "Eakta",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 13517,
      "firstName": "Stefanie",
      "lastName": "Mueller",
      "affiliations": []
    },
    {
      "id": 12493,
      "firstName": "Thomas",
      "lastName": "Ertl",
      "affiliations": []
    },
    {
      "id": 19150,
      "firstName": "Stefan",
      "lastName": "Göb",
      "affiliations": []
    },
    {
      "id": 18642,
      "firstName": "Diederick",
      "lastName": "Niehorster",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 9946,
      "firstName": "Heiko",
      "lastName": "Drewes",
      "affiliations": []
    },
    {
      "id": 14044,
      "firstName": "Frank",
      "lastName": "Borsato",
      "affiliations": []
    },
    {
      "id": 17629,
      "firstName": "Darren",
      "lastName": "Gergle",
      "affiliations": []
    },
    {
      "id": 16606,
      "firstName": "Christophe",
      "lastName": "Hurter",
      "affiliations": []
    },
    {
      "id": 13025,
      "firstName": "Michele",
      "lastName": "Rucci",
      "affiliations": []
    },
    {
      "id": 10978,
      "firstName": "Michael",
      "lastName": "Burch",
      "affiliations": []
    },
    {
      "id": 22245,
      "firstName": "Matei",
      "lastName": "Mancas",
      "affiliations": []
    },
    {
      "id": 17638,
      "firstName": "Wilko",
      "lastName": "Heuten",
      "affiliations": []
    },
    {
      "id": 22761,
      "firstName": "Klaus",
      "lastName": "Mueller",
      "affiliations": []
    },
    {
      "id": 23273,
      "firstName": "Corey",
      "lastName": "Bryant",
      "affiliations": []
    },
    {
      "id": 10473,
      "firstName": "Thomas",
      "lastName": "Wilcockson",
      "affiliations": []
    },
    {
      "id": 15596,
      "firstName": "Yu",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20719,
      "firstName": "Dereck",
      "lastName": "Toker",
      "affiliations": []
    },
    {
      "id": 17650,
      "firstName": "Rana",
      "lastName": "Eisa",
      "middleInitial": "Mohamed",
      "affiliations": []
    },
    {
      "id": 19187,
      "firstName": "Saeed",
      "lastName": "Yahyanejad",
      "affiliations": []
    },
    {
      "id": 12023,
      "firstName": "Raphael",
      "lastName": "Menges",
      "affiliations": []
    },
    {
      "id": 18170,
      "firstName": "Korok",
      "lastName": "Sengupta",
      "affiliations": []
    },
    {
      "id": 24314,
      "firstName": "Radu",
      "lastName": "Jianu",
      "affiliations": []
    },
    {
      "id": 13563,
      "firstName": "Henry",
      "lastName": "Griffith",
      "affiliations": []
    },
    {
      "id": 23292,
      "firstName": "Dimitris",
      "lastName": "Samaras",
      "affiliations": []
    },
    {
      "id": 9468,
      "firstName": "Kenan",
      "lastName": "Bektaş",
      "affiliations": []
    },
    {
      "id": 18172,
      "firstName": "Valentin",
      "lastName": "Bruder",
      "affiliations": []
    },
    {
      "id": 14590,
      "firstName": "Abhra",
      "lastName": "Sarkar",
      "affiliations": []
    },
    {
      "id": 10496,
      "firstName": "Seoyoung",
      "lastName": "Ahn",
      "affiliations": []
    },
    {
      "id": 20736,
      "firstName": "Rouwen",
      "lastName": "Cañal-Bruland",
      "affiliations": []
    },
    {
      "id": 15617,
      "firstName": "Michael",
      "lastName": "Schönenberg",
      "affiliations": []
    },
    {
      "id": 21249,
      "firstName": "Preethi",
      "lastName": "Vaidyanathan",
      "affiliations": []
    },
    {
      "id": 23816,
      "firstName": "Tanya",
      "lastName": "Bafna",
      "affiliations": []
    },
    {
      "id": 17160,
      "firstName": "Paulina",
      "lastName": "Lewandowska",
      "affiliations": []
    },
    {
      "id": 18189,
      "firstName": "Tim",
      "lastName": "Holmes",
      "affiliations": []
    },
    {
      "id": 15119,
      "firstName": "Oleg",
      "lastName": "Špakov",
      "affiliations": []
    },
    {
      "id": 19727,
      "firstName": "Harri",
      "lastName": "Siirtola",
      "affiliations": []
    },
    {
      "id": 17169,
      "firstName": "Howell",
      "lastName": "Istance",
      "affiliations": []
    },
    {
      "id": 14099,
      "firstName": "Leslie",
      "lastName": "Blaha",
      "affiliations": []
    },
    {
      "id": 19220,
      "firstName": "Michael",
      "lastName": "Raschke",
      "affiliations": []
    },
    {
      "id": 22295,
      "firstName": "Mariam",
      "lastName": "Mostafa",
      "affiliations": []
    },
    {
      "id": 19224,
      "firstName": "Ken",
      "lastName": "Pfeuffer",
      "affiliations": []
    },
    {
      "id": 10525,
      "firstName": "Kai",
      "lastName": "Dierkes",
      "affiliations": []
    },
    {
      "id": 22816,
      "firstName": "Norick",
      "lastName": "Bowers",
      "affiliations": []
    },
    {
      "id": 10016,
      "firstName": "Justin",
      "lastName": "Louedec",
      "middleInitial": "Le",
      "affiliations": []
    },
    {
      "id": 17699,
      "firstName": "Sara",
      "lastName": "Fabrikant",
      "middleInitial": "Irina",
      "affiliations": []
    },
    {
      "id": 16165,
      "firstName": "Thomas",
      "lastName": "Le Bras",
      "affiliations": []
    },
    {
      "id": 14118,
      "firstName": "Bonita",
      "lastName": "Sharif",
      "affiliations": []
    },
    {
      "id": 20774,
      "firstName": "Jorge",
      "lastName": "Otero-Millan",
      "affiliations": []
    },
    {
      "id": 16172,
      "firstName": "Jonas",
      "lastName": "Goltz",
      "affiliations": []
    },
    {
      "id": 15150,
      "firstName": "Titus",
      "lastName": "Kervezee",
      "affiliations": []
    },
    {
      "id": 12590,
      "firstName": "Jonas",
      "lastName": "HÖGSTRÖM",
      "affiliations": []
    },
    {
      "id": 18735,
      "firstName": "Ulrica",
      "lastName": "Wikström",
      "affiliations": []
    },
    {
      "id": 23856,
      "firstName": "Kenneth",
      "lastName": "Holmqvist",
      "affiliations": []
    },
    {
      "id": 15665,
      "firstName": "Bertram",
      "lastName": "Shi",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 23348,
      "firstName": "Andoni",
      "lastName": "Larumbe-Bergera",
      "affiliations": []
    },
    {
      "id": 17718,
      "firstName": "Benedikt",
      "lastName": "Hosp",
      "affiliations": []
    },
    {
      "id": 23351,
      "firstName": "Samit",
      "lastName": "Bhattacharya",
      "affiliations": []
    },
    {
      "id": 15160,
      "firstName": "Oleg",
      "lastName": "Komogortsev",
      "affiliations": []
    },
    {
      "id": 16697,
      "firstName": "Pranav",
      "lastName": "Venuprasad",
      "affiliations": []
    },
    {
      "id": 10555,
      "firstName": "Gregory",
      "lastName": "Zelinsky",
      "affiliations": []
    },
    {
      "id": 18237,
      "firstName": "Ao",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 8510,
      "firstName": "Almoctar",
      "lastName": "Hassoumi",
      "affiliations": []
    },
    {
      "id": 23359,
      "firstName": "Arzu",
      "lastName": "Çöltekin",
      "affiliations": []
    },
    {
      "id": 19275,
      "firstName": "Hans",
      "lastName": "Gellersen",
      "affiliations": []
    },
    {
      "id": 10061,
      "firstName": "Michel",
      "lastName": "Menu",
      "affiliations": []
    },
    {
      "id": 17743,
      "firstName": "Amr",
      "lastName": "Elmougy",
      "affiliations": []
    },
    {
      "id": 22869,
      "firstName": "Thomas",
      "lastName": "Mayer",
      "affiliations": []
    },
    {
      "id": 19285,
      "firstName": "Bettina",
      "lastName": "Klugsberger",
      "affiliations": []
    },
    {
      "id": 17751,
      "firstName": "Robin",
      "lastName": "Sharma",
      "affiliations": []
    },
    {
      "id": 21848,
      "firstName": "Cristina",
      "lastName": "Conati",
      "affiliations": []
    },
    {
      "id": 17758,
      "firstName": "Ludwig",
      "lastName": "Sidenmark",
      "affiliations": []
    },
    {
      "id": 14689,
      "firstName": "Kentaro",
      "lastName": "Takemura",
      "affiliations": []
    },
    {
      "id": 21349,
      "firstName": "Ayush",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 20838,
      "firstName": "Takamasa",
      "lastName": "Utsu",
      "affiliations": []
    },
    {
      "id": 19815,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "affiliations": []
    },
    {
      "id": 9575,
      "firstName": "Tobias",
      "lastName": "Schreck",
      "affiliations": []
    },
    {
      "id": 20843,
      "firstName": "Julian",
      "lastName": "Steil",
      "affiliations": []
    },
    {
      "id": 23403,
      "firstName": "Takashi",
      "lastName": "Nagamatsu",
      "affiliations": []
    },
    {
      "id": 22384,
      "firstName": "Sherif",
      "lastName": "Ismail",
      "affiliations": []
    },
    {
      "id": 10097,
      "firstName": "Poika",
      "lastName": "Isokoski",
      "affiliations": []
    },
    {
      "id": 19315,
      "firstName": "Reuben",
      "lastName": "Aronson",
      "affiliations": []
    },
    {
      "id": 16757,
      "firstName": "Zijun",
      "lastName": "Wei",
      "affiliations": []
    },
    {
      "id": 9078,
      "firstName": "Rafael",
      "lastName": "Cabeza",
      "affiliations": []
    },
    {
      "id": 23414,
      "firstName": "Nilavra",
      "lastName": "Bhattacharya",
      "affiliations": []
    },
    {
      "id": 11126,
      "firstName": "Jimin",
      "lastName": "Pi",
      "affiliations": []
    },
    {
      "id": 15225,
      "firstName": "Tanja",
      "lastName": "Blascheck",
      "affiliations": []
    },
    {
      "id": 23929,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "affiliations": []
    },
    {
      "id": 21371,
      "firstName": "Martin",
      "lastName": "Raubal",
      "affiliations": []
    },
    {
      "id": 10114,
      "firstName": "Rens",
      "lastName": "Oostenbach",
      "affiliations": []
    },
    {
      "id": 21379,
      "firstName": "Aruna",
      "lastName": "Balasubramanian",
      "affiliations": []
    },
    {
      "id": 15235,
      "firstName": "Deepak",
      "lastName": "Ganesan",
      "affiliations": []
    },
    {
      "id": 18308,
      "firstName": "Chandan",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 21894,
      "firstName": "Lei",
      "lastName": "Tai",
      "affiliations": []
    },
    {
      "id": 10121,
      "firstName": "Leanne",
      "lastName": "Chukoskie",
      "affiliations": []
    },
    {
      "id": 18314,
      "firstName": "Fabian",
      "lastName": "Göbel",
      "affiliations": []
    },
    {
      "id": 16783,
      "firstName": "Carlos",
      "lastName": "Morimoto",
      "affiliations": []
    },
    {
      "id": 19857,
      "firstName": "Sarah",
      "lastName": "D'Angelo",
      "affiliations": []
    },
    {
      "id": 11665,
      "firstName": "Sandeep",
      "lastName": "Vidyapu",
      "affiliations": []
    },
    {
      "id": 20883,
      "firstName": "Austin",
      "lastName": "Roorda",
      "affiliations": []
    },
    {
      "id": 17813,
      "firstName": "Zhizhuo",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 11669,
      "firstName": "Tauhidur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 15255,
      "firstName": "Yu",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 11163,
      "firstName": "Lucas",
      "lastName": "Peeters",
      "affiliations": []
    },
    {
      "id": 23963,
      "firstName": "Dan",
      "lastName": "Hansen",
      "middleInitial": "Witzner",
      "affiliations": []
    },
    {
      "id": 17821,
      "firstName": "Immo",
      "lastName": "Schuetz",
      "affiliations": []
    },
    {
      "id": 20900,
      "firstName": "Sheikh",
      "lastName": "Rivu",
      "middleInitial": "R.R.",
      "affiliations": []
    },
    {
      "id": 16805,
      "firstName": "Krzysztof",
      "lastName": "Krejtz",
      "affiliations": []
    },
    {
      "id": 10666,
      "firstName": "Conor",
      "lastName": "Kelton",
      "affiliations": []
    },
    {
      "id": 9130,
      "firstName": "Haoyang",
      "lastName": "Ye",
      "affiliations": []
    },
    {
      "id": 11179,
      "firstName": "Reynold",
      "lastName": "Bailey",
      "affiliations": []
    },
    {
      "id": 14763,
      "firstName": "Islam",
      "lastName": "Ebeid",
      "middleInitial": "Akef",
      "affiliations": []
    },
    {
      "id": 13228,
      "firstName": "Nino",
      "lastName": "Zahirovic",
      "affiliations": []
    },
    {
      "id": 9133,
      "firstName": "Raimond",
      "lastName": "Zemblys",
      "affiliations": []
    },
    {
      "id": 10669,
      "firstName": "Sonia",
      "lastName": "Porta",
      "affiliations": []
    },
    {
      "id": 19375,
      "firstName": "Jean-Marie",
      "lastName": "Schaeffer",
      "affiliations": []
    },
    {
      "id": 10159,
      "firstName": "Raquel",
      "lastName": "Chocarro",
      "affiliations": []
    },
    {
      "id": 20400,
      "firstName": "Nora",
      "lastName": "Castner",
      "affiliations": []
    },
    {
      "id": 13237,
      "firstName": "Amir",
      "lastName": "Dini",
      "affiliations": []
    },
    {
      "id": 16310,
      "firstName": "Sophie",
      "lastName": "Jörg",
      "affiliations": []
    },
    {
      "id": 9144,
      "firstName": "Anjul",
      "lastName": "Tyagi",
      "affiliations": []
    },
    {
      "id": 16313,
      "firstName": "Davide",
      "lastName": "Tommaso",
      "middleInitial": "De",
      "affiliations": []
    },
    {
      "id": 13753,
      "firstName": "Pete",
      "lastName": "Sawyer",
      "affiliations": []
    },
    {
      "id": 22983,
      "firstName": "Guangtao",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 10188,
      "firstName": "Subhransu",
      "lastName": "Maji",
      "affiliations": []
    },
    {
      "id": 16335,
      "firstName": "Brendan",
      "lastName": "John",
      "affiliations": []
    },
    {
      "id": 13779,
      "firstName": "Yasmeen",
      "lastName": "Abdrabou",
      "affiliations": []
    },
    {
      "id": 10707,
      "firstName": "Steffen",
      "lastName": "Staab",
      "affiliations": []
    },
    {
      "id": 16851,
      "firstName": "Frederick",
      "lastName": "Shic",
      "affiliations": []
    },
    {
      "id": 17881,
      "firstName": "Tiia",
      "lastName": "Viitanen",
      "affiliations": []
    },
    {
      "id": 11740,
      "firstName": "Nahla",
      "lastName": "Abid",
      "affiliations": []
    },
    {
      "id": 13791,
      "firstName": "Jens",
      "lastName": "Kruger",
      "affiliations": []
    },
    {
      "id": 14305,
      "firstName": "Ming",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 16865,
      "firstName": "Jean-Marc",
      "lastName": "Odobez",
      "affiliations": []
    },
    {
      "id": 17890,
      "firstName": "Alexandre",
      "lastName": "Milisavljevic",
      "affiliations": []
    },
    {
      "id": 12258,
      "firstName": "André",
      "lastName": "Krause",
      "middleInitial": "Frank",
      "affiliations": []
    },
    {
      "id": 23011,
      "firstName": "Gijs",
      "lastName": "Smit",
      "affiliations": []
    },
    {
      "id": 19939,
      "firstName": "James",
      "lastName": "Crowley",
      "affiliations": []
    },
    {
      "id": 24039,
      "firstName": "Alexander",
      "lastName": "Lotz",
      "affiliations": []
    },
    {
      "id": 11239,
      "firstName": "Izabela",
      "lastName": "Krejtz",
      "affiliations": []
    },
    {
      "id": 19945,
      "firstName": "Jing",
      "lastName": "Tian",
      "affiliations": []
    },
    {
      "id": 18923,
      "firstName": "Henny",
      "lastName": "Admoni",
      "affiliations": []
    },
    {
      "id": 23535,
      "firstName": "Thies",
      "lastName": "Pfeiffer",
      "affiliations": []
    },
    {
      "id": 13809,
      "firstName": "Trevor",
      "lastName": "Crawford",
      "affiliations": []
    },
    {
      "id": 17394,
      "firstName": "Sai",
      "lastName": "Punuganti",
      "middleInitial": "Akanksha",
      "affiliations": []
    },
    {
      "id": 16372,
      "firstName": "Cole",
      "lastName": "Peterson",
      "affiliations": []
    },
    {
      "id": 22005,
      "firstName": "Amr",
      "lastName": "Elmougy",
      "affiliations": []
    },
    {
      "id": 17397,
      "firstName": "Samir",
      "lastName": "Das",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 11257,
      "firstName": "Jaret",
      "lastName": "Screws",
      "affiliations": []
    },
    {
      "id": 20473,
      "firstName": "Veronica",
      "lastName": "Sundstedt",
      "affiliations": []
    },
    {
      "id": 15867,
      "firstName": "Lukas",
      "lastName": "Paletta",
      "affiliations": []
    },
    {
      "id": 15868,
      "firstName": "Anurag",
      "lastName": "Paul",
      "affiliations": []
    },
    {
      "id": 12798,
      "firstName": "Ioannis",
      "lastName": "Agtzidis",
      "affiliations": []
    },
    {
      "id": 17919,
      "firstName": "Thiago",
      "lastName": "Santini",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 42,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 13:49:05+00"
  }
}