{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10081,
    "shortName": "IMX",
    "year": 2022,
    "startDate": 1655856000000,
    "endDate": 1656028800000,
    "name": "IMX 2022",
    "fullName": "ACM International Conference on Interactive Media Experiences",
    "url": "https://imx.acm.org/2022/",
    "location": "Aveiro, Portugal",
    "timeZoneOffset": 60,
    "timeZoneName": "Europe/Lisbon",
    "logoUrl": "https://files.sigchi.org/conference/logo/10081/5752054c-0c19-6220-598b-2587000f0c8d.png",
    "accessibilityFaqUrl": "https://files.sigchi.org/conference/accessibility/10081/$52f5d54f-e0e5-b570-b67a-f4f05e8d85df.html"
  },
  "sponsors": [
    {
      "id": 10245,
      "name": "Altice Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10081/logo/f4305ea7-dedd-c5b1-6c6e-a95768c2c147.png",
      "levelId": 10158,
      "order": 1
    },
    {
      "id": 10246,
      "name": "Nokia",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10081/logo/9f62bbab-87b8-0abc-b9e2-a4ba45a83d99.png",
      "levelId": 10158,
      "order": 2
    },
    {
      "id": 10247,
      "name": "Sky",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10081/logo/79111114-bd8e-2e99-76ab-64ac56706482.png",
      "levelId": 10158,
      "order": 3
    }
  ],
  "sponsorLevels": [
    {
      "id": 10157,
      "name": "Sponsors",
      "rank": 2,
      "isDefault": true
    },
    {
      "id": 10158,
      "name": "Silver Sponsor",
      "rank": 1,
      "isDefault": false
    }
  ],
  "floors": [
    {
      "id": 10113,
      "name": "Local venue",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10081/2f8b29f9-6522-bd62-db21-a300e381589a.png",
      "roomIds": [
        10698,
        10694,
        10696,
        10695,
        10697,
        10693,
        10703,
        10705,
        10706,
        10704,
        10699
      ]
    },
    {
      "id": 10114,
      "name": "Virtual venue",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10081/4d950d82-ff2f-fd57-2670-23f6992dfda0.png",
      "roomIds": [
        10700
      ]
    },
    {
      "id": 10123,
      "name": "Social Event",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/10081/cdea8f9f-089d-bc79-729a-229ccbc2e95c.png",
      "roomIds": [
        10707
      ]
    }
  ],
  "rooms": [
    {
      "id": 10693,
      "name": "DeCA - 40.2.5",
      "typeId": 12261,
      "setup": "Classroom"
    },
    {
      "id": 10694,
      "name": "DeCA - 40.1.14",
      "typeId": 12267,
      "setup": "Classroom"
    },
    {
      "id": 10695,
      "name": "DeCA - 40.1.8",
      "typeId": 12267,
      "setup": "Classroom"
    },
    {
      "id": 10696,
      "name": "DeCA - 40.1.15",
      "typeId": 12267,
      "setup": "Classroom"
    },
    {
      "id": 10697,
      "name": "DeCA - 40.2.16",
      "typeId": 12267,
      "setup": "Classroom"
    },
    {
      "id": 10698,
      "name": "Auditorium “Renato Araújo”",
      "typeId": 12265,
      "setup": "Theatre"
    },
    {
      "id": 10699,
      "name": "“Sala do Senado”",
      "typeId": 12289,
      "setup": "Special"
    },
    {
      "id": 10700,
      "name": "Virtual",
      "typeId": 12267,
      "setup": "Special"
    },
    {
      "id": 10703,
      "name": "DeCA - Hall",
      "typeId": 12262,
      "setup": "Special"
    },
    {
      "id": 10704,
      "name": "“Sala de Traduções”",
      "typeId": 12260,
      "setup": "Special"
    },
    {
      "id": 10705,
      "name": "Rectory Lobby",
      "typeId": 12262,
      "setup": "Theatre"
    },
    {
      "id": 10706,
      "name": "University Restaurant",
      "typeId": 12262,
      "setup": "Special"
    },
    {
      "id": 10707,
      "name": "Viva a Ria",
      "typeId": 12262,
      "setup": "Theatre"
    }
  ],
  "tracks": [
    {
      "id": 11840,
      "name": "IMX 2022 Technical Papers",
      "typeId": 12264
    },
    {
      "id": 11841,
      "name": "IMX 2022 Work-in-Progress",
      "typeId": 12266
    },
    {
      "id": 11842,
      "name": "IMX 2022 Demos",
      "typeId": 12260
    },
    {
      "id": 11843,
      "name": "IMX 2022 IMX-in-Industry",
      "typeId": 12269
    },
    {
      "id": 11844,
      "name": "IMX 2022 Doctoral Consortium",
      "typeId": 12261
    },
    {
      "id": 11848,
      "typeId": 12270
    },
    {
      "id": 11849,
      "typeId": 12271
    },
    {
      "id": 11850,
      "typeId": 12267
    }
  ],
  "contentTypes": [
    {
      "id": 12259,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12260,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 5,
      "displayName": "Demos"
    },
    {
      "id": 12261,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 12262,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12263,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 12264,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 12265,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 12266,
      "name": "Work-in-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 12267,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12268,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 12269,
      "name": "Industry",
      "color": "#969696",
      "duration": 5
    },
    {
      "id": 12270,
      "name": "Keynote",
      "color": "#ff99ca",
      "duration": 0
    },
    {
      "id": 12271,
      "name": "Panel",
      "color": "#32d923",
      "duration": 0,
      "displayName": "Panels"
    },
    {
      "id": 12289,
      "name": "Work-In-Progress and Demos",
      "color": "#26e5f1",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 12432,
      "type": "SESSION",
      "startDate": 1655888400000,
      "endDate": 1655893800000
    },
    {
      "id": 12433,
      "type": "BREAK",
      "startDate": 1655893800000,
      "endDate": 1655895000000
    },
    {
      "id": 12434,
      "type": "SESSION",
      "startDate": 1655895000000,
      "endDate": 1655902200000
    },
    {
      "id": 12435,
      "type": "LUNCH",
      "startDate": 1655902200000,
      "endDate": 1655906400000
    },
    {
      "id": 12436,
      "type": "SESSION",
      "startDate": 1655906400000,
      "endDate": 1655913600000
    },
    {
      "id": 12437,
      "type": "BREAK",
      "startDate": 1655913600000,
      "endDate": 1655914800000
    },
    {
      "id": 12438,
      "type": "SESSION",
      "startDate": 1655914800000,
      "endDate": 1655920800000
    },
    {
      "id": 12439,
      "type": "SESSION",
      "startDate": 1655884800000,
      "endDate": 1655888400000
    },
    {
      "id": 12440,
      "type": "SESSION",
      "startDate": 1655920800000,
      "endDate": 1655928900000
    },
    {
      "id": 12441,
      "type": "SESSION",
      "startDate": 1655971200000,
      "endDate": 1655974800000
    },
    {
      "id": 12442,
      "type": "SESSION",
      "startDate": 1655974800000,
      "endDate": 1655976600000
    },
    {
      "id": 12443,
      "type": "SESSION",
      "startDate": 1655976600000,
      "endDate": 1655980200000
    },
    {
      "id": 12444,
      "type": "SESSION",
      "startDate": 1655980200000,
      "endDate": 1655981400000
    },
    {
      "id": 12445,
      "type": "SESSION",
      "startDate": 1655981400000,
      "endDate": 1655984400000
    },
    {
      "id": 12446,
      "type": "SESSION",
      "startDate": 1655984400000,
      "endDate": 1655985000000
    },
    {
      "id": 12447,
      "type": "SESSION",
      "startDate": 1655985000000,
      "endDate": 1655988300000
    },
    {
      "id": 12448,
      "type": "LUNCH",
      "startDate": 1655988300000,
      "endDate": 1655993700000
    },
    {
      "id": 12449,
      "type": "SESSION",
      "startDate": 1655993700000,
      "endDate": 1655997300000
    },
    {
      "id": 12450,
      "type": "SESSION",
      "startDate": 1655997300000,
      "endDate": 1655999400000
    },
    {
      "id": 12451,
      "type": "BREAK",
      "startDate": 1655999400000,
      "endDate": 1656000600000
    },
    {
      "id": 12452,
      "type": "SESSION",
      "startDate": 1656000600000,
      "endDate": 1656002100000
    },
    {
      "id": 12453,
      "type": "SESSION",
      "startDate": 1656002100000,
      "endDate": 1656005700000
    },
    {
      "id": 12454,
      "type": "SESSION",
      "startDate": 1656010800000,
      "endDate": 1656025200000
    },
    {
      "id": 12455,
      "type": "SESSION",
      "startDate": 1656061200000,
      "endDate": 1656063600000
    },
    {
      "id": 12456,
      "type": "SESSION",
      "startDate": 1656063600000,
      "endDate": 1656065700000
    },
    {
      "id": 12457,
      "type": "SESSION",
      "startDate": 1656065700000,
      "endDate": 1656069300000
    },
    {
      "id": 12458,
      "type": "BREAK",
      "startDate": 1656069300000,
      "endDate": 1656070500000
    },
    {
      "id": 12459,
      "type": "SESSION",
      "startDate": 1656070500000,
      "endDate": 1656072000000
    },
    {
      "id": 12460,
      "type": "SESSION",
      "startDate": 1656072000000,
      "endDate": 1656075300000
    },
    {
      "id": 12461,
      "type": "SESSION",
      "startDate": 1656075300000,
      "endDate": 1656076200000
    },
    {
      "id": 12462,
      "type": "LUNCH",
      "startDate": 1656076200000,
      "endDate": 1656081600000
    },
    {
      "id": 12463,
      "type": "SESSION",
      "startDate": 1656081600000,
      "endDate": 1656085200000
    },
    {
      "id": 12464,
      "type": "SESSION",
      "startDate": 1656085200000,
      "endDate": 1656088500000
    },
    {
      "id": 12465,
      "type": "BREAK",
      "startDate": 1656088500000,
      "endDate": 1656089700000
    },
    {
      "id": 12466,
      "type": "SESSION",
      "startDate": 1656089700000,
      "endDate": 1656092400000
    },
    {
      "id": 12467,
      "type": "SESSION",
      "startDate": 1656092400000,
      "endDate": 1656095100000
    }
  ],
  "sessions": [
    {
      "id": 83812,
      "name": "Session 1: Storytelling, Media & Engagement",
      "isParallelPresentation": false,
      "importedId": "P1CVvKjgHllS3BKS9V6uig",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83726
      ],
      "contentIds": [
        83806,
        83807,
        83796
      ],
      "source": "PCS",
      "timeSlotId": 12445,
      "note": ""
    },
    {
      "id": 83813,
      "name": "Session 7: Telepresence & Bodily Interactions",
      "isParallelPresentation": false,
      "importedId": "fD8Y4sY3f1Qw6ObDlduu4g",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83776
      ],
      "contentIds": [
        83810,
        83793,
        83797
      ],
      "source": "PCS",
      "timeSlotId": 12464,
      "note": ""
    },
    {
      "id": 83814,
      "name": "Session 6: User-Centered XR",
      "isParallelPresentation": false,
      "importedId": "jFKXNWJ04tNrWkaLtxsb2Q",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83767
      ],
      "contentIds": [
        83811,
        83808,
        83809
      ],
      "source": "PCS",
      "timeSlotId": 12460,
      "note": ""
    },
    {
      "id": 83815,
      "name": "Session 3: XR Artistic & Creative Experiences",
      "isParallelPresentation": false,
      "importedId": "mF5v7Drpx4O0LRiCmeVSDg",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83779
      ],
      "contentIds": [
        83803,
        83801,
        83805
      ],
      "source": "PCS",
      "timeSlotId": 12449,
      "note": ""
    },
    {
      "id": 83816,
      "name": "Session 4 - Technologies, Systems & Interfaces",
      "isParallelPresentation": false,
      "importedId": "ZfgZ3hRm9WQVZOm_FJmt1A",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83730
      ],
      "contentIds": [
        83804,
        83799
      ],
      "source": "PCS",
      "timeSlotId": 12455,
      "note": ""
    },
    {
      "id": 83817,
      "name": "Session 2: Immersive-Multisensory Experiences",
      "isParallelPresentation": false,
      "importedId": "me9zy3zUZnHT4SLyOleFYg",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83739
      ],
      "contentIds": [
        83798,
        83800,
        83794
      ],
      "source": "PCS",
      "timeSlotId": 12447,
      "note": ""
    },
    {
      "id": 83818,
      "name": "Session 5 - Platforms & Complexities of sharing",
      "isParallelPresentation": false,
      "importedId": "-ggkErdaTamb3N7wgLme_w",
      "typeId": 12264,
      "roomId": 10698,
      "chairIds": [
        83868
      ],
      "contentIds": [
        83795,
        83802
      ],
      "source": "PCS",
      "timeSlotId": 12456,
      "note": ""
    },
    {
      "id": 83819,
      "name": "Welcome Session",
      "isParallelPresentation": false,
      "importedId": "0c74388a-775e-4e3c-a050-14fb7b6bac33",
      "typeId": 12262,
      "roomId": 10698,
      "chairIds": [
        83821,
        83744,
        83750,
        83820
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12442
    },
    {
      "id": 83822,
      "name": "Closing Session",
      "isParallelPresentation": false,
      "importedId": "e9cffba4-d07b-457e-9bd4-50c53dd9e526",
      "typeId": 12262,
      "roomId": 10698,
      "chairIds": [
        83821,
        83744,
        83750,
        83820
      ],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12467
    },
    {
      "id": 84000,
      "name": "Demos madness pitches",
      "isParallelPresentation": false,
      "importedId": "OFx74y53bNsC_hMwy117rQ",
      "typeId": 12260,
      "roomId": 10698,
      "chairIds": [
        83998
      ],
      "contentIds": [
        84098,
        84099,
        84100,
        84101,
        84102,
        84103,
        84104,
        84105
      ],
      "source": "PCS",
      "timeSlotId": 12446,
      "note": ""
    },
    {
      "id": 84001,
      "name": "Wip madness pitches",
      "isParallelPresentation": false,
      "importedId": "niE57VtF01d1n53J_nTJzA",
      "typeId": 12266,
      "roomId": 10698,
      "chairIds": [
        83756
      ],
      "contentIds": [
        84113,
        84114,
        84115,
        84116,
        84117,
        84118,
        84119,
        84120,
        84121,
        84122,
        84123,
        84124,
        84125,
        84126,
        84127,
        84128,
        84129,
        84130,
        84131,
        84132,
        84133,
        84134
      ],
      "source": "PCS",
      "timeSlotId": 12450,
      "note": ""
    },
    {
      "id": 84002,
      "name": "Demos + WIP Posters slot",
      "isParallelPresentation": true,
      "importedId": "JxkSz0ur2-y6KSyyrUEvkQ",
      "typeId": 12265,
      "roomId": 10699,
      "chairIds": [
        83869
      ],
      "contentIds": [
        83993,
        83997,
        83994,
        83989,
        83983,
        83980,
        83971,
        83979,
        83977,
        83982,
        83976,
        83981,
        83984,
        83958,
        83959,
        83963,
        83965,
        83969,
        83991,
        83968,
        83961,
        83960,
        83964,
        83962,
        83996,
        83978,
        83967,
        83990
      ],
      "source": "PCS",
      "timeSlotId": 12452,
      "note": ""
    },
    {
      "id": 84003,
      "name": "Demos + WIP Posters slot",
      "isParallelPresentation": true,
      "importedId": "nEYcTOXlmOFOzGzObmBEtg",
      "typeId": 12265,
      "roomId": 10699,
      "chairIds": [
        83869
      ],
      "contentIds": [
        83993,
        83997,
        83994,
        83989,
        83983,
        83980,
        83971,
        83979,
        83977,
        83982,
        83976,
        83981,
        83984,
        83958,
        83959,
        83963,
        83965,
        83969,
        83991,
        83968,
        83961,
        83960,
        83964,
        83962,
        83996,
        83978,
        83967,
        83990
      ],
      "source": "PCS",
      "timeSlotId": 12459,
      "note": ""
    },
    {
      "id": 84004,
      "name": "DC madness pitches",
      "isParallelPresentation": false,
      "importedId": "HkM1owxHsv3oWhsHBEdVdA",
      "typeId": 12261,
      "roomId": 10698,
      "chairIds": [
        83999
      ],
      "contentIds": [
        84092,
        84093,
        84094,
        84095,
        84096,
        84097
      ],
      "source": "PCS",
      "timeSlotId": 12461,
      "note": ""
    },
    {
      "id": 84005,
      "name": "Industry talks",
      "isParallelPresentation": false,
      "importedId": "4TDNaPEqGORBIaIeowbfFQ",
      "typeId": 12269,
      "roomId": 10698,
      "chairIds": [
        83764
      ],
      "contentIds": [
        83988,
        83987,
        83966,
        83986
      ],
      "source": "PCS",
      "timeSlotId": 12457,
      "note": ""
    },
    {
      "id": 84012,
      "name": "Keynote 1",
      "isParallelPresentation": false,
      "importedId": "3f9b7c7a-900a-4acb-a545-24832227fa18",
      "typeId": 12270,
      "roomId": 10698,
      "chairIds": [
        83750,
        83820
      ],
      "contentIds": [
        84009
      ],
      "source": "SYS",
      "timeSlotId": 12443
    },
    {
      "id": 84013,
      "name": "Keynote 3",
      "isParallelPresentation": false,
      "importedId": "14f9472c-87fd-40f5-9690-957867a7a132",
      "typeId": 12270,
      "roomId": 10698,
      "chairIds": [
        83744
      ],
      "contentIds": [
        84011
      ],
      "source": "SYS",
      "timeSlotId": 12466
    },
    {
      "id": 84014,
      "name": "Keynote 2",
      "isParallelPresentation": false,
      "importedId": "c4cd49f0-9ffe-44de-822e-e161eeaa98f6",
      "typeId": 12270,
      "roomId": 10698,
      "chairIds": [
        83821
      ],
      "contentIds": [
        84010
      ],
      "source": "SYS",
      "timeSlotId": 12453
    },
    {
      "id": 84016,
      "name": "Doctoral Consortium",
      "isParallelPresentation": true,
      "importedId": "7de45403-defc-423c-a991-a1bbaef651eb",
      "typeId": 12261,
      "roomId": 10693,
      "chairIds": [
        83999,
        83757
      ],
      "contentIds": [
        84086,
        84087,
        84088,
        84089,
        84090,
        84091
      ],
      "source": "SYS",
      "timeSlotId": 12436,
      "note": ""
    },
    {
      "id": 84017,
      "name": "Doctoral Consortium",
      "isParallelPresentation": true,
      "importedId": "9a01a821-b5aa-41f5-8db3-48889b2d6d58",
      "typeId": 12261,
      "roomId": 10693,
      "chairIds": [
        83999,
        83757
      ],
      "contentIds": [
        83995,
        83970,
        83972,
        83973,
        83985,
        83974
      ],
      "source": "SYS",
      "timeSlotId": 12438,
      "note": ""
    },
    {
      "id": 84063,
      "name": "Diversity Pannel",
      "isParallelPresentation": false,
      "importedId": "f585aa19-3d74-4255-84c6-9d6664398e4a",
      "typeId": 12271,
      "roomId": 10698,
      "chairIds": [
        84065,
        84064
      ],
      "contentIds": [
        84057
      ],
      "source": "SYS",
      "timeSlotId": 12463
    },
    {
      "id": 84066,
      "name": "Emotion IMX2022",
      "isParallelPresentation": false,
      "importedId": "ccfa3322-6d8a-4c9c-92c3-d437327dfde9",
      "typeId": 12267,
      "roomId": 10695,
      "chairIds": [
        84055,
        84056,
        84018,
        84019
      ],
      "contentIds": [
        84060
      ],
      "source": "SYS",
      "timeSlotId": 12436
    },
    {
      "id": 84067,
      "name": "LIQUE 2022",
      "isParallelPresentation": false,
      "importedId": "82ab4b31-1ad9-4a09-b580-827600dbe15f",
      "typeId": 12267,
      "roomId": 10697,
      "chairIds": [
        84053,
        84031,
        83860,
        84036,
        84038,
        84040
      ],
      "contentIds": [
        84058
      ],
      "source": "SYS",
      "timeSlotId": 12436
    },
    {
      "id": 84068,
      "name": "LIQUE 2022",
      "isParallelPresentation": false,
      "importedId": "2261c823-db5e-4979-b84f-5604b16badd8",
      "typeId": 12267,
      "roomId": 10697,
      "chairIds": [
        84053,
        84031,
        83860,
        84036,
        84038,
        84040
      ],
      "contentIds": [
        84106
      ],
      "source": "SYS",
      "timeSlotId": 12438
    },
    {
      "id": 84070,
      "name": "Performances IMX2022",
      "isParallelPresentation": false,
      "importedId": "7bd3f415-2b10-4be8-b0d3-77b7ab3709ec",
      "typeId": 12267,
      "roomId": 10700,
      "chairIds": [
        84020,
        84022,
        84023,
        84024,
        84025,
        84026
      ],
      "contentIds": [
        84061
      ],
      "source": "SYS",
      "timeSlotId": 12436
    },
    {
      "id": 84071,
      "name": "Emotion IMX2022",
      "isParallelPresentation": false,
      "importedId": "2e901cae-2ddb-48e5-8089-72784540de42",
      "typeId": 12267,
      "roomId": 10695,
      "chairIds": [
        84055,
        84056,
        84018,
        84019
      ],
      "contentIds": [
        84108
      ],
      "source": "SYS",
      "timeSlotId": 12438
    },
    {
      "id": 84072,
      "name": "Performances IMX2022",
      "isParallelPresentation": false,
      "importedId": "09b683f3-4fed-4f51-bb74-c88e9a6e0fe9",
      "typeId": 12267,
      "roomId": 10700,
      "chairIds": [
        84020,
        84022,
        84023,
        84024,
        84025,
        84026
      ],
      "contentIds": [
        84107
      ],
      "source": "SYS",
      "timeSlotId": 12438
    },
    {
      "id": 84073,
      "name": "SensoryX 2022",
      "isParallelPresentation": false,
      "importedId": "d9634e1b-96df-4459-ba52-cf56eb5c1c1f",
      "typeId": 12267,
      "roomId": 10696,
      "chairIds": [
        84023,
        84043,
        83868,
        84045,
        84046
      ],
      "contentIds": [
        84059
      ],
      "source": "SYS",
      "timeSlotId": 12436
    },
    {
      "id": 84074,
      "name": "SensoryX 2022",
      "isParallelPresentation": false,
      "importedId": "5720412e-422b-4699-9dd9-2d3c7c14b756",
      "typeId": 12267,
      "roomId": 10696,
      "chairIds": [
        84023,
        84043,
        83868,
        84045,
        84046
      ],
      "contentIds": [
        84109
      ],
      "source": "SYS",
      "timeSlotId": 12438
    },
    {
      "id": 84075,
      "name": "XRWALC 2022",
      "isParallelPresentation": false,
      "importedId": "b5f4efc3-2ddf-4c7c-bb56-1815c81e5fbb",
      "typeId": 12267,
      "roomId": 10694,
      "chairIds": [
        84029,
        84030,
        84032,
        84035,
        83884,
        84039,
        84041
      ],
      "contentIds": [
        84062
      ],
      "source": "SYS",
      "timeSlotId": 12432
    },
    {
      "id": 84076,
      "name": "XRWALC 2022",
      "isParallelPresentation": false,
      "importedId": "74152c64-9af7-4bc8-8847-dc0d8ae2fb94",
      "typeId": 12267,
      "roomId": 10694,
      "chairIds": [
        84029,
        84030,
        84032,
        84035,
        83884,
        84039,
        84041
      ],
      "contentIds": [
        84110
      ],
      "source": "SYS",
      "timeSlotId": 12434
    },
    {
      "id": 84077,
      "name": "XRWALC 2022",
      "isParallelPresentation": false,
      "importedId": "688f36ac-8f47-4e4d-9d51-eacdd5f9f4af",
      "typeId": 12267,
      "roomId": 10694,
      "chairIds": [
        84029,
        84030,
        84032,
        84035,
        83884,
        84039,
        84041
      ],
      "contentIds": [
        84111
      ],
      "source": "SYS",
      "timeSlotId": 12436
    },
    {
      "id": 84078,
      "name": "XRWALC 2022",
      "isParallelPresentation": false,
      "importedId": "94d872c0-f514-45d7-80a3-a7e14e77a0be",
      "typeId": 12267,
      "roomId": 10694,
      "chairIds": [
        84029,
        84030,
        84032,
        84035,
        83884,
        84039,
        84041
      ],
      "contentIds": [
        84112
      ],
      "source": "SYS",
      "timeSlotId": 12438
    },
    {
      "id": 84079,
      "name": "Check-in",
      "isParallelPresentation": false,
      "importedId": "2d3838e9-a50e-4693-87c4-545de38d6ec5",
      "typeId": 12262,
      "roomId": 10703,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12439
    },
    {
      "id": 84083,
      "name": "Check-in",
      "isParallelPresentation": false,
      "importedId": "99487b72-f4c3-435f-9a9f-f4d898c6ce3f",
      "typeId": 12262,
      "roomId": 10703,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12435
    },
    {
      "id": 84084,
      "name": "Check-in",
      "isParallelPresentation": false,
      "importedId": "1559148c-03cd-4874-8d20-fc8f2049b59e",
      "typeId": 12262,
      "roomId": 10700,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12435
    },
    {
      "id": 84085,
      "name": "Check-in",
      "isParallelPresentation": false,
      "importedId": "516e5da8-9514-4c08-ba45-9cab7ad0cc03",
      "typeId": 12262,
      "roomId": 10705,
      "chairIds": [],
      "contentIds": [],
      "source": "SYS",
      "timeSlotId": 12441
    },
    {
      "id": 84234,
      "name": "Demos slot",
      "isParallelPresentation": true,
      "importedId": "8bad6cbb-b503-4d66-822c-cdb1b575cb72",
      "typeId": 12265,
      "roomId": 10704,
      "chairIds": [
        83869
      ],
      "contentIds": [
        83992,
        83975
      ],
      "source": "SYS",
      "timeSlotId": 12452,
      "note": ""
    },
    {
      "id": 84235,
      "name": "Demos slot",
      "isParallelPresentation": true,
      "importedId": "8cce96ec-17e6-488b-9160-aebb5ba668d6",
      "typeId": 12265,
      "roomId": 10704,
      "chairIds": [
        83869
      ],
      "contentIds": [
        83992,
        83975
      ],
      "source": "SYS",
      "timeSlotId": 12459,
      "note": ""
    }
  ],
  "events": [
    {
      "id": 84135,
      "name": "Social Event",
      "isParallelPresentation": false,
      "importedId": "51bbe0f9-f979-4108-9de3-a08e7e4f9bdd",
      "typeId": 12262,
      "roomId": 10707,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1656010800000,
      "endDate": 1656025200000,
      "link": {
        "href": "https://imx.acm.org/2022/program/social-event/",
        "label": "See more:"
      },
      "description": "The social event includes the following activities:  Boat ride (in a moliceiro) in the canals of Aveiro – The “Moliceiro” is a typical boat of the lagoon that was used in seaweed harvesting.  Today these boats are used without mast for that kind of tourist tours. Arrival by “moliceiro” at the dock of the social event dinner place – The Melia Hotel.  Welcome cocktail outside at the dock. Dinner at RESTAURANTE DO LAGO at Melia Hotel Restaurant.",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84136,
      "name": "Cocktail Party",
      "isParallelPresentation": false,
      "importedId": "89771598-6834-4014-9d73-b311dd0cf870",
      "typeId": 12262,
      "roomId": 10706,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655920800000,
      "endDate": 1655928900000,
      "description": "A Cocktail Party with live music (for all attendees) will take place at the end of DC and Workshop sessions.",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84226,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "be512759-3798-4819-92ae-5ad3b4b2c051",
      "typeId": 12262,
      "roomId": 10703,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655893800000,
      "endDate": 1655895000000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84227,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "7b4815f5-a03c-4b34-8e78-56fa598d78e9",
      "typeId": 12262,
      "roomId": 10703,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655913600000,
      "endDate": 1655914800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84228,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "eba2de9e-862f-4928-bcfa-19dc0735a0af",
      "typeId": 12262,
      "roomId": 10705,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655980200000,
      "endDate": 1655981400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84229,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "a156acc9-b2b0-480c-b56b-7ddeef69c88b",
      "typeId": 12262,
      "roomId": 10706,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655902200000,
      "endDate": 1655906400000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84230,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "c0224365-536f-4ea0-b665-146f064abd16",
      "typeId": 12262,
      "roomId": 10706,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655988300000,
      "endDate": 1655993700000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84231,
      "name": "Lunch",
      "isParallelPresentation": false,
      "importedId": "50457f8a-a8d2-429f-b422-f3a7402a0d9d",
      "typeId": 12262,
      "roomId": 10706,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1656076200000,
      "endDate": 1656081600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84232,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "a4b1f2a0-2350-445b-a7ea-3fdcafbb017b",
      "typeId": 12262,
      "roomId": 10705,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1655999400000,
      "endDate": 1656000600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84233,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "ddbc773f-1255-48ed-814c-1b6e794766c9",
      "typeId": 12262,
      "roomId": 10705,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1656088500000,
      "endDate": 1656089700000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 84237,
      "name": "Coffee break",
      "isParallelPresentation": false,
      "importedId": "42216f8f-8c61-4e7b-b1a0-040b1e211ebe",
      "typeId": 12262,
      "roomId": 10705,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1656069300000,
      "endDate": 1656070500000,
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 83793,
      "typeId": 12264,
      "title": "Evaluation of the Performance of an Immersive System for Tele-education",
      "isBreak": false,
      "importedId": "imx22a-1050",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83813
      ],
      "eventIds": [],
      "abstract": "Tele-education was already a solution for people who cannot attend lessons in person (such as inaccessibility in rural areas or illness issues). However, COVID has revealed problems in tele-education with current technology, causing adolescents and children to slow down their learning curves and experience problems of social distancing with their classmates. This paper presents a user study to validate an immersive communication system for tele-education purposes. This system streams in real time a class using 360-degree cameras, allowing remote students to explore the whole scene and improving the feeling of being in the classroom with their colleagues. Additionally, the prototype provides notifications to the remote students about events (such as a changes in the teacher's presentation or classmates raising their hands) that occur outside their viewport to indicate in which direction they should move their heads to visualize them. To validate the system and investigate its possible added value, socioemotional factors such as presence, perceived quality, usability, and usefulness of the notifications were evaluated through a user test using questionnaires. The obtained results show that using immersive tele-education systems can improve the presence, as well as the benefits of the notifications on the experience of the remote students.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSI de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83763
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSI de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83744
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSI de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83760
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSI de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83770
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSIS de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83787
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": "Extended Reality Lab"
            }
          ],
          "personId": 83764
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Universidad Politécnica de Madrid",
              "dsl": "ETSI de Telecomunicación/ Grupo de Tratamiento de Imágenes (GTI)"
            }
          ],
          "personId": 83730
        }
      ]
    },
    {
      "id": 83794,
      "typeId": 12264,
      "title": "Augmented and Virtual Reality-Driven Interventions for Healthy Behavior Change: A Systematic Review",
      "isBreak": false,
      "importedId": "imx22a-1061",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83817
      ],
      "eventIds": [],
      "abstract": "Augmented Reality (AR) and Virtual Reality (VR) have shown potential benefits in managing healthy behavior. This paper presents a systematic review of AR- or VR-driven interventions for promoting healthy behaviors. The review investigates the design, implementations of the intervention, persuasive strategies, intervention platforms, underlying technologies, current trends, and research gaps. Our review of the past 10-years' work in the area reveals that 1) the considered papers focused on seven main healthy behaviors, where “alcohol use” emerged as the most commonly considered behavior; 2) trustworthiness emerged as the most commonly used persuasive strategy; 3) youth are the most targeted audience; 4) VR is more common than AR, and 5) most AR- or VR-driven interventions are perceived to be effective in motivating healthy behavior in people. We also uncover how they use Artificial Intelligence and Object Tracking in this space. Finally, we identify gaps and offer recommendations for advancing research in this area.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Computer Science/ Persuasive Computing Lab"
            },
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Computer Science/ Persuasive Computing Lab"
            }
          ],
          "personId": 83740
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 83792
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            },
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 83746
        }
      ]
    },
    {
      "id": 83795,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "The TV Ecosystem is Fragmented and in Need of New Business and Technological Strategies",
      "isBreak": false,
      "importedId": "imx22a-1052",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83818
      ],
      "eventIds": [],
      "abstract": "The current TV ecosystem is being disturbed by four major fragmentations: linear vs. on-demand content, traditional vs. newer operators, curated vs. algorithmic selection, and TV set vs. additional visualization devices. These fragmentations are changing the TV ecosystem from being channel-based to one that is app-based. However, this multiple app approach makes for a much more complex user experience for the TV viewer. This paper presents five specific strategies, both technological and business-related, to address this challenge and to have the potential to create a win-win scenario for both pay-TV operators and their customers. Those strategies were derived from the experience obtained during the development of the UltraTV research project, which aimed to create a content unification paradigm to improve the TV viewer user experience and its subsequent successful transfer to a commercial offer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Dept. Communication and Arts"
            }
          ],
          "personId": 83788
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Dept. Communication and Arts"
            }
          ],
          "personId": 83750
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Setúbal",
              "institution": "Escola Superior de Tecnologia de Setúbal do IPS",
              "dsl": ""
            }
          ],
          "personId": 83756
        }
      ]
    },
    {
      "id": 83796,
      "typeId": 12264,
      "title": "Exploring Effect of Level of Storytelling Richness on Science Learning in Interactive and Immersive Virtual Reality",
      "isBreak": false,
      "importedId": "imx22a-1063",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83812
      ],
      "eventIds": [],
      "abstract": "Immersive and interactive storytelling in virtual reality (VR) is an emerging creative practice that has been thriving in recent years. Educational applications using immersive VR storytelling to explain complex science concepts have very promising pedagogical benefits because on the one hand, storytelling breaks down the complexity of science concepts by bridging them to people’s everyday experiences and familiar cognitive models, and on the other hand, the learning process is further reinforced through rich interactivity afforded by the VR experiences. However, it is unclear how different amounts of storytelling in an interactive VR storytelling experience may affect learning outcomes due to a paucity of literature on educational VR storytelling research. This preliminary study aims to add to the literature through an exploration of variations in the designs of essential storytelling elements in educational VR storytelling experiences and their impact on the learning of complex immunology concepts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Human-Centered Design"
            }
          ],
          "personId": 83738
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Center for Human Computer Interaction"
            }
          ],
          "personId": 83765
        }
      ]
    },
    {
      "id": 83797,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "Heart Rate Variability for Non-Intrusive Cybersickness Detection",
      "isBreak": false,
      "importedId": "imx22a-1031",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83813
      ],
      "eventIds": [],
      "abstract": "Cybersickness involves all the adverse effects that can occur during a Virtual Reality (VR) immersion, which can compromise the quality of the user experience and limit the usability, functionality and duration of use of VR systems. Standardised protocols help detect stimuli that may cause cybersickness in multiple users but do not fully discriminate which specific users experience cybersickness. Of the biometric measures used to monitor cybersickness in an individual, Heart Rate Variability (HRV) is one of the most used in previous work. However, these only considered its temporal components and did not allow for rest periods between sessions, even though these can affect users' immersion. Our analysis addresses these limitations in that changes in HRV can measure specific levels of discomfort or \"alertness\" associated with the initial cybersickness stimulus induced in the 360 videos. Primarily, our empirical results show significant differences in the frequency components of HRV in response to cybersickness stimuli. These initial measurements can compete with standard subjective assessment protocols, especially for detecting whether a subject responds to a VR immersion with cybersickness symptoms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Milton Keynes",
              "institution": "The Open University",
              "dsl": "Knowledge Media Institute"
            }
          ],
          "personId": 83729
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": ""
            }
          ],
          "personId": 83764
        }
      ]
    },
    {
      "id": 83798,
      "typeId": 12264,
      "title": "Flavor-Videos: Enhancing the Flavor Perception of Food while Eating with Videos",
      "isBreak": false,
      "importedId": "imx22a-1042",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83817
      ],
      "eventIds": [],
      "abstract": "People are typically involved in different activities while eating, particularly when eating alone, such as watching television or playing games on their phones. Previous research in Human-Food Interaction (HFI) has primarily focused on studying people's motivation and analyzing of the media content watched while eating. However, their impact on human behavioral and cognitive processes, particularly flavor perception and its attributes, remains underexplored. We present a user study to investigate the influence of six types of videos, including mukbang - a new food video genre, on flavor perceptions (taste sensations, liking, and emotions) while eating plain white rice. Our findings revealed that participants perceived positive emotional changes and reported significant differences in their augmented taste sensations (e.g., spicy and salty) with different food-based videos. Our findings provided insights into using our approach to promote digital commensality and healthier eating (digital augmentation without altering the food), highlighting the scope for future research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Orono",
              "institution": "University of Maine",
              "dsl": "Multisensory Interactive Media Lab"
            }
          ],
          "personId": 83752
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maine",
              "city": "Orono",
              "institution": "University of Maine",
              "dsl": "School of Computing and Information Science"
            }
          ],
          "personId": 83777
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Toronto",
              "institution": "University of Toronto",
              "dsl": ""
            }
          ],
          "personId": 83737
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Calgary",
              "institution": "University of Calgary",
              "dsl": ""
            }
          ],
          "personId": 83733
        }
      ]
    },
    {
      "id": 83799,
      "typeId": 12264,
      "title": "TCP-Based Distributed Offloading Architecture for the Future of Untethered Immersive Experiences in Wireless Networks",
      "isBreak": false,
      "importedId": "imx22a-1043",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83816
      ],
      "eventIds": [],
      "abstract": "Task offloading has become a key term in the field of immersive media technologies: it can enable lighter and cheaper devices while providing them higher remote computational capabilities. In this paper we present our TCP-based offloading architecture. The architecture, has been specifically designed for immersive media offloading tasks with a particular care in reducing any processing overhead which can degrade the network performance. We tested the architecture for different offloading scenarios and conditions on two different wireless networks: WiFi and 5G millimeter wave technologies. Besides, to test the network on alternative millimeter wave configurations, currently not available on the actual 5G millimeter rollouts, we used a 5G Radio Access Network (RAN) real-time emulator. The results show a great performance  for the tested immersive  media scenarios, highlighting the relevance of millimeter wave technology for the future of immersive media applications. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Nokia XR Lab Spain",
              "dsl": ""
            }
          ],
          "personId": 83766
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Universidad Carlos III de Madrid",
              "dsl": ""
            }
          ],
          "personId": 83751
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Nokia XR Lab Spain",
              "dsl": ""
            }
          ],
          "personId": 83764
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Madrid",
              "institution": "Nokia XR Lab Spain",
              "dsl": ""
            }
          ],
          "personId": 83772
        }
      ]
    },
    {
      "id": 83800,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "Immersive Music Therapy for Elderly Patients",
      "isBreak": false,
      "importedId": "imx22a-1033",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83817
      ],
      "eventIds": [],
      "abstract": "The preliminary results of immersive virtual reality music therapy experiences are presented, carried out with people between ages of 72 and 96, residents of a nursing home, who suffer from mild mental health problems, potential dementia, moderate cognitive disorders, depressive symptoms, or severe dependency.\r\nThe experiences are carried out in 15-minute sessions, streaming 180º and 360º music videos of genres that the elderly like, recorded for this purpose and performed by musical groups that collaborate voluntarily. The design of the developed application (the web interface, the application logic and the HMD player) is simple so that it is easy to use during music therapy sessions.\r\nPreliminary results show that the elderly experience similar levels of sense of presence as reported by other studies in younger population (< 55), and only 19% of them suffer from simulator sickness. This validates the usage of immersive media for music therapy in elderly patients.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": ""
            }
          ],
          "personId": 83764
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Segovia",
              "institution": "Caritas",
              "dsl": ""
            }
          ],
          "personId": 83748
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Segovia",
              "institution": "Caritas",
              "dsl": ""
            }
          ],
          "personId": 83791
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Villanueva de la Cañada",
              "institution": "Universidad Alfonso X el Sabio",
              "dsl": ""
            }
          ],
          "personId": 83743
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Madrid",
              "city": "Villanueva de la Cañada",
              "institution": "Universidad Alfonso X el Sabio",
              "dsl": ""
            }
          ],
          "personId": 83769
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Nokia",
              "dsl": ""
            }
          ],
          "personId": 83734
        }
      ]
    },
    {
      "id": 83801,
      "typeId": 12264,
      "title": "Evaluating and Updating a Design Space for Augmented Reality Television",
      "isBreak": false,
      "importedId": "imx22a-1056",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83815
      ],
      "eventIds": [],
      "abstract": "As Augmented Reality Television (ARTV) transitions out of the feasibility phase, it is crucial to understand the impact of design decisions on the viewers' ARTV experiences.\r\nIn a previous study, six ARTV design dimensions were identified by relying on insights from existing prototypes. However, the set of possible dimensions is likely to be broader.\r\nBuilding on top of previous work, we create an ARTV design space and present it in a textual cheat sheet. We subsequently evaluate the cheat sheet in a between-subject study (n=10), with participants with wide-ranging expertise. We identified six new dimensions (genre, broadcast mode, audience demographics, cartoonish vs. photoreal representation, modality, and privacy), and a new aspect (360°) for the display dimension. In light of our observations, we provide an updated ARTV design space and observe that asking participants to write ARTV scenarios can be an effective method for harvesting novel design dimensions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83731
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83741
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Salford",
              "institution": "The British Broadcasting Corporation",
              "dsl": "Research and Development"
            }
          ],
          "personId": 83768
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83745
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83728
        }
      ]
    },
    {
      "id": 83802,
      "typeId": 12264,
      "title": "An Exploration of Account Sharing Practices on Media Platforms  ",
      "isBreak": false,
      "importedId": "imx22a-1067",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83818
      ],
      "eventIds": [],
      "abstract": "Often accounts and profiles on media platforms discount the real-world complexities of sharing by naively designing technology that supports one user per account. While recently popular media platforms like Netlfix and Disney+ have started responding here, the solutions are simplistic. Within research, despite recent interest, a holistic narrative of the practicalities of sharing and support which could be effectively leveraged for future design of media platforms that support sharing is yet to be investigated. This paper reports a set of user focus groups and expert interviews that present a holistic set of 5 challenges confronting this scope. We further situate these findings in two practical examples of sharing, one current and one near-future, to demonstrate the real-world applicability of said findings by both explicating the challenges and using them as a lens to start exploring responses in each case. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Horizon Digital Economy Hub"
            }
          ],
          "personId": 83783
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": "Horizon Centre for Doctoral Training"
            }
          ],
          "personId": 83749
        }
      ]
    },
    {
      "id": 83803,
      "typeId": 12264,
      "title": "TeleFest: Augmented Virtual Teleportation for Live Concerts",
      "isBreak": false,
      "importedId": "imx22a-1023",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83815
      ],
      "eventIds": [],
      "abstract": "We present TeleFest, a novel system for live-streaming mixed reality 360° videos to online streaming services. TeleFest allows a producer to control multiple cameras in real time, providing viewers with different locations for experiencing the concert, and an intermediate software stack allows virtual content to be overlaid with coherent illumination that matches the real-world footage. TeleFest was evaluated by livestreaming a concert to almost 2,000 online viewers, allowing them to watch the performance from the crowd, the stage, or via a catered experience controlled by a producer in real time that included camera switching and augmented content. The results of an online survey completed by virtual and physical attendees of the festival are presented, showing positive feedback for our setup and suggesting that the addition of virtual and immersive content to live events could lead to a more enjoyable experience for viewers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": ""
            }
          ],
          "personId": 83732
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "Computational Media Innovation Centre"
            }
          ],
          "personId": 83775
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Wellington",
              "city": "wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "Computational Media Innovation Centre"
            }
          ],
          "personId": 83755
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": ""
            }
          ],
          "personId": 83771
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "School of Design Innovation"
            }
          ],
          "personId": 83784
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "Computational Media Innovation Centre"
            }
          ],
          "personId": 83754
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "Computational Media Innovation Centre"
            }
          ],
          "personId": 83790
        }
      ]
    },
    {
      "id": 83804,
      "typeId": 12264,
      "title": "A content-aware tool for converting videos to narrower aspect ratios",
      "isBreak": false,
      "importedId": "imx22a-1012",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83816
      ],
      "eventIds": [],
      "abstract": "The ability to make videos for different aspect ratios (known as video retargeting) contributes to optimal viewing experience on different video platforms. In this paper, we present an idiom-based tool for retargeting videos, from the most common 16:9 aspect ratio into narrower aspect ratios. In contrast to earlier retargeting approaches, which distort the video and are completely automated, our tool enables cropping and panning with user input and oversight. Users can select and order idioms from six cinematic idioms to control video retargeting, and the tool applies selected idioms in order and generates the retargeting results. We performed a pilot study for the feasibility of the tool, and conducted quantitative analysis to inform further work on crafting intelligent cropping and panning tools. In addition, we interviewed an experience video editor on how retargeting is done manually and the quality of the output of the tool.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "Hordaland",
              "city": "Bergen",
              "institution": "University of Bergen",
              "dsl": "Department of Information Science and Media Studies"
            }
          ],
          "personId": 83742
        },
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "BERGEN",
              "institution": "University of Bergen",
              "dsl": "Information Science and Media Studies"
            }
          ],
          "personId": 83773
        }
      ]
    },
    {
      "id": 83805,
      "typeId": 12264,
      "title": "Kintsugi VR: Designing with Fractured Objects",
      "isBreak": false,
      "importedId": "imx22a-1013",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83815
      ],
      "eventIds": [],
      "abstract": "This paper presents Fractured Objects for the design of virtual and mixed-reality experiences. Drawing on the qualitative analysis of three weeks of artistic activities within a residency program, we present six types of Fractured Objects that were used in sketching a mixed-reality performance. Building on these Fractured Objects, as they were articulated by the artists, we present speculative designs for their use in scenarios inspired by research within the IMX community. In discussion, we look to expand the concept of Fractured Objects by relating it to other design concepts such as Seamful Design and Wabi-Sabi, and explore the relationship to the temporality of interaction. We introduce Kintsugi VR with Fractured Objects, drawing on the concept of `golden repair' in which the act of reconnecting fractured parts improves the resulting whole object.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "RISE Research Institutes of Sweden",
              "dsl": ""
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83726
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83762
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University ",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83767
        }
      ]
    },
    {
      "id": 83806,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "More Weather Tomorrow. Engaging Children and their Families with Data through a Personalised Weather Forecast",
      "isBreak": false,
      "importedId": "imx22a-1047",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83812
      ],
      "eventIds": [],
      "abstract": "As data literacy skills are increasingly important in today’s society, scholars have been exploring strategies to engage people with data, for example through storytelling and familiar media such as video. In this paper, we present the design of a video-based data storytelling application that prompts children and their families to explore and interpret historical weather data through a personalised weather forecast. The application was displayed at a 2-month summer exhibition of a popular television channel. In a controlled comparative study, we investigated how the application triggered reflection, as well as emotional and narrative engagement of families at home and at the exhibition. We combined this approach with an in-the-wild study, in which we observed spontaneous interactions of visitors. Our findings indicate that data engagement is encouraged when family interactions occur, which may be facilitated by external environmental conditions and internal story design. Here, we uncover 5 design recommendations for data video storytellers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussels",
              "institution": "VRT Innovation",
              "dsl": ""
            }
          ],
          "personId": 83753
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Brussels",
              "institution": "Flemish Public Broadcaster VRT",
              "dsl": ""
            }
          ],
          "personId": 83786
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "BE",
              "city": "Brussels",
              "institution": "LUCA School of Arts",
              "dsl": "Mediated Environments"
            }
          ],
          "personId": 83780
        }
      ]
    },
    {
      "id": 83807,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "How Do You Pod? A Study Revealing the Archetypal Podcast Production Workflow",
      "isBreak": false,
      "importedId": "imx22a-1058",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83812
      ],
      "eventIds": [],
      "abstract": "The creation, consumption and commercialisation of podcasts have increased rapidly in recent years, yet there is limited research exploring the creators who are often the source of the products in this relatively new medium, as well as the workflows they utilise in making podcasts. Based on semi-structured interviews with sixteen professional podcast creators, and subsequent thematic analysis, this paper 1)~codifies and quantifies the activities involved in podcast creation, 2)~distils the archetypal podcast production workflow, 3)~finds that this workflow is remarkably consistent as a function of podcast genre and creator affiliation (independent or part of a media organisation), and 4)~sheds light on the ``creator'' role that has become a distinctive feature of the medium. This snapshot of the inner workings of the creation process, in the evolution of a highly engaging medium, could form the basis for identifying potential innovations that would increase the interactivity of podcasts in future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Department of Music"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "BBC R&D",
              "dsl": "Audio Team"
            }
          ],
          "personId": 83735
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Salford",
              "institution": "BBC R&D",
              "dsl": ""
            }
          ],
          "personId": 83727
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Department of Music"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Davis",
              "institution": "Music Artificial Intelligence Algorithms, Inc.",
              "dsl": ""
            }
          ],
          "personId": 83774
        }
      ]
    },
    {
      "id": 83808,
      "typeId": 12264,
      "title": "The User Experience of Journeys in the Realm of Augmented Reality Television",
      "isBreak": false,
      "importedId": "imx22a-1037",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83814
      ],
      "eventIds": [],
      "abstract": "Augmented Reality Television (ARTV) can take many forms, from AR content displayed outside the TV frame to video-projected TV screens to social TV watching in VR to immersive holograms in the living room. While the user experience (UX) of individual forms of ARTV has been documented before, \"journeys\" as transitions between such forms have not. In this work, we examine the UX of watching TV when switching between various levels of augmentation. Our findings from an experiment with fourteen participants reveal an UX characterized by high perceived usability, captivation, and involvement with a low to medium workload and a moderate feeling of dissociation from the physical world. We interpret our results in the context of Garrett's established five-plane model of UX-strategy, scope, structure, skeleton, and surface-and propose a sixth plane, \"switch,\" which separates conceptually the design of user journeys in ARTV from the specifics of the other UX planes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ştefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83758
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83759
        }
      ]
    },
    {
      "id": 83809,
      "typeId": 12264,
      "title": "Who Am I Following? Unveiling Behind-the-Scenes Human Interventions and Examining Consumers’ Source Orientation in Virtual Influencer Endorsements",
      "isBreak": false,
      "importedId": "imx22a-1015",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83814
      ],
      "eventIds": [],
      "abstract": "A growing number of computer-generated virtual influencers are being used as alternatives to human endorsers in brand advertising. Because these virtual influencers are not real people, who gets the credit when the endorsement succeeds? And who takes the blame when they fail? In this study, we investigated how and to what extent consumers attribute responsibility to virtual influencers, as well as the behind-the-scenes human interventions (i.e., influencer company, endorsed brand) based on an internal versus external causality for endorsement failure and success—and how their attributions differ compared to human influencer cases. We also examined consumers’ attitudes and behavioral intentions toward influencers and endorsed brands under the given situations. We conducted a 2 (type of influencer: human versus virtual) × 2 (endorsement outcome: success versus failure) × 2 (locus of causality: influencer versus brand) between-subjects online experiment. The results showed that virtual influencers were attributed less blame for an endorsement failure caused by an influencer’s misbehavior than human influencers. However, virtual influencers’ companies and endorsed brands were attributed significantly more responsibilities than their human counterparts. Finally, we discuss the theoretical and practical implications in this paper.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "College of Journalism and Communications"
            }
          ],
          "personId": 83778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 83789
        }
      ]
    },
    {
      "id": 83810,
      "typeId": 12264,
      "title": "CalmResponses: Displaying Collective Audience Reactions in Remote Communication",
      "isBreak": false,
      "importedId": "imx22a-1005",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83813
      ],
      "eventIds": [],
      "abstract": "We propose a system displaying audience eye gaze and nod reactions for enhancing synchronous remote communication. Recently, we have had increasing opportunities to speak to others remotely. In contrast to offline situations, however, speakers often have difficulty observing audience reactions at once in remote communication, which makes them feel more anxious and less confident in their speeches. Recent studies have proposed methods of presenting various audience reactions to speakers. Since these methods require additional devices to measure audience reactions, they are not appropriate for practical situations. Moreover, these methods do not present overall audience reactions. In contrast, we design and develop CalmResponses, a browser-based system which measures audience eye gaze and nod reactions only with a built-in webcam and collectively presents them to speakers. The results of our two user studies indicated that the number of fillers in speaker's speech decreases when audiences' eye gaze is presented, and their self-rating score increases when audiences' nodding is presented. Moreover, comments from audiences suggested benefits of CalmResponses for them in terms of co-presence and privacy concerns.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 83781
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 83782
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            }
          ],
          "personId": 83761
        }
      ]
    },
    {
      "id": 83811,
      "typeId": 12264,
      "durationOverride": 15,
      "title": "Exploring Attitudes Towards Increasing User Awareness of Reality From Within Virtual Reality",
      "isBreak": false,
      "importedId": "imx22a-1029",
      "source": "PCS",
      "trackId": 11840,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        83814
      ],
      "eventIds": [],
      "abstract": "The occlusive nature of VR headsets introduces significant barriers to a user’s awareness of their surrounding reality.\r\nWhile recent research has explored systems to facilitate a VR user’s interactions with nearby people, objects, etc, we lack a fundamental understanding of user attitudes towards and expectations of these systems. \r\nWe present the results of a card sorting study (N=14) which investigated attitudes towards increasing a VR user's reality awareness (awareness of people, objects, audio, pets, and systems to manage and moderate personal usage) whilst in VR.\r\nOur results confirm VR headsets should be equipped with systems to increase a user's awareness of reality. \r\nHowever, opinions vary on how increased awareness should be achieved as our results also highlight differing expectations regarding: persistent vs temporary notification design, notification content and when, why and how awareness should be increased. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83785
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 83747
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83741
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83736
        }
      ]
    },
    {
      "id": 83958,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "User-Centered Broadcasting Service Utilizing Personal Data Store",
      "isBreak": false,
      "importedId": "imx22b-1018",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Recently, the data management model using personal data store (PDS), which is a mechanism for users to store and manage their personal data, has been discussed in response to stricter privacy protection worldwide. In this regard, we developed a data-driven personalization method for broadcasting services. Various personal data, such as program viewing history and Internet service usage history, are stored centrally in the PDS on the user’s side. This personal data can be utilized under user control when using various services allowing cross-industry service collaboration while maintaining a high level of transparency to the user. This enables users to use broadcasting service more widely and conveniently by linking it with various Internet services. In this study, we developed a prototype system that implements end-to-end components from acquisition to utilization of broadcast program viewing history. The system consists of a set of functions that acquires viewing history from broadcast and Internet streaming, stores it on PDS, and uses it in applications. The PDS implementation utilizes open-source software based on web standards to facilitate data linkage with a variety of Internet services. As an effective example for system evaluation, we designed and prototyped on-demand video viewing services in which separate applications of different service providers are linked via the PDS.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 83873
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 83895
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Setagaya, Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": "Science and Technology Research Labs"
            }
          ],
          "personId": 83850
        }
      ]
    },
    {
      "id": 83959,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Scenario-based Exploration of Integrating Radar Sensing into Everyday Objects for Free-Hand Television Control",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532982"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1035",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "We address gesture input for TV control, for which we examine mid-air free-hand interactions that can be detected via radar sensing. We adopt a scenario-based design approach to explore possible locations from the living room where to integrate radar sensors, e.g., in the TV set, the couch armrest, or the user's smartphone, and we contribute a four-level taxonomy of locations relative to the TV set, the user, personal robot assistants, and the living room environment, respectively. We also present preliminary results about an interactive system using a 15-antenna ultra-wideband 3D radar, for which we implemented a dictionary of six directional swipe gestures for the control of dichotomous TV system functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83875
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ştefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83758
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83759
        }
      ]
    },
    {
      "id": 83960,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Learning Sustainable Locust Control Methods in Virtual Reality",
      "isBreak": false,
      "importedId": "imx22b-1013",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Invasion of locust swarms has affected the crops in many countries in Africa and Asia, which is a significant threat to food security. Therefore, different approaches are adopted to monitor and control the locust swarms to save the crops. Furthermore, it has been proved in various studies that technology can help in agriculture through drones, real-time data monitoring, or teaching the farmers with the latest tools. \r\n\r\nFollowing the UN sustainability goals for food security, this research has presented a Virtual Reality(VR) based educational application to teach sustainable locust management strategies. Using hand tracking technology in the Oculus Quest lets users learn how farmers can deal with locusts without pesticides. Based on a storytelling approach, the methods presented are profitable for the farmers and free of any harm to crops regarding food security. This application can help motivate the adoption of these sustainable locust control strategies in broader interventions for environmental recovery.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 83874
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Applied Science BFI Vienna",
              "dsl": ""
            }
          ],
          "personId": 83933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 83927
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 83892
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": ""
            }
          ],
          "personId": 83900
        }
      ]
    },
    {
      "id": 83961,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "The Augmented Museum: A Multimodal, Game-Based, Augmented Reality Narrative for Cultural Heritage",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532967"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1036",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "As digitization has transformed media and Augmented Reality (AR) is evolving from a research area to a commodity, museums are creating interactive AR experiences to digitally enhance their collection and increase audience engagement. Head-worn AR experiences, though, face interaction challenges as they are often employed in busy spaces and are in need of intuitive multimodal interfaces for users on the move. This paper presents an innovative, work-in-progress, multimodal AR experience integrating non-obtrusive dialogue, music, and sound as well as gesture and gaze-based interaction, while a user is wearing a head-worn AR display. Users are motivated to explore and interact with digital cultural artefacts superimposed onto the real-world museum setting and physical artefacts, while moving around in a museum setting. We initially analyze interactive AR experiences to identify specific user requirements related to head-worn AR experiences. We deploy these requirements for the design of interactive, multimodal AR in a museum setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83911
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83913
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83947
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83922
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83909
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83957
        }
      ]
    },
    {
      "id": 83962,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "CS:NO - an Extended Reality Experience for Cyber Security Education",
      "isBreak": false,
      "importedId": "imx22b-1037",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "This work-in-progress presents the design of an XR prototype for the purpose of educating basic cybersecurity concepts. We have designed an experimental virtual reality cyberspace to visualise data traffic over network, enabling the user to interact with VR representations of data packets. Our objective was to help the user better conceptualise abstract cybersecurity topics such as encryption and decryption, firewall and malicious data. Additionally, to better stimuli the sense of immersion we have used Peltier thermoelectric modules and Arduino Uno to experiment with multisensory XR. Furthermore, we reflect on early evaluation of this experimental prototype and present potential paths for future improvements. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83954
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83918
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83951
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83923
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83862
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83776
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "RISE Research Institutes of Sweden",
              "dsl": "RISE  Cybersecurity"
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83726
        }
      ]
    },
    {
      "id": 83963,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Classification of the Video Type and Device Used in 360-Degree Videos from the Trajectories of its Viewers' Orientations with LSTM Neural Network Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532975"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1015",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "360-degree videos are consumed in diverse devices: some based in immersive interfaces, such as viewed through Virtual Reality headsets and some based in non-immersive interfaces, as in a computer with a pointing device or mobile devices with touchscreens. We have found, in prior work, significant differences in user behavior between these devices. From a dataset of the trajectories of the users’ head orientation in 775 video reproductions, we classify which kind of video was played (two values) and which of the four possible devices was used to reproduce these videos. We found that recurrent neural network models based on LSTM layers are able to classify the video type and the device used to play the video with an average accuracy of over 90% with only four seconds of trajectory. We are convinced that this knowledge can improve techniques to predict future viewports used in viewport-adaptive streaming when diverse devices are used.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83925
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83887
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83945
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83824
        }
      ]
    },
    {
      "id": 83964,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Augmenting a Nature Documentary with a Lifelike Hologram in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532974"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1038",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "While augmented reality television (ARTV) is being investigated in research labs, the high cost of AR headsets makes it difficult for audiences to benefit from the research. However, the relative affordability of virtual reality (VR) headsets provides ARTV researchers with opportunities to test their prototypes in VR. Additionally, as VR becomes an acceptable medium for watching conventional TV, augmenting such viewing experiences in VR creates new opportunities.\r\nWe prototype a nature documentary ARTV experience in VR and conduct a remote user study (n=10) to investigate six points on the visual display design dimension of presenting a lifelike programme-related hologram. We manipulated the starting point and the movement behaviour of the hologram to gain insight into viewer preferences.\r\nOur findings highlight the importance of personal preferences and that of the perceived role of a hologram in relation to the underlying TV content; suggesting there may not be a single way to augment a TV programme. Instead, creators may need to provide the audiences with capabilities to customise ARTV content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83731
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Salford",
              "institution": "The British Broadcasting Corporation",
              "dsl": "Research and Development"
            }
          ],
          "personId": 83768
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83741
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83745
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83728
        }
      ]
    },
    {
      "id": 83965,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Deep Learning Augmented Realistic Avatars for Social VR Human Representation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532976"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1016",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) has created a new and rich medium for people to meet each other digitally. In VR, people can choose from a broad range of representations. In several cases, it is important to provide users with avatars that are a lifelike representation of themselves, to increase the user experience and effectiveness of communication. In this work, we propose a pipeline for generating a realistic and expressive avatar from a single reference image. The pipeline consists of a blendshape-based avatar combined with two deep learning improvements. The first improvement module runs offline and improves the texture map of the base avatar. The second module runs inference in real-time at the rendering stage and performs a style transfer to the avatar's eyes. The deep learning modules effectively improve the visual representation of the avatar and show how AI techniques can be integrated with traditional animation methods to generate realistic human avatars for social VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83839
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83932
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83921
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83903
        }
      ]
    },
    {
      "id": 83966,
      "typeId": 12269,
      "durationOverride": 15,
      "title": "Neuroscience-based Intelligent Video Evaluation System",
      "isBreak": false,
      "importedId": "imx22d-1004",
      "source": "PCS",
      "trackId": 11843,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84005
      ],
      "eventIds": [],
      "abstract": "BrainAnswer is a web-based platform for psychophysiological data acquisition and storage. It enables creating highly customizable protocols for neuroscience studies of media or advertising content and allows the simultaneous recording of content, physiological signals, and self-reporting from the participants using the compatible hardware. The industry has a lot to gain from integrating psychophysiological data in media studies as this data is not dependent on the consciously declared self-report of subjects to obtain measurable results and is able to depict unconscious processes occurring during the experience or the contact with content. As an example, a study of an advertising video with 28 subjects showed a connection between the drop in psychophysiological emotional engagement at its end and its low success. This shows that media and advertising companies can benefit from the BrainAnswer approach of studying psychophysiological data to adapt their content in order to reduce failure risk and improve success rates.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Castelo Branco",
              "city": "Castelo Branco",
              "institution": "BrainAnswer, LDA",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "IPCB",
              "dsl": "ESALD"
            }
          ],
          "personId": 83940
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "BrainAnswer",
              "dsl": ""
            }
          ],
          "personId": 83905
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "IPCB",
              "dsl": "ESALD"
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Castelo Branco",
              "institution": "BrainAnswer Lda",
              "dsl": ""
            }
          ],
          "personId": 83916
        }
      ]
    },
    {
      "id": 83967,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "HiruXR: a Web library for Collaborative and Interactive Data Visualizations in XR and 2D",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532981"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1020",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "HiruXR is a Javascript library aimed at creating collaborative and interactive data visualizations in eXtended Reality (XR). Developers can use it to create environments where multiple users can communicate and collaborate around one or many visualizations. We want to open these environments to users with 2D displaying and interaction capabilities, e.g. allowing them to collaborate from their laptops and phones with others using Head Mounted Displays (HMD). But displaying the same interface is not optimal, users should see an interface according to their device capabilities. Therefore, we are steering the design of HiruXR towards supporting responsiveness without putting all the burden in application developers. The library defines visualization, interaction and collaboration components that adapt to the user device capabilities, i.e. 2D or XR. In this paper, we share the design philosophy, initial implementation examples and lessons learned so far building some of these components for collaboration between VR and 2D devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 83935
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Donostia",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83936
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83893
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83939
        }
      ]
    },
    {
      "id": 83968,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "AR in the OR: exploring use of augmented reality to support endoscopic surgery",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532970"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1043",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Modern operating rooms (OR) are equipped with several ceiling- and wall-mounted screens that display surgical information. These physical displays are restricted in placement, limiting the surgeons' ability to freely position them in the environment. Our work addresses this issue by exploring the feasibility of using an augmented reality (AR) headset (Microsoft HoloLens 2) as an alternative to traditional surgical screens; leading to a reduced OR footprint and improved surgical ergonomics. We developed several prototypes using state-of-the-art hardware/software and conducted various neurosurgery-related exploratory studies. Initial feedback from users suggests that coloration and resolution of the holographic feed were adequate, however, surgeons frequently commented on tactile/visual asynchrony. This emphasizes the need for novel, more efficient hardware/software solutions to support fine motor tasks in the OR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83904
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83908
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83838
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83934
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83825
        }
      ]
    },
    {
      "id": 83969,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "\"I want to be independent. I want to make informed choices.\": An Exploratory Interview Study of the Effects of Personalisation of Digital Media Services on the Fulfilment of Human Values",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532977"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1022",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "From the landing page of a shopping website, to a tailored layout on a video streaming app, digital media experiences are becoming increasingly personalised, and none of us have the same experience as each other. We report on a series of in-depth interviews, with UK media users from 19 to 68 years old, exploring their awareness, feelings, expectations and concerns about the digital media being personalised ’for them’, and the language that they use when talking about it. Our repeatable, extensible methodology develops insights aligned to a framework of fundamental human values.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83863
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Salford, Manchester",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83852
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": ""
            }
          ],
          "personId": 83926
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83848
        }
      ]
    },
    {
      "id": 83970,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "The Promotion of Empathy in Intelligent Assistants for iTV through Proactive Behaviours",
      "isBreak": false,
      "importedId": "imx22e-1004",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "The integration of intelligent assistants, in devices belonging to the television ecosystem, has simplified the accomplishment of more demanding tasks (such as content searches). However, it can be seen that these assistants are restricted to purely reactive behaviours and show a reduced human and empathic dimension in relation to the users. However, in other application domains, there has been an increasing integration of proactive behaviours, which can counteract these barriers and, consequently, improve the respective User Experience (UX). It is precisely in this context of proactivity that this research is designed. The goal is to contribute to the advancement of intelligent assistants in the interactive TV (iTV) domain, studying which proactive behaviours can be integrated in an intelligent assistant for iTV to promote its empathy, the associated UX and, consequently, its adoption in a more fluid and massive way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia"
            }
          ],
          "personId": 83953
        }
      ]
    },
    {
      "id": 83971,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Usability Of Text-To-Speech Technology in Creating News Podcasts using Portuguese Of Portugal",
      "isBreak": false,
      "importedId": "imx22b-1040",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "The increase in the consumption of digital formats has, in many cases, been a penalty for traditional media companies. In the adaptation to digital, the transformation of written news into audio formats, that guarantee spatio-temporal flexibility in its consumption, is one of the differentiating options. Artificial intelligence tools can help accelerate and automate the digitalization processes. It is, therefore, the objective of this paper to evaluate the integration of Text-to-Speech (TTS) technology in the process of creating news podcasts. The study comprised two surveys. The first corresponding to the validation of TTS services in Portuguese from Portugal and, the second for the validation of three models of news podcasts containing human voice, synthesized voice via TTS, and a hybrid model with TTS voice and human voice. The results point to a general acceptance of the integration of voices generated by TTS in news podcasts without prejudice to the consumer experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            },
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83853
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83955
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 83907
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 83869
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83946
        }
      ]
    },
    {
      "id": 83972,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "Enabling User-centric Assessment and Modelling of Immersiveness in Multimodal Multimedia Applications",
      "isBreak": false,
      "importedId": "imx22e-1002",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "Multimodal, immersive systems are the latest development within the field of multimedia. These systems emulate the senses by means of omnidirectional visuals, 360° sound, motion tracking and touch simulation to create a feeling of presence in the virtual environment. They have the potential to substitute physical interactions in application domains such as training (Industry 4.0), or e-health (tele-surgery). However, the COVID-19 pandemic has shown that they are not ready, as they still have room for improvement in terms network streaming quality, usability and the users’ feeling of presence. In addition, these systems can induce feelings of dizziness, nausea etc. (i.e. cybersickness). These factors therefore have an important impact on the user’s total immersion. In this work, we therefore propose that immersiveness can be devised from measuring four aspects, namely: presence (i.e. the feeling of being \"in\" the environment), cybersickness, network related Quality-of-Experience (QoE) and the usability of the application. Therefore, a two-dimensional user-centric approach on the assessment and modelling of immersiveness is proposed. These dimensions include (i) subjective and objective assessment of presence, cybersickness, usability and QoE and (ii) real-time modelling of immersiveness. Furthermore, a proof-of-concept is envisioned including two use cases. As such, we believe that this position paper will significantly advance the state of the art on immersive systems and multimedia in general.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Oost-Vlaanderen",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            }
          ],
          "personId": 83879
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": ""
            }
          ],
          "personId": 83943
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 83941
        }
      ]
    },
    {
      "id": 83973,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "Human-Computer Interaction Patterns for Head-Mounted-Device-based Augmented Reality in the Exhibition Domain",
      "isBreak": false,
      "importedId": "imx22e-1007",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "Head-mounted-device (HMD) based augmented reality (AR) applications bring new opportunities to the exhibition domain. However, first-time users such as exhibition visitors are not familiar with the interaction method of the HMD, which also brings challenges to exhibition authors while implementing HMD based AR technologies. This research project focuses on the accessibility of the HMD based AR application in the exhibition domain, which explores potential interaction patterns based on the technical feature of the HMD for exhibition-related use cases. Both information system research method and design research method are applied while exploring interaction solutions for human factor challenges. Several prototypes will be created and iteratively tested and evaluated at the exhibition for generalizing effective and accessible interaction patterns. As the result, it will provide a pattern-based interaction system as an artifact with design knowledge to developers so that they can convert the interaction pattern into an authoring tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wiesbaden",
              "institution": "RheinMain University of Applied Sciences",
              "dsl": "Design, Computer Science, Media"
            }
          ],
          "personId": 83866
        }
      ]
    },
    {
      "id": 83974,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "Factors influencing video Quality of Experience measured with ecologically valid methods",
      "isBreak": false,
      "importedId": "imx22e-1005",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "Over the last decade, user subjective perception became the subject of interest in video quality studies. Researchers conclude about this Quality of Experience (QoE) based on participants’ statements, behaviors, and psychophysiological reactions to distorted videos. The reason for that is the fact that direct, objective evaluation of QoE is impossible due to its subjective nature. Thus, clear operationalization of variables in QoE studies is crucial. For that purpose theoretical background is necessary. Current theoretical models of QoE consist of many strongly correlated variables and omit the role of content. In effect, most QoE studies related to compression use strict laboratory experiments with Absolute Category Ratings. The ecological validity of such studies is limited. In my Ph.D., I investigate factors influencing QoE in a more natural context. To be able to conclude about those multiple, complex variables I am working on a content-based video QoE model inspirited by Structural Causal Models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Department of Information and Communication Technologies"
            }
          ],
          "personId": 83919
        }
      ]
    },
    {
      "id": 83975,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "Remote Presence: Live Holograms for a Social Classroom",
      "isBreak": false,
      "importedId": "imx22c-1011",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84234,
        84235
      ],
      "eventIds": [],
      "abstract": "Existing communication technologies have displayed a lack of affordances in supporting social-emotional connections, which is of particular interest in educational settings. We are therefore developing a live sensory immersive 3D video technology, built on a prior developed platform. Pilot trials in a Finnish school have yielded promising findings. We continue to advance the state-of-the-art platform in parallel with regards to 3D capture quality and data compression algorithms. Current developments entail joint investigations and evaluations of affordances to support emotional, social, motivational, and achievement impacts with learners and teachers from a Namibian and Finnish school. Participants can experience \"remote presence\" wearing the hololens 2 while others are live streamed from another country captured by two cameras.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83844
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83864
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83851
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 83888
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83938
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": "Interaction Design Lab"
            }
          ],
          "personId": 83831
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 83835
        }
      ]
    },
    {
      "id": 83976,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Acting emotions: physiological correlates of emotional valence and arousal dynamics in theatre",
      "isBreak": false,
      "importedId": "imx22b-1024",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Professional theatre actors are highly specialized in controlling their own expressive behaviour and non-verbal emotional expressiveness, so they are of particular interest in fields of study such as affective computing. We present Acting Emotions, an experimental protocol to investigate the physiological correlates of emotional valence and arousal within professional theatre actors. Ultimately, our protocol investigates the physiological agreement of valence and arousal amongst several actors. Our main contribution lies in the open selection of the emotional set by the participants, based on a set of four categorical emotions, which are self-assessed at the end of each experiment. The experiment protocol was validated by analyzing the inter-rater agreement (> 0.261 arousal, > 0.560 valence), the continuous annotation trajectories, and comparing the box plots for different emotion categories. Results show that the participants successfully induced the expected emotion set to a significant statistical level of distinct valence and arousal distributions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "Faculty of Engineering of Porto",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "INESC-TEC",
              "dsl": ""
            }
          ],
          "personId": 83861
        }
      ]
    },
    {
      "id": 83977,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "A Podcast Creation Platform to Support News Corporations: Results from UX Evaluation",
      "isBreak": false,
      "importedId": "imx22b-1046",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Podcasts as a media format have been increasing popular in recent years. The ease of access to this format have contributed for its success story. However, the creation of Podcasts requires specific hardware and software for recording and editing it. Some platforms have emerged with the proposal to ease this creation process, namely by introducing Text-to-Speech (TTS) technologies removing the need for capturing and editing voice, reducing the effort necessary for producing this format, yet no platform allows the use of TTS in Portuguese of Portugal while retaining the scope of “podcast creation platform”. Taking these limitations in mind we present the proposal of an all-in-one Podcast Creation platform with the availability of TTS Technology in Portuguese of Portugal. The paper describes the usability testing (UX) of the platform using 3 methodologies being: Self-Assessment Manikin (SAM), System Usability Scale (SUS) and Attrakdiff. with promising results regarding its usability and desirability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83955
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 83907
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 83869
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83853
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83946
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83898
        }
      ]
    },
    {
      "id": 83978,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Emotional Virtual Reality Stroop Task: an Immersive Cognitive Test",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532988"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1025",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Stroop Colour-Word Task has been widely used as a cognitive task. There are computerised and Virtual Reality versions of this task that are commonly used. The emotional version of the task, called the Emotional Stroop Colour-Word task is commonly used to induce certain emotions in a person. We are developing an application that brings the Emotional Stroop Colour-Word task into Virtual Reality. The aim of this application is to elicit different stress levels on the user and to record associated brain, heart and skin activity using wearable sensors. It is an immersive application that includes a tutorial, artificial intelligence generated audio instructions and a logging system for the user activity.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Munster",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83890
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83834
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83917
        }
      ]
    },
    {
      "id": 83979,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Integrating 3D Objects in Multimodal Video Annotation",
      "isBreak": false,
      "importedId": "imx22b-1047",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "This paper presents and discusses the introduction of 3D functionalities for an existing web-based multimodal video annotation tool. Over the past years, we have developed a multimodal web video annotation tool that now combines 3D models and 360º content with more traditional annotation types (e.g., text, drawings, images), offering users the possibility of adding extra information in their annotation work. We show how 3D models augment the annotation work and add advantages like viewing or exploring objects in detail and from different angles. The paper reports detailed feedback from a pilot study in form of a workshop with traditional dance experts to whom these new features were presented. We conclude with an outlook of future iterations of the video annotator based on the experts’ feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "NOVA University of Lisboa",
              "dsl": "Faculdade de Ciências e Tecnologia "
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Setúbal",
              "institution": "Polytechnic Institute of Setúbal",
              "dsl": "Escola Superior de Tecnologia "
            }
          ],
          "personId": 83867
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Nova University of Lisboa",
              "dsl": "ICNOVA, FSCH"
            }
          ],
          "personId": 83845
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "FCSH - Universidade Nova de Lisboa",
              "dsl": "ICNOVA"
            }
          ],
          "personId": 83889
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "NOVA School of Science & Technology, NOVA University Lisbon,",
              "dsl": ""
            }
          ],
          "personId": 83896
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS"
            }
          ],
          "personId": 83855
        }
      ]
    },
    {
      "id": 83980,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "ScenaProd: creating interactive medias without programming",
      "isBreak": false,
      "importedId": "imx22c-1013",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "This demonstration aims at presenting ScenaProd, a tool that allows people to produce multisensory scenagrams (multisensory exercises or interactive media). All participants can create their own scenagrams with that tool or test a more complete one that is already created. A scenagram can be defined as an interaction between a human being and different devices. For example, a robot asks a question while displaying a visual clue on a screen. Then, the participant can respond by pressing a large and colored contactor. In case of a correct answer, the robot would play a short victory song and a light would light up green. If a wrong answer were given, the system would have a different reaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83868
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83944
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Vannes",
              "institution": "South Brittany University",
              "dsl": ""
            }
          ],
          "personId": 83956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83894
        }
      ]
    },
    {
      "id": 83981,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Party Mascot: Experimental Prop Design for Streaming Actual Plays ",
      "isBreak": false,
      "importedId": "imx22b-1026",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Party Mascot is an experimental design for a dynamic, interactive prop used in “actual play” streaming. Taking the form of a talking mechanical bird, the Party Mascot extends audience participation on the Twitch platform from its native chat interface to the physical playspace. Building on a critical review of frame analytical approaches to role-playing game studies and supported by an ethnographic study of actual play performers, the Party Mascot is designed to “flicker” between social, gameplay, and fictional frames of interaction. It can accommodate any number of participants and adapts to multiple roles within new mediated performance contexts. Shifting spectatorship from the screen to the physical world, the Party Mascot can reconfigure audience/performer relationships, open new avenues for game design, and engage the genre of actual play as a new site of experimentation and innovation between the producers and consumers of media.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media, School of Literature, Media and Communication"
            }
          ],
          "personId": 83937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology,",
              "dsl": "Digital Media, School of Literature, Media, and Communication"
            }
          ],
          "personId": 83857
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media/ School of Literature, Media, and Communication"
            }
          ],
          "personId": 83876
        }
      ]
    },
    {
      "id": 83982,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Peripheral Light Cues as a Naturalistic Measure of Focus",
      "isBreak": false,
      "importedId": "imx22b-1027",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Deeply immersive experiences are intrinsically rewarding; evoking them for another is a cornerstone of success in artistic or design practice.  At the same time, modern interfaces have created a state of 'partial continuous attention', and frequent self-interruption is more common than ever.  In this paper, we propose a smart-glasses based interaction to quantify self-interruption dynamics in naturalistic settings, in which a slowly changing peripheral LED is monitored as a secondary task by the user.  We demonstrate that this interaction captures useful information about a user's state of engagement in real-world conditions.  These data can provide designers and artists novel, objective insight into the depth of immersive experience evoked in real-world settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments Group"
            }
          ],
          "personId": 83843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments"
            }
          ],
          "personId": 83840
        }
      ]
    },
    {
      "id": 83983,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "Sunflower: An Interactive Artistic Environment based on IoMusT Concepts",
      "isBreak": false,
      "importedId": "imx22c-1012",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "The Internet of Musical Things (IoMusT) is an interdisciplinary area that aims to improve the relationship between musicians and their peers, as well as between musicians and audience members, creating new forms of interaction in concerts, studio productions, and music learning. \r\n  Although emerging, this field already faces some challenges, such as lack of privacy and security, and mainly, lack of standardization and interoperability between its devices. Therefore, an environment design, called Sunflower, was proposed, which tries to contribute to solving the most recurrent problems in this area, specifying an architecture pattern, protocol, and sound features that aim to allow heterogeneity in these systems. \r\n  Its practical implementation resulted in an interoperable, multimedia, and interactive environment. This paper, therefore, shows a demonstration of how Sunflower works in the accomplishment of an artistic presentation, also emphasizing its approach, the technologies that support it, and the advances it can bring to the area of IoMusT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "Niterói",
              "institution": "Fluminense Federal University",
              "dsl": "MidiaCom Lab"
            }
          ],
          "personId": 83841
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Rio de Janeiro",
              "city": "Niterói",
              "institution": "Fluminense Federal University - UFF",
              "dsl": "Computer Science"
            }
          ],
          "personId": 83860
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Minas Gerais",
              "city": "São João del-Rei",
              "institution": "Federal University of São João del-Rei",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 83914
        }
      ]
    },
    {
      "id": 83984,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Towards Multimodal Search and Visualization of Movies Based on Emotions",
      "isBreak": false,
      "importedId": "imx22b-1049",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Movies are one of the most important and impactful forms of entertainment and a powerful vehicle for culture and education, due to the cognitive and emotional impact on the viewers, and technology has been making them more accessible in pervasive services and devices. As such, the huge amount of movies we can access, and the important role emotions play in our lives, make more pertinent the ability to access, visualize and search movies based on their emotional impact. In this paper, we characterize the challenges and approaches in this scenario, and present interactive means to visualize and search movies based on their dominant and actual emotional impact along the movie, with different models and modalities. In particular through emotional highlights and trajectories, the user’s emotional state, or a music being played. Music contributes greatly to the emotional impact of movies and it can also be a trigger to get us into one of them in\r\nserendipitous moments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 83899
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 83942
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculty of Science at University of Lisbon",
              "dsl": ""
            }
          ],
          "personId": 83924
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Universidade de Lisboa",
              "dsl": "LASIGE, Faculdade de Ciências"
            }
          ],
          "personId": 83884
        }
      ]
    },
    {
      "id": 83985,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "Modeling Cognitive Load and Affect in Interactive Game-based Learning Using Physiological Features",
      "isBreak": false,
      "importedId": "imx22e-1009",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "Media use in educational environments has been rapidly developing with the increasing availability and diversity of interactive elements. By understanding how student cognitive load changes when interacting with learning technologies, we can make sense of their learning process and how to provide appropriate, personalized media design to enhance the learning experience. Recent developments in sensing technologies makes it possible to capture learner’s dynamic physiological reactions. In this thesis research, we will identify learner’s cognitive load when interacting with educational media. We will explore how affective reactions contribute to the modeling of cognitive load, and how real-time cognitive load changes alongside learning activities. We focus on modeling such information using physiological reactions that include pupillary, cardiovascular, and electrodermal responses. We are conducting this work in a game-based learning (GBL) environment for reading comprehension. We have implemented a sensing pipeline that will enable the modelling of learner affect and cognitive load. The modeling and analysis of this project can further support the design of interactive learning media that provides real-time adaptation in learning processes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83886
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Dept. of Computing Science"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Language Sciences"
            }
          ],
          "personId": 83910
        }
      ]
    },
    {
      "id": 83986,
      "typeId": 12269,
      "durationOverride": 15,
      "title": "Olympics on the Google Assistant: Modular Conversation Design",
      "isBreak": false,
      "importedId": "imx22d-1002",
      "source": "PCS",
      "trackId": 11843,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84005
      ],
      "eventIds": [],
      "abstract": "While many voice assistants use natural language processing and understanding to determine a user’s intent, Conversation Designers (CxDs) create many responses by producing voice user interface specs. These are often bespoke solutions, optimized for a feature. However, this approach is difficult to scale. This industry talk describes the design process of the first Olympics experience on Google Assistant and how it uses modular design to scalably answer millions of questions. To do this effectively, the design process required grouping together frequently asked queries, creating modular design components based on available data, and emphasizing/de-emphasizing components of an answer to get the widest intent coverage. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": "Assistant/Search"
            }
          ],
          "personId": 83872
        }
      ]
    },
    {
      "id": 83987,
      "typeId": 12269,
      "durationOverride": 15,
      "title": "Immersive Tele-operation Driving thought 5G",
      "isBreak": false,
      "importedId": "imx22d-1003",
      "source": "PCS",
      "trackId": 11843,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84005
      ],
      "eventIds": [],
      "abstract": "In environments of autonomous AGVs operating at industrial areas, carrying material from place to place, and performing different tasks, some unexpected problems may arise. Troublesome objects in the middle of the AGV path can be problematic and sometimes involve a stop in the system. In such cases, a human intervention is necessary.\r\nThe Tele-operation Driving (ToD) with MR and haptics devices is an innovative solution to have AGVs under control, being able to control them remotely without the need to be exposed to the dangers of the industrial environment. \r\nIn order to achieve the immersive ToD an E2E system is designed.\r\n·      Immersive cockpit: an innovative virtual reality application is designed to emulate an ordinary driving experience. A virtual cockpit is visualized at head-mounted display and a real time streaming video from the AGV surroundings is projected around the virtual car. The immersive application allows the operator to feel inside of the AGV and enables an intuitive remote driving of the AGV.\r\n·      Connectivity requirements: to accomplish with necessities of the ToD data transmission some requirements must be fulfilled. The AGV connectivity is set through 5G with millimeter wave. This innovative connectivity enables low latencies at the highest throughput, especially focused on uplink. Those characteristics allow to send video streaming in real time and set the information flux required for the ToD. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Bell Labs",
              "dsl": "Nokia"
            },
            {
              "country": "Spain",
              "state": "",
              "city": "Madrid",
              "institution": "Bell Labs",
              "dsl": "Nokia"
            }
          ],
          "personId": 83836
        }
      ]
    },
    {
      "id": 83988,
      "typeId": 12269,
      "durationOverride": 15,
      "title": "Intelligent Fatigue Driving Detection & Management System based on Sensing Technology and AI",
      "isBreak": false,
      "importedId": "imx22d-1001",
      "source": "PCS",
      "trackId": 11843,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84005
      ],
      "eventIds": [],
      "abstract": "In this presentation, we successfully developed the first intelligent driving fatigue detection system ( IDFDS) system based on GSR signals, and it has been released to car makers. The system has overcome the technical challenges of physiological signals and being transformed into a wearable ring satisfied with drivers’ experience. Thanks to the advanced physiological computing technology, material science and excellent support from industrial production chain, the commercial IDFDS is available to the market. In the future, IDFDS can be integrated with other sensors, or other detection technologies, and to provide more accurate service to users.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "WenZhou",
              "institution": "SenTech Wearable & AI",
              "dsl": "Intelligent Driving AI Lab"
            }
          ],
          "personId": 83929
        }
      ]
    },
    {
      "id": 83989,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "The Co-Creation Space: An Online Safe Space for Community Opera Creation",
      "isBreak": false,
      "importedId": "imx22c-1004",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "This work presents the Co-Creation Space, a multilingual platform for professional and community artists to 1) generate raw artistic ideas, and 2) discuss and reflect on the shared meaning of those ideas. The paper describes the architecture and the technology behind the platform, and how it was used to facilitate the communication process during several user trials. By supporting ideation sessions around media items guided by a facilitator and allowing users to express themselves and be part of the creation of an artistic product, participants were enabled to access new cultural spaces and be part of the creative process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": ""
            }
          ],
          "personId": 83948
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Netherlands",
              "city": "Amsterdam",
              "institution": "CWI ",
              "dsl": "DIS "
            }
          ],
          "personId": 83931
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 83935
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 83915
        }
      ]
    },
    {
      "id": 83990,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Designing a VR Lobby for Remote Opera Social Experiences",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "10.1145/3505284.3532980"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1031",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Several social VR platforms support virtual entertainment events, however their value for post-show activities remains unclear. Through a user-centered approach, we design a social VR lobby experience to enrich four motivations of theatre-goers: social, intellectual, emotional, and spiritual engagement. We ran a context-mapping focus group session with professionals (N=6) to conceptualize the social VR space for digital opera experiences. Based on our findings, we propose a social VR lobby consisting of four rooms: 1) a Bar for social engagement, 2) an Info Booth for intellectual engagement, 3) a Photo Zone for emotional engagement, and 4) an Interactive Stage for spiritual engagement. Based on this work, we plan to experimentally evaluate audience experiences in each room in order to create a social VR lobby template for theater experiences.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 83891
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 83931
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 83915
        }
      ]
    },
    {
      "id": 83991,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Grasping Temperature: Thermal Feedback in VR Robot Teleoperation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532969"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1032",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "This paper presents a proof-of-concept of a robotic teleoperation system, that provides the human operator a thermal sense in addition to the visual sense. With a sensor suite comprising a stereo camera, 360⁰ camera and long-wave infra-red camera, our demonstrator pushes the boundaries of virtual-reality situational awareness by bringing not only 3D visual content but also a 360⁰ thermal experience to the operator. The visual channel of our robotic teleoperation system is represented through a head-mounted-display and the thermal channel is displayed through directional heaters in the operator cockpit and a thermal glove. Initial tests showed that an operator successfully experienced a 360⁰ remote environment, correctly distinguished between and interacted with hot and cold objects, and could notice the presence of nearby people outside her direct field-of-view, based on their emitted heat.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83932
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 83846
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83847
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83920
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83930
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 83823
        }
      ]
    },
    {
      "id": 83992,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "Extended Reality Ulysses Demo",
      "isBreak": false,
      "importedId": "imx22c-1003",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84234,
        84235
      ],
      "eventIds": [],
      "abstract": "This demo paper proposes to exhibit the pilot episode of \\textit{XR Ulysses}, a creative project investigating the possibilities for live performance using three-dimensional volumetric video (VV) techniques via virtual reality (VR) technologies. \\textit{XR Ulysses} is part of a series of innovative performance experiments hybridizing theatre and extended reality (XR) technologies. Conference attendees are invited to don an HMD, embody the character of Stephen Dedalus, and engage Buck Mulligan in the famous opening scene of Joyce's book, situated on the top of the Martello Tower at Sandycove (Dublin). This scene enables individuals to experience a live-action re-enactment of James Joyce's \\textit{Ulysses} in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83883
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83949
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83829
        }
      ]
    },
    {
      "id": 83993,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "Video for Health (V4H) Platform.  A Secure Video Suite Platform for Online Care, Teleconsultation, Tele-orientation and Teleconsulting",
      "isBreak": false,
      "importedId": "imx22c-1006",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "One of the major problems faced by public health managers around the globe is the lack of specialized professionals in remote locations to meet the health demands of society. To propose a solution for these problems, the authors developed the Video for Health (V4H) Platform for Brazil's National Education and Research Network (RNP), an R&D agency from the Brazilian Ministry of Science, Technology and Innovation (MCTI), which will be the topic of this proposed demo. The V4H is a system that aims to reduce the distance between health professionals and the population that needs primary care. The original proposal of the V4H was developed in two teams: Working group phase 1 (WG1) and Working Group phase 2 (WG2) at the Federal University of Paraíba (UFPB). The first working group grant went to develop the design of the system and it was tested at the Telehealth Center and the Federal University of São Paulo (Unifesp), at the São Paulo Area Military Hospital (HMASP) and at the TeleDentistry project at the University of São Paulo's (USP) Dentistry School (FOUSP). In the second phase of the project we developed more complex systems such as blockchain for video preservation, billing, time control and accessibility for the teleconsultations. The platform was tested and integrated with University of São Paulo's Hearth Institute (InCOR). The WG2 was coordinated by Prof. Guido Lemos (UFPB) and Prof. Marco Antonio Gutierrez (InCOR). The V4H Platform supports synchronous and confidential video streaming, with a scalable architecture to simplify integration with telehealth, teleconsulting and Electronic Health Record (HRE) platforms. The V4H system allows the authentication of the participants of the transmission, as well as the recording, retrieval and preservation of the transmitted content, using signature technologies with digital certificates and blockchain to ensure that the content remains immutable and providing the integrity and authenticity of the persisted data. The main focus of the solution is to offer a synchronous video service for platforms that support the electronic health record, where the recorded and preserved contents can be attached together with the patient's data, serving as legal evidence of the healthcare provided. There is also the possibility that these contents can be used as a data source for teleconsulting, tele-diagnosis and preceptorship activities for health professionals from all areas, initially focusing on basic and primary health care, in locations that lack specialized health professionals. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83901
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "SP",
              "city": "São Paulo",
              "institution": "Federal University of São Paulo",
              "dsl": "Educational Design/TEDE"
            }
          ],
          "personId": 83902
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Paraíba Federal Institute of Technology",
              "dsl": "Graduate Studies in Information Technology"
            }
          ],
          "personId": 83858
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 83828
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83912
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83870
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "RJ",
              "city": "Rio de Janeiro",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 83827
        }
      ]
    },
    {
      "id": 83994,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "A Children-Created Virtual Learning Space Station",
      "isBreak": false,
      "importedId": "imx22c-1005",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "We present a virtual learning space station, a design space, which was co-designed by 63 school children from three continents, namely Namibia, Malaysia and Finland. The design space station is developed on the Ohyay platform for children by children to plan, interact and conduct co-design activities. In our hands-on demonstration, attendees can organise/join a design session and test the online collaboration and facilitation tools provided by the space station. Our demo contributes directly to the IMX 2022 theme of\r\n“Interactive Media Brings us Together”.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "ASSET"
            }
          ],
          "personId": 83880
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": ""
            }
          ],
          "personId": 83826
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83885
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 83888
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83851
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Teaching and Learning Unit"
            }
          ],
          "personId": 83877
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "DRAC"
            }
          ],
          "personId": 83849
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "University of Namibia",
              "dsl": ""
            }
          ],
          "personId": 83871
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83835
        }
      ]
    },
    {
      "id": 83995,
      "typeId": 12261,
      "durationOverride": 100,
      "title": "Augmenting Speech Agent with Gaze for Enhancing Interaction. By Drawing from human-human Interaction",
      "isBreak": false,
      "importedId": "imx22e-1015",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84017
      ],
      "eventIds": [],
      "abstract": "Speech technologies are increasing in popularity by offering new interaction modalities for users. Spoken interaction design centers around the use of a wake-word to initiate interaction and the transcription of the users' spoken instruction to complete the task. However, in human-to-human conversation, speech is initiated by and supplemented with a range of other modalities, such as gaze and gesture. My research focuses on the need to better understand how human-technology `conversations' can be improved by borrowing from human-human interaction. Therefore, Tama -- a gaze-activated smart speaker, was designed to explore the use of gaze in conversational interaction. Tama uses gaze to indicate attention and intent to interact on behalf of the user and as feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University ",
              "dsl": "Department of Computer and System Sciences"
            }
          ],
          "personId": 83832
        }
      ]
    },
    {
      "id": 83996,
      "typeId": 12266,
      "durationOverride": 25,
      "title": "Interactive Touch Kiosks Designed for the Elderly: A Compilation of Requisites Acknowledging Physical and Psycho-sociological Age-related Changes",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3505284.3532979"
        }
      },
      "isBreak": false,
      "importedId": "imx22b-1030",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "Aiming cognitive stimulation and physical exercise, interactive touch kiosks designed for the elderly seem to be promising options to promote active aging, ensuring e-health and well-being services. They need to be created and improved according to the elderly population's real needs; however, recommendations to develop these solutions are scattered in several guidelines and standards. In this study standards regarding physical and psycho- sociological age-related changes, such as vision, hearing, cognition, communication, gross and fine motor skills were gathered; physical and social factors were also considered. A total of 107 items were found, and the following categories were defined: Terminals, Interface, Content, and Other. The proposal can be used as: a) a list to guide the creation of services and systems; b) a grid to be color coded according to the level of problems found while usability evaluations are being conducted. This is a contribution to experts who can easily recognize the items that need to be improved in the services and systems, to better support the experience of the elderly user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83881
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83837
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83833
        }
      ]
    },
    {
      "id": 83997,
      "typeId": 12260,
      "durationOverride": 25,
      "title": "The Fushimi Inari Experience: An Interactive Volumetric Film",
      "isBreak": false,
      "importedId": "imx22c-1009",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84002,
        84003
      ],
      "eventIds": [],
      "abstract": "As we move further into a spatial future, the demand for content creation tools is immense. Social media, gaming platforms and e-commerce have been converging into interactive spaces that involve spatial representations of the world the viewer is occupying, including digital humans. Creating digital representations of humans, or holograms, can be achieved through volumetric capture technologies. This method of bringing people to virtual three-dimensional environments has been rapidly increasing in popularity, but still lacks a key element: interactivity. In this paper we describe our work on producing interactive volumetric video that responds to viewers’ actions in real-time. We present the Fushimi Inari project: a commercial use case pushing the boundaries of what can be achieved with volumetric video and describe how our spatial content creation tools allow for interactive films to be created. Our contributions include blending volumetric clips, skeletonizing captures and applying multi-bone retargeting. We also provide means to integrate this in game engines for real-time photorealistic and interactive stories to be enjoyed by any viewer. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83950
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83865
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83906
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83856
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83859
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83952
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83842
        }
      ]
    },
    {
      "id": 84009,
      "typeId": 12270,
      "durationOverride": 60,
      "title": "Designing and Scaling IMX Programs for Community-Level Interventions",
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 11848,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84012
      ],
      "eventIds": [],
      "abstract": "Emerging IMX technologies, such as virtual and mixed reality, have become dramatically more affordable and accessible in recent years.  Consumers can purchase a VR headset for a reasonable price at local retail stores and experience virtual worlds in a matter of minutes. Yet, integrating IMX technologies into everyday use at a community level remains a challenge, with many users questioning the utility of IMX technologies.  In this keynote, I will present recent projects that delivered IMX technologies into the hands of hundreds of families and organizations in local communities to help them make healthier and better informed choices; how these projects were designed to embed IMX into their daily routines; and how we overcame the constraints of places and spaces.  Critical next steps toward the ubiquitous use of emerging technologies will be discussed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "University of Georgia"
            }
          ],
          "personId": 84006
        }
      ]
    },
    {
      "id": 84010,
      "typeId": 12270,
      "durationOverride": 60,
      "title": "Teleporting remote experts with Distributed Reality",
      "isBreak": false,
      "importedId": "2",
      "source": "CSV",
      "trackId": 11848,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84014
      ],
      "eventIds": [],
      "abstract": "Virtual and mixed reality applications are popping up everywhere. Most of them focus on bringing people to a virtual space to communicate, collaborate or just have fun, what eliminates from the experience the real environment surrounding the participants. We present here our Distributed Reality technology, which also belongs to the category of immersive media (XR), and it is also used for communication and collaboration. In our case, however, there is a strong focus on the real environment around the peers, which is captured in real time and blended into a ÒReal RealityÓ experience for all of them. We will present a particular use case of this concept in the industrial area, in which a remote expert is first trained in a realistic (not virtual) immersive scenario and then he/she is virtually ÒteleportedÓ to the real scenario where the assistance is required.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "institution": "Nokia"
            }
          ],
          "personId": 84007
        }
      ]
    },
    {
      "id": 84011,
      "typeId": 12270,
      "durationOverride": 45,
      "title": "Empathic Computing: Delivering the Entire Metaverse",
      "isBreak": false,
      "importedId": "3",
      "source": "CSV",
      "trackId": 11848,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84013
      ],
      "eventIds": [],
      "abstract": "In this talk I will talk about the emerging research field of Empathic Computing and how this can be used to create systems that develop deeper understanding between people. Using AR and VR technology, combined with physiological sensing, people can understand what other people are seeing, hearing and feeling, creating a strong sense of empathy. Using this technology, the Metaverse will go beyond being a place where we see one another, to a place where we can be one another and create a deep emotional connection. In this way Empathic Computing helps develop the entire Metaverse.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "institution": "University of South Australia"
            },
            {
              "country": "New Zealand",
              "institution": "University of Auckland"
            }
          ],
          "personId": 84008
        }
      ]
    },
    {
      "id": 84057,
      "typeId": 12271,
      "durationOverride": 60,
      "title": "Diversity in the Metaverse",
      "isBreak": false,
      "importedId": "4",
      "source": "CSV",
      "trackId": 11849,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84063
      ],
      "eventIds": [],
      "abstract": "As in the previous year, we are planning a panel discussion that talks about the issue of diversity in the areas of interest of IMX attendees. Last year, we discussed ÒInclusive DesignÓ. We had three panelists with different backgrounds and from different parts of the world: Heike Winschiers-Theophilus (Professor, Namibia University of Science and Technology, Namibia), Kalika Bali (Principal Researcher, Microsoft, India), and Frederick van Amstel (Assistant Professor, Federal University of Technology Ð Paran‡, Brazil). \nThis year, we tackle the topic of diversity in the Metaverse. The Metaverse is a hot topic right now which has many people wondering both what it is, and more importantly, what it will look like in the future for immersive media experiences. As a unique space for social interaction, engagement and connection, itÕs important that we address the importance of representation and accessibility during its time of infancy. Our goal is to discuss not only the current scenario in virtual and augmented reality worlds, but also the consequences and challenges of building a diverse Metaverse by taking into account design, content, marketing, and the various barriers faced by different communities across the globe.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "South Africa",
              "institution": "-"
            }
          ],
          "personId": 84048
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "-"
            }
          ],
          "personId": 84049
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "institution": "-"
            }
          ],
          "personId": 84050
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Unity3D"
            }
          ],
          "personId": 84052
        }
      ]
    },
    {
      "id": 84058,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "LIQUE 2022 - Life Improvement in Quality by Ubiquitous Experiences",
      "isBreak": false,
      "importedId": "5",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84067
      ],
      "eventIds": [],
      "abstract": "As Interactive Media Experiences (IMX) become more and more present everywhere, with every uses, in a ubiquitous way there is a growing interest in IMX systems designed for life quality improvements in all possible aspects.\n\nThis is specially relevant nowadays that we all are facing the challenge to keep the comfort related to the technological development and at same time respect the earth equilibrium and ecosystemsÕ maintenance in all aspects and levels.\n\nThis workshop aims to bring together researchers and practitioners working on tools, services and applications enabling interactive experiences for quality of life improvement promoting the development of initiatives that aid on the global quest for new uses to promotes the diversity of interchanges. Interactive Media Experience (IMX) typically includes as many as possible users and digital developers inside the implementation technically. However, little is directed to the quality of life in all aspects that could affect such social/machine/environment interactions in current setting. At this moment, it is important to discuss the implications and possibilities related to them. This workshop proposed to be held together with ACM Interactive Media Experiences (IMX) 2022, aims to bring together practitioners and researchers to discuss the opportunities and challenges of Life Improvement in Quality by Ubiquitous Experiences.\n\nWorkshop Website: http://lique2022.midiacom.uff.br/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal Fluminense"
            }
          ],
          "personId": 84053
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal do Maranhao"
            }
          ],
          "personId": 84031
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal Fluminense"
            }
          ],
          "personId": 84034
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade de Sao Paulo"
            }
          ],
          "personId": 84036
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "Universidade de Lisboa"
            }
          ],
          "personId": 84038
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal do Maranhao"
            }
          ],
          "personId": 84040
        }
      ]
    },
    {
      "id": 84059,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "SensoryX 2022 - Multisensory Experiences",
      "isBreak": false,
      "importedId": "6",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84073
      ],
      "eventIds": [],
      "abstract": "Multimedia applications have primarily engaged two of the human senses Ð aural and visual. With recent advances in computational technology, however it is possible to develop multisensory applications across all senses. Important findings from psychological and neuroscience research, as well as technological advances responsible for increased diversity of devices with higher computational power and communication capabilities, augmented by various sensor and display technologies have enabled targeting other human senses. Moreover, the state of the art for multisensory systems has been pushed forward with the evolution of Mixed Reality-related technologies that allowed several senses to be stimulated at the same time, presenting users with Ôreal experiencesÕ designed in virtual worlds. This enables a more immersive, coherent, and credible experience, raising the level of presence. In this context, it becomes important to explore and understand how one can design effective multisensory experiences in a variety of domains (e.g., education, arts, entertainment, etc.) and how technology can be used to meaningfully stimulate these experiences. Moreover, it is essential to identify the challenges, opportunities and limitations to be overcome in the quest to transcend the overwhelmingly bisensorial nature of digital multimedia into a multisensory one Ð the realm of mulsemedia: multiple sensorial media. The aim of this workshop is to deepen the discussion on multisensory experience design as well as to inspire and support participants in designing multisensory interfaces and experiences.\n\nThis workshop will focus on enhancing the multisensory scope of both designers and developers of multimedia and Mixed Reality experiences who could thus harness the whole spectrum of sensory experiences. For this, the workshop will challenge current practices in designing experiences, will explore meaningful design spaces and will identify future research directions for creating experiences at the intersection of various sensory dimensions.\n\nWorkshop Website: http://sensoryx.midiacom.uff.br/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of Kent"
            }
          ],
          "personId": 84042
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Instituto Federal do Espirito Santo"
            }
          ],
          "personId": 84043
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Paris 8 University"
            }
          ],
          "personId": 84044
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Cefet/RJ"
            }
          ],
          "personId": 84045
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "Brunel University London"
            }
          ],
          "personId": 84046
        }
      ]
    },
    {
      "id": 84060,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "Emotion IMX2022 - Considering Emotions in Multimedia Experience",
      "isBreak": false,
      "importedId": "7",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84066
      ],
      "eventIds": [],
      "abstract": "Emotions are fundamental to human experience, as they impact cognition, perception, and daily tasks (e.g., communication). The Emotion IMX workshop aims to bring together researchers and industries from various fields (including, but not limited to, computer science, design, and cognitive science) to discuss challenges in considering affects and emotions for interactive and media experiences (i.e., to assess and/or improve these experiences) from an interdisciplinary perspective. More specifically, we propose to address the following questions:\nWhy study emotions? For which applications?\nHow to categorise, describe, analyse, quantify, represent emotions?\nHow to take into account emotions in the design and evaluation phases of multimedia experiences?\nWhat are the recent advances in automated emotion recognition? What are the challenges in their evaluation?\nWhat are the different ways of dealing with emotions in interactive media experiences?\nWhat are the existing and future challenges? What are the limitations?\n\nWorkshop Website: https://emotionimx.ls2n.fr/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84055
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84056
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84018
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Ecole de design Nantes Atlantique"
            }
          ],
          "personId": 84019
        }
      ]
    },
    {
      "id": 84061,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "Performances IMX2022 - Designing live performances of the future",
      "isBreak": false,
      "importedId": "8",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84070
      ],
      "eventIds": [],
      "abstract": "For thousands of years, live performances have been an essential part of human culture. Whether it is music, dance, theatre, opera, spoken word poetry or puppetry, live performances have always been a popular source of entertainment for people of all ages and backgrounds, However, nowadays live performances are at a very pivotal and uncertain moment of their story. Due to the ongoing COVID-19 pandemic, artists and audiences alike are fearful that they may not be able to enjoy performances in the same ways they used to. Despite this, the pandemic also demonstrated to artists and audiences everywhere, that there are new alternative ways to experience performances. Artists started to experiment with streaming performances as a way to bridge the gap between physical events and online participation. Even artists who didnÕt use technology before as part of their work, are now leveraging its ability to let them connect with more people, and reach wider audiences. Moreover, recently, technologies that used to be only available for a niche market (such as AI, VR, AR) are now becoming more accessible and user friendly. Developing for and with such technologies is also becoming easier and more accessible for non-technology experts. This creates an opportunity for exploration as to what these types of technologies can do for the future of performance. This workshop aims to start a conversation as to what the performances of the future might look like by inviting both researchers and practitioners to present their work in this domain. It will explore the interception of technology and performance, with a focus on how technology might be leveraged to enhance performances and the process of developing performances, provide new ways to reach and interact with audiences, as well as how it can create new ways of performing.\n\nWorkshop Website: http://performancesofthefuture.live/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London and University of Kent"
            }
          ],
          "personId": 84020
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London"
            }
          ],
          "personId": 84022
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of Kent"
            }
          ],
          "personId": 84023
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "Coventry University"
            }
          ],
          "personId": 84024
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London"
            }
          ],
          "personId": 84025
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "VOLTA"
            }
          ],
          "personId": 84026
        }
      ]
    },
    {
      "id": 84062,
      "typeId": 12267,
      "durationOverride": 90,
      "title": "XRWALC 2022 - 1st Int. Workshop on Analytics, Learning & Collaboration in eXtended Reality",
      "isBreak": false,
      "importedId": "9",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84075
      ],
      "eventIds": [],
      "abstract": "XR, extended or cross reality, technology is gaining relevance in many professional domains, and is used and experimented with for the sake of manifold purposes. The XRWALC workshop at ACM IMX 2022 is aiming to explore the latest advancements with a focus on three particular application aspects:\n- Remote analytics: inspect and interact with remote systems, explore and analyze data within immersive XR environments.\n- Immersive learning: exploit the advantages of safe and affordable learning environments that allow to simulate, try out, experience, learn and train.\n- Collaboration in XR: approaches to design and enable group collaboration.\nTo apply for participation, please submit extended abstracts that can be short summaries of research advancements or studies, novel concepts and approaches, descriptions of demonstrations, or descriptions of practical problems and research questions which the authors seek to discuss.\n\nWorkshop Website: http://xrwalc2022.di.fc.ul.pt/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Know-Center"
            }
          ],
          "personId": 84029
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "BT Research Labs in Adastral Park"
            }
          ],
          "personId": 84030
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Dodds Consulting"
            }
          ],
          "personId": 84032
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84035
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "LASIGE, Faculdade de Ciencias, Universidade de Lisboa"
            }
          ],
          "personId": 84037
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84039
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Institute for Immersive Designs, Experiences, Applications and Stories at American University in Washington DC"
            }
          ],
          "personId": 84041
        }
      ]
    },
    {
      "id": 84086,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "Augmenting Speech Agent with Gaze for Enhancing Interaction. By Drawing from human-human Interaction",
      "isBreak": false,
      "importedId": "d9885d62-6fbd-4567-98ca-6925c8adb1af",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "Speech technologies are increasing in popularity by offering new interaction modalities for users. Spoken interaction design centers around the use of a wake-word to initiate interaction and the transcription of the users' spoken instruction to complete the task. However, in human-to-human conversation, speech is initiated by and supplemented with a range of other modalities, such as gaze and gesture. My research focuses on the need to better understand how human-technology `conversations' can be improved by borrowing from human-human interaction. Therefore, Tama -- a gaze-activated smart speaker, was designed to explore the use of gaze in conversational interaction. Tama uses gaze to indicate attention and intent to interact on behalf of the user and as feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University ",
              "dsl": "Department of Computer and System Sciences"
            }
          ],
          "personId": 83832
        }
      ]
    },
    {
      "id": 84087,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "The Promotion of Empathy in Intelligent Assistants for iTV through Proactive Behaviours",
      "isBreak": false,
      "importedId": "42fa4a60-c9c9-4cd5-aaca-b2afcd49d48c",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "The integration of intelligent assistants, in devices belonging to the television ecosystem, has simplified the accomplishment of more demanding tasks (such as content searches). However, it can be seen that these assistants are restricted to purely reactive behaviours and show a reduced human and empathic dimension in relation to the users. However, in other application domains, there has been an increasing integration of proactive behaviours, which can counteract these barriers and, consequently, improve the respective User Experience (UX). It is precisely in this context of proactivity that this research is designed. The goal is to contribute to the advancement of intelligent assistants in the interactive TV (iTV) domain, studying which proactive behaviours can be integrated in an intelligent assistant for iTV to promote its empathy, the associated UX and, consequently, its adoption in a more fluid and massive way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia"
            }
          ],
          "personId": 83953
        }
      ]
    },
    {
      "id": 84088,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "Enabling User-centric Assessment and Modelling of Immersiveness in Multimodal Multimedia Applications",
      "isBreak": false,
      "importedId": "e83a3cee-a242-4e4e-b389-06250b748ff9",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "Multimodal, immersive systems are the latest development within the field of multimedia. These systems emulate the senses by means of omnidirectional visuals, 360° sound, motion tracking and touch simulation to create a feeling of presence in the virtual environment. They have the potential to substitute physical interactions in application domains such as training (Industry 4.0), or e-health (tele-surgery). However, the COVID-19 pandemic has shown that they are not ready, as they still have room for improvement in terms network streaming quality, usability and the users’ feeling of presence. In addition, these systems can induce feelings of dizziness, nausea etc. (i.e. cybersickness). These factors therefore have an important impact on the user’s total immersion. In this work, we therefore propose that immersiveness can be devised from measuring four aspects, namely: presence (i.e. the feeling of being \"in\" the environment), cybersickness, network related Quality-of-Experience (QoE) and the usability of the application. Therefore, a two-dimensional user-centric approach on the assessment and modelling of immersiveness is proposed. These dimensions include (i) subjective and objective assessment of presence, cybersickness, usability and QoE and (ii) real-time modelling of immersiveness. Furthermore, a proof-of-concept is envisioned including two use cases. As such, we believe that this position paper will significantly advance the state of the art on immersive systems and multimedia in general.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Oost-Vlaanderen",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            }
          ],
          "personId": 83879
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": ""
            }
          ],
          "personId": 83943
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 83941
        }
      ]
    },
    {
      "id": 84089,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "Human-Computer Interaction Patterns for Head-Mounted-Device-based Augmented Reality in the Exhibition Domain",
      "isBreak": false,
      "importedId": "44034260-7a07-476b-96b6-373f75726263",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "Head-mounted-device (HMD) based augmented reality (AR) applications bring new opportunities to the exhibition domain. However, first-time users such as exhibition visitors are not familiar with the interaction method of the HMD, which also brings challenges to exhibition authors while implementing HMD based AR technologies. This research project focuses on the accessibility of the HMD based AR application in the exhibition domain, which explores potential interaction patterns based on the technical feature of the HMD for exhibition-related use cases. Both information system research method and design research method are applied while exploring interaction solutions for human factor challenges. Several prototypes will be created and iteratively tested and evaluated at the exhibition for generalizing effective and accessible interaction patterns. As the result, it will provide a pattern-based interaction system as an artifact with design knowledge to developers so that they can convert the interaction pattern into an authoring tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wiesbaden",
              "institution": "RheinMain University of Applied Sciences",
              "dsl": "Design, Computer Science, Media"
            }
          ],
          "personId": 83866
        }
      ]
    },
    {
      "id": 84090,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "Modeling Cognitive Load and Affect in Interactive Game-based Learning Using Physiological Features",
      "isBreak": false,
      "importedId": "b39c94b9-a024-4542-9683-cebc828ce5a5",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "Media use in educational environments has been rapidly developing with the increasing availability and diversity of interactive elements. By understanding how student cognitive load changes when interacting with learning technologies, we can make sense of their learning process and how to provide appropriate, personalized media design to enhance the learning experience. Recent developments in sensing technologies makes it possible to capture learner’s dynamic physiological reactions. In this thesis research, we will identify learner’s cognitive load when interacting with educational media. We will explore how affective reactions contribute to the modeling of cognitive load, and how real-time cognitive load changes alongside learning activities. We focus on modeling such information using physiological reactions that include pupillary, cardiovascular, and electrodermal responses. We are conducting this work in a game-based learning (GBL) environment for reading comprehension. We have implemented a sensing pipeline that will enable the modelling of learner affect and cognitive load. The modeling and analysis of this project can further support the design of interactive learning media that provides real-time adaptation in learning processes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83886
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Dept. of Computing Science"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Language Sciences"
            }
          ],
          "personId": 83910
        }
      ]
    },
    {
      "id": 84091,
      "typeId": 12261,
      "durationOverride": 120,
      "title": "Factors influencing video Quality of Experience measured with ecologically valid methods",
      "isBreak": false,
      "importedId": "e62e09c5-32b8-4381-a8dc-55976b3c1a50",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84016
      ],
      "eventIds": [],
      "abstract": "Over the last decade, user subjective perception became the subject of interest in video quality studies. Researchers conclude about this Quality of Experience (QoE) based on participants’ statements, behaviors, and psychophysiological reactions to distorted videos. The reason for that is the fact that direct, objective evaluation of QoE is impossible due to its subjective nature. Thus, clear operationalization of variables in QoE studies is crucial. For that purpose theoretical background is necessary. Current theoretical models of QoE consist of many strongly correlated variables and omit the role of content. In effect, most QoE studies related to compression use strict laboratory experiments with Absolute Category Ratings. The ecological validity of such studies is limited. In my Ph.D., I investigate factors influencing QoE in a more natural context. To be able to conclude about those multiple, complex variables I am working on a content-based video QoE model inspirited by Structural Causal Models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Department of Information and Communication Technologies"
            }
          ],
          "personId": 83919
        }
      ]
    },
    {
      "id": 84092,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "Augmenting Speech Agent with Gaze for Enhancing Interaction. By Drawing from human-human Interaction",
      "isBreak": false,
      "importedId": "080600fe-1709-4191-978f-d3a76e81de45",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "Speech technologies are increasing in popularity by offering new interaction modalities for users. Spoken interaction design centers around the use of a wake-word to initiate interaction and the transcription of the users' spoken instruction to complete the task. However, in human-to-human conversation, speech is initiated by and supplemented with a range of other modalities, such as gaze and gesture. My research focuses on the need to better understand how human-technology `conversations' can be improved by borrowing from human-human interaction. Therefore, Tama -- a gaze-activated smart speaker, was designed to explore the use of gaze in conversational interaction. Tama uses gaze to indicate attention and intent to interact on behalf of the user and as feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University ",
              "dsl": "Department of Computer and System Sciences"
            }
          ],
          "personId": 83832
        }
      ]
    },
    {
      "id": 84093,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "The Promotion of Empathy in Intelligent Assistants for iTV through Proactive Behaviours",
      "isBreak": false,
      "importedId": "4051cd3f-090e-49c1-aed5-8921c792a151",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "The integration of intelligent assistants, in devices belonging to the television ecosystem, has simplified the accomplishment of more demanding tasks (such as content searches). However, it can be seen that these assistants are restricted to purely reactive behaviours and show a reduced human and empathic dimension in relation to the users. However, in other application domains, there has been an increasing integration of proactive behaviours, which can counteract these barriers and, consequently, improve the respective User Experience (UX). It is precisely in this context of proactivity that this research is designed. The goal is to contribute to the advancement of intelligent assistants in the interactive TV (iTV) domain, studying which proactive behaviours can be integrated in an intelligent assistant for iTV to promote its empathy, the associated UX and, consequently, its adoption in a more fluid and massive way.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "DigiMedia"
            }
          ],
          "personId": 83953
        }
      ]
    },
    {
      "id": 84094,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "Enabling User-centric Assessment and Modelling of Immersiveness in Multimodal Multimedia Applications",
      "isBreak": false,
      "importedId": "0a3eba6e-084d-443f-9cfe-72d7bb491e60",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "Multimodal, immersive systems are the latest development within the field of multimedia. These systems emulate the senses by means of omnidirectional visuals, 360° sound, motion tracking and touch simulation to create a feeling of presence in the virtual environment. They have the potential to substitute physical interactions in application domains such as training (Industry 4.0), or e-health (tele-surgery). However, the COVID-19 pandemic has shown that they are not ready, as they still have room for improvement in terms network streaming quality, usability and the users’ feeling of presence. In addition, these systems can induce feelings of dizziness, nausea etc. (i.e. cybersickness). These factors therefore have an important impact on the user’s total immersion. In this work, we therefore propose that immersiveness can be devised from measuring four aspects, namely: presence (i.e. the feeling of being \"in\" the environment), cybersickness, network related Quality-of-Experience (QoE) and the usability of the application. Therefore, a two-dimensional user-centric approach on the assessment and modelling of immersiveness is proposed. These dimensions include (i) subjective and objective assessment of presence, cybersickness, usability and QoE and (ii) real-time modelling of immersiveness. Furthermore, a proof-of-concept is envisioned including two use cases. As such, we believe that this position paper will significantly advance the state of the art on immersive systems and multimedia in general.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Oost-Vlaanderen",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": "IDLab"
            }
          ],
          "personId": 83879
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University",
              "dsl": ""
            }
          ],
          "personId": 83943
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 83941
        }
      ]
    },
    {
      "id": 84095,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "Human-Computer Interaction Patterns for Head-Mounted-Device-based Augmented Reality in the Exhibition Domain",
      "isBreak": false,
      "importedId": "eb302222-968b-456f-bd19-9445dee9f382",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "Head-mounted-device (HMD) based augmented reality (AR) applications bring new opportunities to the exhibition domain. However, first-time users such as exhibition visitors are not familiar with the interaction method of the HMD, which also brings challenges to exhibition authors while implementing HMD based AR technologies. This research project focuses on the accessibility of the HMD based AR application in the exhibition domain, which explores potential interaction patterns based on the technical feature of the HMD for exhibition-related use cases. Both information system research method and design research method are applied while exploring interaction solutions for human factor challenges. Several prototypes will be created and iteratively tested and evaluated at the exhibition for generalizing effective and accessible interaction patterns. As the result, it will provide a pattern-based interaction system as an artifact with design knowledge to developers so that they can convert the interaction pattern into an authoring tool.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Wiesbaden",
              "institution": "RheinMain University of Applied Sciences",
              "dsl": "Design, Computer Science, Media"
            }
          ],
          "personId": 83866
        }
      ]
    },
    {
      "id": 84096,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "Modeling Cognitive Load and Affect in Interactive Game-based Learning Using Physiological Features",
      "isBreak": false,
      "importedId": "384ac3f3-f822-4768-ac06-797e6d25e553",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "Media use in educational environments has been rapidly developing with the increasing availability and diversity of interactive elements. By understanding how student cognitive load changes when interacting with learning technologies, we can make sense of their learning process and how to provide appropriate, personalized media design to enhance the learning experience. Recent developments in sensing technologies makes it possible to capture learner’s dynamic physiological reactions. In this thesis research, we will identify learner’s cognitive load when interacting with educational media. We will explore how affective reactions contribute to the modeling of cognitive load, and how real-time cognitive load changes alongside learning activities. We focus on modeling such information using physiological reactions that include pupillary, cardiovascular, and electrodermal responses. We are conducting this work in a game-based learning (GBL) environment for reading comprehension. We have implemented a sensing pipeline that will enable the modelling of learner affect and cognitive load. The modeling and analysis of this project can further support the design of interactive learning media that provides real-time adaptation in learning processes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83886
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": "Dept. of Computing Science"
            },
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Language Sciences"
            }
          ],
          "personId": 83910
        }
      ]
    },
    {
      "id": 84097,
      "typeId": 12261,
      "durationOverride": 2,
      "title": "Factors influencing video Quality of Experience measured with ecologically valid methods",
      "isBreak": false,
      "importedId": "6a66ec38-451d-4d0a-9e81-50f0bf0d3d5b",
      "source": "PCS",
      "trackId": 11844,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84004
      ],
      "eventIds": [],
      "abstract": "Over the last decade, user subjective perception became the subject of interest in video quality studies. Researchers conclude about this Quality of Experience (QoE) based on participants’ statements, behaviors, and psychophysiological reactions to distorted videos. The reason for that is the fact that direct, objective evaluation of QoE is impossible due to its subjective nature. Thus, clear operationalization of variables in QoE studies is crucial. For that purpose theoretical background is necessary. Current theoretical models of QoE consist of many strongly correlated variables and omit the role of content. In effect, most QoE studies related to compression use strict laboratory experiments with Absolute Category Ratings. The ecological validity of such studies is limited. In my Ph.D., I investigate factors influencing QoE in a more natural context. To be able to conclude about those multiple, complex variables I am working on a content-based video QoE model inspirited by Structural Causal Models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Poland",
              "state": "",
              "city": "Kraków",
              "institution": "AGH University of Science and Technology",
              "dsl": "Department of Information and Communication Technologies"
            }
          ],
          "personId": 83919
        }
      ]
    },
    {
      "id": 84098,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "Video for Health (V4H) Platform.  A Secure Video Suite Platform for Online Care, Teleconsultation, Tele-orientation and Teleconsulting",
      "isBreak": false,
      "importedId": "6ccaa395-8869-44e1-8da0-f0d2bb7977c1",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "One of the major problems faced by public health managers around the globe is the lack of specialized professionals in remote locations to meet the health demands of society. To propose a solution for these problems, the authors developed the Video for Health (V4H) Platform for Brazil's National Education and Research Network (RNP), an R&D agency from the Brazilian Ministry of Science, Technology and Innovation (MCTI), which will be the topic of this proposed demo. The V4H is a system that aims to reduce the distance between health professionals and the population that needs primary care. The original proposal of the V4H was developed in two teams: Working group phase 1 (WG1) and Working Group phase 2 (WG2) at the Federal University of Paraíba (UFPB). The first working group grant went to develop the design of the system and it was tested at the Telehealth Center and the Federal University of São Paulo (Unifesp), at the São Paulo Area Military Hospital (HMASP) and at the TeleDentistry project at the University of São Paulo's (USP) Dentistry School (FOUSP). In the second phase of the project we developed more complex systems such as blockchain for video preservation, billing, time control and accessibility for the teleconsultations. The platform was tested and integrated with University of São Paulo's Hearth Institute (InCOR). The WG2 was coordinated by Prof. Guido Lemos (UFPB) and Prof. Marco Antonio Gutierrez (InCOR). The V4H Platform supports synchronous and confidential video streaming, with a scalable architecture to simplify integration with telehealth, teleconsulting and Electronic Health Record (HRE) platforms. The V4H system allows the authentication of the participants of the transmission, as well as the recording, retrieval and preservation of the transmitted content, using signature technologies with digital certificates and blockchain to ensure that the content remains immutable and providing the integrity and authenticity of the persisted data. The main focus of the solution is to offer a synchronous video service for platforms that support the electronic health record, where the recorded and preserved contents can be attached together with the patient's data, serving as legal evidence of the healthcare provided. There is also the possibility that these contents can be used as a data source for teleconsulting, tele-diagnosis and preceptorship activities for health professionals from all areas, initially focusing on basic and primary health care, in locations that lack specialized health professionals. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83901
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "SP",
              "city": "São Paulo",
              "institution": "Federal University of São Paulo",
              "dsl": "Educational Design/TEDE"
            }
          ],
          "personId": 83902
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Paraíba Federal Institute of Technology",
              "dsl": "Graduate Studies in Information Technology"
            }
          ],
          "personId": 83858
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 83828
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83912
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "João Pessoa",
              "institution": "Federal University of Paraíba (UFPB)",
              "dsl": "LAVID"
            }
          ],
          "personId": 83870
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "RJ",
              "city": "Rio de Janeiro",
              "institution": "National Education and Research Network (RNP)",
              "dsl": "GT-V4H"
            }
          ],
          "personId": 83827
        }
      ]
    },
    {
      "id": 84099,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "Extended Reality Ulysses Demo",
      "isBreak": false,
      "importedId": "db932ac1-bfac-4530-8665-097e51dc9308",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "This demo paper proposes to exhibit the pilot episode of \\textit{XR Ulysses}, a creative project investigating the possibilities for live performance using three-dimensional volumetric video (VV) techniques via virtual reality (VR) technologies. \\textit{XR Ulysses} is part of a series of innovative performance experiments hybridizing theatre and extended reality (XR) technologies. Conference attendees are invited to don an HMD, embody the character of Stephen Dedalus, and engage Buck Mulligan in the famous opening scene of Joyce's book, situated on the top of the Martello Tower at Sandycove (Dublin). This scene enables individuals to experience a live-action re-enactment of James Joyce's \\textit{Ulysses} in VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83883
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Co. Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83949
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Dublin",
              "city": "Dublin",
              "institution": "Trinity College Dublin",
              "dsl": "V-SENSE"
            }
          ],
          "personId": 83829
        }
      ]
    },
    {
      "id": 84100,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "The Fushimi Inari Experience: An Interactive Volumetric Film",
      "isBreak": false,
      "importedId": "e382a68b-23cc-4aeb-b736-489328c9c89f",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "As we move further into a spatial future, the demand for content creation tools is immense. Social media, gaming platforms and e-commerce have been converging into interactive spaces that involve spatial representations of the world the viewer is occupying, including digital humans. Creating digital representations of humans, or holograms, can be achieved through volumetric capture technologies. This method of bringing people to virtual three-dimensional environments has been rapidly increasing in popularity, but still lacks a key element: interactivity. In this paper we describe our work on producing interactive volumetric video that responds to viewers’ actions in real-time. We present the Fushimi Inari project: a commercial use case pushing the boundaries of what can be achieved with volumetric video and describe how our spatial content creation tools allow for interactive films to be created. Our contributions include blending volumetric clips, skeletonizing captures and applying multi-bone retargeting. We also provide means to integrate this in game engines for real-time photorealistic and interactive stories to be enjoyed by any viewer. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83950
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83865
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83906
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83856
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83859
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83952
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Arcturus Studio",
              "dsl": ""
            }
          ],
          "personId": 83842
        }
      ]
    },
    {
      "id": 84101,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "Remote Presence: Live Holograms for a Social Classroom",
      "isBreak": false,
      "importedId": "a35afc5f-2e7f-4158-9770-ed6d8873338c",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "Existing communication technologies have displayed a lack of affordances in supporting social-emotional connections, which is of particular interest in educational settings. We are therefore developing a live sensory immersive 3D video technology, built on a prior developed platform. Pilot trials in a Finnish school have yielded promising findings. We continue to advance the state-of-the-art platform in parallel with regards to 3D capture quality and data compression algorithms. Current developments entail joint investigations and evaluations of affordances to support emotional, social, motivational, and achievement impacts with learners and teachers from a Namibian and Finnish school. Participants can experience \"remote presence\" wearing the hololens 2 while others are live streamed from another country captured by two cameras.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83844
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83864
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83851
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 83888
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83938
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": "Interaction Design Lab"
            }
          ],
          "personId": 83831
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Computer Science"
            }
          ],
          "personId": 83835
        }
      ]
    },
    {
      "id": 84102,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "A Children-Created Virtual Learning Space Station",
      "isBreak": false,
      "importedId": "90f7d4c9-612c-4836-b5ac-c18c7b0b2389",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "We present a virtual learning space station, a design space, which was co-designed by 63 school children from three continents, namely Namibia, Malaysia and Finland. The design space station is developed on the Ohyay platform for children by children to plan, interact and conduct co-design activities. In our hands-on demonstration, attendees can organise/join a design session and test the online collaboration and facilitation tools provided by the space station. Our demo contributes directly to the IMX 2022 theme of\r\n“Interactive Media Brings us Together”.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "ASSET"
            }
          ],
          "personId": 83880
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "Sarawak",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": ""
            }
          ],
          "personId": 83826
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83885
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Oldenburg",
              "institution": "Carl von Ossietzky Universität Oldenburg",
              "dsl": ""
            }
          ],
          "personId": 83888
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Turku",
              "institution": "University of Turku",
              "dsl": ""
            }
          ],
          "personId": 83851
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": "Teaching and Learning Unit"
            }
          ],
          "personId": 83877
        },
        {
          "affiliations": [
            {
              "country": "Malaysia",
              "state": "",
              "city": "Sibu",
              "institution": "University of Technology Sarawak",
              "dsl": "DRAC"
            }
          ],
          "personId": 83849
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "University of Namibia",
              "dsl": ""
            }
          ],
          "personId": 83871
        },
        {
          "affiliations": [
            {
              "country": "Namibia",
              "state": "",
              "city": "Windhoek",
              "institution": "Namibia University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 83835
        }
      ]
    },
    {
      "id": 84103,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "The Co-Creation Space: An Online Safe Space for Community Opera Creation",
      "isBreak": false,
      "importedId": "ca50c268-ee1e-4798-b807-a404de78032a",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "This work presents the Co-Creation Space, a multilingual platform for professional and community artists to 1) generate raw artistic ideas, and 2) discuss and reflect on the shared meaning of those ideas. The paper describes the architecture and the technology behind the platform, and how it was used to facilitate the communication process during several user trials. By supporting ideation sessions around media items guided by a facilitator and allowing users to express themselves and be part of the creation of an artistic product, participants were enabled to access new cultural spaces and be part of the creative process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica",
              "dsl": ""
            }
          ],
          "personId": 83948
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "Netherlands",
              "city": "Amsterdam",
              "institution": "CWI ",
              "dsl": "DIS "
            }
          ],
          "personId": 83931
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 83935
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 83915
        }
      ]
    },
    {
      "id": 84104,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "Sunflower: An Interactive Artistic Environment based on IoMusT Concepts",
      "isBreak": false,
      "importedId": "acba3980-72ee-493b-a5d5-e5f097ed7525",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "The Internet of Musical Things (IoMusT) is an interdisciplinary area that aims to improve the relationship between musicians and their peers, as well as between musicians and audience members, creating new forms of interaction in concerts, studio productions, and music learning. \r\n  Although emerging, this field already faces some challenges, such as lack of privacy and security, and mainly, lack of standardization and interoperability between its devices. Therefore, an environment design, called Sunflower, was proposed, which tries to contribute to solving the most recurrent problems in this area, specifying an architecture pattern, protocol, and sound features that aim to allow heterogeneity in these systems. \r\n  Its practical implementation resulted in an interoperable, multimedia, and interactive environment. This paper, therefore, shows a demonstration of how Sunflower works in the accomplishment of an artistic presentation, also emphasizing its approach, the technologies that support it, and the advances it can bring to the area of IoMusT.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "Niterói",
              "institution": "Fluminense Federal University",
              "dsl": "MidiaCom Lab"
            }
          ],
          "personId": 83841
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Rio de Janeiro",
              "city": "Niterói",
              "institution": "Fluminense Federal University - UFF",
              "dsl": "Computer Science"
            }
          ],
          "personId": 83860
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "Minas Gerais",
              "city": "São João del-Rei",
              "institution": "Federal University of São João del-Rei",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 83914
        }
      ]
    },
    {
      "id": 84105,
      "typeId": 12260,
      "durationOverride": 1,
      "title": "ScenaProd: creating interactive medias without programming",
      "isBreak": false,
      "importedId": "13af4d87-1bec-4a6c-aa37-f59c316db524",
      "source": "PCS",
      "trackId": 11842,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84000
      ],
      "eventIds": [],
      "abstract": "This demonstration aims at presenting ScenaProd, a tool that allows people to produce multisensory scenagrams (multisensory exercises or interactive media). All participants can create their own scenagrams with that tool or test a more complete one that is already created. A scenagram can be defined as an interaction between a human being and different devices. For example, a robot asks a question while displaying a visual clue on a screen. Then, the participant can respond by pressing a large and colored contactor. In case of a correct answer, the robot would play a short victory song and a light would light up green. If a wrong answer were given, the system would have a different reaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83868
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83944
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Vannes",
              "institution": "South Brittany University",
              "dsl": ""
            }
          ],
          "personId": 83956
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Denis",
              "institution": "Paris 8 University",
              "dsl": ""
            }
          ],
          "personId": 83894
        }
      ]
    },
    {
      "id": 84106,
      "typeId": 12267,
      "durationOverride": 100,
      "title": "LIQUE 2022 - Life Improvement in Quality by Ubiquitous Experiences",
      "isBreak": false,
      "importedId": "f1f98cd4-b205-4e98-9978-9a48a8a79630",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84068
      ],
      "eventIds": [],
      "abstract": "As Interactive Media Experiences (IMX) become more and more present everywhere, with every uses, in a ubiquitous way there is a growing interest in IMX systems designed for life quality improvements in all possible aspects.\n\nThis is specially relevant nowadays that we all are facing the challenge to keep the comfort related to the technological development and at same time respect the earth equilibrium and ecosystemsÕ maintenance in all aspects and levels.\n\nThis workshop aims to bring together researchers and practitioners working on tools, services and applications enabling interactive experiences for quality of life improvement promoting the development of initiatives that aid on the global quest for new uses to promotes the diversity of interchanges. Interactive Media Experience (IMX) typically includes as many as possible users and digital developers inside the implementation technically. However, little is directed to the quality of life in all aspects that could affect such social/machine/environment interactions in current setting. At this moment, it is important to discuss the implications and possibilities related to them. This workshop proposed to be held together with ACM Interactive Media Experiences (IMX) 2022, aims to bring together practitioners and researchers to discuss the opportunities and challenges of Life Improvement in Quality by Ubiquitous Experiences.\n\nWorkshop Website: http://lique2022.midiacom.uff.br/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal Fluminense"
            }
          ],
          "personId": 84053
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal do Maranhao"
            }
          ],
          "personId": 84031
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal Fluminense"
            }
          ],
          "personId": 84034
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade de Sao Paulo"
            }
          ],
          "personId": 84036
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "Universidade de Lisboa"
            }
          ],
          "personId": 84038
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Universidade Federal do Maranhao"
            }
          ],
          "personId": 84040
        }
      ]
    },
    {
      "id": 84107,
      "typeId": 12267,
      "durationOverride": 100,
      "title": "Performances IMX2022 - Designing live performances of the future",
      "isBreak": false,
      "importedId": "c6e59d3f-fe1d-46f3-9062-50d4a3ef889d",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84072
      ],
      "eventIds": [],
      "abstract": "For thousands of years, live performances have been an essential part of human culture. Whether it is music, dance, theatre, opera, spoken word poetry or puppetry, live performances have always been a popular source of entertainment for people of all ages and backgrounds, However, nowadays live performances are at a very pivotal and uncertain moment of their story. Due to the ongoing COVID-19 pandemic, artists and audiences alike are fearful that they may not be able to enjoy performances in the same ways they used to. Despite this, the pandemic also demonstrated to artists and audiences everywhere, that there are new alternative ways to experience performances. Artists started to experiment with streaming performances as a way to bridge the gap between physical events and online participation. Even artists who didnÕt use technology before as part of their work, are now leveraging its ability to let them connect with more people, and reach wider audiences. Moreover, recently, technologies that used to be only available for a niche market (such as AI, VR, AR) are now becoming more accessible and user friendly. Developing for and with such technologies is also becoming easier and more accessible for non-technology experts. This creates an opportunity for exploration as to what these types of technologies can do for the future of performance. This workshop aims to start a conversation as to what the performances of the future might look like by inviting both researchers and practitioners to present their work in this domain. It will explore the interception of technology and performance, with a focus on how technology might be leveraged to enhance performances and the process of developing performances, provide new ways to reach and interact with audiences, as well as how it can create new ways of performing.\n\nWorkshop Website: http://performancesofthefuture.live/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London and University of Kent"
            }
          ],
          "personId": 84020
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London"
            }
          ],
          "personId": 84022
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of Kent"
            }
          ],
          "personId": 84023
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "Coventry University"
            }
          ],
          "personId": 84024
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of the Arts London"
            }
          ],
          "personId": 84025
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "VOLTA"
            }
          ],
          "personId": 84026
        }
      ]
    },
    {
      "id": 84108,
      "typeId": 12267,
      "durationOverride": 100,
      "title": "Emotion IMX2022 - Considering Emotions in Multimedia Experience",
      "isBreak": false,
      "importedId": "03b584f4-57d7-4a6f-b2ea-8c5788600d82",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84071
      ],
      "eventIds": [],
      "abstract": "Emotions are fundamental to human experience, as they impact cognition, perception, and daily tasks (e.g., communication). The Emotion IMX workshop aims to bring together researchers and industries from various fields (including, but not limited to, computer science, design, and cognitive science) to discuss challenges in considering affects and emotions for interactive and media experiences (i.e., to assess and/or improve these experiences) from an interdisciplinary perspective. More specifically, we propose to address the following questions:\nWhy study emotions? For which applications?\nHow to categorise, describe, analyse, quantify, represent emotions?\nHow to take into account emotions in the design and evaluation phases of multimedia experiences?\nWhat are the recent advances in automated emotion recognition? What are the challenges in their evaluation?\nWhat are the different ways of dealing with emotions in interactive media experiences?\nWhat are the existing and future challenges? What are the limitations?\n\nWorkshop Website: https://emotionimx.ls2n.fr/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84055
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84056
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Universite de Nantes"
            }
          ],
          "personId": 84018
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Ecole de design Nantes Atlantique"
            }
          ],
          "personId": 84019
        }
      ]
    },
    {
      "id": 84109,
      "typeId": 12267,
      "durationOverride": 100,
      "title": "SensoryX 2022 - Multisensory Experiences",
      "isBreak": false,
      "importedId": "baeddb54-a145-4537-9f6b-729173b3a5c2",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84074
      ],
      "eventIds": [],
      "abstract": "Multimedia applications have primarily engaged two of the human senses Ð aural and visual. With recent advances in computational technology, however it is possible to develop multisensory applications across all senses. Important findings from psychological and neuroscience research, as well as technological advances responsible for increased diversity of devices with higher computational power and communication capabilities, augmented by various sensor and display technologies have enabled targeting other human senses. Moreover, the state of the art for multisensory systems has been pushed forward with the evolution of Mixed Reality-related technologies that allowed several senses to be stimulated at the same time, presenting users with Ôreal experiencesÕ designed in virtual worlds. This enables a more immersive, coherent, and credible experience, raising the level of presence. In this context, it becomes important to explore and understand how one can design effective multisensory experiences in a variety of domains (e.g., education, arts, entertainment, etc.) and how technology can be used to meaningfully stimulate these experiences. Moreover, it is essential to identify the challenges, opportunities and limitations to be overcome in the quest to transcend the overwhelmingly bisensorial nature of digital multimedia into a multisensory one Ð the realm of mulsemedia: multiple sensorial media. The aim of this workshop is to deepen the discussion on multisensory experience design as well as to inspire and support participants in designing multisensory interfaces and experiences.\n\nThis workshop will focus on enhancing the multisensory scope of both designers and developers of multimedia and Mixed Reality experiences who could thus harness the whole spectrum of sensory experiences. For this, the workshop will challenge current practices in designing experiences, will explore meaningful design spaces and will identify future research directions for creating experiences at the intersection of various sensory dimensions.\n\nWorkshop Website: http://sensoryx.midiacom.uff.br/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "University of Kent"
            }
          ],
          "personId": 84042
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Instituto Federal do Espirito Santo"
            }
          ],
          "personId": 84043
        },
        {
          "affiliations": [
            {
              "country": "France",
              "institution": "Paris 8 University"
            }
          ],
          "personId": 84044
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "institution": "Cefet/RJ"
            }
          ],
          "personId": 84045
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "Brunel University London"
            }
          ],
          "personId": 84046
        }
      ]
    },
    {
      "id": 84110,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "XRWALC 2022 - 1st Int. Workshop on Analytics, Learning & Collaboration in eXtended Reality",
      "isBreak": false,
      "importedId": "3463d7c0-a1ac-4d77-bad9-2f56608f2e31",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84076
      ],
      "eventIds": [],
      "abstract": "XR, extended or cross reality, technology is gaining relevance in many professional domains, and is used and experimented with for the sake of manifold purposes. The XRWALC workshop at ACM IMX 2022 is aiming to explore the latest advancements with a focus on three particular application aspects:\n- Remote analytics: inspect and interact with remote systems, explore and analyze data within immersive XR environments.\n- Immersive learning: exploit the advantages of safe and affordable learning environments that allow to simulate, try out, experience, learn and train.\n- Collaboration in XR: approaches to design and enable group collaboration.\nTo apply for participation, please submit extended abstracts that can be short summaries of research advancements or studies, novel concepts and approaches, descriptions of demonstrations, or descriptions of practical problems and research questions which the authors seek to discuss.\n\nWorkshop Website: http://xrwalc2022.di.fc.ul.pt/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Know-Center"
            }
          ],
          "personId": 84029
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "BT Research Labs in Adastral Park"
            }
          ],
          "personId": 84030
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Dodds Consulting"
            }
          ],
          "personId": 84032
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84035
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "LASIGE, Faculdade de Ciencias, Universidade de Lisboa"
            }
          ],
          "personId": 84037
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84039
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Institute for Immersive Designs, Experiences, Applications and Stories at American University in Washington DC"
            }
          ],
          "personId": 84041
        }
      ]
    },
    {
      "id": 84111,
      "typeId": 12267,
      "durationOverride": 120,
      "title": "XRWALC 2022 - 1st Int. Workshop on Analytics, Learning & Collaboration in eXtended Reality",
      "isBreak": false,
      "importedId": "c087aa07-8763-4849-8996-d10af451b033",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84077
      ],
      "eventIds": [],
      "abstract": "XR, extended or cross reality, technology is gaining relevance in many professional domains, and is used and experimented with for the sake of manifold purposes. The XRWALC workshop at ACM IMX 2022 is aiming to explore the latest advancements with a focus on three particular application aspects:\n- Remote analytics: inspect and interact with remote systems, explore and analyze data within immersive XR environments.\n- Immersive learning: exploit the advantages of safe and affordable learning environments that allow to simulate, try out, experience, learn and train.\n- Collaboration in XR: approaches to design and enable group collaboration.\nTo apply for participation, please submit extended abstracts that can be short summaries of research advancements or studies, novel concepts and approaches, descriptions of demonstrations, or descriptions of practical problems and research questions which the authors seek to discuss.\n\nWorkshop Website: http://xrwalc2022.di.fc.ul.pt/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Know-Center"
            }
          ],
          "personId": 84029
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "BT Research Labs in Adastral Park"
            }
          ],
          "personId": 84030
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Dodds Consulting"
            }
          ],
          "personId": 84032
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84035
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "LASIGE, Faculdade de Ciencias, Universidade de Lisboa"
            }
          ],
          "personId": 84037
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84039
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Institute for Immersive Designs, Experiences, Applications and Stories at American University in Washington DC"
            }
          ],
          "personId": 84041
        }
      ]
    },
    {
      "id": 84112,
      "typeId": 12267,
      "durationOverride": 100,
      "title": "XRWALC 2022 - 1st Int. Workshop on Analytics, Learning & Collaboration in eXtended Reality",
      "isBreak": false,
      "importedId": "47a8f153-91ab-41ee-8ce6-78dd68db3331",
      "source": "CSV",
      "trackId": 11850,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84078
      ],
      "eventIds": [],
      "abstract": "XR, extended or cross reality, technology is gaining relevance in many professional domains, and is used and experimented with for the sake of manifold purposes. The XRWALC workshop at ACM IMX 2022 is aiming to explore the latest advancements with a focus on three particular application aspects:\n- Remote analytics: inspect and interact with remote systems, explore and analyze data within immersive XR environments.\n- Immersive learning: exploit the advantages of safe and affordable learning environments that allow to simulate, try out, experience, learn and train.\n- Collaboration in XR: approaches to design and enable group collaboration.\nTo apply for participation, please submit extended abstracts that can be short summaries of research advancements or studies, novel concepts and approaches, descriptions of demonstrations, or descriptions of practical problems and research questions which the authors seek to discuss.\n\nWorkshop Website: http://xrwalc2022.di.fc.ul.pt/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Know-Center"
            }
          ],
          "personId": 84029
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "institution": "BT Research Labs in Adastral Park"
            }
          ],
          "personId": 84030
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Dodds Consulting"
            }
          ],
          "personId": 84032
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84035
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "institution": "LASIGE, Faculdade de Ciencias, Universidade de Lisboa"
            }
          ],
          "personId": 84037
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "institution": "Institute of Interactive Systems and Data Science at TU Graz"
            }
          ],
          "personId": 84039
        },
        {
          "affiliations": [
            {
              "country": "USA",
              "institution": "Institute for Immersive Designs, Experiences, Applications and Stories at American University in Washington DC"
            }
          ],
          "personId": 84041
        }
      ]
    },
    {
      "id": 84113,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Usability Of Text-To-Speech Technology in Creating News Podcasts using Portuguese Of Portugal",
      "isBreak": false,
      "importedId": "023dd5b0-71f8-43e4-8d2c-ee429cb5a707",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "The increase in the consumption of digital formats has, in many cases, been a penalty for traditional media companies. In the adaptation to digital, the transformation of written news into audio formats, that guarantee spatio-temporal flexibility in its consumption, is one of the differentiating options. Artificial intelligence tools can help accelerate and automate the digitalization processes. It is, therefore, the objective of this paper to evaluate the integration of Text-to-Speech (TTS) technology in the process of creating news podcasts. The study comprised two surveys. The first corresponding to the validation of TTS services in Portuguese from Portugal and, the second for the validation of three models of news podcasts containing human voice, synthesized voice via TTS, and a hybrid model with TTS voice and human voice. The results point to a general acceptance of the integration of voices generated by TTS in news podcasts without prejudice to the consumer experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            },
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83853
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83955
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 83907
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 83869
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83946
        }
      ]
    },
    {
      "id": 84114,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Integrating 3D Objects in Multimodal Video Annotation",
      "isBreak": false,
      "importedId": "fc85c956-c6a5-44bb-97e4-e9080cec37ae",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "This paper presents and discusses the introduction of 3D functionalities for an existing web-based multimodal video annotation tool. Over the past years, we have developed a multimodal web video annotation tool that now combines 3D models and 360º content with more traditional annotation types (e.g., text, drawings, images), offering users the possibility of adding extra information in their annotation work. We show how 3D models augment the annotation work and add advantages like viewing or exploring objects in detail and from different angles. The paper reports detailed feedback from a pilot study in form of a workshop with traditional dance experts to whom these new features were presented. We conclude with an outlook of future iterations of the video annotator based on the experts’ feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "NOVA University of Lisboa",
              "dsl": "Faculdade de Ciências e Tecnologia "
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Setúbal",
              "institution": "Polytechnic Institute of Setúbal",
              "dsl": "Escola Superior de Tecnologia "
            }
          ],
          "personId": 83867
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Nova University of Lisboa",
              "dsl": "ICNOVA, FSCH"
            }
          ],
          "personId": 83845
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "FCSH - Universidade Nova de Lisboa",
              "dsl": "ICNOVA"
            }
          ],
          "personId": 83889
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "NOVA School of Science & Technology, NOVA University Lisbon,",
              "dsl": ""
            }
          ],
          "personId": 83896
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Caparica",
              "institution": "Faculdade de Ciências e Tecnologia, Universidade Nova de Lisboa",
              "dsl": "NOVA-LINCS"
            }
          ],
          "personId": 83855
        }
      ]
    },
    {
      "id": 84115,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "A Podcast Creation Platform to Support News Corporations: Results from UX Evaluation",
      "isBreak": false,
      "importedId": "f2a525cc-c575-4ac2-ba8f-48b3314a7aa1",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Podcasts as a media format have been increasing popular in recent years. The ease of access to this format have contributed for its success story. However, the creation of Podcasts requires specific hardware and software for recording and editing it. Some platforms have emerged with the proposal to ease this creation process, namely by introducing Text-to-Speech (TTS) technologies removing the need for capturing and editing voice, reducing the effort necessary for producing this format, yet no platform allows the use of TTS in Portuguese of Portugal while retaining the scope of “podcast creation platform”. Taking these limitations in mind we present the proposal of an all-in-one Podcast Creation platform with the availability of TTS Technology in Portuguese of Portugal. The paper describes the usability testing (UX) of the platform using 3 methodologies being: Self-Assessment Manikin (SAM), System Usability Scale (SUS) and Attrakdiff. with promising results regarding its usability and desirability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83955
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": ""
            }
          ],
          "personId": 83907
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of aveiro",
              "dsl": ""
            }
          ],
          "personId": 83869
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83853
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83946
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Aveiro",
              "city": "Aveiro",
              "institution": "Universidade de Aveiro",
              "dsl": "Digimedia"
            }
          ],
          "personId": 83898
        }
      ]
    },
    {
      "id": 84116,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Peripheral Light Cues as a Naturalistic Measure of Focus",
      "isBreak": false,
      "importedId": "1a225908-0451-4f5d-99a2-3a113d7c17e2",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Deeply immersive experiences are intrinsically rewarding; evoking them for another is a cornerstone of success in artistic or design practice.  At the same time, modern interfaces have created a state of 'partial continuous attention', and frequent self-interruption is more common than ever.  In this paper, we propose a smart-glasses based interaction to quantify self-interruption dynamics in naturalistic settings, in which a slowly changing peripheral LED is monitored as a secondary task by the user.  We demonstrate that this interaction captures useful information about a user's state of engagement in real-world conditions.  These data can provide designers and artists novel, objective insight into the depth of immersive experience evoked in real-world settings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments Group"
            }
          ],
          "personId": 83843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT Media Lab",
              "dsl": "Responsive Environments"
            }
          ],
          "personId": 83840
        }
      ]
    },
    {
      "id": 84117,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Acting emotions: physiological correlates of emotional valence and arousal dynamics in theatre",
      "isBreak": false,
      "importedId": "3771ff12-8f96-4612-bab8-2112329b8c66",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Professional theatre actors are highly specialized in controlling their own expressive behaviour and non-verbal emotional expressiveness, so they are of particular interest in fields of study such as affective computing. We present Acting Emotions, an experimental protocol to investigate the physiological correlates of emotional valence and arousal within professional theatre actors. Ultimately, our protocol investigates the physiological agreement of valence and arousal amongst several actors. Our main contribution lies in the open selection of the emotional set by the participants, based on a set of four categorical emotions, which are self-assessed at the end of each experiment. The experiment protocol was validated by analyzing the inter-rater agreement (> 0.261 arousal, > 0.560 valence), the continuous annotation trajectories, and comparing the box plots for different emotion categories. Results show that the participants successfully induced the expected emotion set to a significant statistical level of distinct valence and arousal distributions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "Faculty of Engineering of Porto",
              "dsl": ""
            },
            {
              "country": "Portugal",
              "state": "",
              "city": "Porto",
              "institution": "INESC-TEC",
              "dsl": ""
            }
          ],
          "personId": 83861
        }
      ]
    },
    {
      "id": 84118,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Party Mascot: Experimental Prop Design for Streaming Actual Plays ",
      "isBreak": false,
      "importedId": "5e3defd4-6087-4054-a069-0ec04394cd3f",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Party Mascot is an experimental design for a dynamic, interactive prop used in “actual play” streaming. Taking the form of a talking mechanical bird, the Party Mascot extends audience participation on the Twitch platform from its native chat interface to the physical playspace. Building on a critical review of frame analytical approaches to role-playing game studies and supported by an ethnographic study of actual play performers, the Party Mascot is designed to “flicker” between social, gameplay, and fictional frames of interaction. It can accommodate any number of participants and adapts to multiple roles within new mediated performance contexts. Shifting spectatorship from the screen to the physical world, the Party Mascot can reconfigure audience/performer relationships, open new avenues for game design, and engage the genre of actual play as a new site of experimentation and innovation between the producers and consumers of media.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media, School of Literature, Media and Communication"
            }
          ],
          "personId": 83937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology,",
              "dsl": "Digital Media, School of Literature, Media, and Communication"
            }
          ],
          "personId": 83857
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "Digital Media/ School of Literature, Media, and Communication"
            }
          ],
          "personId": 83876
        }
      ]
    },
    {
      "id": 84119,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Towards Multimodal Search and Visualization of Movies Based on Emotions",
      "isBreak": false,
      "importedId": "1d95755f-1bc2-4500-9d61-f34b8cf8c6e5",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Movies are one of the most important and impactful forms of entertainment and a powerful vehicle for culture and education, due to the cognitive and emotional impact on the viewers, and technology has been making them more accessible in pervasive services and devices. As such, the huge amount of movies we can access, and the important role emotions play in our lives, make more pertinent the ability to access, visualize and search movies based on their emotional impact. In this paper, we characterize the challenges and approaches in this scenario, and present interactive means to visualize and search movies based on their dominant and actual emotional impact along the movie, with different models and modalities. In particular through emotional highlights and trajectories, the user’s emotional state, or a music being played. Music contributes greatly to the emotional impact of movies and it can also be a trigger to get us into one of them in\r\nserendipitous moments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 83899
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Faculdade de Ciências, Universidade de Lisboa",
              "dsl": "LASIGE"
            }
          ],
          "personId": 83942
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisbon",
              "institution": "Faculty of Science at University of Lisbon",
              "dsl": ""
            }
          ],
          "personId": 83924
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "Universidade de Lisboa",
              "dsl": "LASIGE, Faculdade de Ciências"
            }
          ],
          "personId": 83884
        }
      ]
    },
    {
      "id": 84120,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "User-Centered Broadcasting Service Utilizing Personal Data Store",
      "isBreak": false,
      "importedId": "87c35f33-827a-4300-a1f2-c83969293882",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Recently, the data management model using personal data store (PDS), which is a mechanism for users to store and manage their personal data, has been discussed in response to stricter privacy protection worldwide. In this regard, we developed a data-driven personalization method for broadcasting services. Various personal data, such as program viewing history and Internet service usage history, are stored centrally in the PDS on the user’s side. This personal data can be utilized under user control when using various services allowing cross-industry service collaboration while maintaining a high level of transparency to the user. This enables users to use broadcasting service more widely and conveniently by linking it with various Internet services. In this study, we developed a prototype system that implements end-to-end components from acquisition to utilization of broadcast program viewing history. The system consists of a set of functions that acquires viewing history from broadcast and Internet streaming, stores it on PDS, and uses it in applications. The PDS implementation utilizes open-source software based on web standards to facilitate data linkage with a variety of Internet services. As an effective example for system evaluation, we designed and prototyped on-demand video viewing services in which separate applications of different service providers are linked via the PDS.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 83873
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": ""
            }
          ],
          "personId": 83895
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Setagaya, Tokyo",
              "institution": "NHK (Japan Broadcasting Corporation)",
              "dsl": "Science and Technology Research Labs"
            }
          ],
          "personId": 83850
        }
      ]
    },
    {
      "id": 84121,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Scenario-based Exploration of Integrating Radar Sensing into Everyday Objects for Free-Hand Television Control",
      "isBreak": false,
      "importedId": "e8f0231e-5a72-4a0e-a262-9d7b4b6246d8",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "We address gesture input for TV control, for which we examine mid-air free-hand interactions that can be detected via radar sensing. We adopt a scenario-based design approach to explore possible locations from the living room where to integrate radar sensors, e.g., in the TV set, the couch armrest, or the user's smartphone, and we contribute a four-level taxonomy of locations relative to the TV set, the user, personal robot assistants, and the living room environment, respectively. We also present preliminary results about an interactive system using a 15-antenna ultra-wideband 3D radar, for which we implemented a dictionary of six directional swipe gestures for the control of dichotomous TV system functions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83875
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ştefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83758
        },
        {
          "affiliations": [
            {
              "country": "Romania",
              "state": "",
              "city": "Suceava",
              "institution": "Ștefan cel Mare University of Suceava",
              "dsl": "MintViz Lab, MANSiD Research Center"
            }
          ],
          "personId": 83759
        }
      ]
    },
    {
      "id": 84122,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Classification of the Video Type and Device Used in 360-Degree Videos from the Trajectories of its Viewers' Orientations with LSTM Neural Network Models",
      "isBreak": false,
      "importedId": "895019cb-cf2a-4bc8-8c64-43587a635a2e",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "360-degree videos are consumed in diverse devices: some based in immersive interfaces, such as viewed through Virtual Reality headsets and some based in non-immersive interfaces, as in a computer with a pointing device or mobile devices with touchscreens. We have found, in prior work, significant differences in user behavior between these devices. From a dataset of the trajectories of the users’ head orientation in 775 video reproductions, we classify which kind of video was played (two values) and which of the four possible devices was used to reproduce these videos. We found that recurrent neural network models based on LSTM layers are able to classify the video type and the device used to play the video with an average accuracy of over 90% with only four seconds of trajectory. We are convinced that this knowledge can improve techniques to predict future viewports used in viewport-adaptive streaming when diverse devices are used.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83925
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83887
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83945
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Palma",
              "institution": "University of the Balearic Islands",
              "dsl": "Multimedia Information Technologies Laboratory (LTIM), Department of Mathematics and Computer Science"
            }
          ],
          "personId": 83824
        }
      ]
    },
    {
      "id": 84123,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Deep Learning Augmented Realistic Avatars for Social VR Human Representation",
      "isBreak": false,
      "importedId": "b4b82002-b4f0-4ba6-9096-c476b62bb2f9",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) has created a new and rich medium for people to meet each other digitally. In VR, people can choose from a broad range of representations. In several cases, it is important to provide users with avatars that are a lifelike representation of themselves, to increase the user experience and effectiveness of communication. In this work, we propose a pipeline for generating a realistic and expressive avatar from a single reference image. The pipeline consists of a blendshape-based avatar combined with two deep learning improvements. The first improvement module runs offline and improves the texture map of the base avatar. The second module runs inference in real-time at the rendering stage and performs a style transfer to the avatar's eyes. The deep learning modules effectively improve the visual representation of the avatar and show how AI techniques can be integrated with traditional animation methods to generate realistic human avatars for social VR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83839
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83932
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83921
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83903
        }
      ]
    },
    {
      "id": 84124,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "\"I want to be independent. I want to make informed choices.\": An Exploratory Interview Study of the Effects of Personalisation of Digital Media Services on the Fulfilment of Human Values",
      "isBreak": false,
      "importedId": "42160643-af26-44aa-8a23-0aa6ffea3baa",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "From the landing page of a shopping website, to a tailored layout on a video streaming app, digital media experiences are becoming increasingly personalised, and none of us have the same experience as each other. We report on a series of in-depth interviews, with UK media users from 19 to 68 years old, exploring their awareness, feelings, expectations and concerns about the digital media being personalised ’for them’, and the language that they use when talking about it. Our repeatable, extensible methodology develops insights aligned to a framework of fundamental human values.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83863
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Salford, Manchester",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83852
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Nottingham",
              "institution": "University of Nottingham",
              "dsl": ""
            }
          ],
          "personId": 83926
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Manchester",
              "city": "Salford",
              "institution": "BBC",
              "dsl": "BBC R&D"
            }
          ],
          "personId": 83848
        }
      ]
    },
    {
      "id": 84125,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Grasping Temperature: Thermal Feedback in VR Robot Teleoperation",
      "isBreak": false,
      "importedId": "15f76052-59c3-4591-86ca-64d81b6e4d47",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "This paper presents a proof-of-concept of a robotic teleoperation system, that provides the human operator a thermal sense in addition to the visual sense. With a sensor suite comprising a stereo camera, 360⁰ camera and long-wave infra-red camera, our demonstrator pushes the boundaries of virtual-reality situational awareness by bringing not only 3D visual content but also a 360⁰ thermal experience to the operator. The visual channel of our robotic teleoperation system is represented through a head-mounted-display and the thermal channel is displayed through directional heaters in the operator cockpit and a thermal glove. Initial tests showed that an operator successfully experienced a 360⁰ remote environment, correctly distinguished between and interacted with hot and cold objects, and could notice the presence of nearby people outside her direct field-of-view, based on their emitted heat.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83932
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 83846
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83847
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": "Intelligent Imaging"
            }
          ],
          "personId": 83920
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83930
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "The Hague",
              "city": "The Hague",
              "institution": "TNO",
              "dsl": ""
            }
          ],
          "personId": 83897
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Soesterberg",
              "institution": "TNO",
              "dsl": "Perceptual and Cognitive Systems"
            }
          ],
          "personId": 83823
        }
      ]
    },
    {
      "id": 84126,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "AR in the OR: exploring use of augmented reality to support endoscopic surgery",
      "isBreak": false,
      "importedId": "171a1583-1a18-4370-8ee7-8504d5bb99e3",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Modern operating rooms (OR) are equipped with several ceiling- and wall-mounted screens that display surgical information. These physical displays are restricted in placement, limiting the surgeons' ability to freely position them in the environment. Our work addresses this issue by exploring the feasibility of using an augmented reality (AR) headset (Microsoft HoloLens 2) as an alternative to traditional surgical screens; leading to a reduced OR footprint and improved surgical ergonomics. We developed several prototypes using state-of-the-art hardware/software and conducted various neurosurgery-related exploratory studies. Initial feedback from users suggests that coloration and resolution of the holographic feed were adequate, however, surgeons frequently commented on tactile/visual asynchrony. This emphasizes the need for novel, more efficient hardware/software solutions to support fine motor tasks in the OR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83928
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83904
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83908
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83838
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83934
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh Medical Center",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 83878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": "School of Computing and Information"
            }
          ],
          "personId": 83825
        }
      ]
    },
    {
      "id": 84127,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "The Augmented Museum: A Multimodal, Game-Based, Augmented Reality Narrative for Cultural Heritage",
      "isBreak": false,
      "importedId": "07deb50a-5790-4db9-bf3f-186c8ae5e4d3",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "As digitization has transformed media and Augmented Reality (AR) is evolving from a research area to a commodity, museums are creating interactive AR experiences to digitally enhance their collection and increase audience engagement. Head-worn AR experiences, though, face interaction challenges as they are often employed in busy spaces and are in need of intuitive multimodal interfaces for users on the move. This paper presents an innovative, work-in-progress, multimodal AR experience integrating non-obtrusive dialogue, music, and sound as well as gesture and gaze-based interaction, while a user is wearing a head-worn AR display. Users are motivated to explore and interact with digital cultural artefacts superimposed onto the real-world museum setting and physical artefacts, while moving around in a museum setting. We initially analyze interactive AR experiences to identify specific user requirements related to head-worn AR experiences. We deploy these requirements for the design of interactive, multimodal AR in a museum setting.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83911
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83913
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83947
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83922
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Ephorate of Antiquities of Chania",
              "dsl": ""
            }
          ],
          "personId": 83909
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Chania",
              "institution": "Technical University of Crete",
              "dsl": ""
            }
          ],
          "personId": 83957
        }
      ]
    },
    {
      "id": 84128,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Learning Sustainable Locust Control Methods in Virtual Reality",
      "isBreak": false,
      "importedId": "81a5d6ee-525b-4ba0-9f62-fc9fdc9066da",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Invasion of locust swarms has affected the crops in many countries in Africa and Asia, which is a significant threat to food security. Therefore, different approaches are adopted to monitor and control the locust swarms to save the crops. Furthermore, it has been proved in various studies that technology can help in agriculture through drones, real-time data monitoring, or teaching the farmers with the latest tools. \r\n\r\nFollowing the UN sustainability goals for food security, this research has presented a Virtual Reality(VR) based educational application to teach sustainable locust management strategies. Using hand tracking technology in the Oculus Quest lets users learn how farmers can deal with locusts without pesticides. Based on a storytelling approach, the methods presented are profitable for the farmers and free of any harm to crops regarding food security. This application can help motivate the adoption of these sustainable locust control strategies in broader interventions for environmental recovery.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Fairfax",
              "institution": "George Mason University",
              "dsl": ""
            }
          ],
          "personId": 83874
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Applied Science BFI Vienna",
              "dsl": ""
            }
          ],
          "personId": 83933
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 83927
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 83892
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "University College Dublin",
              "dsl": ""
            }
          ],
          "personId": 83900
        }
      ]
    },
    {
      "id": 84129,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Augmenting a Nature Documentary with a Lifelike Hologram in Virtual Reality",
      "isBreak": false,
      "importedId": "7b391571-a449-4a6f-8d23-32a1f5cc2df0",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "While augmented reality television (ARTV) is being investigated in research labs, the high cost of AR headsets makes it difficult for audiences to benefit from the research. However, the relative affordability of virtual reality (VR) headsets provides ARTV researchers with opportunities to test their prototypes in VR. Additionally, as VR becomes an acceptable medium for watching conventional TV, augmenting such viewing experiences in VR creates new opportunities.\r\nWe prototype a nature documentary ARTV experience in VR and conduct a remote user study (n=10) to investigate six points on the visual display design dimension of presenting a lifelike programme-related hologram. We manipulated the starting point and the movement behaviour of the hologram to gain insight into viewer preferences.\r\nOur findings highlight the importance of personal preferences and that of the perceived role of a hologram in relation to the underlying TV content; suggesting there may not be a single way to augment a TV programme. Instead, creators may need to provide the audiences with capabilities to customise ARTV content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83731
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lancashire",
              "city": "Salford",
              "institution": "The British Broadcasting Corporation",
              "dsl": "Research and Development"
            }
          ],
          "personId": 83768
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Lanarkshire",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 83741
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83745
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "The University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 83728
        }
      ]
    },
    {
      "id": 84130,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "CS:NO - an Extended Reality Experience for Cyber Security Education",
      "isBreak": false,
      "importedId": "77bab14e-8ea2-47a6-b865-07b4798496d8",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "This work-in-progress presents the design of an XR prototype for the purpose of educating basic cybersecurity concepts. We have designed an experimental virtual reality cyberspace to visualise data traffic over network, enabling the user to interact with VR representations of data packets. Our objective was to help the user better conceptualise abstract cybersecurity topics such as encryption and decryption, firewall and malicious data. Additionally, to better stimuli the sense of immersion we have used Peltier thermoelectric modules and Arduino Uno to experiment with multisensory XR. Furthermore, we reflect on early evaluation of this experimental prototype and present potential paths for future improvements. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83954
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83918
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83951
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Department of Computer and Systems Sciences, Stockholm University",
              "dsl": ""
            }
          ],
          "personId": 83923
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83862
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83776
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "RISE Research Institutes of Sweden",
              "dsl": "RISE  Cybersecurity"
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "Stockholm University",
              "dsl": "Department of Computer and Systems Sciences"
            }
          ],
          "personId": 83726
        }
      ]
    },
    {
      "id": 84131,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Interactive Touch Kiosks Designed for the Elderly: A Compilation of Requisites Acknowledging Physical and Psycho-sociological Age-related Changes",
      "isBreak": false,
      "importedId": "0fe3012d-2d3c-46a6-87bf-93844e8311c8",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Aiming cognitive stimulation and physical exercise, interactive touch kiosks designed for the elderly seem to be promising options to promote active aging, ensuring e-health and well-being services. They need to be created and improved according to the elderly population's real needs; however, recommendations to develop these solutions are scattered in several guidelines and standards. In this study standards regarding physical and psycho- sociological age-related changes, such as vision, hearing, cognition, communication, gross and fine motor skills were gathered; physical and social factors were also considered. A total of 107 items were found, and the following categories were defined: Terminals, Interface, Content, and Other. The proposal can be used as: a) a list to guide the creation of services and systems; b) a grid to be color coded according to the level of problems found while usability evaluations are being conducted. This is a contribution to experts who can easily recognize the items that need to be improved in the services and systems, to better support the experience of the elderly user.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83881
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83837
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Aveiro",
              "institution": "University of Aveiro",
              "dsl": "Department of Communication and Art, DigiMedia"
            }
          ],
          "personId": 83833
        }
      ]
    },
    {
      "id": 84132,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Emotional Virtual Reality Stroop Task: an Immersive Cognitive Test",
      "isBreak": false,
      "importedId": "7cf13e0e-8334-478f-b6ec-242a032c35e0",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Stroop Colour-Word Task has been widely used as a cognitive task. There are computerised and Virtual Reality versions of this task that are commonly used. The emotional version of the task, called the Emotional Stroop Colour-Word task is commonly used to induce certain emotions in a person. We are developing an application that brings the Emotional Stroop Colour-Word task into Virtual Reality. The aim of this application is to elicit different stress levels on the user and to record associated brain, heart and skin activity using wearable sensors. It is an immersive application that includes a tutorial, artificial intelligence generated audio instructions and a logging system for the user activity.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "Munster",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83890
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83834
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Cork",
              "institution": "University College Cork",
              "dsl": ""
            }
          ],
          "personId": 83917
        }
      ]
    },
    {
      "id": 84133,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "HiruXR: a Web library for Collaborative and Interactive Data Visualizations in XR and 2D",
      "isBreak": false,
      "importedId": "cba83158-0557-402c-a68b-bcd63cc83e6f",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "HiruXR is a Javascript library aimed at creating collaborative and interactive data visualizations in eXtended Reality (XR). Developers can use it to create environments where multiple users can communicate and collaborate around one or many visualizations. We want to open these environments to users with 2D displaying and interaction capabilities, e.g. allowing them to collaborate from their laptops and phones with others using Head Mounted Displays (HMD). But displaying the same interface is not optimal, users should see an interface according to their device capabilities. Therefore, we are steering the design of HiruXR towards supporting responsiveness without putting all the burden in application developers. The library defines visualization, interaction and collaboration components that adapt to the user device capabilities, i.e. 2D or XR. In this paper, we share the design philosophy, initial implementation examples and lessons learned so far building some of these components for collaboration between VR and 2D devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "San Sebastián",
              "institution": "Vicomtech",
              "dsl": ""
            }
          ],
          "personId": 83935
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "",
              "city": "Donostia",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83936
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83893
        },
        {
          "affiliations": [
            {
              "country": "Spain",
              "state": "Gipuzkoa",
              "city": "Donostia-San Sebastian",
              "institution": "Vicomtech",
              "dsl": "Digital Media"
            }
          ],
          "personId": 83939
        }
      ]
    },
    {
      "id": 84134,
      "typeId": 12266,
      "durationOverride": 1,
      "title": "Designing a VR Lobby for Remote Opera Social Experiences",
      "isBreak": false,
      "importedId": "5dd207ae-40ef-44ee-a1d3-faab955c8c62",
      "source": "PCS",
      "trackId": 11841,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        84001
      ],
      "eventIds": [],
      "abstract": "Several social VR platforms support virtual entertainment events, however their value for post-show activities remains unclear. Through a user-centered approach, we design a social VR lobby experience to enrich four motivations of theatre-goers: social, intellectual, emotional, and spiritual engagement. We ran a context-mapping focus group session with professionals (N=6) to conceptualize the social VR space for digital opera experiences. Based on our findings, we propose a social VR lobby consisting of four rooms: 1) a Bar for social engagement, 2) an Info Booth for intellectual engagement, 3) a Photo Zone for emotional engagement, and 4) an Interactive Stage for spiritual engagement. Based on this work, we plan to experimentally evaluate audience experiences in each room in order to create a social VR lobby template for theater experiences.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 83891
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            }
          ],
          "personId": 83931
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Amsterdam",
              "institution": "Centrum Wiskunde & Informatica (CWI)",
              "dsl": ""
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "Delft",
              "institution": "Delft University of Technology (TU Delft)",
              "dsl": ""
            }
          ],
          "personId": 83915
        }
      ]
    }
  ],
  "people": [
    {
      "id": 83726,
      "firstName": "Asreen",
      "lastName": "Rostami",
      "middleInitial": "",
      "importedId": "9lDKGWRcXUhNRYjt7cheTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83727,
      "firstName": "Jon",
      "lastName": "Francombe",
      "middleInitial": "",
      "importedId": "w-pMJdYN4rHE86wTcYvG6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83728,
      "firstName": "Robert",
      "lastName": "Stevens",
      "middleInitial": "",
      "importedId": "RWqicRw8reqU75q8JBbSyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83729,
      "firstName": "Paula",
      "lastName": "Reyero Lobo",
      "middleInitial": "",
      "importedId": "zUkqgn_RUnJi71vRiUh2uA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83730,
      "firstName": "Narciso",
      "lastName": "García",
      "middleInitial": "",
      "importedId": "Af0Qsd-NjzuQocarzjRRzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83731,
      "firstName": "Pejman",
      "lastName": "Saeghe",
      "middleInitial": "",
      "importedId": "wgVYUdzEYo_GHL_g4JUgmw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83732,
      "firstName": "Jacob",
      "lastName": "Young",
      "middleInitial": "",
      "importedId": "vNppDxJR8a_G7rOKHcVh4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83733,
      "firstName": "Lora",
      "lastName": "Oehlberg",
      "middleInitial": "",
      "importedId": "GgOZkpsAdy2_ES3FSLq7NQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83734,
      "firstName": "Jaime",
      "lastName": "Ruiz",
      "middleInitial": "",
      "importedId": "UarTnVr-HA1uu3AeXM1qZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83735,
      "firstName": "Jemily",
      "lastName": "Rime",
      "middleInitial": "I",
      "importedId": "GTajBUP0mrlPdHKmNa29BA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83736,
      "firstName": "Julie",
      "lastName": "Williamson",
      "middleInitial": "R.",
      "importedId": "ZKH_-pB1OqX7SuMTQup9UQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83737,
      "firstName": "Anthony",
      "lastName": "Tang",
      "middleInitial": "",
      "importedId": "YGO4_l5XRHyTv-hGHbY83g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83738,
      "firstName": "Lei",
      "lastName": "Zhang",
      "middleInitial": "",
      "importedId": "DKUDliMOAT1MC5jsBXyXzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83739,
      "firstName": "George",
      "lastName": "Ghinea",
      "middleInitial": "",
      "importedId": "FtdAji75gMcZfs3ZPX-9Bg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83740,
      "firstName": "Ifeanyi",
      "lastName": "Odenigbo",
      "middleInitial": "Paul",
      "importedId": "992REHa4AKuW9xh7AUf-Lw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83741,
      "firstName": "Mark",
      "lastName": "McGill",
      "middleInitial": "",
      "importedId": "ob1mmWPesH21Yfh_UxGqsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83742,
      "firstName": "Than Htut",
      "lastName": "Soe",
      "middleInitial": "",
      "importedId": "i6XkABqZoiMNOOq_35J1eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83743,
      "firstName": "María Victoria",
      "lastName": "Redondo Vega",
      "middleInitial": "",
      "importedId": "LV0D83SyhKm-VspeYuIsww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83744,
      "firstName": "Jesús",
      "lastName": "Gutiérrez",
      "middleInitial": "",
      "importedId": "bx980qr1f7ADN-Ehr6RaIQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83745,
      "firstName": "Sarah",
      "lastName": "Clinch",
      "middleInitial": "",
      "importedId": "DQB5-X25-6OVjYs7WP9zpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83746,
      "firstName": "Rita",
      "lastName": "Orji",
      "middleInitial": "",
      "importedId": "PnF3YyQQT9s3GxP6eQp7ww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83747,
      "firstName": "Mohamed",
      "lastName": "Khamis",
      "middleInitial": "",
      "importedId": "1m3BwpfxBcbKA7ei75xcDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83748,
      "firstName": "Elena",
      "lastName": "Vallejo",
      "middleInitial": "",
      "importedId": "yWShMMQSTd2Pn2LGHjn1Rw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83749,
      "firstName": "Abigail",
      "lastName": "Fowler",
      "middleInitial": "",
      "importedId": "Z2C1AY2zN621vz4d9XjQQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83750,
      "firstName": "Jorge",
      "lastName": "Abreu",
      "middleInitial": "",
      "importedId": "6MoAr8u1FEDx6fsIej_IrQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83751,
      "firstName": "Manuel José",
      "lastName": "López Morales",
      "middleInitial": "",
      "importedId": "_71AUDr2kmuOkaA0QmjXoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83752,
      "firstName": "Meetha Nesam",
      "lastName": "James",
      "middleInitial": "",
      "importedId": "RNPiCsJCOAVCqdYpXWS_PQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83753,
      "firstName": "Catho",
      "lastName": "Van Den Bosch",
      "middleInitial": "",
      "importedId": "blbYpDIzk-oARhLOREGGyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83754,
      "firstName": "Lukas",
      "lastName": "Stoecklein",
      "middleInitial": "",
      "importedId": "gwbDViGfJLVmAH2cYwYkBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83755,
      "firstName": "Holly",
      "lastName": "Downer",
      "middleInitial": "",
      "importedId": "wDHWe2iIKn23g-n8aXrXwQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83756,
      "firstName": "Alcina",
      "lastName": "Prata",
      "middleInitial": "Maria Narciso",
      "importedId": "tDHVAWMIes29ydqOpmXhlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83757,
      "firstName": "Wendy",
      "lastName": "Van den Broeck",
      "middleInitial": "",
      "importedId": "us5OJE7SsfpgAhbL7Abzcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83758,
      "firstName": "Cristian",
      "lastName": "Pamparău",
      "middleInitial": "",
      "importedId": "e_w6ZMaH1FTy8-PwB5uKVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83759,
      "firstName": "Radu-Daniel",
      "lastName": "Vatavu",
      "middleInitial": "",
      "importedId": "8aqyMcEhyX2llYoyIaN2iQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83760,
      "firstName": "Alejandro",
      "lastName": "Sánchez",
      "middleInitial": "",
      "importedId": "11JhGSpz1L1ITn6FDD2LKQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83761,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "importedId": "2Guk0EWER7zUs-hWrmCSsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83762,
      "firstName": "Kasper",
      "lastName": "Karlgren",
      "middleInitial": "",
      "importedId": "TSUM7oaHbBsTvNZxHNH-nA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83763,
      "firstName": "Marta",
      "lastName": "Orduna",
      "middleInitial": "",
      "importedId": "gyQ_1e2soNntTdC4PiDbew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83764,
      "firstName": "Pablo",
      "lastName": "Perez",
      "middleInitial": "",
      "importedId": "7iLj-tDUwFvYdJTfkDLosg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83765,
      "firstName": "Doug",
      "lastName": "Bowman",
      "middleInitial": "",
      "importedId": "vt2L_KgrYMbRY_hbo_B1rA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83766,
      "firstName": "Diego",
      "lastName": "González Morín",
      "middleInitial": "",
      "importedId": "usR7AVrxRo7WrGBIGQoxWQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83767,
      "firstName": "Donald",
      "lastName": "McMillan",
      "middleInitial": "",
      "importedId": "uQdBLO5MYYFcT9ZEdDGvoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83768,
      "firstName": "Bruce",
      "lastName": "Weir",
      "middleInitial": "",
      "importedId": "03rHInfq6BbGK4pbbCiMdw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83769,
      "firstName": "Esther",
      "lastName": "Guervós Sánchez",
      "middleInitial": "",
      "importedId": "z4O8NYK2aJfRp_-J_TQ2Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83770,
      "firstName": "Julián",
      "lastName": "Cabrera",
      "middleInitial": "",
      "importedId": "M1aqWzGxP0E8hr9lrbPpGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83771,
      "firstName": "Benjamin",
      "lastName": "Allen",
      "middleInitial": "",
      "importedId": "sYLEEJ6Ko7oARIvE1fWxmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83772,
      "firstName": "Alvaro",
      "lastName": "Villegas",
      "middleInitial": "",
      "importedId": "Ld_Fq89JVXxPX06eE2dusQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83773,
      "firstName": "Marija",
      "lastName": "Slavkovik",
      "middleInitial": "",
      "importedId": "JGw7omSeRks3qhPWDoG1Gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83774,
      "firstName": "Tom",
      "lastName": "Collins",
      "middleInitial": "",
      "importedId": "P92V9kbqarxMqGfgDtTLBg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83775,
      "firstName": "Stephen",
      "lastName": "Thompson",
      "middleInitial": "",
      "importedId": "TRpIGLmKJPFx3iI1RwmUyw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83776,
      "firstName": "Jordi",
      "lastName": "Solsona Belenguer",
      "middleInitial": "",
      "importedId": "O7JJXgxIpzb9WzeJQQMdDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83777,
      "firstName": "Nimesha",
      "lastName": "Ranasinghe",
      "middleInitial": "",
      "importedId": "PGeCgLRrndebAml85w42sA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83778,
      "firstName": "Fanjue",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "lmCkmrTSDvm1pXR_XG8xKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83779,
      "firstName": "Alan",
      "lastName": "Guedes",
      "middleInitial": "",
      "importedId": "2tGqb7rv25SfQJugnaNlGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83780,
      "firstName": "Sandy",
      "lastName": "Claes",
      "middleInitial": "",
      "importedId": "sZewOHfQHLuf4F_82nLC-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83781,
      "firstName": "Kiyosu",
      "lastName": "Maeda",
      "middleInitial": "",
      "importedId": "lPm-GtATDqqef17aVhyMYQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83782,
      "firstName": "Riku",
      "lastName": "Arakawa",
      "middleInitial": "",
      "importedId": "NneceY8W4BjESWAfuTSpNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83783,
      "firstName": "Neelima",
      "lastName": "Sailaja",
      "middleInitial": "",
      "importedId": "Mrmmlz02EqV73qpFtBss4Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83784,
      "firstName": "Nadia",
      "lastName": "Pantidi",
      "middleInitial": "",
      "importedId": "cVye3CA0c3N6ptMCaRgmFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83785,
      "firstName": "Joseph",
      "lastName": "O'Hagan",
      "middleInitial": "",
      "importedId": "SI2qt_1tX7On0mp75RTcxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83786,
      "firstName": "Nikki",
      "lastName": "Peeters",
      "middleInitial": "",
      "importedId": "7kcQ6762pUaSPkYQqzUmQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83787,
      "firstName": "César",
      "lastName": "Díaz",
      "middleInitial": "",
      "importedId": "aXpagPG0tnEQorCXyYOnmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83788,
      "firstName": "Bernardo",
      "lastName": "Cardoso",
      "middleInitial": "",
      "importedId": "MrQjzsi5ecXNIrFVE1_8cg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83789,
      "firstName": "Yu-hao",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "vXATPpRPjnoigSEBECc6lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83790,
      "firstName": "Taehyun",
      "lastName": "Rhee",
      "middleInitial": "James",
      "importedId": "fDfNE1E8OmQw4I1wh9KVjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83791,
      "firstName": "Marta",
      "lastName": "Revuelta",
      "middleInitial": "",
      "importedId": "LGupQgA5NIm8QHNZNC-bzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83792,
      "firstName": "Alaa",
      "lastName": "AlSlaity",
      "middleInitial": "",
      "importedId": "mYGqHTgp7MvDWoYruU5h6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83820,
      "firstName": "Pedro",
      "lastName": "Almeida",
      "importedId": "9fef87a0-27da-4f0c-a6e7-c41edc4528b3",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Portugal",
          "institution": "University of Aveiro"
        }
      ]
    },
    {
      "id": 83821,
      "firstName": "Debora",
      "lastName": "Muchaluat Saade",
      "importedId": "d09c2c00-0db1-4429-a520-9c1f23adc3df",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Brazil",
          "institution": "Universidade Federal Fluminense"
        }
      ]
    },
    {
      "id": 83823,
      "firstName": "Jan",
      "lastName": "Van Erp",
      "middleInitial": "",
      "importedId": "77Tl4LXKOC1y6TpLZ35GEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83824,
      "firstName": "Antoni",
      "lastName": "Bibiloni Coll",
      "middleInitial": "",
      "importedId": "_iW_kqDe_A0asZ8bTu6s0Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83825,
      "firstName": "Jacob",
      "lastName": "Biehl",
      "middleInitial": "",
      "importedId": "FKU6pyTWBEuUY0srdo7RTw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83826,
      "firstName": "David",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "kZ98m7yjyjb4grptqT6Wjg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83827,
      "firstName": "Marcelo de Abreu Borges",
      "lastName": "Borges",
      "middleInitial": "A",
      "importedId": "Ry9EhCSXPAp6sx_3A3x6og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83828,
      "firstName": "Giuliano",
      "lastName": "Maia",
      "middleInitial": "",
      "importedId": "wWdLGkMT0pn85gMOx81UJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83829,
      "firstName": "Aljosa",
      "lastName": "Smolic",
      "middleInitial": "",
      "importedId": "dXGNiDMLRrNPfmxKM_vYzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83830,
      "firstName": "Paul A",
      "lastName": "Gardner",
      "middleInitial": "",
      "importedId": "HfJ8z1ovDp8kgzX-P5KBOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83831,
      "firstName": "Erkki",
      "lastName": "Sutinen",
      "middleInitial": "",
      "importedId": "DgfLSpRww2YS3uLWVEXUOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83832,
      "firstName": "Razan",
      "lastName": "Jaber",
      "middleInitial": "",
      "importedId": "NLhhSDuE-8e4ckLupD4KYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83833,
      "firstName": "Ana Margarida Pisco",
      "lastName": "Almeida",
      "middleInitial": "",
      "importedId": "AJu3XVGwXkTyFvmtMy0lQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83834,
      "firstName": "Sabin",
      "lastName": "Tabirca",
      "middleInitial": "",
      "importedId": "zHt-CjbCWZ5Nyhrux4Z0dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83835,
      "firstName": "Heike",
      "lastName": "Winschiers-Theophilus",
      "middleInitial": "",
      "importedId": "i-vIgqYGhPAW_WH-dDQF7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83836,
      "firstName": "Ignacio",
      "lastName": "Benito Frontelo",
      "middleInitial": "",
      "importedId": "fkctoyXo0_ivkp3LsKl41A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83837,
      "firstName": "Sofia",
      "lastName": "Nunes",
      "middleInitial": "",
      "importedId": "gwFx3ZSRCAfFQWdtk5LVLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83838,
      "firstName": "Jeffrey R",
      "lastName": "Head",
      "middleInitial": "",
      "importedId": "tMFakF2WDAIPlMQxr1JueQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83839,
      "firstName": "Matthijs",
      "lastName": "van der Boon",
      "middleInitial": "",
      "importedId": "UtRvKA6Uz0lFo4hioMSbag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83840,
      "firstName": "Joe",
      "lastName": "Paradiso",
      "middleInitial": "",
      "importedId": "mPgn9zhAqjthCQ5b5psNcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83841,
      "firstName": "Rômulo",
      "lastName": "Vieira",
      "middleInitial": "",
      "importedId": "bzcHQyKyMbpJgETqpEUThA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83842,
      "firstName": "Devin",
      "lastName": "Horsman",
      "middleInitial": "",
      "importedId": "snmAPIVjeA3iQp9XKihPxA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83843,
      "firstName": "David",
      "lastName": "Ramsay",
      "middleInitial": "",
      "importedId": "3wPvvQe3xQWyeqAYgjVnYw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83844,
      "firstName": "Nicolas",
      "lastName": "Pope",
      "middleInitial": "",
      "importedId": "xzB2-dkpiSdi7p-trHWUPw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83845,
      "firstName": "Stephan",
      "lastName": "Jürgens",
      "middleInitial": "",
      "importedId": "d57P9hVQWW0DyNVmvwTgcQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83846,
      "firstName": "Alexander",
      "lastName": "Toet",
      "middleInitial": "",
      "importedId": "iPSQgnZVSffp-er0Srbp2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83847,
      "firstName": "Nirul",
      "lastName": "Hoeba",
      "middleInitial": "",
      "importedId": "J0y-Oth-8bh5MTL43VWLUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83848,
      "firstName": "Todd",
      "lastName": "Burlington",
      "middleInitial": "",
      "importedId": "38PYcrfBNp2jVfQ6p08lmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83849,
      "firstName": "Daniel Yong Wen",
      "lastName": "Tan",
      "middleInitial": "",
      "importedId": "6Z8abvvrMJH76JLIKlQvww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83850,
      "firstName": "Kinji",
      "lastName": "Matsumura",
      "middleInitial": "",
      "importedId": "p3PcS1b1YpWKmd8EaGEwKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83851,
      "firstName": "Erkki",
      "lastName": "Rötkönen",
      "middleInitial": "",
      "importedId": "DtK3j3WNN_Y8saeasD8uIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83852,
      "firstName": "Lianne",
      "lastName": "Kerlin",
      "middleInitial": "",
      "importedId": "gYQ8vrm9qiKiUBm_kMUPIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83853,
      "firstName": "Marcelo",
      "lastName": "Afonso",
      "middleInitial": "",
      "importedId": "bL7qqWIREQVsQYUDkcGJGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83854,
      "firstName": "Dmitriy",
      "lastName": "Babichenko",
      "middleInitial": "",
      "importedId": "oob8HcMT6uzeDA9cDxozUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83855,
      "firstName": "Nuno",
      "lastName": "Correia",
      "middleInitial": "",
      "importedId": "CgB0401kd5HQqQRbv5o1TA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83856,
      "firstName": "Kyle",
      "lastName": "Bostelmann",
      "middleInitial": "",
      "importedId": "3S5CqbzK6AqM3CIctEXU0g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83857,
      "firstName": "Xingyu",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "lok3Cha4FDtwrG3pPrrFhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83858,
      "firstName": "Denio",
      "lastName": "Mariz",
      "middleInitial": "",
      "importedId": "bnQv_GFRBDrKuXR8TkMxhA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83859,
      "firstName": "Jordan",
      "lastName": "Marczak",
      "middleInitial": "",
      "importedId": "B3RVs6DFpDCQ0Z0glGnbkQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83860,
      "firstName": "Debora",
      "lastName": "Muchaluat-Saade",
      "middleInitial": "Christina",
      "importedId": "HRJQZjcRgBYsf1N_2-b9jQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83861,
      "firstName": "Luis",
      "lastName": "Aly",
      "middleInitial": "",
      "importedId": "w47et0oAFCx1Przh2k76Ew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83862,
      "firstName": "Luis",
      "lastName": "Quintero",
      "middleInitial": "",
      "importedId": "hO__dxBgjSZsz6-i3AzASQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83863,
      "firstName": "Michael",
      "lastName": "Evans",
      "middleInitial": "",
      "importedId": "XjvS_7GSUiG1Gxz-AKMyzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83864,
      "firstName": "Sebastian",
      "lastName": "Hahta",
      "middleInitial": "",
      "importedId": "mSrZmXQ4xLmJuis8sPhsCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83865,
      "firstName": "Abhinav",
      "lastName": "Tyagi",
      "middleInitial": "",
      "importedId": "OCmd7ldyp282zrLqDTJu6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83866,
      "firstName": "Yu",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "54AOVZg1tAZW_SedGi7Bcw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83867,
      "firstName": "Rui",
      "lastName": "Rodrigues",
      "middleInitial": "",
      "importedId": "HOl_opiJs3B3p2SrDr59cA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83868,
      "firstName": "Céline",
      "lastName": "JOST",
      "middleInitial": "",
      "importedId": "JWlDS9y3zTqOeFPZ0XOjiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83869,
      "firstName": "Telmo",
      "lastName": "Silva",
      "middleInitial": "",
      "importedId": "UDkEm4BSJ-8u-eyo4FgSiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83870,
      "firstName": "Leoberto",
      "lastName": "Soares",
      "middleInitial": "",
      "importedId": "cqrQFgbE3HeI_9sW6bAWkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83871,
      "firstName": "Sydney",
      "lastName": "Mutelo",
      "middleInitial": "",
      "importedId": "hvmYXiGk5xBRNZRrGauJ_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83872,
      "firstName": "Ryan",
      "lastName": "Pham",
      "middleInitial": "",
      "importedId": "4Wxx_e4HmnuPaZhmjgqAAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83873,
      "firstName": "Yuki",
      "lastName": "Yamakami",
      "middleInitial": "",
      "importedId": "r5Q8Dttr9kw6nLf-tOy3Ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83874,
      "firstName": "NATHAN",
      "lastName": "HAHN",
      "middleInitial": "",
      "importedId": "RGB-IVBcAYRCfdraFrEVQg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83875,
      "firstName": "Alexandru-Ionuț",
      "lastName": "Șiean",
      "middleInitial": "",
      "importedId": "eI7dvi463f9iODt-6o-vwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83876,
      "firstName": "Michael",
      "lastName": "Nitsche",
      "middleInitial": "",
      "importedId": "h52EmbxgDYXcz-RGJpl_Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83877,
      "firstName": "Helvi",
      "lastName": "Itenge",
      "middleInitial": "",
      "importedId": "J2Ki22s3fJrI13NSx3R_VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83878,
      "firstName": "Georgios",
      "lastName": "A Zenonos",
      "middleInitial": "",
      "importedId": "SPZiF0h3Jx7sV_LOFR-KRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83879,
      "firstName": "Sam",
      "lastName": "Van Damme",
      "middleInitial": "",
      "importedId": "fqOLSGnFptbkmE71Lfyn2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83880,
      "firstName": "Tariq",
      "lastName": "Zaman",
      "middleInitial": "",
      "importedId": "GG2MgwcvLI-gm4lY9FqonA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83881,
      "firstName": "Carla V.",
      "lastName": "Leite",
      "middleInitial": "",
      "importedId": "R4cqxE1aYw-IuSu7SBK6Tw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83882,
      "firstName": "Amir",
      "lastName": "Abbasnejad",
      "middleInitial": "",
      "importedId": "tez3bfiyK8AO3WkrXDexQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83883,
      "firstName": "Néill",
      "lastName": "O'Dwyer",
      "middleInitial": "",
      "importedId": "Eg3iEnB5juPraDbjodXtXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83884,
      "firstName": "Teresa",
      "lastName": "Chambel",
      "middleInitial": "",
      "importedId": "soT7Ey7h0AgZEl1ihx4MGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83885,
      "firstName": "Isaac",
      "lastName": "Makosa",
      "middleInitial": "",
      "importedId": "JkmvKoZKIHBYRH-N_M-pdg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83886,
      "firstName": "Minghao",
      "lastName": "Cai",
      "middleInitial": "",
      "importedId": "q1sXxogr_iHwEAY0_L6M4A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83887,
      "firstName": "Pere",
      "lastName": "Palmer",
      "middleInitial": "",
      "importedId": "ex-PXJcshhHGa78d30waeg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83888,
      "firstName": "Naska",
      "lastName": "Goagoses",
      "middleInitial": "",
      "importedId": "b9Bhpe-oZlG-kDHNlOKteA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83889,
      "firstName": "Carla",
      "lastName": "Fernandes",
      "middleInitial": "",
      "importedId": "DqaHydhnFtpKjjGT3ospbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83890,
      "firstName": "Deniz",
      "lastName": "Mevlevioğlu",
      "middleInitial": "",
      "importedId": "A58SAdsEspNDXMG6Gtzrhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83891,
      "firstName": "Sueyoon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "XcI-WHi9ei1hj45xCZXBUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83892,
      "firstName": "Elijah",
      "lastName": "Cobb",
      "middleInitial": "",
      "importedId": "MK3BU8exKv_MSFe3OLRisg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83893,
      "firstName": "Guillermo",
      "lastName": "Pacho",
      "middleInitial": "",
      "importedId": "HXmoEs72ZRmATRbp4OVRVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83894,
      "firstName": "Gérard",
      "lastName": "Uzan",
      "middleInitial": "",
      "importedId": "CYtuDdK3HpGPzMZyFCXfrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83895,
      "firstName": "Mikihiro",
      "lastName": "Ueno",
      "middleInitial": "",
      "importedId": "wInLyzlSadETsLIOAFdEkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83896,
      "firstName": "João",
      "lastName": "Diogo",
      "middleInitial": "",
      "importedId": "l1FB0RU5m9E4yKEMwjVoNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83897,
      "firstName": "Frank",
      "lastName": "ter Haar",
      "middleInitial": "B.",
      "importedId": "PL35ntpy27j9AgxjVItC_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83898,
      "firstName": "Carolina",
      "lastName": "Nicolau",
      "middleInitial": "Duarte",
      "importedId": "d0stL_qXrl53KdTJtC1qmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83899,
      "firstName": "Francisco",
      "lastName": "Caldeira",
      "middleInitial": "",
      "importedId": "KpAIgOuZiMr72a1NrUXm5Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83900,
      "firstName": "Muhammad Zahid",
      "lastName": "Iqbal",
      "middleInitial": "",
      "importedId": "kYsyNQpKaVS3hWj89-CnYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83901,
      "firstName": "Guido Lemos",
      "lastName": "de Souza Filho",
      "middleInitial": "",
      "importedId": "qozdhZNECjCtGenA2Mnz8w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83902,
      "firstName": "Cicero Inacio",
      "lastName": "da Silva",
      "middleInitial": "",
      "importedId": "FUEDvceQJDgKLHAfhg0d0A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83903,
      "firstName": "Omar",
      "lastName": "Niamut",
      "middleInitial": "",
      "importedId": "ISsiwK1Agw8ejGlnQiIBVQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83904,
      "firstName": "Edward",
      "lastName": "Andrews",
      "middleInitial": "",
      "importedId": "O8RoAwZknoj3tGOmCAGfHg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83905,
      "firstName": "Leonor",
      "lastName": "Godinho",
      "middleInitial": "",
      "importedId": "U_9Vu0FhT87KZXPePvoWvA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83906,
      "firstName": "Andrew",
      "lastName": "MacIntyre",
      "middleInitial": "",
      "importedId": "-wFwt-EWWsU7jRiSBUwYPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83907,
      "firstName": "Pedro",
      "lastName": "Beça",
      "middleInitial": "",
      "importedId": "VvYX293Q9bJSyef8hTbAMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83908,
      "firstName": "Arka N",
      "lastName": "Mallela",
      "middleInitial": "",
      "importedId": "W9Scs84J4GRXw3mz1FX-zA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83909,
      "firstName": "Eleni",
      "lastName": "Papadopoulou",
      "middleInitial": "",
      "importedId": "8eSoBwY_lZNzXR7F7-i1-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83910,
      "firstName": "Carrie",
      "lastName": "Demmans Epp",
      "middleInitial": "",
      "importedId": "RA6CLxYmkLhrPQu7Oku55A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83911,
      "firstName": "Fotis",
      "lastName": "Giariskanis",
      "middleInitial": "",
      "importedId": "LpnZ9kpS1nczJqeY4Siayg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83912,
      "firstName": "Lucas",
      "lastName": "Aversari",
      "middleInitial": "",
      "importedId": "pUyjqouxYNPINj5hkbDUkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83913,
      "firstName": "Yannis",
      "lastName": "Kritikos",
      "middleInitial": "",
      "importedId": "Lzu9ZY76-lNAmycWH4_eog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83914,
      "firstName": "Flávio Luiz",
      "lastName": "Schiavoni",
      "middleInitial": "",
      "importedId": "k5c87U1U2p9gbFCylMZc6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83915,
      "firstName": "Pablo",
      "lastName": "Cesar",
      "middleInitial": "",
      "importedId": "iHc0TJyBZeQDKJv2npPzWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83916,
      "firstName": "Veronika",
      "lastName": "Kozlova",
      "middleInitial": "",
      "importedId": "iAZ7_tbFIAOkQ886WPhnAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83917,
      "firstName": "David",
      "lastName": "Murphy",
      "middleInitial": "",
      "importedId": "2-fgdf7fKZkyfRAiilK38A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83918,
      "firstName": "Arvin",
      "lastName": "Moshfegh",
      "middleInitial": "",
      "importedId": "R7Cdlh2qMsJ2Kw_0Xjbp7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83919,
      "firstName": "Kamil",
      "lastName": "Koniuch",
      "middleInitial": "",
      "importedId": "EyCnKUvD6ZhqCNaGqwH40g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83920,
      "firstName": "Jeanine",
      "lastName": "van Bruggen",
      "middleInitial": "",
      "importedId": "PLDV_B4x5zPOSvtiJKropA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83921,
      "firstName": "Sylvie",
      "lastName": "Dijkstra-Soudarissanane",
      "middleInitial": "",
      "importedId": "nJqYUo48cCccoIiwimPNJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83922,
      "firstName": "Anthi",
      "lastName": "Papanastasiou",
      "middleInitial": "",
      "importedId": "1QiTYx--twY6cliKGjWmMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83923,
      "firstName": "Stefan",
      "lastName": "Bajin",
      "middleInitial": "",
      "importedId": "TFlAsLt0VWa49zOGHk8EDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83924,
      "firstName": "Nuno",
      "lastName": "Silva",
      "middleInitial": "Tavares",
      "importedId": "xWXA-CgoNELwAOd9CtvJlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83925,
      "firstName": "Antoni",
      "lastName": "Oliver",
      "middleInitial": "",
      "importedId": "57LojtMbmhY_Xr0vTwESXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83926,
      "firstName": "Joanne",
      "lastName": "Parkes",
      "middleInitial": "",
      "importedId": "yEdQ3GJIjJOJ48IwvsKXQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83927,
      "firstName": "Max",
      "lastName": "Fortna",
      "middleInitial": "",
      "importedId": "aaj5cj0BBU_xTSfIT4yZMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83928,
      "firstName": "Talha",
      "lastName": "Khan",
      "middleInitial": "",
      "importedId": "_eIhjWkEH-lsm9ifFU-pHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83929,
      "firstName": "Ellen",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "YJxnRbqcH-hsEDADkpXG1g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83930,
      "firstName": "Nanda",
      "lastName": "van der Stap",
      "middleInitial": "",
      "importedId": "MJAW98lERBbcC2gofatWEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83931,
      "firstName": "Alina",
      "lastName": "Striner",
      "middleInitial": "",
      "importedId": "VYr7S94tdKamy_ISzECL6g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83932,
      "firstName": "Leonor",
      "lastName": "Fermoselle",
      "middleInitial": "",
      "importedId": "f0Tuf_nWtZI5yChDJKBTHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83933,
      "firstName": "Beatrice",
      "lastName": "Fuchs",
      "middleInitial": "",
      "importedId": "pb-kCvPVRXe0po-aGZR79Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83934,
      "firstName": "Joseph C",
      "lastName": "Maroon",
      "middleInitial": "",
      "importedId": "5T792mHmrsNsaO0-T5NnfA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83935,
      "firstName": "Héctor",
      "lastName": "Rivas Pagador",
      "middleInitial": "",
      "importedId": "sXK17nxOe81U2KD56z4XoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83936,
      "firstName": "Sergio",
      "lastName": "Cabrero Barros",
      "middleInitial": "",
      "importedId": "agbIvqFO49Xc3N3ZVYhnUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83937,
      "firstName": "Colin",
      "lastName": "Stricklin",
      "middleInitial": "",
      "importedId": "45zx6Opsh7o-CpeBZd9iYA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83938,
      "firstName": "Jason",
      "lastName": "Mendes",
      "middleInitial": "",
      "importedId": "wNEoUzLP460sywnlWJ-qQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83939,
      "firstName": "Mikel",
      "lastName": "Zorrilla",
      "middleInitial": "",
      "importedId": "gj-pkllsbEdUf4CGKjkksA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83940,
      "firstName": "João",
      "lastName": "Valente",
      "middleInitial": "P.S.S.",
      "importedId": "S69qxxmcxE8KNKHYIdSiDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83941,
      "firstName": "Filip",
      "lastName": "De Turck",
      "middleInitial": "",
      "importedId": "NUh5OGYNHOUg4u6IINZCFw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83942,
      "firstName": "João",
      "lastName": "Lourenço",
      "middleInitial": "",
      "importedId": "6jFPXFVLOapR9VtJ6n2xAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83943,
      "firstName": "Maria",
      "lastName": "Torres Vega",
      "middleInitial": "",
      "importedId": "57YcElOPtgHo1jTDHGGuUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83944,
      "firstName": "Justin",
      "lastName": "Debloos",
      "middleInitial": "",
      "importedId": "fA_ysADhZ7oZyfliDhDC-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83945,
      "firstName": "Javier",
      "lastName": "del Molino",
      "middleInitial": "",
      "importedId": "bbGyorH0OxyoJi868zD1Zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83946,
      "firstName": "Iulia",
      "lastName": "Covalenco",
      "middleInitial": "",
      "importedId": "oC9W7pV0HQcJVlH5oRkdpA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83947,
      "firstName": "Eftychia",
      "lastName": "Protopapadaki",
      "middleInitial": "",
      "importedId": "WiEMxCYyseJZTbZVB49LJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83948,
      "firstName": "Thomas",
      "lastName": "Röggla",
      "middleInitial": "",
      "importedId": "oMts-FgVjiqSOtH5p9_mUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83949,
      "firstName": "Gareth",
      "lastName": "Young",
      "middleInitial": "W.",
      "importedId": "2dESPWlxLdKPLgHNVCbdmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83950,
      "firstName": "Filipe",
      "lastName": "Pires",
      "middleInitial": "",
      "importedId": "pi5b3Q6jY2P2TIgRDlTyYg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83951,
      "firstName": "Kevin",
      "lastName": "Lindén",
      "middleInitial": "",
      "importedId": "W66LV7_82eDPMSUdwum9DQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83952,
      "firstName": "Ewan",
      "lastName": "Johnson",
      "middleInitial": "",
      "importedId": "n_E9D5GvmbJuVGeb71mp_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83953,
      "firstName": "Tiffany",
      "lastName": "Marques",
      "middleInitial": "",
      "importedId": "qLBIcv9tX9xzTr7Aa468_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83954,
      "firstName": "Melina",
      "lastName": "Bernsland",
      "middleInitial": "",
      "importedId": "BhXtdkwHKmz9Z4JpfvSp-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83955,
      "firstName": "Pedro",
      "lastName": "Almeida",
      "middleInitial": "",
      "importedId": "3opM6q2OjBUxsBRTKdZVRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83956,
      "firstName": "Brigitte",
      "lastName": "Le Pévédic",
      "middleInitial": "",
      "importedId": "menatvq7NL9BZqpRrwZrBA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83957,
      "firstName": "Katerina",
      "lastName": "Mania",
      "middleInitial": "",
      "importedId": "P0l049N7eDXT2Nu9dawNiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83998,
      "firstName": "Sylvia",
      "lastName": "Rothe",
      "middleInitial": "",
      "importedId": "jUvEnspDZNP5-5m2xNqnug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 83999,
      "firstName": "Teresa",
      "lastName": "Chambel",
      "middleInitial": "",
      "importedId": "EYDwJKWnGIlLddth3sD03A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 84006,
      "firstName": "Sun Joo (Grace)",
      "lastName": "Ahn",
      "importedId": "1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84007,
      "firstName": "Alvaro",
      "lastName": "Villegas",
      "importedId": "2",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84008,
      "firstName": "Mark",
      "lastName": "Billinghurst",
      "importedId": "3",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84018,
      "firstName": "Patrick",
      "lastName": "Le Callet",
      "importedId": "22",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84019,
      "firstName": "Frederique",
      "lastName": "Krupa",
      "importedId": "23",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84020,
      "firstName": "Sophia",
      "lastName": "Ppali",
      "importedId": "24",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84022,
      "firstName": "Vali",
      "lastName": "Lalioti",
      "importedId": "25",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84023,
      "firstName": "Alexandra",
      "lastName": "Covaci",
      "importedId": "26",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84024,
      "firstName": "Boyd",
      "lastName": "Branch",
      "importedId": "27",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84025,
      "firstName": "Bea",
      "lastName": "Wohl",
      "importedId": "28",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84026,
      "firstName": "Alex",
      "lastName": "Kane",
      "importedId": "29",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84029,
      "firstName": "Rene",
      "lastName": "Kaiser",
      "importedId": "30",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84030,
      "firstName": "Anasol",
      "lastName": "Pena-Rios",
      "importedId": "31",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84031,
      "firstName": "Anselmo",
      "lastName": "Paiva",
      "importedId": "10",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84032,
      "firstName": "Heather",
      "lastName": "Elizabeth Dodds",
      "importedId": "32",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84034,
      "firstName": "Debora",
      "lastName": "Muchaluat-Saade",
      "importedId": "11",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84035,
      "firstName": "Johanna",
      "lastName": "Pirker",
      "importedId": "33",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84036,
      "firstName": "Fatima L. S.",
      "lastName": "Nunes Marques",
      "importedId": "12",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84037,
      "firstName": "Teresa",
      "lastName": "Chambel",
      "importedId": "34",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84038,
      "firstName": "Monica",
      "lastName": "Mendes",
      "importedId": "13",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84039,
      "firstName": "Christian",
      "lastName": "GUtl",
      "importedId": "35",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84040,
      "firstName": "Tiago",
      "lastName": "Bonini Borchartt",
      "importedId": "14",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84041,
      "firstName": "Krzysztof",
      "lastName": "Pietroszek",
      "importedId": "36",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84042,
      "firstName": "Alexandra",
      "lastName": "Covaci",
      "importedId": "15",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84043,
      "firstName": "Estevao",
      "lastName": "Bissoli Saleme",
      "importedId": "16",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84044,
      "firstName": "Celine",
      "lastName": "Jost",
      "importedId": "17",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84045,
      "firstName": "Joel",
      "lastName": "dos Santos",
      "importedId": "18",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84046,
      "firstName": "Gheorghita",
      "lastName": "Ghinea",
      "importedId": "19",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84048,
      "firstName": "Mmuso",
      "lastName": "Mafisa",
      "importedId": "4",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84049,
      "firstName": "Nina",
      "lastName": "Salomons",
      "importedId": "5",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84050,
      "firstName": "Micaela",
      "lastName": "Mantegu",
      "importedId": "6",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84052,
      "firstName": "Krystal",
      "lastName": "Cooper",
      "importedId": "8",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84053,
      "firstName": "Aura",
      "lastName": "Conci",
      "importedId": "9",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84055,
      "firstName": "Lucie",
      "lastName": "Leveque",
      "importedId": "20",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84056,
      "firstName": "Matthieu",
      "lastName": "Perreira Da Silva",
      "importedId": "21",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 84064,
      "firstName": "Tara",
      "lastName": "Collingwoode-William",
      "importedId": "297fc9f1-d550-4f02-a14c-059ef5208efa",
      "source": "SYS",
      "affiliations": [
        {
          "country": "United Kingdom",
          "institution": "Goldsmiths University of London"
        }
      ]
    },
    {
      "id": 84065,
      "firstName": "Mylène",
      "lastName": "Farias",
      "importedId": "458ebfcc-f5bc-425d-ac1d-babf8cf0c951",
      "source": "SYS",
      "affiliations": [
        {
          "country": "Brazil",
          "institution": "University of Brasilia"
        }
      ]
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": true,
    "version": 22,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": true,
    "isRegistrationEnabled": false,
    "publicationDate": "2022-06-24 14:59:34+00"
  }
}