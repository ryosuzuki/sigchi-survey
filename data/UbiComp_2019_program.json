{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10036,
    "startDate": 1567987200000,
    "endDate": 1568332800000,
    "shortName": "UbiComp",
    "name": "UbiComp 2019",
    "year": 2019,
    "fullName": "2019 ACM International Joint Conference on Pervasive and Ubiquitous Computing (UbiComp 2019)",
    "url": "http://ubicomp.org/ubicomp2019/",
    "location": "London, UK",
    "timeZoneOffset": 60,
    "logoUrl": "https://files.sigchi.org/conference/logo/6e96cbd9-ce01-ea21-9761-28d25d481fce.png",
    "timeZoneName": "Europe/London"
  },
  "sponsors": [
    {
      "id": 10062,
      "name": "Nokia Bell Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/96257a34-1a76-71f3-dd64-10f72b49c3cf.png",
      "levelId": 10047,
      "order": 3
    },
    {
      "id": 10063,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/677d6336-8f13-7f26-bf26-c8db54e0b575.png",
      "levelId": 10047,
      "order": 1
    },
    {
      "id": 10064,
      "name": "Huawei",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/9dffbac3-9975-272c-1154-e97873f52d1f.png",
      "levelId": 10047,
      "order": 2
    },
    {
      "id": 10065,
      "name": "Emteq",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/fb80eca7-d99f-1892-dbdb-d30f89eb128f.png",
      "levelId": 10048,
      "order": 4
    },
    {
      "id": 10066,
      "name": "Facebook Reality Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/dd5f56f8-883f-3e36-4c28-97d5e9d8fc60.png",
      "levelId": 10048,
      "order": 5
    },
    {
      "id": 10067,
      "name": "Snap Inc.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/1e2235b8-a55d-3ea3-3146-89dcd3cac284.png",
      "levelId": 10048,
      "order": 6
    },
    {
      "id": 10068,
      "name": "University of Cambridge",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0ac6d364-0f71-80a2-9d6b-fac1bfbf8b68.png",
      "levelId": 10048,
      "order": 7
    },
    {
      "id": 10069,
      "name": "Springer",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/16889bcd-73d6-5513-68a3-8fd6d92b3460.png",
      "levelId": 10049,
      "order": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10047,
      "name": "Silver",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10048,
      "name": "Bronze",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10049,
      "name": "Sponsor",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10038,
      "name": "Sponsors",
      "rank": 4,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10065,
      "name": "Ground Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/f20fd9f9-4883-bc58-ca6a-491ebf1d818c.png",
      "roomIds": [
        10281
      ]
    },
    {
      "id": 10068,
      "name": "3rd Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/07752264-340b-c184-eb72-b39d06d18f8d.png"
    },
    {
      "id": 10070,
      "name": "4th Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/b9ae3b22-fb3e-22e3-1ec3-2f1cc438da1a.png",
      "roomIds": [
        10287,
        10289,
        10293,
        10285,
        10282,
        10286,
        10283
      ]
    },
    {
      "id": 10073,
      "name": "5th Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/d1e22a1b-80d1-b218-59ac-fcb2651b5e4d.png",
      "roomIds": [
        10290
      ]
    },
    {
      "id": 10066,
      "name": "1st Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/2a36c297-5215-0894-3cc7-918bb011851e.png"
    },
    {
      "id": 10067,
      "name": "2nd Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/7712a1e4-9112-5b69-5723-c6abe6b16ee3.png",
      "roomIds": [
        10288,
        10284
      ]
    },
    {
      "id": 10072,
      "name": "6th Floor",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/658c07b1-d894-4cec-2f51-8552bf84d7a7.png",
      "roomIds": [
        10280
      ]
    }
  ],
  "rooms": [
    {
      "id": 10288,
      "name": "Albert",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10287,
      "name": "Westminster",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10292,
      "name": "QEII Centre",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10289,
      "name": "Burns\n",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10293,
      "name": "Wesley\n",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10281,
      "name": "Churchill\n",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10280,
      "name": "Mountbatten",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10290,
      "name": "Windsor\n",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10285,
      "name": "Abbey",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10282,
      "name": "Shelley\n",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10286,
      "name": "St. James",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10283,
      "name": "Wordsworth",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10284,
      "name": "Victoria",
      "typeId": 11437,
      "setup": "Theatre"
    },
    {
      "id": 10291,
      "name": "National History Museum",
      "typeId": 11437,
      "setup": "Theatre"
    }
  ],
  "tracks": [
    {
      "id": 10577,
      "typeId": 11437
    }
  ],
  "contentTypes": [
    {
      "id": 11431,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11432,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11433,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11434,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 11435,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11436,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 11437,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 11438,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 11439,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11440,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    }
  ],
  "timeSlots": [
    {
      "id": 11176,
      "type": "SESSION",
      "startDate": 1568191500000,
      "endDate": 1568192400000
    },
    {
      "id": 11177,
      "type": "SESSION",
      "startDate": 1568192400000,
      "endDate": 1568196900000
    },
    {
      "id": 11178,
      "type": "BREAK",
      "startDate": 1568196900000,
      "endDate": 1568198700000
    },
    {
      "id": 11179,
      "type": "SESSION",
      "startDate": 1568198700000,
      "endDate": 1568205000000
    },
    {
      "id": 11182,
      "type": "BREAK",
      "startDate": 1568025000000,
      "endDate": 1568026800000
    },
    {
      "id": 11183,
      "type": "SESSION",
      "startDate": 1568026800000,
      "endDate": 1568032200000
    },
    {
      "id": 11184,
      "type": "LUNCH",
      "startDate": 1568032200000,
      "endDate": 1568037600000
    },
    {
      "id": 11185,
      "type": "SESSION",
      "startDate": 1568037600000,
      "endDate": 1568043000000
    },
    {
      "id": 11186,
      "type": "BREAK",
      "startDate": 1568043000000,
      "endDate": 1568044800000
    },
    {
      "id": 11187,
      "type": "SESSION",
      "startDate": 1568044800000,
      "endDate": 1568050200000
    },
    {
      "id": 11190,
      "type": "BREAK",
      "startDate": 1568111400000,
      "endDate": 1568113200000
    },
    {
      "id": 11191,
      "type": "SESSION",
      "startDate": 1568113200000,
      "endDate": 1568118600000
    },
    {
      "id": 11192,
      "type": "LUNCH",
      "startDate": 1568118600000,
      "endDate": 1568124000000
    },
    {
      "id": 11193,
      "type": "SESSION",
      "startDate": 1568124000000,
      "endDate": 1568129400000
    },
    {
      "id": 11194,
      "type": "BREAK",
      "startDate": 1568129400000,
      "endDate": 1568131200000
    },
    {
      "id": 11195,
      "type": "SESSION",
      "startDate": 1568131200000,
      "endDate": 1568136600000
    },
    {
      "id": 11196,
      "type": "LUNCH",
      "startDate": 1568205000000,
      "endDate": 1568208600000
    },
    {
      "id": 11197,
      "type": "SESSION",
      "startDate": 1568208600000,
      "endDate": 1568214000000
    },
    {
      "id": 11198,
      "type": "BREAK",
      "startDate": 1568214000000,
      "endDate": 1568215800000
    },
    {
      "id": 11199,
      "type": "SESSION",
      "startDate": 1568215800000,
      "endDate": 1568220300000
    },
    {
      "id": 11200,
      "type": "BREAK",
      "startDate": 1568220300000,
      "endDate": 1568221200000
    },
    {
      "id": 11201,
      "type": "SESSION",
      "startDate": 1568221200000,
      "endDate": 1568224800000
    },
    {
      "id": 11202,
      "type": "SESSION",
      "startDate": 1568364300000,
      "endDate": 1568369700000
    },
    {
      "id": 11203,
      "type": "BREAK",
      "startDate": 1568369700000,
      "endDate": 1568371500000
    },
    {
      "id": 11204,
      "type": "SESSION",
      "startDate": 1568371500000,
      "endDate": 1568376900000
    },
    {
      "id": 11205,
      "type": "LUNCH",
      "startDate": 1568376900000,
      "endDate": 1568380500000
    },
    {
      "id": 11206,
      "type": "SESSION",
      "startDate": 1568380500000,
      "endDate": 1568385900000
    },
    {
      "id": 11207,
      "type": "BREAK",
      "startDate": 1568385900000,
      "endDate": 1568387700000
    },
    {
      "id": 11208,
      "type": "SESSION",
      "startDate": 1568387700000,
      "endDate": 1568391300000
    },
    {
      "id": 11211,
      "type": "SESSION",
      "startDate": 1568278800000,
      "endDate": 1568283300000
    },
    {
      "id": 11212,
      "type": "BREAK",
      "startDate": 1568283300000,
      "endDate": 1568285100000
    },
    {
      "id": 11213,
      "type": "SESSION",
      "startDate": 1568285100000,
      "endDate": 1568291400000
    },
    {
      "id": 11214,
      "type": "LUNCH",
      "startDate": 1568291400000,
      "endDate": 1568295000000
    },
    {
      "id": 11215,
      "type": "SESSION",
      "startDate": 1568295000000,
      "endDate": 1568300400000
    },
    {
      "id": 11216,
      "type": "BREAK",
      "startDate": 1568300400000,
      "endDate": 1568302200000
    },
    {
      "id": 11217,
      "type": "SESSION",
      "startDate": 1568302200000,
      "endDate": 1568306700000
    },
    {
      "id": 11218,
      "type": "BREAK",
      "startDate": 1568306700000,
      "endDate": 1568307600000
    },
    {
      "id": 11219,
      "type": "SESSION",
      "startDate": 1568307600000,
      "endDate": 1568311200000
    },
    {
      "id": 11220,
      "type": "SESSION",
      "startDate": 1568311200000,
      "endDate": 1568316600000
    },
    {
      "id": 11180,
      "type": "SESSION",
      "startDate": 1568017800000,
      "endDate": 1568019600000
    },
    {
      "id": 11181,
      "type": "SESSION",
      "startDate": 1568019600000,
      "endDate": 1568025000000
    },
    {
      "id": 11188,
      "type": "SESSION",
      "startDate": 1568104200000,
      "endDate": 1568106000000
    },
    {
      "id": 11189,
      "type": "SESSION",
      "startDate": 1568106000000,
      "endDate": 1568111400000
    },
    {
      "id": 11221,
      "type": "SESSION",
      "startDate": 1568143800000,
      "endDate": 1568154600000
    },
    {
      "id": 11209,
      "type": "BREAK",
      "startDate": 1568391300000,
      "endDate": 1568391600000
    },
    {
      "id": 11210,
      "type": "SESSION",
      "startDate": 1568391600000,
      "endDate": 1568392800000
    }
  ],
  "sessions": [
    {
      "id": 2416,
      "name": "Session: Social Dynamics",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        6135,
        5307,
        5540,
        5272,
        3035,
        2871,
        4236
      ],
      "timeSlotId": 11213
    },
    {
      "id": 2158,
      "name": "Session: Smartphones And Apps 1",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        7351,
        8165,
        5936,
        7718,
        5110
      ],
      "timeSlotId": 11217
    },
    {
      "id": 2123,
      "name": "Session: Smartphones And Apps 2",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        8008,
        4115,
        6414,
        4046
      ],
      "timeSlotId": 11219
    },
    {
      "id": 2133,
      "name": "Session: Cognition, Attention, And Memory",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        4223,
        5071,
        5270,
        2818,
        6642,
        4237,
        5547
      ],
      "timeSlotId": 11215
    },
    {
      "id": 2474,
      "name": "Session: Wearable Health Sensing",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        4941,
        3347,
        8143,
        6285,
        3871,
        3707
      ],
      "timeSlotId": 11217
    },
    {
      "id": 1680,
      "name": "Session: Driver & Vehicle Behavior",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        4266,
        5649,
        6121,
        4260
      ],
      "timeSlotId": 11219
    },
    {
      "id": 1841,
      "name": "Session: Navigation & Behavior 1",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        3612,
        5803,
        2734,
        4295,
        4346,
        7712
      ],
      "timeSlotId": 11197
    },
    {
      "id": 2258,
      "name": "Session: HMD, Smart Glasses, And Gaze",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        6338,
        8087,
        6434,
        7226,
        4513,
        5481,
        7512
      ],
      "timeSlotId": 11204
    },
    {
      "id": 1445,
      "name": "Session: Sports & Fitness",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        4505,
        6218,
        2796,
        2843,
        4964,
        7018
      ],
      "timeSlotId": 11208
    },
    {
      "id": 2269,
      "name": "Session: Mobile Health",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        5475,
        3825,
        5177,
        4636
      ],
      "timeSlotId": 11201
    },
    {
      "id": 1235,
      "name": "Session: Localization Techniques",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        4367,
        3340,
        4423,
        3905
      ],
      "timeSlotId": 11201
    },
    {
      "id": 2039,
      "name": "Session: Authentication 2",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        3391,
        5047,
        3237,
        8128
      ],
      "timeSlotId": 11208
    },
    {
      "id": 1025,
      "name": "Session: Camera-Based Systems",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        3826,
        4724,
        6858,
        6793,
        6269,
        3554
      ],
      "timeSlotId": 11215
    },
    {
      "id": 1040,
      "name": "Session: Navigation & Behavior 2",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        4334,
        2925,
        3447,
        8061,
        4884
      ],
      "timeSlotId": 11217
    },
    {
      "id": 1861,
      "name": "Session: Power",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        7559,
        5702,
        5333,
        2922,
        6316
      ],
      "timeSlotId": 11208
    },
    {
      "id": 2491,
      "name": "Session: Mental Health",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        4047,
        4597,
        6678,
        4859,
        6011
      ],
      "timeSlotId": 11199
    },
    {
      "id": 2129,
      "name": "Session: Authentication 1",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        7309,
        4522,
        4244,
        4076,
        5454,
        4413
      ],
      "timeSlotId": 11204
    },
    {
      "id": 1177,
      "name": "Session: Localization With Vision And Light",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        2982,
        7812,
        6208,
        4828
      ],
      "timeSlotId": 11219
    },
    {
      "id": 2023,
      "name": "Session: Ubicomp @ Work",
      "typeId": 11437,
      "roomId": 10281,
      "chairIds": [
        14765
      ],
      "contentIds": [
        3117,
        7080,
        6399,
        5699,
        4551,
        4614,
        6444
      ],
      "timeSlotId": 11179
    },
    {
      "id": 1206,
      "name": "Session: Geography & Environment",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        3602,
        4544,
        4794,
        4858,
        5587,
        7883,
        5985
      ],
      "timeSlotId": 11213
    },
    {
      "id": 2196,
      "name": "Session: Tangible Interactions",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        5327,
        5831,
        5383,
        5533,
        3746,
        6158,
        2841,
        4541
      ],
      "timeSlotId": 11202
    },
    {
      "id": 2018,
      "name": "Session: IoT And Data For Health",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        4216,
        7065,
        5611,
        7277,
        4114,
        7936
      ],
      "timeSlotId": 11206
    },
    {
      "id": 1270,
      "name": "Session: Acoustic And Sound Sensing",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        6386,
        7101,
        8026,
        7113,
        3044
      ],
      "timeSlotId": 11199
    },
    {
      "id": 1102,
      "name": "Session: Vehicle Data & Management",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        7298,
        4026,
        6124,
        3155,
        3739,
        3427
      ],
      "timeSlotId": 11204
    },
    {
      "id": 1665,
      "name": "Session: IoT & Society",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        4581,
        7915,
        3115,
        7089,
        6814,
        7501,
        7302
      ],
      "timeSlotId": 11179
    },
    {
      "id": 2386,
      "name": "Session: Privacy Attacks",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        5244,
        6559,
        7207,
        3177,
        5167,
        7421
      ],
      "timeSlotId": 11202
    },
    {
      "id": 1085,
      "name": "Session: Wearable Sensing",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        7968,
        7984,
        3302,
        4931,
        7148,
        5794
      ],
      "timeSlotId": 11206
    },
    {
      "id": 1749,
      "name": "Session: Approaches And Algorithms For Activity Recognition",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        6191,
        5993,
        4655,
        7402,
        6643,
        4291,
        5536
      ],
      "timeSlotId": 11213
    },
    {
      "id": 1399,
      "name": "Session: Interaction Paradigms",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        4899,
        6602,
        6905,
        3316,
        3996,
        7700
      ],
      "timeSlotId": 11197
    },
    {
      "id": 1324,
      "name": "Session: Haptics & Kinetics",
      "roomId": 10280,
      "chairIds": [],
      "contentIds": [
        6021,
        7260,
        7957,
        7562,
        3872,
        4206
      ],
      "timeSlotId": 11199
    },
    {
      "id": 1368,
      "name": "Session: E-Textiles And Flexible Electronics",
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [
        6406,
        4535,
        5013,
        6113,
        3460,
        6796,
        4320,
        5050
      ],
      "timeSlotId": 11215
    },
    {
      "id": 2149,
      "name": "Session: Non-Contact Activity Recognition",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        3489,
        5227,
        7636,
        4160,
        4751,
        7770,
        3165
      ],
      "timeSlotId": 11179
    },
    {
      "id": 1103,
      "name": "Session: Preserving Privacy",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        6327,
        5575,
        4415,
        7672,
        4392,
        3741
      ],
      "timeSlotId": 11197
    },
    {
      "id": 1195,
      "name": "Session: Family & Technology",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        5958,
        5946,
        5335,
        7135
      ],
      "timeSlotId": 11201
    },
    {
      "id": 1701,
      "name": "Session: Gait And Walking",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        3894,
        7896,
        8106,
        4369,
        2807,
        7514
      ],
      "timeSlotId": 11202
    },
    {
      "id": 1129,
      "name": "Session: Sound, Speech, And Hearing",
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [
        3553,
        6137,
        4171,
        4024,
        7972,
        7524
      ],
      "timeSlotId": 11206
    }
  ],
  "events": [
    {
      "id": 2556,
      "name": "Drinks reception, gadget show, awards for ISWC Design exhibit, and Design Exhibition at the QEII Centre",
      "typeId": 11433,
      "roomId": 10292,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568311200000,
      "endDate": 1568316600000,
      "presenterIds": []
    },
    {
      "id": 2648,
      "name": "EyeWear 2019: Third Workshop on EyeWear Computing",
      "typeId": 11433,
      "roomId": 10286,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568021400000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2598,
      "name": "CPD 2019: The 2nd Workshop on Combining Physical and Data-Driven Knowledge in Ubiquitous Computing",
      "typeId": 11433,
      "roomId": 10284,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568021400000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2650,
      "name": "SCAH 2019: Addressing Grand Challenges in Healthcare through Smart Clothing",
      "typeId": 11433,
      "roomId": 10282,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568107800000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2569,
      "name": "Mental Health and Well-being: Sensing and Intervention",
      "typeId": 11433,
      "roomId": 10286,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568107800000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2632,
      "name": "Conference Opening",
      "typeId": 11433,
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568191500000,
      "endDate": 1568192400000,
      "presenterIds": []
    },
    {
      "id": 2593,
      "name": "1st International Workshop on Earable Computing (EarComp 2019)",
      "typeId": 11433,
      "roomId": 10287,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568106000000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2609,
      "name": "UBITTENTION 2019: 4th International Workshop on Smart and Ambient Notification and Attention Management",
      "typeId": 11433,
      "roomId": 10283,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568107800000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2642,
      "name": "AppLens 2019: The 2nd Workshop on Mining and Learning from Smartphone Apps for Users",
      "typeId": 11433,
      "roomId": 10285,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568124000000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2611,
      "name": "The Uncomfortable Workshop: Exploring Discomfort Design for Wellbeing and Sustainability",
      "typeId": 11433,
      "roomId": 10288,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568107800000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2604,
      "name": "Beyond Individuals: Exploring Social Experience Around Wearables",
      "typeId": 11433,
      "roomId": 10289,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568107800000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2631,
      "name": "UPA’19: 4th International Workshop on Ubiquitous Personal Assistance",
      "typeId": 11433,
      "roomId": 10288,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568021400000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2610,
      "name": "Tutorial: Modeling Human Behavior Via Inverse Reinforcement Learning",
      "typeId": 11433,
      "roomId": 10287,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568019600000,
      "endDate": 1568032200000,
      "presenterIds": []
    },
    {
      "id": 2558,
      "name": "Keynote 2: Liberating Technologies to Venture into the real world -- Lama Nachman, Intel Fellow, Director of Anticipatory Computing Lab",
      "typeId": 11433,
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568278800000,
      "endDate": 1568283300000,
      "description": "Over the last decade, we have seen amazing adoption of sensing into our daily lives, from smart phones, to wearables to virtual home assistants, to many others.  However, while sensing has indeed become ubiquitous, technology continues to be highly dependent on us, asking for input, expecting us to provide every detail and bombarding us with irrelevant information.  The Ubicomp community has been at the forefront of creating context aware technologies that are needed to liberate technology to become more proactive and take on more agency, but much more is needed.  In this talk, I will discuss some of the work we have done in assistive computing and the Stephen Hawking project, the challenges in getting these technologies out into the world, and highlight some of the interesting research problems that I see moving forward.",
      "presenterIds": []
    },
    {
      "id": 2568,
      "name": "Tutorial: Smartphone App Usage, Understanding, Modelling, And Prediction",
      "typeId": 11433,
      "roomId": 10285,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568106000000,
      "endDate": 1568118600000,
      "presenterIds": []
    },
    {
      "id": 2666,
      "name": "Conference Closing",
      "typeId": 11433,
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568391600000,
      "endDate": 1568392800000,
      "presenterIds": []
    },
    {
      "id": 2545,
      "name": "Keynote 1: Machine learning models for ubiquitous systems with safety and reliability guarantees -- Marta Kwiatkowska, University of Oxford",
      "typeId": 11433,
      "roomId": 10281,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568192400000,
      "endDate": 1568196900000,
      "description": "Computing devices support us in almost all everyday tasks, from smartphones and wearable devices, to self-driving cars and robots. Driven by applications in health and behavioural monitoring, as well as affective computing, there is a growing demand for computational models that are able to accurately predict multimodal features from a multitude of sensor data. While machine learning models excel at identifying features in physiological signals, they lack reliability guarantees and need to be adapted to the user. Using illustrative examples, this lecture will give an overview of modelling and personalisation techniques and their role in a variety of applications, including medical devices, biometric security, cobotics and self-driving cars. It will also explore the problems of ensuring that systems that rely on learning will behave correctly, both in situations that they have seen in training, and in situations that they haven’t. ",
      "presenterIds": []
    },
    {
      "id": 2662,
      "name": "Doctoral Colloquium",
      "typeId": 11433,
      "roomId": 10293,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568105400000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2613,
      "name": "Drinks and Networking Event at the Natural History Museum",
      "typeId": 11433,
      "roomId": 10291,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568143800000,
      "endDate": 1568154600000,
      "description": "http://ubicomp.org/ubicomp2019/attending_reception.html",
      "presenterIds": []
    },
    {
      "id": 2670,
      "name": "LDC 2019: Workshop on Longitudinal Mobile, Wearable and Ubiquitous Data Collection from Human Subject Studies",
      "typeId": 11433,
      "roomId": 10282,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568037600000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2600,
      "name": "WellComp'19: 2nd International Workshop on Computing for Well-being",
      "typeId": 11433,
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568019600000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2643,
      "name": "PURBA 2019: The 8th Workshop on Pervasive Urban Applications",
      "typeId": 11433,
      "roomId": 10283,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568021400000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2667,
      "name": "HASCA 2019: 7th International Workshop on Human Activity Sensing Corpus and Applications",
      "typeId": 11433,
      "roomId": 10290,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568106000000,
      "endDate": 1568136600000,
      "presenterIds": []
    },
    {
      "id": 2626,
      "name": "Continual and Multimodal Learning for Internet of Things",
      "typeId": 11433,
      "roomId": 10285,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568021400000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2671,
      "name": "Tutorial: Data Visualization For UbiComp And ISWC Research",
      "typeId": 11433,
      "roomId": 10287,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568037600000,
      "endDate": 1568050200000,
      "presenterIds": []
    },
    {
      "id": 2619,
      "name": "Tutorial: Eyewear Computing In The Wild",
      "typeId": 11433,
      "roomId": 10284,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1568124000000,
      "endDate": 1568136600000,
      "presenterIds": []
    }
  ],
  "contents": [
    {
      "id": 6149,
      "typeId": 11437,
      "title": "From Sensing to Intervention for Mental and Behavioral Health",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Doctorial Colloquium",
      "authors": [
        {
          "affiliations": [],
          "personId": 8902
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7174,
      "typeId": 11437,
      "title": "AppLens 2019: the 2nd International Workshop on Mining and Learning from Smartphone Apps for Users",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphone apps are becoming ubiquitous in our everyday life. Apps on smartphones sense users' behaviors and activities, providing a lens for understanding users, which is an important point in the community of ubiquitous computing. In UbiComp 2018, we successfully held the first International workshop AppLens 2018: mining and learning from smartphone apps for users. In UbiComp 2019, we would like to run the second International workshop AppLens 2019. It seeks for participants interested in characterizing users from their use of smartphone apps, discovering cultural and social phenomenon by analyzing app usage, recognizing app usage behaviors, studying smartphone apps, user privacy issues, etc. In order to attract more participants, we will open two app datasets. This workshop will include paper sessions, invited talks, and a panel session, to provide a forum for the participants to communicate and discuss issues to promote the emerging research field. Moreover, we will select a few accepted papers to be extended and published in a prestigious journal special issue.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15278
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 9291
        },
        {
          "affiliations": [],
          "personId": 12522
        },
        {
          "affiliations": [],
          "personId": 18197
        },
        {
          "affiliations": [],
          "personId": 12400
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7176,
      "typeId": 11437,
      "title": "Secure Communication Protocol for Smart Transportation based on Vehicular Cloud",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The pioneering concept of connected vehicles has transformed the way of thinking for researchers and entrepreneurs by collecting relevant data from nearby objects. However, this data is useful for a specific vehicle only. Moreover, vehicles get a high amount of data (e.g., traffic, safety, and multimedia infotainment) on the road. Thus, vehicles expect adequate storage device for this data, but it is infeasible to have a large memory in each vehicle. Hence, the vehicular cloud computing (VCC) framework came into the picture to provide a storage facility by connecting a road-side-unit (RSU) with the vehicular cloud (VC). In this, data should be saved in an encrypted form to preserve security, but there is a challenge to search for information over encrypted data. Next, we understand that many of vehicular communication schemes are inefficient for data transmissions due to its poor performance results and vulnerable to different fundamental security attacks. Accordingly, on-device performance is critical, but data damages and secure on-time connectivity are also significant challenges in a public environment. Therefore, we propose reliable data transmission protocols for cutting-edge architecture to search data from the storage, to resist against various security attacks, and provide better performance results. Thus, the proposed data transmission protocol is useful in diverse smart city applications (business, safety, and entertainment) for the benefits of society.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9212
        },
        {
          "affiliations": [],
          "personId": 23950
        },
        {
          "affiliations": [],
          "personId": 16655
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7178,
      "typeId": 11437,
      "title": "Poster: Neural Network-based Indoor Tag-less Localization Using Capacitive Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Many applications aim to make smarter the indoor environments where most people spend much of their time (home, office, transportation, public spaces), but they need long-term low-cost human sensing and monitoring capabilities. Small capacitive sensors match well most requirements, like privacy, power, cost, and unobtrusiveness, and, importantly, they do not rely on wearables or specific human interactions. However, long-range capacitive sensors often need advanced data processing to increase their performance. Our ongoing research experimental results show that four 16 cm ⨉ 16 cm capacitive sensors deployed in a 3 m ⨉ 3 m room can tag-lessly track the movement of a person with a root mean square error as low as 26 cm. Our system uses a median and low-pass filter for sensor signal conditioning before an autoregressive neural network that we trained to infer the location of the person in the room.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10387
        },
        {
          "affiliations": [],
          "personId": 20152
        },
        {
          "affiliations": [],
          "personId": 20226
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6158,
      "typeId": 11437,
      "title": "A Wrist-worn Motion Evaluation System for Fast and Powerful Down Picking of Heavy Metal Guitar",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A fast and powerful down picking skill of rhythm guitar is crucial for heavy metal songs. At more than 400 beats per minute, which is a typical starting point for heavy metal songs, most players feel hard to keep the picking rhythm. In this paper, we propose a wrist-worn inertial motion tracking device for analyzing the guitar picking motion. A population test showed that a wrist twist angular velocity signal would give sufficient information on the picking timing to evaluate the accuracy of rhythm. It was also found that the power of sound had a strong correlation with the wrist twist angle and the upward linear acceleration signals. We developed a picking evaluation system by using characteristic motion parameters that informs the player of the speed, power, and smoothness of the picking motion. As a result, five out of eight beginners to play heavy metal guitar reached the speed of 400 beats per minute in a short training period.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12550
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 7183,
      "typeId": 11437,
      "title": "M3B Corpus: Multi-Modal Meeting Behavior Corpus for Group Meeting Assessment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper is the first trial to create a corpus on human-to-human multi-modal communication among multiple persons in group discussions. Our corpus includes not only video conversations but also the head movement and eye gaze. In addition, it includes detailed labels about the behaviors appeared in the discussion. Since we focused on the micro-behavior, we classified the general behavior into more detailed behaviors based on those meaning. For example, we have four types of smile: response, agree, interesting, sympathy. Because it takes much effort to create such corpus having multiple sensor data and detailed labels, it seems that no one has created it. In this work, we first attempted to create a corpus called M3B Corpus Multi-Modal Meeting Behavior Corpus,'' which includes 320 minutes discussion among 21 Japanese students in total by developing the recording system that can handle multiple sensors and 360-degree camera simultaneously and synchronously. In this paper, we introduce our developed recording system and report the detail of M3B Corpus.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9533
        },
        {
          "affiliations": [],
          "personId": 17636
        },
        {
          "affiliations": [],
          "personId": 22901
        },
        {
          "affiliations": [],
          "personId": 12914
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3090,
      "typeId": 11437,
      "title": "Summary of the Sussex-Huawei Locomotion-Transportation Recognition Challenge 2019",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we summarize the contributions of participants to the Sussex-Huawei Transportation-Locomotion (SHL) Recognition Challenge organized at the HASCA Workshop of UbiComp 2019. The goal of this machine learning/data science challenge is to recognize eight locomotion and transportation activities (Still, Walk, Run, Bike, Bus, Car, Train, Subway) from the inertial sensor data of a smartphone in a placement independent manner. The training data is collected with smartphones placed at three body positions (Torso, Bag and Hips), while the testing data is collected with a smartphone placed at another body position (Hand). We introduce the dataset used in the challenge and the protocol for the competition. We present a meta-analysis of the contributions from 14 submissions, their approaches, the software tools used, computational cost and the achieved results. Overall, three submissions achieved F1 scores between 70% and 80%, five with F1 scores between 60% and 70%, five between between 50% and 60%, and one below 50%, with a latency of a maximum of 5 seconds.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10924
        },
        {
          "affiliations": [],
          "personId": 15053
        },
        {
          "affiliations": [],
          "personId": 20416
        },
        {
          "affiliations": [],
          "personId": 18107
        },
        {
          "affiliations": [],
          "personId": 15153
        },
        {
          "affiliations": [],
          "personId": 19993
        },
        {
          "affiliations": [],
          "personId": 14916
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4114,
      "typeId": 11437,
      "title": "PocketCare: Tracking the Flu with Mobile Phones Using Partial Observations of Proximity and Symptoms",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile phones provide a powerful sensing platform that researchers may adopt to understand proximity interactions among people and the diffusion, through these interactions, of diseases, behaviors, and opinions. However, it remains a challenge to track the proximity-based interactions of a whole community and then model the social diffusion of diseases and behaviors starting from the observations of a small fraction of the volunteer population. In this paper, we propose a novel approach that tries to connect together these sparse observations using a model of how individuals interact with each other and how social interactions happen in terms of a sequence of proximity interactions. We apply our approach to track the spreading of flu in the spatial-proximity network of a 3000-people university campus by mobilizing 300 volunteers from this population to monitor nearby mobile phones through Bluetooth scanning and to daily report flu symptoms about and around them. Our aim is to predict the likelihood for an individual to get flu based on how often her/his daily routine intersects with those of the volunteers. Thus, we use the daily routines of the volunteers to build a model of the volunteers as well as of the non-volunteers. Our results show that we can predict flu infection two weeks ahead of time with an average precision from 0.24 to 0.35 depending on the amount of information. This precision is six to nine times higher than with a no-skill model. At the population level, we can predict infectious population in a two-week window with an r-squared value of 0.95 (a no-skill model obtains an r-squared value of 0.2). These results point to an innovative approach for tracking individuals who have interacted with people showing symptoms, allowing us to warn those in danger of infection and to inform health researchers about the progression of contact-induced diseases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8829
        },
        {
          "affiliations": [],
          "personId": 19601
        },
        {
          "affiliations": [],
          "personId": 23213
        },
        {
          "affiliations": [],
          "personId": 16758
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 4115,
      "typeId": 11437,
      "title": "Modeling and Forecasting the Popularity Evolution of Mobile Apps: A Multivariate Hawkes Process Approach",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In recent years, with the rapid development of mobile app ecosystem, the number and categories of mobile apps have grown tremendously. However, the global prevalence of mobile apps also leads to fierce competition. As a result, many apps will disappear. To thrive in this competitive app market, it is vital for app developers to understand the popularity evolution of their mobile apps, and inform strategic decision-making for better mobile app development. Therefore, it is significant and necessary to model and forecast the future popularity evolution of mobile apps. The popularity evolution of mobile apps is usually a long-term process, affected by various complex factors. However, existing work lacks the capabilities to model such complex factors. To better understand the popularity evolution, in this paper, we aim to forecast the popularity evolution of mobile apps by incorporating complex factors, i.e., exogenous stimulis and endogenous excitations. Specifically, we propose the Multivariate Hawkes Process (MHP), which is an exogenous stimulis-driven self-exciting point process, to model the exogenous stimulis and endogenous excitations simultaneously. Extensive experimental studies on a real-world dataset from app store demonstrate that MHP outperforms the state-of-the-art methods regarding popularity evolution forecasting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10479
        },
        {
          "affiliations": [],
          "personId": 13318
        },
        {
          "affiliations": [],
          "personId": 14376
        },
        {
          "affiliations": [],
          "personId": 14351
        },
        {
          "affiliations": [],
          "personId": 12522
        }
      ],
      "sessionIds": [
        2123
      ],
      "eventIds": []
    },
    {
      "id": 4118,
      "typeId": 11437,
      "title": "POIDEN: Position and Orientation Independent Deep Ensemble Network for the Classification of Locomotion and Transportation Modes",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sensor-based recognition of locomotion and transportation modes has numerous application domains including urban traffic monitoring, transportation planning, and healthcare. However, the use of a smartphone in a fixed position and orientation in previous research works limited the user behavior a lot. Besides, the performance of naive methods for position-independent cases was not up to the mark. In this research, we have designed a position and orientation independent deep ensemble network (POIDEN) to classify eight modes of locomotion and transportation activities. The proposed POIDEN architecture is constructed of a Recurrent Neural Network (RNN) with LSTM that is assigned the task of selecting optimum general classifiers (random forest, decision tree, gradient boosting, etc.) to classify the activity labels. We have trained the RNN architecture using an intermediate feature set (IFS), whereas, the general classifiers have been trained using a statistical classifier feature set (SCFS). The choice of a classifier by RNN is dependent upon the highest probability of those classifiers to recognize particular activity samples. We have also utilized the rotation of acceleration and magnetometer values from phone coordinate to earth coordinate, proposed jerk feature, and position insensitive features along with parameter adjustment to make the POIDEN architecture position and orientation independent. Our team ``Gradient Descent'' has presented this work for the ``Sussex-Huawei Locomotion-Transportation (SHL) recognition challenge''.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12683
        },
        {
          "affiliations": [],
          "personId": 21066
        },
        {
          "affiliations": [],
          "personId": 21030
        },
        {
          "affiliations": [],
          "personId": 24129
        },
        {
          "affiliations": [],
          "personId": 22799
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7191,
      "typeId": 11437,
      "title": "The Uncomfortable Workshop: Exploring Discomfort Design for Wellbeing and Sustainability",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "According to the latest science of human performance, we are wired to thrive and adapt from discomfort. This workshop explores how to leverage that science to improve human wellbeing and to improve sustainability as a side-effect of designing ubiquitous technology to prepare, practice and perform discomfort, for social benefit. We will use Design Jams as a key activity to explore and build up this Uncomfortable Design Methodology. There will be prizes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21407
        },
        {
          "affiliations": [],
          "personId": 8446
        },
        {
          "affiliations": [],
          "personId": 10519
        },
        {
          "affiliations": [],
          "personId": 17243
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6171,
      "typeId": 11437,
      "title": "Pervasive Augmented Reality for Indoor Uninterrupted Experiences: a User Study",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented Reality (AR) adds additional layers of information on top of real environments. Recently, Pervasive AR extends this concept through an AR experience that is continuous in space, being aware of and responsive to the user’s context and pose (position and orientation). This paper focus on an exploratory user study with 27 participants meant to better understand some aspects of Pervasive AR, such as how users explore, select, recognize and manipulate virtual content in uninterrupted AR experiences, as well as their preferences. The approach used to provide this sort of engaging experiences allows the creation of indoor persistent location-based experiences, with a high level of accuracy and resilience to changes in dynamic environments. Results concerning user acceptance of uninterrupted AR experiences were encouraging. In particular, users were positively impressed by the continuous display of virtual content and were willing to use this technology more often and in different contexts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14926
        },
        {
          "affiliations": [],
          "personId": 10471
        },
        {
          "affiliations": [],
          "personId": 10151
        },
        {
          "affiliations": [],
          "personId": 24210
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7204,
      "typeId": 11437,
      "title": "Appraisal theory-based mobile app for physiological data collection and labelling in the wild",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Numerous studies on emotion recognition from physiological signals have been conducted in laboratory settings. However, differences in the data on emotions elicited in the lab and in the wild have been observed. Thus, there is a need for systems collecting and labelling emotion-related physiological data in ecological settings. This paper proposes a new solution to collect and label such data: an open-source mobile application (app) based on the appraisal theory.Our approach exploits a commercially available wearable physiological sensor connected to a smartphone. The app detects relevant events from the physiological data, and prompts the users to report their emotions using a questionnaire based on the Ortony, Clore and Collins (OCC) Model. \nWe believe that the app can be used to collect emotional and physiological data in ecological settings and to ensure high quality of ground truth labels.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23157
        },
        {
          "affiliations": [],
          "personId": 16257
        },
        {
          "affiliations": [],
          "personId": 16824
        },
        {
          "affiliations": [],
          "personId": 11813
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3110,
      "typeId": 11437,
      "title": "Audio Augmented Reality for Human-Object Interactions",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In the past, augmented reality (AR) research focused mostly on visual augmentation, which requires a visual rendering device like head-mounted displays that are usually obtrusive, expensive, and socially unaccepted. In contrast, wearable audio headsets are already popularized and the auditory sense also plays an important role in everyday interactions with the environment. In this PhD project, we explore audio augmented reality (AAR) that augments objects with 3D sounds, which are spatialized virtually but are perceived as originating from real locations in the space. We intend to design, implement, and evaluate such AAR systems that enhance people's intuitive and immersive interactions with objects in various consumer and industrial scenarios. By exploring AAR using pervasive and wearable devices, we hope to contribute to the vision of ubiquitous AR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16632
        },
        {
          "affiliations": [],
          "personId": 20671
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7207,
      "typeId": 11437,
      "title": "Invisible QR Code Hijacking Using Smart LED",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Quick response (QR) codes have found versatile usage in numerous applications, but have also posed severe security threats such as privacy leakage, phishing and even payment inception if the codes are hijacked. The hijacking is often assumed to be\npreventable by physically isolating the codes from possible attackers, e.g., putting the QR code inside a glass cabinet distant to outsiders. In this paper, we explore a new QR code hijacking attack, named Li-Man, that can subvert such protection using smart LED. The key idea is to illuminate a target victim QR code from afar using specialized flickering light waveforms, which can transform the code to be any other predefined malicious ones when being captured by smart-phone cameras, while keeping the attack invisible to human visual perception. Li-Man builds on a modeling framework that harnesses the disparity between camera and human imaging mechanisms. We develop a Li-Man simulator and also implement a prototype to verify the feasibility and threat level of Li-Man. Experiments demonstrate that Li-Man can successfully realize the invisible hijacking of QR codes from multiple hidden positions in constrained space. On the other hand, we propose and verify a primary countermeasure that is promising to defeat the Li-Man attack.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19169
        },
        {
          "affiliations": [],
          "personId": 21507
        },
        {
          "affiliations": [],
          "personId": 9178
        },
        {
          "affiliations": [],
          "personId": 15918
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 5160,
      "typeId": 11437,
      "title": "Harnessing Digital Phenotyping to deliver real-time Interventional Bio-Feedback",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the decreasing cost and increasing capability of sensor and mobile technology along with the proliferation of data from social media, ambient environment and other sources, new concepts for digital prognostic and technological quantification of well-being are emerging. These concepts are referred to as digital phenotyping. One of the main challenges facing the development of these technologies is the design of easy to use and personalised devices which benefit from interventional feedback by leveraging on-device processing in real-time.  Tangible interfaces designed for well-being possess the capabilities to reduce anxiety or manage panic attacks, thus improving the quality of life for both the general population and vulnerable members of society. Real-time bio-feedback paired with Artificial Intelligence (AI) presents new opportunities for mental well-being to be inferred allowing individually personalised interventional feedback to be automatically applied. This research explores future directions for bio-feedback including the opportunity to fuse multiple AI enabled feedback mechanisms that can then be utilised collectively or individually.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14739
        },
        {
          "affiliations": [],
          "personId": 10700
        },
        {
          "affiliations": [],
          "personId": 9276
        },
        {
          "affiliations": [],
          "personId": 17833
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3115,
      "typeId": 11437,
      "title": "The Connected Shower: Studying Intimate Data in Everyday Life",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents the design and field study of the Connected Shower. The Connected Shower is a bespoke IoT device that captures water flow, temperature, showerhead movement, and product weight. We deployed the device in six UK homes for a week to understand the use of `intimate data' in IoT systems. Findings from our contextual interviews unpack a) how intimate data is collaboratively made sense of by accounting for the social order of showering practices as part and parcel of everyday routines; b) how the data makes details of showering accountable to their partners; c) how people reason about sharing intimate data both with third parties and their partners. Our study shows that intimate data is not intimate per se, nor is intimacy a property of the data, but is an interactional outcome arising from the articulation of shower practices to their co-present partners. Thus, judgments whether the data is too sensitive, private, or intimate to share, are contingent on situated sense-making, therefore subject to change; however, there was a general consensus that sharing intimate data with service providers was acceptable if the data was sufficiently abstract and anonymised. We discuss challenges in the design of trustworthy data-driven IoT systems, and how they need to be warranted to be acceptable and adopted into our intimate practices. \\",
      "authors": [
        {
          "affiliations": [],
          "personId": 19451
        },
        {
          "affiliations": [],
          "personId": 18787
        },
        {
          "affiliations": [],
          "personId": 18324
        },
        {
          "affiliations": [],
          "personId": 18833
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 5164,
      "typeId": 11437,
      "title": "Designing interactive interfaces by keeping the natural beauty of public places",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Natural, public places in cities serve often as a place for recreation and relaxation. Additionally, such places often signify historic and social importance about which visitors would like to know more. Screens and other currently existing technology would, however, destroy the natural beauty of such a place. Attention-aware and unobtrusive interfaces seem to offer a solution to this problem. In our approach, we conducted a qualitative survey with 19 people and an epoché at a decommissioned cemetery which is mainly used for recreation and leisure time. Overall, the results show that the majority would like to know more about the deceased with the information closely placed to the grave, but without disturbing the natural, mystical atmosphere of the cemetery. In this work in progress report, we present our research approach to attention-aware, unobtrusive and context-sensitive interactive prototypes that keep the natural beauty and recreational characteristics of such a place.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9807
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4140,
      "typeId": 11437,
      "title": "Non-contact Thermal Sensing on Acoustic-enabled IoT Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Temperature is an important source of information for\npersonal health, weather forecasting, and thermal management in buildings and computing infrastructure, etc.\nTemperature measurement can be achieved via low-cost hardware with reasonable\naccuracy. However, large-scale distributed thermal sensing is non-trivial and\ncurrently relies on dedicated hardware. In this\npaper, we design the first software sonic thermometer (SST) using\ncommercial-off-the-shelf acoustic-enabled devices.\nWe utilize on-board dual microphones on commodity mobile devices to\nestimate sound speed, which has a known relationship with\ntemperature.\nSST is portable, contactless, and cost-effective, making it suitable for ubiquitous\nsensing. We implement SST on an Android smartphone and an acoustic\nsensing platform. Evaluation results show that SST can achieve a median\naccuracy of ${0.5^\\circ \\mathrm{C}}$ even at varying humidity levels.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21013
        },
        {
          "affiliations": [],
          "personId": 19237
        },
        {
          "affiliations": [],
          "personId": 15710
        },
        {
          "affiliations": [],
          "personId": 17947
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6189,
      "typeId": 11437,
      "title": "DeepSleep: A Ballistocardiographic Deep Learning Approach for Classifying Sleep Stages",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Current techniques for tracking sleep are either obtrusive (Polysomnography) or low in accuracy (wearables). In this early work, we model a sleep classification system using an unobtrusive Ballistocardiographic (BCG)-based heart sensor signal collected from a commercially available pressure-sensitive sensor sheet. We present DeepSleep, a hybrid deep neural network architecture comprising of CNN and LSTM layers. We further employed a 2-phase training strategy to build a pre-trained model and to tackle the limited dataset size. Our model results in a classification accuracy of 74%, 82%, 77% and 63% using Dozee BCG, MIT-BIH's ECG, Dozee's ECG and Fitbit's PPG datasets, respectively. Furthermore, our model shows a positive correlation (r=0.43) with the SATED perceived sleep quality scores. We show that BCG signals are effective for long-term sleep monitoring, but currently not suitable for medical diagnostic purposes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22230
        },
        {
          "affiliations": [],
          "personId": 19658
        },
        {
          "affiliations": [],
          "personId": 21268
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3117,
      "typeId": 11437,
      "title": "Differentiating Higher and Lower Job Performers in the Workplace Using Mobile Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Assessing performance in the workplace typically relies on subjective evaluations, such as, peer ratings, supervisor ratings and self-assessments, which are manual, burdensome and potentially biased and unreliable. We use objective mobile sensing data from phones, wearables and beacons to study workplace performance and offer new insights into behavioral patterns that distinguish higher and lower performers including roles (i.e., supervisors and non-supervisors) and different types of companies (e.g., high tech and consultancy). We present initial results from an ongoing year-long study of N=554 information-workers collected over a period ranging from 2-8.5 months. We train a gradient boosting classifier that can classify workers as higher or lower performers with AUROC of 0.83. Our work opens the way to new forms of passive objective assessment and feedback to workers to potentially provide week by week, quarter by quarter guidance in the workplace.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10636
        },
        {
          "affiliations": [],
          "personId": 23891
        },
        {
          "affiliations": [],
          "personId": 11718
        },
        {
          "affiliations": [],
          "personId": 12255
        },
        {
          "affiliations": [],
          "personId": 20108
        },
        {
          "affiliations": [],
          "personId": 14192
        },
        {
          "affiliations": [],
          "personId": 17570
        },
        {
          "affiliations": [],
          "personId": 14713
        },
        {
          "affiliations": [],
          "personId": 21112
        },
        {
          "affiliations": [],
          "personId": 18197
        },
        {
          "affiliations": [],
          "personId": 23107
        },
        {
          "affiliations": [],
          "personId": 22638
        },
        {
          "affiliations": [],
          "personId": 11012
        },
        {
          "affiliations": [],
          "personId": 8930
        },
        {
          "affiliations": [],
          "personId": 19549
        },
        {
          "affiliations": [],
          "personId": 19365
        },
        {
          "affiliations": [],
          "personId": 8681
        },
        {
          "affiliations": [],
          "personId": 12331
        },
        {
          "affiliations": [],
          "personId": 11969
        },
        {
          "affiliations": [],
          "personId": 11622
        },
        {
          "affiliations": [],
          "personId": 17359
        },
        {
          "affiliations": [],
          "personId": 16726
        },
        {
          "affiliations": [],
          "personId": 12385
        },
        {
          "affiliations": [],
          "personId": 15886
        },
        {
          "affiliations": [],
          "personId": 13086
        },
        {
          "affiliations": [],
          "personId": 12079
        },
        {
          "affiliations": [],
          "personId": 19229
        },
        {
          "affiliations": [],
          "personId": 20340
        },
        {
          "affiliations": [],
          "personId": 15813
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 6190,
      "typeId": 11437,
      "title": "Iterative Design and Development of Remotely-Controllable, Dynamic Compression Garment for Novel Haptic Experiences",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This work encompasses the design and development of garment-based shape memory alloy (SMA) compression technology that is dynamic, low-mass, and remotely controllable. Three garment design iterations are presented, consolidated from past user studies [1], [2], [3]. The designed garment system has potential to serve as a research tool for understanding parameters necessary to create a desired compression haptic experience; for broadening the scope of medical/clinical interventions; as well as for enabling new modes of interaction between users separated by distance, especially in areas such as tele-rehabilitation and social mediated touch.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22572
        },
        {
          "affiliations": [],
          "personId": 8497
        },
        {
          "affiliations": [],
          "personId": 10049
        },
        {
          "affiliations": [],
          "personId": 17098
        },
        {
          "affiliations": [],
          "personId": 23059
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5167,
      "typeId": 11437,
      "title": "VLA: A Practical Visible Light-based Attack on Face Recognition Systems in Physical World",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Adversarial example attacks have become a growing menace to neural network-based face recognition systems. Generated by compositing facial images with pixel-level perturbations, adversarial examples change key features of inputs and thereby lead to misclassification of neural networks. However, the perturbation loss caused by complex physical environments prevents perturbations generated by existing attack methods from taking effect when applied in physical world.\n\nIn this paper, we focus on designing new attacks that are effective and inconspicuous in physical world. Motivated by the differences in image-forming principles between cameras and human eyes, we propose VLA, a novel attack against black-box face recognition systems using visible light. In VLA, visible light-based adversarial perturbations are crafted and projected on human faces, which allows an adversary to conduct targeted or un-targeted attacks. VLA decomposes adversarial perturbations into a perturbation frame and a concealing frame, where the former adds modifications on human facial images while the latter is used to make these modifications inconspicuous to human eyes. We conduct extensive experiments to demonstrate the effectiveness, inconspicuousness, and robustness of the adversarial examples crafted by VLA in physical scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18777
        },
        {
          "affiliations": [],
          "personId": 11237
        },
        {
          "affiliations": [],
          "personId": 9457
        },
        {
          "affiliations": [],
          "personId": 21896
        },
        {
          "affiliations": [],
          "personId": 24286
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 6191,
      "typeId": 11437,
      "title": "On the Role of Features in Human Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Traditionally, the sliding window based activity recognition chain (ARC) has been dominating practical applications, in which features are carefully optimized towards  scenario specifics.\n  Recently, end-to-end, deep learning methods, that do not discriminate between representation learning and classifier optimization, have become very popular also for HAR using wearables, promising \"out-of-the-box\" modeling with superior recognition capabilities.\n  In this paper, we revisit and analyze specifically the role feature representations play in HAR using wearables. In a systematic exploration we evaluate eight different feature extraction methods, including conventional heuristics and recent representation learning methods, and assess their capabilities for effective activity recognition on five benchmarks.\n  Optimized feature learning integrated into the conventional ARC leads to comparable if not better recognition results as if using end-to-end learning methods, while at the same time offering practitioners more flexibility to optimize their systems towards specifics of wearables and their constraints and limitations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23430
        },
        {
          "affiliations": [],
          "personId": 19796
        },
        {
          "affiliations": [],
          "personId": 16080
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 7224,
      "typeId": 11437,
      "title": "Effect of Using Smartphone during Breast-feeding",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the spread of smartphones, mothers who operate smartphones while breast-feeding are increasing. This kind of activities are some- times defined as a bad behavior although this is one of few repose in parenting. In this paper, we investigate if the use of smartphone affects the breast-feeding from the viewpoints of the mother’s pos- ture and the quality of communication between the mother and the baby.We measure the behavior of the mother with/without smart- phone using wearable sensors and video camera. As a result of the survey, sensor data did not show the significant difference in the mother’s posture. In the observation of the video camera, the incli- nation of a mother’s back was different depending on the presence or absence of a smartphone operation. As a result of research on communication with infants, it was longer for mothers to notice changes in their baby while operating smartphones. In the future, in order to reduce mother’s stress, we will consider how to operate smartphones properly, instead of prohibiting operation of smart- phones for nursing care.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22756
        },
        {
          "affiliations": [],
          "personId": 9702
        },
        {
          "affiliations": [],
          "personId": 19979
        },
        {
          "affiliations": [],
          "personId": 19823
        },
        {
          "affiliations": [],
          "personId": 15280
        },
        {
          "affiliations": [],
          "personId": 11925
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6201,
      "typeId": 11437,
      "title": "Towards indoor localisation analytics for modelling flows of movement",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Localisation analytics, indoor localisation",
      "authors": [
        {
          "affiliations": [],
          "personId": 14467
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5177,
      "typeId": 11437,
      "title": "Is More Always Better? Discovering Incentivized mHealth Intervention Engagement Related to Health Behavior Trends",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Behavioral medicine is devoting increasing attention to the topic of participant engagement and its role in effective mobile health (mHealth) behavioral interventions. Several definitions of the term \"engagement\" have been proposed and discussed, especially in the context of digital health behavioral interventions. We consider that engagement refers to specific interaction and use patterns with the mHealth tools such as smartphone applications for intervention, whereas adherence refers to compliance with the directives of the health intervention, independent of the mHealth tools. Through our analysis of participant interaction and self-reported behavioral data in a college student health study with incentives, we demonstrate an example of measuring \"effective engagement\" as engagement behaviors that can be linked to the goals of the desired intervention. We demonstrate how clustering of one year of weekly health behavior self-reports generate four interpretable clusters related to participants' adherence to the desired health behaviors: healthy and steady, unhealthy and steady, decliners, and improvers. Based on the intervention goals of this study (health promotion and behavioral change), we show that not all app usage metrics are indicative of the desired outcomes that create effective engagement. As such, mHealth intervention design might consider eliciting not just more engagement or use overall, but rather, effective engagement defined by use patterns related to the desired behavioral outcome.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13032
        },
        {
          "affiliations": [],
          "personId": 19836
        },
        {
          "affiliations": [],
          "personId": 8985
        },
        {
          "affiliations": [],
          "personId": 12232
        },
        {
          "affiliations": [],
          "personId": 18306
        },
        {
          "affiliations": [],
          "personId": 11712
        }
      ],
      "sessionIds": [
        2269
      ],
      "eventIds": []
    },
    {
      "id": 7226,
      "typeId": 11437,
      "title": "Enhancing Augmented VR Interaction via Egocentric Scene Analysis",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented virtual reality (AVR) takes portions of the physical world into the VR world to enable VR users to access physical objects. State-of-the-art solutions focus mainly on extracting and showing physical objects in the VR world. In this work, we go beyond previous solutions and propose a novel approach to realize AVR. We first analyze the physical environment in the user's egocentric view through depth sensing and deep learning, then acquire the layout and geometry of the surrounding objects, and further explore their affordance. Based on the above information, we create visual guidance (hollowed guiding path) and hybrid user interfaces (LR finger slider,  LRRL finger slider, and augmented physical notepad) to augment the AVR interaction. Empirical evaluations showed that the participants responded positively to our AVR techniques.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17175
        },
        {
          "affiliations": [],
          "personId": 12234
        },
        {
          "affiliations": [],
          "personId": 17822
        },
        {
          "affiliations": [],
          "personId": 12809
        },
        {
          "affiliations": [],
          "personId": 22907
        },
        {
          "affiliations": [],
          "personId": 23001
        },
        {
          "affiliations": [],
          "personId": 14316
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 6206,
      "typeId": 11437,
      "title": "Challenges and lessons learned from implementing longitudinal studies for self-care technology assessment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Whilst literature is rich in lessons learned from recruitment and retention of participants in longitudinal studies, papers sharing practical experience of implementing such studies with or about ICT are lacking. We discuss the challenges and lessons learned in four longitudinal studies with older adults and chronic disease patients for the assessment of self-care technology. Despite apparently prosaic, everyday challenges and potential threats to studies with non-mainstream audiences may be hard to anticipate. A reflection by the researchers leading these studies led to three main themes associated to studies' timelines, which are described with practical examples.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17590
        },
        {
          "affiliations": [],
          "personId": 21542
        },
        {
          "affiliations": [],
          "personId": 23405
        },
        {
          "affiliations": [],
          "personId": 20250
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6208,
      "typeId": 11437,
      "title": "RF-Focus: Computer Vision-assisted Region-of-interest RFID Tag Recognition and Localization in Multipath-prevalent Environments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Capturing RFID tags in the region of interest (ROI) is challenging. Many issues, such as multipath interference, frequency-dependent hardware characteristics and phase periodicity, make RF phase difficult to accurately indicate the tag-to-antenna distance for RFID tag localization. In this paper, we propose a comprehensive solution, called RF-Focus, which fuses RFID and computer vision (CV) techniques to recognize and locate moving RFID-tagged objects within ROI. Firstly, we build a multipath propagation model and propose a dual-antenna solution to minimize the impact of multipath interference on RF phase. Secondly, by extending the multipath model, we estimate phase shifts due to hardware characteristics at different operating frequencies. Thirdly, to minimize the tag position uncertainty due to RF phase periodicity, we leverage CV to extract image regions of being likely to contain ROI RFID-tagged objects, and then associate them with the processed RF phase after the removal of the phase shifts due to multipath interference and hardware characteristics for recognition and localization. Our experiments demonstrate the effectiveness of multipath modelling and hardware-related phase shift estimation. When five RFID-tagged objects are moving in the ROI, RF-Focus achieves the average recognition accuracy of 91.67% and localization accuracy of 94.26% given a false positive rate of 10%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22739
        },
        {
          "affiliations": [],
          "personId": 12289
        },
        {
          "affiliations": [],
          "personId": 16574
        },
        {
          "affiliations": [],
          "personId": 17721
        },
        {
          "affiliations": [],
          "personId": 17695
        }
      ],
      "sessionIds": [
        1177
      ],
      "eventIds": []
    },
    {
      "id": 4160,
      "typeId": 11437,
      "title": "Modeling RFID Signal Reflection for Contact-free Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wireless sensing techniques for tracking human activities have been vigorously developed in recent years. Yet current RFID based human activity recognition techniques need either direct contact to human body (e.g., attaching RFIDs to users) or specialized hardware (e.g., software defined radios, antenna array). How to wirelessly track human activities using commodity RFID systems without attaching tags to users (i.e., a contact-free scenario) still faces lots of technical challenges. In this paper, we quantify the correlation between RF phase values and human activities by modeling intrinsic characteristics of signal reflection in contact-free scenarios. Based on the signal reflection model, we introduce TACT that can recognize human activities using commodity RFIDs without attaching any RFID tags to users. TACT first reliably detects the presence of human activities and segments phase values. Then, candidate phase segments are classified according to their coarse-grained features (e.g., moving speed, moving distance, activity duration) as well as their fine-grained feature of phase waveform. We deploy and leverage multiple tags to increase the coverage and enhance the robustness of the system. We implement TACT with commodity RFID systems. The experiment results show that TACT can recognize eight types of human activities with 93.5% precision under different and challenging experiment settings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24110
        },
        {
          "affiliations": [],
          "personId": 8960
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 6209,
      "typeId": 11437,
      "title": "Nurse Care Activity Recognition Challenge Using A Supervised Methodology",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition is one of the important research topics in machine learning. Different algorithms have been proposed for human activity recognition in several years. But nurse care activity recognition is a new section in machine learning which mainly focused on physical activity recognition. We are the members of the team \"Data Digger\" accepted a challenge named “Nurse Care Activity Recognition Challenge\". We used Carecom nurse care activity dataset for that challenge [14]. We compare several machine learning algorithms for the dataset. The proposed method was trained and evaluated on Carecom nurse care activity recognition dataset and got 69% accuracy after splitting the dataset.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19510
        },
        {
          "affiliations": [],
          "personId": 9998
        },
        {
          "affiliations": [],
          "personId": 10862
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3140,
      "typeId": 11437,
      "title": "Sleep Stages Classifier with Eliminated Apnea Impact",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper provides a novel best in the domain automatic classifier of sleep stages based on wrist photoplethysmography and 3D-accelerometer data obtained from smartwatches. The sleep is classified into rapid-eye-movement (REM), Light, Deep, or Wake stages. State of the art classifiers based on wearable sensors suffer from motion artifacts and apnea events. Proposed novel techniques of artifacts and apnea events elimination result in the high accuracy of sleep stages classification and robustness to apnea events. The model provides the Cohen’s Kappa score of 0.65 and accuracy of 0.80 on 254 night-logs of 173 subjects with a broad distribution of apnea-hypopnea index until 90. The approach is applicable to unobtrusive sleep monitoring by wearables.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19928
        },
        {
          "affiliations": [],
          "personId": 13763
        },
        {
          "affiliations": [],
          "personId": 19296
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4169,
      "typeId": 11437,
      "title": "Combining Implicit Gaze and AI for Real-Time Intention Projection",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Intention recognition is the process of using behavioural cues to infer an agent's goals or future behaviour. In face-to-face communication, our gaze implicitly signals our point of interest within the environment and therefore, inadvertently leaks our unspoken intentions to others. In our published body of work, we leverage this implicit function of gaze together with the tendency of humans to plan before executing their actions, resulting in an artificial agent that can project humans intentions while human players engage in a competitive game. In this demo, we created a path-planning game to demonstrate the capability of our artificial agent in a playful manner. The agent projects future plans of players by combining the use of implicit gaze of human players with an AI planning-based model. The demo aims to illustrate that gaze is intentional and that socially interactive agents can harness gaze as a natural input implicitly to assist humans collaboratively with knowledge of their intentions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24251
        },
        {
          "affiliations": [],
          "personId": 22950
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 19379
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6218,
      "typeId": 11437,
      "title": "Device-free Personalized Fitness Assistant Using WiFi",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There is a growing trend for people to perform regular workouts in home/office environments because work-at-home people or office workers can barely squeeze in time to go to dedicated exercise places (e.g., gym). To provide personalized fitness assistance in home/office environments, traditional solutions, e.g., hiring personal coaches incur extra cost and are not always available, while new trends requiring wearing smart devices around the clock are cumbersome. In order to overcome these limitations, we develop a device-free fitness assistant system in home/office environments using existing WiFi infrastructure. Our system aims to provide personalized fitness assistance by differentiating individuals, automatically recording fine-grained workout statistics, and assessing workout dynamics. In particular, our system performs individual identification via deep learning techniques on top of workout interpretation. It further assesses the workout by analyzing both short and long-term workout quality, and provides workout reviews for users to improve their daily exercises. Additionally, our system adopts a spectrogram-based workout detection algorithm along with a Cumulative Short Time Energy (CSTE)-based workout segmentation method to ensure its robustness. \\ Extensive experiments involving 20 participants demonstrate that our system can achieve a 93% accuracy on workout recognition and a 97% accuracy for individual identification.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8755
        },
        {
          "affiliations": [],
          "personId": 20456
        },
        {
          "affiliations": [],
          "personId": 20765
        },
        {
          "affiliations": [],
          "personId": 23288
        },
        {
          "affiliations": [],
          "personId": 17135
        },
        {
          "affiliations": [],
          "personId": 8651
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 4171,
      "typeId": 11437,
      "title": "An Optimized Recurrent Unit for Ultra-Low-Power Keyword Spotting",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There is growing interest in being able to run neural networks on sensors, wearables and internet-of-things (IoT) devices. However, the computational demands of neural networks make them difficult to deploy on resource-constrained edge devices.\n\nTo meet this need, our work introduces a new recurrent unit architecture that is specifically adapted for on-device low power acoustic event detection (AED). The proposed architecture is based on the gated recurrent unit (‘GRU’ – introduced by Cho et al. [9]) but features optimizations that make it implementable on ultra-low power micro-controllers such as the Arm Cortex M0+.\n\nOur new architecture, the Embedded Gated Recurrent Unit (eGRU) is demonstrated to be highly efficient and suitable for short-duration AED and keyword spotting tasks. A single eGRU cell is 60× faster and 10× smaller than a GRU cell. Despite its optimizations, eGRU compares well with GRU across tasks of varying complexities.\n\nThe practicality of eGRU is investigated in a wearable acoustic event detection application. An eGRU model is implemented and tested on the Arm Cortex M0-based Atmel ATSAMD21E18 processor. The Arm M0+ implementation of the eGRU model compares favorably with a full precision GRU that is running on a workstation. The embedded eGRU model achieves a classification accuracy 95.3%, which is only 2% less than the full precision GRU.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8474
        },
        {
          "affiliations": [],
          "personId": 22092
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 6223,
      "typeId": 11437,
      "title": "PURBA 2019: The 8th Workshop on Pervasive Urban Applications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The 8th Workshop on Pervasive Urban Applications (PURBA 2019) aims to build on the success of the previous workshops organized in conjunction with the Pervasive (2011-12) and UbiComp (2013, 2015-18) to continue to disseminate the re- sults of the latest research outcomes and developments of ubiquitous computing technologies for urban applications. All workshop contributions are published in supplemental proceedings of the UbiComp 2019 conference and included in the ACM Digital Library.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9607
        },
        {
          "affiliations": [],
          "personId": 13403
        },
        {
          "affiliations": [],
          "personId": 12947
        },
        {
          "affiliations": [],
          "personId": 23610
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6225,
      "typeId": 11437,
      "title": "P-Loc: A Device-free Indoor Localization System Utilizing Building Power-line Network",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we propose P-Loc, a device-free indoor localization system based on power-line network within the building. P-Loc measures the electromagnetic (EM) coupling between a human body and existing power-lines, which are simultaneously used for electric power transmission. To avoid the impact of AC mains and noise from other electrical sources, we inject a signal into the ground-line to generate the occupant location fingerprint of a specific frequency. A 0.48m average error distance is obtained in the preliminary experiments.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12874
        },
        {
          "affiliations": [],
          "personId": 9934
        },
        {
          "affiliations": [],
          "personId": 17798
        },
        {
          "affiliations": [],
          "personId": 17107
        },
        {
          "affiliations": [],
          "personId": 8532
        },
        {
          "affiliations": [],
          "personId": 14989
        },
        {
          "affiliations": [],
          "personId": 18241
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3155,
      "typeId": 11437,
      "title": "FarSight: A Smartphone-based Vehicle Ranging System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Maintaining an adequate separation from the vehicle in front is key to safe driving. While LIDAR and RADAR sensors could be used for ranging, cost considerations and the huge installed base of vehicles that lack these sensors, especially in developing regions, call for a low-cost yet robust alternative. To this end, we present FarSight, a system that performs vehicle ranging using a smartphone mounted on the windshield or the dashboard. FarSight uses the smartphone’s camera to identify and draw a bounding box around vehicles in front, based on which ranging is performed. Unlike prior smartphone-based work, FarSight does not depend on any infrastructure support such as standard-width lane markers and works with a heterogeneous mix of vehicles, both of which are characteristics of developing regions. We develop a novel hybrid approach for vehicle detection and tracking, which balances accuracy and speed by combining deep neural network based vehicle detection with vision-based object tracking in a pipelined manner. We also devise data augmentation techniques to improve the eﬀectiveness of vehicle detection, thereby increasing the ranging distance. We show that FarSight can range accurately in both daytime and nighttime conditions and up to distances of 90 m. We have implemented FarSight as an Android-app and tested it across various phones. Further, we present two ranging-based applications built on FarSight.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16881
        },
        {
          "affiliations": [],
          "personId": 16094
        },
        {
          "affiliations": [],
          "personId": 14715
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 7260,
      "typeId": 11437,
      "title": "VPS Tactile Display: Tactile Information Transfer of Vibration, Pressure, and Shear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "One of the challenges in the field of haptics is to provide meaningful and realistic sensations to users. While most real world tactile sensations are composed of multiple modalities, most commercial product only include vibration as it is the most cost effective solution. To improve on this, we introduce VPS (Vibration, Pressure, Shear) display, a multimodal tactile array that increases information transfer and enhances realism by combining Vibration, Pressure, and Shear similar to how RGB LED combines red, blue, and green to create new colors. We characterize the device performance and dynamics for each haptic modality in terms of its force and displacement profiles, and evaluate information transfer of the VPS display through a stimulus identification task. Our results indicate that the information transfer through a single taxel increases from 0.56 bits to 2.15 bits when pressure and shear are added to vibrations with a slight decrease in identification accuracy. We also explored the pleasantness and continuity of VPS and the study results reveal that tactile strokes in shear mode alone are rated highest in pleasantness and continuity scales.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22410
        },
        {
          "affiliations": [],
          "personId": 15400
        },
        {
          "affiliations": [],
          "personId": 22053
        },
        {
          "affiliations": [],
          "personId": 21526
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 3165,
      "typeId": 11437,
      "title": "FarSense: Pushing the Range Limit of WiFi-based Respiration Sensing with CSI Ratio of Two Antennas",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The past few years have witnessed the great potential of exploiting channel state information retrieved from commodity WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. On the other hand, most home environments only have one WiFi access point and it may not be located in the same room as the target. This sensing range constraint greatly limits the application of the proposed approaches in real life.\n\nThis paper presents FarSense--the first real-time system that can reliably monitor human respiration when the target is far away from the WiFi transceiver pair. FarSense works well even when one of the transceivers is located in another room, moving a big step towards real-life deployment. We propose two novel schemes to achieve this goal: (1) Instead of applying the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range; (2) The division operation further enables us to utilize the phase information which is not usable with one single antenna for sensing. The orthogonal amplitude and phase are elaborately combined to address the \"blind spots\" issue and further increase the sensing range.  Extensive experiments show that FarSense is able to accurately monitor human respiration even when the target is 8 meters away from the transceiver pair, increasing the sensing range by more than 100%. We believe this is the first system to enable through-wall respiration sensing with commodity WiFi devices and the proposed method could also benefit other sensing applications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15582
        },
        {
          "affiliations": [],
          "personId": 10893
        },
        {
          "affiliations": [],
          "personId": 12597
        },
        {
          "affiliations": [],
          "personId": 22319
        },
        {
          "affiliations": [],
          "personId": 18786
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 6241,
      "typeId": 11437,
      "title": "Social Help: Developing Methods to Support Older Adults in Mobile Privacy and Security",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Older people experience difficulties when managing their security and privacy in mobile environments. However, support from the older adult’s social network, and especially from close-tie relations such as family and close friends, is known to be an effective source of help in coping with technological tasks. On the basis of this existing phenomena, I investigate how new methods can increase the availability of social support to older adults and enhance learning in tackling privacy and security challenges. I will develop and evaluate several technological interventions in the support process within social networks for older adults: finding methods that increase seekers’ technology learning and methods that increase help availability and quality. \nIn my Ph.D., I suggest conducting three studies: the first study aims to analyze existing approaches and scenarios of social support to older adults. The initial results suggest that people have a significant willingness to help their older relatives (specifically, their parents), but the actual instances in which they do so is much rarer. We conclude that the potential for social help is far from being exploited.  In the second study, I plan to explore social support as a system to increase older adults’ self-efficacy and collective efficacy to overcome privacy and security problems. The final study will investigate physiological signals to identify when an older adult required help with mobile security and privacy issues. A successful outcome will be a theoretical model of social support, focused on the domain of privacy and security, and based on vulnerable populations such as older adults. From a practical standpoint, the thesis will offer and evaluate a set of technologies that enable and encourage social support for older adults on mobile platforms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8504
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4195,
      "typeId": 11437,
      "title": "Poster: SpiderHand: Towards quasi-direct interaction with unpleasant creatures using muscle-controlled robotic arm.",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Although encounters with spiders are commonplace for people\nalmost everywhere in the world, fear of spiders is one\nof the most frequently diagnosed phobias and immediate\ncontact is widely perceived as unfavourable. We present a\nsystem for indirect, quasi-tangible interaction with spiders,\nto be applied in an exhibition context - a robotic arm, steered\nthrough gestural input, which mimics user’s actions and\nenables indirect physical interaction with the spider. The\nproof-of-concept prototype has been tested with N=15 users\nin museum-like environment. The concept of implementing\nan interactive modality to the exhibition was commented as\nan asset in terms amusement and education aiding, whilst being\nalso a promising endeavour towards phobia-overcoming\nexercise.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14653
        },
        {
          "affiliations": [],
          "personId": 17685
        },
        {
          "affiliations": [],
          "personId": 14764
        },
        {
          "affiliations": [],
          "personId": 16202
        },
        {
          "affiliations": [],
          "personId": 11363
        },
        {
          "affiliations": [],
          "personId": 20921
        },
        {
          "affiliations": [],
          "personId": 24108
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6243,
      "typeId": 11437,
      "title": "Capturing Contextual Morality: Applying Game Theory on Smartphones",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Individuals increasingly use mobile, wearable, and ubiquitous devices capable of unobtrusive collection of vast amounts of scientifically rich personal data over long periods (months to years), and in the context of their daily life. However, numerous human and\ntechnological factors challenge longitudinal data collection, often limiting research studies to very short data collection periods (days to weeks), spawning recruitment biases, and affecting participant retention over time. This workshop is designed to bring together researchers involved in longitudinal data collection studies to foster an insightful exchange of ideas, experiences, and discoveries to improve the studies’ reliability, validity, and perceived meaning of longitudinal mobile, wearable, and ubiquitous data collection for\nthe participants.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10224
        },
        {
          "affiliations": [],
          "personId": 24094
        },
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 23525
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6244,
      "typeId": 11437,
      "title": "An IoT and Blockchain-based Approach for Ensuring Transparency and Accountability in Regulatory Compliance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Regulatory compliance is an essential exercise in the modern societies confirming safety and prevention of harm to consumers. Despite many efforts from international and national quality control authorities, transparency and accountability in regulatory compliance remain a challenging technical-legal problem sitting atop a heavy reliance on trust. This paper presents a theoretical model of regulatory compliance aiming at improving accountability for systems and data audit and introduces a higher degree of transparency in management and quality control. It explores the technical aspects of two emerging technologies the Internet of Things (IoT) and Blockchain, and using a common use-case in practice shows how to better align these technologies with legal concerns and trust in regulatory compliance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10758
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3177,
      "typeId": 11437,
      "title": "Coconut: An IDE Plugin for Developing Privacy-Friendly Apps",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Although app developers are responsible for protecting users' privacy, this task can be very challenging. In this paper, we present Coconut, an Android Studio plugin that helps developers handle privacy requirements by engaging developers to think about privacy during the development process and providing real-time feedback on potential privacy issues. We start by presenting new findings based on a series of semi-structured interviews with Android developers, probing into the difficulties with privacy that developers face when building apps. Based on these findings, we implemented a proof-of-concept prototype of Coconut and evaluated it in a controlled lab study with 18 Android developers (including eight professional developers). Our study results suggest that apps developed with Coconut handled privacy concerns better, and the developers that used Coconut had a better understanding of their code's behavior and wrote a better privacy policy for their app. We also found that requiring developers to do a small amount of annotating work regarding their apps' personal data practices during the development process may result in a significant improvement in app privacy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22952
        },
        {
          "affiliations": [],
          "personId": 14005
        },
        {
          "affiliations": [],
          "personId": 22231
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 4202,
      "typeId": 11437,
      "title": "Apply Event Extraction Techniques to the Judicial Field",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "While hearing a case, the judge must fully understand the case and make clear the disputed issues between parties, which is the cornerstone of a fair trial. However, manual mining the key of the case from the statements of the litigious parties is a bottleneck, which currently relies on methods like keyword searching and regular matching. To complete this time-consuming and laborious task, judges need to have sufficient prior knowledge of cases belonging to different causes of action. We try to apply the technology of event extraction to faster capture the focus of the case. However, there is no proper definition of events that contains types of focus in the judicial field. And existing event extraction methods can't solve the problem of multiple events sharing the same arguments or trigger words in a single sentence, which is very common in case materials. In this paper, we present a mechanism to define focus events, and a two-level labeling approach, which can solve multiple events sharing the same argument or trigger words, to automatically extract focus events from case materials. Experimental results demonstrate that the method can obtain the focus of case accurately. As far as we know, this is the first time that event extraction technology has been applied to the judicial field.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18516
        },
        {
          "affiliations": [],
          "personId": 21970
        },
        {
          "affiliations": [],
          "personId": 22573
        },
        {
          "affiliations": [],
          "personId": 13578
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5227,
      "typeId": 11437,
      "title": "MilliBack: Real-Time Plug-n-Play Millimeter Level Tracking Using Wireless Backscattering",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Real-time handwriting tracking is important for many emerging applications such as Artificial Intelligence assisted education and healthcare. Existing movement tracking systems, including those based on vision, ultrasound or wireless technologies, fail to offer high tracking accuracy, no learning/training/calibration process, low tracking latency, low cost and easy to deploy at the same time. In this work, we design and evaluate a wireless backscattering based handwriting tracking system, called MilliBack, that satisfies all these requirements. At the heart of MilliBack are two Phase Differential Iterative (PDI) schemes that can infer the position of the backscatter tag (which is attached to a writing tool) from the change in the signal phase. By adopting carefully-designed differential techniques in an iterative manner, we can take the diversity of devices out of the equation. The resulting position calculation has a linear complexity with the number of samples, ensuring fast and accurate tracking.\n\nWe have put together a MilliBack prototype and conducted comprehensive experiments. We show that our system can track various handwriting traces accurately, in some testings it achieve a median error of 4.9mm. We can accurately track and reconstruct arbitrary writing/drawing trajectories such as equations, Chinese characters or just random shapes. We also show that MilliBack can support relatively high writing speed and smoothly adapt to the changes of working environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19644
        },
        {
          "affiliations": [],
          "personId": 9056
        },
        {
          "affiliations": [],
          "personId": 10941
        },
        {
          "affiliations": [],
          "personId": 9748
        },
        {
          "affiliations": [],
          "personId": 22504
        },
        {
          "affiliations": [],
          "personId": 21544
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 7277,
      "typeId": 11437,
      "title": "Integrating Activity Recognition and Nursing Care Records: The System, Deployment, and a Verification Study",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we introduce a system of integrating activity recognition and collecting nursing care records at nursing care facilities as well as activity labels and sensors through smartphones, and describe experiments at a nursing facility for 4 months. \nA system designed to be used even by staff not familiar with smartphones could collected enough number of data without losing but improving their recording. For collected data, we revealed the nature of the collected data as for activities, care details, and timestamps, and considering them, we show a reference accuracy of recognition of nursing activity which is durable to time skewness, overlaps, and class imbalances. \nMoreover, we demonstrate the near future prediction to predict the next day's activities from the previous day's records which could be useful for proactive care management.\nThe dataset collected is to be opened to the research community, and can be the utilized for activity recognition and data mining in care facilities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24129
        },
        {
          "affiliations": [],
          "personId": 18107
        },
        {
          "affiliations": [],
          "personId": 21030
        },
        {
          "affiliations": [],
          "personId": 8218
        },
        {
          "affiliations": [],
          "personId": 11459
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 3182,
      "typeId": 11437,
      "title": "Mapping Human Response to Street Environment: A Study on Comparing Walking with Cycling on Streets through Wearable Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Advancements in wearable biosensor technologies in recent years enable us to witness an unprecedented boom, as interest in fitness health data grow exponentially. Learning about one’s metrics and sharing of this data in various platforms has arguably become a demand and practice for daily life. This research aims to discover potential applications of wearable technology and fitness health data by taking on the existing ubiquitous computing hardware and software technologies on wearable biosensors and push them further to a new level by proposing a methodology and visualization for users response to their surroundings, and how this application could become a critical feedback mechanism for individual users as well as planners, decision makers and designers for our built environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18974
        },
        {
          "affiliations": [],
          "personId": 20660
        },
        {
          "affiliations": [],
          "personId": 23038
        },
        {
          "affiliations": [],
          "personId": 10026
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4206,
      "typeId": 11437,
      "title": "Altered Pinna: Exploring Shape Change of Pinna for Perception and Illusion of Sound Direction Change",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This study demonstrates that by opening and closing the human pinna, we can change the direction of sound perceived by humans. Each ear was independently transformed into a 100% open, 50% open, and 100% closed state, and all 9 combinations of these ear transformations were tested to evaluate the perceived direction of the sound output from 7 speakers placed 180 degrees around the subject. We demonstrate that by deforming the pinna, we could change the perception of the direction of sound, or make it illusory. We also found that except for 1 out of 7 speakers (or directions of sound), closing 100% of the ear on the side of the speaker where the sound is coming from and 50% of the ear on the other side of the speaker tends produce the most alteration to the perceived direction of sound.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13656
        },
        {
          "affiliations": [],
          "personId": 17314
        },
        {
          "affiliations": [],
          "personId": 13356
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 7286,
      "typeId": 11437,
      "title": "Towards in-situ process tomography data processing using augmented reality technology.",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, various branches of industry are based on continuous processes. Therefore, efficient and accurate data analysis became crucial for maintaining control and optimizing the monitored trials. This paper presents a novel solution for in-situ analysis of complex numerical data. The proposed system employs mixed-reality technology to visualize data and enable collaborative analysis in remote locations. The system was implemented using Microsoft HoloLens and tested in laboratory environment, with its proof-of-concept version applied to solve real expert analysis task. After the experiment NASA TLX and SUS questionnaires were filled and demonstrated improvement performance and enabling more extensive analysis, including their spatio-temporal features.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11096
        },
        {
          "affiliations": [],
          "personId": 12138
        },
        {
          "affiliations": [],
          "personId": 24155
        },
        {
          "affiliations": [],
          "personId": 11363
        },
        {
          "affiliations": [],
          "personId": 20921
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4216,
      "typeId": 11437,
      "title": "Clinical Data in Context: Towards Sensemaking Tools for Interpreting Personal Health Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Clinical data augmented with contextual data can help patients with chronic conditions make sense of their disease. However, existing tools do not support interpretation of multiple data streams. To better understand how individuals make sense of clinical and contextual data, we interviewed patients with Type 1 diabetes and their caregivers using context-enhanced visualizations of patients‚Äô data as probes to facilitate interpretation activities. We observed that our participants performed four analytical activities when interpreting their data ‚Äì finding context-based trends and explaining them, triangulating multiple factors, suggesting context-specific actions, and hypothesizing about alternate contextual factors affecting outcomes. We also observed two challenges encountered during analysis ‚Äì the inability to identify clear trends challenged action planning and counterintuitive insights compromised trust in data. Situating our findings within the existing sensemaking frameworks, we demonstrate that sensemaking can not only inform action but can guide the discovery of information needs for exploration. We further argue that sensemaking is a valuable approach for exploring contextual data. Informed by our findings and our reflection on existing sensemaking frameworks, we provide design guidelines for sensemaking tools to improve awareness of contextual factors affecting patients and to support patients‚Äô agency in making sense of health data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17542
        },
        {
          "affiliations": [],
          "personId": 20266
        },
        {
          "affiliations": [],
          "personId": 11632
        },
        {
          "affiliations": [],
          "personId": 23265
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 5244,
      "typeId": 11437,
      "title": "Keyboard Snooping from Mobile Phone Arrays with Mixed Convolutional and Recurrent Neural Networks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The ubiquity of modern smartphones, because they are equipped with a wide range of sensors, poses a potential security risk---malicious actors could utilize these sensors to detect private information such as the keystrokes a user enters on a nearby keyboard. Existing studies have examined the ability of phones to predict typing on a nearby keyboard but are limited by the realism of collected typing data, the expressiveness of employed prediction models, and are typically conducted in a relatively noise-free environment. We investigate the capability of mobile phone sensor arrays (using audio and motion sensor data) for classifying keystrokes that occur on a keyboard in proximity to phones around a table, as would be common in a meeting. We develop a system of mixed convolutional and recurrent neural networks and deploy the system in a human subjects experiment with 20 users typing naturally while talking. Using leave-one-user-out cross validation, we find that mobile phone arrays have the ability to detect 41.8% of keystrokes correctly and 27% of typed words in such a noisy environment---even without user specific training.  To investigate the potential threat of this attack, we further developed the machine learning models into a realtime system capable of discerning keystrokes from an array of mobile phones and evaluated the system's ability with a single user typing in varying conditions. We conclude that, in order to launch a successful attack, the attacker would need advanced knowledge of the table and room from which a user types, and the style of keyboard on which a user types. These constraints greatly limit the feasibility of such an attack to highly capable attackers and we therefore conclude threat level of this attack to be low, but non-zero.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20502
        },
        {
          "affiliations": [],
          "personId": 15709
        },
        {
          "affiliations": [],
          "personId": 11743
        },
        {
          "affiliations": [],
          "personId": 10163
        },
        {
          "affiliations": [],
          "personId": 11306
        },
        {
          "affiliations": [],
          "personId": 16834
        },
        {
          "affiliations": [],
          "personId": 20041
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 6269,
      "typeId": 11437,
      "title": "Rewind: Automatically Reconstructing Everyday Memories with First-Person Perspectives",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Snapping photos or videos on a smartphone makes recording visual memories convenient, but what isn't captured may still be meaningful in retrospect. In this paper, we present our findings from the Rewind system, which assists the recall of location-based minutiae. Rewind is a video-like medium describing people's daily excursions using a sequence of street-level images determined by self-tracked location data. The Rewinds are color-processed to reflect seasonal, time-of-day and weather characteristics. Through two user studies with a combined 40 users, Rewinds were shown to be used as memory artifacts, and are especially meaningful in the many situations when photos or videos are not available. While they are more generic and can possess inaccuracies, Rewinds give people anchors for memories that feel like their own. The small cues in Rewinds evoke longer fragments of memory due to typicality, and the sequence of images provide a first-person style of remembrance for users. Rewind strikes a balance between the overload of automatic logging that is potentially meaningless or undesirable, with the ability for users to curate a personalized memory of their past.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22806
        },
        {
          "affiliations": [],
          "personId": 9114
        },
        {
          "affiliations": [],
          "personId": 19739
        },
        {
          "affiliations": [],
          "personId": 14756
        },
        {
          "affiliations": [],
          "personId": 11791
        },
        {
          "affiliations": [],
          "personId": 11353
        },
        {
          "affiliations": [],
          "personId": 23771
        },
        {
          "affiliations": [],
          "personId": 10780
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 4223,
      "typeId": 11437,
      "title": "BoostMeUp: Improving Cognitive Performance in the Moment by Unobtrusively Regulating Emotions with a Smartwatch",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A person’s emotional state can strongly influence their ability to achieve optimal task performance. Aiming to help individuals manage their feelings, different emotion regulation technologies have been proposed. However, despite the well- known influence that emotions have on task performance, no study to date has shown if an emotion regulation technology can also enhance user’s cognitive performance in the moment. In this paper, we present BoostMeUp, a smartwatch intervention designed to improve user’s cognitive performance by regulating their emotions unobtrusively. Based on studies that show that people tend to associate external signals that resemble heart rates as their own, the intervention provides personalized haptic feedback simulating a different heart rate. Users can focus on their tasks and the intervention acts upon them in parallel, without requiring any additional action. The intervention was evaluated in an experiment with 72 participants, in which they had to do math tests under high pressure. Participants who were exposed to slow haptic feedback during the tests decreased their anxiety, increased their heart rate variability and performed better in the math tests, while fast haptic feedback led to the opposite effects. These results indicate that the BoostMeUp intervention can lead to positive cognitive, physiological and behavioral changes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11552
        },
        {
          "affiliations": [],
          "personId": 16879
        },
        {
          "affiliations": [],
          "personId": 14174
        },
        {
          "affiliations": [],
          "personId": 18535
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 7298,
      "typeId": 11437,
      "title": "A Deep Reinforcement Learning-Enabled Dynamic Redeployment System for Mobile Ambulances",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Protecting citizens' lives from emergent accidents (e.g. traffic accidents) and diseases (e.g. heart attack) is of vital importance in urban computing. Every day many people are caught in emergent accidents or diseases and thus need ambulances to transport them to hospitals. In this paper, we propose a dynamic ambulance redeployment system to reduce the time needed for ambulances to pick up patients and to increase the probability of patients being saved in time. For patients in danger, every second counts. Specifically, whenever there is an ambulance becoming available (e.g. finishing transporting a patient to a hospital), our dynamic ambulance redeployment system will redeploy it to a proper ambulance station such that it can better pick up future patients. However, the dynamic ambulance redeployment is challenging, as when we redeploy an available ambulance we need to simultaneously consider each station's multiple dynamic factors. To trade off these multiple factors using handcrafted rules are almost impossible. To deal with this issue, we propose using a deep neural network, called deep score network, to balance each station's dynamic factors into one score, leveraging the excellent representation ability of deep neural networks. And then we propose a deep reinforcement learning framework to learn the deep score network. Finally, based on the learned deep score network, we provide an effective dynamic ambulance redeployment algorithm. Experiment results using data collected in real world show clear advantages of our method over baselines, e.g. comparing with baselines, our method can save ~100 seconds (~20%) of average pickup time of patients and improve the ratio of patients being picked up within 10 minutes from 0.786 to 0.838. With our method, people in danger can be better saved.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15628
        },
        {
          "affiliations": [],
          "personId": 14172
        },
        {
          "affiliations": [],
          "personId": 9285
        },
        {
          "affiliations": [],
          "personId": 16782
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 7300,
      "typeId": 11437,
      "title": "Towards a Taxonomy of Interactive Continual and Multimodal Learning for the Internet of Things",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With advances in Internet of Things many opportunities arise if the challenges of continual learning in a multimodal setting can be tackled. One common issue in Online Learning is to obtain labelled data, as this generally is costly. Active Learning is a popular approach to collect labelled data efficiently, but in general includes unrealistic assumptions.\n\nIn this work we present a first step towards a taxonomy of Interactive Learning strategies in a multimodal and dynamic setting. By relaxing assumptions of standard Active Learning, the strategies become better suited for real-world settings and can achieve better performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11399
        },
        {
          "affiliations": [],
          "personId": 22968
        },
        {
          "affiliations": [],
          "personId": 14949
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7302,
      "typeId": 11437,
      "title": "Designing Drones: Factors and Characteristics Influencing the Perception of Flying Robots",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The last few years have seen a revolution in aerial robotics where personal drones have become pervasive in our environment and can be bought by anyone anywhere, including at local supermarkets. As they become ubiquitous to our lives, it is crucial to understand how they are perceived and understood by people. The robotics community has extensively theorized and quantified how robotic agents are perceived as social creatures and how this affects users and passersby. However, drones present different form factors that are yet to be systematically explored. This work aims to fill this gap by understanding people's perceptions of drones and how drone features correlate to a series of dimensions. We explored most quadcopters available on the 2018 market and built a dataset of 63 images that were evaluated in a user study (N=307). Our results indicate how people understand drones based on their design and which designs are most suitable for interaction. Our findings highlight that safety features have a negative effect on several dimensions including trust and no positive effect. We contribute a set of design guidelines for future personal drones and conclude on the implications for ubiquitous computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19231
        },
        {
          "affiliations": [],
          "personId": 22277
        },
        {
          "affiliations": [],
          "personId": 18262
        },
        {
          "affiliations": [],
          "personId": 11092
        },
        {
          "affiliations": [],
          "personId": 14237
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 4236,
      "typeId": 11437,
      "title": "Blink as you Sync - Uncovering Eye and Nod Synchrony in Conversation using Wearable Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We tend to synchronize our movements to the person we are talking to during face-to-face conversation. Higher interpersonal synchrony is linked to greater empathy and more effortless interactions. This paper presents a first method and a corresponding dataset to explore synchrony in natural conversation by capturing eye and head movement using commodity smart eyewear.\n  We present a 17 hour dataset, using Electrooculography and inertial sensing, of 42 people in conversation (21 dyads: 10 in Japanese, 10 in English, 1 in Chinese). \n  Initial results on 18 dyads show significant interpersonal synchrony of blink and head nod behaviour during conversation (at frequencies of  0.2 to 0.5 Hz). We also find that people are more likely to synchronise blinks at around 1 Hz when conversing back-to-back than when face-to-face.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9261
        },
        {
          "affiliations": [],
          "personId": 13965
        },
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 9113
        },
        {
          "affiliations": [],
          "personId": 18155
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 3213,
      "typeId": 11437,
      "title": "7th International Workshop on Human Activity Sensing Corpus and Applications (HASCA)",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The wide deployment of sensors in smartphone nourishes the demand\nfor activity recognition, which makes it possible to provide\npersonalized services for users. However, how to recognize the activity\nmode of user regardless of the positions of the smartphone remain\nan open question. Sussex-Huawei Locomotion-Transportation(SHL)\nrecognition challenge devotes to solve this problem and provides a\nvariety of sensor data to train the recognition model. In this paper,\nour team QMUL-IoTLab applies XGBoost and the validation results\nshow a success rate around 70%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13824
        },
        {
          "affiliations": [],
          "personId": 21748
        },
        {
          "affiliations": [],
          "personId": 15053
        },
        {
          "affiliations": [],
          "personId": 18107
        },
        {
          "affiliations": [],
          "personId": 19993
        },
        {
          "affiliations": [],
          "personId": 20007
        },
        {
          "affiliations": [],
          "personId": 20461
        },
        {
          "affiliations": [],
          "personId": 21240
        },
        {
          "affiliations": [],
          "personId": 20416
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6285,
      "typeId": 11437,
      "title": "mORAL: An mHealth model for inferring Oral Hygiene Behaviors in-the-wild using wrist-worn inertial sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We address the open problem of reliably detecting oral health behaviors passively from wrist-worn inertial sensors. We present our model named mORAL (pronounced em oral) for detecting brushing and flossing behaviors, without the use of instrumented toothbrushes so that the model is applicable to brushing with still prevalent manual toothbrushes. We show that for detecting rare daily events such as toothbrushing, adopting a model that is based on identifying candidate windows based on events, rather than fixed-length timeblocks, leads to significantly higher performance. Trained and tested on 2,797 hours of sensor data collected over 192 days on 25 participants (using video annotations for ground truth labels), our brushing model achieves 100% median recall with a false positive rate of one event in every nine days of sensor wearing. The average error in estimating the start/end times of the detected event is 4.1% of the interval of the actual toothbrushing event.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13618
        },
        {
          "affiliations": [],
          "personId": 21018
        },
        {
          "affiliations": [],
          "personId": 12699
        },
        {
          "affiliations": [],
          "personId": 10510
        },
        {
          "affiliations": [],
          "personId": 16120
        },
        {
          "affiliations": [],
          "personId": 10916
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 7309,
      "typeId": 11437,
      "title": "VoltKey: Continuous Secret Key Generation based on Power Line Noise for Zero–Involvement Pairing and Authentication",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The explosive proliferation of Internet-of-Things (IoT) ecosystem fuels the needs for a mechanism for the user to easily and securely interconnect multiple heterogeneous devices with minimal user involvement. However, the current paradigm of context-unaware pairing and authentication methods (i.e., using a preset or user-defined password) poses severe challenges in usability and security aspects due to the limited and siloed user interface that requires substantial effort on establishing or maintaining a secure network. In this paper, we present VoltKey, a method that transparently and continuously generates secret keys for colocated devices, leveraging spatiotemporally unique noise contexts observed in commercial power line infrastructure. The unique noise pattern that is observed only by trusted devices connected to a local power line prevents malicious devices without physical access from obtaining unauthorized access to the network. VoltKey can be implemented on top of standard USB power supplies as a platform-agnostic bolt-on addition to any IoT devices or wireless access points that are constantly connected to the power outlet. Through extensive experiments under various realistic deployment environments, we demonstrate that VoltKey can correctly establish a key pair among colocated devices with over 90% success rate while rejecting malicious devices that do not have access to the local power line (but may have access to a spatially nearby line).",
      "authors": [
        {
          "affiliations": [],
          "personId": 10112
        },
        {
          "affiliations": [],
          "personId": 16150
        },
        {
          "affiliations": [],
          "personId": 16407
        },
        {
          "affiliations": [],
          "personId": 24086
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 4237,
      "typeId": 11437,
      "title": "News From the Background to the Foreground: How People Use Technology To Manage Media Transitions",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "People are the designers and curators of their own news and information ecosystems, due to the disruption of the news industry and developments in media technology. To understand how people use technology to manage their news consumption, we conducted a two-week diary study with 14 participants, focusing on how people transition between news content and behaviors via different media, sources, platforms and devices. We used an inductive, qualitative analysis of the diary study data to analyze the news behaviors and their underlying motivations and found that people frequently shift their focus between ambient background news streams and active foreground news behaviors. Although people often passively consume news content as a background activity, they also actively manage background news habits to increase the chances of relevant foreground experiences. People manage news consumption by developing routines that are often supported by technology use and social interactions. We encourage product designers to treat backgrounding as an essential part of news consumption behavior and suggest new design directions that employ ubiquitous computing technologies--such as context sensing and routine modeling--to more effectively attend to background-to-foreground behaviors and transitions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11643
        },
        {
          "affiliations": [],
          "personId": 16441
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 6288,
      "typeId": 11437,
      "title": "Viewport Prediction for Live 360-Degree Mobile Video Streaming Using User-Content Hybrid Motion Tracking",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "360-degree video streaming has been gaining popularities recently with the rapid growth of adopting mobile head mounted display (HMD) devices in the consumer video market, especially for live broadcasts. The 360-degree video streaming introduces brand new bandwidth and latency challenges in live streaming due to the significantly increased video data. However, most of the existing bandwidth saving approaches based on viewport prediction have only focused on the video-on-demand (VOD) use cases leveraging historical user behavior data, which is not available in live broadcasts. We develop a new viewport prediction scheme for live 360-degree video streaming using video content-based motion tracking and dynamic user interest modeling. To obtain real-time performance, we implement the Gaussian mixture model (GMM) and optical flow algorithms for motion detection and feature tracking. Then, the user's future viewport of interest is generated by leveraging a dynamic user interest model that weighs all the features and motion information abstracted from the live video frames. Furthermore, we develop two enhancement techniques that take into consideration of user feedback for fast error recovery and view updates. Consequently, our predicted viewports are irregular and dynamically adjusted to cover the maximum portions of the actual user viewports and thus ensure a high prediction accuracy. We evaluate our viewport prediction approach using a public user head movement dataset, which contains the data of 48 users watching 6 360-degree videos. The experimental results show that the proposed approach supports sophisticated user head movement patterns and outperforms the existing velocity-based approach in terms of prediction accuracy. In addition, the motion tracking scheme introduces minimum latency overhead to ensure the quality of live streaming experience.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23620
        },
        {
          "affiliations": [],
          "personId": 8271
        },
        {
          "affiliations": [],
          "personId": 17830
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3219,
      "typeId": 11437,
      "title": "Using Contactless Sensors to Estimate Learning Difficulty in Digital Learning Environments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Digital learning environments provide rich and engaging experiences for students to develop different knowledge and skills. However, learning systems in these environments generally lack the capacity to assess student difficulties in real-time. The goal of this research project is to explore the relationship between physiological sensor data and associated cognitive states in a digital learning environment. To do this, we have collected a 100-participant data set using a range of contact-less physiological sensors while participants watched two short video lectures. The project aims to investigate the effect of the difficulty level of the lecture on the eye-movements, facial temperature, and expressions of the participants, and later develop a system which can identify the difficult segments in the lecture.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18619
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6292,
      "typeId": 11437,
      "title": "Explaining automated environments: Interrogating scripts, logs, and provenance using voice-assistants",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The grand vision of pervasive computing and the Internet of Things (IoT) involves providing people with a range of seamless functionality, be it through automation, information delivery, etc. However, much of the IoT is opaque; it is often difficult for users to uncover and understand how and why particular functionality occurs, the sources of information, the entities involved, and so forth. We argue that automation scripts, as well as logs and provenance records could be leveraged to assist in illuminating the workings of connected and automated environments. \n\nThis paper explores the use of voice assistants (an accessible, intuitive and increasingly common interface) as a means for allowing users to interrogate what is happening in the IoT systems that surround them. In presenting an exploratory Alexa ‘Skill’, we discuss several considerations for the implementation of such a system. This work represents a starting point for considering how such assistants could help people better understand—and indeed, evaluate, challenge, and accept—technology that is increasingly pervading our world.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10523
        },
        {
          "affiliations": [],
          "personId": 15818
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4244,
      "typeId": 11437,
      "title": "ASSV: Handwritten Signature Verification Using Acoustic Signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As one kind of biological characteristics of people, handwritten signature has been widely used in the banking industry, government and education. Verifying handwritten signatures manually causes too much human cost, and its high probability of errors can threaten the property safety and even society stability. Therefore, the need for an automatic verification system is emphasized. This paper proposes a device-free on-line handwritten signature verification system ASSV, providing paper-based handwritten signature verification service. As far as  we know, ASSV is the first system which uses the changes of acoustic signals to realize signature verification. ASSV differs from previous on-line signature verification work in two aspects: 1. It requires neither a special sensor-instrumented pen nor a tablet; 2. People do not need to wear a device such as a smartwatch on the dominant hand for hand tracking. Differing from previous acoustic-based sensing systems, ASSV uses a novel chord-based method to estimate phase-related changes caused by tiny actions. Then based on the estimation, frequency-domain features are extracted by a discrete cosine transform (DCT). Moreover, a deep convolutional neural network (CNN) model fed with distance matrices is designed to verify signatures. Extensive experiments show that ASSV is a robust, efficient and secure system achieving an AUC of 98.7% and an EER of 5.5% with a low latency.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23632
        },
        {
          "affiliations": [],
          "personId": 21016
        },
        {
          "affiliations": [],
          "personId": 13416
        },
        {
          "affiliations": [],
          "personId": 17043
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 6293,
      "typeId": 11437,
      "title": "Assistive Wearables: Opportunities and Challenges",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 17969
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5269,
      "typeId": 11437,
      "title": "A Novel Smartphone Application for Indoor Positioning of Users based on Machine Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones are linked with individuals and are valuable and yet easily available sources for characterising users’ behaviour and activities. User’s location is among the characteristics of each individual that can be utilised in the provision of location-based services (LBs) in numerous scenarios such as remote health-care and interactive museums. Mobile phone tracking and positioning techniques approximate the position of a mobile phone and thereby its user, by disclosing the actual coordinate of a mobile phone. Considering the advances in positioning techniques, indoor positioning is still a challenging issue, because the coverage of satellite signals is limited in indoor environments. One of the promising solutions for indoor positioning is fingerprinting in which the signals of some known transmitters are measured in several reference points (RPs). This measured data, which is called dataset is stored and used to train a mathematical model that relates the received signal from the transmitters (model input) and the location of that user (the output of the model). Considering all the improvements in indoor positioning, there is still a gap between practical solutions and the optimal solution that provides near theoretical accuracy for positioning. This accuracy directly impacts the level of usability and reliability in corresponding LBSs. In this paper, we develop a smartphone app with the ability to be trained and detect users’ location, accurately. We use Gaussian Process Regression (GPR) as a probabilistic method to find the parameters of a non-linear and non-convex indoor positioning model. We collect a dataset of received signals’ strength (RSS) in several RPs by using a software which is prepared and installed on an Android smartphone.We also find the accurate 2σ confidence interval in the presented GPR method and evaluate the performance of the proposed method by measured data in a realistic scenario. The measurements confirm that our proposed method outperforms some conventional methods including KNN, SVR and PCA-SVR in terms of accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8432
        },
        {
          "affiliations": [],
          "personId": 22607
        },
        {
          "affiliations": [],
          "personId": 23472
        },
        {
          "affiliations": [],
          "personId": 8491
        },
        {
          "affiliations": [],
          "personId": 11189
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5270,
      "typeId": 11437,
      "title": "Measuring the Effects of Stress on Mobile Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Research shows that environmental factors such as ambient noise and cold ambience can render users situationally impaired, adversely affecting interaction with mobile devices. However, an internal factor which is known to negatively impact cognitive abilities - stress - has not been systematically investigated in terms of its impact on mobile interaction. In this paper, we report a study where we use the Trier Social Stress Test to induce stress on participants, and investigate its effect on three aspects of mobile interaction: target acquisition, visual search, and text entry. We find that stress reduces completion time and accuracy during target acquisition tasks, as well as completion time during visual search tasks. Finally, we are able to directly contrast the magnitude of these effects to previously published effects of environmentally-caused impairments. Our work contributes to the growing body of literature on situational impairments.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13068
        },
        {
          "affiliations": [],
          "personId": 23812
        },
        {
          "affiliations": [],
          "personId": 13422
        },
        {
          "affiliations": [],
          "personId": 14432
        },
        {
          "affiliations": [],
          "personId": 13082
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 11200
        },
        {
          "affiliations": [],
          "personId": 23525
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 3223,
      "typeId": 11437,
      "title": "Recycling Price Prediction of Renewable Resources",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In the renewable resource recycling market, the recycling price is a key factor which can influence the recycling market. The price prediction of renewable resources is important, which is helpful to guide the development of the market. However, it is difficult to make accurate predictions because the data of recycling prices is highly random and complex. Moreover, there is a time lag between the predicted prices and the accurate prices. In this paper, we propose a combined model to solve these problems. Our model decomposes the prediction into two parts: trend price prediction with Moving Average (MA) and residual price prediction with Empirical Mode Decomposition (EMD) and Long Short-Term Memory (LSTM) neural network. Evaluations on a real-world dataset show that our model outperforms those classical prediction models with the error reduced by over 70% and solves the time lag problem.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22611
        },
        {
          "affiliations": [],
          "personId": 22108
        },
        {
          "affiliations": [],
          "personId": 9623
        },
        {
          "affiliations": [],
          "personId": 15476
        },
        {
          "affiliations": [],
          "personId": 14989
        },
        {
          "affiliations": [],
          "personId": 24147
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5272,
      "typeId": 11437,
      "title": "Drinks & Crowds: Characterizing Alcohol Consumption through Crowdsensing and Social Media",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The design of computational methods to recognize alcohol intake is a relevant problem in ubiquitous computing. While mobile crowdsensing and social media analytics are two current approaches to characterize alcohol consumption in everyday life, the question of how they can be integrated, to examine their relative value as informative of the drinking phenomenon and to exploit their complementarity towards the classification of drinking-related attributes, remains as an open issue. In this paper, we present a comparative study based on five years of Instagram data about alcohol consumption and a 200+ person crowdsensing campaign collected in the same country (Switzerland). Our contributions are two-fold. First, we conduct data analyses that uncover temporal, spatial, and social contextual patterns of alcohol consumption on weekend nights as represented by both crowdsensing and social media. This comparative analysis provides a contextual snapshot of the alcohol drinking practices of urban youth dwellers. Second, we use a machine learning framework to classify individual drinking events according to alcohol and non-alcohol categories, using images features and contextual cues from individual and joint data sources. Our best performing models give an accuracy of 82.3% on alcohol category classification (against a baseline of 48.5%) and 90% on alcohol/non-alcohol classification (against a baseline of 65.9%) using a fusion of image features and contextual cues in this task.Our work uncovers important patterns in drinking behaviour across these two datasets and the results of study are promising towards developing systems that use machine learning for self-monitoring of alcohol consumption.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9518
        },
        {
          "affiliations": [],
          "personId": 10306
        },
        {
          "affiliations": [],
          "personId": 20422
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 4249,
      "typeId": 11437,
      "title": "Tracing the Intangible: The Curious Gestures of Crafts Cultural Heritage",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This article describes the methodology and technology for data collection of hand gestures of master craftspeople, with a case study tracing the Intangible Cultural Heritage practice of Horse Tail Embroiderers of Shui ethnic minority in collaboration with GYPEC  in Guiyang, Southwestern China. We describe unique technological design solutions to enable mobility to remote villages, and freedom of movement to capture the hand gestures of master craftspeople. The significance of this work is vitally important, as it outlines a technique to digitally record, analyse and archive the intricate dynamics of craft practices.We contextualise the research within a contemporary context, and describe the aims and objectives of the research for an interactive media museum interface to provide new insights into traditional practices. Finally, we propose future potential to investigate embodied knowledge and alternative pedagogical applications for innovative contemporary design.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11924
        },
        {
          "affiliations": [],
          "personId": 16585
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4254,
      "typeId": 11437,
      "title": "What causes the adoption failure of service robots? A Case of Henn-na Hotel in Japan",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 17967
        },
        {
          "affiliations": [],
          "personId": 22036
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6303,
      "typeId": 11437,
      "title": "Towards Early Detection of Depression through Smartphone Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Major depressive disorder is a complex and common mental health disorder that is heterogeneous and varies between individuals. Predictive measures have previously been used to predict depression in individuals. Given the complexity, heterogeneity of major depressive disorder in individuals, and the scarcity of labelled objective depressive behavioural data, predictive measures have shown limited applicability in detecting the early onset of depression. We present a developed system that collects similar smartphone sensor data like in previous predictive analysis studies. We discuss that anomaly detection and entropy analysis methods are best suited for developing new metrics for the early detection of the onset and progression of major depressive disorder.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12785
        },
        {
          "affiliations": [],
          "personId": 13432
        },
        {
          "affiliations": [],
          "personId": 15187
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4260,
      "typeId": 11437,
      "title": "Amateur: Augmented Reality Based Vehicle Navigation System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents Amateur, an augmented reality based vehicle navigation system using commodity smart phones.  \\ Amateur digests the navigation information from a digital map, matches it into live road condition video captured by smart phone, and directly annotates the navigation instructions on the video stream.  \\ Navigation can thus be augmented with remarkably improved user experience and accuracy.  \\ The Amateur design entails two major challenges, including the lane identification and the intersection inference so as to correctly annotate navigation instructions for lane-changing and intersection-turning.  \\ In this paper, we propose a particle filter based design, assisted by inertial motion sensors and lane markers, to tolerate incomplete and even erroneous detection of road conditions.  \\ We further leverage traffic lights as land markers to estimate the position of each intersection to accurately annotate the navigation instructions.  \\ We develop a prototype system on Android mobile phones and test our system in a total number of more than 300km travel distance on 50 different taxi cabs in a city.  \\ The evaluation results suggest that our system can timely provide correct instructions to navigate drivers.  \\ Our system can identify lanes in 2s with 92.7% accuracy and detect traffic lights with 95.29% accuracy.  \\ Overall, the accuracy of the navigation signs placement is less than 105 pixels on the screen throughout the experiments. \\ The feedback from 30 taxi drivers indicates that Amateur provides an improved experience compared to traditional navigation systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16051
        },
        {
          "affiliations": [],
          "personId": 18342
        },
        {
          "affiliations": [],
          "personId": 15143
        },
        {
          "affiliations": [],
          "personId": 21036
        }
      ],
      "sessionIds": [
        1680
      ],
      "eventIds": []
    },
    {
      "id": 3237,
      "typeId": 11437,
      "title": "EarEcho: Using Ear Canal Echo for Wearable Authentication",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smart wearable devices have recently become one of the major technological trends and been widely adopted by the general public. Wireless earphones, in particular, have seen a skyrocketing growth due to its great usability and convenience. With the goal of seeking a more unobtrusive wearable authentication method that the users can easily use and conveniently access, in this study we present EarEcho as a novel, affordable, user-friendly biometric authentication solution. EarEcho takes advantages of the unique physical and geometrical characteristics of human ear canal and assesses the content-free acoustic features of in-ear sound waves for user authentication in a wearable and mobile manner. We implemented the proposed EarEcho on a proof-of-concept prototype and tested it among 20 subjects under diverse application scenarios. We can achieve a recall of 94.19% and precision of 95.16% for one-time authentication, while a recall of 97.55% and precision of 97.57% for continuous authentication. EarEcho has demonstrated its stability over time and robustness to cope with the uncertainties on the varying background noises, body motions, and sound pressure levels.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20971
        },
        {
          "affiliations": [],
          "personId": 22125
        },
        {
          "affiliations": [],
          "personId": 21817
        },
        {
          "affiliations": [],
          "personId": 24339
        },
        {
          "affiliations": [],
          "personId": 23582
        }
      ],
      "sessionIds": [
        2039
      ],
      "eventIds": []
    },
    {
      "id": 4266,
      "typeId": 11437,
      "title": "A Data-Driven Misbehavior Detection System for Connected Autonomous Vehicles",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In a Connected and Autonomous Vehicle (CAV) system, some malicious CAVs may send out false information in vehicle-to-vehicle communication to gain benefits or cause safety-related accidents. Previous false data detection methods are not sufficient to meet the accuracy and real-time requirements. In this paper, we propose a data-driven misbehavior detection system (MDS) (running by each CAV) that checks the consistency between the estimated and actually reported driving state (i.e., velocity, acceleration, brake status, steering angle) of an alerting CAV. First, MDS predicts the driving state using Gaussian mixture model based Mixture Density Network incorporating Recurrent Neural Network that can catch the driving behavior patterns of a CAV. Second, MDS extends the existing Krauss traffic flow model and uses it to consider the overall traffic flow of the road to make the predicted driving state more accurate. Finally, for a given received alert, a CAV validates the alert by checking the consistency between the predicted and actually reported driving states of the alerting CAV. We conduct extensive simulation studies based on a real driving dataset we collected from 29 participants and the Simulator for Urban MObility (SUMO) traffic simulator. The experimental results show that the false information detection rate of the proposed MDS is higher than other existing systems in different alert scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10245
        },
        {
          "affiliations": [],
          "personId": 13799
        }
      ],
      "sessionIds": [
        1680
      ],
      "eventIds": []
    },
    {
      "id": 7339,
      "typeId": 11437,
      "title": "Poster: Augmented Reality Smartphone Compasses: Opportunity or oxymoron",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The development of augmented reality capabilities on smartphones has led to the emergence of a range of AR apps, including AR compasses.  Some of these apps claim to be as good as professional magnetic navigation compasses, and suitable for navigation use.  This poster presents detailed measurements of compass deviation (error) curves and offset errors for augmented reality compass apps on 17 mobile devices. The magnitude of the deviation errors measured casts serious doubt on claims the apps are appropriate for navigation purposes. This in turn emphasizes the need for the ubiquitous computing community to help ensure adequate awareness of the limitations of some onboard sensors, including compasses, on devices such as smartphones.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21369
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6316,
      "typeId": 11437,
      "title": "Wrinkling and Bending Effect in Wireless Power Transfer on Clothes",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we examine the effect of wrinkling and bending generated in coils when used in wearable devices and investigate how they affect the wireless power supply system. A wrinkle is modeled as a semi-cylindrical deformation in a coil, and a bend is modeled as the deformation of an entire coil plane. The effect of these deformations w evaluated by simulation. The result showed that the maximum efficiency is decreased by both deformations, and wrinkling decreases average efficiency more than bending. In addition, efficiency decreases more from multiple wrinkles than from one bigger wrinkle.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12898
        },
        {
          "affiliations": [],
          "personId": 23230
        },
        {
          "affiliations": [],
          "personId": 13359
        },
        {
          "affiliations": [],
          "personId": 12640
        }
      ],
      "sessionIds": [
        1861
      ],
      "eventIds": []
    },
    {
      "id": 7347,
      "typeId": 11437,
      "title": "The City as a Personal Assistant",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 16928
        },
        {
          "affiliations": [],
          "personId": 18297
        },
        {
          "affiliations": [],
          "personId": 23587
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7348,
      "typeId": 11437,
      "title": "Poster: Exploring the Usefulness of Bluetooth and WiFi Proximity for Transportation Mode Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding the mobility patterns of large groups of people is essential in transport planning. Today's assessments rely on questionnaires or self-reported data, which are cumbersome, expensive, and prone to errors. With recent developments in mobile and ubiquitous computing, it has become feasible to automate this process and classify transportation modes using data collected by users' smartphones. Previous work has mainly considered GPS and accelerometers; however, the achieved accuracies were often insufficient. We propose a novel method which also considers the proximity patterns of WiFi and Bluetooth (BT) devices in the environment, which are expected to be quite specific to the different transportation modes. In this poster, we present the promising results of a preliminary study in Zurich.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17488
        },
        {
          "affiliations": [],
          "personId": 21214
        },
        {
          "affiliations": [],
          "personId": 20671
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7350,
      "typeId": 11437,
      "title": "Demo: ARIA: Interactive Damage Prediction System for Urban Flood using Simulation and Emulation Federation Platform",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This demonstration shows an interactive urban flood damage prediction system \"ARIA\" that simulates urban flood, the sufferer, and network failure in an integrated manner. In terms of disaster mitigation, it is important to confirm an affected area and issue an evacuation advisory. ARIA predicts flood damages - the number of sufferers or the locations of flooded roads - and figures out the suitable timing of an evacuation advisory while incorporating actual measurement values like precipitation, river water level and person flow data observed during flood occurrence using data assimilation method. We propose flood damage prediction system, which cooperates conventional proprietary simulators for flood/evacuation/network damage analysis using simulation and emulation federation platform ``Smithsonian\". ARIA aims to accurately simulate actual disaster phenomena to consider how flood damages affect evacuation behavior based on the mutual impact of road condition and network damage caused by floods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20461
        },
        {
          "affiliations": [],
          "personId": 9102
        },
        {
          "affiliations": [],
          "personId": 12627
        },
        {
          "affiliations": [],
          "personId": 10584
        },
        {
          "affiliations": [],
          "personId": 16679
        },
        {
          "affiliations": [],
          "personId": 9451
        },
        {
          "affiliations": [],
          "personId": 22587
        },
        {
          "affiliations": [],
          "personId": 9569
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6327,
      "typeId": 11437,
      "title": "LeakDoctor: Toward Automatically Diagnosing Privacy Leaks in Mobile Applications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the enormous popularity of smartphones, millions of mobile apps are developed to provide rich functionalities for users by accessing certain personal data, leading to great privacy concerns. To address this problem, many approaches have been proposed to detect privacy disclosures in mobile apps, but they largely fail to automatically determine whether the privacy disclosures are necessary for the functionality of apps. As a result, security analysts may easily face with a large number of false positives when directly adopting such approaches for app analysis. In this paper, we propose LeakDoctor, an analysis system seeking to automatically diagnose privacy leaks by judging if a privacy disclosure from an app is necessary for some functionality of the app. Functionality-irrelevant privacy disclosures are not justifiable, so considered as potential privacy leak cases. To achieve this goal, LeakDoctor integrates dynamic response diﬀerential analysis with static response taint analysis. In addition, it employs a novel technique to locate the program statements of each privacy disclosure. We implement a prototype of LeakDoctor and evaluate it against 1060 apps, which contain 2,095 known disclosure cases. Our experimental results show that LeakDoctor can automatically determine that 71.9% of the privacy disclosure cases indeed serve apps’ functionalities and are justifiable. Hence, with the diagnosis results of LeakDoctor, analysts may avoid analyzing many justifiable privacy disclosures and only focus on the those unjustifiable cases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22461
        },
        {
          "affiliations": [],
          "personId": 14497
        },
        {
          "affiliations": [],
          "personId": 8679
        },
        {
          "affiliations": [],
          "personId": 22528
        },
        {
          "affiliations": [],
          "personId": 19427
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 7351,
      "typeId": 11437,
      "title": "Using Built-In Sensors to Predict and Utilize User Satisfaction for CPU Settings on Smartphones",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding user experience/satisfaction with mobile systems in order to manage computational resources has become a popular approach in recent years. One of the key issues in this area is to gauge user satisfaction. In this paper, we propose and evaluate a system to save energy by altering CPU core count and frequency while keeping users satisfied. Specifically, the system uses the sensor data collected from two popular personal devices: a smartphone and a smartwatch. In the proposed architecture, we first develop prediction models by collecting sensor data along with user performance satisfaction inputs. Then, our system predicts users' current satisfaction and sets CPU core/frequency based on these predictions in real-time. We observe that sensor data gathered from these two devices are highly correlated with users' instantaneous satisfaction of the phone. We evaluate the proposed system by developing and comparing two different models. First, we develop a user-independent (user-oblivious) model by using data gathered from 10 users. Second, we develop user-dependent (personal) models for 20 different users. We demonstrate that both models can predict satisfaction with over 97% accuracy on average when a binary satisfaction model is used (i.e., users indicating satisfied versus unsatisfied). The prediction accuracy is over 91% on average if a 3-level satisfaction model is used. Our results also show that when compared to default scheme, the user-independent and user-dependent models save 8.96% and 10.12% of the total system energy on average, respectively, without impacting user satisfaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16902
        },
        {
          "affiliations": [],
          "personId": 12714
        }
      ],
      "sessionIds": [
        2158
      ],
      "eventIds": []
    },
    {
      "id": 5305,
      "typeId": 11437,
      "title": "Myco-accessories: Sustainable Wearables with Biodegradable Materials",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sustainability has been addressed in fashion, product design and furniture making fields. Recycling and disposing e-textiles has been a concern in this community for years\\cite{etextile_recycling}; however, the impact of designing sustainable wearables is still new territory that requires more exploration. This paper approaches sustainability in the prototyping process by producing wearables that make use of biodegradable material for embedding electronics. We have used mycelium, that unlike other biodegradable materials such as kombucha, algae, or bioplastics, has heat resistance, thermal resistance and hydrophobic properties which makes it suitable to apply in wearables. Moreover, this paper proposes a sustainable life cycle that uses biodegradable materials to embed electronics. For example, we embedded an electronic circuit into mycelium skin to make an accessory. After the accessory has been worn, the electronic components can be reused and the mycelium skin composted. Lastly, we present our method for growing mycelium, our design process using common techniques in embedding electronics, and Myco-accessories as applications to envision the possibilities of this material. This paper aims to contribute to prototyping wearables sustainably by intertwining biomaterials and electronics.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17003
        },
        {
          "affiliations": [],
          "personId": 15643
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3259,
      "typeId": 11437,
      "title": "Detecting Abnormal Behavior in the Transportation Planning using Long Short Term Memories and a Contextualized Dynamic Threshold",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Unsupervised anomaly detection in time-series data is crucial for both machine learning research and industrial applications. Over the past few years, the operational efficiencies of logistics agencies have decreased because of a lack of understanding on how best to address potential client requests. However, current anomaly detec- tion approaches have been inefficient in distinguishing normal and abnormal behaviors from high dimensional data. In this study, we aimed to assist decision makers and improve anomaly detection by proposing a Long Short Term Memory (LSTM) approach with dy- namic threshold detection. In the proposed methodology, first, data were processed and inputted into an LSTM network to determine temporal dependency. Second, a contextualized dynamic threshold was determined to detect anomalies. To demonstrate the practical- ity of our model, real operational data were used for evaluation and our model was shown to more accurately detect anomalies, with values of 0.836 and 0.842 for precision and recall, respectively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14425
        },
        {
          "affiliations": [],
          "personId": 16901
        },
        {
          "affiliations": [],
          "personId": 13403
        },
        {
          "affiliations": [],
          "personId": 12242
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5307,
      "typeId": 11437,
      "title": "Using Unobtrusive Wearable Sensors to Measure the Physiological Synchrony Between Presenters and Audience Members",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The widespread of mobile and wearable devices enables new approaches for unobtrusive and continuous monitoring of humans‚Äô behavior, physiological state, interactions and more. Within this line of research, we investigate whether the physiological synchrony between presenters and audience members can be used to quantify audience‚Äôs experience during presentations. To validate this hypothesis, we collect data from 17 presenters and 6 audience members during a two-days conference. For 40, unique presenter-audience pairs we obtain electrodermal activity (EDA) and for 28 pairs inter-beat interval (IBI) traces as well as self-reports on different aspects of the experience: engagement, immersion, enjoyment, and satisfaction. We then apply seven approaches for measuring the synchrony of physiological signals and we contextualize these measures using metrics derived from the self-reports. Our results show that the physiological synchrony is significantly positively correlated (r=0.46, p=0.002) to the synchrony of presenters and audience members‚Äô self-reported engagement. We further observe that approaches that measure the distance between two physiological signals are those most related to participants‚Äô agreement on engagement. We demonstrate the existence of physiological synchrony between engaged presenter and audience participants by comparing it to the non-engaged pairs. Our findings can be used to provide automated presenter-audience feedback in a conference setting and may be applicable in other scenarios, including education (teacher-student), arts (performer-audience), or industry meetings (presenter-audience).",
      "authors": [
        {
          "affiliations": [],
          "personId": 10354
        },
        {
          "affiliations": [],
          "personId": 23395
        },
        {
          "affiliations": [],
          "personId": 11457
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 3262,
      "typeId": 11437,
      "title": "Attention Computing: Overview of Mobile Sensing Applied to Measuring Attention",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Measuring attention is a frequent by-product of mobile sensing-based studies, which either measure the effectiveness of ESM deliveries or measure the subjects interruptibility. Whether the exclusion of being attention-specific is intentional, due to perceived technological restraints, or a potential oversight from the researchers, varies. Attention level could be effectively measured with the technologies and methodologies we currently have, by adapting to using continuous measurements of attention fluctuations. Many clinically researched technologies, as well as sensing-based analysis methods, could be leveraged for this purpose. This paper invites co-researchers to assess the use of novel ways to measure attention in their future endeavours.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13432
        },
        {
          "affiliations": [],
          "personId": 10224
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6338,
      "typeId": 11437,
      "title": "HeadGesture: Hands-Free Input Approach Leveraging Head Movements for HMD Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose HeadGesture, a hands-free input approach to interact with Head Mounted Display (HMD) devices. Using HeadGesture, users do not need to raise their arms to perform gestures or operate remote controllers in the air. Instead, they perform simple gestures with head movement to interact with the devices. In this way, users' hands are free to perform other tasks. This approach also reduces the hand occlusion and alleviates arm fatigue. However, one main challenge for HeadGesture is to distinguish the defined gestures from unintentional movements. To generate intuitive gestures and address the issue of gesture recognition, we proceed through a process of Exploration‚ÄìDesign-Implementation-Evaluation. We first design the gesture set through experiments on gesture space exploration and gesture elicitation with users. Then, we implement algorithms to recognize the gestures, including gesture segmentation, data reformation, feature extraction, and machine learning based classification. Finally, we evaluate user performance of HeadGesture in the target selection experiment and application tests. The results demonstrate that the performance of HeadGesture is comparable to mid-air hand gestures. Additionally, users feel significantly less fatigue than when using hand gestures. Overall, we expect HeadGesture to be an efficient supplementary input approach for HMD devices.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8655
        },
        {
          "affiliations": [],
          "personId": 14464
        },
        {
          "affiliations": [],
          "personId": 9092
        },
        {
          "affiliations": [],
          "personId": 15758
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 6339,
      "typeId": 11437,
      "title": "Understanding Drivers Wellbeing: Quantitative Study Analysis and Wearable Experiment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Driver’s wellbeing has a positive impact on driving behavior and experience. Another way, driver’s wellbeing depends on their daily lifestyle, demography, traffic and road conditions. Poor conditions in such factors are responsible for low wellbeing. These factors also initiate driving stress. A simple technology approach can play an important role to monitor driver’s wellbeing, and help to provide better ways of increasing self-awareness regarding wellbeing. Here, we conducted a quantitative study on 88 drivers, and finally present a low cost wearable approach to support the drivers in the context of Bangladesh for better wellbeing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23862
        },
        {
          "affiliations": [],
          "personId": 18133
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4291,
      "typeId": 11437,
      "title": "Unsupervised Factory Activity Recognition with Wearable Sensors Using Process Instruction Information",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an unsupervised method for recognizing assembly work done by factory workers by using wearable sensor data. Such assembly work is a common part of line production systems and typically involves the factory workers performing a repetitive work process made up of a sequence of manual operations, such as setting a board on a workbench and screwing parts onto the board. This study aims to recognize the starting and ending times for individual operations in such work processes through analysis of sensor data collected from the workers along with analysis of the process instructions that detail and describe the flow of operations for each work process.\nWe propose a particle-filter-based factory activity recognition method that leverages (i) trend changes in the sensor data detected by a nonparametric Bayesian hidden Markov model, (ii) semantic similarities between operations discovered in the process instructions, (iii) sensor-data similarities between consecutive repetitions of individual operations, and (iv) frequent sensor-data patterns (motifs) discovered in the overall assembly work processes. \nWe evaluated the proposed method using sensor data from six workers collected in actual factories, achieving a recognition accuracy of 80% (macro-averaged F-measure).",
      "authors": [
        {
          "affiliations": [],
          "personId": 14887
        },
        {
          "affiliations": [],
          "personId": 13219
        },
        {
          "affiliations": [],
          "personId": 13653
        },
        {
          "affiliations": [],
          "personId": 24265
        },
        {
          "affiliations": [],
          "personId": 15446
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 4295,
      "typeId": 11437,
      "title": "CellTrans: Private Car or Public Transportation? Infer Users’ Main Transportation Modes at Urban Scale with Cellular Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding citizens' main transportation modes at urban scale is beneficial to a range of applications, such as urban planning, user profiling, transportation management, and precision marketing. Previous methods on mode inference are mostly focused on utilizing GPS data with high spatiotemporal granularity. However, due to high costs of GPS data collection, the previous work typically is in small scales. In contrast, the cellular data logging interactions between cellphone users and cell towers cover much higher population given the ubiquity of cellphones. Nevertheless, utilizing cellular data introduces new challenges given their low spatiotemporal granularity compared to GPS data. In this paper, we design CellTrans, a novel framework to survey users' main transportation modes (public transportation or private car) at urban scale with cellular data. CellTrans extracts various mobility features that are pertinent to users' main transportation modes and presents solutions for different application scenarios including when there are no labeled users in the studied cities. We evaluate CellTrans on two real-world large-scale cellular datasets covering 3 million users, among which 2,589 users are with labels to indicate their main transportation modes. As a result, we assess our method not only quantitatively with labeled users by cross-validation, but also qualitatively with the whole population and survey data. The experiments show that CellTrans infers users' main transportation modes with accuracy over 80% (with a performance gain of 20% compared to state-of-the-art), and CellTrans remains effective when applied at urban scale to the whole population.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23099
        },
        {
          "affiliations": [],
          "personId": 15930
        },
        {
          "affiliations": [],
          "personId": 14974
        },
        {
          "affiliations": [],
          "personId": 11897
        },
        {
          "affiliations": [],
          "personId": 17887
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 5323,
      "typeId": 11437,
      "title": "Demo: AiRite: Infrastructure-Free Cursive Writing & Drawing In Air Using Smart Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Inertial sensors have been used for tracking applications on every scale for decades. Commercial IMUs suffer from a spectrum of errors such as axes misalignment, bias etc. leading to a drifted output. Approximation models are usually implemented to address these issues which are highly sensor-dependent with constrained testing and sometimes use complementary infrastructure which limits their use in the wild. In this demonstration we introduce 'AiRite', an effective solution for 3-D tracking of a smart device using only the onboard IMU. Trajectories of basic shapes and cursive words written in air using smart devices are visualized in 3-D, which are first observations of their kind in the field. We demonstrate device-independence and ubiquity of our tracking method by using different smartphones and smartwatches for writing in air.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9015
        },
        {
          "affiliations": [],
          "personId": 9771
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7373,
      "typeId": 11437,
      "title": "Psychophysiological Monitoring of Aerospace Crew State",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As next-generation space exploration missions necessitate increasingly autonomous systems, there is a critical need to better detect and anticipate crewmember interactions with these systems. The success of present and future autonomous technology in exploration spaceflight is ultimately dependent upon safe and efficient interaction with the human operator. Optimal interaction is particularly important for surface missions during highly coordinated extravehicular activity (EVA), which consists of high physical and cognitive demands with limited ground support. Crew functional state may be affected by a number of variables including workload, stress, and motivation. Real-time assessments of crew state that do not require a crewmember's time and attention to complete will be especially important to assess operational performance and behavioral health during flight. In response to the need for objective, passive assessment of crew state, the aim of this work is to develop an accurate and precise prediction model of human functional state for surface EVA using multi-modal psychophysiological sensing. The psychophysiological monitoring approach relies on extracting a set of features from physiological signals and using these features to classify an operator's cognitive state. This work aims to compile a non-invasive sensor suite to collect physiological data in real-time. Training data during cognitive and more complex functional tasks will be used to develop a classifier to discriminate high and low cognitive workload crew states. The classifier will then be tested in an operationally relevant EVA simulation to predict cognitive workload over time. Once a crew state is determined, further research into specific countermeasures, such as decision support systems, would be necessary to optimize the automation and improve crew state and operational performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13710
        },
        {
          "affiliations": [],
          "personId": 24029
        },
        {
          "affiliations": [],
          "personId": 11779
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5327,
      "typeId": 11437,
      "title": "What is That in Your Hand? Recognizing Grasped Objects via Forearm Electromyography Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Knowing the object in hand can offer essential contextual information revealing a user's fine-grained activities. In this paper, we investigate the feasibility, accuracy, and robustness of recognizing the uninstrumented object in a user's hand by sensing and decoding her forearm muscular activities via off-the-shelf electromyography (EMG) sensors. We present results from three studies to advance our fundamental understanding of the opportunities that EMG brings in object interaction recognition. In the first study, we investigated the influence of physical properties of objects including shape, size, weight, and all these factors together on EMG signals. We also conducted a thorough exploration of the feature spaces and sensor positions which can provide a solid base to rely on for future designers and practitioners for such interactive technique. In the second study, we assessed the feasibility and accuracy of inferring the types of grasped objects via using forearm muscular activity as a cue. Our results indicate that the types of objects can be recognized with up to 94.2% accuracy by employing user-dependent training. In the third study, we investigated the robustness of this approach in a realistic office setting where users were allowed to interact with objects as they would naturally. Our approach achieved up to 82.5% accuracy in discriminating 15 types of objects, even while the training and testing phrases were purposefully performed on different days to incorporate changes in EMG patterns over time. Overall, this work contributes a set of fundamental findings and guidelines on using EMG technologies for object-based activity tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19343
        },
        {
          "affiliations": [],
          "personId": 14380
        },
        {
          "affiliations": [],
          "personId": 23184
        },
        {
          "affiliations": [],
          "personId": 11154
        },
        {
          "affiliations": [],
          "personId": 11148
        },
        {
          "affiliations": [],
          "personId": 24339
        },
        {
          "affiliations": [],
          "personId": 21934
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 6353,
      "typeId": 11437,
      "title": "Poster: Classification of Reading and Not Reading Behavior Based on Eye Movement Analysis",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, many researchers analyze reading behavior with eye trackers. Various traits of reading like engagement, or text difficulty have been observed in laboratory settings. But, their automatic application for daily life is usually prevented by one question: when is somebody reading? We have developed a tool to classify short sequences of fixations from eye gaze data into reading and not reading. Our specific use case is the Vocabulometer, a website for learning English by reading texts. We used supervised learning on data from nonnative English speakers to train decision trees for the classification. With features based on vertical eye movement, we achieved 93.1\\% of correct classifications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16028
        },
        {
          "affiliations": [],
          "personId": 11112
        },
        {
          "affiliations": [],
          "personId": 14504
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3282,
      "typeId": 11437,
      "title": "Transportation Recognition with the Sussex-Huawei Locomotion Challenge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sussex-Huawei Locomotion-Transportation (SHL) recognition challenge provides a large dataset including train, validation and test data. Our team name is ICT-BUPT League. Moreover, the main work is to recognition eight transportation modes. Our method combines the CNN and LSTM. In this system, CNNs allow us to learn suitable features representation for recognition that are robust against transportation mode recognition. We make use of LSTM units on the CNN output, which play the role of a structured dimensionality reduction on the feature vector, leading to drastic improvements in transportation mode recognition performance. As a result, the CNN+LSTM transportation mode recognition system could predict eight classes with a success rate of 76%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18462
        },
        {
          "affiliations": [],
          "personId": 9819
        },
        {
          "affiliations": [],
          "personId": 9419
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3284,
      "typeId": 11437,
      "title": "WellComp 2019: Second International Workshop on Computing for Well-Being",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the advancements in ubiquitous computing, ubicomp technology has deeply spread into our daily lives, including office work, home and house-keeping, health management, transportation, or even urban living environments. Furthermore, beyond the initial metric of computing, such as “efficiency” and “productivity”, the benefits that people (users) benefit on a well-being perspective based on such ubiquitous technology has been greatly paid attention in the recent years. In our second “WellComp” (Computing for Well-being) workshop, we intensively discuss about the contribution of ubiquitous computing towards users’ well-being that covers physical, mental, and social wellness (and their combinations), from the viewpoints of various different layers of computing. Having strong international organization members in various ubicomp research domains, WellComp 2019 will bring together researchers and practitioners from the academia and industry to explore versatile topics related to wellbeing and ubiquitous computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22964
        },
        {
          "affiliations": [],
          "personId": 21348
        },
        {
          "affiliations": [],
          "personId": 13408
        },
        {
          "affiliations": [],
          "personId": 23587
        },
        {
          "affiliations": [],
          "personId": 11390
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5333,
      "typeId": 11437,
      "title": "A Cuttable Wireless Power Transfer Sheet",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a cuttable wireless power transfer sheet which allows users to modify its size and shape. \\ This intuitive manipulation allows the users to easily add wireless power transmission capabilities to everyday objects. \\ The properties of the sheet such as thinness, flexibility, and lightness make our sheet highly compatible with various configurations. \\ We contribute a set of technical principles for the design of circuitry, which integrates H-tree wiring and time division power supply techniques. \\ H-tree wiring allows the sheet to remain functional even when cut from the outside of the sheet, whereas time division power supply avoids the reduction in power transfer efficiency caused by the magnetic interference between adjacent transmitter coils. \\ Through the evaluations, we found that our time division power supply scheme mitigates the degradation of power transfer efficiency and successfully improves the average efficiency. \\ Furthermore, we present four applications which integrates our sheet into daily objects: wireless charging furniture, bag, jacket, and craft; these applications confirmed the feasibility of our prototype.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21356
        },
        {
          "affiliations": [],
          "personId": 14738
        },
        {
          "affiliations": [],
          "personId": 20879
        },
        {
          "affiliations": [],
          "personId": 15754
        },
        {
          "affiliations": [],
          "personId": 21865
        }
      ],
      "sessionIds": [
        1861
      ],
      "eventIds": []
    },
    {
      "id": 5335,
      "typeId": 11437,
      "title": "“Back to Real Pictures”: A Cross-generational Understanding of Users' Mental Models of Photo Cloud Storage",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Personal pictures storage is currently split between a myriad of physical and digital tools. Cloud photo storage and social networks are seeing increasing adoption, and are being recommended to families (especially older generations) as digital pictures solutions. The ubiquity of these platforms raises the question of whether the design of their photo-based operations consider the mental models of their cross-generational users. Understanding mental models is a key factor for the usability (and adoption) of these technologies. Previous works have observed that perceptions of digital storage limit adoption, especially for older users. However, we do not yet understand users’ mental models of these applications. This impedes efforts to design applications better matching diverse user needs. We present here a cross-generational investigation of users' mental models of ubiquitous picture technologies, including cloud storage and social sharing. We find that mental models are split (both between generations and domains), contributing to lower adoption by older adults. Our analysis reveals that digital tools need to understand their roots in physical pictures and bridge this divide by including physical concepts as an aspect of use, if we are to support cross-generational interactions with personal and family pictures.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15085
        },
        {
          "affiliations": [],
          "personId": 19588
        }
      ],
      "sessionIds": [
        1195
      ],
      "eventIds": []
    },
    {
      "id": 4314,
      "typeId": 11437,
      "title": "Poster: Model-Based Real Time Analysis of Distributed Human Activity Recognition Stages in Wireless Sensor Networks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In the paper at hand, a model-based design and energy estimation approach for wireless sensor nodes in human activity recognition systems is extended. Entire wireless body area sensor networks are modeled and analyzed w.r.t. their real time capabilities of different software mappings on a system level.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18166
        },
        {
          "affiliations": [],
          "personId": 13300
        },
        {
          "affiliations": [],
          "personId": 21954
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7386,
      "typeId": 11437,
      "title": "?Hear me out?: Smart speaker based conversational agent to monitor symptoms in mental health",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Difference in features of voice such as tone, volume, intonation, and rate of speech have been suggested as sensitive and valid measures of mental illness. Researchers have used analysis of voice recordings during phone calls, response to the IVR systems and smartphone based conversational agents as a marker in continuous monitoring of symptoms and effect of treatment in patients with mental illness. While these methods of recording the patient's voice have been considered efficient, they come with a number of issues in terms of adoption, privacy, security, data storage etc. To address these issues we propose a smart speaker based conversational agent -- ``Hear me out''. In this paper, we describe the proposed system, rationale behind using smart speakers, and the challenges we are facing in the design of the system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24322
        },
        {
          "affiliations": [],
          "personId": 20929
        },
        {
          "affiliations": [],
          "personId": 23811
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7388,
      "typeId": 11437,
      "title": "Inferring Fine-Grained Air Pollution Map via a Spatiotemporal Super-Resolution Scheme",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Air pollution monitoring is a concerned issue in urban management. Nowadays, vehicle-based mobile sensing systems are deployed to enhance sensing granularity. However, in these systems, the number of online sensors may change over time and the long-term maintenance is costly. Therefore, an inference algorithm is necessary to maintain the high spatiotemporal granularity of pollution field under both dense and sparse sampling. In this paper, we propose an algorithm based on super-resolution scheme. To address the challenges of complex external factors and spatiotemporal dependencies, three modules are included: a heterogeneous data fusion subnet to extract useful information from external data, a spatiotemporal residual subnet to capture the spatiotemporal dependencies in pollution field, and an upsampling subnet to generate the fine-grained pollution map. Experiments on real-world datasets show that our model outperforms the state-of-the-art baselines.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9838
        },
        {
          "affiliations": [],
          "personId": 19569
        },
        {
          "affiliations": [],
          "personId": 18241
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6365,
      "typeId": 11437,
      "title": "Digital Craftsmanship in the Wearable Senses Lab",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable and e-textiles as a field has tended to overlook its own documentation, as notions and overarching ideas are developed over time and across individual projects. We would like to begin addressing this by charting the development of Digital Craftsmanship through a number of projects over time. Practically, we propose to show a small selection of garments and samples alongside a simple framework for future documentation of work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17866
        },
        {
          "affiliations": [],
          "personId": 11548
        },
        {
          "affiliations": [],
          "personId": 19772
        },
        {
          "affiliations": [],
          "personId": 12406
        },
        {
          "affiliations": [],
          "personId": 22538
        },
        {
          "affiliations": [],
          "personId": 15029
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7389,
      "typeId": 11437,
      "title": "A Vision-based Deep On-Device Intelligent Bus Stop Recognition System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Intelligent public transportation systems are the cornerstone to any smart city, given the advancements made in the field of self-driving autonomous vehicles -- particularly for autonomous buses, where it becomes really difficult to systematize a way to identify the arrival of a bus stop on-the-fly for the bus to appropriately halt and notify its passengers. This paper proposes an automatic and intelligent bus stop recognition system built on computer vision techniques, deployed on a low-cost single-board computing platform with minimal human supervision. The on-device recognition engine aims to extract the features of a bus stop and its surrounding environment, which eliminates the need for a conventional Global Positioning System (GPS) look-up, thereby alleviating network latency and accuracy issues. The dataset proposed in this paper consists of images of 11 different bus stops taken at different locations in Chennai, India during day and night. The core engine consists of a convolutional neural network (CNN) of size ~260 kB that is computationally lightweight for training and inference. In order to automatically scale and adapt to the dynamic landscape of bus stops over time, incremental learning (model updation) techniques were explored on-device from real-time incoming data points. Real-time incoming streams of images are unlabeled, hence suitable ground truthing strategies (like Active Learning), should help establish labels on-the-fly. Light-weight Bayesian Active Learning strategies using Bayesian Neural Networks using dropout (capable of representing model uncertainties) enable selection of the most informative images to query from an oracle. Intelligent rendering of the inference module by iteratively looking for better images on either sides of the bus stop environment propels the system towards human-like behavior. The proposed work can be integrated seamlessly into the widespread existing vision-based self-driving autonomous vehicles.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21839
        },
        {
          "affiliations": [],
          "personId": 18590
        },
        {
          "affiliations": [],
          "personId": 10544
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4320,
      "typeId": 11437,
      "title": "JacquardToolkit: Enabling Interactions with the Levi’s Jacquard Jacket",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "JacquardToolkit enables full access to the Levi’s Jacquard Jacket and the ability to create gestures such as our new “Force Touch” gesture. We conduct a 25 participant user study testing all available gestures for accuracy and intuitiveness. “Brush In” and “Brush Out” are rated highly intuitive and have high recognition accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16206
        },
        {
          "affiliations": [],
          "personId": 8240
        },
        {
          "affiliations": [],
          "personId": 11136
        },
        {
          "affiliations": [],
          "personId": 16503
        },
        {
          "affiliations": [],
          "personId": 12224
        },
        {
          "affiliations": [],
          "personId": 13758
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 5349,
      "typeId": 11437,
      "title": "Poster: Smartphone Colorimetry Using Ambient Subtraction: Application to Neonatal Jaundice Screening in Ghana",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A smartphone app to screen for neonatal jaundice has a large potential impact in reducing neonatal death and disability.  Our app, neoSCB, uses a colour measurement of the sclera to make a screening decision. Although there are numerous benefits of a smartphone-based approach, smartphone colour measurement that is accurate and repeatable is a challenge.  Using data from a clinical setting in Ghana, we compare sclera colour measurement using an ambient subtraction method to sclera colour measurement using a standard colour card method, and find they are comparable provided the subtracted signal-to-noise ratio (SSNR) is sufficient. Calculating a screening decision metric via the colour card method gave 100% sensitivity and 69% specificity (n=87), while applying the ambient subtraction method gave 100% sensitivity and 78% specificity (SSNR>3.5; n=50).",
      "authors": [
        {
          "affiliations": [],
          "personId": 11647
        },
        {
          "affiliations": [],
          "personId": 18896
        },
        {
          "affiliations": [],
          "personId": 9200
        },
        {
          "affiliations": [],
          "personId": 23221
        },
        {
          "affiliations": [],
          "personId": 16501
        },
        {
          "affiliations": [],
          "personId": 16718
        },
        {
          "affiliations": [],
          "personId": 10577
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3302,
      "typeId": 11437,
      "title": "Wearer-Centered Design for Animal Biotelemetry: Implementation and Wearability Test of a Prototype",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present an approach to designing wearer-centered biotelemetry for non-human (and human) animal wearers. Drawing from fundamental values and principles of user-centered design, we describe a wearer-centered framework to heuristically establish design requirements, which was used during a series of workshops to perform a requirements analysis for a cat-tracking device. The resulting requirements informed a feline-centered prototype whose wearability was evaluated with cat wearers. Compared to the wearability of previously tested off-the-shelf devices, our findings show an improvement and suggest that our framework-based approach can help design teams with a range of skills to systematically design for wearability.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16604
        },
        {
          "affiliations": [],
          "personId": 12190
        },
        {
          "affiliations": [],
          "personId": 16793
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 5351,
      "typeId": 11437,
      "title": "CaLmi: Stress Management in Intelligent Homes",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In today's fast-paced and demanding society, more and more people are suffering from stress-related problems; however, intelligent environments can be equipped with facilities that assist in keeping it under control. This paper presents CaLmi, a system for Intelligent Homes that aims to reduce the stress of its residents by: (a) monitoring its level through a combination of biometric measurements from a wearable device along with information about user’s everyday life and (b) enabling the ubiquitous presentation of relaxation programs, which deliver multi-sensory, context-aware, personalized interventions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10877
        },
        {
          "affiliations": [],
          "personId": 20294
        },
        {
          "affiliations": [],
          "personId": 17512
        },
        {
          "affiliations": [],
          "personId": 23756
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4330,
      "typeId": 11437,
      "title": "'True Colors': A Social Wearable that Affords Vulnerability",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present 'True Colors,' a social wearable prototype designed to augment co-located social interaction among players in a LARP (live action role play). We designed it to enable the emergence of rich social dynamics between wearers and non-wearers. True Colors is Y-shaped, worn around the upper body, and has distinct front and back interfaces to afford actions taken by the wearer (front), and actions taken by others (back). To design True Colors, we followed a Research-through-Design approach, used experiential qualities and social affordances to guide our process, and co-designed with LARP designers. 13 True Colors wearables were deployed in a 3-day LARP event, attended by 109 people. Out of all the functionalities and interactivity the device afforded, players gravitated most towards those that emphasized the social value of experiencing vulnerability as a prompt to get together.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16906
        },
        {
          "affiliations": [],
          "personId": 19123
        },
        {
          "affiliations": [],
          "personId": 13958
        },
        {
          "affiliations": [],
          "personId": 20849
        },
        {
          "affiliations": [],
          "personId": 10015
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7402,
      "typeId": 11437,
      "title": "Leveraging Active Learning and Conditional Mutual Information to Minimize Data Annotation in Human Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A difficulty in human activity recognition (HAR) with wearable sensors is the acquisition of large amounts of annotated data for training models using supervised learning approaches. While collecting raw sensor data has been made easier with advances in mobile sensing and computing, the process of data annotation remains a time-consuming and onerous process. This paper explores active learning as a way to minimize the labor-intensive task of labeling data. We train models with active learning in both offline and online settings with data from 4 publicly available activity recognition datasets and show that it performs comparably to or better than supervised methods while using around 10% of the training data. Moreover, we introduce a method based on conditional mutual information for determining when to stop the active learning process while maximizing recognition performance. This is an important issue that arises in practice when applying active learning to unlabeled datasets.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19765
        },
        {
          "affiliations": [],
          "personId": 19390
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 4334,
      "typeId": 11437,
      "title": "What Will You Do for the Rest of the Day? An Approach to Continuous Trajectory Prediction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Understanding and predicting human mobility is vital to a large number of applications, ranging from recommendations to safety and urban service planning. In some applications, the ability to accurately predict the user‚Äôs future trajectory is a vital for good service provision. The accurate prediction of detailed trajectories would empower location-based service providers with the ability to deliver more precise recommendations to users. \\  \\ Existing work on human mobility prediction has mainly focused on the prediction of the next location visited by the user, rather than on the prediction of the sequence of further locations and the corresponding arrival and departure times. Furthermore, existing approaches often return predicted locations as regions with coarse granularity rather than geographical coordinates. \\  \\ In this paper, we describe an approach that leverages historical data and a user‚Äôs initial day trajectory, to predict the user‚Äôs daily trajectory for the rest of the. The predicted trajectory includes the sequence of future locations, the staying times, and the departure times; the granularity of the predicted trajectory is the same as the current available part of the daily trajectory. Our evaluation shows results on both labeled and geographical trajectories with a prediction error reduced by 10-60% compared to the baselines. \\",
      "authors": [
        {
          "affiliations": [],
          "personId": 21828
        },
        {
          "affiliations": [],
          "personId": 15087
        },
        {
          "affiliations": [],
          "personId": 17436
        },
        {
          "affiliations": [],
          "personId": 23418
        },
        {
          "affiliations": [],
          "personId": 19553
        },
        {
          "affiliations": [],
          "personId": 21523
        }
      ],
      "sessionIds": [
        1040
      ],
      "eventIds": []
    },
    {
      "id": 6386,
      "typeId": 11437,
      "title": "PCIAS: Precise and Contactless Measurement of Instantaneous Angular Speed Using a Smartphone",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Measuring Instantaneous Angular Speed (IAS) of rotating objects is ubiquitous in industry and our daily life. Engineers diagnose the operation condition of engines with IAS. Anemometers obtain instantaneous wind speed with the IAS of rotating cups. Traditional IAS measurement systems have their limitations in the aspects of installation, accuracy, and cost. In this paper, we propose PCIAS, a system that uses acoustic signals of a smartphone to measure IAS of rotating objects in a contactless manner. PCIAS covers a pretty large IAS measurement range (the numerical interval of IAS) from 10 Revolutions Per Minute (RPM) to 10000 RPM, which outperforms almost all existing Commercial-Off-The-Shelf (COTS) IAS meters. In PCIAS, we first choose an appropriate measurement range according to applications. We then use the smartphone to collect acoustic signals backscattered or generated by the object. Next, we extract acoustic features of the object to eliminate interferences from the environment. After that, we propose a robust tracking algorithm to estimate IAS by matching cycle time length of acoustic features adaptively. We build two testbeds to evaluate the accuracy and the robustness of our system in different IAS ranges. Our experiments show that PCIAS achieves a relative accuracy of more than 92% in the low IAS range, more than 94% in the middle IAS range, and more than 96% in the high IAS range. Finally, We exhibit two typical cases to demonstrate the practical use of our system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14692
        },
        {
          "affiliations": [],
          "personId": 22980
        },
        {
          "affiliations": [],
          "personId": 22125
        },
        {
          "affiliations": [],
          "personId": 21238
        },
        {
          "affiliations": [],
          "personId": 8866
        }
      ],
      "sessionIds": [
        1270
      ],
      "eventIds": []
    },
    {
      "id": 3316,
      "typeId": 11437,
      "title": "SelfSync: Exploring Self-Synchronous Body-Based Hotword Gestures for Initiating Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "SelfSync enables rapid, robust initiation of a gesture interface using synchronized movement of different body parts. SelfSync is the gestural equivalent of a hotword such as OK-Google in a speech interface and is enabled by the increasing trend where a user wears two or more wearables, such as a smartwatch, wireless earbuds, or a smartphone.\n In a user study comparing five potential SelfSync gestures in isolation, our system averages 96%, 98% and  88% for user dependent, user adapted, and user independent accuracy, respectively. \n\nFor when the user has a phone in a pocket and a smartwatch, we suggest twisting the hand about the wrist while moving the leg with the phone in synchrony left and right. When the user has a head worn device and a smartwatch, we suggest twisting the hand while twisting the head left and right.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13876
        },
        {
          "affiliations": [],
          "personId": 16101
        },
        {
          "affiliations": [],
          "personId": 15205
        },
        {
          "affiliations": [],
          "personId": 13758
        },
        {
          "affiliations": [],
          "personId": 13206
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 3319,
      "typeId": 11437,
      "title": "CoAT: A Web-based, Collaborative Annotation Tool",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "During the dataset creation process for activity and context recognition research, manual annotation of ground truth events can be a time-consuming and error-prone task.\nIn the typical use case, one or more annotators have to go over the videos recorded during the experiments and label what happens at what time of the experiment. In this paper, we introduce a new open source, web-based tool to assist researchers to create event annotations easily and to leverage group work by supporting intuitive collaboration features.\nWe provide an overview of the main technical components and their respective technologies used to realize the tool.\nThe first version of the tool with the core features for video annotation is implemented and publicly available. By using containerized services, the deployment involves only a small number of steps and dependencies.\nWe point out some possible direction for future development and customization options for using the tool in other annotation tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10072
        },
        {
          "affiliations": [],
          "personId": 10155
        },
        {
          "affiliations": [],
          "personId": 19888
        },
        {
          "affiliations": [],
          "personId": 17928
        },
        {
          "affiliations": [],
          "personId": 18666
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5368,
      "typeId": 11437,
      "title": "Pneuxels: A Platform for Physically Manifesting Object-Based Crowd Interactions in Large Scales",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We often perceive other peoples’ presence implicitly, through the traces of their interactions with physical objects. What if our urban environments could mediate these traces allowing remotely located people perceive each other’s presence collectively? As a step towards this end, we have developed Pneuxels, a network of programmable inflatables, placed at remote sites, that allow visitors in one site to perceive the presence of visitors in other sites, promoting thereby a sense of collective awareness and place-making. Pneuxels (Pneumatic Pixels) are pneumatically actuated pixels, connected through a web-socket platform, that change their physical state based on input from other Pneuxels, from the environment, or from users. We discuss our experiences in designing, prototyping, and testing Pneuxels, and we report our results from preliminary user studies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14106
        },
        {
          "affiliations": [],
          "personId": 12189
        },
        {
          "affiliations": [],
          "personId": 23668
        },
        {
          "affiliations": [],
          "personId": 13569
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5370,
      "typeId": 11437,
      "title": "A Theoretical Framework To Study Long-term Use Of Smart Eyewear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 19053
        },
        {
          "affiliations": [],
          "personId": 11038
        },
        {
          "affiliations": [],
          "personId": 14708
        },
        {
          "affiliations": [],
          "personId": 11930
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4346,
      "typeId": 11437,
      "title": "TurnsMap: Enhancing Driving Safety at Intersections with Mobile Crowdsensing and Deep Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Left turns are known to be one of the most dangerous driving maneuvers. An effective way to mitigate this safety risk is to install a left-turn enforcement — e.g., a protected left-turn signal — at every turn that preserves a traffic phase exclusively for left turns. Although such an enforcement can significantly increase the driving safety, whether or not a road segment (e.g., intersection) has such an enforcement is not yet available to the public and navigation systems.\n\nThis paper presents TurnsMap, a mobile crowdsensing and deep learning based system for classifying the protection settings of left turns. One of our key findings is that crowdsensed IMU sensor (i.e., gyroscope and accelerometer) data from onboard mobile devices (e.g., driver’s smartphone) can be used to recognize different types of left-turn protection. \n\nWe have built and used a large-scale real-world driving dataset to evaluate TurnsMap, demonstrating its capability of identifying different left-turn enforcements with 90.3% accuracy. A wide range of automotive apps can benefit (e.g., enhancing traffic safety) from the left-turns information unearthed by TurnsMap.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24315
        },
        {
          "affiliations": [],
          "personId": 16611
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 7421,
      "typeId": 11437,
      "title": "To Mask or Not to Mask? Balancing Privacy with Visual Confirmation Utility in Activity-Oriented Wearable Cameras",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Activity-oriented cameras are increasingly being used to provide visual confirmation of specific hand-related activities in real-world settings. However, recent studies have shown that bystander privacy concerns limit participant willingness to wear a camera. Researchers have investigated different image obfuscation methods as an approach to enhance bystander privacy; however, these methods may have varying effects on the visual confirmation utility of the image, which we define as the ability of a human viewer to interpret the activity of the wearer in the image. Visual confirmation utility is needed to annotate and validate hand-related activities for several behavioral-based applications, particularly in cases where a human in the loop method is needed to label (e.g., annotating gestures that can not be automatically detected yet). We propose a new type of obfuscation, \\textit{activity-oriented partial obfuscation}, as a methodological contribution to researchers interested in obtaining visual confirmation of hand-related activities in the wild. We tested the effects of this approach by collecting 10 diverse and realistic video scenarios that involved the wearer performing hand-related activities while bystanders performed activities that could be of concern if recorded. Then we conducted an online experiment with 367 participants to evaluate the effect of varying degrees of obfuscation on bystander privacy and visual confirmation utility. Our results show that activity-oriented partial obfuscation (1) maintains visual confirmation of the wearer's hand-related activity, especially when an object is present in the hand, and even when extreme filters are applied, while (2) significantly reducing bystander concerns and enhancing bystander privacy. Informed by our analysis, we further discuss the impact of the filter method used in activity-oriented partial obfuscation on bystander privacy and concerns.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8985
        },
        {
          "affiliations": [],
          "personId": 15288
        },
        {
          "affiliations": [],
          "personId": 21375
        },
        {
          "affiliations": [],
          "personId": 12432
        },
        {
          "affiliations": [],
          "personId": 13032
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 5374,
      "typeId": 11437,
      "title": "Poster: Predicting Opportune Moments for In-vehicle Proactive Speech Services",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Auditory-verbal or speech interactions with in-vehicle information systems have became increasingly popular. This opens up a whole new realm of possibilities for serving drivers with proactive speech services such as contextualized recommendations and interactive decision-making. However, prior studies have warned that such interactions may consume considerable attentional resources, thus degrade driving performance. This work aims to develop a machine learning model that can find opportune moments for the driver to engage in proactive speech interaction by using the vehicle and environment sensor data. Our machine learning analysis shows that opportune moments for interruption can be conservatively inferred with an accuracy of 0.74.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10169
        },
        {
          "affiliations": [],
          "personId": 20248
        },
        {
          "affiliations": [],
          "personId": 22552
        },
        {
          "affiliations": [],
          "personId": 9777
        },
        {
          "affiliations": [],
          "personId": 22860
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6399,
      "typeId": 11437,
      "title": "Passerby Crowdsourcing:Workers’ Behavior and Data Quality Management",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Worker recruitment is one of the important problems in crowdsourcing, and many proposals have been presented for placing equipment in physical spaces for recruiting workers. One of the essential challenges of the approach is how to keep people attracted because those who perform tasks at first gradually lose interest and do not access the equipment. \\ This study uses a different approach to the worker recruitment problem. In our approach, we dive into people's personal spaces by projecting task images on the floor, thereby allowing the passersby to effortlessly access tasks while walking. \\ The problem then changes from how to keep people engaged to how to manage data quality because many passersby unconsciously or intentionally walk through the task screen on the floor without doing the task, which produces unintended results. \\ We explore a machine-learning approach to select only the intended results and manage the data quality.  The system assesses the workers' intention from their behavior. \\ We identify the features for classifiers based on our observations of the passersby.  We then conduct extensive evaluations with real data. The results show that the features are effective in practice, and the classifiers improve the data quality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9228
        },
        {
          "affiliations": [],
          "personId": 20218
        },
        {
          "affiliations": [],
          "personId": 14552
        },
        {
          "affiliations": [],
          "personId": 8726
        },
        {
          "affiliations": [],
          "personId": 19823
        },
        {
          "affiliations": [],
          "personId": 16434
        },
        {
          "affiliations": [],
          "personId": 18425
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 4355,
      "typeId": 11437,
      "title": "Poster: Robust Classification of Eating Sound Collected in Natural Meal Environment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Increasing the number of chewing can help reduce obesity. Nevertheless, it is difficult for a person to keep track of his mastication rate without the help of an automatic mastication counting device. Such devices do exist, but they are big and non-portable and are not suitable for daily use. In our previous work, we proposed an optimization model for classification of chewing, swallowing, and speaking activities using sound data collected by a bone conduction microphone in a natural eating environment. In this paper, we aim to implement a system that could automatically recognize a person's eating gestures (e.g.,  mastication, swallowing, and utterance) in real time. In realizing this, it is necessary to add the other sounds such as noises in the model so that it is more robust to natural meal environment. Therefore, in this study, we proposed an optimization for classification method adding the other sounds to three eating activities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9431
        },
        {
          "affiliations": [],
          "personId": 16347
        },
        {
          "affiliations": [],
          "personId": 11624
        },
        {
          "affiliations": [],
          "personId": 12118
        },
        {
          "affiliations": [],
          "personId": 12003
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6406,
      "typeId": 11437,
      "title": "Effects of the Textile-Sensor Interface on Stitched Strain Sensor Performance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The influence of the textile substrate on the performance of a textile-based strain sensor has not been well understood or characterized in many wearable sensor evaluations. The underlying textile has its own anisotropic mechanical behaviors due to its woven or knit fabrication process, and introduces non-trivial structural influences on integrated wearable strain sensors. This study considers stitched strain sensors of two stitch geometries, fabricated on two different knit fabrics, with the sensor stitched in different orientations with respect to the knit structure. The resulting mechanical and electrical performance is characterized under cyclic extension, as the angle of extension (relative to the fabric) is also incrementally changed. The results illustrate a shift from linear to non-linear mechanical behavior as fabric stiffness increases, and variations in behavior between stitch geometries. Results show that force direction and sensor placement both introduce variability in calculated elastic modulus, which affect sensor modeling (e.g. predicting applied force from sensor response). A novel stitch geometry (the chainstitch sensor) is characterized as having a higher gauge force and lower transverse sensitivity factor than the coverstitch sensor. This work offers insight into the textile-sensor interface and design implications for development of textile-based sensors.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17647
        },
        {
          "affiliations": [],
          "personId": 8629
        },
        {
          "affiliations": [],
          "personId": 23887
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 3335,
      "typeId": 11437,
      "title": "A Dialogue-Based Annotation for Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a method to collect training labels for human activity recognition by using a dialogue system. To show the feasibility of using dialogue-based annotation, we implemented the dialogue system and conducted experiments in the lab setting. The preliminary performance of activity recognition attained the f-measure of 0.76. We also analyze the collected data to provide a better understanding of what users expect from the system, how they interact with it and its other potential uses. Finally, we discussed the results obtained and possible directions for future works.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8218
        },
        {
          "affiliations": [],
          "personId": 11459
        },
        {
          "affiliations": [],
          "personId": 24129
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5383,
      "typeId": 11437,
      "title": "Exploring Tangible Interactions with Radar Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Previous research has explored miniature radar as a promising sensing technique for the recognition of gestures, objects, users, presence and liquids. However, within HCI, its use remains under-explored, especially in TUI. In this paper, we explore two research questions with radar as a platform for sensing interaction with the stacking, ordering and movement of planar objects, fabricated objects and identification. We detail the design space and practical use-cases for such interaction which allows us to identify a series of design patterns, beyond static interaction, which are continuous and dynamic. With a focus on planar objects, we report on a series of studies which demonstrate the suitability of this approach. This exploration is grounded in both a characterization of the radar sensing and our rigorous experiments which shows that such sensing is accurate with minimal training. With these techniques, we envision both realistic and future applications and scenarios. The motivation for what we refer to as Solinteraction, is to demonstrate the potential for radar-based interaction with objects in HCI.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18933
        },
        {
          "affiliations": [],
          "personId": 20417
        },
        {
          "affiliations": [],
          "personId": 24202
        },
        {
          "affiliations": [],
          "personId": 19949
        },
        {
          "affiliations": [],
          "personId": 8798
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 6411,
      "typeId": 11437,
      "title": "Poster: PASNIC: a thermal based privacy-aware sensor node for image capturing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose PASNIC, a lightweight privacy-aware sensor node for image capturing. PASNIC masks the region of faces and private screens (smartphone and portable computer displays) in captured visible images before sending them to remote sites. Thermal information, which can also be captured with a thermal image sensor, is used to detect the regions of faces and private screens. Our masking algorithms are so simple that even sensor nodes with low processing power can sufficiently process them.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19422
        },
        {
          "affiliations": [],
          "personId": 22370
        },
        {
          "affiliations": [],
          "personId": 12918
        },
        {
          "affiliations": [],
          "personId": 18202
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3340,
      "typeId": 11437,
      "title": "Unsupervised Localization by Learning Transition Model",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "It becomes very convenient to collect synchronized WiFi received signal strength and inertial measurement (RSS+IMU) sequences by mobile devices nowadays, which enables the promising solution to conduct unsupervised indoor localization without the pain of radio-map calibration. To relax the needs of floor-map information or trajectory knowledge, this paper proposes to learn a transition model (TM), which segments the massive unlabeled sequences to train a model that captures the expected relationship between {zt−1, zt } and ut−1, where zt−1, zt are two consecutive signal states at t and t − 1, and ut−1 is the one step motion calculated from the inertial data. We present both a transitional model in signal space (TMS) and a transitional model to predict motion from signal change (TMM) to represent the relationship in different ways. In particular, from the massive sequences, both the signal states and the one step motion are smoothed from the nearest neighbours, so that the transition model learns the expected relative signal state change triggered by the smoothed one step motion. Its distinctive features are that (1) no external floor-map or trajectory knowledge is needed; (2) it can be continuously on-line refined as unlabeled sequences are incrementally collected. Kalman filter based on-line location tracking methods to mobile users are given for both models. Experiments show that the transition model based localization method provides comparable accuracy with the manually fingerprint calibration methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16082
        },
        {
          "affiliations": [],
          "personId": 24185
        },
        {
          "affiliations": [],
          "personId": 9916
        },
        {
          "affiliations": [],
          "personId": 15179
        },
        {
          "affiliations": [],
          "personId": 22235
        }
      ],
      "sessionIds": [
        1235
      ],
      "eventIds": []
    },
    {
      "id": 6414,
      "typeId": 11437,
      "title": "Modeling Spatio-Temporal App Usage for a Large User Population",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the wide adoption of mobile devices, it becomes increasingly important to understand how users use mobile apps. Knowing when and where certain apps are used is instrumental for app developers to improve app usability and for Internet service providers (ISPs) to optimize their network services. However, modeling spatio-temporal patterns of app usage has been a challenging problem due to the complicated usage behavior and the very limited personal data. In this paper, we propose a Bayesian mixture model to capture when, where and what apps are used and predict future app usage. To solve the challenge of data sparsity, we apply a hierarchical Dirichlet process to leverage the shared spatio-temporal patterns to accurately model users with insufficient data. We then evaluate our model using a large dataset of app usage traces involving 1.7 million users over 3503 apps. Our analysis shows a clear correlation between the user’s location and the apps being used. Extensive evaluations show that our model can accurately predict users’ future locations and app usage, outperforming the state-of-the-art algorithms by 11.7% and 11.1%, respectively. In addition, our model can be used to synthesize app usage traces that do not leak user privacy while preserving the key data statistical properties.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9999
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 23246
        },
        {
          "affiliations": [],
          "personId": 19889
        },
        {
          "affiliations": [],
          "personId": 16424
        },
        {
          "affiliations": [],
          "personId": 11677
        },
        {
          "affiliations": [],
          "personId": 9082
        }
      ],
      "sessionIds": [
        2123
      ],
      "eventIds": []
    },
    {
      "id": 5391,
      "typeId": 11437,
      "title": "Personal Bits: Mining Interaction Traces for Personalized Task Intelligence",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As we work, play, shop, and communicate in digital interfaces, we continuously generate traces of information. To turn such noisy sources of personal data into actual insight, my research introduces \\textit{Personal Bits}, a service that enables personalized task support in various kinds of information tasks such as instant message handling, information retrieval, and text entry. Personal Bits mines a user's interaction traces with web apps and native mobile apps and extracts task-centric entities. I present three example apps for Personal Bits: Deja Wu, MessageOnTap, and ContextBoard, to address inefficiencies presented in these information tasks. \\textit{Personal Bits} acts as the central nexus for intelligence between apps and interaction traces, making it easy for apps to acquire personally relevant task entities in fine granularity.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8464
        },
        {
          "affiliations": [],
          "personId": 22231
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4367,
      "typeId": 11437,
      "title": "Batch Localization Based on OFDMA Backscatter",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "OFDMA Wi-Fi backscatter can significantly improve the communication efficiency and meanwhile maintain ultra-low power consumption; however, the ground-up reworking on the core mechanism of traditional Wi-Fi system revolutionizes the basis of many existing Wi-Fi based mechanisms. In this paper, we explore how localization can be realized based on OFDMA backscatter, where a batch localization mechanism utilizing concurrent communication in the OFDMA backscatter system is proposed. We present a series of mechanisms to deal with the fundamental change of assumptions brought by the new paradigm. First, we process signals at the receiver in a finer granularity for signal classification. Then we remove phase offsets in real time without interrupting the communication. Finally, we propose an extended MUSIC algorithm to improve accuracy with limited localization information in OFDMA backscatter mechanism. We implement a prototype under the 802.11g framework in WARP, based on which we conduct comprehensive experiments to evaluate our propose mechanism. Results show that our system can localize 48 tags simultaneously, while achieving average localization errors within 0.49m. The tag's power consumption is about 55-81.3\\\\mu W.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11965
        },
        {
          "affiliations": [],
          "personId": 10835
        },
        {
          "affiliations": [],
          "personId": 8627
        },
        {
          "affiliations": [],
          "personId": 21420
        },
        {
          "affiliations": [],
          "personId": 20919
        }
      ],
      "sessionIds": [
        1235
      ],
      "eventIds": []
    },
    {
      "id": 4368,
      "typeId": 11437,
      "title": "Poster: Combining a Thermal Camera and a Wristband Sensor for Thermal Comfort Estimation",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a method to estimate thermal comfort by combining a thermal camera and a wristband sensor. A wristband sensor continuously monitors physiological data (i.e. heart rate, skin temperature, and electrodermal activity). On the other hand, a thermal camera captures temperature distribution of facial parts only when a user's face is in the camera frame. When a thermal camera cannot capture the user's face, our method estimates thermal comfort based on the current features obtained by the wristband sensor and the past estimation results using the thermal camera. To investigate the effectiveness in the reduction of energy consumption by air conditioning, we evaluate our method by collecting data from 15 subjects for 128 days.\nThe results show our method achieves F-measure of 0.85 for estimating thermal comfort allowing shifts to the neighboring classes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18319
        },
        {
          "affiliations": [],
          "personId": 21374
        },
        {
          "affiliations": [],
          "personId": 23989
        },
        {
          "affiliations": [],
          "personId": 9439
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5393,
      "typeId": 11437,
      "title": "Causal Feature Selection for Physical Sensing Data: A Case Study on Power Events Prediction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Identifying the causal features from multi-dimensional physical data  streams is one of the underpinnings for the success of the data-driven inference tasks. Prior research utilizes the 'correlation' or 'mutual information' to select features, which ignored the crucial inherent causality behind the data. In this work, we consider the problem of selecting causal features from streaming physical sensing data. Inspired by a metric from information theory which calibrates both instantaneous and temporal relations, we formulate the causal feature selection as a cardinality constrained joint directed information maximization (ie. Max-JDI) problem. Then we propose a near optimal greedy algorithm for streaming feature selection and present an information-interpretive solution for the cardinality constraint presetting. The proposed method is evaluated on a real-world case study involving feature selection for the power distribution network event detection. Compared with other selection baselines, the proposed method increases the detection accuracy by around 5%, while concurrently reduces the computation time from several weeks to within a minute. The promising results demonstrate that it can be applied to optimize the energy operation and enhance the resilience  of power buildings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21586
        },
        {
          "affiliations": [],
          "personId": 9251
        },
        {
          "affiliations": [],
          "personId": 21154
        },
        {
          "affiliations": [],
          "personId": 11505
        },
        {
          "affiliations": [],
          "personId": 18241
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4369,
      "typeId": 11437,
      "title": "ShoesHacker: Indoor Corridor Map and User Location Leakage Through Force Sensors in Smart Shoes",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The past few years have witnessed the rise of smart shoes, the wearable devices that measure foot force or track foot motion. However, people are not aware of the possible privacy leakage from in-shoe force sensors. In this paper, we explore the possibility of locating an indoor victim based on the force signals leaked from smart shoes. We present ShoesHacker, an attack scheme that reconstructs the corridor map of the building that the victim walks in based on force data only. The corridor map enables the attacker to recognize the building, and thus locate the victim on a global map. To handle the lack of training data, we design the stair landing detection algorithm, based on which we extract training data when victims are walking in stairwells. We estimate the trajectory of each walk, and propose the path merging algorithm to merge the trajectories. Moreover, we design a metric to quantify the similarity between corridor maps, which makes building recognition possible. Our experimental results show that, the building recognition accuracy reaches 77.5% in a 40-building dataset, and the victim can be located with an average error lower than 6 m, which reveals the danger of privacy leakage through smart shoes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18502
        },
        {
          "affiliations": [],
          "personId": 9864
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 3347,
      "typeId": 11437,
      "title": "Tracking Fatigue and Health State in Multiple Sclerosis Patients Using Connected Wellness Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Multiple Sclerosis requires long-term disease management, but tracking patients through the use of clinical surveys is hindered by high costs and patient burden. In this work, we investigate the feasibility of using data from connected wellness devices to predict MS patients' fatigue and health status, as measured by the Fatigue Severity Scale (FSS) and EQ-5D index. We collected data from 198 MS patients who are given connected wellness devices for over 6 months. We examine how accurately can the collected data predict reported FSS and EQ-5D scores per patient using an ensemble of regressors. In predicting for both FSS and EQ-5D, we are able to achieve errors aligning with the instrument' standard measurement error (SEM), as well as strong and significant correlations between predicted and ground truth values. We also show a simple adaptation method that greatly reduces prediction errors through the use of just 1 user-supplied ground truth datapoint. For FSS (SEM 0.7), the universal model predicts weekly scores with MAE 1.00, while an adapted model predicts with MAE 0.58. For EQ-5D (SEM 0.093), the universal model predicts weekly scores with MAE 0.097, while an adapted model predicts with MAE 0.065. Our study represents the first sets of results on tracking fatigue and health status of MS patients using connected wellness devices, which gives promising prediction performance with errors aligns with the accepted range of error in the widely used clinically-validated questionnaires. Future extensions and potential applications of our results can positively impact MS  patient disease management and support clinical research.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17905
        },
        {
          "affiliations": [],
          "personId": 23751
        },
        {
          "affiliations": [],
          "personId": 9465
        },
        {
          "affiliations": [],
          "personId": 11583
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 7445,
      "typeId": 11437,
      "title": "Beyond Individuals: Exploring Social Experiences Around Wearables",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Much of the research in wearable technology focuses on the primary user's experiences and interactions. However, many wearables are inherently social -- even public -- by nature as they are visible to nearby others. Wearables carry meanings about the wearer (e.g., lifestyle, attitudes, interests, social status). Some wearables are even designed to enable interaction between collocated users or enhance group-experience. While the functions of the technology are often seen to justify its existence, reaching high acceptance of technology requires that also social and cultural aspects are considered. In this workshop, we look into the dynamic and communicative nature of wearable technology designed for both individuals and groups. We are particularly interested in social experiences emerging around personal wearables and the possibilities for technology enhancing group experiences. The goal of this workshop is to bring together a community of researchers, designers, and practitioners who have designed or are interested in designing wearable technology to discuss research agenda and challenges",
      "authors": [
        {
          "affiliations": [],
          "personId": 9801
        },
        {
          "affiliations": [],
          "personId": 18135
        },
        {
          "affiliations": [],
          "personId": 18839
        },
        {
          "affiliations": [],
          "personId": 23845
        },
        {
          "affiliations": [],
          "personId": 22538
        },
        {
          "affiliations": [],
          "personId": 10074
        },
        {
          "affiliations": [],
          "personId": 17103
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4376,
      "typeId": 11437,
      "title": "Cognitive Load Assessment from Facial Temperature Using Smart Eyewear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 21279
        },
        {
          "affiliations": [],
          "personId": 17920
        },
        {
          "affiliations": [],
          "personId": 14326
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5406,
      "typeId": 11437,
      "title": "iCoff: Towards Building an Intelligent Coffee Plate System to Enhance Coffee Shop's Customer Experience",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Blossoming coffee culture give a rise to coffee shops in many cities around the world. In today’s coffee industry, it is no longer customer satisfaction that is important but the customer experience. This work presents a development of an intelligent coffee plate system called iCoff, which aims at enhancing coffee shop’s customer experience with a platform that allows a barista to communicate with a coffee drinker as well as enables the coffee drinker to learn more about his/her coffee, such as ingredients, temperature, and weight through an interactive coffee plate. This paper describes its hardware and software components as well as a preliminary user experience study result. It is an applied ubiquitous/pervasive technology in the context of coffee shop experience as part of our today’s urban living.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17267
        },
        {
          "affiliations": [],
          "personId": 8614
        },
        {
          "affiliations": [],
          "personId": 9607
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7456,
      "typeId": 11437,
      "title": "Poster: Smart Cook: Making Cooking Easier with Multimodal Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Learning how to cook presents at least two significant challenges. First, it can be difficult for novices to find appropriate recipes based on the ingredients available in one's pantry and/or refrigerator. Second, it can be difficult to focus on cooking tasks and following a recipe at the same time. In this poster, we present the design process and implementation of a system that uses deep learning to address the first of these two problems. Our initial design work focuses on streamlining the process of entering and tracking potential ingredients on hand and determining appropriate recommendations for recipes that utilize these ingredients. Here, we present the current state of our project, explaining in particular our contributions to minimizing the overhead of tracking kitchen ingredients and converting this inventory information into effective recipe recommendations using a multimodal machine learning approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15130
        },
        {
          "affiliations": [],
          "personId": 9090
        },
        {
          "affiliations": [],
          "personId": 10442
        },
        {
          "affiliations": [],
          "personId": 22872
        },
        {
          "affiliations": [],
          "personId": 10194
        },
        {
          "affiliations": [],
          "personId": 13545
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4385,
      "typeId": 11437,
      "title": "Cross-dataset Deep Transfer Learning for Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Convolution Neural Network (CNN) filters learned on one domain can be used as feature extractors on another similar domain. Transferring filters allow reusing datasets across domains and reducing labelling costs. In this paper, four activity recognition datasets were analyzed to study the effects of transferring filters across the datasets. A spectro-temporal ResNet was implemented as a deep, end-to-end learning architecture. We analyzed the number of transferred CNN residual blocks with respect to the size of the target-adaptation data. The analysis showed that transfer learning using small adaptation subsets is more useful when the target domain contains a small number of different activities. Furthermore, the similarity between the domains, participating in the transfer learning scenario, seems to play a role in its success. The most successful transfer achieved an F1-score of 93%, which is an increase of 9 percentage points compared to a domain-specific baseline model.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24031
        },
        {
          "affiliations": [],
          "personId": 12074
        },
        {
          "affiliations": [],
          "personId": 13286
        },
        {
          "affiliations": [],
          "personId": 18954
        },
        {
          "affiliations": [],
          "personId": 15053
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6434,
      "typeId": 11437,
      "title": "Readability and Legibility of Fonts Considering Shakiness of Head Mounted Displays",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In wearable computing environments, users acquire visual information in various scenes using a head mounted display (HMD). \nHowever, this induces problems due to their differences from conventional displays such as smartphone and e-books. \nIn this research, we focused on the problem of vertical shock caused by walking. \nThis problem interferes with seeing information on HMDs. \nIn this paper, we discuss selection of font shapes to minimize the effect of this problem. \nIf we can clarify the characteristics of the readability of fonts in wearable computing environments, \nthe application designers can select fonts that strike a balance between intended design elements and readability.\nIn this paper, we first investigated the characteristics of fonts used in Japan considering the HMD swing that results from walking,\nfrom the viewpoints of readability (text readability) and legibility (ease of letter recognition) using six different fonts. \nFrom this evaluation, we find that fonts with very thin horizontal lines and with very thin horizontal and vertical lines should not be presented on HMDs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21922
        },
        {
          "affiliations": [],
          "personId": 19823
        },
        {
          "affiliations": [],
          "personId": 18379
        },
        {
          "affiliations": [],
          "personId": 15083
        },
        {
          "affiliations": [],
          "personId": 12353
        },
        {
          "affiliations": [],
          "personId": 11925
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 5413,
      "typeId": 11437,
      "title": "Position Independent Activity Recognition Using Shallow Neural Architecture and Empirical Modeling",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The goal of the SHL recognition challenge 2019 is to recognize transportation modalities in a sensor placement independent manner. In this paper, the performance of shallow neural networks is benchmarked by Team Orion in such a manner on the dataset provided in the challenge, using 156 handcrafted temporal and spectral features per sensor through the application of parallel processing and out-of-memory architecture. Using scaled conjugate gradient backpropagation (SCGB) algorithm, combining classes 7 and 8 and taking 5000 frames of bag-hips-torso data from validation set, classification accuracy of 87.2% was obtained on the validation dataset of the same labels for a shallow two-layer feed-forward network. 71% accuracy was obtained on the validation set of classes 7 and 8 via transfer of 2500 frames using another shallow neural network of similar architecture. Using empirically observed variable based transfer of 7088 frames from hand validation data to training dataset, 77.5% accuracy was obtained on hand validation data for classes 1 to 7/8, and 70% classification accuracy of classes 7 and 8 via transfer of 1809 frames from hand validation data. The results illustrate how carefully crafted features coupled with empirical transfer of labeled knowledge and combination of problematic classes can tune a neural classifier to work in a new feature space.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16870
        },
        {
          "affiliations": [],
          "personId": 23707
        },
        {
          "affiliations": [],
          "personId": 19324
        },
        {
          "affiliations": [],
          "personId": 24129
        },
        {
          "affiliations": [],
          "personId": 21030
        },
        {
          "affiliations": [],
          "personId": 22799
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4389,
      "typeId": 11437,
      "title": "Technical Challenges to Deliver Sensor-based Psychological Interventions using Smartphones",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Research in the development of tools to successfully deliver psychological interventions through smartphones is growing rapidly. As the research body grows towards more cutting-edge solutions that utilize the smartphone's advanced technical capabilities, various challenges are uncovered to successfully and efficiently deliver safe interventions in critical scenarios and situations. We present the SyMptOMS platform, a configurable set of tools that allows therapists to specify, deploy and follow-up location- and sensor-based assessments and interventions for various mental disorders, run and delivered remotely via the patient's smartphone at any place (ecological) and time (momentarily). From our experience in developing and running experiments with SyMptOMS, we overview and discuss technical challenges and open research questions involved in sensor-based interventions using smartphones.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22301
        },
        {
          "affiliations": [],
          "personId": 18210
        },
        {
          "affiliations": [],
          "personId": 8720
        },
        {
          "affiliations": [],
          "personId": 22187
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6439,
      "typeId": 11437,
      "title": "Poster: Towards Detecting and Mitigating Smartphone Habits",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones have the potential to produce new habits, i.e., habitual phone usage sessions consistently associated with explicit contextual cues. Despite there is evidence that habitual smartphone use is perceived as meaningless and addictive, little is known about what such habits are, how they can be detected, and how their disruptive effect can be mitigated. In this paper, we propose a data analytic methodology based on association rule mining to automatically discover smartphone habits from smartphone usage data. By assessing the methodology with more than 130,000 smartphone sessions collected in-the-wild, we show evidence that smartphone use can be characterized by different types of complex habits, which are highly diversified across users and involve multiple apps. To promote discussion and present our future work, we introduce a mobile app that exploits the proposed methodology to assist users in monitoring and changing their smartphone habits through implementation intentions, i.e., \"if-then\" plans where if's are contextual cues and then's are goal-related behaviors.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23776
        },
        {
          "affiliations": [],
          "personId": 9435
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3368,
      "typeId": 11437,
      "title": "Handwritten signature verification by using a six-axis motion sensor and SVM",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Signature verification, if we consider the muscle memory, is a biometric for identification technology.\nTo access muscle memory, we use a motion sensor that consists of accelerometer and gyroscope to implement a signature verification system.\nThe motion sensor records six motion values including three-axis accelerations and angular velocities while name signing. \n14 features of signature are extracted from the sequence of accelerations and angular velocities.\nA support vector machine (SVM) is then applied to verify the signatures. \nThe proposed method was applied to verify the Chinese signatures. \nThe SVM is trained by the training data from each person. \nThe true positive rate of the proposed method can reach to 95.66\\%. \nFake signatures generated by tracing from true signatures can also be recognized by the proposed method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17557
        },
        {
          "affiliations": [],
          "personId": 19821
        },
        {
          "affiliations": [],
          "personId": 22529
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4392,
      "typeId": 11437,
      "title": "DeepType: On-Device Deep Learning for Input Personalization Service with Minimal Privacy Concern",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile users spend an extensive amount of time typing. A more efficient text input instrument brings a significant enhancement of user experience. Deep learning techniques have been recently applied to suggesting the next words of input, but to achieve more accurate predictions, these models should be customized for individual users. Personalization is often at the expense of privacy concerns. Existing solutions require users to upload the logs of their text input to the cloud so that a deep learning predictor can be trained. In this work, we propose a novel approach, called DeepType, to personalize text input with better privacy. The basic idea is intuitive: training deep learning predictors on the device instead of on the cloud, so that the model gets personalized and private data never leaves the device. With DeepType, a global model is  first trained on the cloud using massive public corpora, and personalization is done by incrementally customizing the global model with data on individual devices. We further propose a set of techniques that effectively reduce the computation cost of training deep learning models on mobile devices at the cost of negligible accuracy loss. Experiments using real-world text input from millions of users demonstrate that DeepType significantly improves the input efficiency for individual users, and its incurred computation and energy costs are within the performance and battery restrictions of typical COTS mobile devices.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11865
        },
        {
          "affiliations": [],
          "personId": 9884
        },
        {
          "affiliations": [],
          "personId": 18846
        },
        {
          "affiliations": [],
          "personId": 21955
        },
        {
          "affiliations": [],
          "personId": 19991
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 6444,
      "typeId": 11437,
      "title": "Evaluating the Impact of Technology Assisted Hotspot Policing on Situational Awareness and Task-Load",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Everyday field work of a police officer requires the perception, filtering and understanding of large amounts of information in highly dynamic situations. This presents opportunities for ICT to alleviate strain on officers by providing adequate information provisioning. We evaluate the usage of a mobile location-based hotspot policing system, comprised of a smartphone, smartwatch and a web-application, during real field work with officers in high and low hotspot density locations. We use a repeated measures design to compare possible effects with our baseline measure, i.e. field work without using the system. Usability, task-load and situational awareness (SA), as well as possible mediators, are evaluated to gain insight into the differences between modes of transportation and the overall viability of the system itself. No significant difference was found between the two locations. Officers using the system scored high on usability  measures and interview feedback was largely positive. Measures on SA  remained stable throughout baseline and experimental shifts. Task-load was significantly higher with the use of the system. The contradiction in these findings can be explained by showing the differences in the nature of field work with and without the system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13757
        },
        {
          "affiliations": [],
          "personId": 12792
        },
        {
          "affiliations": [],
          "personId": 12082
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 7478,
      "typeId": 11437,
      "title": "Poster: A Method to Recognize Entering and Leaving Person Based on Door Opening and Closing Movement using Angular Velocity Sensor",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In our daily lives, we use places shared by multiple people such as homes and offices. Although many methods for individual recognition have been proposed, when they are used in such places, they have some problems in terms of psychological and physical burden. Therefore, in this research, we propose a method to recognize a person who entered or left the room by using the opening and closing movement of a door. In the proposed method, an angular velocity sensor is installed on the doorknob, and the user is identified based on personal characteristics of door's motion that is naturally accompanied with entering and leaving a room. We implement the proposed system on a door with a lever handle typed doorknob for a target scene of general household. From experimental results that F value of user's leaving motion was 0.90 and user's entering motion was 0.73, we confirmed the effectiveness of our method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22612
        },
        {
          "affiliations": [],
          "personId": 19918
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7484,
      "typeId": 11437,
      "title": "Mobile Computing and Well-Being in the Outdoors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The inclusion of mobile computing in outdoor activities raises important questions about its ability to contribute meaningfully to activities without detracting from their benefits to well-being. In this paper, we present results from our research, which seeks to explore and set directions for computing's place in outdoor recreation. Our position is that computing already has a place in outdoor recreation and can contribute meaningfully to well-being in the outdoors now and in the future.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17217
        },
        {
          "affiliations": [],
          "personId": 24028
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4413,
      "typeId": 11437,
      "title": "Recruit Until It Fails: Exploring Performance Limits For Identification Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Distinguishing user identities is useful for several applications such as automated grocery or personalized recommendations. Unfortunately, several recent proposals for user identification are evaluated using poor recruitment practices. We found 20 out of 27 surveyed systems used datasets with 20 participants or less. These 20 studies achieved an average classification accuracy of 93%. We show that the classifier performance is misleading when the participant count is small. This is because the finite precision of measurements creates upper limits on the number of users that can be distinguished. \n\nTo demonstrate where this misleading classifier performance comes from, we used publicly available measurement datasets, which were taken from human subjects, to create five systems with at least 20 participants each. In three cases we achieved accuracies greater than 90% by merely applying readily available machine learning software packages, often with default parameters. For datasets where we had sufficient users, we evaluated how the performance degrades as the number of participants increases. One of the systems built suffered a drop in accuracy that was over 35% as the participant count increased from 20 to 250. We argue that data taken from small participant sets do not adequately explore the variations of the measurements taken. Systems train on such limited data have significant potential to incorrectly identify users in practice.  We conclude by explaining generalizable reasons for this issue and provide insights on how to conduct more robust system analysis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22959
        },
        {
          "affiliations": [],
          "personId": 14254
        },
        {
          "affiliations": [],
          "personId": 12097
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 4415,
      "typeId": 11437,
      "title": "Combating Replay Attacks Against Voice Assistants",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, there has been a surge in the popularity of voice-first devices, such as Amazon Echo, Google Home, etc. While these devices make our life more convenient, they are vulnerable to new attacks, such as voice replay. We develop an end-to-end system to detect replay attacks without requiring a user to wear any wearable device. Our system, called REVOLT, has several distinct features: (i) it intelligently exploits the inherent differences between the spectral characteristics of the original and replayed voice signals, (ii) it exploits both acoustic and WiFi channels in tandem, (iii) it utilizes unique breathing rate extracted from WiFi signal while speaking to test the liveness of human voice. After extensive evaluation, our voice component yields Equal Error Rate (EER) of 0.88% and 10.32% in our dataset and ASV2017 dataset, respectively; and WiFi based breathing\ndetection achieves Breaths Per Minute (BPM) error of 1.8 up to 3m distance. We further combine WiFi and voice based detection and show the overall system offers low false positive and false negative when evaluated against a range of attacks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15573
        },
        {
          "affiliations": [],
          "personId": 24339
        },
        {
          "affiliations": [],
          "personId": 23481
        },
        {
          "affiliations": [],
          "personId": 21478
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 3391,
      "typeId": 11437,
      "title": "Performance Characterization of Deep Learning Models for Breathing-based Authentication on Resource-Constrained Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Providing secure access to smart devices such as mobiles, wearables and IoT devices is becoming increasingly important, especially as these devices store a range of sensitive personal information. Breathing acoustics-based authentication offers a highly usable, possibly secondary authentication mechanism for such authorized access. Executing sophisticated machine learning pipelines for authentication on such devices remains an open problem, given their resource limitations in terms of storage, memory and computational power.  To investigate this possibility, we compare the performance of an end-to-end system for both user identification and user verification tasks based on breathing acoustics on three type of devices: smartphone, smartwatch and Raspberry Pi using both shallow classifiers (SVM, GMM, Logistic Regression)  and deep learning based classifiers (LSTM, MLP). Via detailed investigation, we conclude that LSTM models for acoustic classification are the smallest in size, have lowest inference time and are more accurate than all other compared classifiers. An uncompressed LSTM model provides 80%-94% accuracy while requiring only 50--180 KB of storage (depending on the breathing gesture). The resulting inference can be done on smartphones and smartwatches within approximately 8ms and 18 ms respectively, thereby making them suitable for resource-constrained devices. Further memory and computational savings can be achieved using model compression methods such as weight quantization and fully connected layer factorization.   We also compare the performance on GPUs and show that the use of GPU can reduce the inference time of LSTM models by a factor of 300%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21130
        },
        {
          "affiliations": [],
          "personId": 11109
        },
        {
          "affiliations": [],
          "personId": 11020
        },
        {
          "affiliations": [],
          "personId": 12552
        },
        {
          "affiliations": [],
          "personId": 11721
        },
        {
          "affiliations": [],
          "personId": 11293
        }
      ],
      "sessionIds": [
        2039
      ],
      "eventIds": []
    },
    {
      "id": 6464,
      "typeId": 11437,
      "title": "A Method to Recognize Eye Movement Based on Uplift Movement of Skin",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 22612
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3398,
      "typeId": 11437,
      "title": "Poster: 'Effect of bonding and washing on electronic textile stretch sensor properties'",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Knit e-textile sensors can be used to detect stretch or strain, and when integrated directly into wearable garments, they can be used to detect movement of the human body. However, before they can reliably be used in real-world applications, the garment construction technique and the effects of wear due to washing need to be considered. This paper presents a study examining how thermal bonding and washing affects piezo-resistive textile sensors. Three textile strain sensors are considered all using Technik-tex P130B as the conductive material: i) conductive fabric only, ii) conductive fabric bonded to on one side to Eurojersey fabric, and iii) conductive fabric with Eurojersey bonded on top and bottom of the conductive fabric. The sensors' performance is evaluated using a tensile tester while monitoring their electrical resistance before and after washing. The findings show that a single layer of bonding is the ideal construction and that after three wash cycles the sensor remains reliable.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18537
        },
        {
          "affiliations": [],
          "personId": 21452
        },
        {
          "affiliations": [],
          "personId": 10830
        },
        {
          "affiliations": [],
          "personId": 23884
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4423,
      "typeId": 11437,
      "title": "AdaRF: Adaptive RFID-based Indoor Localization Using Deep Learning Enhanced Holography",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, RFID-based localization systems have been widely deployed in many factories and warehouses for sorting or locating products. These systems are mainly generalized schemes which might suffer severe accuracy degradation in multipath-rich scenarios. In order to suppress environmental interferences, we present a fine-grained RFID-based indoor localization system AdaRF, which leverages deep learning enhanced holography to create adaptive localization models for individual environments. The key idea is to optimize the localization model using signals from a small number of known location tags to achieve high positioning accuracy in its deployed environment. Based on this point, we propose Adjacent Differential Hologram (ADH) which yields a robust location-independent probability map for each tag. AdaRF subsequently leverages the neural network to create an effective hologram-based position estimation method, which estimates the target tag position by analyzing the whole hologram. And we introduce transfer learning technique to significantly lower training cost for the position estimation model while ensuring high accuracy simultaneously. Comparative experiments demonstrate AdaRF achieves $cm$-level positioning accuracy both in the lateral and radial direction with only one moving antenna even in complex scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20075
        },
        {
          "affiliations": [],
          "personId": 21016
        },
        {
          "affiliations": [],
          "personId": 17043
        },
        {
          "affiliations": [],
          "personId": 13416
        }
      ],
      "sessionIds": [
        1235
      ],
      "eventIds": []
    },
    {
      "id": 7498,
      "typeId": 11437,
      "title": "Towards context-free Semantic Localisation",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a context-free semantic localisation approach to visualise and analyse indoor movements. We focus on settings where indoor location or rooms have strongly associated semantics, such as hospitals. We describe an approach that can work with different localisation systems, with little knowledge of the physical space properties, and with minimal bootstrapping required. We propose a movement representation that consists of time-encoded strings, and discuss how our approach can be used for analysing and visualising longitudinal indoor localisation data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14467
        },
        {
          "affiliations": [],
          "personId": 12510
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 18221
        },
        {
          "affiliations": [],
          "personId": 11200
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6474,
      "typeId": 11437,
      "title": "BoardWatch: A Tree-Enhanced Regression Model for Billboard Popularity Prediction with Multi-Source Urban Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Predicting the popularity of outdoor billboards is crucial for many applications such as guidance of billboard placement and estimation of advertising cost. Recently, some researchers have worked on leveraging single traﬀic data to access the performance of billboards, which often leads to coarse-grained performance estimation and undesirable ad placement plans. To solve the problem, we propose a data-driven system, named BoradWatch, for fine-grained billboard popularity prediction. In particular, we extract three types of critical features based on multi-source urban data, including billboard profile, geographic feature and commercial feature. Furthermore, we propose a hybrid model named Tree-Enhanced Regression Model (TERM) based on extracted features for prediction, which takes full advantage of the feature transformation of decision trees model to enhance the prediction performance of the linear model. Experiment results on real-world outdoor billboard data and multi-source urban data demonstrate the eﬀectiveness of our work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16092
        },
        {
          "affiliations": [],
          "personId": 13318
        },
        {
          "affiliations": [],
          "personId": 14679
        },
        {
          "affiliations": [],
          "personId": 17736
        },
        {
          "affiliations": [],
          "personId": 12522
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7501,
      "typeId": 11437,
      "title": "Mobile Money: Understanding and Predicting its Adoption and Use in a Developing Economy",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Access to financial institutions is difficult in developing economies and especially for the poor. However, the widespread adoption of mobile phones has enabled the development of mobile money systems that deliver financial services through the mobile phone network. Despite the success of mobile money, there is a lack of quantitative studies that unveil which factors contribute to the adoption and sustained usage of such services. In this paper, we describe the results of a quantitative study that analyzes data from the world{'}s leading mobile money service, M-Pesa. We analyzed millions of anonymized mobile phone communications and M-Pesa transactions in an Eastern African country. Our contributions are \\ threefold: (1) we analyze the customers' usage of M-Pesa and report large-scale patterns of behavior; (2) we present the results of applying machine learning models to predict mobile money adoption (AUC=0.691), and mobile money spending (AUC=0.619) using multiple data sources: mobile phone data, M-Pesa agent information, the number of M-Pesa friends in the user's social network, and the characterization of the user's geographic location; (3) we discuss the most predictive features in both models and draw key implications for the design of mobile money services in a developing country. We find that the most predictive features are related to mobile phone activity, to the presence of M-Pesa users in a customer's ego-network and to mobility. We believe that our work will contribute to the understanding of the factors playing a role in the adoption and sustained usage of mobile money services in developing economies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8207
        },
        {
          "affiliations": [],
          "personId": 14768
        },
        {
          "affiliations": [],
          "personId": 15243
        },
        {
          "affiliations": [],
          "personId": 13095
        },
        {
          "affiliations": [],
          "personId": 23213
        },
        {
          "affiliations": [],
          "personId": 12691
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 5454,
      "typeId": 11437,
      "title": "Au-Id: Automatic User Identification and Authentication through the Motions Captured from Sequential Human Activities Using RFID",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The advancements of ambient intelligence and ubiquitous computing are driving the unprecedented development of smart spaces where enhanced services are provided based on activity recognition. Meanwhile, user identification, which can enable the personalization of the enhanced services for specific users and the access control of confidential information, becomes increasingly important. Traditional approaches to user identification require either attached wearable sensors or active user participation. This paper presents Au-Id, a non-intrusive automatic user identification and authentication system through the motions captured from sequential activities based on RFID. The key insight is that the RFID tag array can capture human's physiological and behavioral characteristics for user identification. Particularly, phase and RSSI data streams of the RFID tag array are fused to incorporate the information from time, space and modality dimensions. Based on this, a novel sequence labeling-based segmentation method is proposed for target motion extraction. Then Au-Id leverages a multi-modal Convolutional Neural Network (CNN) for user identification and significantly reduces the training efforts by transfer learning. In addition, Au-Id facilitates user authentication by integrating the feature representations extracted by CNN with one-class SVM classifiers. The evaluation shows that Au-Id achieves 97.72% accuracy for user identification and 96.29% accuracy for authentication.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15720
        },
        {
          "affiliations": [],
          "personId": 21016
        },
        {
          "affiliations": [],
          "personId": 17043
        },
        {
          "affiliations": [],
          "personId": 13416
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 3407,
      "typeId": 11437,
      "title": "Tangible Urban Models: Two-way Interaction through 3D Printed Conductive Tangibles and AR for Urban Planning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Physical models are a key component in the architectural process and play an important role in understanding material and space relationships. We present Tangible Urban Models, an approach for leveraging the use of conductive material for 3D printed architectural prototypes. This enables non-interactive objects, such as buildings, to become tangible without the need to attach additional components. We combine this capability with an augmented reality (AR) app and explore the use of gestures for interacting with digital and physical content. The multi-material 3D printed buildings consist of thin layers of white plastic filament and a conductive wireframe to enable touch gestures. In this way, we enable a two-way interaction either with the physical model or with the mobile AR interface.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23428
        },
        {
          "affiliations": [],
          "personId": 24256
        },
        {
          "affiliations": [],
          "personId": 19050
        },
        {
          "affiliations": [],
          "personId": 8420
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7512,
      "typeId": 11437,
      "title": "TIARA: Technology Integrated Apnea Respiration Analyser",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Obstructive sleep apnea (OSA) is a serious disorder in which people repeatedly stop breathing during sleep. This paper presents the Technology Integrated Apnea Respiration Analyser (TIARA), an innovative wearable device as an alternative to polysomnography (PSG) that has the potential to enable cost-effective and repeated overnight tests of OSA even in a home environment. PSG normally collects bio-signals such as brain waves, oxygen saturation, heart rate in a controlled environment (e.g. sleep lab) to diagnose sleep disorders. Here we present the design and development of the TIARA device and demonstrate its potential to perform as well as PSG at differentiating sleep and wake states and different phases of sleep, based on machine learning models.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19002
        },
        {
          "affiliations": [],
          "personId": 21381
        },
        {
          "affiliations": [],
          "personId": 21477
        },
        {
          "affiliations": [],
          "personId": 21512
        },
        {
          "affiliations": [],
          "personId": 18899
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 7514,
      "typeId": 11437,
      "title": "Enhancing Indoor Inertial Odometry with WiFi",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Accurately measuring the distance traversed by a subject, commonly referred to as odometry, in indoor environments is of fundamental importance in many applications such as augmented and virtual reality tracking, indoor navigation, and robot route guidance. While theoretically, odometry can be performed using a simple accelerometer, practically, it is well-known that the distances measured using accelerometers suffer from large drift errors. In this paper, we propose WIO, a WiFi-assisted Inertial Odometry technique that uses WiFi signals as an auxiliary source of information to correct these drift errors. The key intuition behind WIO is that among multiple reflections of a transmitted WiFi signal arriving at the WiFi receiver, WIO first isolates one reflection and then measures the change in the length of the path of that reflection as the subject moves. By identifying the extent through which the length of the path of that reflection changes, along with the direction of motion of the subject relative to that path, WIO can estimate the distance traversed by the subject using WiFi signals. WIO then uses this distance estimate to correct the drift errors. While researchers have previously proposed to use WiFi signals to correct drift errors, prior schemes suffer from one or more of the following six limitations: they 1) do not work indoors, 2) require manual exhaustive fingerprinting, 3) are not resilient against changes in environment including human movements, 4) do not work on commodity WiFi devices, 5) require multiple access points, and/or 6) can measure distance traversed by humans but not by non-human subjects. WIO addresses all of these limitations. We implemented WIO using commodity devices, and extensively evaluated it in a wide variety of complex indoor scenarios on both human and robotic subjects. Our results demonstrate that WIO achieved an average error of just 6.28% in estimating the distances traversed by the subjects.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8566
        },
        {
          "affiliations": [],
          "personId": 24009
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 3425,
      "typeId": 11437,
      "title": "Activity Recognition using ST-GCN with 3D Motion Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "For the Nurse Care Activity Recognition Challenge, an activity recognition algorithm was developed by Team TDU-DSML. A spatial-temporal graph convolutional network (ST-GCN) was applied to process 3D motion capture data included in the challenge dataset. Time-series data was divided into 20-second segments with a 10-second overlap. The recognition model with a tree-structure graph was then created. The prediction result was set to one-minute segments on the basis of a majority decision from each segment output. Our model was evaluated by using leave-one-subject-out cross-validation methods. An average accuracy of 57% for all six subjects was achieved.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19968
        },
        {
          "affiliations": [],
          "personId": 8816
        },
        {
          "affiliations": [],
          "personId": 23634
        },
        {
          "affiliations": [],
          "personId": 12412
        },
        {
          "affiliations": [],
          "personId": 10784
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5475,
      "typeId": 11437,
      "title": "Multi-Stage Receptivity Model for Mobile Just-In-Time Health Intervention",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The critical aspect of mobile Just-in-Time (JIT) health intervention would be proper delivery timing since it is closely related to successfully promoting target behaviors. Despite extensive prior studies on interruptibility, however, our understanding of receptivity for mobile JIT health intervention is limited. This work extends prior interruptibility models to capture the JIT intervention process by including multiple stages of conscious or subconscious decisions. We build BeActive, a mobile intervention for preventing prolonged sedentary behaviors and then perform a three-week field study for data collection. We use the multi-stage model to systematically analyze receptivity with a mixed methodology. We identify the key contextual factors relevant to each stage outcome and show that the receptivity of JIT intervention is nuanced and context-dependent. We propose several practical design implications for mobile JIT health intervention and context-aware computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20248
        },
        {
          "affiliations": [],
          "personId": 24379
        },
        {
          "affiliations": [],
          "personId": 18609
        },
        {
          "affiliations": [],
          "personId": 22915
        },
        {
          "affiliations": [],
          "personId": 22860
        }
      ],
      "sessionIds": [
        2269
      ],
      "eventIds": []
    },
    {
      "id": 3427,
      "typeId": 11437,
      "title": "Pedestrians and Visual Signs of Intent: Towards Expressive Autonomous Passenger Shuttles",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Autonomous Passenger Shuttles (APS) are rapidly becoming an urban public transit alternative. Traversing populous commercial and residential centers, these shuttles are already operating in several cities. In the absence of a human driver and embedded means of communicating the autonomous shuttle's intent, the task of seamlessly navigating crosswalks and pedestrian-friendly zones becomes a challenging pursuit for pedestrians.\n\nWe contribute to the emerging notion of AV-Pedestrian Interaction by examining the context of autonomous passenger shuttles (APS) in real-world settings, and by comparing four different classes of visual signals — namely instructional, symbolic, metaphorical, and anthropomorphic — designed to communicate the shuttle's intentions. Following a participatory methodology involving local residents and public transport service provider, and working within the framework of inflexible road traffic regulations concerning the operation and testing of autonomous vehicles, we conducted a participatory design workshop, a qualitative, and a survey study. The findings revealed differences across these four classes of signals in terms of pedestrians' subjective perceptions. Anthropomorphic signals were identified as the preferred and effective modality in terms of pedestrians' interpretation of the communicated intent and their perceived sense of attention, confidence, and calmness. Additionally, pedestrians' experiences while judging the intention of transitionary vehicular states (starting/slowing) were reported as perplexing and evoked stress. These findings were translated into design and policy implications in collaboration with other stakeholders, and exemplify a viable way for assimilating human factors research in urban mobility.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16871
        },
        {
          "affiliations": [],
          "personId": 17192
        },
        {
          "affiliations": [],
          "personId": 9663
        },
        {
          "affiliations": [],
          "personId": 23396
        },
        {
          "affiliations": [],
          "personId": 15258
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 3428,
      "typeId": 11437,
      "title": "Transportation Mode Classification from Smartphone Sensors via a Long-Short-Term-Memory Network",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This article introduce the architecture of a Long-Short-Term-Memory network for classifying transportation-modes via smartphone data and evaluates its accuracy. By using a Long-Short-Term-Memory with common preprocessing steps such as normalisation for classification tasks an F1-Score accuracy of 63.68%  was achieved with an internal test dataset.\nWe participated as team \"GanbareAMT\" in the “SHL recognition challenge\".",
      "authors": [
        {
          "affiliations": [],
          "personId": 22473
        },
        {
          "affiliations": [],
          "personId": 16340
        },
        {
          "affiliations": [],
          "personId": 18033
        },
        {
          "affiliations": [],
          "personId": 8519
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7524,
      "typeId": 11437,
      "title": "Audio-Based Activities of Daily Living (ADL) Recognition with Large-Scale Acoustic Embeddings from Online Videos",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Over the years, activity sensing and recognition has been shown to play a key enabling role in a wide range of applications, from sustainability and human-computer interaction to health care. While many recognition tasks have traditionally employed inertial sensors, acoustic-based methods offer the benefit of capturing rich contextual information, which can be useful when discriminating complex activities. Given the emergence of deep learning techniques and leveraging new, large-scaled multimedia datasets, this paper revisits the opportunity of training audio-based classifiers without the onerous and time-consuming task of annotating audio data. We propose a framework for audio-based activity recognition that makes use of millions of embedding features from public online video sound clips. Based on the combination of oversampling and deep learning approaches, our framework does not require further feature processing or outliers filtering as in prior work. We evaluated our approach in the context of Activities of Daily Living (ADL) by recognizing 15 everyday activities with 14 participants in their own homes, achieving 64.2% and 83.6% averaged within-subject accuracy in terms of top-1 and top-3 classification respectively. Individual class performance was also examined in the paper to further study the co-occurrence characteristics of the activities and the robustness of the framework.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9611
        },
        {
          "affiliations": [],
          "personId": 19390
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 5481,
      "typeId": 11437,
      "title": "TiPoint: Detecting Fingertip for Mid-Air Interaction on Computational Resource Constrained Smartglasses",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartglasses mostly rely on hardware interfaces such as touch-pad and buttons, which are often cumbersome and counter-intuitive to use. Furthermore, smartglasses feature cheap and low-power hardware preventing the use of advanced pointing techniques. To overcome these issues, we introduce TiPoint,a freehand mid-air interaction technique.  TiPoint uses the monocular camera embedded in smartglasses to detect the user's hand, enabling intuitive and non-intrusive interaction. We introduce a light-weight algorithm for fingertip detection, which is especially suited for the limited hardware specifications and the short battery life time of smartglasses.  Our evaluation shows that TiPoint as a mid-air non-intrusive interface delivers a better experience for users and smartglasses interactions, with users completing typical tasks 1.82 times faster than when using the original hardware.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9835
        },
        {
          "affiliations": [],
          "personId": 20009
        },
        {
          "affiliations": [],
          "personId": 23152
        },
        {
          "affiliations": [],
          "personId": 11677
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 3435,
      "typeId": 11437,
      "title": "From Plastic to Biomaterials: Prototyping DIY Electronics with Mycelium",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Researchers, makers and hobbyists rely on plastics for creating their DIY electronics. Enclosures, battery holders, buttons and wires are used in most of the prototypes in a temporal way, generating waste. This research aims to extend the boundaries of biomaterials applications into electronics. Mycelium is a fast-growing vegetative part of a fungus which adapts to different shapes when growing in a mold and decomposes after 90 days in a natural environment as organic waste. In order to create more sustainable prototypes, we use mycelium composites with common digital fabrication techniques for replacing plastic in electronics. We present our method for growing mycelium, our design process of using digital fabrication techniques with mycelium, applications for embedding electronics in mycelium boards, making enclosures for electronics, and using mycelium within electronics. This paper could contribute with the merge of biomaterials and electronics, an approach which is still under exploration.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17003
        },
        {
          "affiliations": [],
          "personId": 15643
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4468,
      "typeId": 11437,
      "title": "Shadower: Applying Shadows to Children's Outdoor Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Shadows are ubiquitous in our life. Through the phenomena of shadows, we focus on a novel way which can change people's perspectives of observing surroundings and create a natural outdoor interaction for children in their daily life. In this paper, we present a mobile AR game which uses shadows as clues with a treasure hunting game mechanism to make a connection with children and outdoor surroundings. In the field experiment at a kindergarten, children(n=6) participated in the outdoor interaction experience with Shadower. We conducted a preliminary user study with Smileyometer strategy to evaluate children's reaction to our prototype. Qualitative results indicate that the use of shadows from the outdoor environment as AR makers have the potential to expand the approach to facilitate children's engagement through their outdoor interaction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9791
        },
        {
          "affiliations": [],
          "personId": 10670
        },
        {
          "affiliations": [],
          "personId": 15369
        },
        {
          "affiliations": [],
          "personId": 9257
        },
        {
          "affiliations": [],
          "personId": 12367
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3444,
      "typeId": 11437,
      "title": "A Deep Autoencoder Model for Pollution Map Recovery with Mobile Sensing Networks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Air pollution is a global health threat. Nowadays, with the increasing amount of air pollution monitoring data from either conventional official stations or mobile sensing systems, the role of deep learning methods in pollution map recovery becomes gradually apparent. To address the challenges including the irregular sampling from mobile sensing and the non-interpretability of deep learning models, we proposed a deep autoencoder framework based inference algorithm. By separating the process of pollution generation and data sampling, this framework is able to deal with sampling under any irregular intervals in time and space. Also, we adopt a convolutional long short-term memory (ConvLSTM) structure to model the pollution generation after revealing its internal connections with an atmospheric dispersion model. Our algorithm is evaluated over a three-month real-world data collection in Tianjin, China. Results show our method can obtain up to 2x performance improvement over existing methods, benefited from its high robustness against different background pollution level and accidental sensor errors.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19569
        },
        {
          "affiliations": [],
          "personId": 9838
        },
        {
          "affiliations": [],
          "personId": 20516
        },
        {
          "affiliations": [],
          "personId": 8532
        },
        {
          "affiliations": [],
          "personId": 14989
        },
        {
          "affiliations": [],
          "personId": 18241
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3447,
      "typeId": 11437,
      "title": "Route Prediction for Instant Delivery",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Instant delivery has drawn much attention recently, as it greatly facilitates people's daily lives. Unlike postal services, instant delivery imposes a strict deadline on couriers after a customer places an order online. Therefore it is critical to dispatch the order to an appropriate courier to guarantee the timely delivery. Ideally couriers should choose the optimal routes with the lowest overdue rate and the minimal distance. In practice, however, decision-making of the couriers is quite complex because individuals have different psychological perception of the environments (e.g., distance) and delivery requirements (e.g., deadline). To well predict their behaviors, we design multiple features to model the decision-making psychology of individual couriers and predict couriers' route with a machine learning algorithm. In particular, we reveal that perceived distance is the main factor influencing couriers' decision, which should be modeled based on the subjective understanding of the actual distances. Our design is implemented, deployed and evaluated on Ele.me, which is one of the largest instant delivery platforms in the world. Experimental results show that the overdue rate can be reduced by 48.02%, which is a significant improvement.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20986
        },
        {
          "affiliations": [],
          "personId": 23171
        },
        {
          "affiliations": [],
          "personId": 12350
        },
        {
          "affiliations": [],
          "personId": 10086
        },
        {
          "affiliations": [],
          "personId": 19531
        },
        {
          "affiliations": [],
          "personId": 14948
        },
        {
          "affiliations": [],
          "personId": 11734
        },
        {
          "affiliations": [],
          "personId": 11897
        }
      ],
      "sessionIds": [
        1040
      ],
      "eventIds": []
    },
    {
      "id": 6520,
      "typeId": 11437,
      "title": "AttriNet: Learning Mid-Level Features for Human Activity Recognition with Deep Belief Networks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition (HAR) is essential to many context-aware applications in mobile and ubiquitous computing. A human's \nphysical activity can be decomposed into a sequence of simple actions or body movements, corresponding to what we denote as mid-level features.  Such mid-level features, which we contrast to high-level activities (walking, running, ...) and low-level features (raw sensor readings or features derived from them), can be developed manually. While proven to be effective, this manual approach is not scalable and relies heavily on human domain expertise. In this paper, we address this limitation by proposing a machine learning method, AttriNet, based on deep belief networks.  Our AttriNet method automatically constructs mid-level features and outperforms baseline approaches. Interestingly, we show in experiments that some of the features learned by AttriNet highly correlate with manually defined features. This result demonstrates the potential of using deep learning techniques for learning mid-level features that are semantically meaningful, as a replacement to handcrafted features. More broadly, this empirical finding provides an improved intuitive understanding of deep learning methods in HAR.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10187
        },
        {
          "affiliations": [],
          "personId": 9775
        },
        {
          "affiliations": [],
          "personId": 20978
        },
        {
          "affiliations": [],
          "personId": 14827
        },
        {
          "affiliations": [],
          "personId": 16550
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3460,
      "typeId": 11437,
      "title": "Reconstructing Human Joint Motion with Computational Fabrics",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Accurate and continuous monitoring of joint rotational motion is crucial for a wide range of applications such as physical rehabilitation and motion training. Existing motion capture systems, however, either need instrumentation of the environment, or fail to track arbitrary joint motion, or impose wearing discomfort by requiring rigid electrical sensors right around the joint area. This work studies the use of everyday fabrics as a flexible and soft sensing medium to monitor joint angular motion accurately and reliably. Specifically we focus on the primary use of conductive stretchable fabrics to sense the skin deformation during joint motion and infer the joint rotational angle. We tackle challenges of fabric sensing originated by the inherent properties of elastic materials by leveraging two types of sensing fabric and characterizing their properties based on models in material science. We apply models from bio-mechanics to infer joint angles and propose the use of dualstrain sensing to enhance sensing robustness against user diversity and fabric position offsets. We fabricate prototypes using off-the-shelf fabrics and micro-controller. Experiments with ten participants show 9.69° median angular error in tracking joint angle and its sensing robustness across various users and activities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10732
        },
        {
          "affiliations": [],
          "personId": 20156
        },
        {
          "affiliations": [],
          "personId": 8268
        },
        {
          "affiliations": [],
          "personId": 13131
        },
        {
          "affiliations": [],
          "personId": 18674
        },
        {
          "affiliations": [],
          "personId": 15220
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 7557,
      "typeId": 11437,
      "title": "EPARS: Elderly Physical Activity Reminder System using Smartphone and Wearable Sensors full strip",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 17242
        },
        {
          "affiliations": [],
          "personId": 11411
        },
        {
          "affiliations": [],
          "personId": 23748
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3462,
      "typeId": 11437,
      "title": "PDkit: An Open Source Data Science toolkit for Parkinson's Disease",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Parkinson's Disease (PD) is a long-term neurodegenerative disorder that affects over four million people worldwide. State-of-the-art mobile and wearable sensing technologies offer the prospect of enhanced clinical care pathways for PD patients through integration of automated symptom tracking within current healthcare infrastructures. Yet, even though sensor data collection can be performed efficiently today using these technologies, automated inference of high-level severity scores from such data is still limited by the lack of validated evidence, despite a plethora of published research. In this paper, we introduce PDkit, an open source toolkit for PD progression monitoring using multimodal sensor data obtained by smartphone apps or wearables. We discuss how PDkit implements an information processing pipeline incorporating distinct stages for data ingestion and quality assessment, feature and biomarker estimation, and clinical scoring using high-level clinical scales. Finally, we demonstrate how PDkit facilitates outcome reproducibility and algorithmic transparency in the CUSSP clinical trial, a pilot, dual-site, open label study.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14836
        },
        {
          "affiliations": [],
          "personId": 18691
        },
        {
          "affiliations": [],
          "personId": 8550
        },
        {
          "affiliations": [],
          "personId": 9599
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3463,
      "typeId": 11437,
      "title": "Reduction of Marker-Body Matching Work in Activity Recognition Using Motion Capture",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, activity recognition is performed using an optical motion capture system that can measure three-dimensional position information of reflective markers attached to the body. The individual markers detected by motion capture are automatically associated with which part of the body they are attached to. However, due to the overlapping of obstacles and other body parts and misplacement of the markers, these may be hidden from the camera and enter a blind spot, which may frequently cause a marker to be associated to different body parts erroneously. Usually, these errors need to be corrected manually after measurement, but this work is very time consuming, cumbersome and requires some skill. In this research, it is thought that there is no problem in recognizing the activity even if the process of spending the effort of correcting the correspondence between the marker after measurement and the body is omitted in the activity recognition using the motion capture. Because feature quantities are extracted from activity data when performing action recognition, even if an error occurs in part of the marker data, the effect is small because the correct feature quantities are selected and other marker data can compensate for an error. In addition, in this paper, we proposed a method to recognize the activity using the data when the human body template preparation required before Mocap data measurement is omitted, which is one of marker body matching work. The verification showed that even if the marker body matching operation was omitted, it was possible to recognize the action with high accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15477
        },
        {
          "affiliations": [],
          "personId": 18107
        },
        {
          "affiliations": [],
          "personId": 19993
        },
        {
          "affiliations": [],
          "personId": 24129
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7559,
      "typeId": 11437,
      "title": "Alvus: A Reconfigurable 2-D Wireless Charging System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wireless charging pads such as Qi are rapidly gaining ground, but their limited power supply range still requires precise placement on a specific point. 2-D wireless power transfer (WPT) sheets consisting of coil arrays are one well-known counterpart to extend this range. However, these approaches require custom-made designs by expert engineers; what we need is a WPT system that can be reconfigured by simply placing ready-made modules on the intended surface (e.g., table, floor, shelf board, etc). In this paper, we present ``Alvus'', a reconfigurable 2-D WPT system which enables such simple construction of WPT surfaces. Our system is based on multihop WPT that composes ``virtual power cords'' and consists of three types of ready-made resonator modules: (i) transmitter, which outputs energy, (ii) relays, which pass energy down to the next module, and (iii) receivers, which receive energy and charge the loads. We show that power can be transferred efficiently (over 25%) within a range of 19.6 m^2 using a single transmitter. We implemented an end-to-end WPT system and demonstrated that Alvus is capable of intuitive construction/reconfiguration of WPT surfaces, as well as automatically deciding the power routes based on the sensed information (e.g., receiver location, module placement, obstructive objects).",
      "authors": [
        {
          "affiliations": [],
          "personId": 8284
        },
        {
          "affiliations": [],
          "personId": 14738
        },
        {
          "affiliations": [],
          "personId": 18682
        },
        {
          "affiliations": [],
          "personId": 13265
        },
        {
          "affiliations": [],
          "personId": 15754
        },
        {
          "affiliations": [],
          "personId": 21865
        }
      ],
      "sessionIds": [
        1861
      ],
      "eventIds": []
    },
    {
      "id": 5513,
      "typeId": 11437,
      "title": "Exploring Non-Emissive Wearable Display as a Clothing Accessory",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we explore wearable ambient display as a clothing accessory. A majority of wearable display research so far focuses on using displays as a channel to present concrete information and notifications. The aim of our research is to develop an understanding and identify the preferred characteristics of wearable displays as an everyday accessory and embellishment. We designed \\textit{the Scarf Set} headphones in the form of a hooded scarf, as an instance to explore the design space. The Scarf Set is used as a probe to provoke conversation around wearable displays as an everyday clothing and accessory item. The initial user study shows that our participants appreciated the interactive features enabled by the wearable display, but the constant changing and movement, which are part of the interaction, were not liked.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9801
        },
        {
          "affiliations": [],
          "personId": 23885
        },
        {
          "affiliations": [],
          "personId": 11106
        },
        {
          "affiliations": [],
          "personId": 8439
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7562,
      "typeId": 11437,
      "title": "Input, Output and Construction Methods for Custom Fabrication of Room-Scale Deployable Pneumatic Structures",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we examine the future of designing room-scale deployable pneumatic structures that can be fabricated with interactive capabilities and thus be responsive to human input and environments. While there has been recent advances in fabrication methods for creating large-scale structures, they have mainly focused around creating passive structures. Hence in this work, we collectively tackle three main challenges that need to be solved for designing room-scale interactive deployable structures namely -- the input, output (actuation) and construction methods. First, we explore three types of sensing methods --- acoustic, capacitive and pressure --- in order to embed input into these structures. These sensing methods enable users to perform gestures such as knock, squeeze and swipe with specific parts of our fabricated structure such as doors, windows, etc. and make them interactive. Second, we explore three types of actuation mechanisms -- inflatable tendon drive, twisted tendon drive and roll bending actuator -- that are implemented at structural scale and can be embedded into our structures to enable a variety of responsive actuation. Finally, we provide a construction method to custom fabricate and assemble inter-connected pneumatic trusses with embedded sensing and actuation capability to prototype interactions with room-scale deployable structures. To further illustrate the collective (input, output and construction) usage of the system, we fabricated three exemplar interactive deployable structures -- a responsive canopy, an interactive geodesic dome and a portable table (Fig 1). These can be deployed from a compact deflated state to a much larger inflated state which takes on a desired form while offering interactivity.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23180
        },
        {
          "affiliations": [],
          "personId": 8975
        },
        {
          "affiliations": [],
          "personId": 12226
        },
        {
          "affiliations": [],
          "personId": 23417
        },
        {
          "affiliations": [],
          "personId": 22743
        },
        {
          "affiliations": [],
          "personId": 10534
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 6542,
      "typeId": 11437,
      "title": "Demo: A Social Wearable that Affords Vulnerability",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present 'True Colors': a social wearable prototype designed to augment co-located social interaction of players in a LARP (live action role play). We designed it to enable the emergence of rich social dynamics between wearers and non-wearers. True Colors is Y-shaped, worn around the upper body, and has front and back interfaces to distinguish between actions taken by the wearer (front), and actions taken by others (back). To design True Colors, we followed a Research-through-Design approach, used experiential qualities and social affordances to guide our process, and co-designed with LARP designers. 13 True Colors wearables were deployed in a 3-day LARP event, attended by 109 people. From all the functionalities and interactivity the device afforded, players gravitated towards ones that emphasized the social value of experiencing vulnerability as a prompt to get together. This project was recently presented in CHI '19 and may offer useful insights to others in the UbiComp/ISWC community who develop technology to support co-located social experience.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16906
        },
        {
          "affiliations": [],
          "personId": 10515
        },
        {
          "affiliations": [],
          "personId": 19123
        },
        {
          "affiliations": [],
          "personId": 20849
        },
        {
          "affiliations": [],
          "personId": 10015
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7568,
      "typeId": 11437,
      "title": "Syn(es)thetic Reality: Simulating Synesthesia for the Non Synesthetic",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Syn(es)thetic Reality explores a new way of sensing the world by understanding sounds through colors. It looks to simulate projective chromesthesia, an experience of seeing colors involuntarily as a result of sound input. The project achieves this through an augmented reality web application to be run on mobile phones via a wearable device. By displaying audio based color content composited over the camera feed, the user\\textquotesingle s visual perception is altered to reflect their auditory perception. The basic principle is to correlate sound frequencies to hue and amplitude to saturation and visualise these colors creatively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23402
        },
        {
          "affiliations": [],
          "personId": 23283
        },
        {
          "affiliations": [],
          "personId": 15943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5521,
      "typeId": 11437,
      "title": "Benchmarking Deep Classifiers on Mobile Devices for Vision-based Transportation Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Vision-based human activity recognition can provide rich contextual information but has traditionally been computationally prohibitive. We present a characterisation of five convolutional neural networks (DenseNet169, MobileNet, ResNet50, VGG16, VGG19) implemented with TensorFlow Lite running on three state of the art Android mobile phones. The networks have been trained to recognise 8 modes of transportation from camera images using the SHL Locomotion and Transportation dataset. We analyse the effect of thread count and back-ends services (CPU, GPU, Android Neural Network API) to classify the images provided by the rear camera of the phones. We report processing time and classification accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12526
        },
        {
          "affiliations": [],
          "personId": 23750
        },
        {
          "affiliations": [],
          "personId": 12343
        },
        {
          "affiliations": [],
          "personId": 14916
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3477,
      "typeId": 11437,
      "title": "Can a Simple Approach Identify Complex Nurse Care Activity?",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "For the last two decades, more and more complex methods have been developed to identify human activities using various types of sensors, e.g., data from motion capture, accelerometer, and gyroscopes sensors. To date, most of the researches mainly focus on identifying simple human activities, e.g., walking, eating, and running. However, many of our daily life activities are usually more complex than those. To instigate research in complex activity recognition, the \"Nurse Care Activity Recognition Challenge\" [1] is initiated where six nurse activities are to be identified based on location, air pressure, motion capture, and accelerometer data. Our team, \"IITDU\", investigates the use of simple methods for this purpose. We first extract features from the sensor data and use one of the simplest classifiers, namely K-Nearest Neighbors (KNN). Experiment using an ensemble of KNN classifiers demonstrates that it is possible to achieve approximately 87% accuracy on 10-fold cross-validation and 66% accuracy on leave-one-subject-out cross-validation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15524
        },
        {
          "affiliations": [],
          "personId": 21038
        },
        {
          "affiliations": [],
          "personId": 9905
        },
        {
          "affiliations": [],
          "personId": 23156
        },
        {
          "affiliations": [],
          "personId": 21334
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7574,
      "typeId": 11437,
      "title": "ImprovingWearable Sensor Data Quality Using Context Markers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A major challenge in human activity recognition over long periods with multiple sensors is clock synchronization of independent data streams. Poor clock synchronization can lead to poor data and classifiers. In this paper, we propose a hybrid synchronization approach that combines NTP (Network Time Protocol) and context markers. Our evaluation shows that our approach significantly reduces synchronization error (20 ms) when compared to approaches that rely solely on NTP or sensor events. Our proposed approach can be applied to any wearable sensor where an independent sensor stream requires synchronization.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24289
        },
        {
          "affiliations": [],
          "personId": 13068
        },
        {
          "affiliations": [],
          "personId": 24337
        },
        {
          "affiliations": [],
          "personId": 23525
        },
        {
          "affiliations": [],
          "personId": 11200
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4505,
      "typeId": 11437,
      "title": "Swimming Style Recognition and Lap Counting Using a Smartwatch and Deep Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition from raw sensor data has enabled modern wearable devices to track and analyze everyday activities. However, when used in real world conditions, the performance of off-the-shelf devices is often insufficient. This paper tackles the problem of swimming style recognition and lap counting using sensor data from a single smartwatch. In total 17 hours of this data was collected from 40 swimmers of diverse backgrounds. The data was then used to train a convolutional neural network to recognize the four main swimming styles, transition periods and lap turns. Our method achieves an F1 score of 97.4% for style recognition and 99.2% for counting laps. To the best of our knowledge, these results are the first to enable accurate automatic swimming recognition in a realistic and completely uncontrolled environment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14784
        },
        {
          "affiliations": [],
          "personId": 13058
        },
        {
          "affiliations": [],
          "personId": 24026
        },
        {
          "affiliations": [],
          "personId": 15042
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 5533,
      "typeId": 11437,
      "title": "Investigating Gesture Typing for Indirect Touch",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, interacting with a smart TV or a head-mounted display using a handheld touchpad is common in daily life. We abstract this input mode as \"indirect touch\", where the touch surface and the feedback display are decoupled. Entering text using keyboard in indirect touch is challenging because before the finger touch, no control feedback is available for locating the touch finger. In this paper, we investigate the feasibility of gesture typing for indirect touch, since drawing a word gesture naturally provides continuous path feedback. We first examine users' gesture typing ability in terms of the appropriate keyboard size and location in the motor space, and compare the typing performance in direct and indirect touch mode. We then propose an improved design to address the uncertainty and inaccuracy of the first touch. Our evaluation result shows that users can quickly acquire indirect gesture typing, and type 22.3 words per minute after 30 phases, which significantly outperforms previous numbers in literature. Our work provides the empirical support for leveraging gesture typing for indirect touch.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11539
        },
        {
          "affiliations": [],
          "personId": 14464
        },
        {
          "affiliations": [],
          "personId": 9092
        },
        {
          "affiliations": [],
          "personId": 15758
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 6559,
      "typeId": 11437,
      "title": "Light Ears: Information Leakage via Smart Lights",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Modern Internet-enabled smart lights promise energy efficiency and many additional capabilities over traditional lamps. However, these connected lights also create a new attack surface, which can be maliciously used to violate users' privacy and security. In this paper, we design and evaluate novel attacks that take advantage of light emitted by modern smart bulbs, in order to infer users' private data and preferences. The first two attacks are designed to infer users' audio and video playback by a systematic observation and analysis of the multimedia-visualization functionality of smart light bulbs. The third attack utilizes the infrared capabilities of such smart light bulbs to create a covert-channel, which can be used as a gateway to exfiltrate user's private data out of their secured home or office network. A comprehensive evaluation of these attacks in various real-life settings confirms their feasibility and affirms the need for new privacy protection mechanisms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11914
        },
        {
          "affiliations": [],
          "personId": 15462
        }
      ],
      "sessionIds": [
        2386
      ],
      "eventIds": []
    },
    {
      "id": 4512,
      "typeId": 11437,
      "title": "Poster: Human Activity Recognition using Earable Device",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable sensors are monumental for human activity recognition. Researchers are continuously inventing new technology to detect human activity properly. Earable opens up interesting possibilities of monitoring personal scale behavioral activities. In this paper, we explore earables device ‘eSense’ multisensory stereo device for personal scale behavior analysis. We propose an activity recognition framework by exploiting eSense based multi-sensory device. It has a microphone, 6-axis inertial measurement unit, and a dual-mode Bluetooth. We use eSense accelerometer sensor data for detecting head and mouth related behavioral activities. We develop a data collection framework from the eSense through our smartphone application via Bluetooth. Then from the collected data, a few statistical features are computed to classify six personal scale activities related to head and neck movement such as speaking, eating, headshaking and head nodding, as well as, stay and walk. We aggregate the time series data into different action labels that summarize the user activity over a time interval. After, we train the data to induce a predictive model for activity recognition. We explore both machine learning and deep learning approach for data classification. For classification, we use the Support Vector Machine, Random Forest, and K-Nearest Neighbor and Convolutional Neural Network and achieve satisfactory recognition accuracy. The findings provide promising prospect for eSense for personal scale activity recognition in healthcare monitoring service. Based on our study, this work is done for the first time with satisfactory findings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21030
        },
        {
          "affiliations": [],
          "personId": 18968
        },
        {
          "affiliations": [],
          "personId": 22799
        },
        {
          "affiliations": [],
          "personId": 24129
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5536,
      "typeId": 11437,
      "title": "Fixing Mislabeling by Human Annotators via Conflict Resolution and the Exploitation of Prior Knowledge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "According to the ``human in the loop\" paradigm, machine learning algorithms can improve when leveraging on human intelligence, usually in the form of labels or annotation from domain experts. However, in the case of research areas such as ubiquitous computing or lifelong learning, where the annotator is not an expert and is continuously asked for feedback, humans can provide significant fractions of incorrect labels. We propose to address this issue in a series of experiments where students are asked to provide information about their behavior via a dedicated mobile application. Their trustworthiness is tested by employing an architecture where the machine uses all its available knowledge to check the correctness of its own and the user labeling to build a uniform confidence measure for both of them to be used when a contradiction arises. The overarching system runs through a series of modes with progressively higher confidence and features a conflict resolution component to settle the inconsistencies. The results are very promising and show the pervasiveness of annotation mistakes, the extreme diversity of the users' behaviors which provides evidence of the impracticality of a uniform fits-it-all solution, and the substantially improved performance of a skeptical supervised learning strategy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16513
        },
        {
          "affiliations": [],
          "personId": 18049
        },
        {
          "affiliations": [],
          "personId": 13122
        },
        {
          "affiliations": [],
          "personId": 13360
        },
        {
          "affiliations": [],
          "personId": 21458
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 4513,
      "typeId": 11437,
      "title": "Combining Lowand Mid-Level Gaze Features for Desktop Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition (HAR) is an important research area due to its potential for building context-aware interactive systems. Though movement-based activity recognition is an established area of research, recognising sedentary activities remains an open research question. Previous works have explored eye-based activity recognition as a potential approach for this challenge, focusing on statistical measures derived from eye movement properties‚Äîlow-level gaze features‚Äîor some knowledge of the Areas-of-Interest (AOI) of the stimulus‚Äîhigh-level gaze features. In this paper, we extend this body of work by employing the addition of mid-level gaze features; features that add a level of abstraction over low-level features with some knowledge of the activity, but not of the stimulus. We evaluated our approach on a dataset collected from 24 participants performing eight desktop computing activities. We trained a classifier extending 26 low-level features derived from existing literature with the addition of 24 novel candidate mid-level gaze features. Our results show an overall classification performance of 0.72 (F1-Score), with up to 4% increase in accuracy when adding our mid-level gaze features. Finally, we discuss the implications of combining low- and mid-level gaze features, as well as the future directions for eye-based activity recognition.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18619
        },
        {
          "affiliations": [],
          "personId": 24251
        },
        {
          "affiliations": [],
          "personId": 8984
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 3489,
      "typeId": 11437,
      "title": "Exploring the Efficacy of Sparse, General-Purpose Sensor Constellations for Wide-Area Activity Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Future smart homes, offices, stores and many other environments will increasingly be monitored by distributed sensors, supporting rich, context-sensitive applications. There are two opposing instrumentation approaches. On one end is full sensor saturation, where every object of interest is tagged with a sensor. On the other end, we can imagine a hypothetical, omniscient sensor capable of detecting events throughout an entire building from one location. Neither approach is currently practical, and thus we explore the middle ground between these two extremes: a sparse constellation of sensors working together to provide the benefits of full saturation, but without the social, aesthetic, maintenance and financial drawbacks. More specifically, we target a density of one sensor per room (and less), which means the average home could achieve full coverage with perhaps ten sensors. We quantify and characterize the performance of sparse sensor constellations through deployments across three environments and 67 unique activities. Our results illuminate accuracy implications across key spatial configurations important for enabling more practical, wide-area ubiquitous sensing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21779
        },
        {
          "affiliations": [],
          "personId": 13253
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 4514,
      "typeId": 11437,
      "title": "A Multi-modal Approach for Non-invasive Detection of Coronary Artery Disease",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Coronary Artery Disease (CAD) is a leading cause of death globally. Coronary angiography, the clinical diagnosis for CAD involves a surgery and admission to hospital. While this is a proven gold standard, having a less exact low-cost non-invasive screening method would be very helpful in mass diagnosis and pre-diagnosis. However, all physiological manifestations of CAD either appear late in the time-curve or are non-specific surrogate markers. With the advent of Artificial Intelligence (AI), there is new hope using multi-modal non-invasive sensing and analysis. In this paper, we combine domain knowledge with AI based data analysis to propose a novel two-stage approach that effectively incorporates multiple CAD markers in various non-invasive cardiovascular signals for an improved diagnosis system. At first stage, a hierarchical rule-engine identifies the high cardiac risk population using patient demography and medical history, who are further analysed at the second stage using numeric features from various cardiovascular signals. Results show that the proposed approach achieves sensitivity = 0.96 and specificity = 0.91 in classifying CAD patients on an in-house hospital dataset, recorded using commercially available sensors.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20353
        },
        {
          "affiliations": [],
          "personId": 9771
        },
        {
          "affiliations": [],
          "personId": 12036
        },
        {
          "affiliations": [],
          "personId": 14122
        },
        {
          "affiliations": [],
          "personId": 19497
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5540,
      "typeId": 11437,
      "title": "Animo: Sharing Biosignals on a Smartwatch for Lightweight Social Connection",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present Animo, a smartwatch app that enables people to share and view each other's biosignals. We designed and engineered Animo to explore new ground for smartwatch social computing systems: identifying opportunities where these systems can support lightweight and mood-centric interactions. In our work we develop, explore, and evaluate several innovative features designed for dyadic communication of heart rate. We discuss the results of a two-week study (N=34), including new communication patterns participants engaged in, and outline the design landscape for communicating with biosignals on smartwatches.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11325
        },
        {
          "affiliations": [],
          "personId": 16380
        },
        {
          "affiliations": [],
          "personId": 10671
        },
        {
          "affiliations": [],
          "personId": 9171
        },
        {
          "affiliations": [],
          "personId": 23336
        },
        {
          "affiliations": [],
          "personId": 9578
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 4522,
      "typeId": 11437,
      "title": "Butterfly: Environment-Independent Physical-Layer Authentication for Passive RFID",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "RFID tag authentication is challenging because most commodity tags cannot run cryptographic algorithms. Prior research demonstrates that physical layer information based authentication is a promising solution, which uses special features from the physical backscatter signals from tags as their fingerprints. However, our recent studies show that existing physical-layer authentication may fail if feature collection and authentication are conducted in different locations, due to location-dependent noises, environmental factors, or reader hardware differences. \\ This paper presents a new physical layer authentication scheme, called Butterfly, that is resilient to environment and location changes. Butterfly utilizes a pair of adjacent tags as an identifier of certain object. By using the difference between the RF signals of the two tags as their fingerprint, the environmental factors can be effectively canceled. Butterfly is fully compatible with commodity RFID systems and standards. We setup a prototype Butterfly using commodity readers, tags, and RF devices. Extensive experiments show that Butterfly achieves high authentication accuracy for substantially different \\ environments and device changes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8385
        },
        {
          "affiliations": [],
          "personId": 19395
        },
        {
          "affiliations": [],
          "personId": 18065
        },
        {
          "affiliations": [],
          "personId": 15184
        },
        {
          "affiliations": [],
          "personId": 19717
        },
        {
          "affiliations": [],
          "personId": 15493
        },
        {
          "affiliations": [],
          "personId": 16417
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 5547,
      "typeId": 11437,
      "title": "EEG Spectra vs Recurrence Features in Understanding Cognitive Effort",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Characterizing cognitive states of a person based on surface electroencephalography (EEG) is traditionally performed by analyzing spectral power of the electrical brain activity at different frequency bands. Although frequency band activity over different electrode locations can provide valuable input for discriminating conscious and unconscious or drowsy and fully awaken brain states, it fails when fine grained cognitive state and effort estimation is required. This paper demonstrates that nonlinear features can be used to describe aspects of cognitive effort. Recurrence quantification analysis (RQA) features were used to discriminate mental relaxation, math calculations, and contemplating scientific articles. However, only when used in combination with traditional spectral features, the discrimination performance increases. The most dominant RQA features are recurrence rate and ratio, while the most dominant spectral features are relative beta and delta power.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11081
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 7603,
      "typeId": 11437,
      "title": "Towards multi-person motion forecasting: IMU based motion capture approach",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Forecasting body motion has a lot of potential applications such as sports and entertainment. Previous studies have mainly employed cameras and optical motion captures to measure the joint positions of person, and predicted them about 0.5 seconds before by using deep neural networks. However, following two difficulties have to be solved to install the forecasting system into the real world: One is that camera and optical based methods have to take into account the environmental settings and occlusion problems, and the other is that previous studies have not considered plural persons.\nIn this paper, we propose a multi-person motion forecasting system by using inertial measurement unit (IMU) motion captures to overcome these difficulties simultaneously, and demonstrate a preliminary result.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9069
        },
        {
          "affiliations": [],
          "personId": 13543
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5556,
      "typeId": 11437,
      "title": "Awareness Jacket: EMF-Shielding Garment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This project explores ways to augment the sense of awareness. The human  senses are exceptional, this project attempts to empower them in order to augment the senses further.  The project aim is to elevate the senses by deleting  distractions. Although the main sense that has been augmented is awareness, the project is offering a device for the other senses to operate better.\n\nThere has been substantial progress in measuring the level of awareness, however is it not known of any instrument or precise method for the possibility to measure the contents of awareness directly (Seth et al. 2008).  The current methodology suggests verbal reporting for the most direct method used to find out the level of a person's  awareness of some specific knowledge.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21566
        },
        {
          "affiliations": [],
          "personId": 23283
        },
        {
          "affiliations": [],
          "personId": 15943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4535,
      "typeId": 11437,
      "title": "eTextiles: Reviewing a Practice through its Tool/Kits",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Electronic textiles, or eTextiles, and connected research and practice communities grew within and across diverse disciplines over the past 20 years. Initially evolving from academic investigations, eTextiles now play a growing role in both industry and education alike. While we are increasingly confronted with resulting eTextile artefacts, we lack a thorough understanding of the underlying making practices, and in particular what role toolkits play in framing, promoting and supporting creation practices and the communities engaging in the field. It is timely then to undertake a review of currently available eTextile tools and kits, discussing the technical, cultural and social expectations inscribed in these settings and how their design and technology impacts the emerging field of eTextiles. Here we compile the first overview of both academic research and popular available toolkits, as a basis for an analysis of potential strategies for future directions: how to diversify and professionalise the field, the practice, and connected communities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19740
        },
        {
          "affiliations": [],
          "personId": 16143
        },
        {
          "affiliations": [],
          "personId": 13910
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 4537,
      "typeId": 11437,
      "title": "LDC '19: International Workshop on Longitudinal Data Collection in Human Subject Studies",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 9704
        },
        {
          "affiliations": [],
          "personId": 17070
        },
        {
          "affiliations": [],
          "personId": 14596
        },
        {
          "affiliations": [],
          "personId": 11973
        },
        {
          "affiliations": [],
          "personId": 14291
        },
        {
          "affiliations": [],
          "personId": 8217
        },
        {
          "affiliations": [],
          "personId": 10104
        },
        {
          "affiliations": [],
          "personId": 21361
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4541,
      "typeId": 11437,
      "title": "An Early Characterisation of Wearing Variability on Motion Signals for Wearables",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We explore a new variability observed in motion signals acquired from modern wearables. Wearing variability refers to the variations of the device orientation and placement across wearing events. We collect the accelerometer data on a smartwatch and an earbud and analyse how motion signals change due to the wearing variability. Our analysis shows that the wearing variability can bring an unexpected change to motion signals, not only from different users but also from different wearing sessions of the same user. We also provide empirical ranges of changes in device orientations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14247
        },
        {
          "affiliations": [],
          "personId": 16356
        },
        {
          "affiliations": [],
          "personId": 22182
        },
        {
          "affiliations": [],
          "personId": 23587
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 7615,
      "typeId": 11437,
      "title": "Towards Automating Smart Homes: Contextual and Temporal Dynamics of Activity Prediction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The existing smart-home ecosystem has the capability to perceiving the ambient environment by using cutting-edge sensing technologies but is limited to reacting autonomously and timely. Successfully predicting the subsequent human activity can effectively infer human intention and instruct the smart homes to react in a timely, customized and accurate way. However, predicting the next activity and its precise occurrence period are challenging due to the complexity of modelling human behaviour. In this paper, the \\textit{Long Short-Term Memory} (LSTM) network equipped with temporal information is investigated to understand whether integrated temporal information on the model has better prediction performance or not. Our results highlight that, accurately integrating the temporal information into the models bring better prediction accuracy. In terms of modelling and further predicting human activity, comprehending the contextual-temporal dynamics is highly significant.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15436
        },
        {
          "affiliations": [],
          "personId": 13203
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4544,
      "typeId": 11437,
      "title": "Multi-view Commercial Hotness Prediction Using Context-aware Neural Network Ensemble",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Prediction over heterogeneous data attracts much attention in urban computing. Recently, satellite imagery provides a new chance for urban perception but raises the problem of how to fuse visual and non-visual features. So far, the practice is to concatenate the multimodal features into a vector, which may suppress important features. Therefore, we propose a new ensemble learning framework: (1) An estimator is developed for each predictor to score its confidence, which is input adaptive. (2) By applying the output of each predictor to the input of the corresponding estimator as feedback, the estimator learns the performance of the predictor in the input-output space. When a new input is applied to produce a prediction, the similar situations will be recalled by the estimator to score the confidence of the prediction. (3) Using end-to-end training, the estimator learns the weight automatically to minimize the total loss of the neural networks. With the proposed method, data mining based urban computing and computer vision rendered urban perception can be bridged at the task of commercial activeness prediction, where the prediction based on satellite images and social context data are fused to yield better prediction than those based on single view data in the experiments.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22048
        },
        {
          "affiliations": [],
          "personId": 21008
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 6593,
      "typeId": 11437,
      "title": "Efficient Multiplier-Less Inference of Deep Autoencoders on Wearable Healthcare Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an efficient multiplier-less inference (MLI) approach of deep autoencoders (DAE) for wearable healthcare systems. It employs a novel grouped multiplier block (GMB) module to reduce computational/hardwired complexity of DAE during inference process. First, the fixed weights of DAE are transformed into sum-of-powers-of-two (SOPOT) representations so that multiplications in DAE can be realized as limited adds and shifts only. Further, a GMB is designed to reuse the partial sums in generating the products from the same inputs, which can greatly reduce the adds required. Experimental results show that our proposed MLI method is effective and efficient for wearable healthcare systems to reduce computational/hardwired complexity as well as to offer a faster software implementation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17647
        },
        {
          "affiliations": [],
          "personId": 8629
        },
        {
          "affiliations": [],
          "personId": 23887
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7620,
      "typeId": 11437,
      "title": "PDR with Head Swing Detection Only using Hearable Device",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "PDR is a method of estimating the relative position from initial position using only an accelerometer and gyroscope. In recent years, hearable devices are becoming increasingly popular, and there are many researches on head pose estimation with them. In this paper, we aim to realize PDR considering head pose using only sensor data obtained with hearable devices. However, horizontal head swing affects the estimation of the traveling direction of PDR when wearing the sensor on ear. Using the difference of acceleration applied to both ears to detect head swing. For evaluation, we created the device with accelerometer and gyroscope attached to the left and right speaker of the headphone. As a result of the evaluation, the accuracy of swing motion estimation is 88.0%. F-measure of swing is 0.87. In conclusion, the detection result of swing is adopted to PDR, and realized PDR that use sensor data obtained by hearable device.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16096
        },
        {
          "affiliations": [],
          "personId": 23139
        },
        {
          "affiliations": [],
          "personId": 24171
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7623,
      "typeId": 11437,
      "title": "Demo: Hybrid Data-Driven and Context-Aware Activity Recognition with Mobile Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We have designed and implemented a real-time hybrid activity recognition system which combines supervised learning on inertial sensor data from mobile devices and context-aware reasoning. We  demonstrate how the context surrounding the user, combined with common knowledge about the relationship between this context and human activities, can significantly increase the ability to discriminate among activities when machine learning over inertial sensors has clear difficulties.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18298
        },
        {
          "affiliations": [],
          "personId": 9238
        },
        {
          "affiliations": [],
          "personId": 21875
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4551,
      "typeId": 11437,
      "title": "Toccata: Supporting Classroom Orchestration with Activity Based Computing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present Toccata, a system that facilitates the management of rich multi-device pedagogical activities. Through interviews with high schools teachers, we identified a set of practical barriers and technical breakdowns in conducting digital activities in  schools: set-up time, network problems, difficulties to follow and change plans as activities unfold. We designed and developed Toccata to support these problems with innovative functions like the planning of pedagogical activities (scripting), seamless sharing of content across people and devices, live management of activities in the classroom, roaming for situations outside classrooms, resumption across sessions, and resilience to unstable network conditions. Toccata uses an activity centric approach that builds on two conceptual frameworks: Orchestration and Activity Based Computing (ABC). Activities are collaborative, reusable and shareable. Toccata supports tight or loose activity scripting, let teachers conduct digital activities in class and outside, and lets them adjust unfolding activities according to the situation. We deployed Toccata in three classes, over seven teaching sessions, involving a total of 69 students. Together these deployments show that Toccata is a generic solution to manage multi-device activities in schools and pointed design directions to improve orchestration and reflection on activity mechanisms.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23504
        },
        {
          "affiliations": [],
          "personId": 12592
        },
        {
          "affiliations": [],
          "personId": 19360
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 5575,
      "typeId": 11437,
      "title": "Crowd-enabled Processing of Trustworthy, Privacy-Enhanced and Personalised Location Based Services with Quality Guarantee",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a novel approach for enabling trustworthy, privacy-enhanced and personalized location based services (LBSs) that find nearby points of interests (POIs) such as restaurants, ATM booths, and hospitals in a crowdsourced manner. In our crowdsourced approach, a user forms a group from the crowd and processes the LBS using the POI knowledge of the group members without involving an external service provider. We use personalized rating in addition to the distance of a POI for finding the answers of the location based queries. The personalized rating of a POI is computed using individual POI ratings given by the group members and the query requestor's trust and similarity scores for the group members. The major challenges for the crowdsourced data are incompleteness and inaccuracy, which may result in lower quality answer for the LBS. We first present techniques to select knowledgeable group members for processing LBSs and thereby increase the accuracy and the confidence level of the query answers. We then develop efficient algorithms to process LBSs in real time and enhance privacy by reducing the number of POIs shared with the query requestor. Finally, we run experiments using real datasets to show the efficiency and effectiveness of our approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14975
        },
        {
          "affiliations": [],
          "personId": 23735
        },
        {
          "affiliations": [],
          "personId": 21381
        },
        {
          "affiliations": [],
          "personId": 22926
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 6602,
      "typeId": 11437,
      "title": "Tap-to-Pair: Associating Wireless Devices with Synchronous Tapping",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Ad-hoc wireless device pairing enables impromptu interactions in smart spaces, such as resource sharing and remote control. The pairing experience is mainly determined by the device association process, during which users express their pairing intentions between the advertising device and the scanning device. Currently, most wireless devices are associated by selecting the advertiser's name from a list displayed on the scanner's screen, which becomes less efficient and often misplaced as the number of wireless devices increases. In this paper, we propose Tap-to-Pair, a spontaneous device association mechanism that initiates pairing from advertising devices without hardware or firmware modifications. Tapping an area near the advertising device's antenna can change its signal strength. Users can then associate two devices by synchronizing taps on the advertising device with the blinking pattern displayed by the scanning device. By leveraging the wireless transceiver for sensing, Tap-to-Pair does not require additional resources from advertising devices and needs only a binary display (e.g. LED) on scanning devices. We conducted a user study to test users' synchronous tapping ability and demonstrated that Tap-to-Pair can reliably detect users' taps. We ran simulations to optimize parameters for the synchronization recognition algorithm, and provide pattern design guidelines. We used a second user study to evaluate the on-chip performance of Tap-to-Pair, which achieves a successful pairing rate of 93.7% for three devices at different distances.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13882
        },
        {
          "affiliations": [],
          "personId": 9092
        },
        {
          "affiliations": [],
          "personId": 23561
        },
        {
          "affiliations": [],
          "personId": 11984
        },
        {
          "affiliations": [],
          "personId": 14464
        },
        {
          "affiliations": [],
          "personId": 21022
        },
        {
          "affiliations": [],
          "personId": 15758
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 4556,
      "typeId": 11437,
      "title": "Are you Sitting Uncomfortably? A tale of comfort, energy and productivity",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 8225
        },
        {
          "affiliations": [],
          "personId": 13232
        },
        {
          "affiliations": [],
          "personId": 10519
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7633,
      "typeId": 11437,
      "title": "TONG: A wearable system to remind people of interpersonal distance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The range of interpersonal distance is changing according to the environment, gender, culture, and individual person. Thus, realizing interpersonal distance is an important factor in modern society. In this work, we aim to call people’s attention to personal distance. We developed a prototype system, TONG, consisting of a wearable device and a mobile phone application. The system detects the people from behind entering the set distance and reminds them of keeping a proper interpersonal distance. To test the effectiveness of this system, we conducted quantitative and qualitative experiments. The results showed that our system express interpersonal distance more clearly. Further studies will be conducted by using multi-mode to suit different needs of interpersonal distance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12775
        },
        {
          "affiliations": [],
          "personId": 17428
        },
        {
          "affiliations": [],
          "personId": 15097
        },
        {
          "affiliations": [],
          "personId": 21160
        },
        {
          "affiliations": [],
          "personId": 9257
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5587,
      "typeId": 11437,
      "title": "NOSE: A Novel Odor Sensing Engine for Ambient Monitoring of the Frying Cooking Method in Kitchen Environments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "How we cook and prepare our food has an enormous impact on our health and well-being. Specific cooking methods, like deep-frying, are linked to obesity and the degradation of food nutrients, which contribute to various diseases and health issues. We present NOSE, a Novel Odor Sensing Engine, that passively and continuously monitors gas emissions in the kitchen area using an array of six metal oxide semiconductor (MOS) gas sensors and detects the occurrence of deep-frying. To evaluate NOSE, we collected sensor data from five foods (chicken, fish, beef, potato, and onion) cooked with three methods (deep-frying, grilling, and boiling) and three common frying oils (canola, corn, and soybean) in three different kitchens in a controlled manner. We demonstrate that NOSE can classify cooking by deep-frying with an average F1-score of 0.89. Based on the in-laboratory findings, we deployed NOSE in two different real-world households throughout a three-week period and successfully detected the occurrence of frying cooking with an average F1-score of 0.72, which is a promising result considering the relatively small number of data samples collected. Our results show the potential of using NOSE as an assistive dietary monitoring tool that periodically reports to users about their cooking habits.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10050
        },
        {
          "affiliations": [],
          "personId": 9968
        },
        {
          "affiliations": [],
          "personId": 11799
        },
        {
          "affiliations": [],
          "personId": 21097
        },
        {
          "affiliations": [],
          "personId": 13297
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 7636,
      "typeId": 11437,
      "title": "WiDetect: Robust Motion Detection with a Statistical Electromagnetic Model",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Motion detection acts as a key component for a range of applications such as home security, occupancy and activity monitoring, retail analytics, etc. Most existing solutions, however, require special installation and calibration and suffer from frequent false alarms with a very limited coverage. In this paper, we propose WiDetect, a highly accurate, robust, and calibration-free wireless motion detector that achieves almost zero false alarm rate and a large through-the-wall coverage. Different from previous approaches that either extract data-driven features or assume a few reflection multipaths, we model the problem from a perspective of statistical electromagnetic (EM) by accounting for all multipaths indoors. By exploiting the statistical theory of EM waves, we establish a connection between the autocorrelation function of the physical layer channel state information (CSI) and target motion in the environment. On this basis, we devise a novel motion statistic that is independent of environment, location, orientation, and subjects, and then perform a hypothesis testing for motion detection. By harnessing abundant multipaths indoors, WiDetect can detect arbitrary motion, be it in Line-Of-Sight vicinity or behind multiple walls, providing sufficient whole-home coverage for typical apartments and houses using a single link on commodity WiFi. We conduct extensive experiments in a typical office, an apartment, and a single house with different users for an overall period of more than 5 weeks. The results show that WiDetect achieves a remarkable detection accuracy of 99.68% with zero false rate, significantly outperforming the state-of-the-art solutions and setting up the stage for ubiquitous motion sensing in practice.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17035
        },
        {
          "affiliations": [],
          "personId": 23276
        },
        {
          "affiliations": [],
          "personId": 23941
        },
        {
          "affiliations": [],
          "personId": 17683
        },
        {
          "affiliations": [],
          "personId": 11994
        },
        {
          "affiliations": [],
          "personId": 11425
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 3550,
      "typeId": 11437,
      "title": "A Weiner Filter Based Heart Rate Estimation Algorithm From Wrist Based Photoplethysmogram",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The paradigm of wellness consists of both physical and mental wellness. One important parameter of physical wellness in the monitoring of cardiac health during activity, which requires measurement of ambulatory heart rate (HR). With the advent of smart wearable devices, measurement of heart-rate using Photoplethysmogram (PPG) has become a commodity. However, arriving at a reliable heart-rate measurement in real-time during daily activities is an open research problem. In this paper, we propose a method based on Weiner Filter, to estimate the correct HR values in the presence of motion, while being computationally efficient to be run on-device. Results are presented on a public data-set which prove the efficacy and efficiency of the proposed method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17773
        },
        {
          "affiliations": [],
          "personId": 23831
        },
        {
          "affiliations": [],
          "personId": 13748
        },
        {
          "affiliations": [],
          "personId": 23347
        },
        {
          "affiliations": [],
          "personId": 9771
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6623,
      "typeId": 11437,
      "title": "Demo: LiftSmart-A monitoring and warning wearable for weight trainers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we demonstrate LiftSmart, a novel smart wearable to detect, track and analyse weight training activities. LiftSmart is the first wearable for weight training that is based on unsupervised machine learning techniques to eliminate the use of labelled data, which is expensive to collect, computationally intensive, and requires the tuning of multiple key parameters.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19115
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 13331
        },
        {
          "affiliations": [],
          "personId": 15856
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3553,
      "typeId": 11437,
      "title": "V-Speech: Noise-Robust Speech Capturing Glasses Using Vibration Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smart glasses are often used in public environments or industrial scenarios that are relatively noisy.  High background noise and sound from competing speakers deteriorate voice communication or automatic speech recognition (ASR).  Typically, signal processing techniques are used to reduce noise and enhance voice quality, but they have limitations in performance, hardware and/or computing resources.  We present V-Speech, a novel sensing and signal processing solution that enables speech recognition and H2H communication in very noisy environments.  It captures the voice signal with a vibration sensor located in the nasal pads of smart glasses, and performs a transformation to the sensor signal in order to mimic that of a regular microphone in low noise conditions.  The transformation is key, as it eliminates the ‚Äúnasal distortion‚Äù that is introduced for nasal phonemes in the speech induced vibrations of the nasal bone.  We evaluated V-Speech in noise-free and noise-added conditions with 30 volunteer speakers uttering 145 phrases, and validated its performance on ASR engines.  The results show in extreme noise conditions a mean improvement of 50% for Word Error Rate (WER), and 1.0 on a scale of 5.0 for PESQ. Subjective listening tests made under representative high noise conditions (SPL of 93 dBA), show that V-Speech achieved 30 dB SNR, intelligible speech, and naturalness rated as fair to good.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14488
        },
        {
          "affiliations": [],
          "personId": 12384
        },
        {
          "affiliations": [],
          "personId": 17208
        },
        {
          "affiliations": [],
          "personId": 14463
        },
        {
          "affiliations": [],
          "personId": 21867
        },
        {
          "affiliations": [],
          "personId": 19937
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 3554,
      "typeId": 11437,
      "title": "GymCam: Detecting, Recognizing and Tracking Simultaneous Exercises in Unconstrained Scenes",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There has been a surge in exercise tracking and monitoring in the wearables community. However, a wearable attached to one part of the body is inadequate in capturing a wide range of exercises, especially ones involving other limbs. We present GymCam, a system for automatically detecting, recognizing and tracking multiple exercises simultaneously via a camera placed in the environment without any user intervention. We segment all concurrently occurring exercises from other activities in the video with an accuracy of 84.6%; recognize the type of each exercise (acc.=93.6%) and count the number of repetitions (+- 1.7 counts). GymCam advances the field of real-time exercise tracking by filling some crucial gaps, such as tracking whole body motion, handling occlusion, and enabling single-point sensing for a multitude of users.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21236
        },
        {
          "affiliations": [],
          "personId": 11785
        },
        {
          "affiliations": [],
          "personId": 24245
        },
        {
          "affiliations": [],
          "personId": 19714
        },
        {
          "affiliations": [],
          "personId": 13253
        },
        {
          "affiliations": [],
          "personId": 13444
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 4580,
      "typeId": 11437,
      "title": "Poster: 'Eldertainment or functional necessity? How virtual agents affect the home lives of people with dementia using the Quality of Life (QOL-AD) scale'",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A large literature evaluates the impact virtual agents have in the lifestyles of people with dementia by assessing their perceptions of technology. We evaluate the impact of a home virtual agent (Anne) developed as part of the ``Living well with Anne'' project on the \\textit{quality of life} of elderly people with dementia rather than focusing only on their perception of the technology. This comparison of the perception of technology with impact on life is particularly pertinent given the importance of a person's perceived quality of their daily home life and given that positive perception of a technology do not always lead to actual use. We propose an approach to evaluating assistive technology for elderly people with dementia by assessing impact on users' lives using semi-structured interviews and the QOL-AD scale. We describe a preliminary proof-of-concept study that tests whether perceptions of a virtual agent, actual use of a virtual agent and a participant's quality of life is related, as well as whether a virtual agent improves people's quality of life.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10629
        },
        {
          "affiliations": [],
          "personId": 8456
        },
        {
          "affiliations": [],
          "personId": 8776
        },
        {
          "affiliations": [],
          "personId": 18802
        },
        {
          "affiliations": [],
          "personId": 19326
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4581,
      "typeId": 11437,
      "title": "Evolving Needs in IoT Control and Accountability: A Longitudinal Study on Smart Home Intelligibility",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A key issue for smart home systems is supporting non-expert users in managing these distributed cyber-physical systems. Whereas feedback design on use cases (such as energy feedback) have gained attention, current approaches to providing awareness on the system state typically provide a rather technical view. Especially, long-term investigations on the practices and resources needed for maintaining Do-It-Yourself smart home systems, are scarce. We report on a design case study, in which we equipped 12 households with DIY smart home systems for two years and studied participants‚Äô strategies for maintaining system awareness, from learning about its workings to monitoring its behavior. We find that people‚Äôs needs regarding system accountability changed over time. Equally, their privacy needs in the same period were affected. We found that participants initially looked for in-depth awareness information from the dedicated web-based dashboard. In the later phases of appropriation, however, their interaction and information needs shifted towards management by exception on mobile or ambient displays‚Äì only focusing on the system when things were ‚Äògoing wrong‚Äô. In terms of system accountability, we find that a systems self-declaration should focus on socially meaningful rather than technically complete fashion, e.g. in relation to people‚Äôs activities and the home routines.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15862
        },
        {
          "affiliations": [],
          "personId": 9321
        },
        {
          "affiliations": [],
          "personId": 16770
        },
        {
          "affiliations": [],
          "personId": 13701
        },
        {
          "affiliations": [],
          "personId": 19873
        },
        {
          "affiliations": [],
          "personId": 18219
        },
        {
          "affiliations": [],
          "personId": 16924
        },
        {
          "affiliations": [],
          "personId": 19927
        },
        {
          "affiliations": [],
          "personId": 19571
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 7654,
      "typeId": 11437,
      "title": "4th International Workshop on Mental Health and Well-being: Sensing and Intervention",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mental health issues affect a significant portion of the world's population and can result in debilitating and life-threatening outcomes. To address this increasingly pressing healthcare challenge, there is a need to research novel approaches for early detection and prevention. Toward this, ubiquitous systems can play a central role in revealing and tracking clinically relevant behaviors, contexts, and symptoms. Further, such systems can passively detect relapse onset and enable the opportune delivery of effective intervention strategies. However, despite their clear potential, the uptake of ubiquitous technologies into clinical mental healthcare is slow, and a number of challenges still face the overall efficacy of such technology-based solutions. The goal of this workshop is to bring together researchers interested in identifying, articulating, and addressing such issues and opportunities. Following the success of this workshop in the last three years, we aim to continue facilitating the UbiComp community in developing a holistic approach for sensing and intervention in the context of mental health.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17004
        },
        {
          "affiliations": [],
          "personId": 14583
        },
        {
          "affiliations": [],
          "personId": 23811
        },
        {
          "affiliations": [],
          "personId": 14877
        },
        {
          "affiliations": [],
          "personId": 16578
        },
        {
          "affiliations": [],
          "personId": 18535
        },
        {
          "affiliations": [],
          "personId": 24117
        },
        {
          "affiliations": [],
          "personId": 20472
        },
        {
          "affiliations": [],
          "personId": 8902
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5611,
      "typeId": 11437,
      "title": "The PARK Framework for Automated Analysis of Parkinson’s Disease Characteristics",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There are about 900,000 people with Parkinson’s disease (PD), in the United States. Even though there are benefits of early treatment, unfortunately, over 40% of individuals with PD over 65 years old do not see a neurologist. It is often very difficult for these individuals to get to a physician’s office for diagnosis and subsequent monitoring. To address this problem, we present PARK, Parkinson’s Analysis with Remote Kinetic-tasks. PARK instructs and guides users through six motor tasks and one audio task selected from the standardized MDS-UPDRS rating scale and records their performance via webcam. An initial experiment was conducted with 127 participants with PD and 127 age-matched controls, in which 1,778 video recordings were collected. 90.6% of the PD participants agreed that PARK was easy to use, and 93.7% mentioned that they would use the system in the future. We explored objective differences between those with and without PD. A novel motion feature based on the Fast Fourier Transform (FFT) of optical flow in a region of interest was designed to robustly quantify these differences in the collected video recordings. Additionally, facial action units AU4 (brow lowerer) and AU12 (lip corner puller) were shown to be expressed significantly less often in various tasks for participants with PD.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18440
        },
        {
          "affiliations": [],
          "personId": 9274
        },
        {
          "affiliations": [],
          "personId": 17257
        },
        {
          "affiliations": [],
          "personId": 16019
        },
        {
          "affiliations": [],
          "personId": 14723
        },
        {
          "affiliations": [],
          "personId": 23995
        },
        {
          "affiliations": [],
          "personId": 13590
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 6642,
      "typeId": 11437,
      "title": "ProspecFit: In Situ Evaluation of Digital Prospective Memory Training for Older Adults",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Prospective Memory (PM), which involves remembering to perform intended actions, is the primary source of everyday memory lapses. While existing solutions mostly focus on supportive memory aids and reminders, it is also crucial to maintain PM functions and independent living for older adults. We present ProspecFit which digitises implementation intentions, a lab-based memory intervention, making it available on smartphones through iterative design, drawing insights from a focus group and preliminary studies. We evaluated its usability and effectiveness in enhancing PM through user studies that include a 12-day in situ study, and pre- and post-testing with 10 adults (61 to 80 years old). Participants in the digital PM training group were more prompt in performing the in situ PM tasks, compared to the control group without digital training, and reported improvement in their PM compared to before the training. We also show findings from diary entries, reports on forgetful moments and user reactions. Our work provides implications for creating digital memory training tools in HCI.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17571
        },
        {
          "affiliations": [],
          "personId": 22282
        },
        {
          "affiliations": [],
          "personId": 14780
        },
        {
          "affiliations": [],
          "personId": 10105
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 4594,
      "typeId": 11437,
      "title": "Demo: Bedtime Window",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present a design of an always-on system connecting long-distance couples at bedtime, a time and space partners normally share together. The system offers a novel, real-time shared inking space for creative interactivity, a slow photo stream to balance privacy and remote presence. It adapts to the local light level in order to stay in the background, but can be also configured to reflect the remote light level to provide an additional communication channel.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8950
        },
        {
          "affiliations": [],
          "personId": 21096
        },
        {
          "affiliations": [],
          "personId": 19456
        },
        {
          "affiliations": [],
          "personId": 16152
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6643,
      "typeId": 11437,
      "title": "Multi-task Self-Supervised Learning for Human Activity Detection",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Deep learning methods are successfully used in applications pertaining to ubiquitous computing, pervasive intelligence, health, and well-being. Specifically, the area of human activity recognition (HAR) is primarily transformed by the convolutional and recurrent neural networks, thanks to their ability to learn semantic representations directly from raw input. However, in order to extract generalizable features massive amounts of well-curated data are required, which is a notoriously challenging task; hindered by privacy issues and annotation costs. Therefore, unsupervised representation learning (i.e., learning without manually labeling the instances) is of prime importance to leverage the vast amount of unlabeled data produced by smart devices. In this work, we propose a novel self-supervised technique for feature learning from sensory data that does not require access to any form of strongly labeled data, i.e., activity classes. We learn a multi-task temporal convolutional network to recognize transformations applied on an input signal. By exploiting these transformations, we demonstrate that simple auxiliary tasks of the binary classification result in a strong supervisory signal for extracting useful features for the down-stream task. We extensively evaluate the proposed approach on several publicly available datasets for smartphone-based HAR in unsupervised, semi-supervised and transfer learning settings. Our method achieves performance levels superior to or comparable with fully-supervised networks trained directly with activity labels, and it performs significantly better than unsupervised learning through autoencoders. Notably, for the semi-supervised case, the self-supervised features substantially boost the detection rate by attaining a kappa score between 0.7-0.8 with only 10 labeled examples per class. We get similar impressive performance even if the features are transferred from a different data source. Self-supervision drastically reduces the requirement of labeled activity data, effectively narrowing the gap between supervised and unsupervised techniques for learning meaningful representations. While this paper focuses on HAR as the application domain, the proposed approach is general and could be applied to a wide variety of problems in other areas.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20510
        },
        {
          "affiliations": [],
          "personId": 11369
        },
        {
          "affiliations": [],
          "personId": 18532
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 4597,
      "typeId": 11437,
      "title": "Large-scale Automatic Depression Screening Using Meta-data from WiFi Infrastructure",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Depression is a serious public health problem. Current diagnosis techniques rely on physician-administered or patient self-administered interview tools, which are burdensome and suffer from recall bias. Recent studies have proposed new approaches that use sensing data collected on smartphones to serve as ‚Äúhuman sensors‚Äù for automatic depression screening. These approaches, however, require running an app on the phones for continuous data collection. We explore a novel approach that uses data collected from WiFi infrastructure for large-scale automatic depression screening. Specifically, when smartphones connect to a WiFi network, their locations (and hence the locations of the users) can be determined by the access points that they associate with; the location information over time provides important insights into the behavior of the users, which can be used for depression screening. To investigate the feasibility of this approach, we have analyzed two datasets, each collected over several months, involving tens of participants recruited from a university. Our results demonstrate that WiFi meta-data is effective for passive depression screening:  the F1 scores are as high as 0.85 for predicting depression, comparable to those obtained by using sensing data collected directly from smartphones.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14279
        },
        {
          "affiliations": [],
          "personId": 19635
        },
        {
          "affiliations": [],
          "personId": 13272
        },
        {
          "affiliations": [],
          "personId": 13827
        },
        {
          "affiliations": [],
          "personId": 16232
        },
        {
          "affiliations": [],
          "personId": 10460
        },
        {
          "affiliations": [],
          "personId": 23436
        },
        {
          "affiliations": [],
          "personId": 14197
        },
        {
          "affiliations": [],
          "personId": 20128
        },
        {
          "affiliations": [],
          "personId": 22366
        }
      ],
      "sessionIds": [
        2491
      ],
      "eventIds": []
    },
    {
      "id": 5622,
      "typeId": 11437,
      "title": "Cosplay as Inspiration for Wearables Research",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Cosplay, where people dress up with special costumes resembling or inspired by an imaginary character, derived e.g. from game or comic genre, is an interesting area of social clothing design. In our on-going research, we consider cosplay as a source of inspiration for wearable computing. We investigate different aspects related to the identity and expression with cosplay costumes, and seek to identify factors that affect the engagement and user satisfaction in participating in the cosplay. We believe that wearable computing research can learn from cosplay, and take elements that help in designing future wearables.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12409
        },
        {
          "affiliations": [],
          "personId": 9801
        },
        {
          "affiliations": [],
          "personId": 17103
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6647,
      "typeId": 11437,
      "title": "Caloric Expenditure Estimation from Human Kinetic Energy in Wearable Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Accurate and efficient estimation of caloric expenditure during daily activities is desirable in tracking personal activity and health. Kinetic energy harvesting (KEH) has created an opportunity for wearable devices with limited battery power to realize long-term human health monitoring. We postulate to use the data of KEH device as a new source for calorie estimation instead of accelerometer. In this paper, we utilize the output voltage of kinetic energy harvester to classify activity intensity types and then develop activity-specific regression models combined with anthropometric characteristics for caloric expenditure estimation by random forest. To validate our approach, we build a KEH hardware platform and collect the dataset of seven different activities in free-living conditions from ten participants. Experiment results validate that the KEH device can act as a substitute for accelerometer to estimate calorie expenditure.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22516
        },
        {
          "affiliations": [],
          "personId": 14150
        },
        {
          "affiliations": [],
          "personId": 16290
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7672,
      "typeId": 11437,
      "title": "Privacy-preserving Cross-domain Location Recommendation",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Cross-domain recommendation is a typical solution for data sparsity and cold start issue in the field of location recommendation. Specifically, data of an auxiliary domain is leveraged to improve the recommendation of the target domain. There is a typical scenario that two interaction domains (location-based check-in service, for example) combine data to perform the cross-domain location recommendation task. Existing approaches are based on the assumption that the interaction data from the auxiliary domain can be directly shared across domains. However, such an assumption is not reasonable, since in the real world those domains may be operated by different companies. Therefore, directly sharing raw data may violate business privacy policy and increase the risk of privacy leakage since the user-location interaction records are very sensitive. In this paper, we propose a framework named privacy-preserving cross-domain location recommendation which works in two stages. First, for the interaction data from the auxiliary domain, we adopt a differential privacy based protection mechanism to hide the real locations of each user to meet the criterion of differential privacy. Then we share the protected user-location interaction data to the target domain. Second, we develop a new method of Confidence-aware Collective Matrix Factorization (CCMF) to effectively exploit the transferred interaction data. To verify its efficacy, we collect two real-world datasets suitable for the task. Extensive experiments demonstrate that our proposed framework achieves the best performance compared with the state-of-the-art baseline methods. We further demonstrate that our method can alleviate the data sparsity issue significantly while protecting users’ location privacy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11094
        },
        {
          "affiliations": [],
          "personId": 13505
        },
        {
          "affiliations": [],
          "personId": 20531
        },
        {
          "affiliations": [],
          "personId": 9999
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 9082
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 6655,
      "typeId": 11437,
      "title": "An Agile Approach for Human Gesture Detection using Synthetic Radar Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A large and diversified dataset is the cornerstone for the analysis of many real world systems. Data collection, especially involving living beings, is a time and effort consuming approach. Thus, there is a need for synthetic generation of data to be fused with existing data for better system performance. In this paper, we demonstrate a case study of data explosion for radar based human gesture detection. We present a near realistic physical model based simulation framework for obtaining radar signals corresponding to different human gestures. Radar based public datasets have not yet been so easily available because of its high cost and less availability. On the contrary, Kinect based datasets are easily available because of its market dominance and commercialization of devices such as Xbox Kinect. Thus, for the simulation framework, we have used the publicly available dataset for gaming gestures based on Kinect data (obtained from Microsoft Cambridge joint initiative). This Kinect data containing the coordinates of joint positions of human skeletons performing different gaming gestures is then fed to a radar based simulation framework to calculate the radar data. Furthermore, we have tuned different parameters in this simulation framework and exploded it to generate radar micro Doppler signatures in a controlled way, keeping in mind the physical constraints. On this exploded data, machine learning algorithms have been applied to evaluate gesture detection accuracy. The enlarged dataset for four gestures has shown an accuracy of around 94.7 percent for 10 fold cross validation in training phase. A comparison of simulated radar data with respect to experimentally obtained hardware radar data for different gestures has also been reported to show the relevance of the proposed method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15528
        },
        {
          "affiliations": [],
          "personId": 13434
        },
        {
          "affiliations": [],
          "personId": 13748
        },
        {
          "affiliations": [],
          "personId": 20402
        },
        {
          "affiliations": [],
          "personId": 14122
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5638,
      "typeId": 11437,
      "title": "Design and Formative Evaluation of Cognitive Assessment Apps for Wearable Technologies",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Cognitive functioning is a crucial aspect of the individual’s mental health and it affects human’s daily activities. We have developed the Ubiquitous Cognitive Assessment Tool (UbiCAT) including three cognitive assessment apps on the Fitbit smartwatch. In this paper, we present the design and formative evaluation of the UbiCAT apps conducted with five participants who had a background in design and/or human-computer interaction. Moreover, we investigated the adoption of the wearable devices by our participants.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21114
        },
        {
          "affiliations": [],
          "personId": 20989
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4614,
      "typeId": 11437,
      "title": "Scaling Crowdsourcing with MobileWorkforce : A Case Study with Belgian Postal Service",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Traditional urban-scale crowdsourcing approaches suffer from three caveats - lack of complete spatiotemporal coverage, lack of accurate information and lack of sustained engagement of crowd workers. In this paper, we argue that these caveats can be addressed by embedding crowdsourcing tasks into the daily routine of mobile workforces that roam around an urban area. As a use case, we take the bpost who deliver the letters and parcels to the citizens across entire Belgium. We present a study that explores the behavioral attributes of these mobile postal workers both quantitatively (6.3K) and qualitatively (6) to assess the opportunity of leveraging them for crowdsourcing tasks. We report their mobility pattern, workflow, and behavioral traits which collectively inform the design of a purpose-built crowdsourcing solution. In particular, our solution operates on two key techniques - route augmentation, and on-wearable interruptibility management. Together, these mechanisms enhance the spatial coverage, response accuracy and increase workers' engagement with crowdsourcing tasks. We describe these principal components in a wearable smartwatch application supported by a data management infrastructure. Finally, we report a first-of-its-kind real-world trial with ten postal workers for two weeks to assess the quality of road signs at the city center of Antwerp. Our findings suggest that our solution was effective in increasing the spatial coverage (89%), response rate (83.6%) and accuracy (100%) of the crowd-sourcing tasks. Although limited in scale, these and the rest of our findings highlight the way of building an efficient and purposeful crowd-sourcing solution of the future.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16928
        },
        {
          "affiliations": [],
          "personId": 18405
        },
        {
          "affiliations": [],
          "personId": 19481
        },
        {
          "affiliations": [],
          "personId": 23933
        },
        {
          "affiliations": [],
          "personId": 23587
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 4615,
      "typeId": 11437,
      "title": "Balancing caregivers and children interaction to support the development of self-regulation skills using a smartwatch application",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable computing solutions have been used to support children and adults with autism and ADHD previously. The design of these technologies, however, require a deep understanding not only of what is possible from a clinical standpoint but also how the children themselves might understand and orient towards wearable technologies, such as a smartwatch. In this work, we were interested in supporting children and their caregivers in participating in co-design to explore the issue of transitioning from self-regulation to co-regulation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10366
        },
        {
          "affiliations": [],
          "personId": 11236
        },
        {
          "affiliations": [],
          "personId": 8647
        },
        {
          "affiliations": [],
          "personId": 15064
        },
        {
          "affiliations": [],
          "personId": 12693
        },
        {
          "affiliations": [],
          "personId": 15538
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5644,
      "typeId": 11437,
      "title": "Locomotion Recognition Using XGBoost and Neural Network Ensemble",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mode of transport recognition is an important part of understanding the context of a person with a mobile phone being the best device on which to do this since a person typically carries it around for most of the day. In this paper, we present a method to understand the mode of transport (also called locomotion recognition) using data collected from Android mobile phones. The method is part of the submitted solution for “Team Jellyfish” for the Sussex-Huawei Locomotion-Transportation recognition challenge. The goal is to develop a body-position independent classifier that uses data from a number of commonly available sensors on a phone – accelerometer, gyroscope, magnetometer and barometer. The solution is an ensemble of XGBoost and neural network classifiers. The precision and recall using 5-fold cross validation on the validation set is 0.946 and 0.945 respectively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19937
        },
        {
          "affiliations": [],
          "personId": 10744
        },
        {
          "affiliations": [],
          "personId": 14599
        },
        {
          "affiliations": [],
          "personId": 23429
        },
        {
          "affiliations": [],
          "personId": 13630
        },
        {
          "affiliations": [],
          "personId": 14627
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7694,
      "typeId": 11437,
      "title": "User preferences regarding smart assistant for private- and work-related availability ??? design science research",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 12352
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7696,
      "typeId": 11437,
      "title": "Capturing Attentional Problems with Smart Eyewear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 19310
        },
        {
          "affiliations": [],
          "personId": 12081
        },
        {
          "affiliations": [],
          "personId": 14189
        },
        {
          "affiliations": [],
          "personId": 21242
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5649,
      "typeId": 11437,
      "title": "Interrupting Drivers for Interactions: Predicting Opportune Moments for In-vehicle Proactive Auditory-verbal Tasks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Auditory-verbal interaction with in-vehicle information systems has become increasingly popular for driver-safety reasons since they obviate the need for distractive visual-manual operations. This opens up new possibilities of enabling proactive auditory-verbal interaction services where the intelligent agents proactively provide contextualized recommendations and interactive decision-making. However, prior studies warned that such auditory-verbal interactions may consume considerable attentional resources, and thus, they may negatively affect driving performance. This work aims to develop a machine learning model that can find opportune moments for the driver to engage in proactive auditory-verbal tasks by using the vehicle and environment sensor data. Given that there is a lack of definition about what constitutes interruptibility for auditory-verbal tasks, we first define interruptible moments by considering multiple dimensions and then iteratively develop the experimental framework through an extensive literature review and four pilot studies. We integrate our experimental framework into the OsmAnd, an open-source navigation service and perform a real-road field study to collect sensor data and user responses (n = 29). Our machine learning analysis shows that opportune moments for interruption can be conservatively inferred with an accuracy of 0.74. We discuss how our experimental framework and machine learning models can be used to design intelligent auditory-verbal services in practical deployment contexts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10169
        },
        {
          "affiliations": [],
          "personId": 20248
        },
        {
          "affiliations": [],
          "personId": 17438
        },
        {
          "affiliations": [],
          "personId": 9777
        },
        {
          "affiliations": [],
          "personId": 22860
        }
      ],
      "sessionIds": [
        1680
      ],
      "eventIds": []
    },
    {
      "id": 3602,
      "typeId": 11437,
      "title": "GeoLifecycle: User Engagement of Geographical Exploration and Churn Prediction in LBSNs",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As Location-Based Social Networks (LBSNs) have become widely used by users, understanding user engagement and predicting user churn are essential to the maintainability of the services. In this work, we conduct a quantitative analysis to understand user engagement patterns exhibited both offline and online in LBSNs. We employ two large-scale datasets which consist of 1.3 million and 62 million users with 5.3 million reviews and 19 million tips in Yelp and Foursquare, respectively. We discover that users keep traveling to diverse locations where they have not reviewed before, which is in contrast to \"human life\" analogy in real life, an initial exploration followed by exploitation of existing preferences. Interestingly, we find users who eventually leave the community show distinct engagement patterns even with their first ten reviews in various facets, e.g., geographical, venue-specific, linguistic, social aspects. Based on these observations, we construct predictive models to detect potential churners. We then demonstrate the effectiveness of our proposed features in the churn prediction. Our findings of geographical exploration and online interactions of users enhance our understanding of human mobility based on reviews as well as provide important implications for venue recommendations and churn prediction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17379
        },
        {
          "affiliations": [],
          "personId": 17031
        },
        {
          "affiliations": [],
          "personId": 20356
        },
        {
          "affiliations": [],
          "personId": 10891
        },
        {
          "affiliations": [],
          "personId": 11677
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 7699,
      "typeId": 11437,
      "title": "Experiencing Discomfort: Designing for affect from first-person perspective",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 13728
        },
        {
          "affiliations": [],
          "personId": 17308
        },
        {
          "affiliations": [],
          "personId": 16063
        },
        {
          "affiliations": [],
          "personId": 17833
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7700,
      "typeId": 11437,
      "title": "InDexMo: Exploring Finger-Worn RFID Motion Tracking for Activity Recognition on Tagged Objects",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This work explores and evaluates the designs of finger-worn radio-frequency identification (RFID) motion tracking for activity recognition on tagged objects. We propose an index-finger-worn device that consists of a short-range (∼2cm) RFID reader and a pair of two inertial measurement units (IMUs), which are mounted at the locations where an artificial nail and a ring are worn. The short-range RFID reader recognizes the tagged object on finger touch, and then the IMU data are used for activity recognition. Data collected from the user of this device allows for a post hoc analysis, which informs the activity recognition performance in various RFID-IMU and IMU-only configurations on the same task. The results of a ten-participant user study show that when the objects have similar physical form factors, the hybrid RFID motion tracking significantly outperforms the IMU-only tracking, especially in a larger-number set of objects. In our test, three IMU configurations (i.e., NailOnly, RingOnly, and Nail+Ring) achieved comparable action recognition performances, i.e., ≥90% accuracy, with 500 ms recognition time, though the NailOnly RFID+IMU configuration provided the highest wearability. The practical challenges toward a real-world deployment of a finger-worn RFID motion tracking system are also discussed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15724
        },
        {
          "affiliations": [],
          "personId": 16183
        },
        {
          "affiliations": [],
          "personId": 19860
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 6678,
      "typeId": 11437,
      "title": "Prediction of Mood Instability with Passive Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mental health issues are a growing concern worldwide, which are difficult to diagnose. However, for effective care and support, early detection of mood-related concerns is of paramount importance. Typically, survey-based instruments including Ecologically Momentary Assessments (EMA), i.e., active data collections, are the method of choice for assessing mood related health aspects. While effective, these methods require substantial effort and thus both compliance rates as well as quality of responses are often limited.\nWe present a study on predicting mood instabilities, an important class of mental health concerns, using data that are passively sensed from smartphones and automatically assessed using machine learning based analysis techniques. We explore the effectiveness of the proposed method on two large-scale datasets. We demonstrate that not only can passively sensed data be used for reliably predicting mood instabilities, but also that already as little as three weeks of continuous, passive recordings are sufficient.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19961
        },
        {
          "affiliations": [],
          "personId": 19229
        },
        {
          "affiliations": [],
          "personId": 23035
        },
        {
          "affiliations": [],
          "personId": 23810
        },
        {
          "affiliations": [],
          "personId": 21112
        },
        {
          "affiliations": [],
          "personId": 17531
        },
        {
          "affiliations": [],
          "personId": 18818
        }
      ],
      "sessionIds": [
        2491
      ],
      "eventIds": []
    },
    {
      "id": 3612,
      "typeId": 11437,
      "title": "Revisitation in Urban Space vs. Online: A Comparison across POIs, Websites, and Smartphone Apps.∗",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present the first large-scale analysis of POI revisitation patterns, which aims at understanding the periodic behavior in human mobility. We apply the revisitation analysis technique, which has previously been used to understand website revisitation, and smartphone app revisitations. We analyze a 1.5-year-long Foursquare check-in dataset with 266,909 users in 415 cities around the globe, as well as a 1.5-month Tencent localization dataset with 15,000 users in Beijing. Our analysis identifies four major POI revisitation patterns and four user revisitation patterns of distinct characteristics, and demonstrates the role of POI functions and geographic constraints in shaping these patterns. We compare our results to previous analysis on website and app revisitation, and highlight the similarities and differences between physical and cyber revisitation activities. These point to fundamental characteristics of human behavior.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16087
        },
        {
          "affiliations": [],
          "personId": 12871
        },
        {
          "affiliations": [],
          "personId": 8488
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 11200
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 4636,
      "typeId": 11437,
      "title": "Just-in-Time but Not Too Much: Determining Treatment Timing in Mobile Health",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There is a growing scientific interest in the use and development of just-in-time  adaptive interventions in mobile health. These mobile interventions typically involve treatments, such as reminders, activity suggestions and motivational messages, delivered via notifications on a smartphone or a wearable to help users make healthy decisions in the moment. To be effective in influencing health, the combination of the right treatment and right delivery time is likely critical.   A variety of prediction/detection algorithms have been developed with the goal of pinpointing the best delivery times. The best delivery times might be times of greatest risk and/or times at which the user might be most receptive to the treatment notifications. In addition, to avoid over burdening users, there is often a constraint on the number of treatments that should be provided  per time interval (e.g., day or week). Yet there may be many more times at which the user is predicted or detected to be at risk and/or receptive. The goal then is to spread treatment uniformly across all of these times. In this paper, we introduce a method that spreads the treatment uniformly across the delivery times. This method can also be used to provide data for learning whether the treatments are effective at the delivery times.  This work is motivated by our work on two mobile health studies, a smoking cessation study and a physical activity study.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11574
        },
        {
          "affiliations": [],
          "personId": 14112
        },
        {
          "affiliations": [],
          "personId": 23758
        },
        {
          "affiliations": [],
          "personId": 17126
        },
        {
          "affiliations": [],
          "personId": 9581
        },
        {
          "affiliations": [],
          "personId": 23654
        },
        {
          "affiliations": [],
          "personId": 18626
        }
      ],
      "sessionIds": [
        2269
      ],
      "eventIds": []
    },
    {
      "id": 3615,
      "typeId": 11437,
      "title": "ChakraSuit: Experimental Directed Meditation Wearable",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "hakraSuit is an experimental wearable which aids in finding the best location for meditation, enabling one to learn about the interaction between the natural environment, sound, and the body. The prototype continuously listens to ambient sounds, and translates the identified audio frequencies to vibration on several points along the spine which correspond to chakra points. The project consists of a jumpsuit, pocket, and harness with integrated electronics running on a Raspberry Pi W Zero.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9670
        },
        {
          "affiliations": [],
          "personId": 23283
        },
        {
          "affiliations": [],
          "personId": 15943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7712,
      "typeId": 11437,
      "title": "GEVR: An Event Venue Recommendation System for Groups of Mobile Users",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present GEVR, the first Group Event Venue Recommendation system that incorporates mobility via individual location traces and context information into a “social-based” group decision model to provide venue recommendations for groups of mobile users. Our study leverages a real-world dataset collected using the OutWithFriendz mobile app for group event planning, which contains 625 users and over 500 group events. We first develop a novel “social-based” group location prediction model, which adaptively applies different group decision strategies to groups with different social relationship strength to aggregate each group member’s location preference, to predict where groups will meet. Evaluation results show that our prediction model not only outperforms commonly used and state-of-the-art group decision strategies with over 80% accuracy for predicting groups’ final meeting location clusters, but also provides promising qualities in cold-start scenarios. We then integrate our prediction model with the Foursquare Venue Recommendation API to construct an event venue recommendation framework for groups of mobile users. Evaluation results show that GEVR outperforms the comparative models by a significant margin.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20328
        },
        {
          "affiliations": [],
          "personId": 20915
        },
        {
          "affiliations": [],
          "personId": 16528
        },
        {
          "affiliations": [],
          "personId": 12987
        },
        {
          "affiliations": [],
          "personId": 8552
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 6692,
      "typeId": 11437,
      "title": "PRECEPT: Occupancy Presence Prediction Inside A Commercial Building",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing number of low-cost sensing modalities, bulk amount of spatial and temporal data is collected and accumulated from building systems. Substantial information could be extracted about occupant behavior and actions from the data gathered. Understanding the data provides an opportunity to decode movement patterns, circulation-flow i.e. how an occupant tends to move inside the building and extract occupant presence impressions. \\textit{Occupant Presence} can be defined as digital traces of spatial coordinates (x,y) of an occupant at a particular instant that moves within the monitored space and is represented by a chronologically ordered sequence of those position coordinates. This study analyzes the occupant presence inside a building and makes predictions on the next location, i.e., where an occupant possibly could be in the future. This paper introduces a predictive model for occupancy presence prediction using the data collected from an instrumented commercial building spanning for over 30 days - May 2019 to June 2019. The proposed prediction model named PRECEPT - is a variant of Recurrent Neural Network known as Gated Recurrent Unit  (GRU) Network. PRECEPT is capable of learning mobility patterns and predict presence impressions based on the occupant's past spatial coordinates. We evaluate the performance of PRECEPT on a dataset using metrics such as Mean Squared Error (MSE) and Mean Absolute Error (MAE) for each training epoch. The model results in a Root Mean Squared Error (RMSE) value of 4.79 centimeters for a single occupant. We also illustrate how the prediction model can be used for the task of identifying important zones and extract unique space-usage patterns. This could further assist the Building Management System (BMS) authorities to reduce energy wastage and perform efficient HVAC control and intelligent building operations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14901
        },
        {
          "affiliations": [],
          "personId": 14213
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7718,
      "typeId": 11437,
      "title": "Modeling Personality vs. Modeling Personalidad: In-the-wild Mobile Data Analysis in Five Countries Suggests Cultural Impact on Personality Models",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sensor data collected from smartphones provides the possibility to passively infer a user's personality traits. Such models can be used to enable technology personalization, while contributing to our substantive understanding of how human behavior manifests in daily life. A significant challenge in personality modeling involves improving the accuracy of personality inferences, however, research has yet to assess and consider the cultural impact of users' country of residence on model replicability. We collected mobile sensing data and self-reported Big Five traits from 166 participants recruited in five different countries (UK, Spain, Colombia, Peru, and Chile) for 3 weeks. We developed machine learning based personality models using culturally diverse datasets - representing different countries - and we show that such models can achieve state-of-the-art accuracy when tested in new countries, ranging from 63% (Agreeableness) to 71% (Extroversion) of classification accuracy. Our findings also indicate that using country-specific datasets can improve the classification accuracy between 3% and 7% for Extroversion, Agreeableness, and Conscientiousness. We unpack differences in personality models across the five countries, highlight the most predictive data categories (location, noise, unlocks, accelerometer), and provide takeaways to technologists and social scientists interested in passive personality assessment.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21580
        },
        {
          "affiliations": [],
          "personId": 18066
        },
        {
          "affiliations": [],
          "personId": 10359
        },
        {
          "affiliations": [],
          "personId": 18140
        },
        {
          "affiliations": [],
          "personId": 12325
        },
        {
          "affiliations": [],
          "personId": 23179
        }
      ],
      "sessionIds": [
        2158
      ],
      "eventIds": []
    },
    {
      "id": 7719,
      "typeId": 11437,
      "title": "The Impact of Private and Work-Related Smartphone Usage on Interruptibility",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In the last decade, the effects of interruptions through mobile notifications have been extensively researched in the field of human-computer interaction. Breakpoints in tasks and activities, cognitive load, and personality traits have all been shown to correlate with individuals' interruptibility. However, concepts that explain interruptibility in a broader sense are needed to provide a holistic understanding of its characteristics. In this paper, we build upon the theory of social roles to conceptualize and investigate the correlation between individuals' private and work-related smartphone usage and their interruptibility. Through our preliminary study with four participants over 11 weeks, we found that application sequences on smartphones correlate with individuals' private and work roles. We observed that participants engaged in these roles tend to follow specific interruptibility strategies - integrating, combining, or segmenting private and work-related engagements. Understanding these strategies breaks new ground for attention and interruption management systems in ubiquitous computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21386
        },
        {
          "affiliations": [],
          "personId": 18907
        },
        {
          "affiliations": [],
          "personId": 11137
        },
        {
          "affiliations": [],
          "personId": 23699
        },
        {
          "affiliations": [],
          "personId": 9337
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3627,
      "typeId": 11437,
      "title": "Temporal Factors of Listening to Music on Stress Reduction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Listening to music has been studied as a method for combating\nthe rapidly increasing stress levels of adolescents. Previous studies\nyielded inconsistent results and neglected specific factors including\nthe time relative to the stressor and the duration of time in which\nparticipants listened to music. We conducted a survey and lab\nexperiment to investigate the impact of these factors on the stressreducing\neffect of music. The survey contained questions regarding\nmusic preference, stress, and use of music for stress reduction. In the\nlab experiment, the math task of the Trier Social Stress Test (TSST)\nwas used to simulate stress in participants. Three experimental\ngroups listened to music for either five minutes before the stressor,\nfive minutes after the stressor, or ten minutes after the stressor; with\nthe control group not listening to any music. Heart rate variability\nwas continuously monitored with a wearable device, Empatica, and\nused to derive stress levels. The survey received 251 responses and\n42 students participated the lab experiment. The results showed\nthat listening to music before the stressor resulted in significantly\nlower stress levels than listening to music after the stressor (p <\n0.01). This finding, contrary to our survey results, revealed that the\n“preventive” effect of listening to music prior to the stressor was\nmore effective than the “remedial” effect that followed after the\nstressor.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13425
        },
        {
          "affiliations": [],
          "personId": 20730
        },
        {
          "affiliations": [],
          "personId": 8902
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4655,
      "typeId": 11437,
      "title": "Transfer Learning Across Human Activities Using a Cascade Neural Network Architecture",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Cascade Learning (CL) [20] is a new adaptive approach to train deep neural networks. It is particularly suited to transfer learning, as learning is achieved in a layerwise\nfashion, enabling the transfer of selected layers to optimize the quality of transferred features. In the domain of Human Activity Recognition (HAR), where the consideration of resource consumption is critical, CL is of particular interest as it has demonstrated the ability to achieve significant reductions in computational and memory costs with negligible performance loss. In this paper, we evaluate the use of CL and compare it to\nend to end (E2E) learning in various transfer learning experiments, all applied to HAR. We consider transfer learning across objectives, for example opening the door features transferred to opening the dishwasher. We additionally consider transfer across sensor locations on the body, as well as across datasets. Over all of our experiments, we find that CL achieves state of the art performance for transfer learning in comparison to previously published work, improving F1 scores by over 15%. In comparison to E2E learning, CL performs similarly considering F1 scores, with the additional advantage\nof requiring fewer parameters. Finally, the overall results considering HAR classification performance and memory requirements demonstrate that CL is a good approach for transfer learning.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21200
        },
        {
          "affiliations": [],
          "personId": 8670
        },
        {
          "affiliations": [],
          "personId": 9776
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 7733,
      "typeId": 11437,
      "title": "Gesture recognition based on ConvLSTM-Attention implementation of small data sEMG signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a system based on Convolutional Long Short-Term Memory (ConvLSTM)-Attention Mechanism (AM) to preserve spatial features and time characteristics for surface electromyography (sEMG) signals. We assume that this method can perform more robustly under training with small data than a Convolutional Neural Network (CNN). To test the performance of this method, we measured sEMG signals for eight hand gestures. Whereas the pure CNN based model and ConvLSTM (not have AM) model, had accuracies of 88.2 and 89.7%, respectively, our proposed ConvLSTM-AM method achieved 92.8% accuracy. Thus, proposed method has a better recognition rate than the CNN, which only uses spatial features. Through the results of the experiment, we believe that the proposed method can effectively improve the robustness of Deep Learning to small sample sEMG signals.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19968
        },
        {
          "affiliations": [],
          "personId": 24346
        },
        {
          "affiliations": [],
          "personId": 22632
        },
        {
          "affiliations": [],
          "personId": 10784
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5688,
      "typeId": 11437,
      "title": "Demo: Construction of a Device that Automatically Generates Various Touch Interactions",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper proposes a device that automatically generates various touch interactions including multi-touch to realize touch interactions at high speed, continuously, and hands free.\nThe proposed device does not require additional software installations to the touch panel, and it can be expected to improve efficiency and realize automation of work using the touch panel.\nThe proposed device consists of an electrode sheet printed with conductive ink and a voltage control circuit, and generates touch interactions by changing the capacitance of the touch panel temporally and spatially.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20483
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5694,
      "typeId": 11437,
      "title": "Demo: A Contactless Morse Code Text Input System Using COTS WiFi Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "For the patients with speech and motion impairments, there is an indispensable need to facilitate their communication with other people, using approaches such as eyeball tracking. However, these systems are usually complex and expensive. In this demo, we propose a WiFi-based contactless text input system, called WiMorse. The system allows these patients to communicate with other people by using WiFi signals to track single-finger movements and encoding them as Morse code to input text. However, we note that a small change in the target’s location would lead to a significant change in the received WiFi signal pattern, making it impossible to recognize the finger gestures. To tackle this problem, we propose a signal transformation mechanism to obtain a consistent and stable signal pattern at various locations. By deploying only a pair of COTS WiFi devices, WiMorse can achieve real time recognition of finger generated Morse code with high accuracy, and is robust against input position, environment changes, and user diversity.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20721
        },
        {
          "affiliations": [],
          "personId": 13126
        },
        {
          "affiliations": [],
          "personId": 11956
        },
        {
          "affiliations": [],
          "personId": 8724
        },
        {
          "affiliations": [],
          "personId": 20239
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5695,
      "typeId": 11437,
      "title": "Deep Learning Models for Population Flow Generation from Aggregated Mobility Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Population flow data (traffic sets of crowds from one regions to another) is of great value in a wide range of fields from urban traffic resource allocation to public security protection. Since the data of the individual-level mobility requires privacy protection, it's hard to collect detailed population flow data. There have been published works on generating population flow from aggregated data. But they have the limitations of considering the flow between neighbors only or modeling the mapping from aggregated population data to flow by a simple physical model without taking regionally diversity into account. However, long-range dependencies and regionally diversity is very important. And since population flow contains more information than that in aggregated population variation, generating the detailed former from the aggregated latter is quite difficult. In this paper, we proposed an end-to-end structure deep learning based model to generate population flow from aggregated historical population variation data. We use real-world datasets to compare the performance of our model with several baselines, which shows the superiority of our model. This proves the potential of using deep learning in population flow generation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21739
        },
        {
          "affiliations": [],
          "personId": 20063
        },
        {
          "affiliations": [],
          "personId": 24147
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5699,
      "typeId": 11437,
      "title": "EduSense: Practical Classroom Sensing at Scale",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Providing university teachers with high-quality opportunities for professional development cannot happen without data about the classroom environment. Currently, the most effective mechanism is for an expert to observe one or more lectures and provide personalized formative feedback to the instructor. Of course, this is expensive and unscalable, and perhaps most critically, precludes a continuous learning feedback loop. In this paper, we present the culmination of two years of research and development on ClassInsight, a comprehensive sensing system that produces a plethora of theoretically-motivated visual and audio features correlated with effective instruction, which could feed professional development tools in much the same way as a Fitbit sensor reports step count to an end user app. Although previous systems have demonstrated some of our features in isolation, ClassInsight is the first to unify them into a cohesive, real-time, in-the-wild evaluated, and practically-deployable system. Our two studies quantify where contemporary machine learning techniques are robust, and where they fall short, illuminating where future work remains to bring the vision of automated classroom analytics to reality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11785
        },
        {
          "affiliations": [],
          "personId": 21237
        },
        {
          "affiliations": [],
          "personId": 22582
        },
        {
          "affiliations": [],
          "personId": 12927
        },
        {
          "affiliations": [],
          "personId": 9971
        },
        {
          "affiliations": [],
          "personId": 23065
        },
        {
          "affiliations": [],
          "personId": 23922
        },
        {
          "affiliations": [],
          "personId": 13253
        },
        {
          "affiliations": [],
          "personId": 13817
        },
        {
          "affiliations": [],
          "personId": 14005
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 4678,
      "typeId": 11437,
      "title": "Using Sensing Technologies, Self-reported Information, and Interpersonal Observations to Promote Health and Well-being in the Workplace",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper outlines a pilot study aimed at identifying salient themes in the awareness of health and well-being and the effects that this can have on one's home life and work behavior. We also sought to identify the factors that lead to lapses in one's awareness of health and well-being issues. Interviews were conducted with individuals who are known to work for extended durations (taxi / Uber drivers). A number of proven methods were used for the collection of pertinent data (i.e., sensing technologies, self-reported information, and interpersonal observations). Finally, design guidelines are presented for the development of technology-based solutions aimed at raising awareness of health and well-being among workers in high-stress occupations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14131
        },
        {
          "affiliations": [],
          "personId": 13667
        },
        {
          "affiliations": [],
          "personId": 8321
        },
        {
          "affiliations": [],
          "personId": 23639
        },
        {
          "affiliations": [],
          "personId": 10768
        },
        {
          "affiliations": [],
          "personId": 18477
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6726,
      "typeId": 11437,
      "title": "The Practicability of Predicting the Number of Bus Passengers by Monitoring Wi-Fi Signal from Mobile Devices with the Polynomial Regression",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Traveling by public transportation is a daily activity for many people, so the number of passenger can provide useful congestion information to passengers. In this paper, we describe a practicability study of using the polynomial regression method for predicting the number of waiting passengers at the bus stop by passively monitoring the activity Wi-Fi signal from mobile devices. We also show our experimental results by comparing the predicted number of passengers to the actually observed number of passengers at the bus stop.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18036
        },
        {
          "affiliations": [],
          "personId": 18068
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5702,
      "typeId": 11437,
      "title": "Room-Wide Wireless Charging and Load-Modulation Communication via Quasistatic Cavity Resonance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The rise of the Internet of Things (IoT) has led to a significant increase in the number of connected devices that stream data in our homes, offices and industrial spaces. However, as the number of these devices increases, the costs of actively maintaining and replacing batteries becomes prohibitive at scale. Recent work on Quasistatic Cavity Resonance (QSCR), offers the possibility of seamless wireless power transfer (WPT) to receivers placed anywhere inside large indoor spaces. This work aims to solve two unexplored and critical missing pieces needed to realize this vision of ubiquitous WPT. First, we demonstrate a full end-to-end QSCR-based WPT system that is capable of simultaneously charging multiple custom designed nodes nearly anywhere in the 4.9 m*4.9 m*2.3 m test room. Second, this work utilizes the WPT mechanism as a communication channel, where nodes communicate with a centralized reader and to each other via load modulation. Through analysis and experiments, the proposed system shows that 10 receiver nodes can be safely and efficiently wirelessly charged and the end node to end node communication rate can achieve from 1 kbps without occurring any errors, up to 5 kbps with 6% BER while the end node to central unit can achieve 10 kbps without occurring any errors.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14738
        },
        {
          "affiliations": [],
          "personId": 15426
        },
        {
          "affiliations": [],
          "personId": 12800
        },
        {
          "affiliations": [],
          "personId": 21865
        },
        {
          "affiliations": [],
          "personId": 16766
        }
      ],
      "sessionIds": [
        1861
      ],
      "eventIds": []
    },
    {
      "id": 3656,
      "typeId": 11437,
      "title": "Integrated System of Monitoring and Intervention for In-home Healthcare and Treatment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Internet of Things (IoT) and artificial intelligence (AI) technologies transform our lives into a more convenient and advanced ones.\nThe revolutionary transformation arise in healthcare and medical fields.\nIoT devices make it feasible to examine patient status in daily lives. \nClinicians say that continuous examination of patient status out of the hospital should be valuable to manage target disorder or disease.\nMoreover, engineers expect that AI techniques could find novel treatments for the disorder or disease based on huge amount of daily patient data.\nWe have investigated such technology-supported methodologies with an urologist in Severance Hospital for better treatments to patients who have nocturnal enuresis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9063
        },
        {
          "affiliations": [],
          "personId": 9983
        },
        {
          "affiliations": [],
          "personId": 19308
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4684,
      "typeId": 11437,
      "title": "Application of IndRNN for Human Activity Recognition - The Sussex-Huawei Locomotion-Transportation Challenge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a human activity recognition method using Independently Recurrent Neural Network (IndRNN) on the spectrum for the Sussex-Huawei Locomotion-Transportation recognition challenge 2019. The proposed method takes advantage of FFT and IndRNN to obtain short-time and long-time features, respectively. To be specific, since the signal obtained by Smartphone-sensors is strongly periodic, it is first processed into spectrum by FFT in a one-second sliding window to obtain short-time features. Then, the spectrum of 21 overlapped window extracted from 5-second data is processed by IndRNN in order to explore the correlation of FFT spectrums of all windows to obtain long-time features for the final action classification. To obtain a phone-position independent model, the training data of three phone locations (bag, hips, torso) is used to train the IndRNN model, which is further fine-tuned with half of the validation data in hand position (a relatively small amount of data which can be obtained in practical applications). The proposed method achieved 82.6% accuracy of validation in hand position, which have been submitted to SHL recognition challenge as “UESTC_IndRNN”.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22265
        },
        {
          "affiliations": [],
          "personId": 16443
        },
        {
          "affiliations": [],
          "personId": 10770
        },
        {
          "affiliations": [],
          "personId": 24106
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3665,
      "typeId": 11437,
      "title": "Neural Caption Generation over Figures",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Figures are human-friendly but difficult for computers to process automatically. In this work, we investigate the problem of figure captioning. The goal is to automatically generate a natural language description of a given figure. We create a new dataset for figure captioning, FigCAP. To achieve accurate generation of labels in figures, we propose the Label Maps Attention Model. Extensive experiments show that our method outperforms the baselines. A successful solution to this task allows figure content to be accessible to those with visual impairment by providing input to a text-to-speech system; and enables automatic parsing of vast repositories of documents where figures are pervasive.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24331
        },
        {
          "affiliations": [],
          "personId": 12736
        },
        {
          "affiliations": [],
          "personId": 12508
        },
        {
          "affiliations": [],
          "personId": 8591
        },
        {
          "affiliations": [],
          "personId": 17459
        },
        {
          "affiliations": [],
          "personId": 12585
        },
        {
          "affiliations": [],
          "personId": 20494
        },
        {
          "affiliations": [],
          "personId": 10233
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7761,
      "typeId": 11437,
      "title": "Cross-Location Transfer Learning for The Sussex-Huawei Locomotion Recognition Challenge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The Sussex-Huawei Locomotion Challenge 2019 was an open competition in activity recognition where the participants were tasked with recognizing eight different modes of locomotion and transportation. The main difficulty of the challenge is that the training data was recorded with a smartphone that was placed in a different body location than the test data. Only a small validation set with all locations was provided to enable transfer learning. This paper describes our (team JSI First) approach, in which we first derived additional sensor streams from the existing ones and on them calculated a large body of features. We then used cross-location transfer learning via specialized feature selection, and performed two-step classification. Finally, we used Hidden Markov Models to alter the predictions in order to take their temporal dependencies into account. Internal tests using this methodology yielded an accuracy of 83%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20317
        },
        {
          "affiliations": [],
          "personId": 24031
        },
        {
          "affiliations": [],
          "personId": 22700
        },
        {
          "affiliations": [],
          "personId": 22140
        },
        {
          "affiliations": [],
          "personId": 13286
        },
        {
          "affiliations": [],
          "personId": 18954
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6743,
      "typeId": 11437,
      "title": "Electrooculography Dataset for Reading Detection in the Wild",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Because of the diversity of document layouts and reading styles, detecting reading activities in real life is a challenging task compared to the detection in the laboratory setting. For contributing to the implementation of robust reading detection algorithms, we introduce a dataset which contains 220 hours of sensor signals from JINS MEME electrooculography glasses and corresponding ground truth activity labels. As a baseline study, we propose a statistical feature based reading detection approach and evaluate it on the dataset.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17406
        },
        {
          "affiliations": [],
          "personId": 19399
        },
        {
          "affiliations": [],
          "personId": 16028
        },
        {
          "affiliations": [],
          "personId": 14504
        },
        {
          "affiliations": [],
          "personId": 18496
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5719,
      "typeId": 11437,
      "title": "Sampling Rate Dependency in Pedestrian Walking Speed Estimation using DualCNN-LSTM",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There are many inertial sensor based indoor localization methods for smartphone, for example, SINS and PDR. However, most of the MEMS sensors of smartphones are not precise enough for these methods. We proposed end-to-end walking speed estimation method using deep learning to perform robust walking speed estimation with a low-precision sensor. Currently, we use the input data with a fixed format of 200 samples at 100 Hz. However, the sampling rate and sequence length should be changed appropriately depending on the required accuracy and terminal performance. They are critical factors when using our method for a long time on a terminal because continuous processing of a large amount of data leads to shorter battery life. In this paper, we evaluate the accuracy of the estimated speed by our method when changing the sampling rate and sequence length. As a result, using 5 patterns of combinations, the estimation accuracy hardly changed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13635
        },
        {
          "affiliations": [],
          "personId": 19042
        },
        {
          "affiliations": [],
          "personId": 13115
        },
        {
          "affiliations": [],
          "personId": 20461
        },
        {
          "affiliations": [],
          "personId": 11360
        },
        {
          "affiliations": [],
          "personId": 19295
        },
        {
          "affiliations": [],
          "personId": 16907
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5721,
      "typeId": 11437,
      "title": "How to get in Touch with the Passenger: Context-Aware Choices of Output Modality in Smart Public Transport",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Public transport plays an essential role in sustainable urban mobility. Its digitization and the dissemination of interactive devices in the public and in public transport specifically pave the way for smart mobility systems. Urban mobility is characterized by rapid context changes and a very personalized and situational information need of users. Smart mobility systems therefore support users in their mobility in intelligent ways and bring together ubiquitous and mobile computing, the Internet of Things as well as context-awareness. \nIn this paper, we focus on the delivery of mobility information in public transport and present a model and an adaptation scheme for context-aware choices of output modality and device. Our approach enables a smart mobility system to choose output modality and device based on the user's situation and their preferences as part of a context-aware application design. The model and adaptation process are part of our ongoing work in the field of context-aware smart mobility applications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14773
        },
        {
          "affiliations": [],
          "personId": 17353
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4697,
      "typeId": 11437,
      "title": "CML-IOT 2019: The First Workshop on Continual and Multimodal Learning for Internet of Things",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Internet of Things (IoT) provides streaming, large-amount, and multimodal sensing data over time. The statistical properties of these data are always characterized very differently by time and sensing modalities, which are hardly captured by conventional learning methods. Continual and multimodal learning allows integrating, adapting, and generalizing the knowledge learned from previous experiential data collected with heterogeneity to new situations. Therefore, continual and multimodal learning is an important step to improve the estimation, utilization, and security for the real-world data from IoT devices.\nA few major challenges to combine continual learning and multimodal learning with real-world data include 1) how to accurately match, fuse, and transfer knowledge between the multimodal data from the fast-changing dynamic physical environment, 2) how to learn accurately despite the missing, imbalanced or noisy data for continual learning under multimodal sensing scenarios, 3) how to effectively combine information collected in different sensing modalities to improve the understanding of CPS while retaining privacy and security, and 4) how to develop usable systems handling high volume streaming multimodal data on mobile devices.\n\nWe organize this workshop to bring people working on different disciplines together to tackle these challenges in this topic. This workshop aims to explore the intersection and combination of continual machine learning and multimodal modeling with applications in the Internet of Things. The workshop welcomes works addressing these issues in different applications/domains as well as algorithmic and systematic approaches to leverage continual learning on multimodal data. We further seek to develop a community that systematically handles the streaming multimodal data widely available in real-world ubiquitous computing systems. Preliminary and on-going work is welcomed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12585
        },
        {
          "affiliations": [],
          "personId": 12602
        },
        {
          "affiliations": [],
          "personId": 8401
        },
        {
          "affiliations": [],
          "personId": 21554
        },
        {
          "affiliations": [],
          "personId": 19364
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7770,
      "typeId": 11437,
      "title": "LungTrack: Towards Contactless and Zero Dead-Zone Respiration Monitoring with Commodity RFIDs",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Respiration rate sensing plays a critical role in elderly care and patient monitoring. The latest research explores the possibility of employing Wi-Fi signals for respiration sensing without attaching a device to the target. A critical issue with these solutions is that good monitoring performance can only be achieved at some locations, while at other “dead locations”, the performance is poor. In addition, due to the contactless nature, it is challenging to monitor multiple targets simultaneously as the reflected signals are mixed together. In this work, we present our system named LungTrack hosted on commodity RFID devices for respiration monitoring. Our system retrieves the subtle signal fluctuations at the receiver caused by chest displacement to monitor the respiration process, without a need of attaching any device to the target. It addresses the dead-zone issue, and is able to simultaneously monitor multiple targets with one RFID reader. Comprehensive experiments demonstrate that LungTrack can achieve a respiration monitoring accuracy of higher than 98% at all locations (within1st−5th Fresnel zones) for a single target using just one RFID reader and 5 tags. For challenging scenario with two close-by targets separated by only 10cm, LungTrack is still able to achieve higher than 93% accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24361
        },
        {
          "affiliations": [],
          "personId": 12597
        },
        {
          "affiliations": [],
          "personId": 12337
        },
        {
          "affiliations": [],
          "personId": 13297
        },
        {
          "affiliations": [],
          "personId": 17736
        },
        {
          "affiliations": [],
          "personId": 9626
        },
        {
          "affiliations": [],
          "personId": 15645
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 7774,
      "typeId": 11437,
      "title": "The Hitchhiker's Guide to the Eyewear Applications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 18223
        },
        {
          "affiliations": [],
          "personId": 8400
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5730,
      "typeId": 11437,
      "title": "Addressing Grand Challenges in Healthcare through Smart Clothing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 21680
        },
        {
          "affiliations": [],
          "personId": 11842
        },
        {
          "affiliations": [],
          "personId": 8993
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6754,
      "typeId": 11437,
      "title": "Ballroom Dance Step Type Recognition by Random Forest using Video and Wearable Sensor",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The paper presents a hybrid ballroom dance step type recognition method using video and wearable sensors. Learning ballroom dance is hard for less experienced dancers as it has many complex types of steps. Thus, our purpose is to recognize the step types to support step learning. While the major way to recognize dance is to use video, we cannot simply adopt it for ballroom dance as the dancers' images overlap each other. To solve the problem, we propose a hybrid recognition method combining video and wearable sensors for enhancing its accuracy and robustness. We collect seven dancers' video and wearable sensors data including acceleration, angular velocity, and body parts location change. After that, we pre-process them and extract features to recognize the step types. By adopting Random Forest for recognition, we confirmed that our approach achieved f1-score 0.760 for 13 step types recognition. Finally, we will open our dataset to HASCA community for further research opportunities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14053
        },
        {
          "affiliations": [],
          "personId": 20461
        },
        {
          "affiliations": [],
          "personId": 11360
        },
        {
          "affiliations": [],
          "personId": 19295
        },
        {
          "affiliations": [],
          "personId": 16907
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4706,
      "typeId": 11437,
      "title": "Algorithmic Fashion Aesthetics: Mandelbrot",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The lack of real innovation is one of the problems the fashion system suffers from. But there is so much interesting mathematics available now, with such deep meanings, that it is a shame not to exploit it, or at least seriously try. We are interested in the aesthetic reasons, finding new forms and exploiting new types of algorithmic approaches, mixed with innovative mathematics and digital production tools. \nIn this paper we explain how we modified a mathematical principle so that it can be applied in fashion design. This principle is the most famous fractal: Mandelbrot. We present 2 garments based on modified Mandelbrot principles. The lemniscates lines are used as the stitching seams to create interesting volumes. The garments are realized via an alternating process between sublimation printing, traditional draping, first-person on-body fitting, pinning, stitching, hand cutting, digital pattern making and laser cutting. \nVia this work we hope to inspire fashion designers to apply more interesting and innovative methods to develop their fashion lines in innovative manners. This is an interesting look at how data can be used to develop aesthetics and forms for used in fashion design. The connection to wearable technology – as in electronics integrated in garments – is not there, but makes a case for it being an example of wearable algorithms. In future this might enable advanced production processes for wearable technology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11548
        },
        {
          "affiliations": [],
          "personId": 23562
        },
        {
          "affiliations": [],
          "personId": 13914
        },
        {
          "affiliations": [],
          "personId": 11433
        },
        {
          "affiliations": [],
          "personId": 9781
        },
        {
          "affiliations": [],
          "personId": 19811
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7787,
      "typeId": 11437,
      "title": "mLung++: Automated Characterization of Abnormal Lung Sounds in Pulmonary Patients using Multimodal Mobile Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The design of computational methods for detection of abnormal lung sounds (e.g., wheeze) associated with the advent of a pulmonary attack (e.g., asthma) and subsequent characterization of the `severity' or progressive exacerbation in pulmonary patients is a relevant problem in ubiquitous computing. While a few recent works have been done on on-body sensor and smartphone sensor based lung activity detection, designing a comprehensive architecture for the detection and characterization of abnormal lung sounds (e.g., wheeze) is an open issue. In this paper, we present mLung++, which is a comprehensive pulmonary care system for respiration cycle based detection and subsequent characterization of wheezing in chronic pulmonary patients using audio and inertial sensors embedded on a smartphone. \nFor the design, training, and evaluation, we use audio and Inertial Measurement Unit (IMU) data collected by smartphone and watch from 131 human subjects (91 pulmonary patients, 40 healthy control). We show empirical evidence that the performance of mLung++ for characterizing abnormal lung sounds (accuracy 93.4% and f1_score 77.94%) is comparable with that of high-quality on-body sensor based characterization, which is usually done in a hospital or clinical setup.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12471
        },
        {
          "affiliations": [],
          "personId": 23984
        },
        {
          "affiliations": [],
          "personId": 23054
        },
        {
          "affiliations": [],
          "personId": 13133
        },
        {
          "affiliations": [],
          "personId": 23781
        },
        {
          "affiliations": [],
          "personId": 23150
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7795,
      "typeId": 11437,
      "title": "Ensemble-based Domain Adaptation for Transport Mode Recognition with Mobile Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present our submission (team S304) to the Sussex-Huawei Locomotion-Transportation (SHL) recognition challenge 2019. The goal is to recognize 8 modes of transport and locomotion from 5 second frames of inertial sensor data of a smartphone carried in the hand, while most of the labelled data provided for classifier training consists of data from three other smartphone placements: hips, torso and bag. Only a small dataset from a smartphone carried in the hand was provided. Model training is complicated by the fact that the data distribution differs between the phone positions. To optimize classification performance for data from the Hand phone, we employ an ensemble of Multilayer Perceptrons, each trained with data from a different particular smartphone placement, including the small dataset of the Hand phone. We propose an iterative re-weighting scheme for combining the classifiers that takes their agreement with the specialized Hand classifier into account. \nThe proposed method achieves 74% average per-class Recall, significantly improving the performance achieved when training with mixed data from all phone placements (59%) and training with data from the Hand phone only (66%). The ensemble-based method also outperforms domain adaptation by Feature Augmentation, which achieves 70% average Recall.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11285
        },
        {
          "affiliations": [],
          "personId": 16461
        },
        {
          "affiliations": [],
          "personId": 22949
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4724,
      "typeId": 11437,
      "title": "Face Recognition Assistant for People with Visual Impairments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Although there are many face recognition systems to help individuals with visual impairments (VIPs) recognize other people, almost all require a database with the pictures and names of the people who should be tracked. These solutions would not be able to help VIPs recognize people they might not know well. In this work, we investigate the requirements and challenges that must be addressed in the design of a face recognition system for helping VIPs recognize their acquaintances. We first conducted a formative study with eight visually impaired people. Using insights learned from the formative study, we developed a research prototype that runs on a mobile phone worn around the user’s neck. The developed prototype is a wearable face recognition system that opportunistically captures and stores undistorted face images and contextual information about the user’s interaction with each person to a database, without the user intervention, as she interacts with new people. We then used this prototype application as a technology probe—asking VIP participants to use the device in a realistic scenario in which they meet and re-encounter several new people. We analyze and report feedback from VIPs about the design and use of such a service.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11664
        },
        {
          "affiliations": [],
          "personId": 23277
        },
        {
          "affiliations": [],
          "personId": 20868
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 4725,
      "typeId": 11437,
      "title": "A Co-Design Toolkit for Wearable E-textiles",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Individuals with mobility disabilities face physical and social barriers due to a lack of accessible clothing. E-textiles garments and smart clothing could help make garments more accessible by incorporating assistive technologies directly into clothing, but there are limited methods for co-designing prototypes so that users can be involved in the design process. This is due to the specialized knowledge needed for designing smart clothing (garment construction, e-textiles, computing). To help with these issues I am developing a co-design toolkit for wearable e-textiles called Wearable Bits. Wearable Bits expands upon the swatchbook tradition in e-textiles by making swatches that can connect to form any wearable garment. This document covers the proposed studies I will be doing to evaluate the co-design toolkit and the expected contributions of this thesis project.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9834
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6773,
      "typeId": 11437,
      "title": "Smartphone Interaction and Survey Data as Predictors of Snapchat Usage",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Snapchat is a highly popular smartphone app that allows personalised multimedia communication for spontaneous experiences, where the shared content disappears after a short period of time. In this paper, we examine the predictors of Snapchat usage based on a range of data collected through surveys and from interaction with the handset, using a cohort of 64 recruited participants. The results show that age, Smartphone Addiction, happiness and the use of the popular chatting apps WhatsApp and Facebook Messenger are significant predictors for Snapchat usage. We discuss the implications of these findings against the related literature, and also against the design of the app itself.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18024
        },
        {
          "affiliations": [],
          "personId": 12709
        },
        {
          "affiliations": [],
          "personId": 11752
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4727,
      "typeId": 11437,
      "title": "mobEYEle: An Embedded Eye Tracking Platform for Industrial Assistance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 9822
        },
        {
          "affiliations": [],
          "personId": 18704
        },
        {
          "affiliations": [],
          "personId": 18236
        },
        {
          "affiliations": [],
          "personId": 22456
        },
        {
          "affiliations": [],
          "personId": 18778
        },
        {
          "affiliations": [],
          "personId": 23486
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5751,
      "typeId": 11437,
      "title": "Activity Prediction for Mapping Contextual-Temporal Dynamics",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Existing activity recognition technologies empower the smart home for perceiving the ambient environment. Efficient activity prediction, based on activity recognition, can enable the smart home to provide timely personalized services. However, predicting the next activity and its precise occurrence period are challenging due to the complexity of modelling human behaviour. In this work, we aim to understand whether the temporal information integrated into the deep learning networks can improve the prediction accuracy in both predicting the next activity and its timing. We develop two \\textit{Long Short-Term Memory} (LSTM) models, both with \\textit{deep contextualized word representation} on sensor labels, one with temporal information and one without. Our results highlight that if temporal information is used appropriately, the model with timestamp can outperform the model without this information. While modelling human activity prediction, comprehending the contextual-temporal dynamics is highly important.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15436
        },
        {
          "affiliations": [],
          "personId": 13203
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5753,
      "typeId": 11437,
      "title": "Telesuit: Design and Implementation of An Immersive User-Centric Telepresence Control Suit",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Telepresence takes place when a user is afforded with the experience of being in a remote environment or a virtual world, through the use of immersive technologies. The development of a humanoid robot and a control apparatus that correlates the operator’s movements, and provides sufficient sensory feedback, encompass efforts to cre-ate such immersive technologies. This paper considers the control mechanisms that afford telepresence, the requirements for continu-ous or extended telepresence control, and the health implications of engaging in complex time-constrained tasks. We present Telesuit -a full-body telepresence control system used to operate a humanoid telepresence robot. The suit is part of a broader system that takes into consideration the physical constraints of controlling a dexter-ous bimanual robotic torso, and the need for modular hardware and software that allow for high-fidelity immserviness. It incorpo-rates a health-monitoring system that allows the robotic platform to leverage information such as respiratory effort, galvanic skin response, and heart rate, to adjust the telepresence experience and apply control modalities that automate certain tasks. Furthermore, the design of the Telesuit’s garment considers both functionality and aesthetics and sets preface for future implementations of wear-able telepresence control systems that integrate into the Internet of Actions (IoA) - allowing operators to interchangeably transition between the physical world and mixed-reality or virtual worlds.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22810
        },
        {
          "affiliations": [],
          "personId": 15279
        },
        {
          "affiliations": [],
          "personId": 23079
        },
        {
          "affiliations": [],
          "personId": 17705
        },
        {
          "affiliations": [],
          "personId": 10392
        },
        {
          "affiliations": [],
          "personId": 19722
        },
        {
          "affiliations": [],
          "personId": 21814
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3707,
      "typeId": 11437,
      "title": "Remote monitoring of stroke patients’ rehabilitation using wearable accelerometers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we outline an automated system for continuous assessment of patient recovery levels generically in movement-related disorders using wrist-worn accelerometers, with visualisation of aspects of recovery trends. We demonstrate our model on assessment of recoveries from hemiparetic stroke.\n\n    A clinically-assessed functional recovery score from hemiparetic stroke is periodically collected alongside daily-living wrist-based accelerometer data. After accelerometer preprocessing steps, inferences on individual sliding windows are performed, followed by an overall assessment over a period of time (e.g., past three days). Using isotonic regression, excluding less relevant datapoints, and model averaging over hyperparameters further improve predictions.\n    \n    Through time series decomposition methods of the sliding window inferences, we visualise informative aspects of recovery patterns: overall recovery trend, diurnal exertion patterns and times of outliers in capabilities exhibited.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22793
        },
        {
          "affiliations": [],
          "personId": 24405
        },
        {
          "affiliations": [],
          "personId": 14330
        },
        {
          "affiliations": [],
          "personId": 9638
        },
        {
          "affiliations": [],
          "personId": 22103
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 2688,
      "typeId": 11437,
      "title": "UPA '19: 4th International Workshop on Ubiquitous Personal Assistance",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Advancements in ubiquitous technologies and artificial intelligence have paved the way for the recent rise of digital personal assistants in everyday life. The Fourth International Workshop on Ubiquitous Personal Assistance (UPA'19) aims to continue discussing the latest research developments and outcomes on digital personal assistants. UPA'19 builds on the success of our three previous workshops, organized in conjunction with UbiComp'16-18. We welcome contributions focusing on the advancements toward digital assistants that provide a high level of personalization, through proactive and effective support of users' activities, in an unobtrusive manner. All workshop contributions will be published in the supplemental proceedings of the Ubicomp/ISWC 2019 conference and included in the ACM Digital Library.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12482
        },
        {
          "affiliations": [],
          "personId": 18466
        },
        {
          "affiliations": [],
          "personId": 21240
        },
        {
          "affiliations": [],
          "personId": 23395
        },
        {
          "affiliations": [],
          "personId": 16079
        },
        {
          "affiliations": [],
          "personId": 9337
        },
        {
          "affiliations": [],
          "personId": 11538
        },
        {
          "affiliations": [],
          "personId": 18029
        },
        {
          "affiliations": [],
          "personId": 18751
        },
        {
          "affiliations": [],
          "personId": 13519
        },
        {
          "affiliations": [],
          "personId": 11226
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7810,
      "typeId": 11437,
      "title": "Poster: Using vibrations from a smartRing as an out-of-band channel for sharing secret keys",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the rapid growth in the number of Internet of Things (IoT) devices with wireless communication capabilities, and sensitive information collection capabilities, it is becoming increasingly necessary to ensure that these devices communicate securely with only authorized devices. A major requirement of this secure communication is to ensure that both the devices share a secret, which can be used for secure pairing and encrypted communication. Manually imparting this secret to these devices becomes an unnecessary overhead, especially when the device interaction is transient. In this work, we empirically investigate the possibility of using an out-of-band communication channel -- vibration, generated by a custom smartRing -- to share a secret with a compatible IoT device. This exchanged secret can be used to bootstrap a secure wireless channel over which the devices can communicate. Through a user study with 12 participants we show that in the best case we can exchange 85.9% messages successfully. Our technique demonstrates the possibility of sharing messages accurately, quickly and securely as compared to several existing techniques.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18594
        },
        {
          "affiliations": [],
          "personId": 8902
        },
        {
          "affiliations": [],
          "personId": 17447
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7812,
      "typeId": 11437,
      "title": "Learning to Recognize Unmodified Lights with Invisible Feature",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "To enable accurate indoor localization at low cost, recent research in visible light positioning (VLP) proposed to employ existing ceiling lights as location landmarks, and use smartphone cameras or light sensors to identify the different lights using statistical visual/optical features. Despite the potential, we find such solutions are unreliable: the features are easily corrupted with slight rotation of the smartphone, and are not discriminative enough for many practical light models with different size/shape/intensity. In this work, we propose Auto-Litell to resolve these critical challenges and make VLP truly robust. Auto-Litell builds a customized deep-learning neural network model to automatically distill the “invisible” visual features from the lights, which are resilient to smartphone orientation and light models. Moreover, Auto-Litell introduces a Light-CycleGAN to generate “fake” light images to augment the training data, so as to relieve human labors in data collection and labeling.We have implemented Auto-Litell as a real-time localization and navigation system on Android. Our experiments demonstrate Auto-Litell’s high accuracy in discriminating the lights in the same building, and high reliability across a variety of practical usage scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13534
        },
        {
          "affiliations": [],
          "personId": 19169
        },
        {
          "affiliations": [],
          "personId": 13036
        },
        {
          "affiliations": [],
          "personId": 12032
        },
        {
          "affiliations": [],
          "personId": 20504
        },
        {
          "affiliations": [],
          "personId": 15918
        }
      ],
      "sessionIds": [
        1177
      ],
      "eventIds": []
    },
    {
      "id": 4744,
      "typeId": 11437,
      "title": "Poster: Smart Eyewear Enabled Interactive Pet Toy for Users with Limited Mobility",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smart eyewear that detects eye movements and head motions have been applied to studies on detecting one’s mental state, engagement level in social interactions, and measuring concentration. However, applications for these smart eyewear that offers intriguing and novel interactive experience has seldom been used as wearable device-control. As an application of this type of wearable, we used J!NS MEME that enables head motion data to control smart devices with hands-free operation in a pet toy system to facilitate human-animal interaction for people in cases of limited mobility either due to physical disability or the lack of enough space allowance. For our study, we primarily focused on exploring interaction possibilities between pet owners, who experience physical limitations due to the latter context, and their pets. The study result shows a promising start for opening up new interaction opportunities for our targeted audience in the context of physical limitation; however, the impact on those with physical disabilities is still arguable as it is yet to be evaluated.Nevertheless, since smart eyewear can act as an unobtrusive and useful body extension for people with limited mobility, we believe that it can be an alternative input option that can be applied not just to human-animal interaction but also to wider domains such smart devices and home systems for these users.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16727
        },
        {
          "affiliations": [],
          "personId": 19078
        },
        {
          "affiliations": [],
          "personId": 15275
        },
        {
          "affiliations": [],
          "personId": 21279
        },
        {
          "affiliations": [],
          "personId": 17920
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6793,
      "typeId": 11437,
      "title": "FMT: A Wearable Camera-Based Object Tracking Memory Aid for Older Adults",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Older adults sometimes forget about whether or not they have completed routine actions and the states of objects that they have interacted with (e.g., the kitchen stove is on or off). In this work, we explore whether video clips captured from a body-worn camera every time objects of interest are found within its field of view can help older adults determine if they have completed certain actions with these objects and what their states are. We designed FMT (“Fiducial Marker Tracker”)―a real-time capture and access application that opportunistically captures video clips of objects the user interacts with. To do this, the user places fiducial markers close to objects which would be captured when the marker enters the user’s body-worn camera’s field of view. We examine and discuss what objects this system would be best suited to track, and the usefulness and usability of this approach. FMT successfully captured direct interactions with an object at an average rate of 75.6% across all participants (SD = 9.9%). Our results also reveal how, what, and why users would use such a system for help.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23277
        },
        {
          "affiliations": [],
          "personId": 23137
        },
        {
          "affiliations": [],
          "personId": 23488
        },
        {
          "affiliations": [],
          "personId": 20868
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 7819,
      "typeId": 11437,
      "title": "Identifying Causal Patterns from Mobile Sensing Data: A Case Study on Blood Glucose Inference",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The high-dimensional and co-evolved data streams sensed by mobile devices typically exists time delays that form the \"causal-and-effect\" patterns. Understanding the informative causal patterns from the multivariate time series is critical but challenging for the inference tasks with the sensing data. While a large scope of statistical learning methods has undergone great advances in the causal pattern recognition problem, most of them are still limited in the unreliable causal analysis, high computational complexity and the environmental noise interruption. To this end, we propose a novel directed information (DI)-aided approach to efficiently select the casual patterns from a set of feature streams collected from mobile devices. The proposed approach has been evaluated on a real blood glucose sensing dataset. The results demonstrate our proposed approach outperforms the traditional methods in cost efficiency and inference accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21586
        },
        {
          "affiliations": [],
          "personId": 9251
        },
        {
          "affiliations": [],
          "personId": 11505
        },
        {
          "affiliations": [],
          "personId": 18241
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6796,
      "typeId": 11437,
      "title": "A Comparative Feasibility Analysis for Sensing Swelling with Textile-based Soft Strain Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sensing edema or swelling in the body is a parameter of interest for many clinical and cosmetic applications of wearable technology. Edema creates heightened sensitivity to pressure points, making soft textile-based sensors particularly attractive. However, swelling induces slow, gradual changes in body circumferences, which can be difficult to sense accurately, especially with soft e-textile sensors. Anthropometric variability, sensor placement, and body movements further confound accuracy. Here, we explore the feasibility of accurately sensing ankle swelling through a comparative assessment of a set of sensors representing 3 soft sensing mechanisms in a linear-strain bench test under three frequency conditions. These sensors are then applied in a simulated swelling experiment to assess their performance relative to circumferential changes similar to those experienced in swelling. We find that two sensors (stitched and capacitive) demonstrate reliable performance that approximates or exceeds expert human measurement for swelling. Resistive polymer cord and capacitive sensors exhibited accurate response under medium- and high-frequency extensions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24160
        },
        {
          "affiliations": [],
          "personId": 23969
        },
        {
          "affiliations": [],
          "personId": 8993
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 6799,
      "typeId": 11437,
      "title": "Automatic Annotation for Human Activity and Device State Recognition using Smartphone Notification",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The process of human activity recognition needs to construct a model that has learned sensor data with annotations, i.e., groundtruth, label, or answer activity, in advance. Therefore, a large and diverse set of annotated data are needed to improve and evaluate model performance. Since it is difficult to judge the user's situation even after seeing the acceleration data, it is necessary to add an annotation to the collected acceleration data. In this paper we propose a method that estimates the user and device situations from the user's response to the notification generated by the device such as smartphone. User and device situations are estimated from the user's response time to the notification and the acceleration values in the device. Estimation result with high confidence is given to the sensor data as an annotation. Through the evaluation experiment, for seven kinds of annotation classes, an average precision of 0.769 and 0.963 for user-independent experiments and for user-dependent experiments were achieved, respectively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10139
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4751,
      "typeId": 11437,
      "title": "Towards a Diffraction-based Sensing Approach on Human Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In recent years, wireless sensing has been exploited as a promising research direction for contactless human activity recognition. However, one major issue hindering the real deployment of these systems is that the signal variation patterns induced by the human activities with different devices and environmental settings are neither stable nor consistent, resulting in unstable system performance. The existing machine learning based methods usually take the “black box” approach and fails to achieve consistent performance. In this paper, we argue that a deep understanding of radio signal propagation in wireless sensing is needed, and it may be possible to develop a deterministic sensing model to make the signal variation patterns predictable. With this intuition, in this paper we investigate: 1) how wireless signals are affected by human activities taking transceiver location and environment settings into consideration; 2) a new deterministic sensing approach to model the received signal variation patterns for different human activities; 3) a proof-of-concept prototype to demonstrate our approach and a case study to detect diverse activities. In particular, we propose a diffraction-based sensing model to quantitatively determine the signal change with respect to a target’s motions, which eventually links signal variation patterns with motions, and hence can be used to recognize human activities. Through our case study, we demonstrate that the diffraction-based sensing model is effective and robust in recognizing exercises and daily activities. In addition, we demonstrate that the proposed model improves the recognition accuracy of existing machine learning systems by above 10%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13126
        },
        {
          "affiliations": [],
          "personId": 20721
        },
        {
          "affiliations": [],
          "personId": 12597
        },
        {
          "affiliations": [],
          "personId": 10785
        },
        {
          "affiliations": [],
          "personId": 9904
        },
        {
          "affiliations": [],
          "personId": 11956
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [
        2149
      ],
      "eventIds": []
    },
    {
      "id": 4755,
      "typeId": 11437,
      "title": "Mobile Nutrition Monitoring for Well-being",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The EU-funded project WellCo\\footnote{\\url{http://wellco-project.eu} \\\\ WellCo Project has received funding from the European Union’s Horizon 2020 research and innovation programme under grant agreement No 769765} aims to deliver a new mobile app with a virtual coach to encourage the users towards healthier behaviour choices in order to improve their physical, cognitive, mental and social well-being. Healthy nutrition can substantially contribute to health and wellbeing. We will use different techniques for dietary assessment in the WellCo project - eating detection by gesture recognition using a wrist-worn device, and estimating the quality of diet by self-reporting using a Food Frequency Questionnaire (FFQ).\nThis paper describes the latter. We designed a short FFQ, compared it to validated questionnaires, and developed a web service and a web application to determine dietary quality score for each user by using the designed FFQ.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22140
        },
        {
          "affiliations": [],
          "personId": 9503
        },
        {
          "affiliations": [],
          "personId": 20464
        },
        {
          "affiliations": [],
          "personId": 8648
        },
        {
          "affiliations": [],
          "personId": 13286
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3734,
      "typeId": 11437,
      "title": "A Wearable Therapy and Technology Garment for Kids: The Underlying Super Hero",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Children today live in a fast-paced, ever-changing and sensory overloaded society.  With anxiety and depression in children becoming more present within today’s youth, the need to address these concerns including increased health and behavioral issues are imperative.  Giving children the right tools to combat these conditions and potentially dangerous disorders will help them traverse their daily lives while assisting them with their physical, mental, and emotional well-being.  Depression and anxiety can have negative effects on children that directly affect their learning capabilities and functionalities to do so.  Wearable therapy garments were created for these situations and are an easy way to support wellness within these types of conditions.  Fabric and clothing can make a big difference in a child’s life and apparel can be used to help reduce stress and anxiety disorders through functional garments in a way that can contribute to a happier, healthier, and reduced stress childhood.  Hugs have been proven to reduce anxiety, therefore, compression that simulates that feeling has a positive effect on children.  Compression garments exist for children, however, how can these garments be elevated with functional wearable technology, and design aesthetic to incorporate fun, kid-friendly design elements that create body positivity and well-being.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19625
        },
        {
          "affiliations": [],
          "personId": 23079
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2711,
      "typeId": 11437,
      "title": "UDSP+: Stress Detection based on User-Reported Emotional Ratings and Wearable Skin Conductance Sensor",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Detecting stress during user experience (UX) evaluation is particularly important. Studies have shown that skin conductance (SC) is a physiological signal highly associated with stress. This paper investigates how SC Responses (SCRs) can contribute to the development of a publicly available stress detection mechanism. In specific, SCRs located in users’ self-reported stress periods were used as a training dataset for the creation of our UDSP+ predictor. A lab study was conducted to evaluate the accuracy of our approach. The SC of 24 participants was recorded using the wearable Nexus10 sensor. Moreover, participants’ self-reported emotional ratings (valence-arousal) were obtained using the Affect Grid Tool retrospectively. The performance of the UDSP+ was tested using machine learning. Considering the 2-class classification problem (stress vs. non-stress), an accuracy of up to 86% was achieved. This demonstrates the dynamics of users’ self-reported periods to act as a dataset creation mechanism in tow with SCRs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17869
        },
        {
          "affiliations": [],
          "personId": 22253
        },
        {
          "affiliations": [],
          "personId": 9673
        },
        {
          "affiliations": [],
          "personId": 8303
        },
        {
          "affiliations": [],
          "personId": 16970
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3739,
      "typeId": 11437,
      "title": "RAMT: Real-time Aitude and Motion Tracking for Mobile Devices in Moving Vehicle",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recently a class of new in-vehicle technologies based on off-the-shelf mobile devices have been developed to improve driving safety and experience. For instance, wearables like the smartwatches are utilized to monitor the action of the driver and detect possible secondary tasks. Moreover, wearables can allow a driver to use gesture for in-vehicle controls, reducing distractions to driving. The accuracy of these systems can be significantly improved by tracking the real-time attitude of mobile devices. This paper proposes a novel system called Real-time Attitude and Motion Tracking (RAMT) that can enable a mobile device to accurately learn the coordinate system of a moving vehicle, and hence track its attitude and motion in real time. RAMT consists of a series of lightweight algorithms to sense the vehicle's movement and calculate the device's attitude. It provides a solution for trajectory-based gesture recognition. We have implemented RAMT on a smartphone and a smartwatch and evaluated the performance in 10 real driving trips. Our results show that the overall error of the coordinate system alignment is around 5 degree for the smartphone and 10 degree for the smartwatch, and over 84% of customized hand gestures can be accurately recognized with the result of RAMT. A video demo of RAMT is available at https://youtu.be/9rZp7HxyRts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16741
        },
        {
          "affiliations": [],
          "personId": 19348
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 3741,
      "typeId": 11437,
      "title": "\"Why Are They Collecting My Data?\": Inferring the Purposes of Network Traffic in Mobile Apps",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Many smartphone apps collect potentially sensitive personal data and send it to cloud servers. However, most mobile users have a poor understanding of why their data is being collected. We present MobiPurpose, a novel technique that can take a network request made by an Android app and then classify the data collection purposes, as one step towards making it possible to explain to non-experts the data disclosure contexts. Our purpose inference works by leveraging two observations: 1) developer naming conventions (e.g., URL paths) often offer hints as to data collection purposes, and 2) external knowledge, such as app metadata and information about the domain name, are meaningful cues that can be used to infer the behavior of different traffic requests. MobiPurpose parses each traffic request body into key-value pairs, and infers the data type and data collection purpose of each key-value pair using a combination of supervised learning and text pattern bootstrapping. We evaluated MobiPurpose’s effectiveness using a dataset cross-labeled by ten human experts. Our results show that MobiPurpose can predict the data collection purpose with an average precision of 84% (among 19 unique categories).",
      "authors": [
        {
          "affiliations": [],
          "personId": 12722
        },
        {
          "affiliations": [],
          "personId": 9832
        },
        {
          "affiliations": [],
          "personId": 20953
        },
        {
          "affiliations": [],
          "personId": 19756
        },
        {
          "affiliations": [],
          "personId": 17754
        },
        {
          "affiliations": [],
          "personId": 15671
        },
        {
          "affiliations": [],
          "personId": 14005
        },
        {
          "affiliations": [],
          "personId": 22231
        }
      ],
      "sessionIds": [
        1103
      ],
      "eventIds": []
    },
    {
      "id": 6814,
      "typeId": 11437,
      "title": "Perils of Zero-Interaction Security in the Internet of Things",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The Internet of Things (IoT) demands authentication systems which can provide both security and usability. Recent research utilizes the rich sensing capabilities of smart devices to build security schemes operating without human interaction, such as zero-interaction pairing (ZIP) and zero-interaction authentication (ZIA). Prior work proposed a number of ZIP and ZIA schemes and reported promising results. However, those schemes were often evaluated under conditions which do not reflect realistic IoT scenarios. In addition, drawing any comparison among the existing schemes is impossible due to the lack of a common public dataset and unavailability of scheme implementations. In this paper, we address these challenges by conducting the first large-scale comparative study of ZIP and ZIA schemes, carried out under realistic conditions. We collect and release the most comprehensive dataset in the domain to date, containing over 4250 hours of audio recordings and 1 billion sensor readings from three different scenarios, and evaluate five state-of-the-art schemes based on these data. Our study reveals that the effectiveness of the existing proposals is highly dependent on the scenario they are used in. In particular, we show that these schemes are subject to error rates between 0.6% and 52.8%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23780
        },
        {
          "affiliations": [],
          "personId": 23657
        },
        {
          "affiliations": [],
          "personId": 11568
        },
        {
          "affiliations": [],
          "personId": 8810
        },
        {
          "affiliations": [],
          "personId": 20143
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 7839,
      "typeId": 11437,
      "title": "EIS: A Wearable Device for Epidermal American Sign Language Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "American sign language (ASL) is widely used among hearing impaired individuals in English-speaking countries. Various technologies have been developed to perform ASL recognition, including optical signal sensing, electrical signal sensing, and mechanical signal sensing. However, wearable devices using those methods have bulky and complex sensing modules that lead to long-term discomfort as well as poor accuracy. In this paper, we present an epidermal-iontronic sensing (EIS)-based wearable device that wears on finger joints for 35 fingerspelling ASL recognitions (i.e., 26 alphabets from A to Z and 9 digits from one to nine). Compared to current on-market devices, such design is lighter, comfortable to wear and has better appearance according to user comments. When bending the finger, a physical contact forms between the ionic material and the epidermis of skin, leading to an electric double layer (EDL) established at the interface. Therefore, a significant capacitive change can be achieved with various finger gestures. By using Nafion as the ionic sensing material, we developed a sensing device to provide excellent flexibility and optical transparency. We used machine learning methods, such as neural networks to track and perform ASL recognition using the signals obtained from the designed device. The algorithm achieved a within-user accuracy of 99.6% and a cross-user accuracy of 76.1% when adapted the model to different users. This wearable device is low-cost and has broad potential to be integrated in future application of human-machine interactions (HMI), smart home controls, and nonverbal communications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14628
        },
        {
          "affiliations": [],
          "personId": 14489
        },
        {
          "affiliations": [],
          "personId": 14664
        },
        {
          "affiliations": [],
          "personId": 22585
        },
        {
          "affiliations": [],
          "personId": 11932
        },
        {
          "affiliations": [],
          "personId": 8770
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6816,
      "typeId": 11437,
      "title": "Poster: Data Distribution Infrastructure and Applications for Robotic Therapy for Blind Elderly",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In recent years, the elderly population has been increasing rapidly and robot therapy for elderly people has been researched. \nHowever, there are few common platform for robot therapy for elderly people and few researches on robot therapy for blind elderly people. \nIn this research, we develop a data distribution system for robot therapy for elderly people that collectively manages data on the cloud and promotes data utilization. The development system collects, transfers, and visualizes sensor data related to elderly people with robot interaction. As a case study, we also develop a robot therapy system using conversations for blind elderly people to induce positive mental state. \nWe conducted experiment for three blind elderly and confirmed the feasibility of robot therapy using conversations for blind elderly people.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22612
        },
        {
          "affiliations": [],
          "personId": 20296
        },
        {
          "affiliations": [],
          "personId": 12501
        },
        {
          "affiliations": [],
          "personId": 15186
        },
        {
          "affiliations": [],
          "personId": 11925
        },
        {
          "affiliations": [],
          "personId": 22033
        },
        {
          "affiliations": [],
          "personId": 23828
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4769,
      "typeId": 11437,
      "title": "Towards 3D Smooth Pursuit Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 11035
        },
        {
          "affiliations": [],
          "personId": 9822
        },
        {
          "affiliations": [],
          "personId": 23486
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5794,
      "typeId": 11437,
      "title": "WearBreathing: Real World Respiratory Rate Monitoring Using Smartwatches",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Respiratory rate is a vital physiological signal that may be useful for a multitude of clinical applications, especially if measured in the wild rather than controlled settings.  In the wild respiratory rate monitoring is currently done using dedicated chest  band sensors, but these devices are specialized, expensive and cumbersome to wear day after  day. While recent works have proposed using smartwatches and accelerometer/gyroscope data for respiratory rate monitoring, current methods are unreliable and inaccurate in the presence of  motion and have therefore only been applied in controlled or low-motion settings. Thus, measuring  respiratory rate in the wild remains a challenge.  \n\nWe observe that for many applications, having fewer, accurate readings is better than having more, less accurate readings. Based on this, we develop WearBreathing, a novel system for respiratory rate monitoring. WearBreathing consists of a  machine learning based filter that detects and rejects sensor data that are not suitable for  respiratory rate extraction and a convolutional neural network model for extracting respiratory rate from accelerometer and gyroscope data. Using a  diverse, out-of-the-lab dataset that we collect, we show that WearBreathing has a 2.5 to 5.8 times lower mean absolute error (MAE) than existing approaches. We show that WearBreathing is tunable and by changing a single threshold value, it can, for example, deliver a reading every 50 seconds with a MAE of 2.05 breaths/min or a reading every 5 minutes with an MAE of 1.09 breaths/min. Finally, we evaluate power consumption and find that with some power saving measures,\nWearBreathing can run on a smartwatch while providing a full day's worth of battery life.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21426
        },
        {
          "affiliations": [],
          "personId": 14765
        },
        {
          "affiliations": [],
          "personId": 23469
        },
        {
          "affiliations": [],
          "personId": 20404
        },
        {
          "affiliations": [],
          "personId": 8338
        },
        {
          "affiliations": [],
          "personId": 22769
        },
        {
          "affiliations": [],
          "personId": 9397
        },
        {
          "affiliations": [],
          "personId": 19603
        },
        {
          "affiliations": [],
          "personId": 15037
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 3746,
      "typeId": 11437,
      "title": "Using Deep Learning and Mobile Offloading to Control a 3D printed Prosthetic Hand",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Although many children are born with congenital limb malformation, contemporary functional artificial hands are costly and are not meant to be adopted to growing hand. In this work we develop a low cost, adaptable and personalizable system of an artificial prosthetic hand accompanied with hardware and software modules. Our solution consists of (i) a consumer grade electromyography recording hardware, (ii) a mobile companion device empowered by deep learning classification algorithms, (iii) mechanical arm with embedded hardware, and (iv) an optional cloud component for offloading computations. We focus on the flexibility of the designed system making it more affordable than the alternatives. We use 3D-printed materials and open-source software thus enabling the community to contribute and improve the system. In this paper we describe the proposed system, its components and experiments we conducted in order to show feasibility and applicability of our approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11412
        },
        {
          "affiliations": [],
          "personId": 17031
        },
        {
          "affiliations": [],
          "personId": 8830
        },
        {
          "affiliations": [],
          "personId": 11677
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 4775,
      "typeId": 11437,
      "title": "Nurse Care Activity Recognition: A GRU-based approach with attention mechanism",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition is a challenging task due to complexity and variations of human movements while performing activities by different subjects. Extracting features to model the temporal evolution of different movements plays an important role in this task. In this paper, we present the approach followed by our team, Dark_Shadow, to recognize complex nurse activities in the \"Nurse Care Activity Recognition Challenge\" [1]. We present a deep learning method to capture the movements of essential body parts from time series of human activity data collected by sensors and then classify them. Deep learning approaches have provided satisfactory results in various human activity recognition tasks. In this work, we propose a Gated Recurrent Unit (GRU) model with attention mechanism to recognize the nurse activities. We obtain approximately 66.43% accuracy for person-wise one leave out cross validation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19637
        },
        {
          "affiliations": [],
          "personId": 22853
        },
        {
          "affiliations": [],
          "personId": 19082
        },
        {
          "affiliations": [],
          "personId": 21963
        },
        {
          "affiliations": [],
          "personId": 23156
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4778,
      "typeId": 11437,
      "title": "Rotation-Equivariant Convolutional Neural Network Ensembles",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "For the present engineering of neural networks, rotation symmetry is hard to be obtained. As an important characteristic in our physical world, in image processing, using rotated images in training of neural network would largely decrease the performance. This seriously hindered the real-world application of neural networks in image recognition, such as human tracking, self-driving cars, intelligent surveillance, and so on. In this paper, we would like to present a rotation-equivariant design of convolutional neural network ensembles to counteract the problem of rotated image processing task. The convolutional neural network ensembles combine multiple convolutional neural networks trained by different ranges of rotation angles respectively. In proposed theory, the model lowers the training difficulty by learning with smaller separations of random rotation angles instead of a huge one. Experiments are reported in this paper. The CNN ensembles could reach 96.35\\% on rotated MNIST datasets, 84.9\\% on rotated Fashion-MNIST datasets, and 91.35\\% on rotated KMNIST datasets. These results are comparable to current state-of-the-art performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12574
        },
        {
          "affiliations": [],
          "personId": 10329
        },
        {
          "affiliations": [],
          "personId": 8444
        },
        {
          "affiliations": [],
          "personId": 9975
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7851,
      "typeId": 11437,
      "title": "Poster: Bike Type Identification Using Smartphone Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present an approach to identify bike types using smartphone sensors.\nKnowledge of the bike type is necessary to provide ubiquitous services such as navigation services that consider bike-specific road conditions in route planning to improve driving safety.\nIn order to differentiate between bike types, we use four machine learning classifiers.\nTo evaluate our approach, we collected sensor readings on two routes with six rides each for two bike types.\nThe evaluation shows very good predictive performance for all classifiers with F1 scores of up to 0.94.\nOverall, the convolutional neural network (CNN) classifier yields the best results for both bike types and both routes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23746
        },
        {
          "affiliations": [],
          "personId": 22193
        },
        {
          "affiliations": [],
          "personId": 13282
        },
        {
          "affiliations": [],
          "personId": 18597
        },
        {
          "affiliations": [],
          "personId": 10068
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5803,
      "typeId": 11437,
      "title": "Revealing Urban Dynamics by Learning Online and Offline Behaviours Together",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Urban problems and diseases accompanied by the pace of urbanization have drawn attention to the importance of understanding urban dynamics, while a deep and comprehensive understanding is challenging due to our diversified lifestyles in the modern city. In this paper, we propose an urban dynamics modeling system to characterize the regularity of urban activity dynamics as well as urban functions by learning residents' online and offline behaviours together. Built on a state-sharing hidden Markov model, our system utilizes online activities of App usage and offline activities of mobility in different urban regions and different time slots for learning. The learnt state sequence of each region reveals urban dynamics with the corresponding urban functions. We evaluate our system via a large-scale mobile network accessing dataset, which discovers ten hidden states characterizing different life modes and eight representative dynamic patterns corresponding to different urban functions. These discovered dynamic patterns and inferred functions are validated by social media check-ins and the land-use published by the government with 81\\% accuracy. Based on our model, we propose two applications, crowd flow prediction and popular App prediction, which outperforms the state-of-the-art approaches by 36.1\\% and 15.7\\%, respectively. This study paves the way for extensive city-related applications including urban demand analysis, land-use planning, and activity prediction.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13593
        },
        {
          "affiliations": [],
          "personId": 24147
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 2734,
      "typeId": 11437,
      "title": "Understanding Cycling Trip Purpose and Route Choice Using GPS Traces and Open Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Many mobile applications such as Strava or Mapmyride allow cyclists to collect detailed GPS traces of their trips for health or route sharing purposes. However, cycling GPS traces also have a lot of potential from an urban planning perspective. In this paper, we focus on two important issues to characterize urban cyclist behavior: trip purpose and route choice. Cycling trip purpose has been typically analyzed using survey data. Here, we present a method to automatically infer the purpose of a cycling trip using cyclists‚Äô personal data, GPS traces and a variety of built-in and social environment features extracted from open datasets characterizing the streets cycled. We evaluate the proposed method using GPS traces from over 7,000 cycling routes in the city of Philadelphia and report F1 scores of up to 86% when four trip purposes are considered. On the other hand, we also present a novel statistical method to identify the role that certain variables characterizing the built-in and social environment play in the selection of a specific cycling route. Our results show that cyclists in Philadelphia tend to favor routes with green areas, safety and centrality.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11629
        },
        {
          "affiliations": [],
          "personId": 21868
        },
        {
          "affiliations": [],
          "personId": 14522
        },
        {
          "affiliations": [],
          "personId": 18650
        }
      ],
      "sessionIds": [
        1841
      ],
      "eventIds": []
    },
    {
      "id": 3760,
      "typeId": 11437,
      "title": "VADLite: An Open-Source Lightweight System for Real-Time Voice Activity Detection on Smartwatches",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartwatches provide a unique opportunity to collect more speech data because they are always with the user and also have a more exposed microphone compared to smartphones. Speech data could be used to infer various indicators of mental well being such as emotions, stress and social activity. Hence, real-time voice activity detection (VAD) on smartwatches could enable the development of applications for mental health monitoring. In this work, we present VADLite, an open-source, lightweight, system that performs real-time VAD on smartwatches. It extracts mel-frequency cepstral coefficients and classifies speech versus non-speech audio samples using a linear Support Vector Machine. The real-time implementation is done on the Wear OS Polar M600 smartwatch. An offline and online evaluation of VADLite using real-world data showed better performance than WebRTC's open-source VAD system. VADLite can be easily integrated into Wear OS projects that need a lightweight VAD module running on a smartwatch.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11535
        },
        {
          "affiliations": [],
          "personId": 23512
        },
        {
          "affiliations": [],
          "personId": 11417
        },
        {
          "affiliations": [],
          "personId": 23259
        },
        {
          "affiliations": [],
          "personId": 20479
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4784,
      "typeId": 11437,
      "title": "Stress Reduction in Everyday Wearables",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 20409
        },
        {
          "affiliations": [],
          "personId": 23562
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2738,
      "typeId": 11437,
      "title": "SleepThermo: The affect of in-cloth monitored body temperature change during sleep on human well-being",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the advent of 24/7 technology-driven society, there are rising discomforts and increasing concerns over sleep. Quality of sleep critically affects human well-being and everyday performance. A good night's sleep, therefore, is essential to prepare one's mind and body for the next day. In this paper, we propose SleepThermo a system that identifies the affect of in-cloth monitored body temperature change during sleep on human well-being. The primary purpose of this research is to determine the relationship between in-clothing body temperature and mental/physical conditions, and then elicit positive insights to improve sleep quality. Our evaluation proved that there is a potential for body temperature to be used in identifying mental and physical conditions. In addition personalized models achieved better accuracy than overall models in all subjects.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13466
        },
        {
          "affiliations": [],
          "personId": 11737
        },
        {
          "affiliations": [],
          "personId": 17052
        },
        {
          "affiliations": [],
          "personId": 11454
        },
        {
          "affiliations": [],
          "personId": 22964
        },
        {
          "affiliations": [],
          "personId": 21348
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7860,
      "typeId": 11437,
      "title": "AutoTag: Visual Domain Adaptation for Autonomous Retail Stores through Multi-Modal Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Autonomous checkout at retail stores could bring a large array of benefits to both consumers --no lines, better user experience-- and retailers --lower operational cost, detailed insights about customer behavior. Existing approaches include self-checkout stations, on-item sensing (e.g., RFID) and infrastructure-based sensing (e.g., vision and weight). While each has their own pros and cons, the latter offers a good tradeoff between information richness and operational cost. However, several challenges currently limit their accuracy. In particular, visual item recognition is constrained by the huge amount of training data required and the domain adaptation gap that usually exists between the training --e.g., well-lit environment-- and testing distributions --e.g., each store might have different lighting conditions, camera angles, etc. In this preliminary work we explore different ways to leverage multi-modal sensing (e.g., weight load cells on shelves) to automatically label frames from customers picking up or putting down items on the shelf. Then, those annotated frames could be used to continuously expand the initial visual model and tailor it to each store's conditions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16320
        },
        {
          "affiliations": [],
          "personId": 11950
        },
        {
          "affiliations": [],
          "personId": 14989
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2745,
      "typeId": 11437,
      "title": "Proactive Car Navigation: Destination Prediction release us from struggle user input of Car Navigation",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Conventional car navigation systems that require manual input of a user's destination are not frequently used for familiar routes such as daily commutes and regular shopping.\nIn this paper, we propose and realize proactive car navigation, which integrates daily destination prediction system to car navigation system in order to eliminate input by user by displaying information related to destinations automatically.\nBy questionnaire-based evaluation, we found that proactive car navigation innovates a user's driving experiences, and offers them various advantages, even for regularly frequented destinations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16842
        },
        {
          "affiliations": [],
          "personId": 13612
        },
        {
          "affiliations": [],
          "personId": 20380
        },
        {
          "affiliations": [],
          "personId": 20727
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3769,
      "typeId": 11437,
      "title": "Challenges and solutions in a hybrid mHealth mobile application",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The paper describes the various problems and challenges encountered during the development and remote data collection in a cross-platform hybrid application developed for remote monitoring of participants and what solutions were implemented to mitigate them. These problems and challenges are universal for hybrid applications and this paper digs deep into these in the domain of large-scale, long-duration mHealth research studies. From technical issues to issues with user compliance, this paper discusses the core problems inherent to these types of studies and technologies, and how to mitigate them.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10779
        },
        {
          "affiliations": [],
          "personId": 18285
        },
        {
          "affiliations": [],
          "personId": 15608
        },
        {
          "affiliations": [],
          "personId": 11243
        },
        {
          "affiliations": [],
          "personId": 22845
        },
        {
          "affiliations": [],
          "personId": 16639
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4794,
      "typeId": 11437,
      "title": "ICT: In-field Calibration Transfer for Air Quality Sensor Deployments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recent years have witnessed a growing interest in urban air pollution monitoring, where hundreds of low-cost air quality sensors are deployed city-wide. To guarantee data accuracy and consistency, these sensors need periodic calibration after deployment. Since access to ground truth references is often limited in large-scale deployments, it is difficult to conduct city-wide post-deployment sensor calibration. In this work we propose In-field Calibration Transfer (ICT), a calibration scheme that transfers the calibration parameters of source sensors (with access to references) to target sensors (without access to references). On observing that (i) the distributions of ground truth in both source and target locations are similar and (ii) the transformation is approximately linear, ICT derives the transformation based on the similarity of distributions with a novel optimization formulation. The performance of ICT is further improved by exploiting spatial prediction of air quality levels and multi-source fusion. Experiments show that ICT is able to calibrate the target sensors as if they had direct access to the references.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16664
        },
        {
          "affiliations": [],
          "personId": 23674
        },
        {
          "affiliations": [],
          "personId": 24368
        },
        {
          "affiliations": [],
          "personId": 15140
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 7869,
      "typeId": 11437,
      "title": "Poster: Signal Propagation Analysis in Multiuser Human Body Communication",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Herein, we studied a multiuser human body communication system that uses multiple human bodies as a transmission channel when assuming data sharing among users. We evaluated electric field distributions around human bodies and transmission characteristics between wearable transceivers through electromagnetic field simulation using whole-body human models. Simulation results showed that the electric field strongly propagates between human bodies via physical contact; however, the contact area between two users has little influence on the electric field. In addition, the dominant control on the transmission characteristics is the position of the receiver.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10432
        },
        {
          "affiliations": [],
          "personId": 11641
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5821,
      "typeId": 11437,
      "title": "Layered Embroidery for Dynamic Aesthetics",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present an embroidery technique that allows for the creation of dynamic fabrics. These embroidered dynamic fabrics allow for the conceal and reveal of information or visuals based on different viewing angles and lighting conditions. Using this technique, fabrics and garments can be used to hide information and patterns. We explain how the technique works and provide technical details concerning the embroidery technique.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19640
        },
        {
          "affiliations": [],
          "personId": 14155
        },
        {
          "affiliations": [],
          "personId": 22399
        },
        {
          "affiliations": [],
          "personId": 24125
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6848,
      "typeId": 11437,
      "title": "Demo: A Contactless Gesture Interaction System Using LTE (4G) Signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Nowadays, 4G devices are pervasive and most of the homes and offices in modern cities are covered by LTE signals. While it is very attractive to leverage ubiquitous LTE signals and use hand gestures to control the home appliances remotely, there is no work on such contactless gesture interaction systems reported yet. In this work, we present an LTE-based contactless gesture interaction system to recognize various hand gestures around a 4G terminal like mobile phone, which can be used to control the switch, channel and volume of a TV set remotely without holding any devices. The results show that the proposed system can recognize different hand gestures accurately leveraging LTE signals without training, and achieve remote TV control in real time in different settings.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14698
        },
        {
          "affiliations": [],
          "personId": 20721
        },
        {
          "affiliations": [],
          "personId": 17298
        },
        {
          "affiliations": [],
          "personId": 10893
        },
        {
          "affiliations": [],
          "personId": 20239
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4800,
      "typeId": 11437,
      "title": "Towards measuring well-being in smart environments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This study discusses measurement of well-being in the context of smart environments. We propose an experimental design which induces variation in an individual’s flow, stress, and affect for testing different measurement methods. Both subjective and objective measuring methods are applied, with a variety of wearable sensors (EEG sensor, smart ring, heart rate monitor) and video monitoring. Preliminary results show significant agreement with the test structure in the readings of wearable stress and heart rate sensors. Self-assessments, on the contrary, fail to show significant evidence of the experiment structure, reflecting the difficulty of subjective estimation of short-term stress, flow and affect.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9320
        },
        {
          "affiliations": [],
          "personId": 10799
        },
        {
          "affiliations": [],
          "personId": 22666
        },
        {
          "affiliations": [],
          "personId": 8680
        },
        {
          "affiliations": [],
          "personId": 21455
        },
        {
          "affiliations": [],
          "personId": 11390
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6853,
      "typeId": 11437,
      "title": "DualSkin: Ambient Electric Field Sensing Wearable",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Humans are capable of identifying and categorizing objects in their environment with the aid of their sensory organs. DualSkin is a wearable which equips the user with ambient electroreception (electric field sensing). The aim of the project is to develop a new sense that will increase peripersonal space, alter the human perception and their relationship between electronic, organic and inorganic objects that surrounds our lives. DualSkin is a vest that uses a circuit which consists of embedded micro-controllers, sensors, and actuators. It uses capacitance sensors to sense surroundings and uses electrical stimulation as a feedback mechanism around the user's waist.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11775
        },
        {
          "affiliations": [],
          "personId": 23283
        },
        {
          "affiliations": [],
          "personId": 15943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5831,
      "typeId": 11437,
      "title": "Painting an Apple with an Apple: A Tangible Tabletop Interface for Painting with Physical Objects",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce UnicrePaint, a digital painting system that allows the user to paint with physical objects by acquiring three parameters from the interacting object: the form, the color pattern and the contact pressure. The design of the system is motivated by a hypothesis that integrating direct input from physical objects with digital painting offers unique creative experience to the user. A major technical challenge in implementing UnicrePaint is to resolve the conflict between input and output, i.e., to be able to capture the form and color pattern of contacting objects from a camera, while at the same time be able to present the captured data using a projector. We present a solution for this problem. We implemented a prototype and carried out a user study with fifteen novice users. Additionally, five professional users with art-related background participated in a user study to obtain insights into how professionals might view our system. The results show that UnicrePaint offers unique experience with painting in a creative manner. Also, the potentials beyond mere artwork is suggested.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14676
        },
        {
          "affiliations": [],
          "personId": 11026
        },
        {
          "affiliations": [],
          "personId": 17839
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 2762,
      "typeId": 11437,
      "title": "PartsSweeper: Interactive workbench to casually organize electronic parts and tools",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Electronics workbenches often becomes messy, as various electronic parts and tools are used repeatedly. To solve this problem, we propose a system called \"PartsSweeper\", which cleans up both parts and tools on electronics workbenches. The PartsSweeper mainly consists of a customized XY plotter and GUI software on a tablet. We attached several magnets, servomotors, and lift mechanisms on a head of the XY plotter. As most electronic parts and tools are ferromagnetic, the system can move them along with the head while the magnet is lifted. Moreover, the system can selectively move parts and tools using two magnets of different force; while the strong magnet can move both tools and parts, the weak magnet moves only small parts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19471
        },
        {
          "affiliations": [],
          "personId": 20789
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6858,
      "typeId": 11437,
      "title": "Vision2Sensor: Knowledge Transfer Across Sensing Modalities for Human Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile and wearable sensing devices are prevalent, being packed with a growing number of diverse sensors. These are supposed to provide the context and better user understating for current apps and are envisioned to be at the core of the emerging smart buildings to automate and adapt environments based on user needs. However, much of this enormous sensing capability is currently wasted and not tabbed into because it is still hard to build context detection applications without substantial sensor data to train on. Sensor data is hard to interpret and annotate, making it difficult and costly to generate such large training sets of good quality. Here we address this fundamental problem that is stalling the adoption of mobile sensing at scale, by proposing a knowledge transfer system, \\sys{}, which opportunistically generates these labels for sensor data when in front of a camera. We show that through knowledge transfer from other vision datasets we can achieve robust accuracy for human activity recognition with a Convolutional Neural Network, which is transferred to the sensor-based classifier. We find that a Deep Neural Network classifier on sensor data is robust to withstand degradation to vision labels to a high error-rate, which is advantageous to limit the error impact from the vision modality. Our system is able to operate in real-time on embedded compute devices, thus not requiring a server, which guarantees user privacy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15946
        },
        {
          "affiliations": [],
          "personId": 19130
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 7883,
      "typeId": 11437,
      "title": "Beyond Control: Enabling Smart Thermostats for Leakage Detection",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smart thermostats, with multiple sensory abilities, are becoming pervasive and ubiquitous, in both residential and commercial buildings. By analyzing occupants' behavior, adjusting the set temperature automatically, and adapting to temporal and spatial changes in the atmosphere, smart thermostats can maximize both - energy savings and user comfort. In this paper, we study smart thermostats for refrigerant leakage detection.  Retail outlets, such as milk-booths and quick service restaurants, set up cold-rooms to store perishable items. In each room, a refrigeration unit (akin to air-conditioners) is used to maintain a suitable temperature for the stored products (instead of humans). Often, refrigerant leaks through the coils (or valves) of the refrigeration unit and slowly diminishes its cooling capacity while allowing it to be functional. Such leaks waste significant energy, risk occupants' health, and impact the quality of stored perishable items. While store managers usually fail to sense the early symptoms of such leaks, current techniques to report refrigerant leakage are often not scalable. We propose Greina - to continuously monitor the readily available ambient information from the thermostat and timely report such leaks. We evaluate our approach on 74 outlets of a retail enterprise and results indicate that Greina can report the leakage a week in advance when compared to manual reporting.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14157
        },
        {
          "affiliations": [],
          "personId": 12091
        },
        {
          "affiliations": [],
          "personId": 15688
        },
        {
          "affiliations": [],
          "personId": 16707
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 5837,
      "typeId": 11437,
      "title": "Connecting IM Pattern and Selective Perceived Responsiveness to Relationship: A Cluster-Based Approach",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As people utilize instant messaging (IM) to communicate with people of various relationships, they pay different amounts of attention to and have different communication practices with them of different relationships. However, we haven't seen a close investigation of how users' IM communication patterns relate to different groups of IM contacts. We collected IM logs of 547 sender-recipient pairs from 33 smartphone users over the course of 4 weeks, and used k-mean clustering to identify 6 clusters of these users' IM communication patterns. We illustrate the characteristics of the IM patterns of these distinct clusters as well as how the patterns relate to the relationship between the senders and the recipients within these clusters respectively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13142
        },
        {
          "affiliations": [],
          "personId": 21199
        },
        {
          "affiliations": [],
          "personId": 14746
        },
        {
          "affiliations": [],
          "personId": 19905
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3798,
      "typeId": 11437,
      "title": "Poster: 'ShapeSense3D - textile-sensing and reconstruction of body geometries'",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present an implementation of a textile sensing sleeve with attached strain sensors to capture the shape of body parts. A shape reconstruction algorithm was developed to reconstruct a geometrical model of the deformed sleeve using the elongation measurements obtained from the sensors and an optimisation process. The current system achieves a 0.44 mm error when reconstructing the radius of a conical shape. We discuss future improvements required to form a more reliable 3D shape sensing device. After further development, this sleeve could be utilised for health care application such as muscle density measurements, movement tracking or to replace plaster or thermoplastic casting in the fabrication of orthoses and prostheses.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19866
        },
        {
          "affiliations": [],
          "personId": 23416
        },
        {
          "affiliations": [],
          "personId": 12526
        },
        {
          "affiliations": [],
          "personId": 19992
        },
        {
          "affiliations": [],
          "personId": 14916
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5848,
      "typeId": 11437,
      "title": "EEGlass: An EEG-Eyeware Prototype for Ubiquitous Brain-Computer Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 16368
        },
        {
          "affiliations": [],
          "personId": 11407
        },
        {
          "affiliations": [],
          "personId": 10459
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7896,
      "typeId": 11437,
      "title": "ShoesLoc: In-Shoe Force Sensor-Based Indoor Walking Path Tracking",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Currently, in-shoe force sensors have been widely used for step counting and gait analysis. However, it has not been realized that in-shoe force sensors are also capable of tracking walking paths. In this paper, we present ShoesLoc, an indoor walking path tracking method based on in-shoe force sensors. We show that, based on the force signals from a user‚Äôs shoes, it is possible to estimate the walking direction change and the stride length of each step with machine learning techniques. We further apply a particle filter to combine this information with the constraint of barriers in floor maps, and thus can determine the walking path and the current position of the user. To solve the problem of the low accuracy caused by cumulative walking direction errors, we improve the particle filter by designing the direction correction algorithm. Moreover, we propose the weight normalization method to handle the impact of handbags and backpacks. Our experimental results show that, after a convergence phase, ShoesLoc achieves the average location error of 0.9-1.3 m. Compared with traditional indoor tracking technologies, ShoesLoc does not require the installation of wireless anchors, and has good robustness to environment changes such as the magnetic interference.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18502
        },
        {
          "affiliations": [],
          "personId": 8440
        },
        {
          "affiliations": [],
          "personId": 9864
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 4828,
      "typeId": 11437,
      "title": "iVR: Integrated Vision and Radio Localization with Zero Human Effort",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphone localization is essential to a wide range of applications in shopping malls, museums, office buildings, and other public places. Existing solutions relying on radio fingerprints and/or inertial sensors suffer from large location errors and considerable deployment efforts. We observe an opportunity in the recent trend of increasing numbers of security surveillance cameras installed in indoor spaces to overcome these limitations and revisit the problem of smartphone localization with a fresh perspective. However, fusing vision-based and radio-based systems is non-trivial due to the absence of absolute location, incorrespondence of identification and looseness of sensor fusion. This study proposes iVR, an integrated vision and radio localization system that achieves sub-meter accuracy with indoor semantic maps automatically generated from only two surveillance cameras, superior to precedent systems that require manual map construction or plentiful captured images. iVR employs a particle filter to fuse raw estimates from multiple systems, including vision, radio, and inertial sensor systems. By doing so, iVR outputs enhanced accuracy with zero start-up costs, while overcoming the respective drawbacks of each individual sub-system. We implement iVR on commodity smartphones and validate its performance in five different scenarios. The results show that iVR achieves a remarkable localization accuracy of 0.7m, outperforming the state-of-the-art systems by >70%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23667
        },
        {
          "affiliations": [],
          "personId": 12184
        },
        {
          "affiliations": [],
          "personId": 19287
        },
        {
          "affiliations": [],
          "personId": 8756
        },
        {
          "affiliations": [],
          "personId": 23906
        },
        {
          "affiliations": [],
          "personId": 23276
        },
        {
          "affiliations": [],
          "personId": 14431
        },
        {
          "affiliations": [],
          "personId": 17887
        }
      ],
      "sessionIds": [
        1177
      ],
      "eventIds": []
    },
    {
      "id": 5858,
      "typeId": 11437,
      "title": "Unsupervised Domain Adaptation for Robust Sensory Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Despite significant advances in the performance of sensory inference models, their poor robustness to changing environmental conditions and hardware remains a major hurdle for widespread adoption. In this paper, we introduce the concept of unsupervised domain adaptation which is a technique to adapt sensory inference models to new domains only using unlabeled data from the target domain. We present two case-studies to motivate the problem and highlight some of our recent work in this space. Finally, we discuss the core challenges in this space that can trigger further ubicomp research on this topic.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16356
        },
        {
          "affiliations": [],
          "personId": 18048
        },
        {
          "affiliations": [],
          "personId": 21061
        },
        {
          "affiliations": [],
          "personId": 11583
        },
        {
          "affiliations": [],
          "personId": 23587
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2787,
      "typeId": 11437,
      "title": "Emotional Prosthesis for Animating Awe through Performative Biofeedback",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Awe is a heightened emotional state of fear and wonder that creates a physiological response resulting in a cascade of hairs standing on end, also known as piloerection or goosebumps. This latent sense once served an animalian purpose of survival, but now lies dormant and is often not experienced consciously. In fact, 55 percent of the population reports to not feel this sensation that is noted to be healthy. The AWE Goosebumps artifact is an emotion prosthesis that animates the latent sensation of awe for embodiment and externalizes cues for communication. As the sensation is not experienced consciously, the techno fashion invites an opportunity to be a second skin for frisson biofeedback, behavior training, and expression to others as a tool to transform the doldrums of modern day to performative states of wonder.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21735
        },
        {
          "affiliations": [],
          "personId": 10994
        },
        {
          "affiliations": [],
          "personId": 8344
        },
        {
          "affiliations": [],
          "personId": 9719
        },
        {
          "affiliations": [],
          "personId": 22586
        },
        {
          "affiliations": [],
          "personId": 18802
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7909,
      "typeId": 11437,
      "title": "Continuous Stress Detection Using the Sensors of Commercial Smartwatch",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Stress detection is becoming a popular field in machine learning and this study focuses on recognizing stress using the sensors of commercially available smartwatches. In most of the previous studies, stress detection is based on partly or fully on electrodermal activity sensor (EDA). However, if the final aim of the study is to build a smartwatch application, using EDA signal is problematic as the smartwatches currently in the market do not include sensor to measure EDA signal. Therefore, this study surveys what sensors the smartwatches currently in the market include, and which of them 3rd party developers have access to. Moreover, it is studied how accurately stress can be detected user-independently using different sensor combinations. In addition, it is studied how detection rates vary between study subjects and what kind of effect window-size has to the recognition rates. All of the experiments are based on publicly available WESAD dataset. The results show that, indeed, EDA signal is not necessary when detecting stress user-independently, and therefore, commercial smartwatches can be used for recognizing stress when the used window length is big enough. However, it is also noted that recognition rate varies a lot between the study subjects.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20007
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5865,
      "typeId": 11437,
      "title": "Visualizing Health with Emotion Polarity History Using Voice",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Maintaining mental health as well as physical health is essential for our daily lives. We think that we could use a “mood” meter every day or regularly to see our own mental health condition similarly as in the case of weight meters. When performing emotion recognition using human speech, one of linguistic information and prosodic features included in speech is often used. However, by capturing both sides of speech, which is a means of human communication, it is considered that emotion recognition can be more accurately realized. Based on the background, we have developed PNViz, Positive-and-Negative Polarity Visualizer, an application running on an Android phone, to show the state of positive-ness of mental health by recording a short voice message. PNViz consists of the smartphone application and an analyzing server where the recorded voice is processed with both lexical and phonetic analyses and calculates a score ranging from -1 to 1. The calculated score is continuously logged and shown to the user and thus it is expected to encourage the user to take refreshing breaks or holidays.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8481
        },
        {
          "affiliations": [],
          "personId": 17874
        },
        {
          "affiliations": [],
          "personId": 9823
        },
        {
          "affiliations": [],
          "personId": 8415
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7915,
      "typeId": 11437,
      "title": "\"The Internet of What?\" Understanding Differences in Perceptions and Adoption for the Internet of Things",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This study explores people‚Äôs perceptions of and attitudes towards Internet of Things (IoT) devices and their resulting (non)adoption behaviors. Based on 38 interviews (19 pairs each consisting of a Millennial and their parent), we found that few had a clear understanding of IoT, even among those who had already adopted it. Rather, they relied on two distinct conceptual models of IoT that shaped their beliefs, concerns and adoption decisions: Many approached IoT with an ‚Äúuser-centric‚Äù technology mentality, viewing IoT devices as tools to be controlled by the end-user, and focusing on their tangible aspects (e.g. breakability). Others drew on an ‚Äúagentic‚Äù technology perspective, where IoT behaviors were device-driven and, at times, negotiated between the user, other people, and/or the IoT devices. Our study reveals that consumer-oriented IoT currently cater towards the agentic view and raise concerns for those coming from a user-centric perspective. We also found that generational differences in attitudes towards IoT were rather explained by these differing perspectives. Instead of following the trend towards greater automation and agentic modes of interaction, we advocate for a hybrid and personalized approach that supports a spectrum of agentic and user-centric perspectives and provide design recommendations to work towards this end.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21573
        },
        {
          "affiliations": [],
          "personId": 17295
        },
        {
          "affiliations": [],
          "personId": 13292
        },
        {
          "affiliations": [],
          "personId": 21084
        },
        {
          "affiliations": [],
          "personId": 9941
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 2796,
      "typeId": 11437,
      "title": "The Positive Impact of Push vs Pull Progress Feedback: A 6-week Activity Tracking Study in the Wild",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Lack of physical activity has been shown to increase disease and reduce life expectancy. In response, mobile devices are increasingly being used to support people's health and fitness by tracking physical activity. Prior work shows that the type of feedback, either ambient or via notification, affects users' behavior towards their physical activity. Yet, these phone- and watch-based interactions and notifications have primarily been visual in nature. Inspired by prior research, we explored the impact of feedback modality (visual, tactile, and hybrid: visual/tactile) on 44 participants' behavior and exercise mindset in a 6-week field study. We present the differences between modalities and the notion of push vs. pull for interface feedback and notifications. Across \\post{1,662}\ndays of study data, we found statistically significant impact of feedback modality and, in particular, the positive effects of Push feedback on participants' mindset about the process of exercise. Our results also highlight design guidelines for wearables and multimodal notification systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14237
        },
        {
          "affiliations": [],
          "personId": 22277
        },
        {
          "affiliations": [],
          "personId": 8421
        },
        {
          "affiliations": [],
          "personId": 19744
        },
        {
          "affiliations": [],
          "personId": 18468
        },
        {
          "affiliations": [],
          "personId": 16309
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 3821,
      "typeId": 11437,
      "title": "I'm listening: The Effect of Cue Difference to Elicit User's Continuous Turn-Taking with A.I. Agent in TV",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This study investigates the effect of different types of cues in terms of modality and message form applied in a turn-taking interaction with a virtual agent. We divided the message form to implicit and explicit, and modality to visual and auditory. The results from a 2 (message form: implicit vs. explicit) x 2 (modality: auditory vs. visual) between subject experiment revealed that implicit auditory cue has a significantly positive effect on inducing the perceived contingency and perceived intelligence in human-agent conversation. These results suggest that designing continuous turn-taking cue should be carefully crafted with elaborated research in various aspects such as social psychology and technical usability. Finally, Implication and limits of these findings are discussed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9384
        },
        {
          "affiliations": [],
          "personId": 24077
        },
        {
          "affiliations": [],
          "personId": 24172
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4846,
      "typeId": 11437,
      "title": "Following the Heart - What Does Variation of Resting Heart Rate Tell about Us as Individuals and as Population",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Resting heart rate (RHR) and heart rate variability (HRV) reflect the autonomic control of cardiac chronotropic activity, and they associate with cardiovascular fitness, acute and chronic health status, and mental stress. Relatively low RHR and relatively high HRV are generally seen as marks of better health, performance, and recovery levels. Nevertheless, the values are highly individual and comparison between individuals is not straightforward. On the other hand, evolution of wearable devices has made it possible to follow the course of individual RHR and HRV as long-term time series, which in turn enables observation of how behavioral, societal and seasonal factors affect RHR and HRV at individual and population scale. In this article, data measured by the Oura ring is used to study how alcohol and training affect these values, and moreover, how societal and seasonal factors affect us as a population.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20146
        },
        {
          "affiliations": [],
          "personId": 19129
        },
        {
          "affiliations": [],
          "personId": 10858
        },
        {
          "affiliations": [],
          "personId": 13171
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3825,
      "typeId": 11437,
      "title": "Identifying and Planning for Individualized Change: Patient-Provider Collaboration Using Lightweight Food Diaries in Healthy Eating and Irritable Bowel Syndrome",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Identifying and planning strategies that support a healthy lifestyle or manage a chronic disease often require patient-provider collaboration. For example, people with healthy eating goals often share everyday food, exercise, or sleep data with health coaches or nutritionists to find opportunities for change, and patients with irritable bowel syndrome (IBS) often gather food and symptom data as part of working with providers to diagnose and manage symptoms. However, a lack of effective support often prevents health experts from reviewing large amounts of data in time-constrained visits, prevents focusing on individual goals, and prevents generating correct, individualized, and actionable recommendations. To examine how to design photo-based diaries to help people and health experts exchange knowledge and focus on collaboration goals when reviewing the data together, we designed and developed Foodprint, a photo-based food diary. Foodprint includes three components: (1) A mobile app supporting lightweight data collection, (2) a web app with photo-based visualization and quantitative visualizations supporting collaborative reflection, and (3) a pre-visit note communicating an individual’s expectations and questions to experts. We deployed Foodprint in two studies: (1) with 17 people with healthy eating goals and 7 health experts, and (2) with 16 IBS patients and 8 health experts. Building upon the lens of boundary negotiating artifacts and findings from two field studies, our research contributes design principles to (1) prepare individuals to collect data relevant to their health goals and for collaboration, (2) help health experts focus on an individual’s eating context, experiences, and goals in collaborative review, and (3) support individuals and experts to develop individualized, actionable plans and strategies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9208
        },
        {
          "affiliations": [],
          "personId": 14566
        },
        {
          "affiliations": [],
          "personId": 23422
        },
        {
          "affiliations": [],
          "personId": 24250
        },
        {
          "affiliations": [],
          "personId": 20697
        },
        {
          "affiliations": [],
          "personId": 23909
        },
        {
          "affiliations": [],
          "personId": 19760
        }
      ],
      "sessionIds": [
        2269
      ],
      "eventIds": []
    },
    {
      "id": 6898,
      "typeId": 11437,
      "title": "Demo: A CSI-ratio Model Based House-level Respiration Monitoring System Using COTS WiFi Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The past few years have witnessed the great potential of exploiting channel state information (CSI) retrieved from COTS WiFi devices for respiration monitoring. However, existing approaches only work when the target is close to the WiFi transceivers and the performance degrades significantly when the target is far away. This sensing range constraint greatly limits the application of the proposed approaches in real life. Different from the existing approaches that apply the raw CSI readings of individual antenna for sensing, we employ the ratio of CSI readings from two antennas, whose noise is mostly canceled out by the division operation to significantly increase the sensing range. In this demo, we will demonstrate FarSense -- a CSI-ratio model based house-level real-time respiration monitoring system using COTS WiFi devices.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15582
        },
        {
          "affiliations": [],
          "personId": 22319
        },
        {
          "affiliations": [],
          "personId": 10893
        },
        {
          "affiliations": [],
          "personId": 18786
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3826,
      "typeId": 11437,
      "title": "MegaLight: Learning-based Color Adaptation for Barcode Stream Recognition over Screen-Camera Links",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Screen-camera communication using dynamic barcode streaming has emerged as a convenient and secure method for short-range, impromptu device-to-device communication. Conventional dynamic barcode systems adopt a rule-based approach to recognize the color barcode stream in the receiver, which is empirical, inflexible, and lacks self-adaptiveness. In this paper, we proposed a novel solution framework for color barcode stream recognition based on machine learning techniques.\nBy including a number of training frames into the barcode to build a classification model, the proposed framework can achieve high accuracy in color barcode recognition and is resilient to changes in the environment, such as differences in ambient light.\nA semi-supervised learning approach based on the Mixture of Experts (MoE) model was further proposed to reduce the start-up time. We implemented MegaLight on both black-white and color barcode systems. Extensive experiments showed that MegaLight can significantly reduce the frame demodulation error and reach up to 3 times improvement in system goodput comparing to the conventional barcode stream recognition approaches.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13654
        },
        {
          "affiliations": [],
          "personId": 21956
        },
        {
          "affiliations": [],
          "personId": 22519
        },
        {
          "affiliations": [],
          "personId": 19545
        }
      ],
      "sessionIds": [
        1025
      ],
      "eventIds": []
    },
    {
      "id": 2804,
      "typeId": 11437,
      "title": "Demo: Dyslexic and Private Reader An Eye-tracking Platform for Reading Interactions with Applications to Increase Empathy and Privacy",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present a demonstration based on a MobileHCI 2019 paper to use eye gaze to selectively render or obscure text. Obscuring is used to \"simulate\" a specific kind of dyslexia. Selectively render text is to give users a more private reading experience in public spaces.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19239
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4854,
      "typeId": 11437,
      "title": "Semi-supervised Learning for Human Activity Recognition using Adversarial Autoencoders",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "SHL recognition challenge 2019 goal is to recognize eight locomotion and transportation (activities) from the inertial sensor data of a smartphone. The dataset contains information from different mobile-phones placement (torso, bag, hips, hand). Participants must provide their predictions based on test data that contains Hand phone sensors information. Only a small amount of Hand phone labeled data exists in the validation data. Train data consists only of torso, bag and hips placed mobile devices. Team DB proposes to apply deep semi-supervised learning. As the base for our model, we have chosen Adversarial Autoencoder (AAE) and employ Convolutional Networks for feature extraction. We prove that semi-supervised learning gives possibility to utilize test unlabeled data during AAE training with small amount of validation labeled data and achieve high model accuracy for Human Activity Recognition task.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23777
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2807,
      "typeId": 11437,
      "title": "How Does a Nation Walk? Interpreting Large-Scale Step Count Activity with Weekly Streak Patterns",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Activity trackers are being deployed in large-scale physical activity intervention programs, but analyzing them is difficult due to the large data size and complexity. As large datasets of steps become more available, it is paramount to develop analysis methods to more deeply interpret them to understand the variety and changing nature of human steps behavior. In this work, we explored ways to analyze the heterogeneous steps activity data and propose a framework of dimensions and time aggregations to interpret how activity trackers with monetary incentives influence the steps behavior of a city-wide population. We analyzed the daily step counts of 140,000 individuals walking a combined 74 billion steps in 305 days during a city-wide public health campaign. We performed data mining clustering to segment users into 16 types of users with various types of walking behaviors and demonstrate that these clusters help with interpreting how some user increased their steps level. Our findings inform how to scalably interpret large steps data to draw behavioral insights from intervention studies and to predictively anticipate user outcomes from their first few days of tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12841
        },
        {
          "affiliations": [],
          "personId": 12915
        },
        {
          "affiliations": [],
          "personId": 18299
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 6905,
      "typeId": 11437,
      "title": "ProxiTalk: Activate Speech Input by Bringing Smartphone to the Mouth",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Speech input, such as voice assistant and voice message, is an attractive interaction option for mobile users today. However, despite its popularity, there is a use limitation for smartphone speech input: users need to press a button or say a wake word to activate it before use, which is not very convenient. To address it, we match the motion that brings the phone to mouth with the user's intention to use voice input. In this paper, we present ProxiTalk, an interaction technique that allows users to enable smartphone speech input by simply moving it close to their mouths. We study how users use ProxiTalk and systematically investigate the recognition abilities of various data sources (e.g., using a front camera to detect facial features, using two microphones to estimate the distance between phone and mouth). Results show that it is feasible to utilize the smartphone's built-in sensors and instruments to detect ProxiTalk use and classify gestures. An evaluation study in the wild shows that users can quickly acquire ProxiTalk and are willing to use it. In conclusion, our work provides the empirical support that ProxiTalk is a practical and promising option to enable smartphone speech input, which coexists with current trigger methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11539
        },
        {
          "affiliations": [],
          "personId": 14464
        },
        {
          "affiliations": [],
          "personId": 12425
        },
        {
          "affiliations": [],
          "personId": 15758
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 6906,
      "typeId": 11437,
      "title": "City-Scale Vehicle Tracking and Traffic Flow Estimation using Low Frame-Rate Traffic Cameras",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Vehicle flow estimation has many potential smart cities and transportation applications. Many cities have existing camera networks which broadcast image feeds; however, the resolution and frame-rate are too low for existing computer vision algorithms to accurately estimate flow. In this work, we present a computer vision and deep learning framework for vehicle tracking. We demonstrate a novel tracking pipeline which enables accurate flow estimates in a range of environments under low resolution and frame-rate constraints. We demonstrate that our system is able to track vehicles in New York City's traffic camera video feeds at 1 Hz or lower frame-rate, and produces higher traffic flow accuracy than popular open source tracking frameworks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18309
        },
        {
          "affiliations": [],
          "personId": 19004
        },
        {
          "affiliations": [],
          "personId": 9172
        },
        {
          "affiliations": [],
          "personId": 18863
        },
        {
          "affiliations": [],
          "personId": 19065
        },
        {
          "affiliations": [],
          "personId": 12880
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4858,
      "typeId": 11437,
      "title": "Deep Multi-Task Learning based Urban Air Quality Index Modelling",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Obtaining comprehensive air quality information can help protect human health from air pollution. Existing spatially fine-grained estimation methods and forecasting methods have the following problems: 1) Only a part of data related to air quality is considered. 2) Features are defined and extracted artificially. 3) Due to the lack of training samples, they usually cannot achieve good generalization performance. Therefore, we propose a deep multi-task learning (MTL) based urban air quality index (AQI) modelling method (PANDA). On one hand, a variety of air quality-related urban big data (meteorology, traffic, factory air pollutant emission, point of interest (POI) distribution, road network distribution, etc.) are considered. Deep neural networks are used to learn the representations of these relevant spatial and sequential data, as well as to build the correlation between AQI and these representations. On the other hand, PANDA solves spatially fine-grained AQI level estimation task and AQI forecasting task jointly, which can leverage the commonalities and differences between these two tasks to improve generalization performance. We evaluate PANDA on the dataset of Hangzhou city. The experimental results show that our method can yield a better performance compared to the state-of-the-art methods.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23962
        },
        {
          "affiliations": [],
          "personId": 20636
        },
        {
          "affiliations": [],
          "personId": 14270
        },
        {
          "affiliations": [],
          "personId": 17810
        },
        {
          "affiliations": [],
          "personId": 18632
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 4859,
      "typeId": 11437,
      "title": "Leveraging Routine Behavior and Contextually-Filtered Features for Depression Detection among College Students",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The rate of depression in college students is rising, which is known to increase suicide risk, lower academic performance and double the likelihood of dropping out. Researchers have used passive mobile sensing technology to assess mental health. Existing work on fnding relationships between mobile sensing and depression, as well as identifying depression via sensing features, mainly utilize single data channels or simply concatenate multiple channels. There is an opportunity to identify better features by reasoning about co-occurrence across multiple sensing channels. We present a new method to extract contextually fltered features on passively collected, time-series data from mobile devices via rule mining algorithms. We frst employ association rule mining algorithms on two diﬀerent user groups (e.g., depression vs. non-depression). We then introduce a new metric to select a subset of rules that identifes distinguishing behavior patterns between the two groups. Finally, we consider co-occurrence across the features that comprise the rules in a feature extraction stage to obtain contextually fltered features with which to train classifers. Our results reveal that the best model with these features signifcantly outperforms a standard model that uses unimodal features by an average of 9.7% across a variety of metrics. We further verifed the generalizability of our approach on a second dataset, and achieved very similar results.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16329
        },
        {
          "affiliations": [],
          "personId": 12988
        },
        {
          "affiliations": [],
          "personId": 18793
        },
        {
          "affiliations": [],
          "personId": 14258
        },
        {
          "affiliations": [],
          "personId": 15237
        },
        {
          "affiliations": [],
          "personId": 15155
        },
        {
          "affiliations": [],
          "personId": 23268
        },
        {
          "affiliations": [],
          "personId": 11283
        },
        {
          "affiliations": [],
          "personId": 10892
        },
        {
          "affiliations": [],
          "personId": 12575
        },
        {
          "affiliations": [],
          "personId": 19714
        },
        {
          "affiliations": [],
          "personId": 18197
        }
      ],
      "sessionIds": [
        2491
      ],
      "eventIds": []
    },
    {
      "id": 7936,
      "typeId": 11437,
      "title": "A Language for Online State Processing of Binary Sensors, Applied to Ambient Assisted Living",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "There is a large variety of binary sensors in use today, and useful context-aware services can be defined using such binary sensors. However, the currently available approaches for programming context-aware services do not conveniently support binary sensors. Indeed, no existing approach simultaneously supports a notion of state, central to binary sensors, offers a complete set of operators to compose states, allows to define reusable abstractions by means of such compositions, and implements efficient online processing of these operators. \\ This paper proposes a new language for event processing specifically targeted to binary sensors. The central contributions of this language are a native notion of state and semi-causal operators for temporal state composition including: Allen‚Äôs interval relations generalized for handling multiple intervals, and temporal filters for handling delays. Compared to other approaches such as CEP (complex event processing), our language provides less discontinued information, allows less restricted compositions, and supports reusable abstractions. We implemented an interpreter for our language and applied it to successfully rewrite a full set of real Ambient Assisted Living services. The performance of our prototype interpreter is shown to compete well with a commercial CEP engine when expressing the same services.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14468
        },
        {
          "affiliations": [],
          "personId": 9678
        },
        {
          "affiliations": [],
          "personId": 12680
        },
        {
          "affiliations": [],
          "personId": 10087
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 4865,
      "typeId": 11437,
      "title": "Let There Be IMU Data: Generating Training Data for Wearable, Motion Sensor Based Activity Recognition from Monocular RGB Videos",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recent advances in Machine Learning, in particular Deep Learning have been driving rapid progress in fields such as computer vision and natural language processing. Human activity recognition (HAR) using wearable sensors, which has been a thriving research field for the last 20 years, has benefited much less from such advances. This is largely due to the lack of  adequate amounts of labeled training data.\nIn this paper we propose a method to mitigate the labeled data problem in wearable HAR  by generating wearable motion data from monocular RGB videos,\nwhich can be collected from popular video platforms such as YouTube. Our method works by extracting 2D poses from video frames and then using a regression model to map them to sensor signals. We have validated it on fitness exercises as the domain for which activity recognition is trained and shown that we can improve a HAR recognition model using data that was produced from a YouTube video.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17928
        },
        {
          "affiliations": [],
          "personId": 10155
        },
        {
          "affiliations": [],
          "personId": 13703
        },
        {
          "affiliations": [],
          "personId": 18666
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2818,
      "typeId": 11437,
      "title": "Classifying Attention Types with Thermal Imaging and Eye Tracking",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Despite the importance of attention in user performance, current methods for attention classification do not allow to discriminate between different attention types. We propose a novel method that combines thermal imaging and eye tracking to unobtrusively classify four types of attention: sustained, alternating, selective, and divided. We collected a data set in which we stimulate these four attention types in a user study (N = 22) using combinations of audio and visual stimuli while measuring users’ facial temperature and eye movement. Using a Logistic Regression on features extracted from both sensing technologies, we can classify the four attention types with high AUC scores up to 75.7% user independent-condition independent, 87% for the user-independent-condition dependent, and 77.4% for user-dependent. Our findings not only demonstrate the potential of thermal imaging and eye tracking for unobtrusive classification of different attention types but also pave the way for novel applications for attentive user interfaces and attention-aware computing.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17230
        },
        {
          "affiliations": [],
          "personId": 12557
        },
        {
          "affiliations": [],
          "personId": 24251
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 11079
        },
        {
          "affiliations": [],
          "personId": 21175
        },
        {
          "affiliations": [],
          "personId": 23018
        },
        {
          "affiliations": [],
          "personId": 19379
        },
        {
          "affiliations": [],
          "personId": 8605
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 3843,
      "typeId": 11437,
      "title": "Challenges in Supporting Social Practices around Personal Data for Long-Term Mental Health Management",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We discuss the importance of designing self-tracking technologies for serious mental illness (SMI) that allow individuals with SMI to collect, share, and sense-make over data with a dynamic set of support system members. Our collaborative work with individuals diagnosed with bipolar disorder has suggested the following design and technical challenges for supporting social practices around personal data in long-term mental health management: allowing for fine-grained control over data disclosure by individuals with SMI, supporting dynamism in relationships and roles over long-term use of a system, and allowing individuals flexibility in the variables that they self-track. We discuss these challenges and how they relate to the goals of predictive modelling and intervention in mental health personal informatics systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14280
        },
        {
          "affiliations": [],
          "personId": 13545
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3847,
      "typeId": 11437,
      "title": "Wearables for Health: Developing Designs for Functional Practicality",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 15550
        },
        {
          "affiliations": [],
          "personId": 9879
        },
        {
          "affiliations": [],
          "personId": 15010
        },
        {
          "affiliations": [],
          "personId": 16373
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6922,
      "typeId": 11437,
      "title": "AI-Mediated Gaze-Based Intention Recognition for Smart Eyewear: Opportunities & Challenges",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 24251
        },
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 22950
        },
        {
          "affiliations": [],
          "personId": 8984
        },
        {
          "affiliations": [],
          "personId": 19379
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7947,
      "typeId": 11437,
      "title": "Gesture Recognition Method with Acceleration Data Weighted by sEMG",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a gesture recognition method with acceleration data weighted by sEMG. Acceleration and sEMG are collected as training data, and gestures are recognized using only acceleration as input data. The dynamic time warping (DTW) algorithm is used for the distance calculation. Three axis acceleration data and sEMG were collected for three types of baseball pitching forms: overarm, sidearm, and underarm beginning from three types of preliminary motions: no-windup, quick, and windup. We investigate the changes in sEMG during pitching motion. The distance calculation method is changed according to the sEMG amplitude, reducing the influence of unstable motions. In evaluation experiments, the proposed method achieved higher accuracy than the comparison method that does not use sEMG for windup form, even if the number of training data changes.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12601
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4876,
      "typeId": 11437,
      "title": "iSCAN: Automatic Speaker Adaptation via Iterative Cross-modality Association",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Speaker recognition is a key component for emerging Internet of Things (IoT) smart services, such as voice-control and personalized applications. Although speaker recognition systems can attain excellent performance on synthetic datasets, operation in the real-world can lead to a significant degradation in performance. The key reason for this is the lack of enough labeled datasets for model adaptation, primarily due to the cost of manual annotation and enrollment. A recent solution to this problem is to use cross-modal identifiers e.g. WiFi sniffing to gradually associate an identity with a certain vocal feature e.g. Simultaneous Clustering and Naming (SCAN). In this paper we demonstrate how to further improve performance of these cross-modal systems in the wild by iteratively adapting the feature extractor based on the output of the noisy association and clustering step. We show how this feedback loop can not only improve overall accuracy, but also labeling coverage in association result. iSCAN is a further step towards a robust and zero-effort speaker recognition system for the IoT.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15289
        },
        {
          "affiliations": [],
          "personId": 17525
        },
        {
          "affiliations": [],
          "personId": 11555
        },
        {
          "affiliations": [],
          "personId": 23544
        },
        {
          "affiliations": [],
          "personId": 16133
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7949,
      "typeId": 11437,
      "title": "Poster: IoT Skullfort: Exploring the Impact of Internet Connected Cosplay",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we explore the potential impact of Internet of Things (IoT) technology may have on the cosplay community. We developed a costume (an IoT Skullfort) and embedded IoT technology to enhance its capabilities and user interactions.  Sensing technologies are widely used in many different wearable domains including cosplay scenarios. However, in most of these scenarios, typical interaction pattern is that the costume responds to its environment or the player's behaviour (e.g., colour of  lights may get changed when  player moves hands). In contrast, our research focuses on exploring scenarios where the audience (third party) get to manipulate the costume behaviour (e.g., the audience get to change the colour of the Skullfort using a mobile application).  We believe such an audience (third party) influenced cosplay brings new opportunities for enhanced entertainment. However, it also creates significant challenges. We report the results gathered through a focus group conducted in collaboration with cosplay community experts.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21411
        },
        {
          "affiliations": [],
          "personId": 21117
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6932,
      "typeId": 11437,
      "title": "Audio-Visual TED Corpus: Enhancing the TED-LIUM Corpus with Facial Information, Contextual Text and Object Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present a variety of new visual features in extension to the TED-LIUM corpus. We re-aligned the original TED talk audio transcriptions with official TED.com videos. By utilizing state-of-the-art models for face and facial landmarks detection, optical character recognition, object detection and classification, we extract four new visual features that can be used for Large-Vocabulary Continuous Speech Recognition (LVCSR) systems, including facial images and landmarks and text and objects in the scene. The facial images and landmarks can be used in combination with audio for audio-visual acoustic modeling where the visual modality provides robust features in adverse acoustic environments. The contextual information, i.e., extracted text and detected objects in the scene can be used as prior knowledge to create contextual language models. Experimental results showed the efficacy of using visual features on top of acoustic features for speech recognition in overlapping speech scenarios.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10885
        },
        {
          "affiliations": [],
          "personId": 22090
        },
        {
          "affiliations": [],
          "personId": 11879
        },
        {
          "affiliations": [],
          "personId": 16550
        },
        {
          "affiliations": [],
          "personId": 13137
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3860,
      "typeId": 11437,
      "title": "EyeControl: Wearable Assistance for Industrial Maintenance Tasks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 9822
        },
        {
          "affiliations": [],
          "personId": 18778
        },
        {
          "affiliations": [],
          "personId": 11035
        },
        {
          "affiliations": [],
          "personId": 8689
        },
        {
          "affiliations": [],
          "personId": 23486
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4884,
      "typeId": 11437,
      "title": "Predicting Episodes of Non-Conformant Mobility in Indoor Environments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Traditional mobility prediction literature focuses primarily on improved methods to extract latent patterns from individual-specific movement data. When such predictions are incorrect, we ascribe it to `random' or `unpredictable' changes in a user's movement behavior. Our hypothesis, however, is that such apparently-random deviations from daily movement patterns can, in fact, often be anticipated. In particular, we develop a methodology for predicting Likelihood of Future Non-Conformance (LFNC), based on two central hypotheses: (a) the likelihood of future deviations in movement behavior is positively correlated to the intensity of such trajectory deviations observed in the user's recent past, and (b) the likelihood of such future deviations increases if the user's strong-ties have also recently exhibited such non-conformant movement behavior. We use extensive longitudinal indoor location data (spanning 4+ months) from an urban university campus to validate these hypotheses, and then show that these features can be used to build an accurate non-conformance predictor: it can predict non-conformant mobility behavior two hours in advance with an AUC greater than 0.85, significantly outperforming the baseline. We also show that this prediction methodology holds for a representative outdoor public-transport based mobility dataset. Finally, we use a real-world mobile crowdsourcing application to show the practical impact of such non-conformance: failure to identify such likely anomalous movement behavior causes workers to suffer a noticeable drop in task completion rates and reduces the spatial spread of successfully completed tasks.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23232
        },
        {
          "affiliations": [],
          "personId": 12552
        }
      ],
      "sessionIds": [
        1040
      ],
      "eventIds": []
    },
    {
      "id": 7957,
      "typeId": 11437,
      "title": "Boosting Word Recognition for Vibrotactile Skin Reading",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Proficiency in any form of reading requires a considerable amount of practice. With exposure, people get better at recognising words, because they develop strategies that enable them to read faster. This paper describes a study investigating recognition of words encoded with a 6-channel vibrotactile display. We train 22 users to recognise ten letters of the English alphabet. Additionally, we repeatedly expose users to 12 words in the form of training and reinforcement testing. Then, we test participants on exposed and unexposed words to observe the effects of exposure to words. Our study shows that, with exposure to words, participants did significantly improve on recognition of exposed words. \nThe findings suggest that such a word exposure technique could be used during the training of novice users in order to boost the word recognition of a particular dictionary of words.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13778
        },
        {
          "affiliations": [],
          "personId": 13682
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 2839,
      "typeId": 11437,
      "title": "Poster: Estimation of Student's Engagement Based on the Posture",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "It is important for teachers to grasp students' engagement in order to improve the quality of lectures.\nHowever, in the e-learning environment, there is no teacher to grasp the students' engagement and it may cause ineffective learning.\nThe purpose of this study is to grasp the students' engagement by using a pressure mat and web camera. \nWe recorded students' postural data, that is upper body pressure distribution and upper body pose, during e-learning lectures. \nThen we extracted 38 features from upper body pressure distribution and 33 features from upper body pose for every minute, selected proper features and trained classifiers to estimate whether he or she was engaged in the lecture. \nAs a result, the average accuracy was 79.3\\% for student-dependent estimation.  \nThis result shows it is possible to predict the student's engagement automatically.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14130
        },
        {
          "affiliations": [],
          "personId": 20052
        },
        {
          "affiliations": [],
          "personId": 11112
        },
        {
          "affiliations": [],
          "personId": 14504
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2841,
      "typeId": 11437,
      "title": "WLCSSCuda: A CUDA Accelerated Template Matching Method for Gesture Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Template matching methods can benefit from multi-cores architecture in order to parallelise and accelerate the matching of multiple templates. We present WLCSSCuda: a GPU accelerated implementation of the Warping Longest Common Subsequence (WLCSS) pattern recognition algorithm. We evaluate our method on 4 NVIDIA GPUs and 4 multi-cores CPUs. We observe a 67-times speedup for the GPU implementation in the best case against the multithreaded CPU implementation.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20416
        },
        {
          "affiliations": [],
          "personId": 14916
        }
      ],
      "sessionIds": [
        2196
      ],
      "eventIds": []
    },
    {
      "id": 2843,
      "typeId": 11437,
      "title": "Enhancing Support for Optimal Muscle Usage in Sports: Coaching and Skill-Improvement Tracking with sEMG",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Cardiopulmonary function and power as well as efficient motion skill are extremely important for athletes. Thanks to the latest sensing technology and smart devices, many researchers have focused on sports-skill analysis. Electromyography (EMG), in particular, is gaining attention as a method of understanding the power-generating process in motions. However, most existing applications using EMG have remained being one-time measurement. This is because athletes do not know how to use the results and how to measure their improvement. We propose a sports-skill-training framework with muscle-usage indicators based on EMG and an EMG live visualization system. With this framework, athletes can determine the skill they need to improve by focusing on skills whose indicators are poor, activate their muscles with live feedback to overcome weaknesses, and quantitatively measure their improvement as the improvement of the indicators during the activation training. We also verified the effect of coaching in this framework on cycling athletes. The experimental results quantitatively indicate the effectiveness of continuous skill training with our framework.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21248
        },
        {
          "affiliations": [],
          "personId": 22457
        },
        {
          "affiliations": [],
          "personId": 21853
        },
        {
          "affiliations": [],
          "personId": 23218
        },
        {
          "affiliations": [],
          "personId": 14502
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 2844,
      "typeId": 11437,
      "title": "EyeWear 2019: Third Workshop on EyeWear Computing - Focus: Social Interactions",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 23757
        },
        {
          "affiliations": [],
          "personId": 10481
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5917,
      "typeId": 11437,
      "title": "Demo: CityAtmosphere: VR Image to Glimpse Wishes in the Air",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a newly UX virtual reality demonstration program that provides people immersive urban experience, i.e. helps people understand characteristics of the city atmosphere from big data analysis on GPS location logs and search query logs. In contrast to the other demonstration systems that show the characteristics of areas of interests by using tag cloud, or VR systems using 3D computer graphics, our system synthesizes both the functionalities by showing 3D point clouds, where each particle shows search queries made by people staying at the area of interests. This synthesis invokes people feel the atmosphere of the city while traditional VR systems could not offer it. To make demonstration system effective, a new feature extraction process on search query logs is proposed by focusing on the users who visit an area of interests. In the demonstration at the conference, visitors could enjoy immersive urban experience with VR headset. Furthermore, the paper also shows that our empirical evaluation on our new feature representation based on search query logs.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23655
        },
        {
          "affiliations": [],
          "personId": 19342
        },
        {
          "affiliations": [],
          "personId": 13926
        },
        {
          "affiliations": [],
          "personId": 18271
        },
        {
          "affiliations": [],
          "personId": 22592
        },
        {
          "affiliations": [],
          "personId": 10470
        },
        {
          "affiliations": [],
          "personId": 20380
        },
        {
          "affiliations": [],
          "personId": 20727
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2846,
      "typeId": 11437,
      "title": "PDMove: Towards Passive Medication Adherence Monitoring of Parkinson’s Disease Using Smartphone-based Gait Assessment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The medicine adherence in Parkinson's disease (PD) treatment has attracted tremendous attention due to the critical consequences it can lead to otherwise. As a result, clinics need to ensure that the medicine intake is performed on time. Existing approaches, such as self-report, family reminder, and pill counts, heavily rely on the patients themselves to log the medicine intake (hereafter, patient involvement). Unfortunately, PD patients usually suffer from impaired cognition or memory loss, which leads to the so-called medication non-adherence, including missed doses, extra doses, and mistimed doses. These instances can nullify the treatment or even harm the patients. In this paper, we present \\textit{PDMove}, a smartphone-based passive sensing system to facilitate medication adherence monitoring without the need for patient involvement. Specifically, \\textit{PDMove} builds on the fact that PD patients will present gait abnormality if they don't follow medication treatment. To begin with, \\textit{PDMove} passively collects gait data while putting the smartphone in the pocket. Afterward, the gait preprocessor helps extract gait cycle containing the Parkinsonism-related biomarkers. Finally, the medicine intake detector consisting of a multi-view convolutional neural network predicts the medicine intake. In this way, \\textit{PDMove} enables the monitoring of medication adherence. To evaluate \\textit{PDMove}, we enroll 247 participants with PD and collect more than 100,000 gait cycle samples. Our results show that smartphone-based gait assessment is a feasible approach to the AI-care strategy to monitor the medication adherence of PD patients.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17178
        },
        {
          "affiliations": [],
          "personId": 14847
        },
        {
          "affiliations": [],
          "personId": 23863
        },
        {
          "affiliations": [],
          "personId": 19774
        },
        {
          "affiliations": [],
          "personId": 22256
        },
        {
          "affiliations": [],
          "personId": 19903
        },
        {
          "affiliations": [],
          "personId": 10157
        },
        {
          "affiliations": [],
          "personId": 13453
        },
        {
          "affiliations": [],
          "personId": 9038
        },
        {
          "affiliations": [],
          "personId": 21332
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4895,
      "typeId": 11437,
      "title": "Measuring Parkinson's Disease Motor Symptoms with Smartphone-based Drawing Tasks",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Parkinson's disease (PD) patients' motor functionalities are measured by various tests. Spiral drawing is one of the proven techniques for assessing the severity of PD motor symptoms. Commonly the test is performed with pen and paper, with the following visual observation by a clinician. This paper describes the implementation of the digitized version of the spiral drawing test for Android devices. Moreover, the application extends the spiral test and utilizes square-shape drawing accordingly. This artifact was tested in a trial with 8 PD patients and 6 age matching controls. The results have shown the observable difference in performance between PD and non-PD users in drawing accuracy and speed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16720
        },
        {
          "affiliations": [],
          "personId": 22338
        },
        {
          "affiliations": [],
          "personId": 13432
        },
        {
          "affiliations": [],
          "personId": 17061
        },
        {
          "affiliations": [],
          "personId": 10225
        },
        {
          "affiliations": [],
          "personId": 15187
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3871,
      "typeId": 11437,
      "title": "Recurrent Network based Automatic Detection of Chronic Pain Protective Behavior using MoCap and sEMG data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In chronic pain physical rehabilitation, physiotherapists adapt exercise sessions according to the movement behavior of patients. As rehabilitation moves beyond clinical sessions, technology is needed to similarly assess movement behaviors and provide such personalized support. In this paper, as a first step, we investigate automatic detection of protective behavior (movement behavior due to pain-related fear or pain) based on wearable motion capture and electromyography sensor data. We investigate two recurrent networks (RNN) referred to as stacked-LSTM and dual-stream LSTM, which we compare with related deep learning (DL) architectures. We further explore data augmentation techniques and additionally analyze the impact of segmentation window lengths on detection performance. The leading performance of 0.815 mean F1 score achieved by stacked-LSTM provides important grounding for the development of wearable technology to support chronic pain physical rehabilitation during daily activities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13447
        },
        {
          "affiliations": [],
          "personId": 12248
        },
        {
          "affiliations": [],
          "personId": 16356
        },
        {
          "affiliations": [],
          "personId": 23876
        },
        {
          "affiliations": [],
          "personId": 11583
        },
        {
          "affiliations": [],
          "personId": 9605
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 7968,
      "typeId": 11437,
      "title": "Facial Expression Recognition Using Ear Canal Transfer Function",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we propose a new input method for mobile and wearable computing using facial expressions. Facial muscle movements induce physical deformation in the ear canal. Our system utilizes such characteristics and estimates facial expressions using the ear canal transfer function (ECTF). Herein, a user puts on earphones with an equipped microphone that can record an internal sound of the ear canal. The system transmits ultrasonic band-limited swept sine signals and acquires the ECTF by analyzing the response. An important novelty feature of our method is that it is easy to incorporate into a product because the speaker and the microphone are equipped with many hearables, which is technically advanced electronic in-ear-device designed for multiple purposes. We investigated the performance of our proposed method for 21 facial expressions with 11 participants. Moreover, we proposed a signal correction method that reduces positional errors caused by attaching/detaching the device. The evaluation results confirmed that the f-score was 40.2% for the uncorrected signal method and 62.5% for the corrected signal method. We also investigated the practical performance of six facial expressions and confirmed that the f-score was 74.4% for the uncorrected signal method and 90.0% for the corrected signal method. We found the ECTF can be used for recognizing facial expressions with high accuracy equivalent to other related work.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10477
        },
        {
          "affiliations": [],
          "personId": 20318
        },
        {
          "affiliations": [],
          "personId": 22560
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 3872,
      "typeId": 11437,
      "title": "User Experiences of Garment-Based Dynamic Compression for Novel Haptic Applications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Compression is a haptic stimulus used in medical interventions (e.g., compression stockings) and has the potential to be integrated into new research areas (e.g., immersive VR experiences, distributed notification mechanisms), yet remains largely understudied. This work investigates the user experience of compression garment technologies that are dynamic, remotely controllable, and low mass, to better address this research gap. Shape memory alloy-based compression garments, capable of creating spatially- and temporally- dynamic on-body compression, were designed and deployed in a user study (n=17, 8M/9F) to understand the effects of compression, and to draw insights for future compression-based applications. The major takeaways are: (1) importance of and sizing/fit, (2) individual/gender preferences and need for customizability, and (3) the relationship between context-specific stimulation and perception.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22572
        },
        {
          "affiliations": [],
          "personId": 8497
        },
        {
          "affiliations": [],
          "personId": 17098
        },
        {
          "affiliations": [],
          "personId": 10049
        },
        {
          "affiliations": [],
          "personId": 23059
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 4899,
      "typeId": 11437,
      "title": "Quadmetric Optimized Thumb-to-Finger Interaction for Force Assisted One-Handed Text Entry on Mobile Headsets",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Augmented reality head-worn computers often feature small-sized touch interfaces that complicate interaction with content, provide insufficient space for comfortable text input, and can be awkward to use in social situations. This paper presents a novel one-handed thumb-to-finger text entry solution for augmented reality head-worn computers. We design a glove composed of 12 force-sensitive nodes featuring an ambiguous keyboard layout. We first explore the viability of force disambiguation to evaluate the force division within the force spectrum. We select a 3-level force division as it allows to considerably reduce the number of keys while featuring a high (83.9%) accuracy. Following this pilot study, we map the 26 English characters onto the 9 nodes located on the index, middle and ring fingers in a 3-3-3 configuration, and attribute the space, enter and backspace functions to the remaining three nodes. We consider text entry performance as a quadmetric optimization problem considering the following criteria: goodness of character pairs, layout similarity to the QWERTY keyboard, easiness of force interaction, and comfort level of thumb reach. The resulting layout strikes a balance between performance and usability. We finally evaluate the quadmetric optimized layout over 6 sessions with 12 participants. The participants achieve an average text entry rate of 6.47 WPM with 6.85% error rate in the final session, which is significantly faster than existing thumb-to-finger solutions. In addition, our one-handed text entry system enhances the user mobility compared to other state-of-the-art solutions by freeing one hand, while allowing the user to direct his visual attention to other activities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9835
        },
        {
          "affiliations": [],
          "personId": 17032
        },
        {
          "affiliations": [],
          "personId": 14160
        },
        {
          "affiliations": [],
          "personId": 20009
        },
        {
          "affiliations": [],
          "personId": 18139
        },
        {
          "affiliations": [],
          "personId": 11677
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 6948,
      "typeId": 11437,
      "title": "Challenges and Opportunities of Textile Based Smart Sanitary Napkin Design",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 18385
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7972,
      "typeId": 11437,
      "title": "Knocker: Vibroacoustic-based Object Recognition with Smartphones",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "While smartphones have enriched our lives with diverse applications and functionalities, the user experience still often involves manual cumbersome inputs. To purchase a bottle of water for instance, a user must locate an e-commerce app, type the keyword for a search, select the right item from the list, and finally place an order. This process could be greatly simplified if the smartphone identifies the object of interest and automatically executes the user preferred actions for the object. We present Knocker that identifies the object when a user simply knocks on an object with a smartphone. The basic principle of Knocker is leveraging a unique set of responses generated from the knock. Knocker takes a multimodal sensing approach that utilizes microphones, accelerometers, and gyroscopes to capture the knock responses, and exploits machine learning to accurately identify objects. We also present 15 applications enabled by Knocker that showcase the novel interaction method between users and objects. Knocker uses only the built-in smartphone sensors and thus is fully deployable without specialized hardware or tags on either the objects or the smartphone. Our experiments with 23 objects show Knocker achieves the accuracy of 98% in a controlled lab and 83% in the wild environments.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10469
        },
        {
          "affiliations": [],
          "personId": 20753
        },
        {
          "affiliations": [],
          "personId": 22775
        },
        {
          "affiliations": [],
          "personId": 12763
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 6949,
      "typeId": 11437,
      "title": "Demo: Feel the Pressure - a Haptic-Feedback Device for Wearable Musical Instrument Interaction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable gestural interfaces for effect control during concerts enable artists to experiment with new ways to express their work. These interfaces can be seen as novel musical instruments, however compared to traditional musical instruments they often lack fine-grained haptic feedback. The artist effectively plays these instrument \"blind\". In this paper, we present an easily built pressure-feedback device to close the loop between gesture interaction and haptic feedback. This can give the artist an idea of the currently enabled musical effect, and indicates the strength of this effect which can serve as a warning signal when the effect reaches a otherwise invisible limit with respect to a starting gesture. To evaluate the system, study participants were asked to distinguish different levels of pressure, giving insights on the accuracy of such a haptic interface.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23464
        },
        {
          "affiliations": [],
          "personId": 15749
        },
        {
          "affiliations": [],
          "personId": 22911
        },
        {
          "affiliations": [],
          "personId": 21240
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6951,
      "typeId": 11437,
      "title": "Demo: IMU-Kinect: A Motion Sensor-based Gait Monitoring System for Intelligent Healthcare",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Gait rehabilitation is a common method of postoperative recovery after the user sustains an injury or disability. However, traditional gait rehabilitations are usually performed under the supervision of rehabilitation specialists, meaning the patients can not receive adequate care continuously. In this paper, we propose IMU-Kinect, a novel system to remotely and continuously monitor the gait rehabilitation via the wearable kit.  This system consists of a wearable hardware platform and a user-friendly software application. The hardware platform is composed of four Inertial Measurement Units (IMU), which are attached on the shanks and thighs of the human body. The software application is able to estimate the rotation and displacement of these sensors, then reconstruct the gait movements and calculate the gait parameters according to the geometric model of human lower limbs. Based on IMU-Kinect system, the users of gait rehabilitation just need to walk normally by wearing the IMU-Kinect kit, and then the rehabilitation specialists can analyze the status of postoperative recovery by remotely viewing the animations about users' gait movements and charts of the general gait parameters. Extend experiments in real environment show that our system can efficiently track the gait movements with $7$cm displacement error and $5^\\circ$ rotation error.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14609
        },
        {
          "affiliations": [],
          "personId": 18937
        },
        {
          "affiliations": [],
          "personId": 10786
        },
        {
          "affiliations": [],
          "personId": 19545
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5929,
      "typeId": 11437,
      "title": "Nurse Care Activity Recognition Challenge: Summary and Results",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Although activity recognition has been studied for a long time now, research and applications have focused on physical activity recognition. Even if many application domains require the recognition of more complex activities, research on such activities has attracted less attention.\nOne reason for this gap is the lack of datasets to evaluate and compare different methods.\nTo promote research in such scenarios, we organized the Open Lab Nursing Activity Recognition Challenge focusing on the recognition of complex activities related to the nursing domain. Nursing domain is one of the domains that can benefit enormously from activity recognition but has not been researched due to lack of datasets. \nThe competition used the CARECOM Nurse Care Activity Dataset, featuring 7 activities performed by 8 subjects in a controlled environment with accelerometer sensors, motion capture and indoor location sensor. \nIn this paper, we summarize the results of the competition",
      "authors": [
        {
          "affiliations": [],
          "personId": 18107
        },
        {
          "affiliations": [],
          "personId": 18164
        },
        {
          "affiliations": [],
          "personId": 15477
        },
        {
          "affiliations": [],
          "personId": 11459
        },
        {
          "affiliations": [],
          "personId": 8218
        },
        {
          "affiliations": [],
          "personId": 17517
        },
        {
          "affiliations": [],
          "personId": 9177
        },
        {
          "affiliations": [],
          "personId": 20389
        },
        {
          "affiliations": [],
          "personId": 19993
        },
        {
          "affiliations": [],
          "personId": 20027
        },
        {
          "affiliations": [],
          "personId": 24129
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2860,
      "typeId": 11437,
      "title": "LightStress: Targeting Stress Reduction through Affective Objects",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Long-term stress is a leading cause of global health loss. Despite its clear influence on productivity, and overall social and economic development, mental health continues to be neglected in work environments. To help reduce this problem, we present LightStress, a tangible affective artifact that adapts itself to the user’s needs in order to improve their mental well-being. Our prototype is inspired by existing techniques and coping mechanisms collected during the two ethnographic studies we conducted (a cultural probe and an online imagery research).",
      "authors": [
        {
          "affiliations": [],
          "personId": 12008
        },
        {
          "affiliations": [],
          "personId": 20315
        },
        {
          "affiliations": [],
          "personId": 11659
        },
        {
          "affiliations": [],
          "personId": 16077
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3886,
      "typeId": 11437,
      "title": "Tap2Pair: Associating Wireless Devices with Tapping",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The IoT era demands ad-hoc wireless devices association for a convenient and spontaneous cross-device interaction experience. Currently, users associate two devices by selecting the advertising device (e.g. a mouse) from a list displayed by the scanning device (e.g. a laptop). However, the association can be misplaced since it is often more convenient to initiate association from the advertising device (e.g. when switching a mouse between two computers). Tap2Pair allows users to simply tap on an advertising device to associate with the target scanning device. It does not require any modification of existing wireless devices and is compatible with most wireless protocols. Hands tap near the advertising device's antenna can change the strength of the signal received by scanning devices. Scanning devices can then calculate signal features and initiate association if certain criteria are met. We demonstrate two association strategies for different scenarios: 1. Hold and tap an advertising device near the target scanning device; 2. Tap at the corresponding frequency of the target scanning device.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21204
        },
        {
          "affiliations": [],
          "personId": 13882
        },
        {
          "affiliations": [],
          "personId": 17784
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5936,
      "typeId": 11437,
      "title": "GoalKeeper: Exploring Interaction Lockout Mechanisms for Regulating Smartphone Use",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We often experience difficulties in achieving behavioral goals related to smartphone use. Most of prior studies approached this problem with various behavior change strategies such as self-reflection and social support. However, little is known about the effectiveness and user experiences of coercive and restrictive interventions such as blocking. In this work, we developed ‚ÄúGoalKeeper,‚Äù a smartphone intervention app that locks the user into the self-defined daily use time limit with restrictive intervention mechanisms. We conducted a four-week field experiment with 36 participants to investigate the effects and user experiences of varying intensities of restrictive interventions. The results showed that restrictive mechanisms are more effective than non-restrictive mechanisms such as warning. However, we found that restrictive mechanisms caused more frustration and pressure to the users, mainly due to diversity of usage contexts and needs. Based on our study results, we extracted practical implications for designing restrictive mechanisms that balance the intervention effectiveness for behavioral changes and the flexibility for user acceptability.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19892
        },
        {
          "affiliations": [],
          "personId": 10818
        },
        {
          "affiliations": [],
          "personId": 17882
        },
        {
          "affiliations": [],
          "personId": 22860
        }
      ],
      "sessionIds": [
        2158
      ],
      "eventIds": []
    },
    {
      "id": 7984,
      "typeId": 11437,
      "title": "Estimating Load Positions of Wearable Devices based on Difference in Pulse Wave Arrival Time",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing use of wearable devices equipped with various sensors, human activities, biometric information, and surrounding situations can be obtained via sensor data regardless of time and place. When position-free wearable devices are attached to an arbitrary part of the body, the attached position should be identified because the application process changes relative to the position. For systems that use multiple wearable devices to capture body-wide movement, estimating the attached position of the devices is meaningful. Most conventional studies estimate the loading position of the sensor using accelerometer and gyroscope data; therefore, users must perform specific motions so that each sensor produces values unique to the given position. We propose a method that estimates the load position of wearable devices without forcing the wearer to perform specific actions. The proposed method estimates the time difference between a heartbeat obtained by an electrocardiogram and a pulse wave obtained using a pulse sensor and classifies the sensor position from the estimated time difference. We assume that pulse sensor is embedded in the wearable devices to be attached to the user. From the results of an evaluation experiment with five subjects, an average F-measure of 0.805 was achieved over 15 body parts. The left ear and the right finger achieved an F-measure of 0.9+ when the proposed system uses data of approximately 20 seconds as an input.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8529
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 6962,
      "typeId": 11437,
      "title": "EmbraceNet for Activity: A Deep Multimodal Fusion Architecture for Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity recognition using multiple sensors is a challenging but promising task in recent decades. In this paper, we propose a deep multimodal fusion model for activity recognition based on the recently proposed feature fusion architecture named EmbraceNet. Our model processes each sensor data independently, combines the features with the EmbraceNet architecture, and post-processes the fused feature to predict the activity. In addition, we propose additional processes to boost the performance of our model. We submit the results obtained from our proposed model to the SHL recognition challenge with the team name “Yonsei-MCML.”",
      "authors": [
        {
          "affiliations": [],
          "personId": 13717
        },
        {
          "affiliations": [],
          "personId": 21543
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7987,
      "typeId": 11437,
      "title": "The Second Workshop on Combining Physical and Data-Driven Knowledge in Ubiquitous Computing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In the real-world ubiquitous computing systems, it is difficult to require a significant amount of data to obtain accurate information through pure data-driven methods. Performance of data-driven methods relies on the quantity and `quality' of data. They perform well when a sufficient amount of data is available, which is regarded as ideal conditions. However, in real-world systems, collecting data can be costly or impossible due to practical limitations. On the other hand, it is promising to utilize physical knowledge to alleviate these issues of data limitation. The physical knowledge includes domain knowledge from experts, heuristics from experiences, analytic models of the physical phenomena and etc. The goal of the workshop is to explore the intersection between (and the combination of) data and physical knowledge. The workshop aims to bring together domain experts that explore the physical understanding of the data, practitioners that develop systems and the researchers in traditional data-driven domains. The workshop welcomes papers, which focuses on addressing these issues in different applications/domains as well as algorithmic and systematic approaches to applying physical knowledge. Therefore, we further seek to develop a community that systematically analyzes the data quality regarding inference and evaluates the improvements from physical knowledge. Preliminary and on-going work is welcomed.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17798
        },
        {
          "affiliations": [],
          "personId": 12602
        },
        {
          "affiliations": [],
          "personId": 9354
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3891,
      "typeId": 11437,
      "title": "VitaBoot - Footwear with Dynamic Graphical Patterning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "VitaBoot is a footwear concept incorporating dynamic graphical patterns which indicate the wearer's activity level. Whilst shoes are an important element in fashion wear and a lot of research has focused on shoes as input devices, comparable few concepts have explored the potential for their use as an output space. We created a boot design that incorporates dynamic patterning through the use of electrochromic (EC) displays embedded in the surface material. The boot was designed and constructed from scratch, using a faux leather material, ensuring the overall aesthetic of the design, including the integration of the required control electronics and power source. The boot connects wirelessly to a chest-worn heart rate belt, and pattern changes indicate when the wearer's heart rate is above a predefined threshold. VitaBoot demonstrates the potential for dynamic shoe patterning for aesthetic or functional means, and the suitability of flexible, low-power EC display technology in this domain.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16800
        },
        {
          "affiliations": [],
          "personId": 23391
        },
        {
          "affiliations": [],
          "personId": 10889
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3894,
      "typeId": 11437,
      "title": "ObstacleWatch: Acoustic-based Obstacle Collision Detection for Pedestrian Using Smartphone",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Walking while using a smartphone is becoming a major pedestrian safety concern as people may unknowingly bump into various obstacles that could lead to severe injuries. In this paper, we propose ObstacleWatch, an acoustic-based obstacle collision detection system to improve the safety of pedestrians who are engaged in smartphone usage while walking. ObstacleWatch leverages the advanced audio hardware of the smartphone to sense the surrounding obstacles and infers fine-grained information about the frontal obstacle for collision detection. In particular, our system emits well-designed inaudible beep signals from the smartphone built-in speaker and listens to the reflections with the stereo recording of the smartphone. By analyzing the reflected signals received at two microphones, ObstacleWatch is able to extract fine-grained information of the frontal obstacle including the distance, angle and size for detecting the possible collisions and to alert users. Our experimental evaluation under two real-world environments with different types of phones and obstacles shows that ObstacleWatch achieves over 92% accuracy in predicting obstacle collisions with distance estimation errors at about 2 cm. Results also show that ObstacleWatch is robust to different sizes of objects and is compatible to different phone models with low energy consumption.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10782
        },
        {
          "affiliations": [],
          "personId": 18515
        },
        {
          "affiliations": [],
          "personId": 24146
        },
        {
          "affiliations": [],
          "personId": 19616
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 2871,
      "typeId": 11437,
      "title": "Understanding Social Perceptions Towards Interacting with On-Skin Interfaces in Public",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Wearable devices have evolved towards intrinsic human augmentation, unlocking the human skin as an interface for seamless interaction. However, the non-traditional form factor of these on-skin interfaces, as well as the gestural interactions performed on them may raise concerns for public wear. These perceptions will influence whether a new form of technology will eventually be accepted, or rejected by society. Therefore, it is essential for researchers to consider the societal implications of device design. In this paper, we investigate the third person perceptions of a user's interactions with an on-skin touch sensor. Specifically, we examine social perceptions towards the placement of the on-skin interface in different body locations, as well as gestural interactions performed on the device. The study was conducted in the United States and Taiwan to examine cross-cultural attitudes towards device usage. The results of this structured examination offer insight into the design of on-skin interfaces for public use.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14131
        },
        {
          "affiliations": [],
          "personId": 19145
        },
        {
          "affiliations": [],
          "personId": 14593
        },
        {
          "affiliations": [],
          "personId": 13667
        },
        {
          "affiliations": [],
          "personId": 10518
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 5946,
      "typeId": 11437,
      "title": "How Much is Too Much? Understanding the Information Needs of Parents of Young Children",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "While technologies exist that are designed for parents to monitor their toddlers and school-age children, the actual structure and underlying mechanisms of parents information needs have received only limited attention. A systematic understanding of these core components is crucial for designing appropriate solutions and reducing barriers to using this type of technology. Based on accumulated findings from a three-phase study, this paper (1) describes the structure of information that parents seek about their children, (2) explores parents’ underlying motivations for seeking this information, (3) presents a classification of major uses of this information, and, (4) through a set of design  recommendations, discusses the role that ubiquitous technology can play in addressing information needs and current obstacles to technologically mediated solutions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11651
        },
        {
          "affiliations": [],
          "personId": 9247
        }
      ],
      "sessionIds": [
        1195
      ],
      "eventIds": []
    },
    {
      "id": 3905,
      "typeId": 11437,
      "title": "CrowdX: Enhancing Automatic Construction of Indoor Floorplan with Opportunistic Encounters",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The lack of floorplan limits the spread of pervasive indoor location-based services. Existing crowdsourcing based approaches mostly rely on identifying, locating landmarks in the environment and utilizing the spatial relationship between the landmarks and traces for efficiently constructing fine-grained floorplan. However, these methods are always restricted by the sparse landmark distribution or may cause privacy leakage. In this paper, we propose CrowdX, a crowdsourcing system for accurate, low-cost indoor floorplan construction enhanced with opportunistic encounters among mobile users. The key insight is that the spatial relation (i.e., the displacement of each user and the distance between each other during the encounter) will be extracted from the audio and inertia data, which are aligned by the proposed vibration event-based method. Such information can be used to calibrate the drift of encounter position. The calibrated encounter position is beneficial to most of the floorplan generation steps, such as trace drift elimination, landmark positioning, hallway assembling and room area estimation. Our experiments in three shopping malls show that CrowdX achieves an average F-measure around 89.4%. In addition, the average estimated room area error within about 20%. The evaluation results demonstrate a significant improvement of accuracy enhanced with opportunistic encounters.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16151
        },
        {
          "affiliations": [],
          "personId": 20396
        },
        {
          "affiliations": [],
          "personId": 23609
        },
        {
          "affiliations": [],
          "personId": 17188
        }
      ],
      "sessionIds": [
        1235
      ],
      "eventIds": []
    },
    {
      "id": 4931,
      "typeId": 11437,
      "title": "MenstruLoss: Sensor For Menstrual Blood Loss Monitoring",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Self-monitoring of menstrual blood loss volume could lead to early detection of multiple gynecological diseases. In this paper, we describe the development of a textile-based blood volume sensor which can be integrated into the sanitary napkin to quantify the menstrual blood loss during menstruation. It is based on sensing the resistance change detected as the output voltage change, with the added volume of fluid. Benchtop characterization tests with 5 mL of fluid determined the effect of spacing, orientation and weight, and location of fluid drop on the sensor. The sensor has been evaluated by intravenous blood samples collected from 18 participants and menstrual blood samples collected from 10 participants for four months. The collected intravenous blood samples and menstrual blood samples were used to create two regression model that can predict the blood volume and menstrual blood volume from the voltage input with Mean Absolute Percentage Error (MAPE) of 11-15% and 15-30% respectively.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18385
        },
        {
          "affiliations": [],
          "personId": 14793
        },
        {
          "affiliations": [],
          "personId": 10881
        },
        {
          "affiliations": [],
          "personId": 12878
        },
        {
          "affiliations": [],
          "personId": 11695
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 5958,
      "typeId": 11437,
      "title": "Understanding Parents’ Perspectives on Mealtime Technology",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "For young children, family meals are an enjoyable and developmentally useful part of daily life. Although prior work has shown that ubiquitous computing solutions can enhance children‚Äôs eating habits and mealtime experiences in valuable ways, other work demonstrates that many families are hesitant to use technology in this context. This paper examines adoption barriers for technology for family meals to understand with more nuance what parents value and resist in this space. Using mixed methods, we first observed family dinnertime experiences and then surveyed 122 parents with children from 2-6 years old. We found that parents prefer screen-based technology over voice interfaces and smart objects, because parents perceive the latter two systems to intrude on their relationship with children. The pervasiveness of smart objects embedded at meals led parents to worry about distraction and technology dependence, while the anthropomorphization of voice interfaces led parents to worry that this technology could displace parenting relationships or disrupt interpersonal interactions among family members. Parents mindlessly applied social scripts to voice interfaces, suggesting families may be more likely to apply concerns from interpersonal interactions to voice interfaces than to other technologies. We discuss the ways different form factors appeal to and worry parents, providing designers with insights about the likelihood of adoption and acceptance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16440
        },
        {
          "affiliations": [],
          "personId": 11404
        },
        {
          "affiliations": [],
          "personId": 14404
        },
        {
          "affiliations": [],
          "personId": 20638
        }
      ],
      "sessionIds": [
        1195
      ],
      "eventIds": []
    },
    {
      "id": 8008,
      "typeId": 11437,
      "title": "CAP: Context-aware App Usage Prediction With Heterogeneous Graph Embedding",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Context-aware mobile application (App) usage prediction benefits a variety of applications such as precise bandwidth allocation, App launch acceleration, etc. Prior works have explored this topic through individual data profiles and contextual information. However, it is still a challenging problem because of the following three aspects: i. App usage behavior is usually influenced by multiple factors, especially temporal and spatial factors. ii. It is difficult to describe individuals' preferences, which are usually time-variant. iii. A single user's data is sparse on the spatial domain and only covers a limited number of locations. Prediction becomes more difficult when the user appears at a new location. This paper presents CAP, a context-aware App usage prediction algorithm that takes both contextual information (location & time) and attribution (App with type information) into consideration. We find that the relationships between App-location, App-time, and App-App type are essential to prediction and propose a heterogeneous graph embedding algorithm to map them into the common comparable latent space. In addition, we create a user profile for each user with App usage and trajectory history to describe the individual dynamic preference for personalized prediction. We evaluate the performance of our proposed CAP with two large-scale real-world datasets. Extensive evaluations demonstrate that CAP achieves 30% higher accuracy than a state-of-the-art method Personalized Ranking Metric Embedding (PRME) in terms of Accuracy@5. In terms of mean reciprocal rank (MRR), CAP achieves 1.5X higher than the straightforward baseline Sta and 2X higher than PRME. Our investigation enables a range of applications to benefit from such timely predictions, including network operators, service providers, and etc.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17798
        },
        {
          "affiliations": [],
          "personId": 17188
        },
        {
          "affiliations": [],
          "personId": 15905
        },
        {
          "affiliations": [],
          "personId": 12602
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 14989
        }
      ],
      "sessionIds": [
        2123
      ],
      "eventIds": []
    },
    {
      "id": 6985,
      "typeId": 11437,
      "title": "Touch Mood: GSR and FITI Enabled Wearable",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Touch Mood is a wearable technology project which obtains skin signals from subjects via GSR and an MLX90614 thermal camera. The system gives real-time feedback of the subject’s emotional state via the embedded RGB LEDs and nitinol wire enabled transforming fabric array.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20895
        },
        {
          "affiliations": [],
          "personId": 23283
        },
        {
          "affiliations": [],
          "personId": 15943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4941,
      "typeId": 11437,
      "title": "Towards Reliable, Automated General Movement Assessment for Perinatal Stroke Screening in Infants using Wearable Accelerometers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Perinatal stroke (PS) is a serious condition that, if undetected and thus untreated, often leads to life-long disability, in particular Cerebral Palsy (CP). In clinical settings, Prechtl‚Äôs General Movement Assessment (GMA) can be used to classify infant movements using a Gestalt approach, identifying infants at high risk of developing PS. Training and maintenance of assessment skills are essential and expensive for the correct use of GMA, yet many practitioners lack these skills, preventing larger-scale screening and leading to significant risks of missing opportunities for early detection and intervention for affected infants. We present an automated approach to GMA, based on body-worn accelerometers and a novel sensor data analysis \\ method‚ÄìDiscriminative Pattern Discovery (DPD)‚Äìthat is designed to cope with scenarios where only coarse annotations of data are available for model training. We demonstrate the effectiveness of our approach in a study with 34 newborns (21 typically developing infants and 13 PS infants with abnormal movements). Our method is able to correctly recognise the trials with abnormal movements with at least the accuracy that is required by newly trained human annotators (75%), which is encouraging towards our ultimate goal of an automated PS screening system that can be used population-wide.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14070
        },
        {
          "affiliations": [],
          "personId": 18026
        },
        {
          "affiliations": [],
          "personId": 14330
        },
        {
          "affiliations": [],
          "personId": 17083
        },
        {
          "affiliations": [],
          "personId": 11098
        },
        {
          "affiliations": [],
          "personId": 18818
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 3919,
      "typeId": 11437,
      "title": "CNN for Human Activity Recognition on Small Datasets of Acceleration and Gyro Sensors Using Transfer Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper describes an activity recognition method for the Sussex-Huawei  Locomotion-Transportation  (SHL)  recognition  challenge  by Team TDU-DSML. The CNN model reported in our 2018 SHL Challenge was adopted. 5-second FFT spectrogram images from all axes of acceleration and gyro sensor data were treated as input data. We  confirmed  that  a  multiple-sensor  input  model  combining  acceleration   and   gyro   sensors   improves   the   recognition   rate.   However, there was insufficient training data in the SHL dataset for the  target  sensor  labeled  as  Hand.  To  overcome  this  difficulty,  a  transfer  learning  method  was  applied  to  the  pre-training  model  from  other  sensors  labeled  as  Hips  and  Torso.  After  evaluation  from all combination of sensors, the transfer learning model from Acc_norm and Gyr_x for Hips and Torso had the best recognition rate of 82.1% at Hand in the submission phase.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23634
        },
        {
          "affiliations": [],
          "personId": 12412
        },
        {
          "affiliations": [],
          "personId": 10784
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4944,
      "typeId": 11437,
      "title": "Gaze Assisted Voice Note Taking System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "On one hand, note-taking helps students to effectively record the essence of information learned from the subject content, but on the other hand, this act of learning is a cognitive effortful which triggers learning process that increases the cognitive load suffered by the student. To facilitate this act of learning we proposed a gaze assisted note-taking system that implicitly annotates learning content with the student\\textquotesingle s audio notes. This research project aims at leveraging novel sensing devices and machine learning techniques to enable the development of intelligent voice note-taking system that could support self-regulatory learning. To do this, we have collected a dataset of 32 PhD students using a range of unobtrusive sensors while students record voice notes during reading a research paper. The result from this study will help us to monitor the cognitive efforts involved when PhD students use audio notes to reflect on their understanding and encode learning material as well as build robust machine learning model which can predict the text of the research paper to implicitly annotate students\\textquotesingle audio notes with the learning material.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22819
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5970,
      "typeId": 11437,
      "title": "Seamless Internet connectivity for ubiquitous communication",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We describe seamless communication across different networks without the use\nof middleware, proxies, tunnels, or address translation, with minimal\n(near-zero) packet loss to communication flows as handoff occurs between\nnetworks. Our solution does not require any new functions in existing\nnetworks, will work on existing infrastructure, and does not require\napplications to be re-designed or re-engineered. Our solution requires only\nmodifications to the end-systems involved in communication, so can be deployed\nincrementally only for those end-systems that require the functionality. We\ndescribe our approach and its design, based on the use of the\nIdentifier-Locator Network Protocol (ILNP), which can be realised directly on\nIPv6. We demonstrate the efficacy of our solution with experiments based on\nmodifications to the Linux kernel v4.9 (LTS), operating directly over IPv6,\nand using unmodified binary applications utilising directly the standard\nsocket(2) POSIX.1-2008 API, and standard C library calls. As our approach is\n`end-to-end', we also describe how to maintain packet-\\-level secrecy and\nidentity privacy for the communication flow as part of our approach.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12781
        },
        {
          "affiliations": [],
          "personId": 9524
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2904,
      "typeId": 11437,
      "title": "EEG Visualising Pendant for Social Situations",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This abstract discusses the EEG Visualising Pendant (2012), an emotive wearable that maps and visualises EEG data from a NeuroSky MindWave Mobile Bluetooth EEG headset. It has been developed through several iterations as a doctoral research prototype for studies evaluating the use of bespoke, aesthetic wearables in the role of nonverbal communication.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19474
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3928,
      "typeId": 11437,
      "title": "HiveTracker: 3d Positioning for Ubiquitous Embedded Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recent advances in positional tracking systems for virtual and augmented reality have opened up the possibilities for ubiquitous motion capture technology in the consumer market. However, for many applications such as in performance art, athletics, neuroscience, and medicine, these systems remain too bulky, expensive, and limited to tracking a few objects at a time. In this work, we present a small wireless device that takes advantage of existing HTC Vive lighthouse tracking technology to provide affordable, scalable, and highly accurate positional tracking capabilities. This open hardware and open software project contains several elements, and the latest contributions described in this paper include: (1) a characterization of the optical distortions of the lighthouses, (2) a new cross-platform WebBLE interface, and (3) a real-time in-browser visualization. We also introduce new possibilities with an adaptive calibration to estimate transformation matrices of lighthouses, and an FPGA approach to improve precision and adaptability. Finally, we show how the new developments reduce setup costs and increase the accessibility of our tracking technology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8391
        },
        {
          "affiliations": [],
          "personId": 19884
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8026,
      "typeId": 11437,
      "title": "Your Table Can Be an Input Panel: Acoustic-based Device-Free Interaction Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper explores the possibility of extending the input and interactions beyond the small screen of the mobile device onto ad hoc adjacent surfaces, e.g., a wooden tabletop with acoustic signals. While the existing finger tracking approaches employ the active acoustic signal with a fixed frequency, our proposed system Ipanel employs the acoustic signals generated by sliding of fingers on the table for tracking. Different from active signal tracking, the frequency of the finger-table generated acoustic signals keeps changing, making accurate tracking much more challenging than the traditional approaches with fix frequency signal from the speaker. Unique features are extracted by exploiting the spatio-temporal and frequency domain properties of the generated acoustic signals. The features are transformed into images and then we employ the convolutional neural network (CNN) to recognize the finger movement on the table. Ipanel is able to support not only commonly used gesture (click, Ô¨Çip, scroll, zoom, etc.) recognition, but also handwriting (10 numbers and 26 alphabets) recognition at high accuracies. We implement Ipanel on smartphones, and conduct extensive real environment experiments to evaluate its performance. The results validate the robustness of Ipanel, and show that it maintains high accuracies across different users with varying input behaviours (e.g., input strength, speed and region). Further, Ipanel‚Äôs performance is robust against different levels of ambient noise and varying surface materials.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15281
        },
        {
          "affiliations": [],
          "personId": 9056
        },
        {
          "affiliations": [],
          "personId": 12597
        },
        {
          "affiliations": [],
          "personId": 21431
        },
        {
          "affiliations": [],
          "personId": 11293
        },
        {
          "affiliations": [],
          "personId": 17493
        },
        {
          "affiliations": [],
          "personId": 22767
        }
      ],
      "sessionIds": [
        1270
      ],
      "eventIds": []
    },
    {
      "id": 8029,
      "typeId": 11437,
      "title": "Fingers and Angles: Exploring the Comfort of Touch Input on Smartwatches",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartwatches present a unique touch input context: small, fixed to one wrist and approachable from a limited range of angles by the touching hand. Techniques to expand their input expressivity often involve variations in how a watch must be touched, such as with different fingers, poses or from specific angles. {While objective performance with such systems is commonly reported, subjective qualities such as comfort remain overlooked. We argue that techniques that involve uncomfortable input will be of limited value and contribute the first data on the comfort of input on smartwatches via two studies that combine subjective ratings of comfort with objective performance data. We examine both static and dynamic touches and three finger poses. Based on the study results, we contribute a set of design recommendations for comfortable, effective smartwatch input. We close by instantiating the recommendations in interface prototypes that we evaluate in a final qualitative study.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12164
        },
        {
          "affiliations": [],
          "personId": 11082
        },
        {
          "affiliations": [],
          "personId": 15163
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5985,
      "typeId": 11437,
      "title": "Personal Thermal Perception Models using Skin Temperatures and HR/HRV Features - Comparison of Smartwatch and Professional Measurement Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In order to detect a person's individual momentary thermal sensation and comfort, an increased number of models are developed that involve physiological data, especially skin temperatures and features based on the heart activity. In this paper, we investigate the feasibility of machine learning (ML) models which include physiological data based on two data sources: (1) a smartwatch and (2) a portable chest belt device. Further, we investigate the difference between thermal sensation and comfort votes and propose a new combined ground truth label. Data were collected in lab-like studies in a single-bed hotel room. Our study focuses on the detection of cold-induced thermal discomfort by varying the room air temperature between 24°C and 20°C. Results show that ML based approaches lead to accuracies up to 83.1% when using a chest belt device. Using a smartwatch, accuracies drop down to 79.8%. Our investigation shows that even though Heart Rate (HR) based features improve the prediction accuracy, the highest feature importances have distal skin temperature features.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8805
        },
        {
          "affiliations": [],
          "personId": 20265
        },
        {
          "affiliations": [],
          "personId": 16129
        },
        {
          "affiliations": [],
          "personId": 8294
        }
      ],
      "sessionIds": [
        1206
      ],
      "eventIds": []
    },
    {
      "id": 4964,
      "typeId": 11437,
      "title": "CoRSA: a Cardio-Respiratory Monitor in Sport Activities",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present the system CoRSA to incorporate integrated sensors in millimeter-scale packages for continuous cardiorespiratory (CR) evaluation in sports activities. CoRSA retrofits trending sports apparel to add on CR sensing capability. The system uses an air pressure sensor inside a vented mask to approximate a spirometer, and an earlobe pulse-oximeter (PO) to monitor heart rate (HR) and oxygen saturation (SpO2). CoRSA also includes an inertial measurement unit for tracking activity and future study on motion artifact correction on the CR signals in active sports. An aerobic exercise evaluation is also performed which shows results similar to sports studies using bulkier conventional medical equipment in the CR signals’ characteristics.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8197
        },
        {
          "affiliations": [],
          "personId": 14623
        },
        {
          "affiliations": [],
          "personId": 18666
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 3941,
      "typeId": 11437,
      "title": "Decision Making Support for Privacy Data Upload in Smart Home",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A smart home equipped with various smart devices allows a service provider to automatically identify daily living activities from sensor/appliance data, but it is risky for dwellers to upload all the data generated in the home. In this paper, we define a threat model in which an attacker(s) can access all or part of the smart home data uploaded to the untrusted cloud server and can physically observe activities. Hence, the attacker can identify the association between the data and the home by matching the uploaded data and the observed data. The proposed method employs k-anonymity for dwellers to make a decision on whether the data should be uploaded or not. We computed values of k from the existing datasets and asked 18 participants to answer upload/no-upload for each pair of activities and time zone. As a result,  our k-anonymity based method can reflect the dweller's sensitivity of privacy in uploading the data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15716
        },
        {
          "affiliations": [],
          "personId": 11441
        },
        {
          "affiliations": [],
          "personId": 12914
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5992,
      "typeId": 11437,
      "title": "Poster: Identifying Urban Villages from City-Wide Satellite Imagery Leveraging Mask R-CNN",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Urban villages emerge with the rapid urbanization process in many developing countries, and bring serious social and economic challenges to urban authorities, such as overcrowding and low living standards. A comprehensive understanding of the locations and regional boundaries of urban villages in a city is crucial for urban planning and management, especially when urban authorities need to renovate these regions. Traditional methods greatly rely on surveys and investigations of city planners, which consumes substantial time and human labor. In this work, we propose a low-cost and automatic framework to accurately identify urban villages from high-resolution remote sensing satellite imagery. Specifically, we leverage the Mask Regional Convolutional Neural Network (Mask-RCNN) model for end-to-end urban village detection and segmentation. We evaluate our framework on the city-wide satellite imagery of Xiamen, China. Results show that our framework successfully detects 87.18\\% of the urban villages in the city, and accurately segments their regional boundaries with an IoU of 74.48\\%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20714
        },
        {
          "affiliations": [],
          "personId": 11263
        },
        {
          "affiliations": [],
          "personId": 23110
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7017,
      "typeId": 11437,
      "title": "Activity Prediction for Improving Well-Being of Both The Elderly and Caregivers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The issue of ageing population is gaining significant attention across the world, while the caregivers' psychological burden caused by the variety of geriatric symptoms is often overlooked. Efficient collaboration between the elderly and caregivers has great potential to relieve the caregivers' psychological burden and improve caregiving quality. For instance, activity prediction can provide a promising approach to cultivate this efficient collaboration. Given the ability to predict the elderly patients' activity and its timing, caregivers can provide timely and appropriate care, which not only can relieve caregiving stress for professional or family caregivers, but also can reduce the unwanted conflicts between both parties. In this paper, we train an activity predictor by integrating the activity temporal information into the Long Short-Term Memory (LSTM) networks. The approach leads to significant improvements in the prediction accuracy both in the next activity and its precise occurrence time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15436
        },
        {
          "affiliations": [],
          "personId": 13203
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5993,
      "typeId": 11437,
      "title": "Handling Annotation Uncertainty in Human Activity Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Developing systems for Human Activity Recognition (HAR) using wearables typically relies on datasets that were manually annotated by human experts with regards to precise timings of instances of relevant activities.\nHowever, obtaining such data annotations is often very challenging in the predominantly mobile scenarios of Human Activity Recognition.\nAs a result, labels often carry a degree of uncertainty--\\textit{label jitter}--with regards to:\n\\textit{i)} correct temporal alignments of activity boundaries; and\n\\textit{ii)} correctness of the actual label provided by the human annotator.\nIn this work, we present a scheme that explicitly incorporates label jitter into the model training process.\nWe demonstrate the effectiveness of the proposed method through a systematic experimental evaluation on standard recognition tasks for which our method leads to significant increases of mean F1 scores.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15863
        },
        {
          "affiliations": [],
          "personId": 17531
        },
        {
          "affiliations": [],
          "personId": 16080
        }
      ],
      "sessionIds": [
        1749
      ],
      "eventIds": []
    },
    {
      "id": 7018,
      "typeId": 11437,
      "title": "Towards a Wearable Low-Cost Ultrasound Device for Classification of Muscle Activity and Muscle Fatigue",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Being able to reliably predict muscle contractions is important for athletes and rehabilitation patients alike. Numerous techniques and surrogates exist for this task. However, they are in general not well suited for everyday use and not able to extract information of muscles located in deeper body layers. To address this shortcoming, we present an approach to classify muscle contractions with raw ultrasound radio-frequency data (A-Scans) collected with a wearable system. It consists of a single element ultrasound transducer connected to custom-built acquisition hardware and an Android app to receive, store and analyze the data. We rely on data from the lower legs of healthy volunteers performing squats as sample exercises and use machine learning methods, ranging from sequence similarity measurement techniques to artificial neural networks, to classify the radio-frequency data. Results of our preliminary experimental setup prove its feasibility to classify muscle contractions based on ultrasound measurements.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12067
        },
        {
          "affiliations": [],
          "personId": 16104
        },
        {
          "affiliations": [],
          "personId": 18666
        }
      ],
      "sessionIds": [
        1445
      ],
      "eventIds": []
    },
    {
      "id": 2922,
      "typeId": 11437,
      "title": "Designing Batteryless Wearables for Hospitalized Older People",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Older people have expressed a clear desire for unobtrusive wearablemonitoring devices. Emerging batteryless sensor technologies suchas sensor enabled RFID (Radio Frequency Identification) create newopportunities for building unobtrusive wearables for older people.This study aims to: i) uncover user perceptions and acceptability ofa batteryless wearable sensor concept for hospitalized older people;and ii) present a construction of anewtextile integrated wearablesensor incorporating user feedback. We recruited 40 older people(age: 81.0±7.0 years) to wear our initial sensor prototype and usedtwo modified versions of validated questionnaires to evaluate userperceptions and acceptability. Our results showed: i) allowing olderpeople to experience the system created the opportunity for themto develop confidence and trust in the sensing technology, evenwhen they were initially anxious and skeptical: and ii) the firstdesign prototype should ideally be modified to reduce its visibility.To this end, we built a new wearable sensor design.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18351
        },
        {
          "affiliations": [],
          "personId": 23300
        },
        {
          "affiliations": [],
          "personId": 13555
        },
        {
          "affiliations": [],
          "personId": 12191
        },
        {
          "affiliations": [],
          "personId": 15859
        },
        {
          "affiliations": [],
          "personId": 10232
        }
      ],
      "sessionIds": [
        1861
      ],
      "eventIds": []
    },
    {
      "id": 2925,
      "typeId": 11437,
      "title": "MAC: Measuring the Impacts of Anomalies on Travel Time of Multiple Transportation Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Urban anomalies have a large impact on passengers' travel behavior and city infrastructures, which cause uncertainty on the travel time estimation.\nUnderstanding the impact of urban anomalies on travel time is of great value for various applications such as urban planning, human mobility studies and navigation systems.\nMost existing works on travel time have been focused on the total riding time between two locations on individual transportation modality.\nHowever, passengers often take different modes of transportation, e.g. taxis, subways, buses or personal vehicles, and a significant portion of the travel time is spent in the uncertain waiting.\nIn this paper, we study the fine-grained travel time patterns in multiple transportation systems under the impact of urban anomalies. \nSpecifically,  \n(i) we investigate the implicit components (including waiting and riding time) in multiple transportation systems; \n(ii) we measure the impact of real-world anomalies on the travel time components; \n(iii) we design a learning-based model for travel time component prediction with anomalies. \nDifferent from existing works, we implement and evaluate our measurement framework on multiple data sources, including four city-scale transportation systems, which are\n(a) a 15 thousand taxicab network;\n(b) a 13 thousand bus network;\n(c) a 10 thousand private vehicle network;\n(d) an automatic fare collection system for a public transit network (i.e., subway and bus) with 5 million smart cards.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16615
        },
        {
          "affiliations": [],
          "personId": 18113
        },
        {
          "affiliations": [],
          "personId": 9532
        },
        {
          "affiliations": [],
          "personId": 11883
        },
        {
          "affiliations": [],
          "personId": 22972
        },
        {
          "affiliations": [],
          "personId": 21598
        },
        {
          "affiliations": [],
          "personId": 11897
        }
      ],
      "sessionIds": [
        1040
      ],
      "eventIds": []
    },
    {
      "id": 8052,
      "typeId": 11437,
      "title": "Discovering Eating Routines in Context with a Smartphone App",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In everyday life, eating follows patterns and occurs in context. We present an approach to discover daily eating routines of a population following a multidimensional representation of eating episodes, using data collected with the Bites’n’Bits smartphone app. Our approach integrates multiple contextual cues provided in-situ (food type, time, location, social context, concurrent activities, and motivations) with probabilistic topic models, which discover representative patterns across these contextual dimensions. We show that this approach, when applied on eating episode data for over 120 people and 1200 days, allows describing the main eating routines of the population in meaningful ways. This approach, resulting from a collaboration between ubiquitous computing and nutrition science, can support interdisciplinary work on contextual analytics for promotion of healthy eating.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20422
        },
        {
          "affiliations": [],
          "personId": 19887
        },
        {
          "affiliations": [],
          "personId": 11277
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4983,
      "typeId": 11437,
      "title": "Diversifying Pro-Environmental Behaviours",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 15234
        },
        {
          "affiliations": [],
          "personId": 23097
        },
        {
          "affiliations": [],
          "personId": 8446
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7032,
      "typeId": 11437,
      "title": "Using Electrochromic Displays to Display Ambient Informationand Notifications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Ambient Displays are a promising means to reduce notification overload and work towards the vision of Calm Computing. In this paper, we present electrochromic displays as a novel class of displays to convey information. electrochromic displays are non-light-emitting, flexible, free-form, transparent, energy-efficient, easily integrated, and slow-switching, making them ideal candidates for information that changes over time and does not require immediate user attention. We describe the key features of electrochromic displays as well as application areas and provide an outlook into future developments of the technology.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17879
        },
        {
          "affiliations": [],
          "personId": 23391
        },
        {
          "affiliations": [],
          "personId": 8439
        },
        {
          "affiliations": [],
          "personId": 16800
        },
        {
          "affiliations": [],
          "personId": 24366
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3960,
      "typeId": 11437,
      "title": "Crowdsensing Under Recent Mobile Platform Background Service Restrictions - A Practical Approach",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Crowdsensing applications are a popular and common research tool, because they allow volunteering participants to provide valuable data via their mobile phones with minimal effort. In most scenarios, it is an important goal to gather data in a reliable and continuous way, while the app runs in the background to avoid disturbing the user. However, in recent versions, Android as well as iOS severely restrict the functionality of an app when it does not have the authorization of a foreground process. \nIn this work, we present a structured overview of the technical stmobile phone sensors; participatory data collection; influenza monitoring; crowdsensingate of background service restrictions under iOS (12) and Android (9). We demonstrate a practical approach for working with these restrictions by utilizing the respective operating system's location provider solution.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22975
        },
        {
          "affiliations": [],
          "personId": 19888
        },
        {
          "affiliations": [],
          "personId": 10037
        },
        {
          "affiliations": [],
          "personId": 13797
        },
        {
          "affiliations": [],
          "personId": 18666
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4985,
      "typeId": 11437,
      "title": "A Sequence-to-Sequence Model For Cell-ID Trajectory Prediction",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "It is expensive to collect trajectory data on a mobile phone by continuously pinpointing its location, which limits the application of trajectory data mining (e.g., trajectory prediction). In this poster, we propose a method for trajectory prediction by collecting cell-id trajectory data without explicit locations. First, it exploits the spatial correlation between cell towers based on graph embedding technique. Second, it employs the sequence-to-sequence (seq2seq) framework to train the prediction model by designing a novel spatial loss function. Experiment results based on real datasets have demonstrated the effectiveness of the proposed method.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8905
        },
        {
          "affiliations": [],
          "personId": 22965
        },
        {
          "affiliations": [],
          "personId": 13622
        },
        {
          "affiliations": [],
          "personId": 23962
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4986,
      "typeId": 11437,
      "title": "Cohort Analyses of In-Person Interactions in Temporally Evolving Student Social Groups",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In social interaction systems, the formation and testing of theories is significantly difficult because social interaction systems cannot be easily manipulated and controlled. It is also not possible to reproduce large-scale systems in a lab setting or in a short fixed time duration. Detecting short-term non-recurrent interactions between individuals is very different from studying an individual's long term social group(s). However, over the last decade the rate of digital data availability using smartphones and wearables has increased consistently at a high pace which allows social scientists gain a comprehensive understanding of how groups form and evolve over time using recurrent in-person interaction networks. In this paper, we design a long term data-driven study on a finite student population of a residential university campus. Our aim is to study a student's recurrent in-person interactions, or long-term social groups, between the time that one enters into a cohort, e.g. Class of 2022, and until that cohort graduates. In this sensor-data driven study using state-of-the-art interaction-detection algorithms, we monitor parameters such as social group size, formation-time and longevity. We also conduct a retrospective cohort analysis of self-reported social group parameters, e.g. social group size, time spent with each group type and associated satisfaction. Preliminary results from the same make an extremely strong case for a longitudinal study, especially indicated by the evolution of one's social circles over a long period of time.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9497
        },
        {
          "affiliations": [],
          "personId": 10400
        },
        {
          "affiliations": [],
          "personId": 21177
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6011,
      "typeId": 11437,
      "title": "Two Tell-tale Perspectives of PTSD: Neurobiological Abnormalities and Bayesian Regulatory Network of the Underlying Disorder in a Refugee Context",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Global refugee crisis around the world has displaced millions of people from their homes. Although some of them adjust well, many suffer from significant psychological distress, such as post-traumatic stress disorder (PTSD), owing to exposure to traumatic events and hardships. Here, diagnosis and access to psychological health care present particular challenges for various human-centered design issues. Therefore, analyzing the case of Rohingya refugees in Bangladesh, we propose a two-way diagnosis of PTSD using (i) short inexpensive questionnaire to determine its prevalence, and (ii) low-cost portable EEG headset to identify potential neurobiological markers of PTSD. To the best of our knowledge, this study is the first to use consumer-grade EEG devices in the scarce-resource settings of refugees. Moreover, we explored the underlying structure of PTSD and its symptoms via developing various hybrid models based on Bayesian inference by combining aspects from both reflective and formative models of PTSD, which is also the first of its kind. Our findings revealed several key components of PTSD and its neurobiological abnormality. Moreover, challenges faced during our study would inform design processes of screening tools and treatments of PTSD to incorporate refugee experience in a more meaningful way during contemporary and future humanitarian crisis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17312
        },
        {
          "affiliations": [],
          "personId": 9154
        },
        {
          "affiliations": [],
          "personId": 12955
        },
        {
          "affiliations": [],
          "personId": 14261
        },
        {
          "affiliations": [],
          "personId": 18417
        },
        {
          "affiliations": [],
          "personId": 20627
        },
        {
          "affiliations": [],
          "personId": 11830
        },
        {
          "affiliations": [],
          "personId": 14438
        }
      ],
      "sessionIds": [
        2491
      ],
      "eventIds": []
    },
    {
      "id": 4988,
      "typeId": 11437,
      "title": "Sensor network to measure MAAI on value co-creation process",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The purpose of this study is the measurement of MAAI and value co-creation in Japanese-style service interaction through quantification of service interaction between a customer and a salesperson in retail stores. In this paper, we propose the measurement technique of MAAI of service interaction in retail store. We confirmed that measurement of MAAI enables modeling value co-creation in this customer service situation. As a result, the technique proposed here to measure MAAI is expected to be useful to manage and computing customer service.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9299
        },
        {
          "affiliations": [],
          "personId": 17226
        },
        {
          "affiliations": [],
          "personId": 9330
        },
        {
          "affiliations": [],
          "personId": 17319
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8061,
      "typeId": 11437,
      "title": "DeepNavi: A Deep Signal-Fusion Framework for Accurate and Applicable Indoor Navigation",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Indoor navigation plays a crucial role in indoor location-based services. Single signal-based navigation systems, however, are prone to sensor noises, signal ambiguities and are specific to trial sites. To address these, existing work fuses different signals with user trajectories. Despite their accuracy, many of them are specific to input signals and navigation modes (e.g., spot-based or sequence-based) and are computationally expensive in large sites. Additionally, they do not give predictive uncertainty estimations, leading to a lack of trust in navigation instructions.\n\nIn this paper, we propose a unified framework for accurate indoor navigation in various modes with different inputs, termed DeepNavi. We exploit either convolutional or recurrent neural networks for initial feature extraction. Afterwards, we insert fully connected layers to generalize extracted signal-dependent features to a shared domain before fusion. Then, we leverage state-of-the-art ensemble learning to learn multiple predictive models. By combining them together, we further reduce the impact of signal noise and achieve high accuracy. Finally, we insert mixture density networks to model more generalized data distributions and provide uncertainty estimations. We have implemented DeepNavi and conducted extensive experiments in two different trial sites with different signal combinations. Experimental results show that DeepNavi reduces location errors by more than 20% with comparable orientation accuracy.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21011
        },
        {
          "affiliations": [],
          "personId": 9838
        },
        {
          "affiliations": [],
          "personId": 22308
        },
        {
          "affiliations": [],
          "personId": 12985
        },
        {
          "affiliations": [],
          "personId": 9929
        },
        {
          "affiliations": [],
          "personId": 21502
        },
        {
          "affiliations": [],
          "personId": 18987
        },
        {
          "affiliations": [],
          "personId": 8861
        }
      ],
      "sessionIds": [
        1040
      ],
      "eventIds": []
    },
    {
      "id": 6014,
      "typeId": 11437,
      "title": "iSNoW: User Perceptions of an Interactive Social Novelty Wearable",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We developed a highly-visible head-mounted novelty wearable to be used in social settings. We tested our Interactive Social Novelty Wearable (iSNoW) prototype in a partner-based user study to see if perceptions of the experience would change if the information displayed on the wearable was contextually relevant. Thematic analyses revealed important considerations for the design of future devices, regarding distraction and pressure to understand the rules of the game. Participants wearing contextually relevant information were more likely to recommend the device to their friends. We highlight future opportunities for exploration in this relatively untouched space.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17108
        },
        {
          "affiliations": [],
          "personId": 15850
        },
        {
          "affiliations": [],
          "personId": 14644
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3967,
      "typeId": 11437,
      "title": "Identifying self-changeable actions toward regulating rhythm of daily life",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The increasing number of people with lifestyle-related diseases has become a social problem. It is well known that improving lifestyle habits is effective in preventing illness. \n We focus on regulating the rhythm of daily life and investigate a support technology that adjusts the user's schedule to enhance health outcomes. Its main goals are to schedule the ideal goal (target action at desired time given by the user) and advise what actions should be taken and when to perform them to achieve the goal.\nThis paper proposes a method to find trigger actions whose scheduling is flexible and that should precede the target action. Our study started with the collection of activity logs and flex logs. The flex logs record the periods in which the user feels the actions could be performed. We report on the preliminary findings from analyzing the collected logs and discuss how the findings can be used in designing future studies.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15246
        },
        {
          "affiliations": [],
          "personId": 15484
        },
        {
          "affiliations": [],
          "personId": 24333
        },
        {
          "affiliations": [],
          "personId": 21212
        },
        {
          "affiliations": [],
          "personId": 12107
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8063,
      "typeId": 11437,
      "title": "Linn Dress: Enabling a Dynamically Adjustable Neckline",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present the design and prototype of the Linn Dress, a transforming dress that allows its wearer to dynamically reveal and conceal areas of skin, as they feel comfortable and appropriate for different contexts. Electrochromic displays are employed as the elements enabling the garment's dynamic transparency. The Linn Dress is designed for office workers with a lively lifestyle, who shift directly from the work environment to social evening events. The dress presents professional working attire, and when transformed to reveal exciting open cuts and patterns, a cocktail dress.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9801
        },
        {
          "affiliations": [],
          "personId": 21967
        },
        {
          "affiliations": [],
          "personId": 17054
        },
        {
          "affiliations": [],
          "personId": 23391
        },
        {
          "affiliations": [],
          "personId": 17103
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2944,
      "typeId": 11437,
      "title": "Inferring the Character of Urban Commercial Areas from Age-biased Online Search Results",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We analyze the consumer-age-specific patterns of restaurant preferences in commercial areas of Seoul, through the mining of place recommendation results from the Naver Place online service. We calculate indices for 188 distinct areas of Seoul measuring the heterogeneity of taste across age groups, and the dominance of any one age group over the general options presented to the public. Our results suggest that both high-traffic and rapidly changing commercial areas present diverse options appealing to all age groups, and that this diversity is primarily driven by the tastes of younger age groups. Recognizing these patterns may help stakeholders predict gentrification and proactively shape neighborhood transformation from business turnover. This study contributes to the broader literature on applying online behavioral data to study urban economic activity.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10265
        },
        {
          "affiliations": [],
          "personId": 9463
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3969,
      "typeId": 11437,
      "title": "Theory Informed Framework for Integrating Environmental and Physiologic Data in Applications Targeting Productivity and Well-being in Workplace",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a framework targeting mental health in workplace by tracking the environmental and physiological variables. Environmental variables (noise, air quality, light intensity, etc.) will be collected by sensor nodes installed for each variable and physiological changes are measured by recording Electrocardiogram (ECG) signal, providing information about heart activity. ECG data is used to get Heart Rate Variability (HRV) values, indicating the physiological state based on Polyvagal Theory.  Data analysis from these two sources (environment and physiology) together helps to generate a system alert suggesting ways (short walk, listening to music) to manage the mental health without aggravating it, throughout the day. This framework creates awareness about the mental state of an individual, giving them opportunity to restore their mental state and protect it from getting worse. Mental health affects the productivity of a person. Targeting mental health improves productivity in a workplace. This paper proposes the framework and suggests future steps in deployment. The framework improves upon existing methods by considering dependence of environmental factors on individual autonomic state.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16556
        },
        {
          "affiliations": [],
          "personId": 12903
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3970,
      "typeId": 11437,
      "title": "Designing to Support Uncomfortable Breathing Exercises: Ethical Considerations",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 23097
        },
        {
          "affiliations": [],
          "personId": 15234
        },
        {
          "affiliations": [],
          "personId": 8446
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3971,
      "typeId": 11437,
      "title": "Improving Heart Rate Variability Measurements from Consumer Smartwatches with Machine Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The reactions of the human body to physical exercise, psychophysiological stress and heart diseases are reflected in heart rate variability (HRV). Thus, continuous monitoring of HRV can contribute to determining and predicting issues in well-being and mental health. HRV can be measured in everyday life by consumer wearable devices such as smartwatches which are easily accessible and affordable. However, they are arguably accurate due to the stability of the sensor. We hypothesize a systematic error which is related to the wearer movement. Our evidence builds upon explanatory and predictive modeling: we find a statistically significant correlation between error in HRV measurements and the wearer movement. We show that this error can be minimized by bringing into context additional available sensor information, such as accelerometer data. This work demonstrates our research-in-progress on how neural learning can minimize the error of such smartwatch HRV measurements.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14132
        },
        {
          "affiliations": [],
          "personId": 15929
        },
        {
          "affiliations": [],
          "personId": 8408
        },
        {
          "affiliations": [],
          "personId": 23588
        },
        {
          "affiliations": [],
          "personId": 8716
        },
        {
          "affiliations": [],
          "personId": 12614
        },
        {
          "affiliations": [],
          "personId": 20479
        },
        {
          "affiliations": [],
          "personId": 11337
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7044,
      "typeId": 11437,
      "title": "Prediction of Sleep Efficiency from Big Physical Exercise Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Physical exercise can improve sleep quality. However, how to perform physical exercise to achieve the best possible improvements is not clear. In this article, we build predictive models based on volume real data collected from wearable devices to predict the sleep efficiency related to users' daily exercise information. As far as we know, this is the first study to investigate insights of prediction of sleep efficiency from volume physical exercise data collected from real world.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21171
        },
        {
          "affiliations": [],
          "personId": 22460
        },
        {
          "affiliations": [],
          "personId": 23689
        },
        {
          "affiliations": [],
          "personId": 8515
        },
        {
          "affiliations": [],
          "personId": 11943
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8069,
      "typeId": 11437,
      "title": "Visual Large-scale Industrial Interaction Processing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this work we investigate the coordination of human-machine interactions from a bird's-eye view using a single panoramic color camera. Our approach replaces conventional physical hardware sensors, such as light barriers and switches, by location-aware virtual regions. We employ recent methods from the field of pose estimation to detect human and robot joint configurations. By fusing 2D human and robot pose information with prior scene knowledge, we can lift these perceptions to a 3D metric space. In this way, our system can initiate environmental reactions induced by geometric events among humans, robots and virtual regions. We demonstrate the diverse application possibilities and robustness of our system in three use cases.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14991
        },
        {
          "affiliations": [],
          "personId": 12435
        },
        {
          "affiliations": [],
          "personId": 12174
        },
        {
          "affiliations": [],
          "personId": 23320
        },
        {
          "affiliations": [],
          "personId": 17184
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6021,
      "typeId": 11437,
      "title": "Reevaluating Passive Haptic Learning of Morse Code",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Passive Haptic Learning (PHL) describes the learning of a motion, sequence or pattern without voluntary involvement of attention, focus or motivation through a haptic interface.\nIn previous PHL studies about teaching Morse code, we suspect that active learning processes were at least partially involved in causing extraordinarily high learning rates.\nTherefore we conduct a similar, 50-participant user study to investigate whether PHL is able to teach Morse code without active attention. Our study design differs from previous studies by preventing active learning explicitly (no feedback given) and implicitly (non-Morse patterns, tests prevent learning by rule of elimination) for three groups. Two other groups get the same feedback as was present in other studies.\nOur results show much lower learning rates when there is no active learning possible. Including active learning, we replicate the same learning results as have been reported.\nThis suggests that our suspicion was correct and that PHL of Morse Code is possible, but greatly limited. \nFurther studies about PHL must pay attention to whether their study design allows participants to learn actively - even small amounts of information that is actively learned may have a great impact on learning performance.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17448
        },
        {
          "affiliations": [],
          "personId": 19835
        },
        {
          "affiliations": [],
          "personId": 21333
        },
        {
          "affiliations": [],
          "personId": 16399
        }
      ],
      "sessionIds": [
        1324
      ],
      "eventIds": []
    },
    {
      "id": 8071,
      "typeId": 11437,
      "title": "Activity Recognition in Outdoor Sports Environments: Smart Data for End-Users Involving Mobile Pervasive Augmented Reality Systems",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Activity recognition is an increasingly relevant topic in the context of the most varied end-user services. In outdoor environments, activity recognition based on close-to-real-time information is key in providing awareness to the user about their habits and thus to allow the user to better perform. Such promptness is today provided via multiple sensors and performed both actively and passively via fitness gadgets, smartphones, mobile pervasive systems, and other personal mobile gadgets. In this context, it is relevant to understand how data from multiple sensors can be fused, interpreted and classified, to best provide feedback using the recognized activities, especially for smart data feedback. This paper presents a first analysis in activity recognition for adjustment of the information feedback to the user based on data from common sensors that are today available in personal devices. Regarding the special case of Mobile Augmented Reality Systems for outdoors, the paper also debates the existent restrictions imposed by applications' usage in these environments, describing possible use scenarios and presenting results of an experiment for discriminating activities when using mobile sensors' tracking.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21921
        },
        {
          "affiliations": [],
          "personId": 8565
        },
        {
          "affiliations": [],
          "personId": 20667
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7048,
      "typeId": 11437,
      "title": "Demo: UbiCAT - Wearable Technology for Ubiquitous Cognitive Assessment",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The Ubiquitous Cognitive Assessment Tool (UbiCAT) is a wearable technology designed for `in-the-wild' cognitive assessment. UbiCAT includes three smartwatch-based applications adapted from the Stroop color-word, n-back, and two-choice reaction time tests, respectively. UbiCAT aims to measure selective attention and processing speed, working memory, and inhibition control. UbiCAT can be used for real-life cognitive assessment and for experiments on human cognitive performance. Within the field of ubiquitous computing, it contributes to the cognition-aware systems.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21114
        },
        {
          "affiliations": [],
          "personId": 23811
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3979,
      "typeId": 11437,
      "title": "Using Smart Clothing to Improve Movement",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 9662
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2955,
      "typeId": 11437,
      "title": "Demo: ASSV: Handwritten Signature Verification Using Acoustic Signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "As one kind of biological characteristics of people, handwritten signature has been widely used in the banking industry, government and education. Verifying handwritten signatures manually causes too much human cost, and its high probability of errors can threaten the property safety and even society stability. Therefore, the need for an automatic verification system is emphasized. This paper proposes a device-free on-line handwritten signature verification system ASSV, providing paper-based handwritten signature verification service. As far as  we know, ASSV is the first system which uses the changes of acoustic signals to realize signature verification. ASSV differs from previous on-line signature verification work in two aspects: 1. It requires neither a special sensor-instrumented pen nor a tablet; 2. People do not need to wear a device such as a smartwatch on the dominant hand for hand tracking. Differing from previous acoustic-based sensing systems, ASSV uses a novel chord-based method to estimate phase-related changes caused by tiny actions. Then based on the estimation, frequency-domain features are extracted by a discrete cosine transform (DCT). Moreover, a deep convolutional neural network (CNN) model fed with distance matrices is designed to verify signatures. Extensive experiments show that ASSV is a robust, efficient and secure system achieving an AUC of 98.7% and an EER of 5.5% with a low latency.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23632
        },
        {
          "affiliations": [],
          "personId": 21016
        },
        {
          "affiliations": [],
          "personId": 13416
        },
        {
          "affiliations": [],
          "personId": 17043
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2961,
      "typeId": 11437,
      "title": "Degradable Inference for Energy Autonomous Vision Applications",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile vision systems, often battery-powered, are now incredibly powerful in capturing, analyzing, and understanding real-world events uncovering interminable opportunities for new applications in the areas of life-logging, cognitive augmentation, security, safety, wildlife surveillance, etc. There are two complementary challenges in the design of a mobile vision system today - improving the recognition accuracy at the expense of minimum energy consumption. In this work, we posit that best-effort sensing with degradable featurization and an elastic inference pipeline offers an interesting avenue to bring energy autonomy to mobile vision systems while ensuring acceptable recognition performance. Borrowing principles from Intermittent Computing, and Numerical Computing we propose such best-effort sensing using a Degradable-Inference pipeline supported by a parameterized Discrete Cosine Transformation (DCT) based featurization and an Anytime Deep Neural Network. These two principles aim at extending the lifetime of a mobile vision system while minimizing compute and communication cost without compromising recognition performance. We report the design and early characterization of our proposed solution.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22182
        },
        {
          "affiliations": [],
          "personId": 21039
        },
        {
          "affiliations": [],
          "personId": 23587
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8082,
      "typeId": 11437,
      "title": "Poster: Designing Navigation Aides for Wildland Firefighters",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this research, we explored the design of navigation technology for wildland firefighters. We worked within a set of empirically informed design constraints to create prototypes of a wearable system that provides peripheral navigation cues via visual and haptic feedback. We used physical and interactive prototypes of this system as technology probes to provoke discussions with wildland firefighters about their navigation and location technology needs. Our pilot study results indicate that our prototypes helped to uncover ideas for future technical work in the domain of wildland firefighting, as well as on mobile and wearable navigation systems, more broadly.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14280
        },
        {
          "affiliations": [],
          "personId": 15380
        },
        {
          "affiliations": [],
          "personId": 14512
        },
        {
          "affiliations": [],
          "personId": 13545
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2964,
      "typeId": 11437,
      "title": "Safe Street Rangers: Crowdsourcing Approach for Monitoring and Reportng Street Safety",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Looking out for each other’s safety is always a nice thing to do. Crowdsourcing enables us to do just that as we’ve developed a system called Safe Street Rangers that allows the user or ranger to monitor and report via a mobile app the level of safety concern of any street segment regarding seven aspects including traffic signs, road obstacles, brightness, road condition, animals, solitariness, and traffic accidents, in relation to a transport mode (i.e., driving or walking). The system consists of three main components; mobile app, web app, and data server. Each submitted report will be verified and approved by an admin user via our web app. The data server handles all data storage and processing. It checks for overlapping of reported street segments with the existing ones and updates the safety rating values of street waypoints accordingly. The system has been tested with the real users from which its usefulness is highly perceived.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18617
        },
        {
          "affiliations": [],
          "personId": 23364
        },
        {
          "affiliations": [],
          "personId": 9607
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5013,
      "typeId": 11437,
      "title": "FlexTouch: Enabling Large-Scale Interaction Sensing Beyond Touchscreens Using Flexible and Conductive Materials",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present FlexTouch, a technique that enables large-scale interaction sensing beyond the spatial constraints of capacitive touchscreens using passive low-cost conductive materials. This is achieved by customizing 2D circuit-like patterns with an array of conductive strips that can be easily attached to the sensing nodes on the edge of the touchscreen. This retrofit requires no hardware modification to the device. FlexTouch is compatible with various conductive materials (copper foil tape, silver nanoparticle ink, ITO frame, and carbon paint), as well as fabrication methods (cutting, coating, and ink-jet printing). Through a series of studies and illustrative examples, we demonstrate that FlexTouch can support long-range touch sensing for up to 4 meters and everyday object presence detection for up to 2 meters. Finally, we show the versatility and feasibility of FlexTouch through applications such as whole-body posture recognition, human-object interaction as well as enhanced fitness training experiences.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11984
        },
        {
          "affiliations": [],
          "personId": 20648
        },
        {
          "affiliations": [],
          "personId": 11773
        },
        {
          "affiliations": [],
          "personId": 13882
        },
        {
          "affiliations": [],
          "personId": 10407
        },
        {
          "affiliations": [],
          "personId": 20826
        },
        {
          "affiliations": [],
          "personId": 14464
        },
        {
          "affiliations": [],
          "personId": 11044
        },
        {
          "affiliations": [],
          "personId": 15758
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 8087,
      "typeId": 11437,
      "title": "W!NCE: Unobtrusive Sensing of Upper Facial Action Units with EOG-based Eyewear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The ability to unobtrusively and continuously monitor one’s facial expressions has implications for a variety of application domains ranging from affective computing to health-care and the entertainment industry. The standard Facial Action Coding System (FACS) along with camera based methods have been shown to provide objective indicators of facial expressions; however, these approaches can also be fairly limited for mobile applications due to privacy concerns and awkward positioning of the camera. To bridge this gap, W!NCE re-purposes a commercially available Electrooculography-based eyeglass (J!NS MEME) for continuously and unobtrusively sensing of upper facial action units with high fidelity. W!NCE detects facial gestures using a two-stage processing pipeline involving motion artifact removal and facial action detection. We validate our system’s applicability through extensive evaluation on data from 17 users under stationary and ambulatory settings, a pilot study for continuous pain monitoring and several performance benchmarks. Our results are very encouraging, showing that we can detect five distinct facial action units with a mean F1 score of 0.88 in stationary and 0.82 in ambulatory settings, and that we can accurately detect facial gestures that due to pain.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22477
        },
        {
          "affiliations": [],
          "personId": 9345
        },
        {
          "affiliations": [],
          "personId": 20354
        },
        {
          "affiliations": [],
          "personId": 21097
        },
        {
          "affiliations": [],
          "personId": 17655
        }
      ],
      "sessionIds": [
        2258
      ],
      "eventIds": []
    },
    {
      "id": 2969,
      "typeId": 11437,
      "title": "Electrodermal Activity Sensing Using Smart EyeWear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 14326
        },
        {
          "affiliations": [],
          "personId": 17920
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7065,
      "typeId": 11437,
      "title": "Modeling Biobehavioral Rhythms with Passive Sensing in the Wild: A Case Study to Predict Readmission Risk after Pancreatic Surgery",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Biobehavioral rhythms are associated with numerous health and life outcomes. We study the feasibility of detecting rhythms in data that is passively collected from Fitbit devices and using the obtained model parameters to predict readmission risk after pancreatic surgery. We analyze data from 49 patients who were tracked before surgery, in hospital, and after discharge. Our analysis produces a model of individual patients' rhythms for each stage of treatment that is predictive of readmission. All of the rhythm-based models outperform the traditional approaches to readmission risk stratification that uses administrative data.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18793
        },
        {
          "affiliations": [],
          "personId": 18197
        },
        {
          "affiliations": [],
          "personId": 22030
        },
        {
          "affiliations": [],
          "personId": 8588
        }
      ],
      "sessionIds": [
        2018
      ],
      "eventIds": []
    },
    {
      "id": 3996,
      "typeId": 11437,
      "title": "Detachable Smartwatch: More Than A Wearable",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Glanceability and low access time are arguably the key assets of a smartwatch. However, smartwatches are currently limited to micro-interactions. They do not enable complex interactions and, in general, they do not afford continuous use for long. We believe that smartwatches can retain micro-interactions and glanceability, but also get better at long and complex interactions. We propose a smartwatch that a user can detach, and use as more than a wearable depending on their context, requirements, and preference. Detaching the watch enables it to morph into different forms, and thereby become a better interaction device, better display, and a better sensor suite. First, we interview participants to elicit usage themes for a detachable watch. Then, we build several applications that showcase the range of use-cases where a detachable smartwatch offers additional functionality compared to an always-worn one, and highlights the affordances and benefits enabled due to detachability.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21236
        },
        {
          "affiliations": [],
          "personId": 13444
        },
        {
          "affiliations": [],
          "personId": 15661
        }
      ],
      "sessionIds": [
        1399
      ],
      "eventIds": []
    },
    {
      "id": 8101,
      "typeId": 11437,
      "title": "Poster: Data Transmission Method using Touch Generation Device for Multi-touch Display",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We propose and implement data transmission method for capacitive touch panel using physical interface and microcomputer that can generate touch to capacitive touch panel. The proposed device consists of one-board microcomputer, an external power supply, and multiple touch control devices that can be individually controlled. \nOne touch control device is in contact with one touch area, and touch is individually generated for the touch panel. Our software in the touch panel detects touch patterns. \nThe snapshot of the detected touch patterns is interpreted as a bit string. By controlling the touch generation device at high speed, long bit string can be transmitted .",
      "authors": [
        {
          "affiliations": [],
          "personId": 10106
        },
        {
          "affiliations": [],
          "personId": 13824
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 2982,
      "typeId": 11437,
      "title": "NALoc: Nonlinear Ambient-Light-Sensor-based Localization System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Visible light position (VLP) is an revolutionary technique which enables many promising applications. As human eyes are sensitive to low-rate changes, VLP systems often convey location information through light flickering over 1 KHz, which induces a heavy burden on the VLP receiver. Existing solutions either rely on high-resolution camera or dedicated photodiode to capture the location information, but the high power consumption and extract deployment cost hinder their wide adoption. In this paper, we present a light-weight VLP system, NALoc, which leverages the ambient light sensor (ALS) readily on many mobile devices to sense the high-frequency-modulation location information. To overcome the insufficient sampling ability of ALS, we exploit the nonlinearity of ALS to sense the leaked energy from high frequency(>1 KHz) at a low sampling rate(100 Hz). Extensive evaluations demonstrate that our system can achieve a decimeter-level localization accuracy with about 1 mW power consumption, which is 2000 times less than existing camera-based VLP solutions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13687
        },
        {
          "affiliations": [],
          "personId": 16806
        },
        {
          "affiliations": [],
          "personId": 22125
        },
        {
          "affiliations": [],
          "personId": 13416
        }
      ],
      "sessionIds": [
        1177
      ],
      "eventIds": []
    },
    {
      "id": 6056,
      "typeId": 11437,
      "title": "Ubiquitous Smart Eyewear Interactions using Implicit Sensing and Unobtrusive Information Output",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 23064
        },
        {
          "affiliations": [],
          "personId": 24251
        },
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 13142
        },
        {
          "affiliations": [],
          "personId": 24289
        },
        {
          "affiliations": [],
          "personId": 8984
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7080,
      "typeId": 11437,
      "title": "FarmChat: A Conversational Agent to Answer Farmer Queries",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Farmers constitute 54.6% of the Indian population, but earn only 13.9% of the national GDP. This gross mismatch can be alleviated by improving farmers' access to information and expert advice (e.g., knowing which seeds to sow and how to treat pests can significantly impact yield). In this paper, we report our experience of designing a conversational agent, called FarmChat, to meet the information needs of farmers in rural India. We conducted an evaluative study with 34 farmers near Ranchi in India, focusing on assessing the usability of the system, acceptability of the information provided, and understanding the user population's unique preferences, needs, and challenges in using the technology. We performed a comparative study with two different modalities: audio-only and audio+text. Our results provide a detailed understanding on how literacy level, digital literacy, and other factors impact users' preferences for the interaction modality. We found that a conversational agent has the potential to effectively meet the information needs of farmers at scale. More broadly, our results could inform future work on designing conversational agents for user populations with limited literacy and technology experience.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19667
        },
        {
          "affiliations": [],
          "personId": 17701
        },
        {
          "affiliations": [],
          "personId": 21504
        },
        {
          "affiliations": [],
          "personId": 23868
        },
        {
          "affiliations": [],
          "personId": 20868
        },
        {
          "affiliations": [],
          "personId": 11044
        }
      ],
      "sessionIds": [
        2023
      ],
      "eventIds": []
    },
    {
      "id": 8106,
      "typeId": 11437,
      "title": "Mobile Gait Analysis Using Foot-Mounted UWB Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We demonstrate a new foot-mounted sensor system for mobile gait analysis which is based on Ultra Wideband (UWB) technology. Our system is wireless, inexpensive, portable, and able to estimate clinical measurements that are not currently available in traditional Inertial Measurement Unit (IMU) based wearables such as step width and foot positioning.  We collect a dataset of over 2000 steps across 21 people to test our system in comparison with the clinical gold-standard GAITRite, and other IMU-based algorithms. We propose methods to calculate gait metrics from the UWB data that our system collects. Our system is then validated against the GAITRite mat, measuring step width, step length, and step time with mean absolute errors of 0.033m, 0.035m, and 0.016s respectively. This system has the potential for use in many fields including sports medicine, neurological diagnostics, fall risk assessment, and monitoring of the elderly.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12206
        },
        {
          "affiliations": [],
          "personId": 10619
        },
        {
          "affiliations": [],
          "personId": 12072
        },
        {
          "affiliations": [],
          "personId": 8795
        }
      ],
      "sessionIds": [
        1701
      ],
      "eventIds": []
    },
    {
      "id": 4013,
      "typeId": 11437,
      "title": "Orange Labs contribution to the sussex-huawei locomotion-transportation recognition challenge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper,  our team (Orange Labs)  propose to address the task of the Sussex-Huawei Locomotion-Transportation (SHL) recognition challenge (2019), which consists in recognizing the user transportation mode based on the smartphone inertial sensors data, by using method of recurrent neural networks. The bidirectional LSTM architecture has been proposed to solve this challenge. The model was trained on rotation and translation invariant features in order to ignore the information of smartphone orientation and location. Our preliminary results show that the proposed method reaches the accuracy of 60.4% for the user transportation mode recognition by using hand validation data as testing data. Based on this SHL recognition challenge, it has been proposed to take advantage of the obtained model and try to integrate it into our continuous identity authentication project.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13149
        },
        {
          "affiliations": [],
          "personId": 23690
        },
        {
          "affiliations": [],
          "personId": 22364
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5038,
      "typeId": 11437,
      "title": "Automated Inference of Cognitive Performance by Fusing Multimodal Information Acquired by Smartphone",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Recognizing human cognitive performance is important for preserving working efficiency and preventing human error. This paper presents a method for estimating cognitive performance by leveraging multiple information available in a smartphone. The method employs the Go-NoGo task to measure cognitive performance, and fuses contextual and behavioral features to identify the level of performance. It was confirmed that the proposed method could recognize whether cognitive performance was high or low with an average accuracy of 71%,  even when only referring to inertial sensor logs. Combining sensing modalities improved the accuracy up to 74%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13054
        },
        {
          "affiliations": [],
          "personId": 19947
        },
        {
          "affiliations": [],
          "personId": 19699
        },
        {
          "affiliations": [],
          "personId": 12048
        },
        {
          "affiliations": [],
          "personId": 20541
        },
        {
          "affiliations": [],
          "personId": 15102
        },
        {
          "affiliations": [],
          "personId": 13623
        },
        {
          "affiliations": [],
          "personId": 22300
        },
        {
          "affiliations": [],
          "personId": 16714
        },
        {
          "affiliations": [],
          "personId": 13007
        },
        {
          "affiliations": [],
          "personId": 13419
        },
        {
          "affiliations": [],
          "personId": 10222
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5041,
      "typeId": 11437,
      "title": "Poster Efficient Convolutional Neural Network for FMCW radar based Hand Gesture Recognition",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "FMCW radar could detect object's range, speed and Angle-of-Arrival, \nadvantages are robust to bad weather, good range resolution, and good speed resolution. \nIn this paper, we consider the FMCW radar as a novel interacting interface on laptop. \nWe merge sequences of object's range, speed, azimuth information into single input,\nthen feed to a convolution neural network to learn spatial and temporal patterns.\nOur model achieved 96\\% accuracy on test set and real-time test.",
      "authors": [
        {
          "affiliations": [],
          "personId": 21972
        },
        {
          "affiliations": [],
          "personId": 9865
        },
        {
          "affiliations": [],
          "personId": 14517
        },
        {
          "affiliations": [],
          "personId": 13630
        },
        {
          "affiliations": [],
          "personId": 20351
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7089,
      "typeId": 11437,
      "title": "Understanding Motivators, Constraints, and Practices of Sharing Internet of Things",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smart devices such as mobile phones, tablets, and smart watches are designed under the assumption that they will be used by a single user. In contrast, many other devices such as smart thermostats and smart speakers are inherently sharable. This paper presents results from a multi-methods study that we conducted with 20 participants to gain a nuanced understanding of purposes, motivators, and constraints of sharing such smart devices, which are cumulatively referred to as Internet of Things. We also illustrate users' practices of coordinating their shared use with sharees/co-users, impact of not understanding smart device's behavior and user's context on shared use, and differences between sharing personal and inherently sharable devices in terms of content accessed or available, trust between sharees, and measures implemented for accountable use. Finally, we discuss the implications of our findings and provide guidelines for the design of future smart devices.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9158
        },
        {
          "affiliations": [],
          "personId": 11837
        }
      ],
      "sessionIds": [
        1665
      ],
      "eventIds": []
    },
    {
      "id": 5047,
      "typeId": 11437,
      "title": "CORMORANT: Ubiquitous Risk-Aware Multi-Modal Biometric Authentication Across Mobile Devices",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "People own and carry an increasing number of ubiquitous mobile devices, such as smartphones, tablets, and notebooks. Being small and mobile, those devices have a high propensity to become lost or stolen. Since mobile devices provide access to their owners’ digital lives, strong authentication is vital to protect sensitive information and services against unauthorized access. However, at least one in three devices is unprotected, with inconvenience of traditional authentication being the paramount reason.\n\nWe present the concept of CORMORANT, an approach to significantly reduce the manual burden of mobile user verification through risk-aware, multi-modal biometric, cross-device authentication. Transparent behavioral and physiological biometrics like gait, voice, face, and keystroke dynamics are used to continuously evaluate the user’s identity without explicit interaction. The required level of confidence in the user’s identity is dynamically adjusted based on the risk of unauthorized access derived from signals like location, time of day and nearby devices. Authentication results are shared securely with trusted devices to facilitate cross-device authentication for co-located devices. Conducting a large-scale agent-based simulation of 4 000 users based on more than 720 000 days of real-world device usage traces and 6.7 million simulated robberies and thefts sourced from police reports, we found the proposed approach is able to reduce the frequency of password entries required on smartphones by 97.82% whilst simultaneously reducing the risk of unauthorized access in the event of a crime by 97.72%, compared to conventional knowledge-based authentication.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9952
        },
        {
          "affiliations": [],
          "personId": 17691
        },
        {
          "affiliations": [],
          "personId": 15623
        },
        {
          "affiliations": [],
          "personId": 20145
        },
        {
          "affiliations": [],
          "personId": 22900
        },
        {
          "affiliations": [],
          "personId": 18936
        },
        {
          "affiliations": [],
          "personId": 17212
        },
        {
          "affiliations": [],
          "personId": 16878
        }
      ],
      "sessionIds": [
        2039
      ],
      "eventIds": []
    },
    {
      "id": 4024,
      "typeId": 11437,
      "title": "SoundSignaling: Realtime, Stylistic Modification of a Personal Music Corpus for Information Delivery",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Drawing inspiration from the notion of cognitive incongruence associated with Stroop's famous experiment, from musical principles, and from the observation that music consumption on an individual basis is becoming increasingly ubiquitous, we present the SoundSignaling system -- a software platform designed to make real-time, stylistically relevant modifications to a personal corpus of music as a means of conveying information or notifications. In this work, we discuss in detail the system's technical implementation and its motivation from a musical perspective, and validate these design choices through a crowd-sourced signal identification experiment consisting of 200 independent tasks performed by 50 online participants.  We then qualitatively discuss the potential implications of such a system from the standpoint of switch cost, cognitive load, and listening behavior by considering the anecdotal outcomes of a small-scale, in-the-wild experiment consisting of over 180 hours of usage from 6 participants. Through this work, we suggest a re-evaluation of the age-old paradigm of binary audio notifications in favor of a system designed to operate upon the relatively unexplored medium of a user's musical preferences.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9571
        },
        {
          "affiliations": [],
          "personId": 9949
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 5050,
      "typeId": 11437,
      "title": "Towards Making Kinetic Garments Based on Conductive Fabric and Smart Hair",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce a novel making method for kinetic garments, which can change their shape and are thus used for decoration and self-expression. Such garments are generally realized by attaching devices to garments, and wiring and control systems are required to make them. In particular, wiring becomes complicated when many actuators are used. This issue is an obstacle for fashion designers and increases production difficulty. We propose a power supply and communication method for devices distributed on conductive fabric. This method allows the individual control of devices on the fabric without complicated wiring. We use soft hair-like bending actuators, called Smart Hair, as devices. The present study evaluates the application of the proposed method to Smart Hair.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14332
        },
        {
          "affiliations": [],
          "personId": 16689
        },
        {
          "affiliations": [],
          "personId": 21115
        },
        {
          "affiliations": [],
          "personId": 23802
        },
        {
          "affiliations": [],
          "personId": 15212
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 4026,
      "typeId": 11437,
      "title": "sharedCharging: Data-Driven Shared Charging Scheduling for Large-Scale Heterogeneous Electric Vehicle Fleets",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Our society is witnessing a rapid vehicle electrification process. Even though being environmental-friendly, electric vehicles have not reached their full potentials due to prolonged charging time. Moreover, unbalanced spatiotemporal charging demand/supply along with the uneven number of charging stations between heterogeneous fleets make electric vehicle management more challenging, e.g., surplus charging stations across a city for electric buses but limited charging stations in some regions for electric taxis, which severely limit the charging performance of the whole electric vehicle network in a city. In this paper, we first analyze a large-scale real-world dataset from two heterogeneous electric vehicle fleets in the Chinese city Shenzhen. We investigate their mobility and charging patterns and then verify the practicability and necessity of shared charging. Based on the insights we found, we design a generic real-time shared charging scheduling system called sharedCharging to improve overall charging efficiency for heterogeneous electric vehicle fleets. Our sharedCharging also considers sophisticated real-world constraints, e.g., station spaces, availability of charging points, real-time timetable guarantee, etc. More importantly, we take the electric bus and electric taxi fleets as a concrete example of heterogeneous electric vehicle fleets given their different operating patterns. We implement and evaluate sharedCharging with streaming data from over 13,000 electric taxis and 16,000 electric buses, coupled with the charging station data in the Chinese city Shenzhen, which is the largest public electric vehicle network in the world. The evaluation results demonstrate that the proposed sharedCharging reduces the waiting time by 63.5% and reduces the total charging time by 15% on average for e-taxis.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14507
        },
        {
          "affiliations": [],
          "personId": 21956
        },
        {
          "affiliations": [],
          "personId": 16595
        },
        {
          "affiliations": [],
          "personId": 21167
        },
        {
          "affiliations": [],
          "personId": 23100
        },
        {
          "affiliations": [],
          "personId": 21598
        },
        {
          "affiliations": [],
          "personId": 10401
        },
        {
          "affiliations": [],
          "personId": 11897
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 6076,
      "typeId": 11437,
      "title": "Poster: SmartLobby: Using a 24/7 Remote Head-Eye-Tracking System for Content Personalization",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we present the SmartLobby, an intelligent environment system integrated into the lobby of a research institute. \nThe SmartLobby is running 24/7, i.e. it can be used any time by anyone without any preparations.\nThe goal of the system is to conduct research in the domain of human machine cooperation.\nOne important first step towards this goal is a detailed human state modelling and estimation with head-eye-tracking as key component.\nThe SmartLobby mainly integrates state-of-the-art algorithms that enable a thorough analysis of human behavior and state. \nThese algorithms constitute the fundamental basis for the development of higher level system components.\nHere, we present our system with its various hardware and software components.\nThereby, we focus on the head-eye-tracking as a key component to continuously observe persons using the system and customize content shown to them.\nThe results of a multi-week lasting experiment demonstrate the effectiveness of the system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20429
        },
        {
          "affiliations": [],
          "personId": 8279
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7101,
      "typeId": 11437,
      "title": "GestEar: Combining Audio and Motion Sensing for Gesture Recognition on Smartwatches",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present GestEar, a gesture recognition method for sound-emitting gestures, such as snapping, knocking, or clapping, using only a simple smartwatch. Besides the motion information from the built-in accelerometer and gyroscope, we exploit audio data recorded by the smartwatch microphone as input. We propose a lightweight convolutional neural network architecture for gesture recognition, specifically designed to run locally on resource-constrained devices, which achieves a user-independent recognition accuracy of 97.2% for nine distinct gestures. We further show how to incorporate gesture detection and gesture classification in the same network, compare different network designs, and showcase a number of different applications built with our method. We find that the audio input drastically reduces the false positive rate in continuous recognition compared to using only motion.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18837
        },
        {
          "affiliations": [],
          "personId": 15086
        },
        {
          "affiliations": [],
          "personId": 13125
        }
      ],
      "sessionIds": [
        1270
      ],
      "eventIds": []
    },
    {
      "id": 8128,
      "typeId": 11437,
      "title": "AcousticID: Gait-based Human Identification Using Acoustic Signal",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human identification plays an important role in our daily lives.Previous work has successfully used fingerprints, irises, and face, etc for identity recognition. However, these methods require the user being close to the sensing device, which may cause inconvenience to users. In this paper, we present AcousticID, a framework that uses fine-grained gait information derived from acoustic signals generated by Commercial Off-The-Shelf devices to identify people. We demonstrate the feasibility of gait recognition by analyzing the Doppler effect of various body parts on acoustic signals while walking, and discuss the fine-grained gait features that can distinguish different people from both macro and micro dimensions. Similar to an access control system in a home or office, AcousticID is an accessible, easy, low cost, and universal solution. We evaluate AcousticID using experiments with 50 volunteers in an area of 60 m2, and the results show that it can identify a person with an average accuracy of 96.6%. We conclude by discussing the challenges of acoustic signal based person identiﬁcation and potential future improvements.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19356
        },
        {
          "affiliations": [],
          "personId": 12522
        },
        {
          "affiliations": [],
          "personId": 12564
        },
        {
          "affiliations": [],
          "personId": 13318
        },
        {
          "affiliations": [],
          "personId": 21070
        }
      ],
      "sessionIds": [
        2039
      ],
      "eventIds": []
    },
    {
      "id": 6087,
      "typeId": 11437,
      "title": "How to shape the future of Smart Clothing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 13023
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7112,
      "typeId": 11437,
      "title": "Poster: 'Toward Digital Image Processing and Eye Tracking to Promote Visual Attention for People with Autism'",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Visual attention is guided by the integration of two streams: the global, that rapidly processes the scene, and the local, that processes details. For people with autism, the integration of these two streams can be disrupted by the tendency to privilege details (local processing) instead of seeing the big picture (global processing). Consequently, people with autism struggle with typical visual attention, evidenced by their verbal description of local features when asked to describe overall scenes, disrupting their social understanding. This work aims to explore an augmentation for global processing by digitally filtering visual stimuli. This work contributes initial prototypes to improve global processing and leverages an eye tracking dataset to compare results as a validation technique.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14579
        },
        {
          "affiliations": [],
          "personId": 10366
        },
        {
          "affiliations": [],
          "personId": 13019
        },
        {
          "affiliations": [],
          "personId": 15354
        },
        {
          "affiliations": [],
          "personId": 9147
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7113,
      "typeId": 11437,
      "title": "Detecting Door Events Using a Smartphone via Active Sound Sensing",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Event detection of indoor objects, including doors, has a wide variety of applications, including intruder detection, HVAC control, and surveillance of independently living elderly people. Hence, this has been the focus of multiple research projects in the UbiComp research community. Herein, we propose a method to accurately detect door events in an indoor environment, without the installation and maintenance costs of using distributed ubiquitous sensors. In particular, we recognize the events of multiple doors existing in the environment via active sound probing using a disused smartphone installed in the environment. We perform event recognition by fusing the analysis of the Doppler shift caused by the moving doors with the acoustic characteristics describing the open/close states of the doors acquired via impulse response. To accurately distinguish between the events of different doors via sound probing, our method employs the time-series analysis of the Doppler shift as well as the active sound probing using directional high-frequency sine waves and stereo sound recording. In addition, by incorporating prior knowledge about the state transitions of a door object into a recognition model, we attempt to improve the accuracy of event recognition. Moreover, our method is capable of recognizing walking activities of a person related to door events in the environment, which are necessary information for applications such as HVAC control that require information about both door events and human presence.",
      "authors": [
        {
          "affiliations": [],
          "personId": 9640
        },
        {
          "affiliations": [],
          "personId": 24265
        },
        {
          "affiliations": [],
          "personId": 16880
        },
        {
          "affiliations": [],
          "personId": 10040
        }
      ],
      "sessionIds": [
        1270
      ],
      "eventIds": []
    },
    {
      "id": 8139,
      "typeId": 11437,
      "title": "Demo: http://eyewear.pro - An Open Platform to Record and Analyze Large Scale Data Sets from Smart Eyewear",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this demo, we present the smart eyewear toolchain consisting of smart glasses prototypes and a software platform for cognitive and social interaction assessments in the wild, with several application cases and a demonstration of activity recognition in real-time. The platform is designed to work with Jins MEME, smart EOG enabled glasses, The user software is capable data logging, posture tracking and recognition of several activities, such as talking, reading and blinking. During the demonstration we will walk through several applications and studies that the platform has been used for.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17920
        },
        {
          "affiliations": [],
          "personId": 16863
        },
        {
          "affiliations": [],
          "personId": 23757
        },
        {
          "affiliations": [],
          "personId": 10481
        },
        {
          "affiliations": [],
          "personId": 9113
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8140,
      "typeId": 11437,
      "title": "Indexing Visual Qualities of Streetscapes and its Influence to Human Physiology through Wearable Sensors",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we explore the influence of street space visual qualities on human physiology and perception of comfort at selected street corners (i.e., Fumin, Changde, Xinle and Donghu Road, Jing'an district, Shanghai). The visual qualities of the street were identified by the physical space variations, sky visibility, wall continuity and the cross-sectional proportion. These three variables contribute to the \"enclosure index\", a dimensionless numeral which defines the occupant perception of the street space. We used a custom biosensor kit to collect 15 participants' average heart rates for one minute measured at aforementioned street corners. We compared participant's heart rates when they looked toward the intersection (open street space) and looked down the street (enclosed street space) and we asked them to complete a questionnaire on comfort level. The data of questionnaire was then compared to the corresponding heart rates. Results demonstrated that heart rates of participants who looked at the street view (more enclosed) were lower than those when looking in corner view (less enclosed).",
      "authors": [
        {
          "affiliations": [],
          "personId": 18974
        },
        {
          "affiliations": [],
          "personId": 24402
        },
        {
          "affiliations": [],
          "personId": 17674
        },
        {
          "affiliations": [],
          "personId": 10026
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7117,
      "typeId": 11437,
      "title": "Motion2Vector: Unsupervised Learning in Human Activity Recognition Using Wrist-Sensing Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing popularity of consumer wearable devices augmented with sensing capabilities (smart bands, smart watches), there is a significant focus on extracting meaningful information about human behaviour through large scale real-world wearable sensor data. The focus of this work is to develop techniques to detect human activities, utilising a large dataset of wearable data where no ground truth has been produced on the actual activities performed. We propose a deep learning variational auto encoder activity recognition model - Motion2Vector. The model is trained using large amounts of unlabelled human activity data to learn a representation of a time period of activity data. The learned activity representations can be mapped into an embedded activity space and grouped with regards to the nature of the activity type. In order to evaluate the proposed model, we have applied our method on public dataset - The Heterogeneity Human Activity Recognition (HHAR) dataset. The results showed that our method can achieve improved result over the HHAR dataset. In addition, we have collected our own lab-based activity dataset. Our experimental results show that our system achieves good accuracy in detecting such activities, and has the potential to provide additional insights in understanding the real-world activity in the situations where there is no ground truth available.",
      "authors": [
        {
          "affiliations": [],
          "personId": 24295
        },
        {
          "affiliations": [],
          "personId": 23841
        },
        {
          "affiliations": [],
          "personId": 18972
        },
        {
          "affiliations": [],
          "personId": 8527
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4046,
      "typeId": 11437,
      "title": "From Fingerprint to Footprint: Cold-start Location Recommendation by Learning User Interest from App Data",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "With increasing diversity of user interest and preference, personalized location recommendation is essential and beneficial to our daily life. To achieve this, the most critical challenge is the cold-start recommendation problem, for we cannot learn preference from cold-start users without any historical records. In this paper, we demonstrate that it is feasible to make personalized location recommendation by learning user interest and location features from app usage data. By proposing a novel generative model to transfer user interests from app usage behavior to location preference, we achieve personalized location recommendation via learning the interest's correlation between locations and apps. Based on two real-world datasets, we evaluate our method's performance with a variety of scenarios and parameters. The results demonstrate that our method outperforms the state-of-the-art solutions in solving cold-start problem, $i.e.$, when there are 60\\% cold-start users, we can still achieve a 77.0\\% hitrate in recommending the top five locations, which is at least 9.6\\% higher than the baselines. Our study is the first step forward for transferring user interests learning from online fingerprints to offline footprints, which paves the way for better personalized location recommendation services.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20606
        },
        {
          "affiliations": [],
          "personId": 18685
        },
        {
          "affiliations": [],
          "personId": 24147
        },
        {
          "affiliations": [],
          "personId": 15961
        },
        {
          "affiliations": [],
          "personId": 13666
        },
        {
          "affiliations": [],
          "personId": 9082
        }
      ],
      "sessionIds": [
        2123
      ],
      "eventIds": []
    },
    {
      "id": 7118,
      "typeId": 11437,
      "title": "Listening Space: Satellite Ikats",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Listening Space is an artistic research that was born during the eTextile Spring Break camp that took place in upstate New York at the beginning of April 2019. Following their previous explorations of ecologies of transmissions and wanting to experiment with Software-Defined Radio, the authors, setup a DIY satellite tracking station and aimed at intercepting the NOAA weather satellite audiovisual transmissions. During the course of three days, they observed five satellite passes, intercepted successfully three transmissions and decoded the audio signals into images which they later knitted in order to create a textile archive of the transmissions. Conceptually the project seeks to explore transmissions ecologies as raw material for artistic exploration, to understand and re-imagine in poetic means, representations of audio and images broadcasted from space, while regarding knitted textiles as a physical medium for memory storage and archiving.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8537
        },
        {
          "affiliations": [],
          "personId": 18733
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8143,
      "typeId": 11437,
      "title": "On-body Sensing of Cocaine Craving, Euphoria and Drug-Seeking Behavior Using Cardiac and Respiratory Signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Drug addiction is a chronic brain-based disorder that affects a person's behavior and leads to an inability to control drug usage. Ubiquitous physiological sensing technologies to detect illicit drug use have been well studied and understood for different types of drugs. However, we currently lack the ability to continuously and passively measure the user state in ways that might shed light on the complex relationships between cocaine-induced subjective states (e.g., craving and euphoria) and compulsive drug-seeking behavior. More specifically, the applicability of wearable sensors to detect drug-related states is underexplored. In the current work, we take an initial step in the modeling of cocaine craving, euphoria and drug-seeking behavior using electrocardiographic (ECG) and respiratory signals unobtrusively collected from a wearable chest band. Ten experienced cocaine users were studied using a human laboratory paradigm of self-regulated (i.e., \"binge\") cocaine administration, during which self-reported visual analog scale (VAS) ratings of cocaine-induced subjective effects (i.e., craving and euphoria)  and behavioral measures of drug-seeking behavior (i.e., button clicks for drug infusions) are collected. Our results are encouraging and show that self-reported VAS Craving scores are predicted with a normalized root-mean-squared error (NRMSE) of 17.6% and a Pearson correlation coefficient of 0.49. Similarly, for VAS Euphoria prediction, an NRMSE of 16.7% and a Pearson correlation coefficient of 0.73 were achieved. We further analyze the relative importance of different morphology-related ECG and respiratory features for craving and euphoria prediction. A demographic factor analysis reveals how one single factor (i.e., average dollar ($) per cocaine use) can help to further boost the performance of our craving and euphoria models. Lastly, we model drug-seeking behavior using cardiac and respiratory signals. Specifically, we demonstrate that the latter signals can predict participant button clicks with an  F1 score of 0.80 and estimate different levels of click density with a correlation coefficient of 0.85 and an NRMSE of 17.9%.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14446
        },
        {
          "affiliations": [],
          "personId": 21318
        },
        {
          "affiliations": [],
          "personId": 17793
        },
        {
          "affiliations": [],
          "personId": 8597
        },
        {
          "affiliations": [],
          "personId": 17655
        },
        {
          "affiliations": [],
          "personId": 21097
        }
      ],
      "sessionIds": [
        2474
      ],
      "eventIds": []
    },
    {
      "id": 5071,
      "typeId": 11437,
      "title": "Exploring the Touch and Motion Features in Game-Based Cognitive Assessments",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Early detection of cognitive decline is important for timely intervention and treatment strategies to prevent further deterioration or development of more severe cognitive impairment, as well as identify at risk individuals for research. In this paper, we explored the feasibility of using data collected from built-in sensors of mobile phone and gameplay performance in mobile-game-based cognitive assessments. Twenty-two healthy participants took part in the two-session experiment where they were asked to take a series of standard cognitive assessments followed by playing three popular mobile games in which user-game interaction data were passively collected. The results from bivariate analysis reveal correlations between our proposed features and scores obtained from paper-based cognitive assessments. Our results show that touch gestural interaction and device motion patterns can be used as supplementary features on mobile game-based cognitive measurement. This study demonstrates that game related metrics on existing off-the-shelf games can be used as proxies for conventional cognitive measures, specifically for visuospatial function, visual search capability, mental flexibility, memory and attention.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14441
        },
        {
          "affiliations": [],
          "personId": 23879
        },
        {
          "affiliations": [],
          "personId": 18972
        },
        {
          "affiliations": [],
          "personId": 13287
        },
        {
          "affiliations": [],
          "personId": 24111
        }
      ],
      "sessionIds": [
        2133
      ],
      "eventIds": []
    },
    {
      "id": 4047,
      "typeId": 11437,
      "title": "micro-Stress EMA: A Passive Sensing Framework for Detecting in-the-wild Stress in Pregnant Mothers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "High levels of stress during pregnancy increase the chances of having a premature baby or a low-birthweight. Perceived self-reported stress does not often capture or align with this physiologic and behavioral response. But what if there was a self-report measure that could better capture the physiological response? Current perceived stress self-report assessments require users to answer multi-item scales at different time points of the day. Reducing it to one question, using microinteraction-based Ecological Momentary Assessment (micro-EMA, collecting a single in situ self-report to assess behaviors) allows us to identify smaller or more subtle changes in physiology. It also allows for more frequent responses to capture perceived stress while at the same time reducing burden on the participant. We propose a framework for selecting the optimal micro-EMA that combines unbiased feature selection and unsupervised Agglomerative Clustering. We test our framework on 22 young women performing 16 activities in-lab wearing a Biostamp, a NeuLog, and a Polar chest-strap. We validated our results on 18 pregnant women in real-world situations. Our framework shows that the question ``How stressed were you?'' results in over 100% improvement in inter-rater agreement, point biserial correlation, and percent absolute error compared to the widely adopted multi-item PSS-Q4 scale. Our results provide further in-depth exposure to the challenges of evaluating stress models in real-world situations.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14861
        },
        {
          "affiliations": [],
          "personId": 15564
        },
        {
          "affiliations": [],
          "personId": 8412
        },
        {
          "affiliations": [],
          "personId": 18999
        },
        {
          "affiliations": [],
          "personId": 21990
        },
        {
          "affiliations": [],
          "personId": 21305
        },
        {
          "affiliations": [],
          "personId": 14849
        },
        {
          "affiliations": [],
          "personId": 19706
        },
        {
          "affiliations": [],
          "personId": 23982
        },
        {
          "affiliations": [],
          "personId": 13032
        }
      ],
      "sessionIds": [
        2491
      ],
      "eventIds": []
    },
    {
      "id": 5075,
      "typeId": 11437,
      "title": "UbiTtention 2019: 4th International Workshop on Smart & Ambient Notification and Attention Management",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Users are increasingly being confronted with a tremendous amount of information proactively provided via notifications from versatile applications and services, through multiple devices and screens in their environment ubiquitously.\nHowever, human attention is limited. Further, the latest computing trends including versatile IoT devices, and contexts, such as smart cities and vehicles, are further competing for limited attention.\nTo counter this challenge, \"attention management\", including attention representation, sensing, prediction, analysis, and adaptive behavior is needed in our computing systems.\nFollowing the successful UbiTtention 2016, 2017 and 2018 workshops with up to 50 participants, the UbiTtention 2019 workshop brings together researchers and practitioners from academia and industry to explore the management of human attention and notifications across versatile devices and contexts to overcome information overload and overchoice.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11804
        },
        {
          "affiliations": [],
          "personId": 10623
        },
        {
          "affiliations": [],
          "personId": 13252
        },
        {
          "affiliations": [],
          "personId": 22173
        },
        {
          "affiliations": [],
          "personId": 21698
        },
        {
          "affiliations": [],
          "personId": 11709
        },
        {
          "affiliations": [],
          "personId": 22964
        },
        {
          "affiliations": [],
          "personId": 9337
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 3035,
      "typeId": 11437,
      "title": "Multi-target Affect Detection in the Wild: An Exploratory Study",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Affective computing aims to detect a person's affective state (e.g. emotion) based on observables. The link between affective states and biophysical data, collected in lab settings, has been established successfully. However, the number of realistic studies targeting affect detection in the wild is still limited. In this paper we present an exploratory field study, using physiological data of 11 healthy subjects. We aim to classify arousal,  State-Trait Anxiety Inventory (STAI), stress, and valence self-reports, utilizing feature-based and convolutional neural network (CNN) methods. In addition, we extend the CNNs to multi-task CNNs, classifying all labels of interest simultaneously. Comparing the $F_1$ score averaged over the different tasks and classifiers the CNNs reach an 1.8\\% higher score than the classical methods. However, the $F_1$ scores barely exceed 45\\%. In the light of these results, we discuss pitfalls and challenges for physiology-based affective computing in the wild.",
      "authors": [
        {
          "affiliations": [],
          "personId": 20621
        },
        {
          "affiliations": [],
          "personId": 8294
        },
        {
          "affiliations": [],
          "personId": 9963
        },
        {
          "affiliations": [],
          "personId": 18132
        },
        {
          "affiliations": [],
          "personId": 16080
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 7133,
      "typeId": 11437,
      "title": "Preferred Notification Modalities Depending on the Location and the Location-Based Activity",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Smartphones offer different modalities to inform about incoming notifications. They are perceived differently in terms of pleasantness and disruptiveness, depending on factors such as the receptivity and interruptibility of the user. Contextual factors such as the current location and activity as well as the task engagement level further influence this perception. Within a user study with 40 participants, we investigated suitable notification modalities for different place types. We found that a user's receptivity, the disruptiveness of a notification, and the task engagement correlate and that they differ per place type with statistical significance and small to large effect sizes. Due to their unobtrusive nature, silent mode and vibration are preferred notification modalities at all places - silent mode especially at \"do not disturb\" locations (\"library\", \"movie theater\"), at places where users tend to be in company (\"café\", \"restaurant\"), or at locations where users have to focus (\"university\", \"work\"). Ringtone is considered obtrusive and undesired and is only tolerated at a few places at which users tend to be alone (\"home\") or which are rather loud so that the auditory alert does not disturb others too much (\"gas station\"). We recommend to consider a user's location and location-based activities",
      "authors": [
        {
          "affiliations": [],
          "personId": 11804
        },
        {
          "affiliations": [],
          "personId": 21385
        },
        {
          "affiliations": [],
          "personId": 16399
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 7135,
      "typeId": 11437,
      "title": "SoberComm: Using Mobile Phones to Facilitate Inter-family Communication with Alcohol-dependent Patients",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "This paper reports on the development of a mobile support system (called SoberComm) to facilitate the sharing of alcohol-related data among patients and their family members, while providing the treatment team the data necessary to make constructive suggestions concerning inter-family communications. We began by conducting a pilot study to identify salient themes, which could be used to guide the overall design of the system. We then conducted an iterative process of prototyping and evaluating a user interface using medium-fidelity wireframes. The final high-fidelity design was used in the practical implementation of SoberComm in a two-week field study involving nine dyads of alcohol dependent patients and their family members. Results demonstrate that SoberComm had the potential of enhancing the ability of patients to refuse alcohol and deal with many of their underlying issues. Responses collected via qualitative interviews also demonstrated  qualitative evidences that the proposed system can enhance problem-solving skills and facilitating communication between alcohol-dependent patients and their family members.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14131
        },
        {
          "affiliations": [],
          "personId": 23639
        },
        {
          "affiliations": [],
          "personId": 13667
        },
        {
          "affiliations": [],
          "personId": 16817
        },
        {
          "affiliations": [],
          "personId": 22057
        },
        {
          "affiliations": [],
          "personId": 13697
        },
        {
          "affiliations": [],
          "personId": 19124
        },
        {
          "affiliations": [],
          "personId": 12606
        },
        {
          "affiliations": [],
          "personId": 14987
        },
        {
          "affiliations": [],
          "personId": 13609
        },
        {
          "affiliations": [],
          "personId": 16537
        }
      ],
      "sessionIds": [
        1195
      ],
      "eventIds": []
    },
    {
      "id": 6113,
      "typeId": 11437,
      "title": "Phyjama: Physiological Sensing via Fiber-enhanced Pyjamas",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Unobtrusive and continuous monitoring of cardiac and respiratory rhythm, especially during sleeping, can have significant clinical utility. An exciting new possibility for such monitoring is the design of textiles that use all-textile sensors that can be woven or stitched directly into a textile or garment. Our work explores how we can make such monitoring possible by leveraging something that is already familiar, such as pyjama made of cotton/silk fabric, and imperceptibly adapt it to enable sensing of physiological signals to yield natural fitting, comfortable, and less obtrusive smart clothing. \n\nWe face several challenges in enabling this vision including requiring new sensor design to measure physiological signals via everyday textiles and new methods to deal with the inherent looseness of normal garments, particularly sleepwear like pyjamas. We design two types of textile-based sensors that obtain a ballistic signal due to cardiac and respiratory rhythm—the first a novel resistive sensor that leverages pressure between the body and various surfaces and the second is a triboelectric sensor that leverages changes in separation between layers to measure ballistics induced by the heart. We then integrate several instances of such sensors on a pyjama and design a signal processing pipeline that fuses information from the different sensors such that we\ncan robustly measure physiological signals across a range of sleep and stationary postures. We show that the sensor and signal processing pipeline has high accuracy by benchmarking performance both under restricted settings with twenty one users as well as more naturalistic settings with seven users.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15348
        },
        {
          "affiliations": [],
          "personId": 10896
        },
        {
          "affiliations": [],
          "personId": 22548
        },
        {
          "affiliations": [],
          "personId": 21600
        },
        {
          "affiliations": [],
          "personId": 17655
        }
      ],
      "sessionIds": [
        1368
      ],
      "eventIds": []
    },
    {
      "id": 3044,
      "typeId": 11437,
      "title": "Beyond Respiration: Contactless Sleep Sound-Activity Recognition Using RF Signals",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Sleep plays an important role in an individual’s health. Sleep sound-activities including snore, cough and somniloquy are closely related to sleep quality, sleep disorder and even illnesses. Continuous sensing of these activities is useful in sleep quality monitoring and early detection of many diseases. To obtain the information of these activities, current solutions either require the user to wear various sensors or require to use the camera or microphone to record the image and sound data. However, many people are reluctant to wear sensors/devices during sleep. The video-based and audio-based approaches raise privacy concerns. In this work, we propose a novel system TagSleep to address the issues mentioned above. For the first time, we propose the concept of two-layer sensing. We employ the respiration sensing information as the basic first-layer information, which is applied to further obtain rich second-layer sensing information including snore, cough and somniloquy. Specifically, without attaching any device to the human body, by just deploying low-cost and flexible RFID tags near to the user, we can accurately obtain the respiration information with careful signal processing of the received wireless signal. What’s more interesting, the user’s cough, snore and somniloquy all affect his/her respiration, so the fine-grained respiration changes can be used to infer these sleep sound-activities without recording the sound data. We design and implement our system with just three RFID tags and one RFID reader. We evaluate the performance of TagSleep with 30 users (13 males and 17 females) for a period of 2 months in total. TagSleep is able to achieve higher than 96.58% sensing accuracy in recognizing snore, cough and somniloquy under various sleep postures. TagSleep also boosts the sleep posture recognition accuracy to 98.94%, outperforming the state-of-the-art solutions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 10307
        },
        {
          "affiliations": [],
          "personId": 12597
        },
        {
          "affiliations": [],
          "personId": 20091
        },
        {
          "affiliations": [],
          "personId": 22821
        },
        {
          "affiliations": [],
          "personId": 12337
        },
        {
          "affiliations": [],
          "personId": 15645
        }
      ],
      "sessionIds": [
        1270
      ],
      "eventIds": []
    },
    {
      "id": 8165,
      "typeId": 11437,
      "title": "I^3: Sensing Scrolling Human-Computer Interactions for Intelligent Interest Inference on Smartphones",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The scrolling interaction is a pervasive human-computer interaction on smartphones, which can reflect intrinsic characteristics during dynamic browsings. Different from extrinsic statistical measures like frequency of visits and dwell time, intrinsic features underlying scrolling interactions reveal fine-grained implicit feedbacks about user interests. Toward this end, we explore user interest inference by extracting efficient browsing features from scrolling human-computer interactions on smartphones. In this paper, we first analyze browsing traces of 40 volunteers, and find two intrinsic browsing features underlying scrolling interactions, i.e., browsing velocity stability and browsing velocity sequence, which are tightly related to user interests. Inspired by the observation, we propose an Intelligent Interest Inference system, I^3, which infers user interests through sensing scrolling interactions during browsings. Specifically, I^3 first extracts the two intrinsic browsing features from users' scrolling interactions. Then, I^3 applies a Naive Bayesian-based approach to construct an interest discriminator for coarse-grained user interest (i.e., like-preferred or dislike-preferred) inference. Furthermore, we develop a deep learning-based approach for I^3 to train rating classifiers for fine-grained rating inference in like-preferred and dislike-preferred browsings respectively. Finally, I^3 utilizes the interest discriminator and rating classifiers to infer exact user ratings about browsing contents on smartphones. Experimental results under browsing traces of 46 volunteers present that I^3 achieves 92.4% overall accuracy in interest inference.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14755
        },
        {
          "affiliations": [],
          "personId": 13428
        },
        {
          "affiliations": [],
          "personId": 17135
        },
        {
          "affiliations": [],
          "personId": 24300
        },
        {
          "affiliations": [],
          "personId": 13329
        },
        {
          "affiliations": [],
          "personId": 15342
        }
      ],
      "sessionIds": [
        2158
      ],
      "eventIds": []
    },
    {
      "id": 6121,
      "typeId": 11437,
      "title": "Driving with the Fishes: Towards Calming and Mindful Virtual Reality Experiences for the Car",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We present the use of in-car virtual reality (VR) as a way to create calm, mindful experiences for passengers and, someday, autonomous vehicle occupants. Specifically, we describe a series of studies aimed at exploring appropriate VR content, understanding the influence of car movement, and determining the length and other parameters of the simulation to avoid physical discomfort. Overall, our quantitative and qualitative insights suggest calm VR applications are well suited to an automotive context. Testing combinations of VR content designed to provide the participant with a static or dynamic experience versus stationary and moving vehicle modes, we find that a simulated experience of diving in the ocean while in a moving car elicited significantly lower levels of autonomic arousal as compared with a static VR plus stationary car condition. No significant motion sickness effects were subjectively reported by participants nor observable in the data, though a crossover interaction effect reveals how incongruence between the movement of the car and movement in VR could affect nausea. We conclude with recommendations for the design of calming and mindful VR experiences in moving vehicles.",
      "authors": [
        {
          "affiliations": [],
          "personId": 14023
        },
        {
          "affiliations": [],
          "personId": 23967
        },
        {
          "affiliations": [],
          "personId": 17013
        },
        {
          "affiliations": [],
          "personId": 16578
        },
        {
          "affiliations": [],
          "personId": 8809
        },
        {
          "affiliations": [],
          "personId": 14585
        },
        {
          "affiliations": [],
          "personId": 16309
        }
      ],
      "sessionIds": [
        1680
      ],
      "eventIds": []
    },
    {
      "id": 7148,
      "typeId": 11437,
      "title": "Automated Detection of Infant Holding Using Wearable Sensing: Implications for Developmental Science And Intervention",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Physical contact is critical for physical growth and parent-child bonding.\nPrevious studies of physical contact are limited to relatively short periods of direct observation and self-report methods. These methods limit researchers' understanding of the natural variation in physical contact across families and its broader impacts on child development. This study uses wearable motion sensors to provide objective, unobtrusive, and continuous measurements of physical contact in naturalistic home interactions. Our model reaches an accuracy of 0.870 (std: 0.059) for a second-by-second binary classification of holding. In addition, we detail five assessment scenarios for this and similar activity recognition models in social science research, where required accuracy may vary as a function of the use. Finally, we propose a grand vision for leveraging mobile sensors to access high-density markers of multiple determinants of early parent-child interactions, with implications for basic science and intervention.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23415
        },
        {
          "affiliations": [],
          "personId": 18818
        },
        {
          "affiliations": [],
          "personId": 22870
        },
        {
          "affiliations": [],
          "personId": 13374
        }
      ],
      "sessionIds": [
        1085
      ],
      "eventIds": []
    },
    {
      "id": 4076,
      "typeId": 11437,
      "title": "CueAuth: Comparing Touch, Mid-Air Gestures, and Gaze for Cue-based Authentication on Situated Displays",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Secure authentication on situated displays s (e.g., to access sensitive information or to make purchases) is becoming increasingly important. A promising approach are authentication schemes that employ cues that users respond to while authenticating; these schemes overwhelm observers by requiring them to observe the cue itself as well as users' response to the cue.  Although previous work proposed a variety of modalities, such as gaze and mid-air gestures, to further improve security, an understanding of how they compare with regard to usability and security is still missing as of today. In this paper, we compare modalities for cue-based authentication on situated displays. We provide the first comparison between touch, mid-air gestures, and calibration-free gaze using a state-of-the-art authentication concept. In two user studies (N=37) we found that the choice of touch or gaze presents a clear trade-off between usability and security. For example, while gaze input is more secure, it is also more demanding and requires longer authentication times. Mid-air gestures are slightly slower and more secure than touch but users hesitate using them in public. We conclude with design implications for authentication using touch, mid-air gestures, and gaze and discuss how the choice of modality creates opportunities and challenges for improved authentication in public.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19077
        },
        {
          "affiliations": [],
          "personId": 18330
        },
        {
          "affiliations": [],
          "personId": 20397
        },
        {
          "affiliations": [],
          "personId": 15667
        },
        {
          "affiliations": [],
          "personId": 22733
        },
        {
          "affiliations": [],
          "personId": 23018
        },
        {
          "affiliations": [],
          "personId": 13139
        }
      ],
      "sessionIds": [
        2129
      ],
      "eventIds": []
    },
    {
      "id": 6124,
      "typeId": 11437,
      "title": "coSense: Collaborative Urban-Scale Vehicle Sensing Based on Heterogeneous Fleets",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The real-time vehicle sensing at urban scale is essential to various urban services.  To date, most existing approaches rely on static infrastructures (e.g., traffic cameras) or mobile services (e.g., smartphone apps). However, these approaches are often inadequate for urban scale vehicle sensing at the individual level because of their static natures or low penetration rates. In this paper, we design a sensing system called coSense to utilize commercial vehicular fleets (e.g., taxis, buses, and trucks) for real-time vehicle sensing at urban scale, given (i) the availability of well-equipped commercial fleets sensing other vehicles by onboard cameras or peer-to-peer communication, and (ii) an increasing trend of connected vehicles and autonomous vehicles with periodical status broadcasts for safety applications. Compared to existing solutions based on cameras and smartphones, the key features of coSense are in its high penetration rates and transparent sensing for participating drivers. The key technical challenge we addressed is how to recover spatiotemporal sensing gaps by considering various mobility patterns of commercial vehicles with deep learning. We evaluate coSense with a preliminary road test and a large-scale trace-driven evaluation based on vehicular fleets in the Chinese city Shenzhen, including 14 thousand taxis, 13 thousand buses, 13 thousand trucks, and 10 thousand regular vehicles. We compare coSense to infrastructure and cellphone-based approaches, and the results show that we increase the sensing accuracy by 10.1% and 16.6% on average.",
      "authors": [
        {
          "affiliations": [],
          "personId": 13964
        },
        {
          "affiliations": [],
          "personId": 18113
        },
        {
          "affiliations": [],
          "personId": 16615
        },
        {
          "affiliations": [],
          "personId": 14507
        },
        {
          "affiliations": [],
          "personId": 21598
        },
        {
          "affiliations": [],
          "personId": 23171
        },
        {
          "affiliations": [],
          "personId": 11897
        }
      ],
      "sessionIds": [
        1102
      ],
      "eventIds": []
    },
    {
      "id": 7154,
      "typeId": 11437,
      "title": "Applying 1D Sensor DenseNet to Sussex-Huawei Locomotion-Transportation Recognition Challenge",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The Sussex-Huawei Transportation-Locomotion (SHL) Recognition Challenge organized at the HASCA Workshop of UbiComp 2019 presents a large and realistic dataset with different activities and transportation. The goal of this machine learning/data science challenge is to recognize eight modes of locomotion and transportation from the inertial sensor data of a smartphone in a mobile-phone placement independent manner. In this paper, our team (We can fly) summarize our submission to the competition. We proposed a 1D DenseNet model, a deep learning method for transportation classification.  We first convert sensor readings from phone coordinate system to navigation coordinate system. Then, we normalized each sensor using different maximums and minimums and construct multi-channel sensor input. Finally, 1D DenseNet model output the predictions. In the experiment, we utilized three internal datasets to train our model and achieved averaged F1 score 0.78 on four internal datasets.",
      "authors": [
        {
          "affiliations": [],
          "personId": 23199
        },
        {
          "affiliations": [],
          "personId": 19764
        },
        {
          "affiliations": [],
          "personId": 11540
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5107,
      "typeId": 11437,
      "title": "Demo: A Real-time and Robust Intrusion Detection System with Commodity Wi-Fi",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Intrusion detection plays a rather important role in many applications, like asset protection and elder caring.  Since we cannot make any requirements on the intruder, a device-free passive way of intrusion detection is much more promising and practical. In order to achieve robust passive intrusion detection, various techniques have been proposed, including video-based, infra-based and sensor-based approaches, among which dedicated device installation is often required. In this work, we present a real-time and robust device-free intrusion detection system, named RR-Alarm. By reusing the existing Wi-Fi signals, RR-Alarm is able to detect human intrusion in real time, by the same time, requiring no additional facilities installation.  By utilizing the Doppler effects incurred by human motion on multiple Wi-Fi devices, RR-Alarm is not only able to accurately detect the intrusion without any extra human efforts but also avoids a large number of false alarms caused by the human motion from outside the house. A long-term trial in a nursing home verifies the effectiveness of our Wi-Fi based RR-Alarm system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17717
        },
        {
          "affiliations": [],
          "personId": 18585
        },
        {
          "affiliations": [],
          "personId": 9934
        },
        {
          "affiliations": [],
          "personId": 10235
        },
        {
          "affiliations": [],
          "personId": 20239
        },
        {
          "affiliations": [],
          "personId": 17736
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 5110,
      "typeId": 11437,
      "title": "Blocks: Collaborative and Persistent Augmented Reality Experiences",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce Blocks, a mobile application that enables people to co-create AR structures that persist in the physical environment. Using Blocks, end users can collaborate synchronously or asynchronously, whether they are colocated or remote. Additionally, the AR structures can be tied to a physical location or can be accessed from anywhere. We evaluated how people used Blocks through a series of lab and field deployment studies with over 160 participants, and explored the interplay between two collaborative dimensions: space and time. We found that participants preferred creating structures synchronously with colocated collaborators. Additionally, they were most active when they created structures anywhere at any time. Unlike most of today's AR experiences, which focus on content consumption, this work outlines new design opportunities for persistent and collaborative AR experiences that empower anyone to collaborate and create AR content.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18115
        },
        {
          "affiliations": [],
          "personId": 23948
        },
        {
          "affiliations": [],
          "personId": 23435
        },
        {
          "affiliations": [],
          "personId": 13301
        },
        {
          "affiliations": [],
          "personId": 11691
        }
      ],
      "sessionIds": [
        2158
      ],
      "eventIds": []
    },
    {
      "id": 3063,
      "typeId": 11437,
      "title": "JacquardToolkit Demo: Enabling and exploring interactions with the Levi's Jacquard Jacket",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "New user interfaces afford innovations in the user experience of computers– the interactive textile approach posed by Jacquard is another step in this evolution[1,5]. Although it demonstrates four gestures via the Levi’s Jacquard Jacket [4], we experiment with potential applications to explore new forms of user inputs. We have built a toolkit that enables developers full access to the Jacquard technology and custom gesture creation [2,3]. We subsequently used this to develop a custom Force Touch gesture, which is now integrated into the aforementioned iOS toolkit such that developers may use it to implement original and new jacket functionality in to their own applications. The playground application helps developers understand how to effectively utilize the library. In parallel, we evaluated the Jacquard hardware by conducting a detailed teardown of the Jacket cuff and tag. We mapped how such sensing technology can be removed from the jacket and applied to other textiles with conductive threads. Moreover, our findings motivate future testing on other garments and accessories as we isolated theJacquard swatch from the cuff. In order to evaluate the intuitiveness of the jacket gestures and test learnability of the new custom gestures, we designed a user assessment application with the toolkit to conduct a user study with 25 subjects. Results showed that bothBrush In and Brush Out had the easiest learning curve and were perceived as most intuitive. Brush In (48%) stood out as the most preferred gesture followed by Double Tap (20%). Although the cus-tom gesture, Force Touch, had the lowest perceived intuitiveness, it serves as the first key step towards a future of rapidly prototyped and consumer-ready custom gestures.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16206
        },
        {
          "affiliations": [],
          "personId": 8240
        },
        {
          "affiliations": [],
          "personId": 11136
        },
        {
          "affiliations": [],
          "personId": 18371
        },
        {
          "affiliations": [],
          "personId": 12224
        },
        {
          "affiliations": [],
          "personId": 13758
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 4087,
      "typeId": 11437,
      "title": "A Smartphone-Based Behavioural Activation Application Using Recommender System",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "The efficacy of behavioural activation in the treatment of major depressive disorders has been established in a number of studies over the last four decades. Although a number of recent studies show that behavioural activation administered via a smartphone application has the potential to be effective in the treatment of depression, these opportunities are tempered by the problem that these interventions have high dropout rates. However, recent research finds that personalisation of content can positively influence engagement. We present MindTick, a smartphone-based behavioural activation application using a recommender system to deliver personalized content to encourage users to engage in behavioural activation activities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15241
        },
        {
          "affiliations": [],
          "personId": 18589
        },
        {
          "affiliations": [],
          "personId": 18626
        },
        {
          "affiliations": [],
          "personId": 19013
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6135,
      "typeId": 11437,
      "title": "Detecting Conversing Groups Using Social Dynamics from Wearable Acceleration: Group Size Awareness",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose a method for detecting conversing groups. More specifically, we detect pairwise F-formation membership using a single worn accelerometer. We focus on crowded real life scenarios, specifically mingling events, where groups of different sizes naturally occur and evolve over time. Our method uses the dynamics of interaction, derived from people‚Äôs coordinated social actions and movements. The social actions, speaking, head and hand gesturing, are inferred from wearable acceleration with a transfer learning approach. These automatically labeled actions, together with the raw acceleration, are used to define joint representations of interaction between people through the extraction of pairwise features. We present a new feature set based on the overlap patterns of social actions and utilise some others that were previously proposed in other domains. Our approach considers various interaction patterns of different sized groups by training multiple classifiers with respect to cardinality. The final estimation is then dynamically performed by meta-classifier learning using the local neighbourhood of the current test sample. We experimentally show that the proposed method outperforms state of the art approaches. Finally, we show how the accuracy of the social action detection affects group detection performance, analyze the effectiveness of features for different group sizes in detail, discuss how different types of features contribute to the final performance and evaluate the effects of using the local neighbourhood for meta-classifier learning.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19221
        },
        {
          "affiliations": [],
          "personId": 16561
        }
      ],
      "sessionIds": [
        2416
      ],
      "eventIds": []
    },
    {
      "id": 7160,
      "typeId": 11437,
      "title": "One Leg At A Time: Towards Optimised Design Engineering of Textile Sensors in Trousers",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Custom made textile sensors encounter design and manufacturing challenges that differ from conventional printed circuit board-based sensors. The field of e-textiles commonly deploys such sensors on the human body, meaning that overcoming these challenges are crucial for reliable sensor performance.\n   In this paper, we present and evaluate the design of trousers with embedded fabric sensors. Two iterative prototypes were manufactured and tested in two user studies, focusing on mechanical aspects of the design for applications in capturing body movement within social interaction. We report on failures and risks of the design, and furthermore propose solutions for a more robust, yet soft wearable sensing system.",
      "authors": [
        {
          "affiliations": [],
          "personId": 19620
        },
        {
          "affiliations": [],
          "personId": 21452
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 8185,
      "typeId": 11437,
      "title": "Skeletonator: Semantic Human Activity Annotation Tool Using Skeletonized Surveillance Videos",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Human activity data sets are fundamental for intelligent activity recognition in context-aware computing and intelligent video analysis. Surveillance videos include rich human activity data that are more realistic compared to data collected from a controlled environment. However, there are several challenges in annotating large data sets: 1) inappropriateness for crowd-sourcing because of public privacy, and 2) tediousness to manually select activities of people from busy scenes. \n\nWe present Skeletonotator, a web-based annotation tool that creates human activity data sets using anonymous skeletonized poses. The tool generates 2D skeletons from surveillance videos using computer vision techniques, and visualizes and plays back the skeletonized poses. Skeletons are tracked between frames, and a unique id is automatically assigned to each skeleton. For the annotation process, users can add annotations by selecting the target skeleton and applying activity labels to a particular time period, while only watching skeletonized poses. The tool outputs human activity data sets which include the type of activity, relevant skeletons, and timestamps. We plan to open source Skeletonotator together with our data sets for future researchers.",
      "authors": [
        {
          "affiliations": [],
          "personId": 16243
        },
        {
          "affiliations": [],
          "personId": 15651
        },
        {
          "affiliations": [],
          "personId": 11787
        },
        {
          "affiliations": [],
          "personId": 15080
        },
        {
          "affiliations": [],
          "personId": 10751
        }
      ],
      "sessionIds": [],
      "eventIds": []
    },
    {
      "id": 6137,
      "typeId": 11437,
      "title": "ScratchThat: Supporting Command-Agnostic Speech Repair in Voice-Driven Assistants",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "Speech interfaces have become an increasingly popular input method for smartphone-based virtual assistants, smart speakers, and Internet of Things (IoT) devices. While they facilitate rapid and natural interaction in the form of voice commands, current speech interfaces lack natural methods for command correction. We present ScratchThat, a method for supporting command-agnostic speech repair in voice-driven assistants, suitable for enabling corrective functionality within third-party commands. Unlike existing speech repair methods, ScratchThat is able to automatically infer query parameters and intelligently select entities in a correction clause for editing. We conducted three evaluations to (1) elicit natural forms of speech repair in voice commands, (2) compare the interaction speed and NASA TLX score of the system to existing voice-based correction method, and (3) assess the accuracy of the ScratchThat algorithm. Our results show that (1) speech repair for voice commands differ from previous models for conversational speech repair, (2) methods for command correction based on speech repair are significantly faster than other voice-based methods, and (3) the ScratchThat algorithm facilitates accurate command repair as rated by humans (77% accuracy) and machines (0.94 BLEU score). Finally, we present several ScratchThat use cases, which collectively demonstrate its utility across many applications.",
      "authors": [
        {
          "affiliations": [],
          "personId": 15205
        },
        {
          "affiliations": [],
          "personId": 11785
        },
        {
          "affiliations": [],
          "personId": 23035
        },
        {
          "affiliations": [],
          "personId": 22412
        },
        {
          "affiliations": [],
          "personId": 9853
        }
      ],
      "sessionIds": [
        1129
      ],
      "eventIds": []
    },
    {
      "id": 5114,
      "typeId": 11437,
      "title": "Optimizing Activity Data Collection with Gamification Points Using Uncertainty Based Active Learning",
      "trackId": 10577,
      "tags": [],
      "keywords": [],
      "abstract": "A big challenge for activity data collection is unavoidable to rely on users and to keep them motivated to provide labels. In this paper, we propose the idea of exploiting gamification points to motivate the users for activity data collection by using an uncertainty based active learning approach to evaluate those points. The novel idea behind this is that we approximate the score of the unlabeled examples according to the current model's uncertainty in its prediction of the corresponding activity labels, and using that score as gamification points. Thus, the users are motivated by getting gamification points as feedback based on their data annotation quality. 1,236 activity labels with smartphone sensors that we collected help to validate our proposed method. By evaluating with the dataset, the results show our proposed method has improvements in data quality, data quantity, and user engagement that reflect the improvement in activity data collection.",
      "authors": [
        {
          "affiliations": [],
          "personId": 11459
        },
        {
          "affiliations": [],
          "personId": 8218
        },
        {
          "affiliations": [],
          "personId": 24129
        }
      ],
      "sessionIds": [],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 12289,
      "firstName": "Min",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 20483,
      "firstName": "Masahiro",
      "lastName": "Okamoto",
      "affiliations": []
    },
    {
      "id": 8197,
      "firstName": "Bo",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 20494,
      "firstName": "Ryan",
      "lastName": "Rossi",
      "affiliations": []
    },
    {
      "id": 16399,
      "firstName": "Michael",
      "lastName": "Beigl",
      "affiliations": []
    },
    {
      "id": 8207,
      "firstName": "Simone",
      "lastName": "Centellegher",
      "affiliations": []
    },
    {
      "id": 20502,
      "firstName": "Tyler",
      "lastName": "Giallanza",
      "affiliations": []
    },
    {
      "id": 16407,
      "firstName": "Suman",
      "lastName": "Banerjee",
      "affiliations": []
    },
    {
      "id": 20504,
      "firstName": "Xinyu",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 8217,
      "firstName": "Hannah",
      "lastName": "Bayer",
      "affiliations": []
    },
    {
      "id": 8218,
      "firstName": "Tittaya",
      "lastName": "Mairittha",
      "affiliations": []
    },
    {
      "id": 20510,
      "firstName": "Aaqib",
      "lastName": "Saeed",
      "affiliations": []
    },
    {
      "id": 8225,
      "firstName": "Adam",
      "lastName": "Tyler",
      "affiliations": []
    },
    {
      "id": 16417,
      "firstName": "Kui",
      "lastName": "Ren",
      "affiliations": []
    },
    {
      "id": 20516,
      "firstName": "Xiangxiang",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 12325,
      "firstName": "A. Aldo",
      "lastName": "Faisal",
      "affiliations": []
    },
    {
      "id": 16424,
      "firstName": "Pengyu",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 12331,
      "firstName": "Gloria",
      "lastName": "Mark",
      "affiliations": []
    },
    {
      "id": 8240,
      "firstName": "Caleb",
      "lastName": "Rudnicki",
      "affiliations": []
    },
    {
      "id": 12337,
      "firstName": "Xiaojiang",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 16434,
      "firstName": "Hiroyuki",
      "lastName": "Kitagawa",
      "affiliations": []
    },
    {
      "id": 20531,
      "firstName": "Yue",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 12343,
      "firstName": "Philip",
      "lastName": "Birch",
      "affiliations": []
    },
    {
      "id": 16440,
      "firstName": "Ying-Yu",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 16441,
      "firstName": "Katie",
      "lastName": "Quehl",
      "affiliations": []
    },
    {
      "id": 16443,
      "firstName": "Shuai",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20541,
      "firstName": "Yusuke",
      "lastName": "Fukazawa",
      "affiliations": []
    },
    {
      "id": 12350,
      "firstName": "Genjian",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 12352,
      "firstName": "Zofia",
      "lastName": "Saternus",
      "affiliations": []
    },
    {
      "id": 12353,
      "firstName": "Naoya",
      "lastName": "Isoyama",
      "affiliations": []
    },
    {
      "id": 8268,
      "firstName": "Siqi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16461,
      "firstName": "Norbert",
      "lastName": "BrÌ_ndle",
      "affiliations": []
    },
    {
      "id": 8271,
      "firstName": "Viswanathan",
      "lastName": "Swaminathan",
      "affiliations": []
    },
    {
      "id": 12367,
      "firstName": "Fangtian",
      "lastName": "Ying",
      "affiliations": []
    },
    {
      "id": 8279,
      "firstName": "Nils",
      "lastName": "Einecke",
      "affiliations": []
    },
    {
      "id": 8284,
      "firstName": "Kazunobu",
      "lastName": "Sumiya",
      "affiliations": []
    },
    {
      "id": 12384,
      "firstName": "Paulo",
      "lastName": "Lopez-Meyer",
      "affiliations": []
    },
    {
      "id": 12385,
      "firstName": "Subigya",
      "lastName": "Nepal",
      "affiliations": []
    },
    {
      "id": 8294,
      "firstName": "Robert",
      "lastName": "Dürichen",
      "affiliations": []
    },
    {
      "id": 8303,
      "firstName": "Michalis",
      "lastName": "Xenos",
      "affiliations": []
    },
    {
      "id": 12400,
      "firstName": "Gang",
      "lastName": "Pan",
      "affiliations": []
    },
    {
      "id": 16501,
      "firstName": "Judith",
      "lastName": "Meek",
      "affiliations": []
    },
    {
      "id": 12406,
      "firstName": "Angella",
      "lastName": "Mackey",
      "affiliations": []
    },
    {
      "id": 16503,
      "firstName": "Kenzy",
      "lastName": "Mina",
      "affiliations": []
    },
    {
      "id": 12409,
      "firstName": "Ella-Noora",
      "lastName": "Polvi",
      "affiliations": []
    },
    {
      "id": 12412,
      "firstName": "Masaki",
      "lastName": "Shuzo",
      "affiliations": []
    },
    {
      "id": 20606,
      "firstName": "Zhen",
      "lastName": "Tu",
      "affiliations": []
    },
    {
      "id": 8321,
      "firstName": "Lu-Hua",
      "lastName": "Shih",
      "affiliations": []
    },
    {
      "id": 16513,
      "firstName": "Mattia",
      "lastName": "Zeni",
      "affiliations": []
    },
    {
      "id": 12425,
      "firstName": "Fengshi",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 20621,
      "firstName": "Philip",
      "lastName": "Schmidt",
      "affiliations": []
    },
    {
      "id": 12432,
      "firstName": "Josiah",
      "lastName": "Hester",
      "affiliations": []
    },
    {
      "id": 16528,
      "firstName": "Richard",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 8338,
      "firstName": "Tatiana",
      "lastName": "Son",
      "affiliations": []
    },
    {
      "id": 12435,
      "firstName": "Gernot",
      "lastName": "StÌ_bl",
      "affiliations": []
    },
    {
      "id": 20627,
      "firstName": "Mohammad Saifur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 8344,
      "firstName": "Edwin",
      "lastName": "Dertien",
      "affiliations": []
    },
    {
      "id": 16537,
      "firstName": "Hui-Ching",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 20636,
      "firstName": "Yifang",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 20638,
      "firstName": "Alexis",
      "lastName": "Hiniker",
      "affiliations": []
    },
    {
      "id": 16550,
      "firstName": "John Paul",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 20648,
      "firstName": "Jianyu",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 16556,
      "firstName": "Sujata",
      "lastName": "Punait",
      "affiliations": []
    },
    {
      "id": 16561,
      "firstName": "Haley",
      "lastName": "Hung",
      "affiliations": []
    },
    {
      "id": 20660,
      "firstName": "Franziska",
      "lastName": "GÌ_nther",
      "affiliations": []
    },
    {
      "id": 12471,
      "firstName": "Soujanya",
      "lastName": "Chatterjee",
      "affiliations": []
    },
    {
      "id": 20667,
      "firstName": "Rute C.",
      "lastName": "Sofia",
      "affiliations": []
    },
    {
      "id": 16574,
      "firstName": "Ning",
      "lastName": "Ye",
      "affiliations": []
    },
    {
      "id": 20671,
      "firstName": "Friedemann",
      "lastName": "Mattern",
      "affiliations": []
    },
    {
      "id": 8385,
      "firstName": "Jinsong",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 16578,
      "firstName": "Elizabeth L.",
      "lastName": "Murnane",
      "affiliations": []
    },
    {
      "id": 12482,
      "firstName": "Alejandro Sanchez",
      "lastName": "Guinea",
      "affiliations": []
    },
    {
      "id": 8391,
      "firstName": "Cedric",
      "lastName": "Honnet",
      "affiliations": []
    },
    {
      "id": 16585,
      "firstName": "Dr. Angelo",
      "lastName": "Fraietta",
      "affiliations": []
    },
    {
      "id": 8400,
      "firstName": "Sunghee",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 8401,
      "firstName": "Susu",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 16595,
      "firstName": "Jun",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 12501,
      "firstName": "Hiroshi",
      "lastName": "Hoshino",
      "affiliations": []
    },
    {
      "id": 8408,
      "firstName": "Mathias",
      "lastName": "Kraus",
      "affiliations": []
    },
    {
      "id": 20697,
      "firstName": "Jasmine",
      "lastName": "Zia",
      "affiliations": []
    },
    {
      "id": 12508,
      "firstName": "Eunyee",
      "lastName": "Koh",
      "affiliations": []
    },
    {
      "id": 16604,
      "firstName": "Patrizia",
      "lastName": "Paci",
      "affiliations": []
    },
    {
      "id": 8412,
      "firstName": "Begum",
      "lastName": "Egilmez",
      "affiliations": []
    },
    {
      "id": 12510,
      "firstName": "Jorge",
      "lastName": "Gonçalves",
      "affiliations": []
    },
    {
      "id": 8415,
      "firstName": "Yoshito",
      "lastName": "Tobe",
      "affiliations": []
    },
    {
      "id": 16611,
      "firstName": "Kang G.",
      "lastName": "Shin",
      "affiliations": []
    },
    {
      "id": 8420,
      "firstName": "Michael",
      "lastName": "MÌ_hlhaus",
      "affiliations": []
    },
    {
      "id": 8421,
      "firstName": "Octavia",
      "lastName": "Zahrt",
      "affiliations": []
    },
    {
      "id": 16615,
      "firstName": "Zhihan",
      "lastName": "Fang",
      "affiliations": []
    },
    {
      "id": 12522,
      "firstName": "Zhiwen",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 20714,
      "firstName": "Longbiao",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 12526,
      "firstName": "Sebastien",
      "lastName": "Richoz",
      "affiliations": []
    },
    {
      "id": 8432,
      "firstName": "Elaheh",
      "lastName": "Homayounvala",
      "affiliations": []
    },
    {
      "id": 20721,
      "firstName": "Kai",
      "lastName": "Niu",
      "affiliations": []
    },
    {
      "id": 8439,
      "firstName": "Jonna",
      "lastName": "HÌ_kkilÌ_",
      "affiliations": []
    },
    {
      "id": 20727,
      "firstName": "Masamichi",
      "lastName": "Shimosaka",
      "affiliations": []
    },
    {
      "id": 16632,
      "firstName": "Jing",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 8440,
      "firstName": "Haiming",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 20730,
      "firstName": "Ching Hua",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 8444,
      "firstName": "Zheying",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 8446,
      "firstName": "Scott",
      "lastName": "Bateman",
      "affiliations": []
    },
    {
      "id": 16639,
      "firstName": "Zulqarnain",
      "lastName": "Rashid",
      "affiliations": []
    },
    {
      "id": 12550,
      "firstName": "Soichiro",
      "lastName": "Matsushita",
      "affiliations": []
    },
    {
      "id": 8456,
      "firstName": "Marike",
      "lastName": "Hettinga",
      "affiliations": []
    },
    {
      "id": 12552,
      "firstName": "Archan",
      "lastName": "Misra",
      "affiliations": []
    },
    {
      "id": 12557,
      "firstName": "Anan Ahmad",
      "lastName": "Khan",
      "affiliations": []
    },
    {
      "id": 16655,
      "firstName": "Sanjay K",
      "lastName": "Sahay",
      "affiliations": []
    },
    {
      "id": 8464,
      "firstName": "Fanglin",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20753,
      "firstName": "Hyunsung",
      "lastName": "Cho",
      "affiliations": []
    },
    {
      "id": 12564,
      "firstName": "Zhu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16664,
      "firstName": "Yun",
      "lastName": "Cheng",
      "affiliations": []
    },
    {
      "id": 8474,
      "firstName": "Justice",
      "lastName": "Amoh",
      "affiliations": []
    },
    {
      "id": 20765,
      "firstName": "Cong",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 12574,
      "firstName": "Liyao",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 12575,
      "firstName": "J. David",
      "lastName": "Creswell",
      "affiliations": []
    },
    {
      "id": 8481,
      "firstName": "Yui",
      "lastName": "Yamashita",
      "affiliations": []
    },
    {
      "id": 16679,
      "firstName": "Toshiyuki",
      "lastName": "Miyachi",
      "affiliations": []
    },
    {
      "id": 8488,
      "firstName": "Fengli",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 12585,
      "firstName": "Tong",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 8491,
      "firstName": "Seyed Ali",
      "lastName": "Ghorashi",
      "affiliations": []
    },
    {
      "id": 12592,
      "firstName": "Christine",
      "lastName": "Michel",
      "affiliations": []
    },
    {
      "id": 16689,
      "firstName": "Masaru",
      "lastName": "Ohkubo",
      "affiliations": []
    },
    {
      "id": 8497,
      "firstName": "J. Walter",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 20789,
      "firstName": "Koji",
      "lastName": "Tsukada",
      "affiliations": []
    },
    {
      "id": 12597,
      "firstName": "Jie",
      "lastName": "Xiong",
      "affiliations": []
    },
    {
      "id": 8504,
      "firstName": "Tamir",
      "lastName": "Mendel",
      "affiliations": []
    },
    {
      "id": 12601,
      "firstName": "Daiki",
      "lastName": "Kajiwara",
      "affiliations": []
    },
    {
      "id": 12602,
      "firstName": "Shijia",
      "lastName": "Pan",
      "affiliations": []
    },
    {
      "id": 12606,
      "firstName": "Shan Jean",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 8515,
      "firstName": "Juha",
      "lastName": "RÌ¦ning",
      "affiliations": []
    },
    {
      "id": 16707,
      "firstName": "Vikas",
      "lastName": "Chandan",
      "affiliations": []
    },
    {
      "id": 12614,
      "firstName": "Stefan",
      "lastName": "Feuerriegel",
      "affiliations": []
    },
    {
      "id": 8519,
      "firstName": "Sebastian",
      "lastName": "Fudickar",
      "affiliations": []
    },
    {
      "id": 16714,
      "firstName": "Jun",
      "lastName": "Ota",
      "affiliations": []
    },
    {
      "id": 16718,
      "firstName": "Christabel",
      "lastName": "Enweronu-Laryea",
      "affiliations": []
    },
    {
      "id": 8527,
      "firstName": "Moyra",
      "lastName": "Chikomo",
      "affiliations": []
    },
    {
      "id": 16720,
      "firstName": "Elina",
      "lastName": "Kuosmanen",
      "affiliations": []
    },
    {
      "id": 8529,
      "firstName": "Kazuki",
      "lastName": "Yoshida",
      "affiliations": []
    },
    {
      "id": 12627,
      "firstName": "Kunio",
      "lastName": "Akashi",
      "affiliations": []
    },
    {
      "id": 8532,
      "firstName": "Hae Young",
      "lastName": "Noh",
      "affiliations": []
    },
    {
      "id": 16726,
      "firstName": "Raghu",
      "lastName": "Mulukutla",
      "affiliations": []
    },
    {
      "id": 16727,
      "firstName": "Yuyu",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 8537,
      "firstName": "Afroditi",
      "lastName": "Psarra",
      "affiliations": []
    },
    {
      "id": 20826,
      "firstName": "Zhuolin",
      "lastName": "Cheng",
      "affiliations": []
    },
    {
      "id": 12640,
      "firstName": "Yuichi",
      "lastName": "Miyaji",
      "affiliations": []
    },
    {
      "id": 16741,
      "firstName": "Chongguang",
      "lastName": "Bi",
      "affiliations": []
    },
    {
      "id": 8550,
      "firstName": "David",
      "lastName": "Weston",
      "affiliations": []
    },
    {
      "id": 8552,
      "firstName": "Shivakant",
      "lastName": "Mishra",
      "affiliations": []
    },
    {
      "id": 20849,
      "firstName": "Miguel",
      "lastName": "Flores",
      "affiliations": []
    },
    {
      "id": 8565,
      "firstName": "Ana de",
      "lastName": "Almeida",
      "affiliations": []
    },
    {
      "id": 16758,
      "firstName": "Chunming",
      "lastName": "Qiao",
      "affiliations": []
    },
    {
      "id": 8566,
      "firstName": "Raghav H.",
      "lastName": "Venkatnarayan",
      "affiliations": []
    },
    {
      "id": 16766,
      "firstName": "Alanson P.",
      "lastName": "Sample",
      "affiliations": []
    },
    {
      "id": 16770,
      "firstName": "Nico",
      "lastName": "Castelli",
      "affiliations": []
    },
    {
      "id": 20868,
      "firstName": "Khai",
      "lastName": "Truong",
      "affiliations": []
    },
    {
      "id": 12680,
      "firstName": "Adrien",
      "lastName": "Carteron",
      "affiliations": []
    },
    {
      "id": 12683,
      "firstName": "Masud",
      "lastName": "Ahmed",
      "affiliations": []
    },
    {
      "id": 8588,
      "firstName": "Carissa",
      "lastName": "Low",
      "affiliations": []
    },
    {
      "id": 16782,
      "firstName": "Tianrui",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 8591,
      "firstName": "Sungchul",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 20879,
      "firstName": "Fuminori",
      "lastName": "Okuya",
      "affiliations": []
    },
    {
      "id": 12691,
      "firstName": "Nuria",
      "lastName": "Oliver",
      "affiliations": []
    },
    {
      "id": 12693,
      "firstName": "Kayla",
      "lastName": "Guzman",
      "affiliations": []
    },
    {
      "id": 8597,
      "firstName": "Robert T.",
      "lastName": "Malison",
      "affiliations": []
    },
    {
      "id": 16793,
      "firstName": "Blaine A",
      "lastName": "Price",
      "affiliations": []
    },
    {
      "id": 12699,
      "firstName": "Shahin Alan",
      "lastName": "Samiei",
      "affiliations": []
    },
    {
      "id": 8605,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "affiliations": []
    },
    {
      "id": 20895,
      "firstName": "Yimeng",
      "lastName": "Wei",
      "affiliations": []
    },
    {
      "id": 16800,
      "firstName": "Walther",
      "lastName": "Jensen",
      "affiliations": []
    },
    {
      "id": 12709,
      "firstName": "Liam D.",
      "lastName": "Turner",
      "affiliations": []
    },
    {
      "id": 8614,
      "firstName": "Panuwat",
      "lastName": "Phunsuk",
      "affiliations": []
    },
    {
      "id": 16806,
      "firstName": "Zeyu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 12714,
      "firstName": "Gokhan",
      "lastName": "Memik",
      "affiliations": []
    },
    {
      "id": 16817,
      "firstName": "Jui-Ting",
      "lastName": "Tsai",
      "affiliations": []
    },
    {
      "id": 12722,
      "firstName": "Haojian",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 20915,
      "firstName": "Mike",
      "lastName": "Gartrell",
      "affiliations": []
    },
    {
      "id": 8627,
      "firstName": "Yang",
      "lastName": "Wan",
      "affiliations": []
    },
    {
      "id": 8629,
      "firstName": "Shing Chow",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 20919,
      "firstName": "Xinbing",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16824,
      "firstName": "Giacinto",
      "lastName": "Barresi",
      "affiliations": []
    },
    {
      "id": 20921,
      "firstName": "Andrzej",
      "lastName": "Romanowski",
      "affiliations": []
    },
    {
      "id": 12736,
      "firstName": "Ruiyi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 20929,
      "firstName": "Per",
      "lastName": "BÌ_kgaard",
      "affiliations": []
    },
    {
      "id": 16834,
      "firstName": "Mitchell A.",
      "lastName": "Thornton",
      "affiliations": []
    },
    {
      "id": 8647,
      "firstName": "Sabrina",
      "lastName": "Schuck",
      "affiliations": []
    },
    {
      "id": 8648,
      "firstName": "Enej",
      "lastName": "Mlinaric",
      "affiliations": []
    },
    {
      "id": 16842,
      "firstName": "Ryo",
      "lastName": "Imai",
      "affiliations": []
    },
    {
      "id": 8651,
      "firstName": "Mooichoo",
      "lastName": "Chuah",
      "affiliations": []
    },
    {
      "id": 8655,
      "firstName": "Yukang",
      "lastName": "Yan",
      "affiliations": []
    },
    {
      "id": 20953,
      "firstName": "Kevan",
      "lastName": "Dodhia",
      "affiliations": []
    },
    {
      "id": 12763,
      "firstName": "Sung-Ju",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 8670,
      "firstName": "Katayoun",
      "lastName": "Farrahi",
      "affiliations": []
    },
    {
      "id": 16863,
      "firstName": "Benjamin",
      "lastName": "Tag",
      "affiliations": []
    },
    {
      "id": 16870,
      "firstName": "Swapnil Sayan",
      "lastName": "Saha",
      "affiliations": []
    },
    {
      "id": 12775,
      "firstName": "Shuo",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 16871,
      "firstName": "Himanshu",
      "lastName": "Verma",
      "affiliations": []
    },
    {
      "id": 8679,
      "firstName": "Yuexiang",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 8680,
      "firstName": "Ella",
      "lastName": "Peltonen",
      "affiliations": []
    },
    {
      "id": 8681,
      "firstName": "Qiang",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 20971,
      "firstName": "Yang",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 12781,
      "firstName": "Ryo",
      "lastName": "Yanagida",
      "affiliations": []
    },
    {
      "id": 16878,
      "firstName": "René",
      "lastName": "Mayrhofer",
      "affiliations": []
    },
    {
      "id": 16879,
      "firstName": "François",
      "lastName": "Guimbretière",
      "affiliations": []
    },
    {
      "id": 16880,
      "firstName": "Daichi",
      "lastName": "Amagata",
      "affiliations": []
    },
    {
      "id": 12785,
      "firstName": "Kennedy Opoku",
      "lastName": "Asare",
      "affiliations": []
    },
    {
      "id": 8689,
      "firstName": "Pratheeban",
      "lastName": "Elancheliyan",
      "affiliations": []
    },
    {
      "id": 16881,
      "firstName": "Akshay Uttama Nambi",
      "lastName": "S.N.",
      "affiliations": []
    },
    {
      "id": 20978,
      "firstName": "Ming",
      "lastName": "Zeng",
      "affiliations": []
    },
    {
      "id": 12792,
      "firstName": "Stephan G.",
      "lastName": "Lukosch",
      "affiliations": []
    },
    {
      "id": 20986,
      "firstName": "Yan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 20989,
      "firstName": "Jakob",
      "lastName": "Bardram",
      "affiliations": []
    },
    {
      "id": 12800,
      "firstName": "Matthew J.",
      "lastName": "Chabalko",
      "affiliations": []
    },
    {
      "id": 16901,
      "firstName": "Van-Nam",
      "lastName": "Huynh",
      "affiliations": []
    },
    {
      "id": 16902,
      "firstName": "Emirhan",
      "lastName": "Poyraz",
      "affiliations": []
    },
    {
      "id": 12809,
      "firstName": "Ruihui",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 16906,
      "firstName": "Ella",
      "lastName": "Dagan",
      "affiliations": []
    },
    {
      "id": 16907,
      "firstName": "Nobuo",
      "lastName": "Kawaguchi",
      "affiliations": []
    },
    {
      "id": 8716,
      "firstName": "Thomas",
      "lastName": "ZÌ_ger",
      "affiliations": []
    },
    {
      "id": 8720,
      "firstName": "Carlos",
      "lastName": "Granell",
      "affiliations": []
    },
    {
      "id": 21008,
      "firstName": "Su",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 21011,
      "firstName": "Qui",
      "lastName": "Niu",
      "affiliations": []
    },
    {
      "id": 8724,
      "firstName": "Zhaoxin",
      "lastName": "Chang",
      "affiliations": []
    },
    {
      "id": 21013,
      "firstName": "Henglin",
      "lastName": "Pu",
      "affiliations": []
    },
    {
      "id": 8726,
      "firstName": "Satoshi",
      "lastName": "Nakamura",
      "affiliations": []
    },
    {
      "id": 21016,
      "firstName": "Dong",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 21018,
      "firstName": "Nazir",
      "lastName": "Saleheen",
      "affiliations": []
    },
    {
      "id": 16924,
      "firstName": "Dave",
      "lastName": "Randall",
      "affiliations": []
    },
    {
      "id": 21022,
      "firstName": "Yiqin",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 16928,
      "firstName": "Utku Gunay",
      "lastName": "Acer",
      "affiliations": []
    },
    {
      "id": 21030,
      "firstName": "Tahera",
      "lastName": "Hossain",
      "affiliations": []
    },
    {
      "id": 12841,
      "firstName": "Brian Y.",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 21036,
      "firstName": "Mo",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 21038,
      "firstName": "Pritom Saha",
      "lastName": "Akash",
      "affiliations": []
    },
    {
      "id": 21039,
      "firstName": "Mohammed",
      "lastName": "Alloulah",
      "affiliations": []
    },
    {
      "id": 8755,
      "firstName": "Xiaonan",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 8756,
      "firstName": "Erqun",
      "lastName": "Dong",
      "affiliations": []
    },
    {
      "id": 8770,
      "firstName": "Zhou",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 21061,
      "firstName": "Nadia",
      "lastName": "Berthouze",
      "affiliations": []
    },
    {
      "id": 12871,
      "firstName": "Zhilong",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 8776,
      "firstName": "Vera",
      "lastName": "Stara",
      "affiliations": []
    },
    {
      "id": 16970,
      "firstName": "Theofanis",
      "lastName": "Orphanoudakis",
      "affiliations": []
    },
    {
      "id": 21066,
      "firstName": "Anindya Das",
      "lastName": "Antar",
      "affiliations": []
    },
    {
      "id": 12874,
      "firstName": "Tian",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 12878,
      "firstName": "Debarka",
      "lastName": "Sengupta",
      "affiliations": []
    },
    {
      "id": 21070,
      "firstName": "Qi",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 12880,
      "firstName": "Xiaofan",
      "lastName": "Jiang",
      "affiliations": []
    },
    {
      "id": 8795,
      "firstName": "Ye",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 21084,
      "firstName": "Bart P.",
      "lastName": "Knijnenburg",
      "affiliations": []
    },
    {
      "id": 8798,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "affiliations": []
    },
    {
      "id": 12898,
      "firstName": "Ren",
      "lastName": "Ohmura",
      "affiliations": []
    },
    {
      "id": 8805,
      "firstName": "Fanny",
      "lastName": "Kobiela",
      "affiliations": []
    },
    {
      "id": 12903,
      "firstName": "Gregory F.",
      "lastName": "Lewis",
      "affiliations": []
    },
    {
      "id": 21096,
      "firstName": "James",
      "lastName": "Scott",
      "affiliations": []
    },
    {
      "id": 21097,
      "firstName": "Tauhidur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 8809,
      "firstName": "Francisco",
      "lastName": "Ordóñez",
      "affiliations": []
    },
    {
      "id": 8810,
      "firstName": "Alejandro",
      "lastName": "Molina",
      "affiliations": []
    },
    {
      "id": 17003,
      "firstName": "Eldy S. Lazaro",
      "lastName": "Vasquez",
      "affiliations": []
    },
    {
      "id": 17004,
      "firstName": "Akane",
      "lastName": "Sano",
      "affiliations": []
    },
    {
      "id": 8816,
      "firstName": "Wataru",
      "lastName": "Kudo",
      "affiliations": []
    },
    {
      "id": 12914,
      "firstName": "Keiichi",
      "lastName": "Yasumoto",
      "affiliations": []
    },
    {
      "id": 12915,
      "firstName": "Judy",
      "lastName": "Kay",
      "affiliations": []
    },
    {
      "id": 17013,
      "firstName": "Kyle",
      "lastName": "Qian",
      "affiliations": []
    },
    {
      "id": 12918,
      "firstName": "Takayuki",
      "lastName": "Suyama",
      "affiliations": []
    },
    {
      "id": 21112,
      "firstName": "Munmun De",
      "lastName": "Choudhury",
      "affiliations": []
    },
    {
      "id": 21114,
      "firstName": "Pegah",
      "lastName": "Hafiz",
      "affiliations": []
    },
    {
      "id": 21115,
      "firstName": "Sho",
      "lastName": "Sakurai",
      "affiliations": []
    },
    {
      "id": 21117,
      "firstName": "Charith",
      "lastName": "Perera",
      "affiliations": []
    },
    {
      "id": 8829,
      "firstName": "Wen",
      "lastName": "Dong",
      "affiliations": []
    },
    {
      "id": 8830,
      "firstName": "Alex Wong Tat",
      "lastName": "Hang",
      "affiliations": []
    },
    {
      "id": 12927,
      "firstName": "Virag",
      "lastName": "Varga",
      "affiliations": []
    },
    {
      "id": 17031,
      "firstName": "Dimitris",
      "lastName": "Chatzopoulos",
      "affiliations": []
    },
    {
      "id": 17032,
      "firstName": "Kit Yung",
      "lastName": "Lam",
      "affiliations": []
    },
    {
      "id": 21130,
      "firstName": "Jagmohan",
      "lastName": "Chauhan",
      "affiliations": []
    },
    {
      "id": 17035,
      "firstName": "Feng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 12947,
      "firstName": "Sourav",
      "lastName": "Bhattacharya",
      "affiliations": []
    },
    {
      "id": 17043,
      "firstName": "Run",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 12955,
      "firstName": "Anika Binte",
      "lastName": "Islam",
      "affiliations": []
    },
    {
      "id": 17052,
      "firstName": "Naohiro",
      "lastName": "Isokawa",
      "affiliations": []
    },
    {
      "id": 8861,
      "firstName": "Xiaonan",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 17054,
      "firstName": "Heiko",
      "lastName": "Müller",
      "affiliations": []
    },
    {
      "id": 21154,
      "firstName": "Yuxun",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 8866,
      "firstName": "Guihai",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 17061,
      "firstName": "Assam",
      "lastName": "Boudjelthia",
      "affiliations": []
    },
    {
      "id": 21160,
      "firstName": "Ying",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 17070,
      "firstName": "Allan",
      "lastName": "Berrocal",
      "affiliations": []
    },
    {
      "id": 21167,
      "firstName": "Yingqiang",
      "lastName": "Ge",
      "affiliations": []
    },
    {
      "id": 21171,
      "firstName": "Xiaoli",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 21175,
      "firstName": "James",
      "lastName": "Bailey",
      "affiliations": []
    },
    {
      "id": 21177,
      "firstName": "Vadlamudi Pratiksha",
      "lastName": "Sharma",
      "affiliations": []
    },
    {
      "id": 12985,
      "firstName": "Yangze",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 12987,
      "firstName": "Qin",
      "lastName": "Lv",
      "affiliations": []
    },
    {
      "id": 17083,
      "firstName": "Anna",
      "lastName": "Basu",
      "affiliations": []
    },
    {
      "id": 12988,
      "firstName": "Prerna",
      "lastName": "Chikersal",
      "affiliations": []
    },
    {
      "id": 8902,
      "firstName": "Varun",
      "lastName": "Mishra",
      "affiliations": []
    },
    {
      "id": 8905,
      "firstName": "Mingqi",
      "lastName": "Lv",
      "affiliations": []
    },
    {
      "id": 17098,
      "firstName": "Crystal",
      "lastName": "Compton",
      "affiliations": []
    },
    {
      "id": 13007,
      "firstName": "Yuri",
      "lastName": "Terasawa",
      "affiliations": []
    },
    {
      "id": 21199,
      "firstName": "Kuan-Yin",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 17103,
      "firstName": "Jonna",
      "lastName": "Häkkilä",
      "affiliations": []
    },
    {
      "id": 21200,
      "firstName": "Xin",
      "lastName": "Du",
      "affiliations": []
    },
    {
      "id": 17107,
      "firstName": "Khaild M.",
      "lastName": "Mosalam",
      "affiliations": []
    },
    {
      "id": 17108,
      "firstName": "Dawson",
      "lastName": "Clark",
      "affiliations": []
    },
    {
      "id": 21204,
      "firstName": "Jianfei",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 13019,
      "firstName": "Jazette",
      "lastName": "Johnson",
      "affiliations": []
    },
    {
      "id": 21212,
      "firstName": "Tatsushi",
      "lastName": "Matsubayashi",
      "affiliations": []
    },
    {
      "id": 21214,
      "firstName": "Can",
      "lastName": "Turk",
      "affiliations": []
    },
    {
      "id": 13023,
      "firstName": "Kaspar",
      "lastName": "Jansen",
      "affiliations": []
    },
    {
      "id": 8930,
      "firstName": "Krithika",
      "lastName": "Jagannath",
      "affiliations": []
    },
    {
      "id": 17126,
      "firstName": "Syed Monowar",
      "lastName": "Hossain",
      "affiliations": []
    },
    {
      "id": 13032,
      "firstName": "Nabil",
      "lastName": "Alshurafa",
      "affiliations": []
    },
    {
      "id": 13036,
      "firstName": "Dongzhu",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 17135,
      "firstName": "Yingying",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 21236,
      "firstName": "Rushil",
      "lastName": "Khurana",
      "affiliations": []
    },
    {
      "id": 21237,
      "firstName": "Dohyun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 8950,
      "firstName": "Jan",
      "lastName": "Ku?era",
      "affiliations": []
    },
    {
      "id": 21238,
      "firstName": "Alex X.",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 21240,
      "firstName": "Philipp M.",
      "lastName": "Scholl",
      "affiliations": []
    },
    {
      "id": 21242,
      "firstName": "Satu",
      "lastName": "Pakarinen",
      "affiliations": []
    },
    {
      "id": 13054,
      "firstName": "Takashi",
      "lastName": "Hamatani",
      "affiliations": []
    },
    {
      "id": 21248,
      "firstName": "Osamu",
      "lastName": "Saisho",
      "affiliations": []
    },
    {
      "id": 8960,
      "firstName": "Yuanqing",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 13058,
      "firstName": "Darya",
      "lastName": "Melnyk",
      "affiliations": []
    },
    {
      "id": 13068,
      "firstName": "Zhanna",
      "lastName": "Sarsenbayeva",
      "affiliations": []
    },
    {
      "id": 8975,
      "firstName": "Michael",
      "lastName": "Rivera",
      "affiliations": []
    },
    {
      "id": 21268,
      "firstName": "Pablo",
      "lastName": "Cesar",
      "affiliations": []
    },
    {
      "id": 17175,
      "firstName": "Yang",
      "lastName": "Tian",
      "affiliations": []
    },
    {
      "id": 8984,
      "firstName": "Eduardo",
      "lastName": "Velloso",
      "affiliations": []
    },
    {
      "id": 8985,
      "firstName": "Rawan",
      "lastName": "Alharbi",
      "affiliations": []
    },
    {
      "id": 17178,
      "firstName": "Hanbin",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13082,
      "firstName": "Tilman",
      "lastName": "Dingler",
      "affiliations": []
    },
    {
      "id": 13086,
      "firstName": "Manikanta D.",
      "lastName": "Reddy",
      "affiliations": []
    },
    {
      "id": 21279,
      "firstName": "Fangqing",
      "lastName": "Zhengren",
      "affiliations": []
    },
    {
      "id": 17184,
      "firstName": "Josef",
      "lastName": "Scharinger",
      "affiliations": []
    },
    {
      "id": 8993,
      "firstName": "Lucy E.",
      "lastName": "Dunne",
      "affiliations": []
    },
    {
      "id": 17188,
      "firstName": "Yu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 13095,
      "firstName": "Devyani",
      "lastName": "Parameshwar",
      "affiliations": []
    },
    {
      "id": 17192,
      "firstName": "Guillaume",
      "lastName": "Pythoud",
      "affiliations": []
    },
    {
      "id": 9015,
      "firstName": "Vivek",
      "lastName": "Chandel",
      "affiliations": []
    },
    {
      "id": 17208,
      "firstName": "Jonathan",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 21305,
      "firstName": "Michael",
      "lastName": "Bass",
      "affiliations": []
    },
    {
      "id": 13115,
      "firstName": "Kenta",
      "lastName": "Urano",
      "affiliations": []
    },
    {
      "id": 17212,
      "firstName": "Eckhard",
      "lastName": "Koch",
      "affiliations": []
    },
    {
      "id": 17217,
      "firstName": "Zann",
      "lastName": "Anderson",
      "affiliations": []
    },
    {
      "id": 13122,
      "firstName": "Enrico",
      "lastName": "Bignotti",
      "affiliations": []
    },
    {
      "id": 13125,
      "firstName": "Gábor",
      "lastName": "Sörös",
      "affiliations": []
    },
    {
      "id": 13126,
      "firstName": "Fusang",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 21318,
      "firstName": "Annamalai",
      "lastName": "Natarajan",
      "affiliations": []
    },
    {
      "id": 17226,
      "firstName": "Yusuke",
      "lastName": "Chida",
      "affiliations": []
    },
    {
      "id": 13131,
      "firstName": "Christina",
      "lastName": "Ru",
      "affiliations": []
    },
    {
      "id": 13133,
      "firstName": "Viswam",
      "lastName": "Nathan",
      "affiliations": []
    },
    {
      "id": 9038,
      "firstName": "Kun",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 17230,
      "firstName": "Yomna",
      "lastName": "Abdelrahman",
      "affiliations": []
    },
    {
      "id": 13137,
      "firstName": "Ian",
      "lastName": "Lane",
      "affiliations": []
    },
    {
      "id": 13139,
      "firstName": "Florian",
      "lastName": "Alt",
      "affiliations": []
    },
    {
      "id": 21332,
      "firstName": "Wenyao",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 21333,
      "firstName": "Andrea",
      "lastName": "Schankin",
      "affiliations": []
    },
    {
      "id": 21334,
      "firstName": "Mohammad",
      "lastName": "Shoyaib",
      "affiliations": []
    },
    {
      "id": 13142,
      "firstName": "Hao-Ping",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 17242,
      "firstName": "Larissa Cardoso",
      "lastName": "Zimmermann",
      "affiliations": []
    },
    {
      "id": 17243,
      "firstName": "Josh",
      "lastName": "Andres",
      "affiliations": []
    },
    {
      "id": 13149,
      "firstName": "Azzam",
      "lastName": "Alwan",
      "affiliations": []
    },
    {
      "id": 9056,
      "firstName": "Panlong",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 21348,
      "firstName": "Jin",
      "lastName": "Nakazawa",
      "affiliations": []
    },
    {
      "id": 9063,
      "firstName": "Junhyung",
      "lastName": "Moon",
      "affiliations": []
    },
    {
      "id": 17257,
      "firstName": "Taylan",
      "lastName": "Sen",
      "affiliations": []
    },
    {
      "id": 21356,
      "firstName": "Ryo",
      "lastName": "Takahashi",
      "affiliations": []
    },
    {
      "id": 9069,
      "firstName": "Yasuo",
      "lastName": "Katsuhara",
      "affiliations": []
    },
    {
      "id": 21361,
      "firstName": "Euan",
      "lastName": "Ashley",
      "affiliations": []
    },
    {
      "id": 13171,
      "firstName": "Ben",
      "lastName": "Smarr",
      "affiliations": []
    },
    {
      "id": 17267,
      "firstName": "Thanakrit",
      "lastName": "Jitapinyakul",
      "affiliations": []
    },
    {
      "id": 21369,
      "firstName": "David S",
      "lastName": "Bowers",
      "affiliations": []
    },
    {
      "id": 9082,
      "firstName": "Depeng",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 21374,
      "firstName": "Akira",
      "lastName": "Uchiyama",
      "affiliations": []
    },
    {
      "id": 21375,
      "firstName": "Lucia C.",
      "lastName": "Petito",
      "affiliations": []
    },
    {
      "id": 9090,
      "firstName": "Xiaolei",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 9092,
      "firstName": "Xin",
      "lastName": "Yi",
      "affiliations": []
    },
    {
      "id": 21381,
      "firstName": "Flora",
      "lastName": "Salim",
      "affiliations": []
    },
    {
      "id": 21385,
      "firstName": "Zeynep",
      "lastName": "GÌ_nes",
      "affiliations": []
    },
    {
      "id": 21386,
      "firstName": "Christoph",
      "lastName": "Anderson",
      "affiliations": []
    },
    {
      "id": 9102,
      "firstName": "Tomoya",
      "lastName": "Inoue",
      "affiliations": []
    },
    {
      "id": 17295,
      "firstName": "Paritosh",
      "lastName": "Bahirat",
      "affiliations": []
    },
    {
      "id": 17298,
      "firstName": "Deng",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 13203,
      "firstName": "Hamed",
      "lastName": "Haddadi",
      "affiliations": []
    },
    {
      "id": 13206,
      "firstName": "Woontack",
      "lastName": "Woo",
      "affiliations": []
    },
    {
      "id": 9113,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 9114,
      "firstName": "Han",
      "lastName": "Sha",
      "affiliations": []
    },
    {
      "id": 17308,
      "firstName": "Miquel",
      "lastName": "Alfaras",
      "affiliations": []
    },
    {
      "id": 21407,
      "firstName": "m.c.",
      "lastName": "schraefel",
      "affiliations": []
    },
    {
      "id": 17312,
      "firstName": "Farhana",
      "lastName": "Shahid",
      "affiliations": []
    },
    {
      "id": 17314,
      "firstName": "Roshan Lalintha",
      "lastName": "Peiris",
      "affiliations": []
    },
    {
      "id": 21411,
      "firstName": "Rhys",
      "lastName": "Beckett",
      "affiliations": []
    },
    {
      "id": 13219,
      "firstName": "Atsushi",
      "lastName": "Wada",
      "affiliations": []
    },
    {
      "id": 17319,
      "firstName": "Keiichi",
      "lastName": "Zempo",
      "affiliations": []
    },
    {
      "id": 21420,
      "firstName": "Xiaohua",
      "lastName": "Tian",
      "affiliations": []
    },
    {
      "id": 13232,
      "firstName": "Kathy",
      "lastName": "New",
      "affiliations": []
    },
    {
      "id": 21426,
      "firstName": "Daniyal",
      "lastName": "Liaqat",
      "affiliations": []
    },
    {
      "id": 21431,
      "firstName": "Maotian",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 9147,
      "firstName": "LouAnne",
      "lastName": "Boyd",
      "affiliations": []
    },
    {
      "id": 9154,
      "firstName": "Wasifur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 13252,
      "firstName": "Dominik",
      "lastName": "Weber",
      "affiliations": []
    },
    {
      "id": 13253,
      "firstName": "Chris",
      "lastName": "Harrison",
      "affiliations": []
    },
    {
      "id": 9158,
      "firstName": "Radhika",
      "lastName": "Garg",
      "affiliations": []
    },
    {
      "id": 17353,
      "firstName": "Thomas",
      "lastName": "Schlegel",
      "affiliations": []
    },
    {
      "id": 21452,
      "firstName": "Rebecca",
      "lastName": "Stewart",
      "affiliations": []
    },
    {
      "id": 21455,
      "firstName": "Ekaterina",
      "lastName": "Gilman",
      "affiliations": []
    },
    {
      "id": 17359,
      "firstName": "Edward",
      "lastName": "Moskal",
      "affiliations": []
    },
    {
      "id": 13265,
      "firstName": "Kenji",
      "lastName": "Tsushio",
      "affiliations": []
    },
    {
      "id": 21458,
      "firstName": "Fausto",
      "lastName": "Giunchiglia",
      "affiliations": []
    },
    {
      "id": 9171,
      "firstName": "Geoff",
      "lastName": "Kaufman",
      "affiliations": []
    },
    {
      "id": 9172,
      "firstName": "Jiaying",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 13272,
      "firstName": "Reynaldo",
      "lastName": "Morillo",
      "affiliations": []
    },
    {
      "id": 9177,
      "firstName": "Yusuke",
      "lastName": "Nishimura",
      "affiliations": []
    },
    {
      "id": 9178,
      "firstName": "Shilin",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 13282,
      "firstName": "Peter",
      "lastName": "Zdankin",
      "affiliations": []
    },
    {
      "id": 17379,
      "firstName": "Young D.",
      "lastName": "Kwon",
      "affiliations": []
    },
    {
      "id": 21477,
      "firstName": "Russell",
      "lastName": "Conduit",
      "affiliations": []
    },
    {
      "id": 13286,
      "firstName": "Mitja",
      "lastName": "Lustrek",
      "affiliations": []
    },
    {
      "id": 21478,
      "firstName": "Lili",
      "lastName": "Qiu",
      "affiliations": []
    },
    {
      "id": 13287,
      "firstName": "Luke William Feidhlim",
      "lastName": "Dickens",
      "affiliations": []
    },
    {
      "id": 13292,
      "firstName": "Muhammad I.",
      "lastName": "Safi",
      "affiliations": []
    },
    {
      "id": 9200,
      "firstName": "Nana Okai",
      "lastName": "Brako",
      "affiliations": []
    },
    {
      "id": 13297,
      "firstName": "Sunghoon Ivan",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 13300,
      "firstName": "Benjamin",
      "lastName": "Beichler",
      "affiliations": []
    },
    {
      "id": 13301,
      "firstName": "Andrés",
      "lastName": "Monroy-Hernández",
      "affiliations": []
    },
    {
      "id": 9208,
      "firstName": "Chia-Fang",
      "lastName": "Chung",
      "affiliations": []
    },
    {
      "id": 9212,
      "firstName": "Trupil",
      "lastName": "Limbasiya",
      "affiliations": []
    },
    {
      "id": 17406,
      "firstName": "Shoya",
      "lastName": "Ishimaru",
      "affiliations": []
    },
    {
      "id": 21502,
      "firstName": "Tao",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 21504,
      "firstName": "Ishita",
      "lastName": "Bhansali",
      "affiliations": []
    },
    {
      "id": 21507,
      "firstName": "Guangyuan",
      "lastName": "Su",
      "affiliations": []
    },
    {
      "id": 13318,
      "firstName": "Bin",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 21512,
      "firstName": "Paul",
      "lastName": "Beckett",
      "affiliations": []
    },
    {
      "id": 9228,
      "firstName": "Eiichi",
      "lastName": "Iwamoto",
      "affiliations": []
    },
    {
      "id": 13329,
      "firstName": "Minglu",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 13331,
      "firstName": "Lars",
      "lastName": "Kulik",
      "affiliations": []
    },
    {
      "id": 21523,
      "firstName": "Cecilia",
      "lastName": "Mascolo",
      "affiliations": []
    },
    {
      "id": 17428,
      "firstName": "Yifan",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 9238,
      "firstName": "Riccardo",
      "lastName": "Presotto",
      "affiliations": []
    },
    {
      "id": 21526,
      "firstName": "Ali",
      "lastName": "Israr",
      "affiliations": []
    },
    {
      "id": 17436,
      "firstName": "Yongli",
      "lastName": "Ren",
      "affiliations": []
    },
    {
      "id": 17438,
      "firstName": "Jungmi",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 9247,
      "firstName": "Edward",
      "lastName": "Lank",
      "affiliations": []
    },
    {
      "id": 9251,
      "firstName": "Weixi",
      "lastName": "Gu",
      "affiliations": []
    },
    {
      "id": 21542,
      "firstName": "InÌ»s",
      "lastName": "Lopes",
      "affiliations": []
    },
    {
      "id": 21543,
      "firstName": "Jong-Seok",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 17447,
      "firstName": "David",
      "lastName": "Kotz",
      "affiliations": []
    },
    {
      "id": 21544,
      "firstName": "Hao",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 17448,
      "firstName": "Erik",
      "lastName": "Pescara",
      "affiliations": []
    },
    {
      "id": 9257,
      "firstName": "Cheng",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 13356,
      "firstName": "Kouta",
      "lastName": "Minamizawa",
      "affiliations": []
    },
    {
      "id": 9261,
      "firstName": "Aman",
      "lastName": "Gupta",
      "affiliations": []
    },
    {
      "id": 13359,
      "firstName": "Takanori",
      "lastName": "Okuda",
      "affiliations": []
    },
    {
      "id": 13360,
      "firstName": "Andrea",
      "lastName": "Passerini",
      "affiliations": []
    },
    {
      "id": 21554,
      "firstName": "Yilin",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 17459,
      "firstName": "Scott",
      "lastName": "Cohen",
      "affiliations": []
    },
    {
      "id": 9274,
      "firstName": "Mohammad Rafayet",
      "lastName": "Ali",
      "affiliations": []
    },
    {
      "id": 9276,
      "firstName": "Muhammad",
      "lastName": "Umir",
      "affiliations": []
    },
    {
      "id": 13374,
      "firstName": "Kaya De",
      "lastName": "Barbaro",
      "affiliations": []
    },
    {
      "id": 21566,
      "firstName": "Tarek",
      "lastName": "Kassouf",
      "affiliations": []
    },
    {
      "id": 9285,
      "firstName": "Zhaoyuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 21573,
      "firstName": "Xinru",
      "lastName": "Page",
      "affiliations": []
    },
    {
      "id": 9291,
      "firstName": "Sasu",
      "lastName": "Tarkoma",
      "affiliations": []
    },
    {
      "id": 21580,
      "firstName": "Mohammed",
      "lastName": "Khwaja",
      "affiliations": []
    },
    {
      "id": 17488,
      "firstName": "Vlad C.",
      "lastName": "Coroama",
      "affiliations": []
    },
    {
      "id": 21586,
      "firstName": "Miao",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 9299,
      "firstName": "Taiga",
      "lastName": "Arai",
      "affiliations": []
    },
    {
      "id": 17493,
      "firstName": "Chaocan",
      "lastName": "Xiang",
      "affiliations": []
    },
    {
      "id": 13403,
      "firstName": "Teerayut",
      "lastName": "Horanont",
      "affiliations": []
    },
    {
      "id": 21598,
      "firstName": "Fan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13408,
      "firstName": "JeongGil",
      "lastName": "Ko",
      "affiliations": []
    },
    {
      "id": 21600,
      "firstName": "Trisha",
      "lastName": "Andrew",
      "affiliations": []
    },
    {
      "id": 9320,
      "firstName": "Eija",
      "lastName": "Halkola",
      "affiliations": []
    },
    {
      "id": 13416,
      "firstName": "Qian",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 17512,
      "firstName": "Margherita",
      "lastName": "Antona",
      "affiliations": []
    },
    {
      "id": 9321,
      "firstName": "Gunnar",
      "lastName": "Stevens",
      "affiliations": []
    },
    {
      "id": 13419,
      "firstName": "Tsukasa",
      "lastName": "Okimura",
      "affiliations": []
    },
    {
      "id": 17517,
      "firstName": "Farina",
      "lastName": "Faiz",
      "affiliations": []
    },
    {
      "id": 13422,
      "firstName": "Danula",
      "lastName": "Hettiachchi",
      "affiliations": []
    },
    {
      "id": 13425,
      "firstName": "Grace",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9330,
      "firstName": "Yukihiko",
      "lastName": "Okada",
      "affiliations": []
    },
    {
      "id": 13428,
      "firstName": "Jiadi",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 17525,
      "firstName": "Chris Xiaoxuan",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 13432,
      "firstName": "Aku",
      "lastName": "Visuri",
      "affiliations": []
    },
    {
      "id": 9337,
      "firstName": "Veljko",
      "lastName": "Pejovic",
      "affiliations": []
    },
    {
      "id": 13434,
      "firstName": "Smriti",
      "lastName": "Rani",
      "affiliations": []
    },
    {
      "id": 17531,
      "firstName": "Gregory D.",
      "lastName": "Abowd",
      "affiliations": []
    },
    {
      "id": 9345,
      "firstName": "Alexander",
      "lastName": "Lamson",
      "affiliations": []
    },
    {
      "id": 13444,
      "firstName": "Mayank",
      "lastName": "Goel",
      "affiliations": []
    },
    {
      "id": 17542,
      "firstName": "Shriti",
      "lastName": "Raj",
      "affiliations": []
    },
    {
      "id": 13447,
      "firstName": "Chongyang",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9354,
      "firstName": "Jorge",
      "lastName": "Ortiz",
      "affiliations": []
    },
    {
      "id": 13453,
      "firstName": "Feng",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 17557,
      "firstName": "Chang-Chieh",
      "lastName": "Cheng",
      "affiliations": []
    },
    {
      "id": 13466,
      "firstName": "Kento",
      "lastName": "Katsumata",
      "affiliations": []
    },
    {
      "id": 17570,
      "firstName": "Nitesh V.",
      "lastName": "Chawla",
      "affiliations": []
    },
    {
      "id": 17571,
      "firstName": "Samantha W. T.",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 9384,
      "firstName": "Yohan",
      "lastName": "Moon",
      "affiliations": []
    },
    {
      "id": 21680,
      "firstName": "Daniel",
      "lastName": "Tetteroo",
      "affiliations": []
    },
    {
      "id": 9397,
      "firstName": "Andrea",
      "lastName": "Gershon",
      "affiliations": []
    },
    {
      "id": 17590,
      "firstName": "Ana",
      "lastName": "Vasconcelos",
      "affiliations": []
    },
    {
      "id": 13505,
      "firstName": "Chao",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 21698,
      "firstName": "Nitesh",
      "lastName": "Goyal",
      "affiliations": []
    },
    {
      "id": 9419,
      "firstName": "Haiyong",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 13519,
      "firstName": "Max",
      "lastName": "MÌ_hlhÌ_user",
      "affiliations": []
    },
    {
      "id": 9431,
      "firstName": "Takumi",
      "lastName": "Kondo",
      "affiliations": []
    },
    {
      "id": 9435,
      "firstName": "Luigi De",
      "lastName": "Russis",
      "affiliations": []
    },
    {
      "id": 13534,
      "firstName": "Huanhuan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 9439,
      "firstName": "Teruo",
      "lastName": "Higashino",
      "affiliations": []
    },
    {
      "id": 17636,
      "firstName": "Yuki",
      "lastName": "Matsuda",
      "affiliations": []
    },
    {
      "id": 13543,
      "firstName": "Hirotaka",
      "lastName": "Kaji",
      "affiliations": []
    },
    {
      "id": 21735,
      "firstName": "Kristin",
      "lastName": "Neidlinger",
      "affiliations": []
    },
    {
      "id": 13545,
      "firstName": "Stephen",
      "lastName": "Voida",
      "affiliations": []
    },
    {
      "id": 21739,
      "firstName": "Can",
      "lastName": "Rong",
      "affiliations": []
    },
    {
      "id": 9451,
      "firstName": "Hayate",
      "lastName": "Hironaka",
      "affiliations": []
    },
    {
      "id": 17647,
      "firstName": "Jiafei",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 9457,
      "firstName": "Liehuang",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 13555,
      "firstName": "Shengjian Jammy",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 21748,
      "firstName": "Yu",
      "lastName": "Enokibori",
      "affiliations": []
    },
    {
      "id": 9463,
      "firstName": "Seolha",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 17655,
      "firstName": "Deepak",
      "lastName": "Ganesan",
      "affiliations": []
    },
    {
      "id": 9465,
      "firstName": "Matthieu",
      "lastName": "Vegreville",
      "affiliations": []
    },
    {
      "id": 13569,
      "firstName": "Atefeh Mahdavi",
      "lastName": "Goloujeh",
      "affiliations": []
    },
    {
      "id": 17674,
      "firstName": "Yifan",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 13578,
      "firstName": "Bin",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 17683,
      "firstName": "Hung-Quoc",
      "lastName": "Lai",
      "affiliations": []
    },
    {
      "id": 21779,
      "firstName": "Gierad",
      "lastName": "Laput",
      "affiliations": []
    },
    {
      "id": 17685,
      "firstName": "Julia",
      "lastName": "Dominiak",
      "affiliations": []
    },
    {
      "id": 13590,
      "firstName": "Mohammed (Ehsan)",
      "lastName": "Hoque",
      "affiliations": []
    },
    {
      "id": 9497,
      "firstName": "Rahul",
      "lastName": "Majethia",
      "affiliations": []
    },
    {
      "id": 13593,
      "firstName": "Tong",
      "lastName": "Xia",
      "affiliations": []
    },
    {
      "id": 17691,
      "firstName": "Matthias",
      "lastName": "Füller",
      "affiliations": []
    },
    {
      "id": 9503,
      "firstName": "Eva",
      "lastName": "Valencic",
      "affiliations": []
    },
    {
      "id": 17695,
      "firstName": "Haiping",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 17701,
      "firstName": "Pratyush",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 13609,
      "firstName": "Jane Yung-Jen",
      "lastName": "Hsu",
      "affiliations": []
    },
    {
      "id": 17705,
      "firstName": "Kelsey A",
      "lastName": "Vitullo",
      "affiliations": []
    },
    {
      "id": 13612,
      "firstName": "Kosuke",
      "lastName": "Watanabe",
      "affiliations": []
    },
    {
      "id": 9518,
      "firstName": "Thanh-Trung",
      "lastName": "Phan",
      "affiliations": []
    },
    {
      "id": 13618,
      "firstName": "Sayma",
      "lastName": "Akther",
      "affiliations": []
    },
    {
      "id": 9524,
      "firstName": "Saleem",
      "lastName": "Bhatti",
      "affiliations": []
    },
    {
      "id": 17717,
      "firstName": "Shengjie",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 13622,
      "firstName": "Tieming",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 21814,
      "firstName": "Linda",
      "lastName": "Ohrn-McDaniel",
      "affiliations": []
    },
    {
      "id": 13623,
      "firstName": "Kazuki",
      "lastName": "Kiriu",
      "affiliations": []
    },
    {
      "id": 17721,
      "firstName": "Ruchuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 21817,
      "firstName": "Vir V.",
      "lastName": "Phoha",
      "affiliations": []
    },
    {
      "id": 9532,
      "firstName": "Shuai",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9533,
      "firstName": "Yusuke",
      "lastName": "Soneda",
      "affiliations": []
    },
    {
      "id": 13630,
      "firstName": "Hemin",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 13635,
      "firstName": "Takuto",
      "lastName": "Yoshida",
      "affiliations": []
    },
    {
      "id": 21828,
      "firstName": "Amin",
      "lastName": "Sadri",
      "affiliations": []
    },
    {
      "id": 17736,
      "firstName": "Daqing",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 21839,
      "firstName": "Gautham Krishna",
      "lastName": "Gudur",
      "affiliations": []
    },
    {
      "id": 13653,
      "firstName": "Joseph",
      "lastName": "Korpela",
      "affiliations": []
    },
    {
      "id": 13654,
      "firstName": "Tong",
      "lastName": "Zhan",
      "affiliations": []
    },
    {
      "id": 13656,
      "firstName": "Kenichiro",
      "lastName": "Shirota",
      "affiliations": []
    },
    {
      "id": 17754,
      "firstName": "Gaurav",
      "lastName": "Srivastava",
      "affiliations": []
    },
    {
      "id": 21853,
      "firstName": "Hiroshi",
      "lastName": "Nakashima",
      "affiliations": []
    },
    {
      "id": 9569,
      "firstName": "Yoichi",
      "lastName": "Shinoda",
      "affiliations": []
    },
    {
      "id": 13666,
      "firstName": "Li",
      "lastName": "Su",
      "affiliations": []
    },
    {
      "id": 13667,
      "firstName": "Hung-Yeh",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 9571,
      "firstName": "Ishwarya",
      "lastName": "Ananthabhotla",
      "affiliations": []
    },
    {
      "id": 21865,
      "firstName": "Yoshihiro",
      "lastName": "Kawahara",
      "affiliations": []
    },
    {
      "id": 9578,
      "firstName": "AndréS",
      "lastName": "Monroy-HernáNdez",
      "affiliations": []
    },
    {
      "id": 21867,
      "firstName": "Lama",
      "lastName": "Nachman",
      "affiliations": []
    },
    {
      "id": 21868,
      "firstName": "Kiran",
      "lastName": "Javkar",
      "affiliations": []
    },
    {
      "id": 17773,
      "firstName": "Nasimuddin",
      "lastName": "Ahmed",
      "affiliations": []
    },
    {
      "id": 9581,
      "firstName": "Mustafa",
      "lastName": "Al'Absi",
      "affiliations": []
    },
    {
      "id": 13682,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "affiliations": []
    },
    {
      "id": 21875,
      "firstName": "Claudio",
      "lastName": "Bettini",
      "affiliations": []
    },
    {
      "id": 13687,
      "firstName": "Lin",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 17784,
      "firstName": "Yiqiang",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9599,
      "firstName": "George",
      "lastName": "Roussos",
      "affiliations": []
    },
    {
      "id": 13697,
      "firstName": "Chia-Hua",
      "lastName": "Kuo",
      "affiliations": []
    },
    {
      "id": 17793,
      "firstName": "Gustavo A.",
      "lastName": "Angarita",
      "affiliations": []
    },
    {
      "id": 9605,
      "firstName": "Nadia",
      "lastName": "Bianchi-Berthouze",
      "affiliations": []
    },
    {
      "id": 13701,
      "firstName": "Corinna",
      "lastName": "Ogonowski",
      "affiliations": []
    },
    {
      "id": 17798,
      "firstName": "Xinlei",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 13703,
      "firstName": "Onorina",
      "lastName": "Kovalenko",
      "affiliations": []
    },
    {
      "id": 9607,
      "firstName": "Santi",
      "lastName": "Phithakkitnukoon",
      "affiliations": []
    },
    {
      "id": 21896,
      "firstName": "Ke",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 9611,
      "firstName": "Dawei",
      "lastName": "Liang",
      "affiliations": []
    },
    {
      "id": 13710,
      "firstName": "Grace C.",
      "lastName": "Wusk",
      "affiliations": []
    },
    {
      "id": 17810,
      "firstName": "Xiaoze",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 13717,
      "firstName": "Jun-Ho",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 9623,
      "firstName": "BoJie",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9626,
      "firstName": "Tao",
      "lastName": "Yan",
      "affiliations": []
    },
    {
      "id": 17822,
      "firstName": "Shengdong",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 13728,
      "firstName": "Muhammad",
      "lastName": "Umair",
      "affiliations": []
    },
    {
      "id": 21921,
      "firstName": "Rui Miguel",
      "lastName": "Pascoal",
      "affiliations": []
    },
    {
      "id": 21922,
      "firstName": "Yuki",
      "lastName": "Matsuura",
      "affiliations": []
    },
    {
      "id": 17830,
      "firstName": "Sheng",
      "lastName": "Wei",
      "affiliations": []
    },
    {
      "id": 9638,
      "firstName": "Jian Qing",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 9640,
      "firstName": "Thilina",
      "lastName": "Dissanayake",
      "affiliations": []
    },
    {
      "id": 17833,
      "firstName": "Corina",
      "lastName": "Sas",
      "affiliations": []
    },
    {
      "id": 21934,
      "firstName": "Hongan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 17839,
      "firstName": "Bipin",
      "lastName": "Indurkhya",
      "affiliations": []
    },
    {
      "id": 13748,
      "firstName": "Arijit",
      "lastName": "Chowdhury",
      "affiliations": []
    },
    {
      "id": 13757,
      "firstName": "Hendrik",
      "lastName": "Engelbrecht",
      "affiliations": []
    },
    {
      "id": 13758,
      "firstName": "Thad",
      "lastName": "Starner",
      "affiliations": []
    },
    {
      "id": 9662,
      "firstName": "Marc",
      "lastName": "Tompkins",
      "affiliations": []
    },
    {
      "id": 9663,
      "firstName": "Grace",
      "lastName": "Eden",
      "affiliations": []
    },
    {
      "id": 21954,
      "firstName": "Christian",
      "lastName": "Haubelt",
      "affiliations": []
    },
    {
      "id": 13763,
      "firstName": "Illia",
      "lastName": "Fedorin",
      "affiliations": []
    },
    {
      "id": 21955,
      "firstName": "Kang",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 21956,
      "firstName": "Wenzhong",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 9670,
      "firstName": "Kristine",
      "lastName": "Kuprijanova",
      "affiliations": []
    },
    {
      "id": 9673,
      "firstName": "Nikos",
      "lastName": "Karousos",
      "affiliations": []
    },
    {
      "id": 17866,
      "firstName": "Kristina",
      "lastName": "Andersen",
      "affiliations": []
    },
    {
      "id": 21963,
      "firstName": "Lutfun Nahar",
      "lastName": "Lota",
      "affiliations": []
    },
    {
      "id": 17869,
      "firstName": "Alexandros",
      "lastName": "Liapis",
      "affiliations": []
    },
    {
      "id": 9678,
      "firstName": "Bernard",
      "lastName": "Serpette",
      "affiliations": []
    },
    {
      "id": 21967,
      "firstName": "Emmi",
      "lastName": "Harjuniemi",
      "affiliations": []
    },
    {
      "id": 21970,
      "firstName": "Yu",
      "lastName": "Sheng",
      "affiliations": []
    },
    {
      "id": 17874,
      "firstName": "Masaru",
      "lastName": "Onodera",
      "affiliations": []
    },
    {
      "id": 13778,
      "firstName": "Granit",
      "lastName": "Luzhnica",
      "affiliations": []
    },
    {
      "id": 21972,
      "firstName": "xiaodong",
      "lastName": "cai",
      "affiliations": []
    },
    {
      "id": 17879,
      "firstName": "Heiko",
      "lastName": "MÌ_ller",
      "affiliations": []
    },
    {
      "id": 17882,
      "firstName": "Minsam",
      "lastName": "Ko",
      "affiliations": []
    },
    {
      "id": 17887,
      "firstName": "Zheng",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 13797,
      "firstName": "PÌ©ter",
      "lastName": "Hevesi",
      "affiliations": []
    },
    {
      "id": 9702,
      "firstName": "Sachine",
      "lastName": "Yoshida",
      "affiliations": []
    },
    {
      "id": 21990,
      "firstName": "Lida",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13799,
      "firstName": "Haiying",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 9704,
      "firstName": "Vlad",
      "lastName": "Manea",
      "affiliations": []
    },
    {
      "id": 17905,
      "firstName": "Catherine",
      "lastName": "Tong",
      "affiliations": []
    },
    {
      "id": 9719,
      "firstName": "Khiet P.",
      "lastName": "Truong",
      "affiliations": []
    },
    {
      "id": 13817,
      "firstName": "Amy",
      "lastName": "Ogan",
      "affiliations": []
    },
    {
      "id": 13824,
      "firstName": "Kazuya",
      "lastName": "Murao",
      "affiliations": []
    },
    {
      "id": 17920,
      "firstName": "George",
      "lastName": "Chernyshov",
      "affiliations": []
    },
    {
      "id": 13827,
      "firstName": "Jin",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 17928,
      "firstName": "Vitor Fortes",
      "lastName": "Rey",
      "affiliations": []
    },
    {
      "id": 22030,
      "firstName": "Grace",
      "lastName": "Kao",
      "affiliations": []
    },
    {
      "id": 22033,
      "firstName": "Daisuke",
      "lastName": "Kotani",
      "affiliations": []
    },
    {
      "id": 22036,
      "firstName": "Pei-Yi",
      "lastName": "Kuo",
      "affiliations": []
    },
    {
      "id": 9748,
      "firstName": "Yanyong",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 17947,
      "firstName": "Rong",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 22048,
      "firstName": "Zhiyuan",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 22053,
      "firstName": "Sean",
      "lastName": "Follmer",
      "affiliations": []
    },
    {
      "id": 22057,
      "firstName": "Yi-Ching",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 9771,
      "firstName": "Avik",
      "lastName": "Ghose",
      "affiliations": []
    },
    {
      "id": 9775,
      "firstName": "Cathy",
      "lastName": "Tan",
      "affiliations": []
    },
    {
      "id": 17967,
      "firstName": "Raden Agoeng",
      "lastName": "Bhimasta",
      "affiliations": []
    },
    {
      "id": 9776,
      "firstName": "Mahesan",
      "lastName": "Niranjan",
      "affiliations": []
    },
    {
      "id": 17969,
      "firstName": "Vivian Genaro",
      "lastName": "Motti",
      "affiliations": []
    },
    {
      "id": 9777,
      "firstName": "Kyeyoon",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 13876,
      "firstName": "Juyoung",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 9781,
      "firstName": "Nita",
      "lastName": "Virtala",
      "affiliations": []
    },
    {
      "id": 13882,
      "firstName": "Tengxiang",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 9791,
      "firstName": "Yang",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9801,
      "firstName": "Pradthana",
      "lastName": "Jarusriboonchai",
      "affiliations": []
    },
    {
      "id": 22090,
      "firstName": "Chih Chi",
      "lastName": "Hu",
      "affiliations": []
    },
    {
      "id": 22092,
      "firstName": "Kofi M.",
      "lastName": "Odame",
      "affiliations": []
    },
    {
      "id": 9807,
      "firstName": "Linda",
      "lastName": "Hirsch",
      "affiliations": []
    },
    {
      "id": 13910,
      "firstName": "Geraldine",
      "lastName": "Fitzpatrick",
      "affiliations": []
    },
    {
      "id": 22103,
      "firstName": "Janet",
      "lastName": "Eyre",
      "affiliations": []
    },
    {
      "id": 13914,
      "firstName": "Daisy van",
      "lastName": "Loenhout",
      "affiliations": []
    },
    {
      "id": 9819,
      "firstName": "Chenxing",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 22108,
      "firstName": "XinLei",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9822,
      "firstName": "Florian",
      "lastName": "Jungwirth",
      "affiliations": []
    },
    {
      "id": 9823,
      "firstName": "Koichi",
      "lastName": "Shimoda",
      "affiliations": []
    },
    {
      "id": 13926,
      "firstName": "Takayuki",
      "lastName": "Sakamoto",
      "affiliations": []
    },
    {
      "id": 18024,
      "firstName": "Beryl",
      "lastName": "Noë",
      "affiliations": []
    },
    {
      "id": 9832,
      "firstName": "Minyi",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 9834,
      "firstName": "Lee",
      "lastName": "Jones",
      "affiliations": []
    },
    {
      "id": 18026,
      "firstName": "Yang",
      "lastName": "Long",
      "affiliations": []
    },
    {
      "id": 9835,
      "firstName": "Lik Hang",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 18029,
      "firstName": "Cristina",
      "lastName": "Mihale-Wilson",
      "affiliations": []
    },
    {
      "id": 22125,
      "firstName": "Wei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9838,
      "firstName": "Ning",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 18033,
      "firstName": "Andreas",
      "lastName": "Hein",
      "affiliations": []
    },
    {
      "id": 18036,
      "firstName": "Thongtat",
      "lastName": "Oransirikul",
      "affiliations": []
    },
    {
      "id": 22140,
      "firstName": "Nina",
      "lastName": "Rescic",
      "affiliations": []
    },
    {
      "id": 9853,
      "firstName": "Jeffrey",
      "lastName": "Bigham",
      "affiliations": []
    },
    {
      "id": 18048,
      "firstName": "Anton",
      "lastName": "Isopoussu",
      "affiliations": []
    },
    {
      "id": 18049,
      "firstName": "Wanyi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 13958,
      "firstName": "Elena Márquez",
      "lastName": "Segura",
      "affiliations": []
    },
    {
      "id": 9864,
      "firstName": "Klara",
      "lastName": "Nahrstedt",
      "affiliations": []
    },
    {
      "id": 9865,
      "firstName": "Jingyi",
      "lastName": "Ma",
      "affiliations": []
    },
    {
      "id": 13964,
      "firstName": "Xiaoyang",
      "lastName": "Xie",
      "affiliations": []
    },
    {
      "id": 13965,
      "firstName": "Finn L",
      "lastName": "Strivens",
      "affiliations": []
    },
    {
      "id": 18065,
      "firstName": "Yuqin",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 18066,
      "firstName": "Sumar S.",
      "lastName": "Vaid",
      "affiliations": []
    },
    {
      "id": 18068,
      "firstName": "Hideyuki",
      "lastName": "Takada",
      "affiliations": []
    },
    {
      "id": 9879,
      "firstName": "Catherine",
      "lastName": "Harty",
      "affiliations": []
    },
    {
      "id": 9884,
      "firstName": "Feng",
      "lastName": "Qian",
      "affiliations": []
    },
    {
      "id": 22173,
      "firstName": "Martin",
      "lastName": "Pielot",
      "affiliations": []
    },
    {
      "id": 22182,
      "firstName": "Alessandro",
      "lastName": "Montanari",
      "affiliations": []
    },
    {
      "id": 22187,
      "firstName": "Sven",
      "lastName": "Casteleyn",
      "affiliations": []
    },
    {
      "id": 9904,
      "firstName": "Tao",
      "lastName": "Gu",
      "affiliations": []
    },
    {
      "id": 9905,
      "firstName": "Sadia",
      "lastName": "Sharmin",
      "affiliations": []
    },
    {
      "id": 22193,
      "firstName": "Marian",
      "lastName": "Waltereit",
      "affiliations": []
    },
    {
      "id": 14005,
      "firstName": "Yuvraj",
      "lastName": "Agarwal",
      "affiliations": []
    },
    {
      "id": 18107,
      "firstName": "Paula",
      "lastName": "Lago",
      "affiliations": []
    },
    {
      "id": 9916,
      "firstName": "Yongcai",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18113,
      "firstName": "Yu",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 18115,
      "firstName": "Anhong",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 14023,
      "firstName": "Pablo E.",
      "lastName": "Paredes",
      "affiliations": []
    },
    {
      "id": 9929,
      "firstName": "Suining",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 9934,
      "firstName": "Yue",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 18132,
      "firstName": "Kristof Van",
      "lastName": "Laerhoven",
      "affiliations": []
    },
    {
      "id": 18133,
      "firstName": "Nova",
      "lastName": "Ahmed",
      "affiliations": []
    },
    {
      "id": 9941,
      "firstName": "Pamela",
      "lastName": "Wisniewski",
      "affiliations": []
    },
    {
      "id": 22230,
      "firstName": "Shashank Prasad",
      "lastName": "Rao",
      "affiliations": []
    },
    {
      "id": 22231,
      "firstName": "Jason I.",
      "lastName": "Hong",
      "affiliations": []
    },
    {
      "id": 18135,
      "firstName": "Felix A.",
      "lastName": "Epp",
      "affiliations": []
    },
    {
      "id": 22235,
      "firstName": "Deying",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 18139,
      "firstName": "Xiang",
      "lastName": "Su",
      "affiliations": []
    },
    {
      "id": 18140,
      "firstName": "Gabriella M.",
      "lastName": "Harari",
      "affiliations": []
    },
    {
      "id": 9949,
      "firstName": "Joseph A.",
      "lastName": "Paradiso",
      "affiliations": []
    },
    {
      "id": 9952,
      "firstName": "Daniel",
      "lastName": "Hintze",
      "affiliations": []
    },
    {
      "id": 14053,
      "firstName": "Hitoshi",
      "lastName": "Matsuyama",
      "affiliations": []
    },
    {
      "id": 9963,
      "firstName": "Attila",
      "lastName": "Reiss",
      "affiliations": []
    },
    {
      "id": 18155,
      "firstName": "Jamie",
      "lastName": "Award",
      "affiliations": []
    },
    {
      "id": 22253,
      "firstName": "Christos",
      "lastName": "Katsanos",
      "affiliations": []
    },
    {
      "id": 22256,
      "firstName": "Chen",
      "lastName": "Song",
      "affiliations": []
    },
    {
      "id": 9968,
      "firstName": "Brandon",
      "lastName": "Oubre",
      "affiliations": []
    },
    {
      "id": 9971,
      "firstName": "Anne",
      "lastName": "Xie",
      "affiliations": []
    },
    {
      "id": 18164,
      "firstName": "sayeda shamma",
      "lastName": "alia",
      "affiliations": []
    },
    {
      "id": 18166,
      "firstName": "Florian",
      "lastName": "GrÌ_tzmacher",
      "affiliations": []
    },
    {
      "id": 14070,
      "firstName": "Yan",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 9975,
      "firstName": "Guang",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 22265,
      "firstName": "Longfei",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 9983,
      "firstName": "Kyoungwoo",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 22277,
      "firstName": "Jeremy",
      "lastName": "Frey",
      "affiliations": []
    },
    {
      "id": 22282,
      "firstName": "Thisum",
      "lastName": "Buddhika",
      "affiliations": []
    },
    {
      "id": 9998,
      "firstName": "Protap Kumar",
      "lastName": "Saha",
      "affiliations": []
    },
    {
      "id": 9999,
      "firstName": "Huandong",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18197,
      "firstName": "Anind K.",
      "lastName": "Dey",
      "affiliations": []
    },
    {
      "id": 14106,
      "firstName": "Dimitris",
      "lastName": "Papanikolaou",
      "affiliations": []
    },
    {
      "id": 18202,
      "firstName": "Shin",
      "lastName": "Mizutani",
      "affiliations": []
    },
    {
      "id": 22300,
      "firstName": "Kouhei",
      "lastName": "Kaminishi",
      "affiliations": []
    },
    {
      "id": 22301,
      "firstName": "Alberto",
      "lastName": "GonzÌÁlez-PÌ©rez",
      "affiliations": []
    },
    {
      "id": 10015,
      "firstName": "Katherine",
      "lastName": "Isbister",
      "affiliations": []
    },
    {
      "id": 14112,
      "firstName": "Walter",
      "lastName": "Dempsey",
      "affiliations": []
    },
    {
      "id": 18210,
      "firstName": "Ignacio",
      "lastName": "Miralles",
      "affiliations": []
    },
    {
      "id": 22308,
      "firstName": "Jianjun",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 10026,
      "firstName": "Chaoran",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 14122,
      "firstName": "Arpan",
      "lastName": "Pal",
      "affiliations": []
    },
    {
      "id": 18219,
      "firstName": "Nils",
      "lastName": "Vindice",
      "affiliations": []
    },
    {
      "id": 18221,
      "firstName": "Raja",
      "lastName": "Jurdak",
      "affiliations": []
    },
    {
      "id": 18223,
      "firstName": "Suzi",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 22319,
      "firstName": "Enze",
      "lastName": "Yi",
      "affiliations": []
    },
    {
      "id": 14130,
      "firstName": "Kazuaki",
      "lastName": "Nomura",
      "affiliations": []
    },
    {
      "id": 14131,
      "firstName": "Chuang-Wen",
      "lastName": "You",
      "affiliations": []
    },
    {
      "id": 14132,
      "firstName": "Martin",
      "lastName": "Maritsch",
      "affiliations": []
    },
    {
      "id": 10037,
      "firstName": "Eshan",
      "lastName": "Mushtaq",
      "affiliations": []
    },
    {
      "id": 10040,
      "firstName": "Takahiro",
      "lastName": "Hara",
      "affiliations": []
    },
    {
      "id": 18236,
      "firstName": "Johannes",
      "lastName": "Selymes",
      "affiliations": []
    },
    {
      "id": 18241,
      "firstName": "Lin",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 10049,
      "firstName": "Simon",
      "lastName": "Ozbek",
      "affiliations": []
    },
    {
      "id": 22338,
      "firstName": "Valerii",
      "lastName": "Kan",
      "affiliations": []
    },
    {
      "id": 10050,
      "firstName": "Pooya",
      "lastName": "Khaloo",
      "affiliations": []
    },
    {
      "id": 14150,
      "firstName": "KAI",
      "lastName": "WU",
      "affiliations": []
    },
    {
      "id": 14155,
      "firstName": "Evy",
      "lastName": "Murraij",
      "affiliations": []
    },
    {
      "id": 14157,
      "firstName": "Milan",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 14160,
      "firstName": "Tong",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 10068,
      "firstName": "Torben",
      "lastName": "Weis",
      "affiliations": []
    },
    {
      "id": 18262,
      "firstName": "Esther",
      "lastName": "Mandelblum",
      "affiliations": []
    },
    {
      "id": 10072,
      "firstName": "Aziret",
      "lastName": "Satybaldiev",
      "affiliations": []
    },
    {
      "id": 10074,
      "firstName": "Eric",
      "lastName": "Paulos",
      "affiliations": []
    },
    {
      "id": 22364,
      "firstName": "GaÌÇl Le",
      "lastName": "Lan",
      "affiliations": []
    },
    {
      "id": 14172,
      "firstName": "Yu",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 14174,
      "firstName": "Malte F.",
      "lastName": "Jung",
      "affiliations": []
    },
    {
      "id": 22366,
      "firstName": "Bing",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18271,
      "firstName": "Hiraki",
      "lastName": "Yasuda",
      "affiliations": []
    },
    {
      "id": 22370,
      "firstName": "Yasue",
      "lastName": "Kishino",
      "affiliations": []
    },
    {
      "id": 10086,
      "firstName": "Yi",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 10087,
      "firstName": "Charles",
      "lastName": "Consel",
      "affiliations": []
    },
    {
      "id": 14189,
      "firstName": "Laura",
      "lastName": "Sokka",
      "affiliations": []
    },
    {
      "id": 18285,
      "firstName": "Amos A",
      "lastName": "Folarin",
      "affiliations": []
    },
    {
      "id": 14192,
      "firstName": "Andrew T.",
      "lastName": "Campbell",
      "affiliations": []
    },
    {
      "id": 14197,
      "firstName": "Jinbo",
      "lastName": "Bi",
      "affiliations": []
    },
    {
      "id": 10104,
      "firstName": "Sune",
      "lastName": "Lehmann",
      "affiliations": []
    },
    {
      "id": 18297,
      "firstName": "Marc van den",
      "lastName": "Broeck",
      "affiliations": []
    },
    {
      "id": 10105,
      "firstName": "Suranga",
      "lastName": "Nanayakkara",
      "affiliations": []
    },
    {
      "id": 18298,
      "firstName": "Gabriele",
      "lastName": "Civitarese",
      "affiliations": []
    },
    {
      "id": 10106,
      "firstName": "Yuuki",
      "lastName": "Nagamatsu",
      "affiliations": []
    },
    {
      "id": 18299,
      "firstName": "Weilong",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 22399,
      "firstName": "Elze",
      "lastName": "Schers",
      "affiliations": []
    },
    {
      "id": 10112,
      "firstName": "Kyuin",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 18306,
      "firstName": "Bonnie",
      "lastName": "Spring",
      "affiliations": []
    },
    {
      "id": 14213,
      "firstName": "Mikkel Baun",
      "lastName": "Kjærgaard",
      "affiliations": []
    },
    {
      "id": 18309,
      "firstName": "Peter",
      "lastName": "Wei",
      "affiliations": []
    },
    {
      "id": 22410,
      "firstName": "Lawrence H.",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 22412,
      "firstName": "Victor",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 18319,
      "firstName": "Hiroki",
      "lastName": "Yoshikawa",
      "affiliations": []
    },
    {
      "id": 18324,
      "firstName": "Martin",
      "lastName": "Flintham",
      "affiliations": []
    },
    {
      "id": 18330,
      "firstName": "Ludwig",
      "lastName": "Trotter",
      "affiliations": []
    },
    {
      "id": 10139,
      "firstName": "Ryota",
      "lastName": "Sawano",
      "affiliations": []
    },
    {
      "id": 14237,
      "firstName": "Jessica R.",
      "lastName": "Cauchard",
      "affiliations": []
    },
    {
      "id": 18342,
      "firstName": "Zhenjiang",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 10151,
      "firstName": "Paulo",
      "lastName": "Dias",
      "affiliations": []
    },
    {
      "id": 14247,
      "firstName": "Chulhong",
      "lastName": "Min",
      "affiliations": []
    },
    {
      "id": 10155,
      "firstName": "Peter",
      "lastName": "Hevesi",
      "affiliations": []
    },
    {
      "id": 10157,
      "firstName": "Dongmei",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 14254,
      "firstName": "Can",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 18351,
      "firstName": "Asangi",
      "lastName": "Jayatilaka",
      "affiliations": []
    },
    {
      "id": 14258,
      "firstName": "Daniella K.",
      "lastName": "Villalba",
      "affiliations": []
    },
    {
      "id": 10163,
      "firstName": "Erik",
      "lastName": "Gabrielsen",
      "affiliations": []
    },
    {
      "id": 14261,
      "firstName": "Nipi",
      "lastName": "Paul",
      "affiliations": []
    },
    {
      "id": 22456,
      "firstName": "Michael",
      "lastName": "HaslgrÌ_bler",
      "affiliations": []
    },
    {
      "id": 10169,
      "firstName": "Auk",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 22457,
      "firstName": "Shingo",
      "lastName": "Tsukada",
      "affiliations": []
    },
    {
      "id": 22460,
      "firstName": "Satu",
      "lastName": "Tamminen",
      "affiliations": []
    },
    {
      "id": 22461,
      "firstName": "Xiaolei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 14270,
      "firstName": "Dandan",
      "lastName": "Lyu",
      "affiliations": []
    },
    {
      "id": 18371,
      "firstName": "Kenzy Michelle Wasseem",
      "lastName": "Mina",
      "affiliations": []
    },
    {
      "id": 14279,
      "firstName": "Shweta",
      "lastName": "Ware",
      "affiliations": []
    },
    {
      "id": 14280,
      "firstName": "Lucy Van",
      "lastName": "Kleunen",
      "affiliations": []
    },
    {
      "id": 22473,
      "firstName": "BjÌ¦rn",
      "lastName": "Friedrich",
      "affiliations": []
    },
    {
      "id": 10187,
      "firstName": "Harideep",
      "lastName": "Nair",
      "affiliations": []
    },
    {
      "id": 18379,
      "firstName": "Tomohiro",
      "lastName": "Aoki",
      "affiliations": []
    },
    {
      "id": 22477,
      "firstName": "Soha",
      "lastName": "Rostaminia",
      "affiliations": []
    },
    {
      "id": 18385,
      "firstName": "Manideepa",
      "lastName": "Mukherjee",
      "affiliations": []
    },
    {
      "id": 10194,
      "firstName": "Timothy",
      "lastName": "Euken",
      "affiliations": []
    },
    {
      "id": 14291,
      "firstName": "Katarzyna",
      "lastName": "Wac",
      "affiliations": []
    },
    {
      "id": 18405,
      "firstName": "Marc Van Den",
      "lastName": "Broeck",
      "affiliations": []
    },
    {
      "id": 22504,
      "firstName": "Yubo",
      "lastName": "Yan",
      "affiliations": []
    },
    {
      "id": 14316,
      "firstName": "Pheng-Ann",
      "lastName": "Heng",
      "affiliations": []
    },
    {
      "id": 10222,
      "firstName": "Takaki",
      "lastName": "Maeda",
      "affiliations": []
    },
    {
      "id": 10224,
      "firstName": "Niels van",
      "lastName": "Berkel",
      "affiliations": []
    },
    {
      "id": 10225,
      "firstName": "Lokmane",
      "lastName": "Krizou",
      "affiliations": []
    },
    {
      "id": 18417,
      "firstName": "Nabila",
      "lastName": "Khan",
      "affiliations": []
    },
    {
      "id": 22516,
      "firstName": "Ling",
      "lastName": "Xiao",
      "affiliations": []
    },
    {
      "id": 14326,
      "firstName": "Dingding",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 22519,
      "firstName": "Xu",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 10232,
      "firstName": "Damith C.",
      "lastName": "Ranasinghe",
      "affiliations": []
    },
    {
      "id": 10233,
      "firstName": "Razvan",
      "lastName": "Bunescu",
      "affiliations": []
    },
    {
      "id": 18425,
      "firstName": "Atsuyuki",
      "lastName": "Morishima",
      "affiliations": []
    },
    {
      "id": 14330,
      "firstName": "Yu",
      "lastName": "Guan",
      "affiliations": []
    },
    {
      "id": 10235,
      "firstName": "Xiaopeng",
      "lastName": "Niu",
      "affiliations": []
    },
    {
      "id": 14332,
      "firstName": "Takuto",
      "lastName": "Hayashi",
      "affiliations": []
    },
    {
      "id": 22528,
      "firstName": "Yongzhong",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 22529,
      "firstName": "Yu-Tai",
      "lastName": "Ching",
      "affiliations": []
    },
    {
      "id": 10245,
      "firstName": "Ankur",
      "lastName": "Sarker",
      "affiliations": []
    },
    {
      "id": 18440,
      "firstName": "Raina",
      "lastName": "Langevin",
      "affiliations": []
    },
    {
      "id": 22538,
      "firstName": "Oscar",
      "lastName": "Tomico",
      "affiliations": []
    },
    {
      "id": 14351,
      "firstName": "Longbing",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 22548,
      "firstName": "Jeremy",
      "lastName": "Gummeson",
      "affiliations": []
    },
    {
      "id": 22552,
      "firstName": "Jung-Mi",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 10265,
      "firstName": "David",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 18462,
      "firstName": "Yanjun",
      "lastName": "Qin",
      "affiliations": []
    },
    {
      "id": 22560,
      "firstName": "Masanori",
      "lastName": "Sugimoto",
      "affiliations": []
    },
    {
      "id": 18466,
      "firstName": "Usman",
      "lastName": "Naeem",
      "affiliations": []
    },
    {
      "id": 18468,
      "firstName": "Alia",
      "lastName": "Crum",
      "affiliations": []
    },
    {
      "id": 14376,
      "firstName": "Tong",
      "lastName": "Guo",
      "affiliations": []
    },
    {
      "id": 22572,
      "firstName": "Esther W",
      "lastName": "Foo",
      "affiliations": []
    },
    {
      "id": 14380,
      "firstName": "Xiangmin",
      "lastName": "Fan",
      "affiliations": []
    },
    {
      "id": 18477,
      "firstName": "Hao-Chuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 22573,
      "firstName": "Jidong",
      "lastName": "Ge",
      "affiliations": []
    },
    {
      "id": 22582,
      "firstName": "Franceska",
      "lastName": "Xhakaj",
      "affiliations": []
    },
    {
      "id": 22585,
      "firstName": "Zhichao",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 22586,
      "firstName": "Hermie",
      "lastName": "Hermens",
      "affiliations": []
    },
    {
      "id": 22587,
      "firstName": "Hironobu",
      "lastName": "Kanno",
      "affiliations": []
    },
    {
      "id": 18496,
      "firstName": "Andreas",
      "lastName": "Dengel",
      "affiliations": []
    },
    {
      "id": 22592,
      "firstName": "Akira",
      "lastName": "Tsujimoto",
      "affiliations": []
    },
    {
      "id": 10306,
      "firstName": "Skanda",
      "lastName": "Muralidhar",
      "affiliations": []
    },
    {
      "id": 10307,
      "firstName": "Chen",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 14404,
      "firstName": "Daniela",
      "lastName": "Rosner",
      "affiliations": []
    },
    {
      "id": 18502,
      "firstName": "Tuo",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 22607,
      "firstName": "Mohammad",
      "lastName": "Nabati",
      "affiliations": []
    },
    {
      "id": 22611,
      "firstName": "Ye",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 18515,
      "firstName": "Sheng",
      "lastName": "Tan",
      "affiliations": []
    },
    {
      "id": 22612,
      "firstName": "Kyosuke",
      "lastName": "Futami",
      "affiliations": []
    },
    {
      "id": 18516,
      "firstName": "Chuanyi",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 10329,
      "firstName": "Hongshan",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 14425,
      "firstName": "Thananut",
      "lastName": "Phiboonbanakit",
      "affiliations": []
    },
    {
      "id": 14431,
      "firstName": "Li",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 14432,
      "firstName": "Weiwei",
      "lastName": "Jiang",
      "affiliations": []
    },
    {
      "id": 18532,
      "firstName": "Johan",
      "lastName": "Lukkien",
      "affiliations": []
    },
    {
      "id": 14438,
      "firstName": "A. B. M. Alim Al",
      "lastName": "Islam",
      "affiliations": []
    },
    {
      "id": 18535,
      "firstName": "Tanzeem",
      "lastName": "Choudhury",
      "affiliations": []
    },
    {
      "id": 22632,
      "firstName": "Jun",
      "lastName": "Inoue",
      "affiliations": []
    },
    {
      "id": 18537,
      "firstName": "AN",
      "lastName": "LIANG",
      "affiliations": []
    },
    {
      "id": 14441,
      "firstName": "Jittrapol",
      "lastName": "Intarasirisawat",
      "affiliations": []
    },
    {
      "id": 14446,
      "firstName": "Bhanu Teja",
      "lastName": "Gullapalli",
      "affiliations": []
    },
    {
      "id": 22638,
      "firstName": "Ge",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 10354,
      "firstName": "Shkurta",
      "lastName": "Gashi",
      "affiliations": []
    },
    {
      "id": 10359,
      "firstName": "Sara",
      "lastName": "Zannone",
      "affiliations": []
    },
    {
      "id": 10366,
      "firstName": "Franceli L.",
      "lastName": "Cibrian",
      "affiliations": []
    },
    {
      "id": 14463,
      "firstName": "Willem Marco",
      "lastName": "Beltman",
      "affiliations": []
    },
    {
      "id": 14464,
      "firstName": "Chun",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 14467,
      "firstName": "Gabriele",
      "lastName": "Marini",
      "affiliations": []
    },
    {
      "id": 14468,
      "firstName": "Nic",
      "lastName": "Volanschi",
      "affiliations": []
    },
    {
      "id": 22666,
      "firstName": "Marta",
      "lastName": "Cortes",
      "affiliations": []
    },
    {
      "id": 10387,
      "firstName": "Osama Bin",
      "lastName": "Tariq",
      "affiliations": []
    },
    {
      "id": 10392,
      "firstName": "Michelle",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 14488,
      "firstName": "Héctor A. Cordourier",
      "lastName": "Maruri",
      "affiliations": []
    },
    {
      "id": 18585,
      "firstName": "Zhaopeng",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 14489,
      "firstName": "Xuewei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18589,
      "firstName": "Geoff",
      "lastName": "Jarrad",
      "affiliations": []
    },
    {
      "id": 18590,
      "firstName": "Ateendra",
      "lastName": "Ramesh",
      "affiliations": []
    },
    {
      "id": 10400,
      "firstName": "Gurleen",
      "lastName": "Kaur",
      "affiliations": []
    },
    {
      "id": 14497,
      "firstName": "Andrea",
      "lastName": "Continella",
      "affiliations": []
    },
    {
      "id": 10401,
      "firstName": "Yang",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 18594,
      "firstName": "Sougata",
      "lastName": "Sen",
      "affiliations": []
    },
    {
      "id": 18597,
      "firstName": "Maximilian",
      "lastName": "Uphoff",
      "affiliations": []
    },
    {
      "id": 14502,
      "firstName": "Kyoichi",
      "lastName": "Takaori",
      "affiliations": []
    },
    {
      "id": 10407,
      "firstName": "Minxuan",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 14504,
      "firstName": "Koichi",
      "lastName": "Kise",
      "affiliations": []
    },
    {
      "id": 14507,
      "firstName": "Guang",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 22700,
      "firstName": "Carlo Maria De",
      "lastName": "Masi",
      "affiliations": []
    },
    {
      "id": 14512,
      "firstName": "Daniel",
      "lastName": "Strawn",
      "affiliations": []
    },
    {
      "id": 18609,
      "firstName": "Duyeon",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 14517,
      "firstName": "Wei",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 18617,
      "firstName": "Peerawit",
      "lastName": "Naprae",
      "affiliations": []
    },
    {
      "id": 14522,
      "firstName": "Jiahui",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 18619,
      "firstName": "Namrata",
      "lastName": "Srivastava",
      "affiliations": []
    },
    {
      "id": 10432,
      "firstName": "Dairoku",
      "lastName": "Muramatsu",
      "affiliations": []
    },
    {
      "id": 18626,
      "firstName": "Susan",
      "lastName": "Murphy",
      "affiliations": []
    },
    {
      "id": 18632,
      "firstName": "Hanyu",
      "lastName": "Long",
      "affiliations": []
    },
    {
      "id": 10442,
      "firstName": "Samuel",
      "lastName": "Miller",
      "affiliations": []
    },
    {
      "id": 22733,
      "firstName": "Jens",
      "lastName": "Le",
      "affiliations": []
    },
    {
      "id": 22739,
      "firstName": "Zhongqin",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 22743,
      "firstName": "Kadri Bugra",
      "lastName": "Ozutemiz",
      "affiliations": []
    },
    {
      "id": 14552,
      "firstName": "Chihiro",
      "lastName": "Ota",
      "affiliations": []
    },
    {
      "id": 18650,
      "firstName": "Vanessa",
      "lastName": "Frias-Martinez",
      "affiliations": []
    },
    {
      "id": 10459,
      "firstName": "Michail",
      "lastName": "Giannakos",
      "affiliations": []
    },
    {
      "id": 10460,
      "firstName": "Jayesh",
      "lastName": "Kamath",
      "affiliations": []
    },
    {
      "id": 22756,
      "firstName": "Haruka",
      "lastName": "Nakagawa",
      "affiliations": []
    },
    {
      "id": 10469,
      "firstName": "Taesik",
      "lastName": "Gong",
      "affiliations": []
    },
    {
      "id": 10470,
      "firstName": "Takuto",
      "lastName": "Usami",
      "affiliations": []
    },
    {
      "id": 14566,
      "firstName": "Qiaosi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 10471,
      "firstName": "Raphael",
      "lastName": "Carvalho",
      "affiliations": []
    },
    {
      "id": 18666,
      "firstName": "Paul",
      "lastName": "Lukowicz",
      "affiliations": []
    },
    {
      "id": 10477,
      "firstName": "Takashi",
      "lastName": "Amesaka",
      "affiliations": []
    },
    {
      "id": 22767,
      "firstName": "Chang",
      "lastName": "Tian",
      "affiliations": []
    },
    {
      "id": 10479,
      "firstName": "Yi",
      "lastName": "Ouyang",
      "affiliations": []
    },
    {
      "id": 10481,
      "firstName": "Yuji",
      "lastName": "Uema",
      "affiliations": []
    },
    {
      "id": 22769,
      "firstName": "Robert",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 18674,
      "firstName": "Devin",
      "lastName": "Balkcom",
      "affiliations": []
    },
    {
      "id": 14579,
      "firstName": "Viseth",
      "lastName": "Sean",
      "affiliations": []
    },
    {
      "id": 14583,
      "firstName": "Saeed",
      "lastName": "Abdullah",
      "affiliations": []
    },
    {
      "id": 22775,
      "firstName": "Bowon",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 14585,
      "firstName": "Wendy",
      "lastName": "Ju",
      "affiliations": []
    },
    {
      "id": 18682,
      "firstName": "Yuki",
      "lastName": "Nishizawa",
      "affiliations": []
    },
    {
      "id": 18685,
      "firstName": "Yali",
      "lastName": "Fan",
      "affiliations": []
    },
    {
      "id": 14593,
      "firstName": "Elle",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 18691,
      "firstName": "Cosmin",
      "lastName": "Stamate",
      "affiliations": []
    },
    {
      "id": 14596,
      "firstName": "Alexandre De",
      "lastName": "Masi",
      "affiliations": []
    },
    {
      "id": 14599,
      "firstName": "Mengchun",
      "lastName": "Lv",
      "affiliations": []
    },
    {
      "id": 22793,
      "firstName": "Shane",
      "lastName": "Halloran",
      "affiliations": []
    },
    {
      "id": 10510,
      "firstName": "Vivek",
      "lastName": "Shetty",
      "affiliations": []
    },
    {
      "id": 22799,
      "firstName": "Md Atiqur Rahman",
      "lastName": "Ahad",
      "affiliations": []
    },
    {
      "id": 18704,
      "firstName": "Michaela",
      "lastName": "Murauer",
      "affiliations": []
    },
    {
      "id": 14609,
      "firstName": "Yang",
      "lastName": "Peicheng",
      "affiliations": []
    },
    {
      "id": 10515,
      "firstName": "Elena Marquez",
      "lastName": "Segura",
      "affiliations": []
    },
    {
      "id": 10518,
      "firstName": "Hsin-Liu (Cindy)",
      "lastName": "Kao",
      "affiliations": []
    },
    {
      "id": 22806,
      "firstName": "Neille-Ann H.",
      "lastName": "Tan",
      "affiliations": []
    },
    {
      "id": 10519,
      "firstName": "Adrian",
      "lastName": "Friday",
      "affiliations": []
    },
    {
      "id": 22810,
      "firstName": "Irvin Steve",
      "lastName": "Cardenas",
      "affiliations": []
    },
    {
      "id": 10523,
      "firstName": "Chris",
      "lastName": "Norval",
      "affiliations": []
    },
    {
      "id": 14623,
      "firstName": "Alejandro Baucells",
      "lastName": "Costa",
      "affiliations": []
    },
    {
      "id": 22819,
      "firstName": "Anam Ahmad",
      "lastName": "Khan",
      "affiliations": []
    },
    {
      "id": 14627,
      "firstName": "Rahul C.",
      "lastName": "Shah",
      "affiliations": []
    },
    {
      "id": 14628,
      "firstName": "Zijie",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 22821,
      "firstName": "Lin",
      "lastName": "Feng",
      "affiliations": []
    },
    {
      "id": 10534,
      "firstName": "Scott E.",
      "lastName": "Hudson",
      "affiliations": []
    },
    {
      "id": 18733,
      "firstName": "Audrey",
      "lastName": "Briot",
      "affiliations": []
    },
    {
      "id": 10544,
      "firstName": "Srinivasan",
      "lastName": "R",
      "affiliations": []
    },
    {
      "id": 14644,
      "firstName": "Audrey",
      "lastName": "Girouard",
      "affiliations": []
    },
    {
      "id": 14653,
      "firstName": "Mikolaj",
      "lastName": "Wozniak",
      "affiliations": []
    },
    {
      "id": 22845,
      "firstName": "Richard",
      "lastName": "Dobson",
      "affiliations": []
    },
    {
      "id": 18751,
      "firstName": "Muhammad Awais",
      "lastName": "Azam",
      "affiliations": []
    },
    {
      "id": 22853,
      "firstName": "Mahir",
      "lastName": "Mahbub",
      "affiliations": []
    },
    {
      "id": 14664,
      "firstName": "Aakaash",
      "lastName": "Kapoor",
      "affiliations": []
    },
    {
      "id": 22860,
      "firstName": "Uichin",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 10577,
      "firstName": "Terence S.",
      "lastName": "Leung",
      "affiliations": []
    },
    {
      "id": 14676,
      "firstName": "Kaori",
      "lastName": "Fujinami",
      "affiliations": []
    },
    {
      "id": 22870,
      "firstName": "Mckensey",
      "lastName": "Johnson",
      "affiliations": []
    },
    {
      "id": 14679,
      "firstName": "Yan",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 10584,
      "firstName": "Tsubasa",
      "lastName": "Yumura",
      "affiliations": []
    },
    {
      "id": 22872,
      "firstName": "Joshua",
      "lastName": "Edelmann",
      "affiliations": []
    },
    {
      "id": 18777,
      "firstName": "Meng",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 18778,
      "firstName": "Benedikt",
      "lastName": "Gollan",
      "affiliations": []
    },
    {
      "id": 18786,
      "firstName": "Ruiyang",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 18787,
      "firstName": "Joel E.",
      "lastName": "Fischer",
      "affiliations": []
    },
    {
      "id": 14692,
      "firstName": "Zeshui",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 18793,
      "firstName": "Afsaneh",
      "lastName": "Doryab",
      "affiliations": []
    },
    {
      "id": 14698,
      "firstName": "Weiyan",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 18802,
      "firstName": "Vanessa",
      "lastName": "Evers",
      "affiliations": []
    },
    {
      "id": 14708,
      "firstName": "Oscar",
      "lastName": "Peters",
      "affiliations": []
    },
    {
      "id": 22900,
      "firstName": "Muhammad",
      "lastName": "Muaaz",
      "affiliations": []
    },
    {
      "id": 22901,
      "firstName": "Yutaka",
      "lastName": "Arakawa",
      "affiliations": []
    },
    {
      "id": 14713,
      "firstName": "Vedant Das",
      "lastName": "Swain",
      "affiliations": []
    },
    {
      "id": 22907,
      "firstName": "Xiao",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 10619,
      "firstName": "Mingqian",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 14715,
      "firstName": "Venkata N.",
      "lastName": "Padmanabhan",
      "affiliations": []
    },
    {
      "id": 22911,
      "firstName": "Sai Sourabh",
      "lastName": "Tiruvaipati",
      "affiliations": []
    },
    {
      "id": 10623,
      "firstName": "Alexandra",
      "lastName": "Voit",
      "affiliations": []
    },
    {
      "id": 18818,
      "firstName": "Thomas",
      "lastName": "Ploetz",
      "affiliations": []
    },
    {
      "id": 22915,
      "firstName": "Youn-Kyung",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 14723,
      "firstName": "Taylor",
      "lastName": "Myers",
      "affiliations": []
    },
    {
      "id": 10629,
      "firstName": "Michiel de",
      "lastName": "Jong",
      "affiliations": []
    },
    {
      "id": 10636,
      "firstName": "Shayan",
      "lastName": "Mirjafari",
      "affiliations": []
    },
    {
      "id": 22926,
      "firstName": "Mehnaz Tabassum",
      "lastName": "Mahin",
      "affiliations": []
    },
    {
      "id": 18833,
      "firstName": "James",
      "lastName": "Colley",
      "affiliations": []
    },
    {
      "id": 14738,
      "firstName": "Takuya",
      "lastName": "Sasatani",
      "affiliations": []
    },
    {
      "id": 14739,
      "firstName": "Kieran",
      "lastName": "Woodward",
      "affiliations": []
    },
    {
      "id": 18837,
      "firstName": "Vincent",
      "lastName": "Becker",
      "affiliations": []
    },
    {
      "id": 18839,
      "firstName": "Thomas",
      "lastName": "Olsson",
      "affiliations": []
    },
    {
      "id": 14746,
      "firstName": "Chih-Heng",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 18846,
      "firstName": "Qiaozhu",
      "lastName": "Mei",
      "affiliations": []
    },
    {
      "id": 14755,
      "firstName": "Li",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 14756,
      "firstName": "Phucanh",
      "lastName": "Tran",
      "affiliations": []
    },
    {
      "id": 22949,
      "firstName": "Maximilian",
      "lastName": "Leodolter",
      "affiliations": []
    },
    {
      "id": 22950,
      "firstName": "Ronal",
      "lastName": "Singh",
      "affiliations": []
    },
    {
      "id": 22952,
      "firstName": "Tianshi",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 14764,
      "firstName": "Maya",
      "lastName": "Malaya",
      "affiliations": []
    },
    {
      "id": 14765,
      "firstName": "Mohamed",
      "lastName": "Abdalla",
      "affiliations": []
    },
    {
      "id": 10670,
      "firstName": "Yuyu",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 18863,
      "firstName": "Jingyi",
      "lastName": "Qian",
      "affiliations": []
    },
    {
      "id": 22959,
      "firstName": "Shridatt",
      "lastName": "Sugrim",
      "affiliations": []
    },
    {
      "id": 10671,
      "firstName": "Maria",
      "lastName": "Pavlovskaia",
      "affiliations": []
    },
    {
      "id": 14768,
      "firstName": "Giovanna",
      "lastName": "Miritello",
      "affiliations": []
    },
    {
      "id": 22964,
      "firstName": "Tadashi",
      "lastName": "Okoshi",
      "affiliations": []
    },
    {
      "id": 22965,
      "firstName": "Dajian",
      "lastName": "Zeng",
      "affiliations": []
    },
    {
      "id": 14773,
      "firstName": "Christine",
      "lastName": "Keller",
      "affiliations": []
    },
    {
      "id": 22968,
      "firstName": "Paul",
      "lastName": "Davidsson",
      "affiliations": []
    },
    {
      "id": 14780,
      "firstName": "Haimo",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 22972,
      "firstName": "Zixing",
      "lastName": "Song",
      "affiliations": []
    },
    {
      "id": 22975,
      "firstName": "Oliver",
      "lastName": "Petter",
      "affiliations": []
    },
    {
      "id": 14784,
      "firstName": "Gino",
      "lastName": "Brunner",
      "affiliations": []
    },
    {
      "id": 22980,
      "firstName": "Haipeng",
      "lastName": "Dai",
      "affiliations": []
    },
    {
      "id": 14793,
      "firstName": "Sana Ali",
      "lastName": "Naqvi",
      "affiliations": []
    },
    {
      "id": 10700,
      "firstName": "Eiman",
      "lastName": "Kanjo",
      "affiliations": []
    },
    {
      "id": 18896,
      "firstName": "Miranda",
      "lastName": "Nixon",
      "affiliations": []
    },
    {
      "id": 18899,
      "firstName": "Stephen R.",
      "lastName": "Robinson",
      "affiliations": []
    },
    {
      "id": 23001,
      "firstName": "Xiaowei",
      "lastName": "Hu",
      "affiliations": []
    },
    {
      "id": 18907,
      "firstName": "Judith Simone",
      "lastName": "Heinisch",
      "affiliations": []
    },
    {
      "id": 23018,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "affiliations": []
    },
    {
      "id": 14827,
      "firstName": "Ole J.",
      "lastName": "Mengshoel",
      "affiliations": []
    },
    {
      "id": 10732,
      "firstName": "Ruibo",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 14836,
      "firstName": "Joan",
      "lastName": "Saez-Pons",
      "affiliations": []
    },
    {
      "id": 18933,
      "firstName": "Hui-Shyong",
      "lastName": "Yeo",
      "affiliations": []
    },
    {
      "id": 10744,
      "firstName": "Maximilian",
      "lastName": "Pinaroc",
      "affiliations": []
    },
    {
      "id": 18936,
      "firstName": "Philipp",
      "lastName": "Kapfer",
      "affiliations": []
    },
    {
      "id": 18937,
      "firstName": "Lei",
      "lastName": "Xie",
      "affiliations": []
    },
    {
      "id": 23035,
      "firstName": "Richard",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 23038,
      "firstName": "Liuyi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 10751,
      "firstName": "Azam",
      "lastName": "Khan",
      "affiliations": []
    },
    {
      "id": 14847,
      "firstName": "Chenhan",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 14849,
      "firstName": "John",
      "lastName": "Rogers",
      "affiliations": []
    },
    {
      "id": 10758,
      "firstName": "Niaz",
      "lastName": "Chowdhury",
      "affiliations": []
    },
    {
      "id": 18954,
      "firstName": "Matjaz",
      "lastName": "Gams",
      "affiliations": []
    },
    {
      "id": 14861,
      "firstName": "Zachary D.",
      "lastName": "King",
      "affiliations": []
    },
    {
      "id": 23054,
      "firstName": "Ebrahim",
      "lastName": "Nemati",
      "affiliations": []
    },
    {
      "id": 10768,
      "firstName": "Chien Wen (Tina)",
      "lastName": "Yuan",
      "affiliations": []
    },
    {
      "id": 10770,
      "firstName": "Ce",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 23059,
      "firstName": "Brad",
      "lastName": "Holschuh",
      "affiliations": []
    },
    {
      "id": 18968,
      "firstName": "Md Shafiqul",
      "lastName": "Islam",
      "affiliations": []
    },
    {
      "id": 23064,
      "firstName": "Qiushi",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 23065,
      "firstName": "Stanley",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 10779,
      "firstName": "Yatharth",
      "lastName": "Ranjan",
      "affiliations": []
    },
    {
      "id": 18972,
      "firstName": "Christos",
      "lastName": "Efstratiou",
      "affiliations": []
    },
    {
      "id": 10780,
      "firstName": "Jeff",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 14877,
      "firstName": "Sandra",
      "lastName": "Servia",
      "affiliations": []
    },
    {
      "id": 18974,
      "firstName": "Ercument",
      "lastName": "Gorgul",
      "affiliations": []
    },
    {
      "id": 10782,
      "firstName": "Zi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 10784,
      "firstName": "Eisaku",
      "lastName": "Maeda",
      "affiliations": []
    },
    {
      "id": 10785,
      "firstName": "Beihong",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 10786,
      "firstName": "Chuyu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 23079,
      "firstName": "Margarita",
      "lastName": "Benitez",
      "affiliations": []
    },
    {
      "id": 14887,
      "firstName": "Xia",
      "lastName": "Qingxin",
      "affiliations": []
    },
    {
      "id": 18987,
      "firstName": "S.-H Gary",
      "lastName": "Chan",
      "affiliations": []
    },
    {
      "id": 10799,
      "firstName": "Lauri",
      "lastName": "LÌ_ven",
      "affiliations": []
    },
    {
      "id": 14901,
      "firstName": "Anooshmita",
      "lastName": "Das",
      "affiliations": []
    },
    {
      "id": 18999,
      "firstName": "Shibo",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 23097,
      "firstName": "Aaron",
      "lastName": "Tabor",
      "affiliations": []
    },
    {
      "id": 19002,
      "firstName": "Abdur Rahim Mohammad",
      "lastName": "Forkan",
      "affiliations": []
    },
    {
      "id": 23099,
      "firstName": "Yi",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 19004,
      "firstName": "Haocong",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 23100,
      "firstName": "Zuohui",
      "lastName": "Fu",
      "affiliations": []
    },
    {
      "id": 10818,
      "firstName": "Hayoung",
      "lastName": "Jung",
      "affiliations": []
    },
    {
      "id": 23107,
      "firstName": "Sidney K.",
      "lastName": "D’Mello",
      "affiliations": []
    },
    {
      "id": 14916,
      "firstName": "Daniel",
      "lastName": "Roggen",
      "affiliations": []
    },
    {
      "id": 19013,
      "firstName": "Niranjan",
      "lastName": "Bidargaddi",
      "affiliations": []
    },
    {
      "id": 23110,
      "firstName": "Xueyi",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 10830,
      "firstName": "Rachel",
      "lastName": "Freire",
      "affiliations": []
    },
    {
      "id": 14926,
      "firstName": "Bernardo",
      "lastName": "Marques",
      "affiliations": []
    },
    {
      "id": 10835,
      "firstName": "Fengyuan",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 23137,
      "firstName": "Di Laura",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 19042,
      "firstName": "Junto",
      "lastName": "Nozaki",
      "affiliations": []
    },
    {
      "id": 23139,
      "firstName": "Hiroto",
      "lastName": "Asai",
      "affiliations": []
    },
    {
      "id": 14948,
      "firstName": "Hao",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 14949,
      "firstName": "Jan A.",
      "lastName": "Persson",
      "affiliations": []
    },
    {
      "id": 10858,
      "firstName": "Salla",
      "lastName": "RÌ¦nkÌ_",
      "affiliations": []
    },
    {
      "id": 19050,
      "firstName": "Sarah L.",
      "lastName": "Jenney",
      "affiliations": []
    },
    {
      "id": 19053,
      "firstName": "Niek",
      "lastName": "Zuidhof",
      "affiliations": []
    },
    {
      "id": 23150,
      "firstName": "Jilong",
      "lastName": "Kuang",
      "affiliations": []
    },
    {
      "id": 10862,
      "firstName": "Ifthakhar",
      "lastName": "Ahmed",
      "affiliations": []
    },
    {
      "id": 23152,
      "firstName": "Farshid Hassani",
      "lastName": "Bijarbooneh",
      "affiliations": []
    },
    {
      "id": 23156,
      "firstName": "Amin Ahsan",
      "lastName": "Ali",
      "affiliations": []
    },
    {
      "id": 23157,
      "firstName": "fanny",
      "lastName": "larradet",
      "affiliations": []
    },
    {
      "id": 19065,
      "firstName": "Yinan",
      "lastName": "Ji",
      "affiliations": []
    },
    {
      "id": 10877,
      "firstName": "Eirini",
      "lastName": "Sykianaki",
      "affiliations": []
    },
    {
      "id": 14974,
      "firstName": "Jianbo",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 14975,
      "firstName": "Tanzima",
      "lastName": "Hashem",
      "affiliations": []
    },
    {
      "id": 10881,
      "firstName": "Anushika",
      "lastName": "Verma",
      "affiliations": []
    },
    {
      "id": 23171,
      "firstName": "Yunhuai",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 10885,
      "firstName": "Guan-Lin",
      "lastName": "Chao",
      "affiliations": []
    },
    {
      "id": 19077,
      "firstName": "Mohamed",
      "lastName": "Khamis",
      "affiliations": []
    },
    {
      "id": 19078,
      "firstName": "Michi",
      "lastName": "Kanda",
      "affiliations": []
    },
    {
      "id": 10889,
      "firstName": "Markus",
      "lastName": "Löchtefeld",
      "affiliations": []
    },
    {
      "id": 19082,
      "firstName": "Md. Hasan",
      "lastName": "Tarek",
      "affiliations": []
    },
    {
      "id": 23179,
      "firstName": "Aleksander",
      "lastName": "Matic",
      "affiliations": []
    },
    {
      "id": 14987,
      "firstName": "Frank Wencheng",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 10891,
      "firstName": "Raymond Chi-Wing",
      "lastName": "Wong",
      "affiliations": []
    },
    {
      "id": 10892,
      "firstName": "Kasey G.",
      "lastName": "Creswell",
      "affiliations": []
    },
    {
      "id": 23180,
      "firstName": "Saiganesh",
      "lastName": "Swaminathan",
      "affiliations": []
    },
    {
      "id": 14989,
      "firstName": "Pei",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 10893,
      "firstName": "Dan",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 14991,
      "firstName": "Christoph",
      "lastName": "Heindl",
      "affiliations": []
    },
    {
      "id": 23184,
      "firstName": "Feng",
      "lastName": "Tian",
      "affiliations": []
    },
    {
      "id": 10896,
      "firstName": "Zohreh",
      "lastName": "Homayounfar",
      "affiliations": []
    },
    {
      "id": 23199,
      "firstName": "Yida",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 15010,
      "firstName": "Heidi",
      "lastName": "Knutsen",
      "affiliations": []
    },
    {
      "id": 10916,
      "firstName": "Santosh",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 19115,
      "firstName": "Yousef",
      "lastName": "Kowsar",
      "affiliations": []
    },
    {
      "id": 10924,
      "firstName": "Lin",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 23213,
      "firstName": "Bruno",
      "lastName": "Lepri",
      "affiliations": []
    },
    {
      "id": 23218,
      "firstName": "Hiroshi",
      "lastName": "Imamura",
      "affiliations": []
    },
    {
      "id": 19123,
      "firstName": "Ferran Altarriba",
      "lastName": "Bertran",
      "affiliations": []
    },
    {
      "id": 19124,
      "firstName": "Ming-Chyi",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 15029,
      "firstName": "Troy",
      "lastName": "Nachtigall",
      "affiliations": []
    },
    {
      "id": 23221,
      "firstName": "Lindsay W.",
      "lastName": "MacDonald",
      "affiliations": []
    },
    {
      "id": 19129,
      "firstName": "Hannu",
      "lastName": "Kinnunen",
      "affiliations": []
    },
    {
      "id": 19130,
      "firstName": "Maximilian",
      "lastName": "Henne",
      "affiliations": []
    },
    {
      "id": 10941,
      "firstName": "Xiang-Yang",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 15037,
      "firstName": "Eyal De",
      "lastName": "Lara",
      "affiliations": []
    },
    {
      "id": 23230,
      "firstName": "Kikuya",
      "lastName": "Miyamura",
      "affiliations": []
    },
    {
      "id": 23232,
      "firstName": "Kasthuri",
      "lastName": "Jayarajah",
      "affiliations": []
    },
    {
      "id": 15042,
      "firstName": "Roger",
      "lastName": "Wattenhofer",
      "affiliations": []
    },
    {
      "id": 19145,
      "firstName": "Ya-Fang",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 15053,
      "firstName": "Hristijan",
      "lastName": "Gjoreski",
      "affiliations": []
    },
    {
      "id": 23246,
      "firstName": "Sihan",
      "lastName": "Zeng",
      "affiliations": []
    },
    {
      "id": 15064,
      "firstName": "Arya",
      "lastName": "Tavakoulnia",
      "affiliations": []
    },
    {
      "id": 23259,
      "firstName": "Urte",
      "lastName": "Scholz",
      "affiliations": []
    },
    {
      "id": 23265,
      "firstName": "Mark",
      "lastName": "Newman",
      "affiliations": []
    },
    {
      "id": 19169,
      "firstName": "Anfu",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 23268,
      "firstName": "Tim",
      "lastName": "Althoff",
      "affiliations": []
    },
    {
      "id": 15080,
      "firstName": "Alexander",
      "lastName": "Tessier",
      "affiliations": []
    },
    {
      "id": 15083,
      "firstName": "Susumu",
      "lastName": "Sonoda",
      "affiliations": []
    },
    {
      "id": 23276,
      "firstName": "Chenshu",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 15085,
      "firstName": "Benett",
      "lastName": "Axtell",
      "affiliations": []
    },
    {
      "id": 23277,
      "firstName": "Franklin Mingzhe",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 15086,
      "firstName": "Linus",
      "lastName": "Fessler",
      "affiliations": []
    },
    {
      "id": 15087,
      "firstName": "Flora D.",
      "lastName": "Salim",
      "affiliations": []
    },
    {
      "id": 10994,
      "firstName": "Lianne",
      "lastName": "Toussaint",
      "affiliations": []
    },
    {
      "id": 23283,
      "firstName": "Luis Edgardo",
      "lastName": "Fraguada",
      "affiliations": []
    },
    {
      "id": 23288,
      "firstName": "Hongbo",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 15097,
      "firstName": "Mingxuan",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 15102,
      "firstName": "Masatoshi",
      "lastName": "Kimoto",
      "affiliations": []
    },
    {
      "id": 23300,
      "firstName": "Quoc Hung",
      "lastName": "Dang",
      "affiliations": []
    },
    {
      "id": 11012,
      "firstName": "Julie M.",
      "lastName": "Gregg",
      "affiliations": []
    },
    {
      "id": 11020,
      "firstName": "Suranga",
      "lastName": "Seneviratne",
      "affiliations": []
    },
    {
      "id": 11026,
      "firstName": "Mami",
      "lastName": "Kosaka",
      "affiliations": []
    },
    {
      "id": 19221,
      "firstName": "Ekin",
      "lastName": "Gedik",
      "affiliations": []
    },
    {
      "id": 23320,
      "firstName": "Andreas",
      "lastName": "Pichler",
      "affiliations": []
    },
    {
      "id": 15130,
      "firstName": "Hyoyoung",
      "lastName": "Lim",
      "affiliations": []
    },
    {
      "id": 11035,
      "firstName": "Marcel",
      "lastName": "Breitenfellner",
      "affiliations": []
    },
    {
      "id": 19229,
      "firstName": "Koustuv",
      "lastName": "Saha",
      "affiliations": []
    },
    {
      "id": 11038,
      "firstName": "Somaya Ben",
      "lastName": "Allouch",
      "affiliations": []
    },
    {
      "id": 19231,
      "firstName": "Anna",
      "lastName": "Wojciechowska",
      "affiliations": []
    },
    {
      "id": 11044,
      "firstName": "Shwetak",
      "lastName": "Patel",
      "affiliations": []
    },
    {
      "id": 15140,
      "firstName": "Lothar",
      "lastName": "Thiele",
      "affiliations": []
    },
    {
      "id": 19237,
      "firstName": "Chao",
      "lastName": "Cai",
      "affiliations": []
    },
    {
      "id": 19239,
      "firstName": "Kirill",
      "lastName": "Ragozin",
      "affiliations": []
    },
    {
      "id": 15143,
      "firstName": "Pengfei",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 23336,
      "firstName": "Laura",
      "lastName": "Dabbish",
      "affiliations": []
    },
    {
      "id": 15153,
      "firstName": "Murao",
      "lastName": "Kazuya",
      "affiliations": []
    },
    {
      "id": 23347,
      "firstName": "Shalini",
      "lastName": "Mukhopadhyay",
      "affiliations": []
    },
    {
      "id": 15155,
      "firstName": "Michael J.",
      "lastName": "Tumminia",
      "affiliations": []
    },
    {
      "id": 15163,
      "firstName": "Ian",
      "lastName": "Oakley",
      "affiliations": []
    },
    {
      "id": 23364,
      "firstName": "Panurat",
      "lastName": "Sutigoolabud",
      "affiliations": []
    },
    {
      "id": 11079,
      "firstName": "Sherine Ashraf",
      "lastName": "Safwat",
      "affiliations": []
    },
    {
      "id": 11081,
      "firstName": "Vojkan",
      "lastName": "Mihajlović",
      "affiliations": []
    },
    {
      "id": 11082,
      "firstName": "Hongmin",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 15179,
      "firstName": "Wenping",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 15184,
      "firstName": "Ge",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 15186,
      "firstName": "Aiko",
      "lastName": "Matsumori",
      "affiliations": []
    },
    {
      "id": 15187,
      "firstName": "Denzil",
      "lastName": "Ferreira",
      "affiliations": []
    },
    {
      "id": 11092,
      "firstName": "Yair",
      "lastName": "Amichai-Hamburger",
      "affiliations": []
    },
    {
      "id": 11094,
      "firstName": "Chen",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 19287,
      "firstName": "Kun",
      "lastName": "Qian",
      "affiliations": []
    },
    {
      "id": 11096,
      "firstName": "Adam",
      "lastName": "Nowak",
      "affiliations": []
    },
    {
      "id": 11098,
      "firstName": "Jessica",
      "lastName": "Baggaley",
      "affiliations": []
    },
    {
      "id": 23391,
      "firstName": "Ashley",
      "lastName": "Colley",
      "affiliations": []
    },
    {
      "id": 19295,
      "firstName": "Takuro",
      "lastName": "Yonezawa",
      "affiliations": []
    },
    {
      "id": 19296,
      "firstName": "Wonkyu",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 11106,
      "firstName": "Oskar",
      "lastName": "Juhlin",
      "affiliations": []
    },
    {
      "id": 23395,
      "firstName": "Elena Di",
      "lastName": "Lascio",
      "affiliations": []
    },
    {
      "id": 23396,
      "firstName": "Denis",
      "lastName": "Lalanne",
      "affiliations": []
    },
    {
      "id": 15205,
      "firstName": "Jason",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 11109,
      "firstName": "Jathushan",
      "lastName": "Rajasegaran",
      "affiliations": []
    },
    {
      "id": 11112,
      "firstName": "Olivier",
      "lastName": "Augereau",
      "affiliations": []
    },
    {
      "id": 23402,
      "firstName": "Vinay",
      "lastName": "Khare",
      "affiliations": []
    },
    {
      "id": 19308,
      "firstName": "Yong Seung",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 15212,
      "firstName": "Takuya",
      "lastName": "Nojima",
      "affiliations": []
    },
    {
      "id": 23405,
      "firstName": "Jorge",
      "lastName": "Ribeiro",
      "affiliations": []
    },
    {
      "id": 19310,
      "firstName": "Kati",
      "lastName": "Pettersson",
      "affiliations": []
    },
    {
      "id": 15220,
      "firstName": "Xia",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 23415,
      "firstName": "Xuewen",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 23416,
      "firstName": "Leonardo Garcia",
      "lastName": "Garcia",
      "affiliations": []
    },
    {
      "id": 23417,
      "firstName": "Zheng",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 23418,
      "firstName": "Wei",
      "lastName": "Shao",
      "affiliations": []
    },
    {
      "id": 19324,
      "firstName": "Zarin Rezwana Ridita",
      "lastName": "Haque",
      "affiliations": []
    },
    {
      "id": 19326,
      "firstName": "Jamy",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 23422,
      "firstName": "Jessica",
      "lastName": "Schroeder",
      "affiliations": []
    },
    {
      "id": 11136,
      "firstName": "Rhea",
      "lastName": "Chatterjee",
      "affiliations": []
    },
    {
      "id": 11137,
      "firstName": "Sandra",
      "lastName": "Ohly",
      "affiliations": []
    },
    {
      "id": 15234,
      "firstName": "Connor",
      "lastName": "Wilson",
      "affiliations": []
    },
    {
      "id": 23428,
      "firstName": "Marla",
      "lastName": "Narazani",
      "affiliations": []
    },
    {
      "id": 23429,
      "firstName": "Shouwei",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 15237,
      "firstName": "Janine",
      "lastName": "M.Dutcher",
      "affiliations": []
    },
    {
      "id": 23430,
      "firstName": "Harish",
      "lastName": "Haresamudram",
      "affiliations": []
    },
    {
      "id": 15241,
      "firstName": "Tao",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 15243,
      "firstName": "Daniel",
      "lastName": "Villatoro",
      "affiliations": []
    },
    {
      "id": 23435,
      "firstName": "Hannah",
      "lastName": "Murphy",
      "affiliations": []
    },
    {
      "id": 23436,
      "firstName": "Athanasios",
      "lastName": "Bamis",
      "affiliations": []
    },
    {
      "id": 11148,
      "firstName": "Zitao",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 15246,
      "firstName": "Masami",
      "lastName": "Takahashi",
      "affiliations": []
    },
    {
      "id": 19342,
      "firstName": "Yuta",
      "lastName": "Hayakawa",
      "affiliations": []
    },
    {
      "id": 19343,
      "firstName": "Junjun",
      "lastName": "Fan",
      "affiliations": []
    },
    {
      "id": 11154,
      "firstName": "Yang",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 19348,
      "firstName": "Guoliang",
      "lastName": "Xing",
      "affiliations": []
    },
    {
      "id": 15258,
      "firstName": "Florian",
      "lastName": "Evéquoz",
      "affiliations": []
    },
    {
      "id": 19356,
      "firstName": "Wei",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 19360,
      "firstName": "Aurélien",
      "lastName": "Tabard",
      "affiliations": []
    },
    {
      "id": 19364,
      "firstName": "Botao",
      "lastName": "Hao",
      "affiliations": []
    },
    {
      "id": 19365,
      "firstName": "Suwen",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 23464,
      "firstName": "Jean-Baptiste",
      "lastName": "Biernacki",
      "affiliations": []
    },
    {
      "id": 15275,
      "firstName": "JIARUI",
      "lastName": "ZU",
      "affiliations": []
    },
    {
      "id": 23469,
      "firstName": "Pegah",
      "lastName": "Abed-Esfahani",
      "affiliations": []
    },
    {
      "id": 15278,
      "firstName": "Sha",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 15279,
      "firstName": "Jong-Hoon",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 15280,
      "firstName": "Hiromasa",
      "lastName": "Funato",
      "affiliations": []
    },
    {
      "id": 23472,
      "firstName": "Reza",
      "lastName": "Shahbazian",
      "affiliations": []
    },
    {
      "id": 15281,
      "firstName": "Mingshi",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 19379,
      "firstName": "Frank",
      "lastName": "Vetere",
      "affiliations": []
    },
    {
      "id": 11189,
      "firstName": "Vahideh",
      "lastName": "Moghtadaiee",
      "affiliations": []
    },
    {
      "id": 15288,
      "firstName": "Mariam",
      "lastName": "Tolba",
      "affiliations": []
    },
    {
      "id": 15289,
      "firstName": "Yuanbo",
      "lastName": "Xiangli",
      "affiliations": []
    },
    {
      "id": 23481,
      "firstName": "Ghufran",
      "lastName": "Baig",
      "affiliations": []
    },
    {
      "id": 23486,
      "firstName": "Alois",
      "lastName": "Ferscha",
      "affiliations": []
    },
    {
      "id": 19390,
      "firstName": "Edison",
      "lastName": "Thomaz",
      "affiliations": []
    },
    {
      "id": 11200,
      "firstName": "Vassilis",
      "lastName": "Kostakos",
      "affiliations": []
    },
    {
      "id": 23488,
      "firstName": "Mingming",
      "lastName": "Fan",
      "affiliations": []
    },
    {
      "id": 19395,
      "firstName": "Chen",
      "lastName": "Qian",
      "affiliations": []
    },
    {
      "id": 19399,
      "firstName": "Takanori",
      "lastName": "Maruichi",
      "affiliations": []
    },
    {
      "id": 23504,
      "firstName": "Valentin",
      "lastName": "Lachand",
      "affiliations": []
    },
    {
      "id": 23512,
      "firstName": "Prabhakaran",
      "lastName": "Santhanam",
      "affiliations": []
    },
    {
      "id": 11226,
      "firstName": "Christian",
      "lastName": "Meurisch",
      "affiliations": []
    },
    {
      "id": 19422,
      "firstName": "Yoshinari",
      "lastName": "Shirai",
      "affiliations": []
    },
    {
      "id": 19427,
      "firstName": "Sencun",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 11236,
      "firstName": "Kimberley D.",
      "lastName": "Lakes",
      "affiliations": []
    },
    {
      "id": 23525,
      "firstName": "Jorge",
      "lastName": "Goncalves",
      "affiliations": []
    },
    {
      "id": 11237,
      "firstName": "Zelin",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 11243,
      "firstName": "Pauline",
      "lastName": "Conde",
      "affiliations": []
    },
    {
      "id": 15342,
      "firstName": "Xiangyu",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 15348,
      "firstName": "Ali",
      "lastName": "Kiaghadi",
      "affiliations": []
    },
    {
      "id": 23544,
      "firstName": "Changhao",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 15354,
      "firstName": "Hollis",
      "lastName": "Pass",
      "affiliations": []
    },
    {
      "id": 19451,
      "firstName": "Hyosun",
      "lastName": "Kwon",
      "affiliations": []
    },
    {
      "id": 11263,
      "firstName": "Tianqi",
      "lastName": "Xie",
      "affiliations": []
    },
    {
      "id": 19456,
      "firstName": "SiÌ¢n",
      "lastName": "Lindley",
      "affiliations": []
    },
    {
      "id": 15369,
      "firstName": "Lijuan",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 23561,
      "firstName": "Ruolin",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 23562,
      "firstName": "Loe",
      "lastName": "Feijs",
      "affiliations": []
    },
    {
      "id": 11277,
      "firstName": "David Labbe and Nathalie",
      "lastName": "Martin",
      "affiliations": []
    },
    {
      "id": 19471,
      "firstName": "Masayuki",
      "lastName": "Orihara",
      "affiliations": []
    },
    {
      "id": 19474,
      "firstName": "Rain",
      "lastName": "Ashford",
      "affiliations": []
    },
    {
      "id": 11283,
      "firstName": "Sheldon",
      "lastName": "Cohen",
      "affiliations": []
    },
    {
      "id": 15380,
      "firstName": "Joel",
      "lastName": "Holton",
      "affiliations": []
    },
    {
      "id": 11285,
      "firstName": "Peter",
      "lastName": "Widhalm",
      "affiliations": []
    },
    {
      "id": 19481,
      "firstName": "Claudio",
      "lastName": "Forlivesi",
      "affiliations": []
    },
    {
      "id": 11293,
      "firstName": "Youngki",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 23582,
      "firstName": "Zhanpeng",
      "lastName": "Jin",
      "affiliations": []
    },
    {
      "id": 23587,
      "firstName": "Fahim",
      "lastName": "Kawsar",
      "affiliations": []
    },
    {
      "id": 23588,
      "firstName": "Vera",
      "lastName": "Lehmann",
      "affiliations": []
    },
    {
      "id": 15400,
      "firstName": "Pablo",
      "lastName": "Castillo",
      "affiliations": []
    },
    {
      "id": 19497,
      "firstName": "Dr K M",
      "lastName": "Mandana",
      "affiliations": []
    },
    {
      "id": 11306,
      "firstName": "Ian",
      "lastName": "Johnson",
      "affiliations": []
    },
    {
      "id": 19510,
      "firstName": "Nazmus Sakib",
      "lastName": "Patwary",
      "affiliations": []
    },
    {
      "id": 23609,
      "firstName": "Xiaojun",
      "lastName": "Hei",
      "affiliations": []
    },
    {
      "id": 23610,
      "firstName": "Yoshihide",
      "lastName": "Sekimoto",
      "affiliations": []
    },
    {
      "id": 11325,
      "firstName": "Fannie",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 15426,
      "firstName": "Chouchang (Jack)",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 23620,
      "firstName": "Xianglong",
      "lastName": "Feng",
      "affiliations": []
    },
    {
      "id": 11337,
      "firstName": "Felix",
      "lastName": "Wortmann",
      "affiliations": []
    },
    {
      "id": 19531,
      "firstName": "Ning",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 15436,
      "firstName": "Yuting",
      "lastName": "Zhan",
      "affiliations": []
    },
    {
      "id": 23632,
      "firstName": "Feng",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 23634,
      "firstName": "Chihiro",
      "lastName": "Ito",
      "affiliations": []
    },
    {
      "id": 15446,
      "firstName": "Yasuo",
      "lastName": "Namioka",
      "affiliations": []
    },
    {
      "id": 23639,
      "firstName": "Yaliang",
      "lastName": "Chuang",
      "affiliations": []
    },
    {
      "id": 19545,
      "firstName": "Sanglu",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 11353,
      "firstName": "Gifford",
      "lastName": "Cheung",
      "affiliations": []
    },
    {
      "id": 19549,
      "firstName": "Kaifeng",
      "lastName": "Jiang",
      "affiliations": []
    },
    {
      "id": 11360,
      "firstName": "Katsuhiko",
      "lastName": "Kaji",
      "affiliations": []
    },
    {
      "id": 19553,
      "firstName": "John C.",
      "lastName": "Krumm",
      "affiliations": []
    },
    {
      "id": 11363,
      "firstName": "Krzysztof",
      "lastName": "Grudzien",
      "affiliations": []
    },
    {
      "id": 15462,
      "firstName": "Murtuza",
      "lastName": "Jadliwala",
      "affiliations": []
    },
    {
      "id": 23654,
      "firstName": "Predrag",
      "lastName": "Klasnja",
      "affiliations": []
    },
    {
      "id": 23655,
      "firstName": "Mutsuhiro",
      "lastName": "Iwamoto",
      "affiliations": []
    },
    {
      "id": 11369,
      "firstName": "Tanir",
      "lastName": "Ozcelebi",
      "affiliations": []
    },
    {
      "id": 23657,
      "firstName": "Max",
      "lastName": "Maass",
      "affiliations": []
    },
    {
      "id": 19569,
      "firstName": "Rui",
      "lastName": "Ma",
      "affiliations": []
    },
    {
      "id": 19571,
      "firstName": "Volker",
      "lastName": "Wulf",
      "affiliations": []
    },
    {
      "id": 23667,
      "firstName": "Jingao",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 23668,
      "firstName": "Saquib",
      "lastName": "Sarwar",
      "affiliations": []
    },
    {
      "id": 15476,
      "firstName": "TengYue",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 15477,
      "firstName": "Shingo",
      "lastName": "Takeda",
      "affiliations": []
    },
    {
      "id": 23674,
      "firstName": "Xiaoxi",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 15484,
      "firstName": "Masahiro",
      "lastName": "Kohjima",
      "affiliations": []
    },
    {
      "id": 11390,
      "firstName": "Susanna",
      "lastName": "Pirttikangas",
      "affiliations": []
    },
    {
      "id": 19588,
      "firstName": "Cosmin",
      "lastName": "Munteanu",
      "affiliations": []
    },
    {
      "id": 15493,
      "firstName": "Xin",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 11399,
      "firstName": "Agnes",
      "lastName": "Tegen",
      "affiliations": []
    },
    {
      "id": 23689,
      "firstName": "Topi",
      "lastName": "Kerhonen",
      "affiliations": []
    },
    {
      "id": 23690,
      "firstName": "Vincent",
      "lastName": "Frey",
      "affiliations": []
    },
    {
      "id": 11404,
      "firstName": "Ziyue",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 11407,
      "firstName": "Evangelos",
      "lastName": "Niforatos",
      "affiliations": []
    },
    {
      "id": 19601,
      "firstName": "Tong",
      "lastName": "Guan",
      "affiliations": []
    },
    {
      "id": 23699,
      "firstName": "Klaus",
      "lastName": "David",
      "affiliations": []
    },
    {
      "id": 11411,
      "firstName": "Kamila",
      "lastName": "Rodrigues",
      "affiliations": []
    },
    {
      "id": 19603,
      "firstName": "Frank",
      "lastName": "Rudzicz",
      "affiliations": []
    },
    {
      "id": 11412,
      "firstName": "Kirill A.",
      "lastName": "Shatilov",
      "affiliations": []
    },
    {
      "id": 11417,
      "firstName": "Janina",
      "lastName": "LÌ_scher",
      "affiliations": []
    },
    {
      "id": 23707,
      "firstName": "Shafizur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 19616,
      "firstName": "Jie",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 11425,
      "firstName": "K. J. Ray",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 19620,
      "firstName": "Sophie",
      "lastName": "Skach",
      "affiliations": []
    },
    {
      "id": 15524,
      "firstName": "Md. Eusha",
      "lastName": "Kadir",
      "affiliations": []
    },
    {
      "id": 15528,
      "firstName": "Andrew",
      "lastName": "Gigie",
      "affiliations": []
    },
    {
      "id": 11433,
      "firstName": "Cindy",
      "lastName": "Tieleman",
      "affiliations": []
    },
    {
      "id": 19625,
      "firstName": "Kelsey",
      "lastName": "Vitullo",
      "affiliations": []
    },
    {
      "id": 11441,
      "firstName": "Wataru",
      "lastName": "Sasaki",
      "affiliations": []
    },
    {
      "id": 15538,
      "firstName": "Gillian",
      "lastName": "Hayes",
      "affiliations": []
    },
    {
      "id": 19635,
      "firstName": "Chaoqun",
      "lastName": "Yue",
      "affiliations": []
    },
    {
      "id": 19637,
      "firstName": "Md. Nazmul",
      "lastName": "Haque",
      "affiliations": []
    },
    {
      "id": 23735,
      "firstName": "Rubaba",
      "lastName": "Hasan",
      "affiliations": []
    },
    {
      "id": 19640,
      "firstName": "Anne",
      "lastName": "Lamers",
      "affiliations": []
    },
    {
      "id": 19644,
      "firstName": "Ning",
      "lastName": "Xiao",
      "affiliations": []
    },
    {
      "id": 11454,
      "firstName": "Shin",
      "lastName": "Katayama",
      "affiliations": []
    },
    {
      "id": 15550,
      "firstName": "Martha",
      "lastName": "Hall",
      "affiliations": []
    },
    {
      "id": 11457,
      "firstName": "Silvia",
      "lastName": "Santini",
      "affiliations": []
    },
    {
      "id": 23746,
      "firstName": "Viktor",
      "lastName": "Matkovic",
      "affiliations": []
    },
    {
      "id": 11459,
      "firstName": "Nattaya",
      "lastName": "Mairittha",
      "affiliations": []
    },
    {
      "id": 23748,
      "firstName": "Maria Da Graca",
      "lastName": "Pimentel",
      "affiliations": []
    },
    {
      "id": 23750,
      "firstName": "Andres",
      "lastName": "Perez-Uribe",
      "affiliations": []
    },
    {
      "id": 23751,
      "firstName": "Matthew",
      "lastName": "Craner",
      "affiliations": []
    },
    {
      "id": 19658,
      "firstName": "Abdallah El",
      "lastName": "Ali",
      "affiliations": []
    },
    {
      "id": 23756,
      "firstName": "Constantine",
      "lastName": "Stephanidis",
      "affiliations": []
    },
    {
      "id": 15564,
      "firstName": "Judith",
      "lastName": "Moskowitz",
      "affiliations": []
    },
    {
      "id": 23757,
      "firstName": "Jamie A",
      "lastName": "Ward",
      "affiliations": []
    },
    {
      "id": 23758,
      "firstName": "Hillol",
      "lastName": "Sarker",
      "affiliations": []
    },
    {
      "id": 19667,
      "firstName": "Mohit",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 15573,
      "firstName": "Swadhin",
      "lastName": "Pradhan",
      "affiliations": []
    },
    {
      "id": 23771,
      "firstName": "Philip",
      "lastName": "Hinch",
      "affiliations": []
    },
    {
      "id": 15582,
      "firstName": "Youwei",
      "lastName": "Zeng",
      "affiliations": []
    },
    {
      "id": 23776,
      "firstName": "Alberto Monge",
      "lastName": "Roffarello",
      "affiliations": []
    },
    {
      "id": 23777,
      "firstName": "Dmitrijs",
      "lastName": "Balabka",
      "affiliations": []
    },
    {
      "id": 23780,
      "firstName": "Mikhail",
      "lastName": "Fomichev",
      "affiliations": []
    },
    {
      "id": 23781,
      "firstName": "Korosh",
      "lastName": "Vatanparvar",
      "affiliations": []
    },
    {
      "id": 11505,
      "firstName": "Ying",
      "lastName": "Kong",
      "affiliations": []
    },
    {
      "id": 19699,
      "firstName": "Akiya",
      "lastName": "Inagaki",
      "affiliations": []
    },
    {
      "id": 15608,
      "firstName": "Callum",
      "lastName": "Stewart",
      "affiliations": []
    },
    {
      "id": 23802,
      "firstName": "Koichi",
      "lastName": "Hirota",
      "affiliations": []
    },
    {
      "id": 19706,
      "firstName": "Roozbeh",
      "lastName": "Ghaffari",
      "affiliations": []
    },
    {
      "id": 23810,
      "firstName": "Sidney K.",
      "lastName": "D'Mello",
      "affiliations": []
    },
    {
      "id": 19714,
      "firstName": "Jennifer",
      "lastName": "Mankoff",
      "affiliations": []
    },
    {
      "id": 23811,
      "firstName": "Jakob E.",
      "lastName": "Bardram",
      "affiliations": []
    },
    {
      "id": 23812,
      "firstName": "Niels Van",
      "lastName": "Berkel",
      "affiliations": []
    },
    {
      "id": 19717,
      "firstName": "Han",
      "lastName": "Ding",
      "affiliations": []
    },
    {
      "id": 15623,
      "firstName": "Sebastian",
      "lastName": "Scholz",
      "affiliations": []
    },
    {
      "id": 19722,
      "firstName": "Chanjuan",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 15628,
      "firstName": "Shenggong",
      "lastName": "Ji",
      "affiliations": []
    },
    {
      "id": 11535,
      "firstName": "George",
      "lastName": "Boateng",
      "affiliations": []
    },
    {
      "id": 11538,
      "firstName": "Alexander",
      "lastName": "Seeliger",
      "affiliations": []
    },
    {
      "id": 11539,
      "firstName": "Zhican",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 23828,
      "firstName": "Yasuo",
      "lastName": "Okabe",
      "affiliations": []
    },
    {
      "id": 11540,
      "firstName": "Runze",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 23831,
      "firstName": "Varsha",
      "lastName": "Sharma",
      "affiliations": []
    },
    {
      "id": 15643,
      "firstName": "Katia",
      "lastName": "Vega",
      "affiliations": []
    },
    {
      "id": 19739,
      "firstName": "Eda",
      "lastName": "Celen",
      "affiliations": []
    },
    {
      "id": 11548,
      "firstName": "Marina",
      "lastName": "Toeters",
      "affiliations": []
    },
    {
      "id": 19740,
      "firstName": "Irene",
      "lastName": "Posch",
      "affiliations": []
    },
    {
      "id": 15645,
      "firstName": "Dingyi",
      "lastName": "Fang",
      "affiliations": []
    },
    {
      "id": 11552,
      "firstName": "Jean",
      "lastName": "Costa",
      "affiliations": []
    },
    {
      "id": 19744,
      "firstName": "Krister",
      "lastName": "Johnson",
      "affiliations": []
    },
    {
      "id": 23841,
      "firstName": "Chris",
      "lastName": "Yeung",
      "affiliations": []
    },
    {
      "id": 11555,
      "firstName": "Peijun",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 15651,
      "firstName": "Michael",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 23845,
      "firstName": "Andrés",
      "lastName": "Lucero",
      "affiliations": []
    },
    {
      "id": 19756,
      "firstName": "Yuanchun",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 15661,
      "firstName": "Kent",
      "lastName": "Lyons",
      "affiliations": []
    },
    {
      "id": 19760,
      "firstName": "Sean A.",
      "lastName": "Munson",
      "affiliations": []
    },
    {
      "id": 11568,
      "firstName": "Lars",
      "lastName": "Almon",
      "affiliations": []
    },
    {
      "id": 15667,
      "firstName": "Emanuel Von",
      "lastName": "Zezschwitz",
      "affiliations": []
    },
    {
      "id": 19764,
      "firstName": "Fang",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 19765,
      "firstName": "Rebecca",
      "lastName": "Adaimi",
      "affiliations": []
    },
    {
      "id": 23862,
      "firstName": "Rahat Jahangir",
      "lastName": "Rony",
      "affiliations": []
    },
    {
      "id": 11574,
      "firstName": "Peng",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 15671,
      "firstName": "Matthew",
      "lastName": "Fredrikson",
      "affiliations": []
    },
    {
      "id": 23863,
      "firstName": "Huining",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 19772,
      "firstName": "Bruna",
      "lastName": "Goveia",
      "affiliations": []
    },
    {
      "id": 23868,
      "firstName": "Q. Vera",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 19774,
      "firstName": "Aditya Singh",
      "lastName": "Rathore",
      "affiliations": []
    },
    {
      "id": 11583,
      "firstName": "Nicholas D.",
      "lastName": "Lane",
      "affiliations": []
    },
    {
      "id": 23876,
      "firstName": "Amanda C. De C.",
      "lastName": "Williams",
      "affiliations": []
    },
    {
      "id": 23879,
      "firstName": "Chee Siang",
      "lastName": "Ang",
      "affiliations": []
    },
    {
      "id": 15688,
      "firstName": "Amarjeet",
      "lastName": "Singh",
      "affiliations": []
    },
    {
      "id": 23884,
      "firstName": "Nick",
      "lastName": "Bryan-Kinns",
      "affiliations": []
    },
    {
      "id": 23885,
      "firstName": "Emma",
      "lastName": "Napari",
      "affiliations": []
    },
    {
      "id": 23887,
      "firstName": "Shuai",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 23891,
      "firstName": "Kizito",
      "lastName": "Masaba",
      "affiliations": []
    },
    {
      "id": 19796,
      "firstName": "David V.",
      "lastName": "Anderson",
      "affiliations": []
    },
    {
      "id": 15709,
      "firstName": "Travis",
      "lastName": "Siems",
      "affiliations": []
    },
    {
      "id": 15710,
      "firstName": "Menglan",
      "lastName": "Hu",
      "affiliations": []
    },
    {
      "id": 23906,
      "firstName": "Min",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 19811,
      "firstName": "Grete-Karmen",
      "lastName": "Jaakson",
      "affiliations": []
    },
    {
      "id": 15716,
      "firstName": "Sopicha",
      "lastName": "Stirapongsasuti",
      "affiliations": []
    },
    {
      "id": 23909,
      "firstName": "James",
      "lastName": "Fogarty",
      "affiliations": []
    },
    {
      "id": 11622,
      "firstName": "Stephen M.",
      "lastName": "Mattingly",
      "affiliations": []
    },
    {
      "id": 11624,
      "firstName": "Shun",
      "lastName": "Ishii",
      "affiliations": []
    },
    {
      "id": 15720,
      "firstName": "Anna",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 15724,
      "firstName": "Rong-Hao",
      "lastName": "Liang",
      "affiliations": []
    },
    {
      "id": 19821,
      "firstName": "Yi-Chi",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 11629,
      "firstName": "Suraj",
      "lastName": "Nair",
      "affiliations": []
    },
    {
      "id": 19823,
      "firstName": "Tsutomu",
      "lastName": "Terada",
      "affiliations": []
    },
    {
      "id": 11632,
      "firstName": "Ashley",
      "lastName": "Garrity",
      "affiliations": []
    },
    {
      "id": 23922,
      "firstName": "Jay Eric",
      "lastName": "Townsend",
      "affiliations": []
    },
    {
      "id": 11641,
      "firstName": "Ken",
      "lastName": "Sasaki",
      "affiliations": []
    },
    {
      "id": 19835,
      "firstName": "Tobias",
      "lastName": "Polly",
      "affiliations": []
    },
    {
      "id": 11643,
      "firstName": "Jordan",
      "lastName": "Wirfs-Brock",
      "affiliations": []
    },
    {
      "id": 19836,
      "firstName": "Jayalakshmi",
      "lastName": "Jain",
      "affiliations": []
    },
    {
      "id": 23933,
      "firstName": "Florian",
      "lastName": "Heller",
      "affiliations": []
    },
    {
      "id": 11647,
      "firstName": "Felix",
      "lastName": "Outlaw",
      "affiliations": []
    },
    {
      "id": 11651,
      "firstName": "Anastasia",
      "lastName": "Kuzminykh",
      "affiliations": []
    },
    {
      "id": 15749,
      "firstName": "Po-Wen",
      "lastName": "Kao",
      "affiliations": []
    },
    {
      "id": 23941,
      "firstName": "Beibei",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 15754,
      "firstName": "Yoshiaki",
      "lastName": "Narusue",
      "affiliations": []
    },
    {
      "id": 11659,
      "firstName": "Carlos AndrÌ© Freitas",
      "lastName": "Costa",
      "affiliations": []
    },
    {
      "id": 23948,
      "firstName": "Ilter",
      "lastName": "Canberk",
      "affiliations": []
    },
    {
      "id": 15758,
      "firstName": "Yuanchun",
      "lastName": "Shi",
      "affiliations": []
    },
    {
      "id": 23950,
      "firstName": "Debasis",
      "lastName": "Das",
      "affiliations": []
    },
    {
      "id": 11664,
      "firstName": "Mohammad",
      "lastName": "Kianpisheh",
      "affiliations": []
    },
    {
      "id": 19860,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 23962,
      "firstName": "Ling",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 19866,
      "firstName": "Pasindu",
      "lastName": "Lugoda",
      "affiliations": []
    },
    {
      "id": 11677,
      "firstName": "Pan",
      "lastName": "Hui",
      "affiliations": []
    },
    {
      "id": 23967,
      "firstName": "Stephanie",
      "lastName": "Balters",
      "affiliations": []
    },
    {
      "id": 19873,
      "firstName": "Florian",
      "lastName": "Schaub",
      "affiliations": []
    },
    {
      "id": 23969,
      "firstName": "Md. Tahmidul Islam",
      "lastName": "Molla",
      "affiliations": []
    },
    {
      "id": 11691,
      "firstName": "Rajan",
      "lastName": "Vaish",
      "affiliations": []
    },
    {
      "id": 19884,
      "firstName": "GonÌ¤alo",
      "lastName": "Lopes",
      "affiliations": []
    },
    {
      "id": 23982,
      "firstName": "Laurie",
      "lastName": "Wakschlag",
      "affiliations": []
    },
    {
      "id": 11695,
      "firstName": "Aman",
      "lastName": "Parnami",
      "affiliations": []
    },
    {
      "id": 19887,
      "firstName": "Joan-Isaac",
      "lastName": "Biel",
      "affiliations": []
    },
    {
      "id": 23984,
      "firstName": "Md Mahbubur",
      "lastName": "Rahman",
      "affiliations": []
    },
    {
      "id": 19888,
      "firstName": "Marco",
      "lastName": "Hirsch",
      "affiliations": []
    },
    {
      "id": 19889,
      "firstName": "Gang",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 19892,
      "firstName": "Jaejeung",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 23989,
      "firstName": "Yuki",
      "lastName": "Nishikawa",
      "affiliations": []
    },
    {
      "id": 23995,
      "firstName": "E. Ray",
      "lastName": "Dorsey",
      "affiliations": []
    },
    {
      "id": 11709,
      "firstName": "Sven",
      "lastName": "Gehring",
      "affiliations": []
    },
    {
      "id": 19903,
      "firstName": "Zhisheng",
      "lastName": "Yan",
      "affiliations": []
    },
    {
      "id": 11712,
      "firstName": "Angela",
      "lastName": "Pfammatter",
      "affiliations": []
    },
    {
      "id": 19905,
      "firstName": "Yung-Ju",
      "lastName": "Chang",
      "affiliations": []
    },
    {
      "id": 15813,
      "firstName": "Aaron",
      "lastName": "Striegel",
      "affiliations": []
    },
    {
      "id": 11718,
      "firstName": "Ted",
      "lastName": "Grover",
      "affiliations": []
    },
    {
      "id": 24009,
      "firstName": "Muhammad",
      "lastName": "Shahzad",
      "affiliations": []
    },
    {
      "id": 11721,
      "firstName": "Aruna",
      "lastName": "Seneviratne",
      "affiliations": []
    },
    {
      "id": 15818,
      "firstName": "Jatinder",
      "lastName": "Singh",
      "affiliations": []
    },
    {
      "id": 19918,
      "firstName": "Akari",
      "lastName": "Fukao",
      "affiliations": []
    },
    {
      "id": 11734,
      "firstName": "Tian",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 19927,
      "firstName": "Peter",
      "lastName": "Tolmie",
      "affiliations": []
    },
    {
      "id": 19928,
      "firstName": "Kostyantyn",
      "lastName": "Slyusarenko",
      "affiliations": []
    },
    {
      "id": 11737,
      "firstName": "Yuka",
      "lastName": "Noda",
      "affiliations": []
    },
    {
      "id": 24026,
      "firstName": "Birkir",
      "lastName": "Sigfússon",
      "affiliations": []
    },
    {
      "id": 24028,
      "firstName": "Michael",
      "lastName": "Jones",
      "affiliations": []
    },
    {
      "id": 24029,
      "firstName": "Andrew F.",
      "lastName": "Abercromby",
      "affiliations": []
    },
    {
      "id": 24031,
      "firstName": "Martin",
      "lastName": "Gjoreski",
      "affiliations": []
    },
    {
      "id": 11743,
      "firstName": "Elena",
      "lastName": "Smith",
      "affiliations": []
    },
    {
      "id": 19937,
      "firstName": "Hong",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 11752,
      "firstName": "Roger M.",
      "lastName": "Whitaker",
      "affiliations": []
    },
    {
      "id": 15850,
      "firstName": "Fiona",
      "lastName": "Westin",
      "affiliations": []
    },
    {
      "id": 19947,
      "firstName": "Keiichi",
      "lastName": "Ochiai",
      "affiliations": []
    },
    {
      "id": 19949,
      "firstName": "George",
      "lastName": "Shaker",
      "affiliations": []
    },
    {
      "id": 15856,
      "firstName": "Christopher",
      "lastName": "Leckie",
      "affiliations": []
    },
    {
      "id": 15859,
      "firstName": "Christophe",
      "lastName": "Fumeaux",
      "affiliations": []
    },
    {
      "id": 15862,
      "firstName": "Timo",
      "lastName": "Jakobi",
      "affiliations": []
    },
    {
      "id": 15863,
      "firstName": "Hyeokhyen",
      "lastName": "Kwon",
      "affiliations": []
    },
    {
      "id": 19961,
      "firstName": "Mehrab Bin",
      "lastName": "Morshed",
      "affiliations": []
    },
    {
      "id": 11773,
      "firstName": "Hanchuan",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 11775,
      "firstName": "Doruk",
      "lastName": "Yildirim",
      "affiliations": []
    },
    {
      "id": 19968,
      "firstName": "Xin",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 11779,
      "firstName": "Hampton C.",
      "lastName": "Gabler",
      "affiliations": []
    },
    {
      "id": 11785,
      "firstName": "Karan",
      "lastName": "Ahuja",
      "affiliations": []
    },
    {
      "id": 19979,
      "firstName": "Ayumi",
      "lastName": "Ohnishi",
      "affiliations": []
    },
    {
      "id": 11787,
      "firstName": "Pan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24077,
      "firstName": "Yeri",
      "lastName": "Jeong",
      "affiliations": []
    },
    {
      "id": 15886,
      "firstName": "Kari",
      "lastName": "Nies",
      "affiliations": []
    },
    {
      "id": 11791,
      "firstName": "Kelly",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 24086,
      "firstName": "Younghyun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 11799,
      "firstName": "Jeremy",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 19991,
      "firstName": "Xuanzhe",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 19992,
      "firstName": "Niko",
      "lastName": "Munzenrieder",
      "affiliations": []
    },
    {
      "id": 19993,
      "firstName": "Tsuyoshi",
      "lastName": "Okita",
      "affiliations": []
    },
    {
      "id": 11804,
      "firstName": "Anja",
      "lastName": "Exler",
      "affiliations": []
    },
    {
      "id": 24094,
      "firstName": "Simo",
      "lastName": "Hosio",
      "affiliations": []
    },
    {
      "id": 15905,
      "firstName": "Jiayou",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 11813,
      "firstName": "leonardo de",
      "lastName": "mattos",
      "affiliations": []
    },
    {
      "id": 20007,
      "firstName": "Pekka",
      "lastName": "Siirtola",
      "affiliations": []
    },
    {
      "id": 20009,
      "firstName": "Tristan",
      "lastName": "Braud",
      "affiliations": []
    },
    {
      "id": 24106,
      "firstName": "Yanbo",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 24108,
      "firstName": "Zbigniew",
      "lastName": "Chaniecki",
      "affiliations": []
    },
    {
      "id": 24110,
      "firstName": "Yanwen",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 15918,
      "firstName": "Huadong",
      "lastName": "Ma",
      "affiliations": []
    },
    {
      "id": 24111,
      "firstName": "Rupert",
      "lastName": "Page",
      "affiliations": []
    },
    {
      "id": 24117,
      "firstName": "Mirco",
      "lastName": "Musolesi",
      "affiliations": []
    },
    {
      "id": 11830,
      "firstName": "Md Munirul",
      "lastName": "Haque",
      "affiliations": []
    },
    {
      "id": 15929,
      "firstName": "Caterina",
      "lastName": "BÌ©rubÌ©",
      "affiliations": []
    },
    {
      "id": 15930,
      "firstName": "Xu",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 20027,
      "firstName": "Francois",
      "lastName": "Charpillet",
      "affiliations": []
    },
    {
      "id": 11837,
      "firstName": "Christopher",
      "lastName": "Moreno",
      "affiliations": []
    },
    {
      "id": 24125,
      "firstName": "Armando Rodríguez",
      "lastName": "Pérez",
      "affiliations": []
    },
    {
      "id": 24129,
      "firstName": "Sozo",
      "lastName": "Inoue",
      "affiliations": []
    },
    {
      "id": 11842,
      "firstName": "Bruna Goveia da",
      "lastName": "Rocha",
      "affiliations": []
    },
    {
      "id": 15943,
      "firstName": "Elizabeth Esther",
      "lastName": "Bigger",
      "affiliations": []
    },
    {
      "id": 20041,
      "firstName": "Eric C.",
      "lastName": "Larson",
      "affiliations": []
    },
    {
      "id": 15946,
      "firstName": "Valentin",
      "lastName": "Radu",
      "affiliations": []
    },
    {
      "id": 24146,
      "firstName": "Linghan",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24147,
      "firstName": "Yong",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20052,
      "firstName": "Motoi",
      "lastName": "Iwata",
      "affiliations": []
    },
    {
      "id": 15961,
      "firstName": "Xiang",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 11865,
      "firstName": "Mengwei",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 24155,
      "firstName": "Zdzislawa",
      "lastName": "Rowinska",
      "affiliations": []
    },
    {
      "id": 20063,
      "firstName": "Jie",
      "lastName": "Feng",
      "affiliations": []
    },
    {
      "id": 24160,
      "firstName": "Alireza",
      "lastName": "Golgouneh",
      "affiliations": []
    },
    {
      "id": 11879,
      "firstName": "Bing",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 24171,
      "firstName": "Nobuhiko",
      "lastName": "Nishio",
      "affiliations": []
    },
    {
      "id": 11883,
      "firstName": "Boyang",
      "lastName": "Fu",
      "affiliations": []
    },
    {
      "id": 20075,
      "firstName": "Huatao",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 24172,
      "firstName": "Yujin",
      "lastName": "Seo",
      "affiliations": []
    },
    {
      "id": 11897,
      "firstName": "Desheng",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 24185,
      "firstName": "Shuo",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 20091,
      "firstName": "Lin",
      "lastName": "Cai",
      "affiliations": []
    },
    {
      "id": 11914,
      "firstName": "Anindya",
      "lastName": "Maiti",
      "affiliations": []
    },
    {
      "id": 24202,
      "firstName": "Kirill",
      "lastName": "Rodriguez",
      "affiliations": []
    },
    {
      "id": 20108,
      "firstName": "Pino",
      "lastName": "Audia",
      "affiliations": []
    },
    {
      "id": 24210,
      "firstName": "Beatriz Sousa",
      "lastName": "Santos",
      "affiliations": []
    },
    {
      "id": 16019,
      "firstName": "Christopher",
      "lastName": "Snyder",
      "affiliations": []
    },
    {
      "id": 11924,
      "firstName": "Dr. Patricia Jean",
      "lastName": "Flanagan",
      "affiliations": []
    },
    {
      "id": 11925,
      "firstName": "Masahiko",
      "lastName": "Tsukamoto",
      "affiliations": []
    },
    {
      "id": 11930,
      "firstName": "Peter-Paul",
      "lastName": "Verbeek",
      "affiliations": []
    },
    {
      "id": 16028,
      "firstName": "Manuel",
      "lastName": "Landsmann",
      "affiliations": []
    },
    {
      "id": 11932,
      "firstName": "Tingrui",
      "lastName": "Pan",
      "affiliations": []
    },
    {
      "id": 20128,
      "firstName": "Alexander",
      "lastName": "Russell",
      "affiliations": []
    },
    {
      "id": 11943,
      "firstName": "Jukka",
      "lastName": "Riekki",
      "affiliations": []
    },
    {
      "id": 11950,
      "firstName": "Joao",
      "lastName": "Falcao",
      "affiliations": []
    },
    {
      "id": 20143,
      "firstName": "Matthias",
      "lastName": "Hollick",
      "affiliations": []
    },
    {
      "id": 20145,
      "firstName": "Rainhard D.",
      "lastName": "Findling",
      "affiliations": []
    },
    {
      "id": 20146,
      "firstName": "Heli",
      "lastName": "KoskimÌ_ki",
      "affiliations": []
    },
    {
      "id": 16051,
      "firstName": "Chu",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 11956,
      "firstName": "Yuhang",
      "lastName": "Jiang",
      "affiliations": []
    },
    {
      "id": 24245,
      "firstName": "Zac",
      "lastName": "Yu",
      "affiliations": []
    },
    {
      "id": 20152,
      "firstName": "Mihai Teodor",
      "lastName": "Lazarescu",
      "affiliations": []
    },
    {
      "id": 24250,
      "firstName": "Allison",
      "lastName": "Cole",
      "affiliations": []
    },
    {
      "id": 24251,
      "firstName": "Joshua",
      "lastName": "Newn",
      "affiliations": []
    },
    {
      "id": 20156,
      "firstName": "Qijia",
      "lastName": "Shao",
      "affiliations": []
    },
    {
      "id": 11965,
      "firstName": "Xinyu",
      "lastName": "Tong",
      "affiliations": []
    },
    {
      "id": 16063,
      "firstName": "Hugo",
      "lastName": "Gamboa",
      "affiliations": []
    },
    {
      "id": 24256,
      "firstName": "Chloe",
      "lastName": "Eghtebas",
      "affiliations": []
    },
    {
      "id": 11969,
      "firstName": "Gonzalo J.",
      "lastName": "Martinez",
      "affiliations": []
    },
    {
      "id": 11973,
      "firstName": "Naja Holten",
      "lastName": "MÌüller",
      "affiliations": []
    },
    {
      "id": 24265,
      "firstName": "Takuya",
      "lastName": "Maekawa",
      "affiliations": []
    },
    {
      "id": 16077,
      "firstName": "Diogo",
      "lastName": "Cabral",
      "affiliations": []
    },
    {
      "id": 16079,
      "firstName": "Pei-Yi (Patricia)",
      "lastName": "Kuo",
      "affiliations": []
    },
    {
      "id": 11984,
      "firstName": "Yuntao",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16080,
      "firstName": "Thomas",
      "lastName": "Plötz",
      "affiliations": []
    },
    {
      "id": 16082,
      "firstName": "Xuehan",
      "lastName": "Ye",
      "affiliations": []
    },
    {
      "id": 16087,
      "firstName": "Hancheng",
      "lastName": "Cao",
      "affiliations": []
    },
    {
      "id": 11994,
      "firstName": "Yi",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 16092,
      "firstName": "Yao",
      "lastName": "Jing",
      "affiliations": []
    },
    {
      "id": 24286,
      "firstName": "Xiaojiang",
      "lastName": "Du",
      "affiliations": []
    },
    {
      "id": 16094,
      "firstName": "Adtiya",
      "lastName": "Virmani",
      "affiliations": []
    },
    {
      "id": 16096,
      "firstName": "Koki",
      "lastName": "Tamura",
      "affiliations": []
    },
    {
      "id": 24289,
      "firstName": "Chaofan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 12003,
      "firstName": "Guillaume",
      "lastName": "Lopez",
      "affiliations": []
    },
    {
      "id": 16101,
      "firstName": "Shaurye",
      "lastName": "Aggarwal",
      "affiliations": []
    },
    {
      "id": 24295,
      "firstName": "Lu",
      "lastName": "Bai",
      "affiliations": []
    },
    {
      "id": 12008,
      "firstName": "Ana",
      "lastName": "Rodrigues",
      "affiliations": []
    },
    {
      "id": 16104,
      "firstName": "Holger",
      "lastName": "Hewener",
      "affiliations": []
    },
    {
      "id": 24300,
      "firstName": "Yanmin",
      "lastName": "Zhu",
      "affiliations": []
    },
    {
      "id": 16120,
      "firstName": "Emre",
      "lastName": "Ertin",
      "affiliations": []
    },
    {
      "id": 20218,
      "firstName": "Masaki",
      "lastName": "Matsubara",
      "affiliations": []
    },
    {
      "id": 24315,
      "firstName": "Dongyao",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 12032,
      "firstName": "Shaoqing",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 16129,
      "firstName": "Marcel",
      "lastName": "Schweiker",
      "affiliations": []
    },
    {
      "id": 20226,
      "firstName": "Luciano",
      "lastName": "Lavagno",
      "affiliations": []
    },
    {
      "id": 24322,
      "firstName": "Raju",
      "lastName": "Maharjan",
      "affiliations": []
    },
    {
      "id": 12036,
      "firstName": "Aniruddha",
      "lastName": "Sinha",
      "affiliations": []
    },
    {
      "id": 16133,
      "firstName": "Andrew",
      "lastName": "Markham",
      "affiliations": []
    },
    {
      "id": 24331,
      "firstName": "Charles",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 24333,
      "firstName": "Takeshi",
      "lastName": "Kurashima",
      "affiliations": []
    },
    {
      "id": 20239,
      "firstName": "Leye",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16143,
      "firstName": "Liza",
      "lastName": "Stark",
      "affiliations": []
    },
    {
      "id": 12048,
      "firstName": "Naoki",
      "lastName": "Yamamoto",
      "affiliations": []
    },
    {
      "id": 24337,
      "firstName": "Chu",
      "lastName": "Luo",
      "affiliations": []
    },
    {
      "id": 24339,
      "firstName": "Wei",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 16150,
      "firstName": "Neil",
      "lastName": "Klingensmith",
      "affiliations": []
    },
    {
      "id": 16151,
      "firstName": "Huijie",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20248,
      "firstName": "Woohyeok",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 16152,
      "firstName": "Patrick",
      "lastName": "Olivier",
      "affiliations": []
    },
    {
      "id": 20250,
      "firstName": "Ana Correia de",
      "lastName": "Barros",
      "affiliations": []
    },
    {
      "id": 24346,
      "firstName": "Masami",
      "lastName": "Iwase",
      "affiliations": []
    },
    {
      "id": 12067,
      "firstName": "Lukas",
      "lastName": "Brausch",
      "affiliations": []
    },
    {
      "id": 12072,
      "firstName": "Vincent Y. F.",
      "lastName": "Tan",
      "affiliations": []
    },
    {
      "id": 24361,
      "firstName": "Lili",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20265,
      "firstName": "Rongrong",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 12074,
      "firstName": "Stefan",
      "lastName": "Kalabakov",
      "affiliations": []
    },
    {
      "id": 20266,
      "firstName": "Joyce",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 24366,
      "firstName": "Markus",
      "lastName": "LÌ¦chtefeld",
      "affiliations": []
    },
    {
      "id": 12079,
      "firstName": "Pablo",
      "lastName": "Robles-Granda",
      "affiliations": []
    },
    {
      "id": 24368,
      "firstName": "Zimu",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 12081,
      "firstName": "Kiti",
      "lastName": "MÌ_ller",
      "affiliations": []
    },
    {
      "id": 12082,
      "firstName": "Dragos",
      "lastName": "Datcu",
      "affiliations": []
    },
    {
      "id": 16183,
      "firstName": "Shun-Yao",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24379,
      "firstName": "Sangkeun",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 12091,
      "firstName": "Mridula",
      "lastName": "Gupta",
      "affiliations": []
    },
    {
      "id": 12097,
      "firstName": "Janne",
      "lastName": "Lindqvist",
      "affiliations": []
    },
    {
      "id": 20294,
      "firstName": "Asterios",
      "lastName": "Leonidis",
      "affiliations": []
    },
    {
      "id": 20296,
      "firstName": "Yutaka",
      "lastName": "Yanagisawa",
      "affiliations": []
    },
    {
      "id": 16202,
      "firstName": "Piotr",
      "lastName": "Luczak",
      "affiliations": []
    },
    {
      "id": 12107,
      "firstName": "Hiroyuki",
      "lastName": "Toda",
      "affiliations": []
    },
    {
      "id": 16206,
      "firstName": "Aayush",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 24402,
      "firstName": "Katrina Karyee",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 24405,
      "firstName": "Lin",
      "lastName": "Tang",
      "affiliations": []
    },
    {
      "id": 12118,
      "firstName": "Anna",
      "lastName": "Yokokubo",
      "affiliations": []
    },
    {
      "id": 20315,
      "firstName": "LaÌ_s",
      "lastName": "Lopes",
      "affiliations": []
    },
    {
      "id": 20317,
      "firstName": "Vito",
      "lastName": "Janko",
      "affiliations": []
    },
    {
      "id": 20318,
      "firstName": "Hiroki",
      "lastName": "Watanabe",
      "affiliations": []
    },
    {
      "id": 20328,
      "firstName": "Jason Shuo",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 16232,
      "firstName": "Chao",
      "lastName": "Shang",
      "affiliations": []
    },
    {
      "id": 12138,
      "firstName": "Mikolaj P.",
      "lastName": "Wozniak",
      "affiliations": []
    },
    {
      "id": 16243,
      "firstName": "Bokyung",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 20340,
      "firstName": "Anusha",
      "lastName": "Sirigiri",
      "affiliations": []
    },
    {
      "id": 20351,
      "firstName": "Lili",
      "lastName": "Ma",
      "affiliations": []
    },
    {
      "id": 16257,
      "firstName": "Radoslaw",
      "lastName": "Niewiadomski",
      "affiliations": []
    },
    {
      "id": 20353,
      "firstName": "Rohan",
      "lastName": "Banerjee",
      "affiliations": []
    },
    {
      "id": 20354,
      "firstName": "Subhransu",
      "lastName": "Maji",
      "affiliations": []
    },
    {
      "id": 20356,
      "firstName": "Ehsan Ul",
      "lastName": "Haq",
      "affiliations": []
    },
    {
      "id": 12164,
      "firstName": "Hyunjae",
      "lastName": "Gil",
      "affiliations": []
    },
    {
      "id": 12174,
      "firstName": "Thomas",
      "lastName": "PÌ¦nitz",
      "affiliations": []
    },
    {
      "id": 12184,
      "firstName": "Hengjie",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20380,
      "firstName": "Kota",
      "lastName": "Tsubouchi",
      "affiliations": []
    },
    {
      "id": 12189,
      "firstName": "Manoj",
      "lastName": "Deshpande",
      "affiliations": []
    },
    {
      "id": 12190,
      "firstName": "Clara",
      "lastName": "Mancini",
      "affiliations": []
    },
    {
      "id": 12191,
      "firstName": "Renuka",
      "lastName": "Visvanathan",
      "affiliations": []
    },
    {
      "id": 16290,
      "firstName": "JUAN",
      "lastName": "LUO",
      "affiliations": []
    },
    {
      "id": 20389,
      "firstName": "Kohei",
      "lastName": "Adachi",
      "affiliations": []
    },
    {
      "id": 20396,
      "firstName": "Fan",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20397,
      "firstName": "Ville",
      "lastName": "Mäkelä",
      "affiliations": []
    },
    {
      "id": 12206,
      "firstName": "Boyd",
      "lastName": "Anderson",
      "affiliations": []
    },
    {
      "id": 20402,
      "firstName": "Tapas",
      "lastName": "Chakravarty",
      "affiliations": []
    },
    {
      "id": 20404,
      "firstName": "Moshe",
      "lastName": "Gabel",
      "affiliations": []
    },
    {
      "id": 16309,
      "firstName": "James A.",
      "lastName": "Landay",
      "affiliations": []
    },
    {
      "id": 20409,
      "firstName": "Brigitte van der",
      "lastName": "Lugt",
      "affiliations": []
    },
    {
      "id": 16320,
      "firstName": "Carlos",
      "lastName": "Ruiz",
      "affiliations": []
    },
    {
      "id": 20416,
      "firstName": "Mathias",
      "lastName": "Ciliberto",
      "affiliations": []
    },
    {
      "id": 12224,
      "firstName": "Obinna",
      "lastName": "Onyeije",
      "affiliations": []
    },
    {
      "id": 20417,
      "firstName": "Ryosuke",
      "lastName": "Minami",
      "affiliations": []
    },
    {
      "id": 12226,
      "firstName": "Runchang",
      "lastName": "Kang",
      "affiliations": []
    },
    {
      "id": 20422,
      "firstName": "Daniel",
      "lastName": "Gatica-Perez",
      "affiliations": []
    },
    {
      "id": 12232,
      "firstName": "Gleb",
      "lastName": "Iakovlev",
      "affiliations": []
    },
    {
      "id": 16329,
      "firstName": "Xuhai",
      "lastName": "Xu",
      "affiliations": []
    },
    {
      "id": 12234,
      "firstName": "Chi-Wing",
      "lastName": "Fu",
      "affiliations": []
    },
    {
      "id": 20429,
      "firstName": "Stefan",
      "lastName": "Fuchs",
      "affiliations": []
    },
    {
      "id": 12242,
      "firstName": "Thepchai",
      "lastName": "Supnithi",
      "affiliations": []
    },
    {
      "id": 16340,
      "firstName": "Benjamin",
      "lastName": "Cauchi",
      "affiliations": []
    },
    {
      "id": 12248,
      "firstName": "Temitayo A.",
      "lastName": "Olugbade",
      "affiliations": []
    },
    {
      "id": 16347,
      "firstName": "Haruka",
      "lastName": "Kamachi",
      "affiliations": []
    },
    {
      "id": 12255,
      "firstName": "Weichen",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 16356,
      "firstName": "Akhil",
      "lastName": "Mathur",
      "affiliations": []
    },
    {
      "id": 20456,
      "firstName": "Jian",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 20461,
      "firstName": "Kei",
      "lastName": "Hiroi",
      "affiliations": []
    },
    {
      "id": 20464,
      "firstName": "Barbara Korousic",
      "lastName": "Seljak",
      "affiliations": []
    },
    {
      "id": 16368,
      "firstName": "Athanasios",
      "lastName": "Vourvopoulos",
      "affiliations": []
    },
    {
      "id": 16373,
      "firstName": "Jaeah",
      "lastName": "Yoo",
      "affiliations": []
    },
    {
      "id": 20472,
      "firstName": "Giovanna Nunes",
      "lastName": "Vilaza",
      "affiliations": []
    },
    {
      "id": 16380,
      "firstName": "Mario",
      "lastName": "Esparza",
      "affiliations": []
    },
    {
      "id": 20479,
      "firstName": "Tobias",
      "lastName": "Kowatsch",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 26,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 13:41:20+00"
  }
}