{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10084,
    "shortName": "VRST",
    "year": 2022,
    "startDate": 1669680000000,
    "endDate": 1669852800000,
    "fullName": "28th ACM Symposium on Virtual Reality Software and Technology",
    "url": "https://vrst.acm.org/vrst2022/",
    "location": "Tsukuba",
    "timeZoneOffset": 540,
    "timeZoneName": "Asia/Tokyo",
    "logoUrl": "https://files.sigchi.org/conference/logo/10084/8f44fdb4-46dd-5676-b1ec-b9221d8a475c.png",
    "name": "VRST 2022"
  },
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 33,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2022-12-01 07:20:48+00"
  },
  "sponsors": [
    {
      "id": 10285,
      "name": "Virtualwindow Co., Ltd.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/9a296ede-fd01-e4c6-bc06-89d49789d40d.png",
      "levelId": 10185,
      "url": "http://en.virtualwindow.net/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10286,
      "name": "Polyphony Digital Inc.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/f56e7bce-ed91-0abc-b462-caae0b8e541b.png",
      "levelId": 10186,
      "url": "https://www.polyphony.co.jp/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10287,
      "name": "ENHANCE",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/35a9dee8-dcd7-79b0-f2d2-71bb0873896c.png",
      "levelId": 10186,
      "url": "https://enhance-experience.com/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10288,
      "name": "Association for Computing Machinery",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/e2c98b03-332f-c3e1-69fd-65f86094e780.png",
      "levelId": 10171,
      "url": "https://www.acm.org/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10289,
      "name": "ACM Special Interest Group on Computer-Human Interaction",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/bf12f965-3e52-79e1-6127-a72b8f711143.png",
      "levelId": 10171,
      "url": "https://sigchi.org/",
      "order": 1,
      "extraPadding": 0
    },
    {
      "id": 10290,
      "name": "ACM SIGGRAPH",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/82687c34-fbec-c9c8-eac7-e4f3716135e3.png",
      "levelId": 10171,
      "url": "https://www.siggraph.org/",
      "order": 2,
      "extraPadding": 0
    },
    {
      "id": 10291,
      "name": "ACM Symposium on Virtual Reality Software and Technology",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/83419639-94d2-af9b-c9c3-f5ee15db8e4a.png",
      "levelId": 10171,
      "url": "https://vrst.acm.org/",
      "order": 3,
      "extraPadding": 0
    },
    {
      "id": 10292,
      "name": "Computer Graphic Arts Society",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/81815b20-09e0-e9e3-2449-633ffd63dbdb.png",
      "levelId": 10171,
      "url": "https://www.cgarts.or.jp/english/index.html",
      "order": 4,
      "extraPadding": 0
    },
    {
      "id": 10296,
      "name": "Cluster, Inc.",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/687565e6-6eb6-d2e4-9cea-d05fc18c207d.png",
      "levelId": 10190,
      "url": "https://cluster.mu/en/",
      "order": 0,
      "extraPadding": 0
    },
    {
      "id": 10297,
      "name": "The Telecommunications Advancement Foundation",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/10084/logo/6d757e26-2661-1f80-9a12-130a8acaceb6.png",
      "levelId": 10171,
      "url": "https://www.taf.or.jp/",
      "order": 5,
      "extraPadding": 0
    }
  ],
  "sponsorLevels": [
    {
      "id": 10171,
      "name": "Sponsors",
      "rank": 4,
      "isDefault": true
    },
    {
      "id": 10185,
      "name": "GOLD SPONSOR",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10186,
      "name": "SILVER SPONSOR",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10190,
      "name": "PLATINUM SPONSOR",
      "rank": 1,
      "isDefault": false
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10733,
      "name": "Main room",
      "typeId": 12411,
      "setup": "Theatre",
      "note": "Zoom URL"
    },
    {
      "id": 10734,
      "name": "demo",
      "typeId": 12407,
      "setup": "Special"
    },
    {
      "id": 10735,
      "name": "Poster room",
      "typeId": 12412,
      "setup": "Special"
    }
  ],
  "tracks": [
    {
      "id": 11939,
      "name": "VRST 2022 Posters and Demos",
      "typeId": 12457
    },
    {
      "id": 11940,
      "name": "VRST 2022 Papers",
      "typeId": 12411
    },
    {
      "id": 11985
    }
  ],
  "contentTypes": [
    {
      "id": 12406,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12407,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 0,
      "displayName": "Demos"
    },
    {
      "id": 12408,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 12409,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12410,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 12411,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 12412,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 0,
      "displayName": "Posters"
    },
    {
      "id": 12413,
      "name": "Work-in-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 12414,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12415,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 12457,
      "name": "demo&poster",
      "color": "#ff7a00",
      "duration": 0
    },
    {
      "id": 12458,
      "name": "short paper",
      "color": "#006d2c",
      "duration": 0
    },
    {
      "id": 12465,
      "name": "Keynote",
      "color": "#969696",
      "duration": 60
    }
  ],
  "timeSlots": [
    {
      "id": 12802,
      "type": "SESSION",
      "startDate": 1669717800000,
      "endDate": 1669722300000
    },
    {
      "id": 12803,
      "type": "LUNCH",
      "startDate": 1669723200000,
      "endDate": 1669728600000
    },
    {
      "id": 12804,
      "type": "SESSION",
      "startDate": 1669728600000,
      "endDate": 1669733100000
    },
    {
      "id": 12805,
      "type": "BREAK",
      "startDate": 1669733100000,
      "endDate": 1669734000000
    },
    {
      "id": 12806,
      "type": "SESSION",
      "startDate": 1669734000000,
      "endDate": 1669737300000
    },
    {
      "id": 12808,
      "type": "SESSION",
      "startDate": 1669744500000,
      "endDate": 1669749900000
    },
    {
      "id": 12809,
      "type": "SESSION",
      "startDate": 1669738200000,
      "endDate": 1669743600000
    },
    {
      "id": 12810,
      "type": "SESSION",
      "startDate": 1669716000000,
      "endDate": 1669717800000
    },
    {
      "id": 12811,
      "type": "SESSION",
      "startDate": 1669802400000,
      "endDate": 1669807200000
    },
    {
      "id": 12812,
      "type": "LUNCH",
      "startDate": 1669807800000,
      "endDate": 1669813200000
    },
    {
      "id": 12813,
      "type": "SESSION",
      "startDate": 1669813200000,
      "endDate": 1669818000000
    },
    {
      "id": 12814,
      "type": "BREAK",
      "startDate": 1669818000000,
      "endDate": 1669818600000
    },
    {
      "id": 12815,
      "type": "SESSION",
      "startDate": 1669818600000,
      "endDate": 1669823100000
    },
    {
      "id": 12816,
      "type": "BREAK",
      "startDate": 1669823100000,
      "endDate": 1669823700000
    },
    {
      "id": 12817,
      "type": "SESSION",
      "startDate": 1669823700000,
      "endDate": 1669827300000
    },
    {
      "id": 12818,
      "type": "SESSION",
      "startDate": 1669827600000,
      "endDate": 1669834800000
    },
    {
      "id": 12819,
      "type": "SESSION",
      "startDate": 1669834800000,
      "endDate": 1669838400000
    },
    {
      "id": 12820,
      "type": "SESSION",
      "startDate": 1669888800000,
      "endDate": 1669893300000
    },
    {
      "id": 12821,
      "type": "BREAK",
      "startDate": 1669893300000,
      "endDate": 1669894200000
    },
    {
      "id": 12822,
      "type": "SESSION",
      "startDate": 1669894200000,
      "endDate": 1669897800000
    },
    {
      "id": 12825,
      "type": "SESSION",
      "startDate": 1669905000000,
      "endDate": 1669908600000
    },
    {
      "id": 12826,
      "type": "SESSION",
      "startDate": 1669914000000,
      "endDate": 1669918800000
    },
    {
      "id": 12827,
      "type": "BREAK",
      "startDate": 1669908600000,
      "endDate": 1669909500000
    },
    {
      "id": 12828,
      "type": "SESSION",
      "startDate": 1669909500000,
      "endDate": 1669913100000
    },
    {
      "id": 12829,
      "type": "SESSION",
      "startDate": 1669918800000,
      "endDate": 1669920600000
    },
    {
      "id": 12833,
      "type": "BREAK",
      "startDate": 1669913100000,
      "endDate": 1669914000000
    },
    {
      "id": 12836,
      "type": "SESSION",
      "startDate": 1669897800000,
      "endDate": 1669905000000
    }
  ],
  "sessions": [
    {
      "id": 90308,
      "name": "Applications",
      "isParallelPresentation": false,
      "importedId": "4553c568-8ada-447e-bcce-50cf01fd390f",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90248
      ],
      "contentIds": [
        90273,
        90279,
        90274,
        90285
      ],
      "source": "SYS",
      "timeSlotId": 12802
    },
    {
      "id": 90309,
      "name": "Virtual Humans, Collaboration, and Social Interaction 1",
      "isParallelPresentation": false,
      "importedId": "2087869b-3ad7-40af-bfdf-1a20db5a2e0b",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90270
      ],
      "contentIds": [
        90298,
        90303,
        90283,
        90292
      ],
      "source": "SYS",
      "timeSlotId": 12804
    },
    {
      "id": 90310,
      "name": "Virtual Humans, Collaboration, and Social Interaction 2",
      "isParallelPresentation": false,
      "importedId": "e0765107-babf-4795-ba3b-7dd45ab88d14",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90360
      ],
      "contentIds": [
        90306,
        90301,
        90294
      ],
      "source": "SYS",
      "timeSlotId": 12806
    },
    {
      "id": 90312,
      "name": "Input and Interaction 1",
      "isParallelPresentation": false,
      "importedId": "0fa57aca-723e-4610-845d-72adc7ba90f6",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90211
      ],
      "contentIds": [
        90278,
        90296,
        90280,
        90297
      ],
      "source": "SYS",
      "timeSlotId": 12811
    },
    {
      "id": 90313,
      "name": "Input and Interaction 2",
      "isParallelPresentation": false,
      "importedId": "9bff1bf2-81a5-49af-9404-3f2a8eab860b",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90468
      ],
      "contentIds": [
        90291,
        90281,
        90272,
        90302
      ],
      "source": "SYS",
      "timeSlotId": 12813
    },
    {
      "id": 90314,
      "name": "Visualization and Displays 1",
      "isParallelPresentation": false,
      "importedId": "12526aa9-ba9e-4ddd-8639-ef4498823f63",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90260
      ],
      "contentIds": [
        90277,
        90299,
        90295,
        90300
      ],
      "source": "SYS",
      "timeSlotId": 12815
    },
    {
      "id": 90315,
      "name": "Visualization and Displays 2",
      "isParallelPresentation": false,
      "importedId": "b9f09fc1-178e-44c5-ab64-eb82e5306a23",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90129
      ],
      "contentIds": [
        90290,
        90289,
        90304
      ],
      "source": "SYS",
      "timeSlotId": 12817
    },
    {
      "id": 90318,
      "name": "Vision Perception",
      "isParallelPresentation": false,
      "importedId": "a57f4f45-32dc-4e4b-a4a9-93ffb73e73aa",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90243
      ],
      "contentIds": [
        90275,
        90284,
        90287
      ],
      "source": "SYS",
      "timeSlotId": 12822
    },
    {
      "id": 90319,
      "name": "Body Perception",
      "isParallelPresentation": false,
      "importedId": "e97a7f43-d249-4330-8c6f-627bc2aaf85c",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90239
      ],
      "contentIds": [
        90305,
        90293,
        90276
      ],
      "source": "SYS",
      "timeSlotId": 12825
    },
    {
      "id": 90320,
      "name": "Interaction design",
      "isParallelPresentation": false,
      "importedId": "fc1b4efa-39fd-48eb-addc-52a86dec9da9",
      "typeId": 12411,
      "roomId": 10733,
      "chairIds": [
        90186
      ],
      "contentIds": [
        90282,
        90288,
        90286
      ],
      "source": "SYS",
      "timeSlotId": 12828
    },
    {
      "id": 90529,
      "name": "Demo & Poster",
      "isParallelPresentation": false,
      "importedId": "c4303233-f6ad-47f4-97a6-094a7283dede",
      "typeId": 12457,
      "roomId": 10734,
      "chairIds": [],
      "contentIds": [
        90523,
        90526,
        90528,
        90490,
        90498,
        90481,
        90484,
        90515,
        90488,
        90480,
        90497,
        90504,
        90511,
        90516,
        90514,
        90524,
        90512,
        90509,
        90502,
        90519,
        90520,
        90505,
        90503,
        90482,
        90527,
        90507,
        90495
      ],
      "source": "SYS",
      "timeSlotId": 12808
    },
    {
      "id": 90628,
      "name": "Remote Demo&Poster",
      "isParallelPresentation": false,
      "importedId": "0757a61c-8854-459f-bcfa-4f21c62aef0a",
      "typeId": 12457,
      "roomId": 10734,
      "chairIds": [],
      "contentIds": [
        90494,
        90506,
        90485,
        90525,
        90499,
        90518,
        90513,
        90522,
        90517,
        90483,
        90500,
        90521,
        90501,
        90496,
        90491,
        90489,
        90486,
        90508,
        90487,
        90493,
        90492,
        90510
      ],
      "source": "SYS",
      "timeSlotId": 12836
    }
  ],
  "events": [
    {
      "id": 90311,
      "name": "Opening",
      "isParallelPresentation": false,
      "importedId": "dddca2e5-9179-48ca-97ad-4d304433f5a6",
      "typeId": 12409,
      "roomId": 10733,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1669716000000,
      "endDate": 1669717800000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 90316,
      "name": "Opening Keynote",
      "isParallelPresentation": false,
      "importedId": "e9439c68-285a-4022-ad9c-fd64bbba2e45",
      "typeId": 12409,
      "roomId": 10733,
      "chairIds": [],
      "contentIds": [
        90646
      ],
      "startDate": 1669738200000,
      "endDate": 1669743600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 90317,
      "name": "Closing Keynote",
      "isParallelPresentation": false,
      "importedId": "c3b0c1b2-c225-4d6e-9fcd-eb41af1a9f5f",
      "typeId": 12409,
      "roomId": 10733,
      "chairIds": [],
      "contentIds": [
        90648
      ],
      "startDate": 1669914000000,
      "endDate": 1669918800000,
      "description": "Rhizomatiks Behind The Scenes",
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 90321,
      "name": "Closing",
      "isParallelPresentation": false,
      "importedId": "da282928-0d63-42b9-8b9a-2262c09f562b",
      "typeId": 12409,
      "roomId": 10733,
      "chairIds": [],
      "contentIds": [],
      "startDate": 1669918800000,
      "endDate": 1669920600000,
      "presenterIds": [],
      "source": "SYS"
    },
    {
      "id": 90651,
      "name": "Japan Special Keynote",
      "isParallelPresentation": false,
      "importedId": "8af36ecd-7b94-4769-8c68-6ec2cbf81b49",
      "typeId": 12409,
      "roomId": 10733,
      "chairIds": [],
      "contentIds": [
        90647
      ],
      "startDate": 1669888800000,
      "endDate": 1669893300000,
      "presenterIds": [],
      "source": "SYS"
    }
  ],
  "contents": [
    {
      "id": 90272,
      "typeId": 12411,
      "title": "Performance Analysis of Saccades for Primary and Confirmatory Target Selection",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565619"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-9876",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90313
      ],
      "eventIds": [],
      "abstract": "In eye-gaze-based selection, dwell suffers from several issues, e.g., the Midas Touch problem. Here we investigate saccade-based selection techniques as an alternative to dwell. First, we designed a novel user interface (UI) for Actigaze and used it with (goal-crossing) saccades for confirming the selection of small targets (i.e., < 1.5-2°). We compared it with three other variants of Actigaze (with button press, dwell, and target reverse crossing) and two variants of target magnification (with button press and dwell). Magnification-dwell exhibited the most promising performance. Among Actigaze, goal-crossing was the fastest and achieved the highest throughput but suffered the most errors. We then evaluated goal-crossing as a primary selection technique for targets ≥ 2° and implemented a novel UI for such interaction. Results revealed that dwell achieved the best performance. Yet, we identified goal-crossing as a good compromise between dwell and button press. Our findings thus identify novel options for gaze-only interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts and Technology"
            }
          ],
          "personId": 90132
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Concordia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 90166
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "istanbul",
              "institution": "Kadir Has University",
              "dsl": "Mechatronics Engineering"
            }
          ],
          "personId": 90153
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 90227
        }
      ]
    },
    {
      "id": 90273,
      "typeId": 12411,
      "title": "Carousel: Improving the Accuracy of Virtual Reality Assessments for Inspection Training Tasks",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565618"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-7433",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90308
      ],
      "eventIds": [],
      "abstract": "Training simulations in virtual reality (VR) have become a focal point of both research and development due to allowing users to familiarize themselves with procedures and tasks without needing physical objects to interact with or needing to be physically present. However, the increasing popularity of VR training paradigms raises the question: Are VR-based training assessments accurate? Many VR training programs, particularly those focused on inspection tasks, employ simple pass or fail assessments. However, these types of assessments do not necessarily reflect the user's knowledge.  \r\n\r\nIn this paper, we present Carousel, a novel VR-based assessment method that requires users to actively employ their training knowledge by considering all relevant scenarios during assessments. We also present a within-subject user study that compares the accuracy of our new Carousel method to a conventional pass or fail method for a series of virtual object inspection tasks involving shapes and colors. The results of our study indicate that the Carousel method affords significantly more-accurate assessments of a user's knowledge than the binary-choice method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90229
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90200
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90241
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90250
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90135
        }
      ]
    },
    {
      "id": 90274,
      "typeId": 12411,
      "title": "Leveraging VR Techniques for Efficient Exploration and Interaction in Large and Complex AR Space with Clipped and Small FOV AR Display",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565613"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-2281",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90308
      ],
      "eventIds": [],
      "abstract": "In this paper, we propose to take advantage of the digital twinned environment to interact more efficiently in the large and complex AR space in spite of the limited sized and clipped FOV of the AR display. Using the digital twin of the target environment, “magical” VR interaction techniques can be applied, as visualized and overlaid through the small window, while still maintaining the spatial association to the augmented real world. First we consider the use of amplified movement within the corresponding VR twinned space to help the user plan, navigate and explore efficiently by providing an effectively larger view of the same AR space with less amount of physical movements. Secondly, we also apply the amplified movement and in addition, the stretchable arm to interact with relatively large objects (or largely spaced objects) which cannot be seen in their entirety at a time with the small FOV glass. The results of the experiment with the proposed methods have showed advantages with regards to the interaction performance as the scene became more complex and task more difficult. The work illustrates the concept of and potential for XR based interaction where the user can leverage on the advantages of both VR and AR mode operations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Department of Computer Science/Korea University/Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Department of Computer Science/Korea University/Digital Experience Laboratory"
            }
          ],
          "personId": 90195
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            },
            {
              "country": "Korea, Republic of",
              "state": "NA",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Digital Experience Laboratory"
            }
          ],
          "personId": 90239
        }
      ]
    },
    {
      "id": 90275,
      "typeId": 12411,
      "title": "The Relative Importance of Depth Cues and Semantic Edges for Indoor Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565620"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-1493",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90318
      ],
      "eventIds": [],
      "abstract": "Visual neuroprostheses (bionic eyes) have the potential to treat degenerative eye diseases that often result in low vision or complete blindness. These devices rely on an external camera to capture the visual scene, which is then translated frame-by-frame into an electrical stimulation pattern that is sent to the implant in the eye. To highlight more meaningful information in the scene, recent studies have tested the effectiveness of deep-learning based computer vision techniques, such as depth estimation to highlight nearby obstacles (DepthOnly mode) and semantic edge detection to outline important objects in the scene (EdgesOnly mode). However, nobody has attempted to combine the two, either by presenting them together (EdgesAndDepth) or by giving the user the ability to flexibly switch between them (EdgesOrDepth). Here, we used a neurobiologically inspired model of simulated prosthetic vision (SPV) in an immersive virtual reality (VR) environment to test the relative importance of semantic edges and relative depth cues to support the ability to avoid obstacles and identify objects. We found that participants were significantly better at avoiding obstacles using depth-based cues as opposed to relying on edge information alone, and that roughly half the participants preferred the flexibility to switch between modes (EdgesOrDepth). This study highlights the relative importance of depth cues for SPV mobility and is an important first step towards a visual neuroprosthesis that uses computer vision to improve a user's scene understanding.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90184
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Barbara",
              "institution": "University of California, Santa Barbara",
              "dsl": "Psychological & Brain Sciences"
            }
          ],
          "personId": 90265
        }
      ]
    },
    {
      "id": 90276,
      "typeId": 12411,
      "title": "Walk This Beam: Impact of Different Balance Assistance Strategies and Height Exposure on Performance and Physiological Arousal in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3567818"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-1074",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90319
      ],
      "eventIds": [],
      "abstract": "Dynamic balance is an essential skill for the human upright gait; therefore, regular balance training can improve postural control and reduce the risk of injury. Even slight variations in walking conditions like height or ground conditions can significantly impact walking performance. Virtual reality is used as a helpful tool to simulate such challenging situations. However, there is no agreement on design strategies for balance training in virtual reality under stressful environmental conditions such as height exposure. We investigate how two different training strategies, imitation learning, and gamified learning, can help dynamic balance control performance across different stress conditions. Moreover, we evaluate the stress response as indexed by peripheral physiological measures of stress, perceived workload, and user experience. Both approaches were tested against a baseline of no instructions and against each other. Thereby, we show that a learning-by-imitation approach immediately helps dynamic balance control, decreases stress, improves attention focus, and diminishes perceived workload. A gamified approach can lead to users being overwhelmed by the additional task. Finally, we discuss how our approaches could be adapted for balance training and applied to injury rehabilitation and prevention.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90251
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90154
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90124
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90137
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "PD",
              "city": "Padova",
              "institution": "University of Padova ",
              "dsl": "Department of Biomedical Sciences "
            }
          ],
          "personId": 90161
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90179
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 90264
        }
      ]
    },
    {
      "id": 90277,
      "typeId": 12411,
      "title": "Effects of Environmental Noise Levels on Patient Handoff Communication in a Mixed Reality Simulation",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565627"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6285",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90314
      ],
      "eventIds": [],
      "abstract": "When medical caregivers transfer patients to another person's care (a patient handoff), it is essential they effectively communicate the patient's condition to ensure the best possible health outcomes. Emergency situations caused by mass casualty events (e.g., natural disasters) introduce additional difficulties to handoff procedures such as environmental noise. These scenarios are expensive and difficult to train in real settings because of their unpredictable and crucial nature, so training people for them using simulation is a good fit. We created a projected mixed reality simulation of a handoff scenario involving a medical evacuation by air and tested how low, medium, and high levels of helicopter noise affected participants' handoff experience, handoff performance, and behaviors. Through a human-subjects experimental design study (N = 21), we found that the addition of noise increased participants' subjective stress and task load, decreased their self-assessed and actual performance, and caused participants to speak louder. Participants also stood closer to the virtual human sending the handoff information when listening to the handoff than they stood to the receiver when relaying the handoff information. We discuss implications for the design of handoff training simulations and avenues for future handoff communication research.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "SREAL"
            }
          ],
          "personId": 90270
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "SREAL"
            }
          ],
          "personId": 90164
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Synthetic Reality Lab"
            }
          ],
          "personId": 90247
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 90171
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "SREAL"
            }
          ],
          "personId": 90206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "ORLANDO",
              "institution": "University of Central Florida",
              "dsl": "SREAL"
            }
          ],
          "personId": 90141
        }
      ]
    },
    {
      "id": 90278,
      "typeId": 12411,
      "title": "Effect of Stereo Deficiencies on Virtual Distal Pointing ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565621"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-1696",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90312
      ],
      "eventIds": [],
      "abstract": "Previous work has shown that the mismatch between disparities and optical focus cues, known as the vergence and accommodation conflict (VAC) affects virtual hand 3D selection. To investigate if the VAC also affects the ray casting interaction technique in distal pointing, we ran a user study with an ISO 9241:411 task where participants selected 3D targets at three different VAC conditions, no VAC (i.e., targets placed roughly at the focal point of the VR headset), constant VAC (i.e. 4D distance from the user) and varying VAC (i.e., depth distance of the next target consecutively changed either 75 cm and 400 cm). According to our results varying VAC condition increases time and decreases throughput performance of the participants. Moreover, it takes longer for users to select target at constant VAC condition. Our results show that placing objects at different depth planes has detrimental effect on the user performance in distal pointing.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Concordia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 90166
        },
        {
          "affiliations": [
            {
              "country": "Turkey",
              "state": "Istanbul",
              "city": "istanbul",
              "institution": "Kadir Has University",
              "dsl": "Mechatronics Engineering"
            }
          ],
          "personId": 90153
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Nova Scotia",
              "city": "Halifax",
              "institution": "Dalhousie University",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 90235
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "Simon Fraser University",
              "dsl": "School of Interactive Arts + Technology (SIAT)"
            }
          ],
          "personId": 90227
        }
      ]
    },
    {
      "id": 90279,
      "typeId": 12411,
      "durationOverride": 15,
      "title": "Research on the Emotions Expressed by the Posture of Kemo-mimi",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565610"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3656",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90308
      ],
      "eventIds": [],
      "abstract": " Kemo-mimi means the dog- or cat-like ears on a humanoid character, or the ears of the animal itself.\r\nKemo-mimi is often used as an element of the avatar's appearance.\r\nIt is generally considered that the posture of animal ears represents the animal's emotional state.\r\nAnd the idea has been used as a technique for expressing emotions in many cartoon and animation works.\r\nBut despite this fact, there are few examples of studies on the emotions that can be expressed by animal ears.\r\nTherefore, we decided to investigate the relationship between the posture of the animal ears and emotions and to establish a method of expressing emotions using the ears.\r\nIn the experiments, three-dimensional animations of animal ears changing posture were presented to the subjects, and they were asked to answer the emotion corresponding to the posture.\r\nThe results showed that there was a certain degree of a common understanding of people's impressions concerning the animal ears.\r\nIn this paper, we report the emotions that can be expressed by the posture of the animal ear as revealed in this study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Electro-Communications",
              "dsl": "Graduate School of Informatics and Engineering"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Electro-Communications",
              "dsl": "Graduate School of Informatics and Engineering"
            }
          ],
          "personId": 90177
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chofu",
              "institution": "The University of Electro-Communications",
              "dsl": "Graduate School of Informatics and Engineering"
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chofu",
              "institution": "The University of Electro-Communications",
              "dsl": "Graduate School of Informatics and Engineering"
            }
          ],
          "personId": 90165
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chofu",
              "institution": "The University of Electro-Communications",
              "dsl": "Faculty of Informatics and Engineering"
            },
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Chofu",
              "institution": "The University of Electro-Communications",
              "dsl": "Faculty of Informatics and Engineering"
            }
          ],
          "personId": 90199
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Electro-Communications",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "University of Electro-Communications",
              "dsl": ""
            }
          ],
          "personId": 90238
        }
      ]
    },
    {
      "id": 90280,
      "typeId": 12411,
      "title": "Rich virtual feedback from sensorimotor interaction may harm, not help, learning in immersive virtual reality ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565633"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6669",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90312
      ],
      "eventIds": [],
      "abstract": "A major distinction between sensorimotor interactions in the physical world and those in immersive virtual reality (IVR) is the feedback users receive when conducting an action. Actions in the physical world almost always offer multimodal feedback. For example, pouring out a jug of water offers tactile (weight-change), aural (the sound of water hitting a surface) and visual (water running out the spout) feedback. Feedback from pouring a virtual jug, however, depends on how that specific system was designed. This study examines if the richness of feedback from IVR actions causes a detectable cognitive impact on users. We compared verb-learning outcomes between two conditions: (1) where participants' can make actions with objects but no audiovisual feedback is presented; and (2) where participants' actions with objects provide additional audiovisual feedback. We found that users have cognitively distinct outcomes in IVR based on the type of audiovisual feedback experienced, with a high feedback experience harming learning outcomes compared with a low feedback one. This result has implications for IVR system design and theories of cognition and memorisation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": ""
            }
          ],
          "personId": 90191
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "London",
              "city": "London",
              "institution": "Queen Mary University of London",
              "dsl": "EECS"
            }
          ],
          "personId": 90192
        }
      ]
    },
    {
      "id": 90281,
      "typeId": 12411,
      "title": "Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object Augmented Reality Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565637"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3536",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90313
      ],
      "eventIds": [],
      "abstract": "As augmented reality technology and hardware become more mature and affordable, researchers have been exploring more intuitive and discoverable interaction techniques for immersive environments. In this paper, we investigate multimodal interaction for 3D object manipulation in a multi-object virtual environment. To identify the user-defined gestures, we conducted an elicitation study involving 24 participants for 22 referents with an augmented reality headset. It yielded 528 proposals and generated a winning gesture set with 25 gestures after binning and ranking all gesture proposals. We found that for the same task, the same gesture was preferred for both one and two object manipulation, although both hands were used in the two object scenario. We presented the gestures and speech results, and the differences compared to similar studies in a single object virtual environment. The study also explored the association between speech expressions and gesture stroke during object manipulation, which could improve the recognizer efficiency in augmented reality headsets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Fort Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Fort Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90134
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Fort Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Fort Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90224
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Ft. Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "Colorado",
              "city": "Ft. Collins",
              "institution": "Colorado State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90142
        }
      ]
    },
    {
      "id": 90282,
      "typeId": 12411,
      "title": "\"Kapow!\": Studying the Design of Visual Feedback for Representing Contacts in Extended Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565607"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-2045",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90320
      ],
      "eventIds": [],
      "abstract": "In absence of haptic feedback, the perception of contact with virtual objects can rapidly become a problem in extended reality (XR) applications. XR developers often rely on visual feedback to inform the user and display contact information. However, as for today, there is no clear path on how to design and assess such visual techniques. In this paper, we propose a design space for the creation of visual feedback techniques meant to represent contact with virtual surfaces in XR. Based on this design space, we conceived a set of various visual techniques, including novel approaches based on onomatopoeia and inspired by cartoons, or visual effects based on physical phenomena. Then, we conducted an online user study with 60 participants, consisting in assessing 6 visual feedback techniques in terms of user experience. We could notably assess, for the first time, the potential influence of the interaction context by comparing the participants' answers in two different scenarios: industrial (serious) versus entertainment (ludic) conditions. Taken together, our design space and initial results could inspire XR developers for a wide range of applications in which the augmentation of contact seems prominent, such as for vocational training, industrial assembly/maintenance, surgical simulation, videogames, etc.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Laval",
              "institution": "Arts et Métiers Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 90233
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria Rennes",
              "dsl": ""
            }
          ],
          "personId": 90237
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Université de Lille",
              "dsl": "UMR 9189 - CRIStAL"
            },
            {
              "country": "France",
              "state": "",
              "city": "Lille",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 90130
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Nantes",
              "institution": "Ecole Centrale de Nantes",
              "dsl": "AAU UMR CNRS 1563 & Inria Hybrid"
            }
          ],
          "personId": 90217
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 90123
        }
      ]
    },
    {
      "id": 90283,
      "typeId": 12411,
      "title": "VCPoser: Interactive Pose Generation of Virtual Characters Corresponding to Human Pose Input",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565640"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-7679",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90309
      ],
      "eventIds": [],
      "abstract": "Virtual characters (VCs) play a significant role in the entertainment industry, and AI-driven VCs are being developed to enable interaction with users. People are attracted to these VCs, resulting in a demand for them to co-exist in the same world. An approach to allow recording of the memories with the VCs is to capture videos or photos with them, where users are usually required to adapt their poses to the pre-rendered VC's action. To allow a more seamless collaboration with VCs in photography scenarios, we propose VCPoser, which enables VCs to adapt their pose to the pose of the user. We created a deep neural network-based system that predicts a VC's pose using the user's pose data as input by learning the paired pose data. Our quantitative evaluations and user studies demonstrate that our system can predict and generate poses of VCs and allow them to be combined next to the posing user in a photo. We also provide an analysis of the human mindsets of paired poses for a better understanding of them and to share insights for aesthetic pose design. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Bandai Namco Research Inc.",
              "dsl": ""
            }
          ],
          "personId": 90244
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Bandai Namco Research Inc.",
              "dsl": ""
            }
          ],
          "personId": 90240
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Koto-ku",
              "institution": "Bandai Namco Research Inc.",
              "dsl": ""
            }
          ],
          "personId": 90126
        }
      ]
    },
    {
      "id": 90284,
      "typeId": 12411,
      "title": "Adaptive Field-of-view Restriction: Limiting Optical Flow to Mitigate Cybersickness in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565611"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6621",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90318
      ],
      "eventIds": [],
      "abstract": "Dynamic field-of-view (FOV) restriction is a widely used software technique to mitigate cybersickness in commercial virtual reality (VR) applications. The classical FOV restrictor is implemented using a symmetric mask that occludes the periphery in response to translational and/or angular velocity of virtual motion.   In this paper, we introduce adaptive field-of-view restriction, a novel technique that responds dynamically based on real-time assessment of the optical flow generated by movement through a virtual environment.  The adaptive restrictor utilizes an asymmetric mask to obscure regions of the periphery with higher optical flow during virtual locomotion while leaving regions with lower optical flow visible.  To evaluate the proposed technique, we conducted a gender-balanced user study (N = 38) in which participants completed in a navigation task in two different types of virtual scenes using controller-based locomotion.  Participants were instructed to navigate through either close-quarter or open virtual environments using adaptive restriction, traditional symmetric restriction, or an unrestricted control condition in three virtual reality sessions separated by at least 24 hours.  The results showed that the adaptive restrictor was effective in mitigating cybersickness and reducing subjective discomfort, while simultaneously enabling participants to remain immersed for a longer amount of time compared to the control condition.  Additionally, presence ratings were significantly higher when using the adaptive restrictor compared to symmetric FOV restriction.  In general, these results suggest that adaptive field-of-view restriction based on real-time measurement of optical flow is a promising new approach for virtual reality applications that seek to provide a better cost-benefit tradeoff between user comfort and a high-fidelity experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            },
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            }
          ],
          "personId": 90202
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            },
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "Department of Computer Science & Engineering"
            }
          ],
          "personId": 90160
        }
      ]
    },
    {
      "id": 90285,
      "typeId": 12411,
      "title": "VR Games for Managing Chronic Pain Relief",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565624"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-5279",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90308
      ],
      "eventIds": [],
      "abstract": "Chronic pain is a continuous ailment lasting for long periods after the initial injury or disease has healed. Chronic pain is challenging to treat and affects the daily lives of patients. Distraction therapy is a proven method of relieving patients' discomfort by taking their attention away from the pain. Virtual reality (VR) is a platform for distraction therapy by immersing the user in a virtual world detached from reality. However, there is little research on how physical interactions in VR affect pain management. We present a study to evaluate different VR interactions for chronic pain patients to determine which are most effective for pain relief. Our results indicate that physical and mental activities in VR are equally effective at reducing pain. These actively engage patients, while the effects of observing relaxing content persist outside of VR. These findings can inform the design of future VR games targeted at pain management.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "School of Engineering and Computer Science"
            }
          ],
          "personId": 90249
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": "School of Engineering and Computer Science"
            }
          ],
          "personId": 90193
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Wellington",
              "institution": "Victoria Univeristy of Wellington",
              "dsl": "School of Engineering and Computer Science"
            }
          ],
          "personId": 90174
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Wellington",
              "city": "Wellington",
              "institution": "Victoria University of Wellington",
              "dsl": ""
            }
          ],
          "personId": 90230
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 90219
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Lisboa",
              "institution": "INESC-ID",
              "dsl": "DEI/IST/U Lisboa"
            }
          ],
          "personId": 90268
        }
      ]
    },
    {
      "id": 90286,
      "typeId": 12411,
      "title": "Design and Evaluation of Electrotactile Rendering Effects for Finger-Based Interactions in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565634"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-4149",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90320
      ],
      "eventIds": [],
      "abstract": "The use of electrotactile feedback in Virtual Reality (VR) has shown promising results for providing tactile information and sensations. While progress has been made to provide custom electrotactile feedback for specific interaction tasks, it remains unclear which modulations and rendering algorithms are preferred in rich interaction scenarios. In this paper, we propose a unified tactile rendering architecture and explore the most promising modulations to render finger interactions in VR.  Based on a literature review, we designed six electrotactile stimulation patterns/effects (EFXs) striving to render different tactile sensations. In a user study (N=18), we assessed the six EFXs in three diverse finger interactions: 1) tapping on a virtual object; 2) pressing down a virtual button; 3) sliding along a virtual surface. Results showed that the preference for certain EFXs depends on the task at hand. No significant preference was detected for tapping (short and quick contact); EFXs that render dynamic intensities or dynamic spatio-temporal patterns were preferred for pressing (continuous dynamic force); EFXs that render moving sensations were preferred for sliding (surface exploration). The results showed the importance of the coherence between the modulation an the interaction being performed and the study proved the versatility of electrotactile feedback and its efficiency in rendering different haptic information and sensations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria Rennes",
              "dsl": ""
            }
          ],
          "personId": 90143
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 90181
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria",
              "dsl": ""
            }
          ],
          "personId": 90266
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "CNRS",
              "dsl": "IRISA"
            }
          ],
          "personId": 90131
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Univ. Rennes, INSA, IRISA, Inria",
              "dsl": ""
            }
          ],
          "personId": 90261
        }
      ]
    },
    {
      "id": 90287,
      "typeId": 12411,
      "title": "Sweating Avatars Decrease Perceived Exertion and Increase Perceived Endurance while Cycling in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565628"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6865",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90318
      ],
      "eventIds": [],
      "abstract": "Avatars are used to represent users in virtual reality (VR) and create embodied experiences. Previous work showed that avatars' stereotypical appearance can affect users' physical performance and perceived exertion while exercising in VR. Although sweating is a natural human response to physical effort, surprisingly little is known about the effects of sweating avatars on users. Therefore, we conducted a study with 24 participants to explore the effects of sweating avatars while cycling in VR. We found that visualizing sweat decreases the perceived exertion and increases perceived endurance. Thus, users feel less exerted while embodying sweating avatars. We conclude that sweating avatars contribute to more effective exergames and fitness applications.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 90205
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media informatics group"
            }
          ],
          "personId": 90253
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media Informatics Group"
            }
          ],
          "personId": 90222
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media Informatics Group"
            }
          ],
          "personId": 90204
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media Informatics Group"
            }
          ],
          "personId": 90214
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 90139
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 90149
        }
      ]
    },
    {
      "id": 90288,
      "typeId": 12411,
      "title": "Understanding Perspectives for Single- and Multi-Limb Movement Guidance in Virtual 3D Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565635"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3939",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90320
      ],
      "eventIds": [],
      "abstract": "Movement guidance in virtual reality has many applications ranging from physical therapy, assistive systems to sport learning. These movements range from simple single-limb to complex multi-limb movements. While VR supports many perspectives -- e.g., first person and third person -- it remains unclear how accurate these perspectives communicate different movements. In a user study (N=18), we investigated the influence of perspective, feedback, and movement properties on the accuracy of movement guidance. Participants had on average an angle error of 6.2° for single arm movements, 7.4° for synchronous two arm movements, and 10.3° for synchronous two arm and leg movements. Furthermore, the results show that the two variants of third-person perspectives outperform a first-person perspective for movement guidance (19.9% and 24.3% reduction in angle errors). Qualitative feedback confirms the quantitative data and show users have a clear preference for third-person perspectives. Through our findings we provide guidance for designers and developers of future VR movement guidance systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 90194
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 90196
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 90182
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Saarland Informatics Campus"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "Technical University of Darmstadt",
              "dsl": ""
            }
          ],
          "personId": 90257
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Darmstadt",
              "institution": "TU Darmstadt",
              "dsl": "Telecooperation Lab"
            }
          ],
          "personId": 90234
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Giessen",
              "institution": "Technische Hochschule Mittelhessen",
              "dsl": ""
            }
          ],
          "personId": 90246
        }
      ]
    },
    {
      "id": 90289,
      "typeId": 12411,
      "title": "Nebula: An Affordable Open-Source and Autonomous Olfactory Display for VR Headsets",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565617"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3913",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90315
      ],
      "eventIds": [],
      "abstract": "The impact of olfactory cues on user experience in virtual reality is increasingly studied. However, results are still heterogeneous and existing studies difficult to replicate, mainly due to a lack of standardized olfactory displays. In that context, we present Nebula, a low-cost, open-source, olfactory display capable of diffusing scents at different diffusion rates using a nebulization process. Nebula can be used with PC VR or autonomous head-mounted displays, making it easily transportable without the need for an external computer. The device was calibrated to diffuse at three diffusion rates: no diffusion, low and high. For each level, the quantity of delivered odor was precisely characterized using a repeated weighting method. The corresponding perceived olfactory intensities were evaluated by a psychophysical experiment on sixteen participants. Results demonstrated the device capability to successfully create three significantly different perceived odor intensities (Friedman test p < 10^{-6}, Wilcoxon tests p_{adj} < 10^{-4}), without noticeable smell persistence and with limited noise and discomfort. For reproducibility and to stimulate further research in the area, 3D printing files,  electronic hardware schemes, and firmware/software source-code are made publicly available.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Etienne",
              "institution": "Centrale Lyon, Univ Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR5205, ENISE",
              "dsl": ""
            }
          ],
          "personId": 90201
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Etienne",
              "institution": "Centrale Lyon, ENISE",
              "dsl": ""
            }
          ],
          "personId": 90231
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Ecully",
              "institution": "Institut Paul Bocuse Research Center, UNIQUE Center, the Quebec Neuro-AI research center",
              "dsl": ""
            }
          ],
          "personId": 90190
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Etienne",
              "institution": "Centrale Lyon, Univ Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR5205, ENISE",
              "dsl": ""
            }
          ],
          "personId": 90263
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Saint-Etienne",
              "institution": "Centrale Lyon, Univ Lyon, CNRS, INSA Lyon, UCBL, LIRIS, UMR5205, ENISE",
              "dsl": ""
            }
          ],
          "personId": 90188
        }
      ]
    },
    {
      "id": 90290,
      "typeId": 12411,
      "title": "Virtual Air Conditioner’s Airflow Simulation and Visualization in AR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565615"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-2392",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90315
      ],
      "eventIds": [],
      "abstract": "This paper presents a mobile AR system for visualizing airflow and temperature change made by virtual air conditioners. Even though there have been efforts to integrate the results of airflow/temperature simulation into the real world via AR, they support neither interactive  modeling of the environments nor real-time simulation. This paper presents a fully-interactive AR system, where 3D mapping, installing air conditioners, simulating airflow and temperature change, and visualizing the simulation results are all made at real time. The proposed system is designed in a client-server architecture, where the server is in charge of simulation and the rest is taken by the client.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": ""
            }
          ],
          "personId": 90215
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": ""
            }
          ],
          "personId": 90209
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": ""
            }
          ],
          "personId": 90228
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": ""
            }
          ],
          "personId": 90180
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 90178
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "High-performance Visual Computing Lab (HVCL)"
            }
          ],
          "personId": 90170
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 90168
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 90128
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "LG Electronics",
              "dsl": ""
            }
          ],
          "personId": 90207
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "LG Electronics",
              "dsl": ""
            }
          ],
          "personId": 90187
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Korea University",
              "dsl": "Media Laboratory"
            }
          ],
          "personId": 90210
        }
      ]
    },
    {
      "id": 90291,
      "typeId": 12411,
      "title": "PORTAL: Portal Widget for Remote Target Acquisition and Control in Immersive Virtual Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565639"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-9886",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90313
      ],
      "eventIds": [],
      "abstract": "This paper introduces PORTAL (POrtal widget for Remote Target Acquisition and controL) that allows the user to interact with out-of-reach objects in a virtual environment. We described the PORTAL interaction technique for placing a portal widget and interacting with target objects through the portal. We conducted two formal user studies to evaluate PORTAL for selection and manipulation functionalities. The results show PORTAL supports participants to interact with remote objects successfully and precisely. Following that, we discuss its potential and limitations, and future works.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Hyrum",
              "institution": "Utah State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90147
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Logan",
              "institution": "Utah State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90226
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Utah",
              "city": "Logan",
              "institution": "Utah State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90216
        }
      ]
    },
    {
      "id": 90292,
      "typeId": 12411,
      "durationOverride": 15,
      "title": "Assessment of Instructor's Capacity in One-to-Many AR Remote Instruction Giving",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565631"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-4011",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90309
      ],
      "eventIds": [],
      "abstract": "In this study, we focused on one-to-many remote collaboration, which is becoming one of the essential topics in CSCW. Such remote collaboration requires more mental resources from the remote instructor compared to the case of one-to-one since it is \"multitasking\". The main contribution of our study is that we assessed instructor’s capacity in one-to-many AR remote instruction giving both subjectively and objectively. We compared the remote instructor's workload while interacting with a different number of local workers, assuming tasks at an industrial site. The results showed that the remote instructors perceived stronger workload and the communication quality became lower when interacting with multiple local workers. Based on the results, we discussed how to support the remote instructor in a one-to-many AR remote collaboration system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 90186
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": "School of Knowledge Science"
            }
          ],
          "personId": 90260
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Bunkyo-ku",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 90198
        }
      ]
    },
    {
      "id": 90293,
      "typeId": 12411,
      "title": "The Rubber Hand Illusion in Virtual Reality and the Real World - Comparable but Different",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565614"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6334",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90319
      ],
      "eventIds": [],
      "abstract": "Feeling ownership of a virtual body is crucial for immersive experiences in VR. Knowledge about body ownership is mainly based on rubber hand illusion (RHI) experiments in the real world. Watching a rubber hand being stroked while one’s own hidden hand is synchronously stroked, humans experience the rubber hand as their own hand and underestimate the distance between the rubber hand and the real hand (proprioceptive drift). There is also evidence for a decrease in hand temperature. Although the RHI has been induced in VR, it is unknown whether effects in VR and the real world differ. We conducted a RHI experiment with 24 participants in the real world and in VR and found comparable effects in both environments. However, irrespective of the RHI, proprioceptive drift and temperature differences varied between settings. Our findings validate the utilization of the RHI in VR to increase our understanding of embodying virtual avatars.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 90205
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 90133
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media informatics group"
            }
          ],
          "personId": 90253
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": ""
            }
          ],
          "personId": 90139
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Regensburg",
              "institution": "University of Regensburg",
              "dsl": "Media Informatics Group"
            }
          ],
          "personId": 90140
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Frankfurt",
              "institution": "Frankfurt University of Applied Sciences",
              "dsl": ""
            }
          ],
          "personId": 90149
        }
      ]
    },
    {
      "id": 90294,
      "typeId": 12411,
      "title": "Evaluating the Effects of Virtual Human Animation on Students in an Immersive VR Classroom Using Eye Movements",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565623"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-9489",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90310
      ],
      "eventIds": [],
      "abstract": "Virtual humans presented in VR learning environments have been suggested in previous research to increase immersion and further positively influence learning outcomes. However, how virtual human animations affect students' real-time behavior during VR learning has not yet been investigated. This work examines the effects of social animations (i.e., hand raising of virtual peer learners) on students' cognitive response and visual attention behavior during immersion in a VR classroom based on eye movement analysis. Our results show that animated peers that are designed to enhance immersion and provide companionship and social information elicit different responses in students (i.e., cognitive, visual attention, and visual search responses), as reflected in various eye movement metrics such as pupil diameter, fixations, saccades, and dwell times. Furthermore, our results show that the effects of animations on students differ significantly between conditions (20%, 35%, 65%, and 80% of virtual peer learners raising their hands). Our research provides a methodological foundation for investigating the effects of avatar animations on users, further suggesting that such effects should be considered by developers when implementing animated virtual humans in VR. Our findings have important implications for future works on the design of more effective, immersive, and authentic VR environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": ""
            }
          ],
          "personId": 90144
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": ""
            }
          ],
          "personId": 90232
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": "Chair for Human-Computer Interaction"
            }
          ],
          "personId": 90203
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Tübingen",
              "institution": "University of Tübingen",
              "dsl": ""
            }
          ],
          "personId": 90138
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Technical University of Munich",
              "dsl": ""
            }
          ],
          "personId": 90155
        }
      ]
    },
    {
      "id": 90295,
      "typeId": 12411,
      "title": "Automated Blendshape Personalization for Faithful Face Animations Using Commodity Smartphones",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565622"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-2571",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90314
      ],
      "eventIds": [],
      "abstract": "A faithful digital reconstruction of humans has various interesting use-cases. Animated virtual humans, avatars and agents alike, are the central entities in virtual embodied human-computer and human-human encounters in social XR. Here, a faithful reconstruction of facial expressions becomes paramount due to their prominent role in non-verbal behavior and social interaction, and current XR-platforms like Unity 3D or the Unreal Engine already integrate recent smartphone technologies to animate faces of virtual humans. Using the same technology, this article presents an optimization-based approach to generate personalized blendshapes as animation targets for facial expressions. The proposed method combines a position-based optimization with a seamless partial deformation transfer, necessary for a faithful reconstruction. Our method is fully automated and significantly outperforms existing solutions based on example-based facial rigging or deformation transfer, and overall results in a much lower reconstruction error. It also neatly integrates with recent smartphone-based reconstruction pipelines for mesh generation and automated rigging, further paving the way to a widespread application of human-like and personalized avatars and agents in various use-cases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 90223
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dortmund",
              "institution": "TU Dortmund University",
              "dsl": "Computer Graphics Group"
            }
          ],
          "personId": 90148
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg",
              "institution": "Department of Computer Science, HCI Group",
              "dsl": "University of Würzburg"
            }
          ],
          "personId": 90156
        }
      ]
    },
    {
      "id": 90296,
      "typeId": 12411,
      "title": "Puppeteer: Exploring Intuitive Hand Gestures and Upper-Body Postures for Manipulating Human Avatar Actions",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565609"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-5481",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90312
      ],
      "eventIds": [],
      "abstract": "Body-controlled avatars provide a more intuitive method to real-time control virtual avatars but require larger environment space and more user effort. In contrast, hand-controlled avatars give more dexterous and fewer fatigue manipulations within a close-range space for avatar control but provide fewer sensory cues than the body-based method.\r\nThis paper investigates the differences between the two manipulations and explores the possibility of a combination. We first performed a formative study to understand when and how users prefer manipulating hands and bodies to represent avatars' actions in current popular video games. Based on the survey of the top video games, we decided to represent human avatars' motions. Besides, we found that players used their bodies to represent avatar actions but changed to using hands when they were too unrealistic and exaggerated to mimic by bodies (e.g., flying in the sky, rolling over quickly). Hand gestures also provide an alternative to lower-body motions when players want to sit during gaming and do not want extensive effort to move their avatars. Hence, we focused on the design of hand gestures and upper-body postures. We present Puppeteer, an input prototype system that allows players directly control their avatars through intuitive hand gestures and upper-body postures. We selected 17 avatar actions discovered in the formative study and conducted a gesture elicitation study to invite 12 participants to design best representing hand gestures and upper-body postures for each action.\r\nThen we implemented a prototype system using the MediaPipe framework to detect keypoints and a self-trained model to recognize 17 hand gestures and 17 upper-body postures. Finally, three applications demonstrate the interactions enabled by Puppeteer.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Graduate Institute of Networking and Multimedia"
            }
          ],
          "personId": 90218
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer science and engineering"
            }
          ],
          "personId": 90175
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Graduate Institute of Networking and Multimedia"
            }
          ],
          "personId": 90208
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90146
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Hsinchu",
              "institution": "National Chiao Tung University",
              "dsl": ""
            }
          ],
          "personId": 90129
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "National Taiwan University",
              "dsl": ""
            }
          ],
          "personId": 90157
        }
      ]
    },
    {
      "id": 90297,
      "typeId": 12411,
      "title": "Exploration of Form Factor and Bimanual 3D Manipulation Performance of Rollable In-hand VR Controller",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565625"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-9585",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90312
      ],
      "eventIds": [],
      "abstract": "Virtual reality (VR) environments are expected to be a future workspace. An effective bi-manual 3D manipulation technique would be essential to support this. A ball-shaped tangible input device that can be rolled in a hand is known to be useful for 3D object manipulation since users can utilize their finger dexterity. In this paper, we comprehensively explored this rollable in-hand controller. First, we evaluated the effect of the form factor on the rollable controller. Although the size and shape of the controller may influence user behavior and performance, the effect of them has not been clarified yet. Next, we evaluated the rollable controllers in bi-manual 3D assembly tasks. Rollable controllers may require a high cognitive load by utilizing finger dexterity, so the benefit of using them with two hands is not clear. We found that the 5cm-sized ball-shaped controller is the most effective and that the in-hand rollable controllers show significantly faster completion time than VR controllers in complex assembly tasks involving numerous rotations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 90262
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "HCI Lab, School of Computing"
            }
          ],
          "personId": 90173
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": "HCI Lab"
            }
          ],
          "personId": 90221
        }
      ]
    },
    {
      "id": 90298,
      "typeId": 12411,
      "title": "VISTA: User-centered VR Training System for Effectively Deriving Characteristics of People with Autism Spectrum Disorder",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565608"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3480",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90309
      ],
      "eventIds": [],
      "abstract": "Pervasive symptoms of autistic people, such as a lack of social and communication skills, are major challenges to be embraced in the workplace. Although much research has proposed VR training programs, their effectiveness is somewhat unclear, since they provide limited, one-sided interactions through fixed scenarios or do not sufficiently reflect the characteristics of autistic people (e.g., preference for predictable interfaces, sensory issues). In this paper, we present VISTA, a VR-based interactive social skill training system for autistic people. We ran a user study with 10 autistic people and 10 neurotypical people to evaluate user experience in VR training and to examine the characteristics of autistic people based on their physical responses generated by sensor data. The results showed that autistic participants were highly engaged with VISTA and improved self-efficacy after experiencing VISTA. The two groups showed significant differences in sensor signals as the task complexity increased, which demonstrates the importance of considering task complexity in eliciting the characteristics of autistic people in VR training. Our findings not only extend findings (e.g., low ROI ratio, EDA increase) in previous studies but also provide new insights (e.g., large variation of pupil diameter, high utterance rate), broadening our quantitative understanding of autistic people.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "School of Intelligence Computing"
            }
          ],
          "personId": 90122
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 90163
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 90136
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence,"
            }
          ],
          "personId": 90167
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul Dasiseogi Homless Support Center ",
              "dsl": "Medical division"
            }
          ],
          "personId": 90172
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Department of Industrial Design"
            }
          ],
          "personId": 90254
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Seoul National University",
              "dsl": "Department of Communication"
            }
          ],
          "personId": 90183
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seongnam",
              "institution": "Seoul National University Bundang Hospital",
              "dsl": ""
            }
          ],
          "personId": 90236
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "School of Interactive Computing"
            }
          ],
          "personId": 90158
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Deajeon",
              "institution": "KAIST",
              "dsl": ""
            }
          ],
          "personId": 90176
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Data Science"
            }
          ],
          "personId": 90271
        }
      ]
    },
    {
      "id": 90299,
      "typeId": 12411,
      "title": "Timeline Design Space for Immersive Exploration of Time-Varying Spatial 3D Data",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565612"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-2151",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90314
      ],
      "eventIds": [],
      "abstract": "Timelines are common visualizations to represent and manipulate temporal data, from historical events storytelling to animation authoring. However, timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or volumetric models) directly. In this paper, leveraging the increased workspace and 3D interaction capabilities of virtual reality (VR), we propose to use timelines for the visualization of 3D temporal data to support exploration and analysis. First, we propose a timeline design space for 3D temporal data extending the timeline design space proposed by Brehmer et al. The proposed design space adapts the scale, layout and representation dimensions to account for the depth dimension and how the 3D temporal data can be partitioned and structured. In our approach, an additional dimension is introduced, the support, which further characterizes the 3D dimension of the visualization. To complement the design space and the interaction capabilities of VR systems, we discuss the interaction methods required for the efficient visualization of 3D timelines. Then, we evaluate the benefits of 3D timelines through a formal evaluation (n=21). Our results showed that time-related tasks can be achieved more comfortably using timelines, and more efficiently for specific tasks requiring the analysis of the surrounding temporal context. Though the comparison between different timeline designs were inconclusive, participants reported a clear preference towards timeline designs that did not occupy the vertical space. Finally, we illustrate the use of the 3D timelines to a real use-case on the analysis of biological 3D temporal datasets in which domain experts in cell imaging were involved in the design and evaluation process.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria, Univ. Rennes, IRISA, CNRS",
              "dsl": ""
            }
          ],
          "personId": 90189
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria, Univ. Rennes, IRISA, CNRS",
              "dsl": ""
            }
          ],
          "personId": 90266
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Montpellier",
              "institution": "LIRMM, Univ Montpellier, CNRS",
              "dsl": ""
            }
          ],
          "personId": 90258
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Rennes",
              "institution": "Inria de l'Université de Rennes, UMR144 CNRS Institut Curie, PSL Research University, Sorbonne Universités",
              "dsl": ""
            }
          ],
          "personId": 90225
        }
      ]
    },
    {
      "id": 90300,
      "typeId": 12411,
      "durationOverride": 15,
      "title": "NeARportation: A Remote Real-time Neural Rendering Framework",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565616"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-1585",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90314
      ],
      "eventIds": [],
      "abstract": "While the presentation of photo-realistic appearance plays a major role in immersion in an augmented virtuality environment, displaying the photo-realistic appearance of real objects remains a challenging problem. Recent developments in photogrammetry have facilitated the incorporation of real objects into virtual space. However, photo-realistic photogrammetry requires a dedicated measurement environment, and there is a trade-off between measurement cost and quality. Furthermore, even with photo-realistic appearance measurements, there is a trade-off between rendering quality and framerate. There is no framework that could resolve these trade-offs and easily provide a photo-realistic appearance in real-time.\r\n\r\nOur NeARportation framework combines server-client bidirectional communication and neural rendering to resolve these trade-offs. Neural rendering on the server receives the client's head posture and generates a novel-view image with realistic appearance reproduction, which is streamed onto the client's display. By applying our framework to a stereoscopic display, we confirmed that it could display a high-fidelity appearance on full-HD stereo videos at 35-40 frames-per-second (fps), according to the user's head motion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 90243
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "RIKEN",
              "dsl": "AIP"
            }
          ],
          "personId": 90125
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Sony CSL Kyoto",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 90159
        }
      ]
    },
    {
      "id": 90301,
      "typeId": 12411,
      "title": " Marcus or Mira - Investigating the effect of gender in virtualreality role play training ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565629"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-8419",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90310
      ],
      "eventIds": [],
      "abstract": "Immersive virtual training environments are used in various domains. In this work, we focus on role-play training in virtual reality. In virtual role-play, training conversations and interactions with non-player characters (NPCs) are often fundamental to the training. Therefore, the appearance and behavior of the NPCs play an important role when designing role-play training. We focus on the gender appearance of NPCs, as gender is an important aspect of differentiation between characters. We conducted a study with 40 participants in which we investigated how NPCs gender appearance influences the perception of NPC personality traits and the self-perception of a participants’ assumed role in training for social skills. This work contributes towards understanding the design-space of NPC design, NPC gender identity, and the relation to the design and development of immersive virtual reality role-play training.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna ",
              "institution": "AIT Austrian Institute of Technology GmbH",
              "dsl": "Center for Technology Experience"
            }
          ],
          "personId": 90220
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Austrian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 90256
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Institute for Advanced Studies",
              "dsl": "Science, technology and social transformation"
            }
          ],
          "personId": 90127
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Austrian Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 90151
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Institute for Advanced Studies",
              "dsl": "Science, Technology and Social Transformation"
            }
          ],
          "personId": 90267
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "Institute for Advanced Studies",
              "dsl": "Science, Technology and Social Transformation"
            }
          ],
          "personId": 90162
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "AIT Austrian Institute of Technology",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "University of Vienna",
              "dsl": ""
            }
          ],
          "personId": 90145
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Salzburg & Vienna",
              "institution": "University of Salzburg & AIT",
              "dsl": ""
            }
          ],
          "personId": 90255
        }
      ]
    },
    {
      "id": 90302,
      "typeId": 12411,
      "title": "Precueing Sequential Rotation Tasks in Augmented Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565641"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-6236",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90313
      ],
      "eventIds": [],
      "abstract": "Augmented reality has been used to improve sequential-task performance by cueing information about a current task step and precueing information about future steps. Existing work has shown the benefits of precueing movement (translation) information. However, rotation is also a major component in many real-life tasks, such as turning knobs to adjust parameters on a console. We developed an AR testbed to investigate whether and how much precued rotation information can improve user performance. We consider two unimanual tasks: one requires a user to make sequential rotations of a single object, and the other requires the user to move their hand between multiple objects to rotate them in sequence. \r\n\r\nWe conducted a user study to explore these two tasks using circular arrows to communicate rotation. In the single-object task, we examined the impact of number of precues and visualization style on user performance. Results show that precues improved performance and that arrows with highlighted heads and tails, with each destination aligned with the next origin, yielded the shortest completion time on average. In the multiple-object task, we explored whether rotation precues can be helpful in conjunction with movement precues. Here, using a rotation cue without rotation precues in conjunction with a movement cue and movement precues performed the best, implying that rotation precues were not helpful for study participants in this task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90212
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia Teachers College",
              "dsl": "Cognitive Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Psychology"
            }
          ],
          "personId": 90152
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90248
        }
      ]
    },
    {
      "id": 90303,
      "typeId": 12411,
      "title": "Exploring User Behaviour in Asymmetric Collaborative Mixed Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565630"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3500",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90309
      ],
      "eventIds": [],
      "abstract": "A key issue for collaborative mixed reality is the asymmetry of interaction with the shared virtual environment. For example, an augmented reality (AR) user might use one type of head-mounted display (HMD) in a physical environment, while a virtual reality (VR) user might wear a different type of HMD and see a virtual model of that physical environment. To investigate the effects of such asymmetric interfaces on collaboration we present a study that investigates the behavior of pairs of users performing a word puzzle task where one uses AR and one VR. We examined the collaborative process through questionnaires and behavioral measures based on recorded positional and audio data. We identified relationships between presence and co-presence, accord and co-presence, leadership and talkativeness, head rotation velocity and leadership as well as talkativeness. We did not find that the AR or VR biased subjective responses, though there were interesting behavioral differences: AR users spoke more words, AR users had a higher median head rotation velocity, and VR users traveled further.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90259
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "University College London",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90213
        }
      ]
    },
    {
      "id": 90304,
      "typeId": 12411,
      "title": "3D Reconstruction of Sculptures from Single Images via Unsupervised Domain Adaptation on Implicit Models",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565632"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3929",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90315
      ],
      "eventIds": [],
      "abstract": "Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep learning based 3D reconstruction approaches allow us to recover 3D shapes from 2D observations, among which single-view-based approaches can reduce the need for human intervention and specialised equipment in acquiring 3D sculptures for VR museums. However, there exist two challenges when attempting to use the well-researched human reconstruction methods: limited data availability and domain shift. Considering sculptures are usually related to humans, we propose our unsupervised 3D domain adaptation method for adapting a single-view 3D implicit reconstruction model from the source (real-world humans) to the target (sculptures) domain. We have compared the generated shapes with other methods and conducted ablation studies as well as a user study to demonstrate the effectiveness of our adaptation method. We also deploy our results in a VR application.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90185
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90169
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Durham",
              "institution": "Durham University",
              "dsl": ""
            }
          ],
          "personId": 90242
        }
      ]
    },
    {
      "id": 90305,
      "typeId": 12411,
      "title": "Standing Balance Improvement Using Vibrotactile Feedback in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565638"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-3407",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90319
      ],
      "eventIds": [],
      "abstract": "Virtual Reality (VR) users often encounter postural instability, i.e., balance issues, which can be a significant impediment to universal usability and accessibility, particularly for those with balance impairments. Prior research has validated imbalance issues, but little effort has been made to mitigate them. We recruited 39 participants (with balance impairments: 18, without balance impairments: 21) to examine the effect of various vibrotactile feedback techniques on balance in virtual reality, specifically spatial vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile feedback mapped to the center of pressure (CoP). Participants completed standing visual exploration and standing reach and grasp tasks. According to within-subject results, each vibrotactile feedback enhanced balance in VR significantly (p < .001) for those with and without balance impairments. Spatial and CoP vibrotactile feedback enhanced balance significantly more (p < .001) than other vibrotactile feedback. This study presents strategies that might be used in future virtual environments to enhance standing balance and bring VR closer to universal usage.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "San Antonio",
              "institution": "The University of Texas At San Antonio",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90150
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "San Antonio",
              "institution": "University of Texas at San Antonio",
              "dsl": "Health and Kinesiology"
            }
          ],
          "personId": 90245
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "San Antonio",
              "institution": "University of Texas - San Antonio",
              "dsl": "Kinesiology"
            }
          ],
          "personId": 90269
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "San Antonio",
              "institution": "University of Texas at San Antonio",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90197
        }
      ]
    },
    {
      "id": 90306,
      "typeId": 12411,
      "durationOverride": 15,
      "title": "Investigating the Perceived Realism of the Other User's Look-Alike Avatars",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565636"
        }
      },
      "isBreak": false,
      "importedId": "vrst22a-5490",
      "source": "PCS",
      "trackId": 11940,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90310
      ],
      "eventIds": [],
      "abstract": "Complex interactions occur in how we perceive realistic avatars in order to obtain believable virtual characters. Realistic self-avatars are avatars that closely resembles the user. There are outstanding questions regarding the fidelity of self-avatars that shows that there is still substantial development to be done, especially as the virtual world plays a more vital role in our education, work and recreation. The use of realistic self avatars could completely change how we interact virtually. This paper investigates which features of avatars increase its perceived realism: non-interactive, lip movement, facial expressions and full body movement. Results show that facial expression is very important in increased the perceived realism of realistic self-avatars. Investigating how to bring the most effective and immersive avatars to users provides the opportunity to answer some of these ever-pressing questions about the future of our virtual world. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Bayside",
              "institution": "Queensborough Community College",
              "dsl": ""
            }
          ],
          "personId": 90252
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "City University of New York - Hunter College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90211
        }
      ]
    },
    {
      "id": 90480,
      "typeId": 12457,
      "title": "Covid Reflections: AR in Public Health Awareness",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565666"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1056",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Augmented reality in public health communications is an under-explored field with great potential. Researchers forward Covid Reflections, a public health communications installation which employs augmented reality enhanced with AI LiDAR body tracking to engage public audiences in short duration health-oriented experiences. Covid Reflections helps audiences to visualize potential health outcomes of Covid-19 through depicting the process of disease contraction, sickness, and potential hospitalization on a virtual avatar which mirrors the user’s physical body in real-time. The user is immersed in a “virtual first-hand experience” of Covid-19, and is thus supported in drawing concrete conclusions about the potential personal implications of contracting Covid-19.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "Digital Worlds Institute"
            }
          ],
          "personId": 90378
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 90441
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 90334
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Digital Worlds Institute",
              "dsl": "University of Florida"
            }
          ],
          "personId": 90394
        }
      ]
    },
    {
      "id": 90481,
      "typeId": 12457,
      "title": "Appling Artificial Intelligence Techniques on Singing Teaching of Taiwanese Opera",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565650"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1052",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "In recently decades, Taiwanese Opera is seriously its culture inheritance. Although the Taiwan government and various Taiwanese Opera troupes have worked hard for many years to promote this culture to campuses and held interest courses of Taiwanese Opera, this culture is still losing. For elder people, Taiwanese Opera and Taiwanese cultures are both precious culture treasures and parts of their childhood memories. Nowadays, young people in Taiwan are no longer familiar to Taiwanese, neither to Taiwanese Opera singing. It is hard for young people to learn how appreciating this traditional culture. To this end, we refer to the current promotion methods of drama troupes used to promote singing Taiwanese Opera tune music nowadays.  We tried to combine new techniques into traditional Taiwanese Opera tune singing, and use machine learning for singing and figure scoring to assist teaching or promotion, and it would set up on mobile devices so that people can used at home. Students of Taiwanese Opera interest class can practice independently without professional teacher’s guidance at home. In campus promotion, this game-like promotion method brings young people acceptance of Taiwanese Opera.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "New Taipei City",
              "institution": "Tamkang University",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 90333
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Taipei University of Education",
              "dsl": "Department of Mathematics and Information Education"
            }
          ],
          "personId": 90466
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei City",
              "institution": "National Taiwan College of Performing Arts",
              "dsl": "Department of XiQu Music"
            }
          ],
          "personId": 90413
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "New Taipei City",
              "institution": "Tamkang University",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 90443
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "New Taipei City",
              "institution": "Tamkang University",
              "dsl": "Department of Electrical Engineering"
            }
          ],
          "personId": 90471
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Tainan",
              "institution": "Silver2Gold Studio",
              "dsl": ""
            }
          ],
          "personId": 90380
        }
      ]
    },
    {
      "id": 90482,
      "typeId": 12457,
      "title": "TeleStick: Video Recording and Playing System Optimized for Tactile Interaction using a Stick",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565653"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1054",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "TeleStick is a system to record and replicate tactile experience along\r\nwith visual information with common types of camera and video\r\nmonitor environment. The system uses a stick-type device with a\r\ntactile microphone attached to a camera so that it is consistently\r\nvisible in the video, and records video along with tactile and audio\r\ninformation using stereo 2ch . The users can feel as if they were in\r\nthe video as they watch it while holding a stick-type device with a\r\nspeaker and a vibrator",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90415
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90422
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90436
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90411
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90370
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90384
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90350
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90412
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90356
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Nakano",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90390
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shibuya",
              "institution": "Cidre Interaction Design Inc",
              "dsl": ""
            }
          ],
          "personId": 90476
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90381
        }
      ]
    },
    {
      "id": 90483,
      "typeId": 12457,
      "title": "Mapping of Locomotion Paths between Remote Environments in Mixed Reality using Mesh Deformation  ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565665"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1055",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Remote mixed reality (RMR) allows users to be present and interact in other users’ environments through their photorealistic avatars. Common interaction objects are placed on surfaces in each user’s environments and interacting with these objects require users to walk towards them first. However, since the user’s and their avatar’s room’s spatial configuration are not exactly similar, for a particular user’s walking path, an equivalent path must be found in the avatar’s environment, according to its environment’s spatial configuration. In this work, we use the concept of mesh deformation to obtain this path, where we deform the mesh associated with the user’s environment to fit to the spatial configuration of the avatar’s environment. This gives us the corresponding mapping of every point between the two environments from which the equivalent path can be generated.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University",
              "dsl": "EECS"
            }
          ],
          "personId": 90393
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Tennessee",
              "city": "Nashville",
              "institution": "Vanderbilt University School of Engineering",
              "dsl": "Robotics & Autonomous Systems Lab"
            }
          ],
          "personId": 90335
        }
      ]
    },
    {
      "id": 90484,
      "typeId": 12457,
      "title": "Avatar Voice Morphing to Match Subjective and Objective Self Voice Perception",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565671"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1091",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "We investigated the effect of morphing the avatar's voice from the user's voice on its impressions. We also investigated whether the image of morphing differed between those who liked and disliked their voice. The experiment was conducted by morphing the acoustic parameters such as fundamental frequency, spectral envelope, and aperiodic component based on the acoustic signals recorded by the participants themselves, and investigating their impressions of an avatar speaking with that voice. The result showed that those who liked their voice were most impressed by their original voice, while those who disliked it were more impressed by the morphed voice. This suggests that people who dislike their voice tend to seek their ideal in the avatar's voice.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "College of Engineering Systems"
            }
          ],
          "personId": 90386
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 90385
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Graduate School of Science and Technology"
            }
          ],
          "personId": 90400
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Engineering, Information and Systems"
            }
          ],
          "personId": 90416
        }
      ]
    },
    {
      "id": 90485,
      "typeId": 12457,
      "title": "CourseExpo: An Immersive Collaborative Learning Ecosystem ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565646"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1045",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Inspired by the need for remote learning technologies due to the Covid-19 pandemic and the isolated sense of lonely learners, we reimagined a remote classroom that fosters collaboration, builds community and yet without the constraints of the physical world. This paper presents a collaborative learning ecosystem that resembles a traditional city square where avatars of learners and facilitators wander, commingle, discover, and learn together. Buildings in the city square are learning modules which include typical knowledge units, assessment booths, or custom collaborative sketching studios. Our attempted prototype at realizing this conceptualization demonstrated initial success and we offer recommendations for future work.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Monroe",
              "institution": "University of Washington Bothell",
              "dsl": "CSS"
            }
          ],
          "personId": 90428
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "BOTHELL",
              "institution": "University of Washington Bothell",
              "dsl": ""
            }
          ],
          "personId": 90475
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Bothell",
              "institution": "University of Washington",
              "dsl": "Computing & Software Systems"
            }
          ],
          "personId": 90326
        }
      ]
    },
    {
      "id": 90486,
      "typeId": 12457,
      "title": "Using Virtual Reality Food Environments to Study Individual Food Consumer Behavior in an Urban Food Environment",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565685"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1122",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "The objective of this research was to explore whether virtual reality can be used to study individual food consumer decision-making and behavior through a public health lens by developing a simulation of an urban food environment that included a street-level scene and three prototypical stores. Twelve participants completed the simulation and a survey. Preliminary results showed that 72.7% of participants bought food from the green grocer, 18.2%  from the fast food store, and 9.1% from the supermarket. The mean presence score was 38.9 out of 49 and the mean usability score was 85.9 out of 100. This experiment demonstrates that virtual reality should be further considered as a tool for studying food consumer behavior within a food environment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York City",
              "institution": "City University of New York - Hunter College",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90211
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Brooklyn",
              "institution": "Brooklyn College",
              "dsl": "Department of Health and Nutrition Sciences"
            }
          ],
          "personId": 90388
        }
      ]
    },
    {
      "id": 90487,
      "typeId": 12457,
      "title": "Colorimetry Evaluation for Video Mapping Rendering",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565684"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1123",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Perceptually accurate colour reproduction is a core requirement of video mapping applications. Hence, objective evaluation of colour rendering chain taking into account human perception is greatly beneficial. In this article, we present a general workflow for colorimetry evaluation of video mapping software rendering chain. We provide an implementation covering the workflow of colorimetry evaluation in open-source video mapping software Splash. We present a set a common metrics for image quality assessment and tools for colour reproduction evaluation. We introduce an accompanying graphical visualization template to help accurate interpretation of the metrics used. We describe different use case examples that we performed with our tool. They proved the workflow efficient for simple, understandable and reproducible colorimetry evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technologies",
              "dsl": "Metalab"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Mines Paris, PSL University",
              "dsl": ""
            }
          ],
          "personId": 90336
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technology",
              "dsl": "Metalab"
            }
          ],
          "personId": 90383
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Arts and Technology",
              "dsl": "Metalab"
            }
          ],
          "personId": 90354
        }
      ]
    },
    {
      "id": 90488,
      "typeId": 12457,
      "title": "Common Experience Sample 1.0: Developing a sample for comparing the characteristics of haptic displays",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565649"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1003",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Many haptic displays that provide haptic feedback to users have been proposed;however, differences in experimental environments make comparisons of displays difficult. Therefore, we categorized the characteristics of feedback based on existing research, and developed a common experience sample that includes virtual objects necessary for the expression of each characteristic. Additionally, we will study the methods of evaluating displays using the proposed sample, and aim at comparative evaluation of multiple displays.\r\n\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90338
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90442
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Unity Technologies Japan",
              "dsl": ""
            }
          ],
          "personId": 90477
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90381
        }
      ]
    },
    {
      "id": 90489,
      "typeId": 12457,
      "title": "The Effect of Training Communication Medium on the Social Constructs Co-Presence, Engagement, Rapport, and Trust",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565686"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1124",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "In the pandemic-induced era of frequent remote workplace communication, it is important to understand the impact of remote versus in-person interactions on communication. Communication performance is highly context sensitive and difficult to quantify. This poster describes SCOTTIE, Systematic Communication Objectives and Telecommunications Technology Investigations and Evaluations. The goal of SCOTTIE is to investigate the impact of the communication medium on team performance and effectiveness. The human decision to travel or replace travel with telecommunications can be extracted from SCOTTIE, rather than relying on intuition and opinion. This poster analyzes four social communication constructs and compares them in Face-to-Face, Video Conferencing, and Virtual Reality training scenarios. Co-presence, engagement, rapport, and trust were the four constructs. Data from 105 participants across the three between-subject conditions showed that engagement was the only construct that had a statistically significant difference between the three training environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Ames",
              "institution": "Iowa State University",
              "dsl": "Industrial Engineering"
            }
          ],
          "personId": 90391
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Ames",
              "institution": "Iowa State University",
              "dsl": "Psychology"
            }
          ],
          "personId": 90451
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "McKinney",
              "institution": "Raytheon Intelligence & Space",
              "dsl": ""
            }
          ],
          "personId": 90435
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "East Hartford",
              "institution": "Raytheon Technologies Research Center",
              "dsl": ""
            }
          ],
          "personId": 90423
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Ames",
              "institution": "Iowa State University",
              "dsl": "Engineering Administration"
            }
          ],
          "personId": 90367
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Iowa",
              "city": "Ames",
              "institution": "Iowa State University",
              "dsl": "Virtual Reality Applications Center"
            }
          ],
          "personId": 90460
        }
      ]
    },
    {
      "id": 90490,
      "typeId": 12457,
      "title": "An AI-empowered Cloud Solution towards End-to-End 2D-to-3D Image Conversion for Autostereoscopic 3D Display",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565674"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1048",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Autostereoscopic or glass-free 3D displays allow the users to view the 3D content on electronic displays (e.g. LCD, projectors) without wearing any glasses. However, the content for glass-free 3D displays needs to be in 3D format such that novel views could be synthesized and multiplexed together. Unfortunately, nowadays images/videos are still normally captured in 2D which cannot be directly utilized for glass-free 3D displays. In the past few years, artificial intelligence (AI) technologies, especially deep learning neural networks, have found rapid advancements and demonstrated outstanding capabilities in converting 2D images/videos into 3D and have brought promising directions towards content generation suitable for autostereoscopic 3D displays. In this paper, we introduce an AI-empowered cloud solution towards end-to-end 2D-to-3D image conversion for autostereoscopic 3D displays, or “CONVAS (3D)” in short. Taking a single 2D image as the input, CONVAS (3D) is able to automatically convert the input 2D image and generate an image suitable for a target autostereoscopic 3D display. It is implemented on a web-based server such that it can allow the users to submit the conversion task and to retrieve the results without geographical constraints.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Select Region",
              "city": "Singapore",
              "institution": "Singapore Institute of Technology",
              "dsl": "Infocomm Technology Cluster"
            }
          ],
          "personId": 90357
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Institute of Technology",
              "dsl": "ICT Cluster"
            }
          ],
          "personId": 90431
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shanghai",
              "institution": "Shanghai University",
              "dsl": "School of Mechatronic Engineering and Automation"
            }
          ],
          "personId": 90406
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "Singapore Institute of Technology",
              "dsl": "InfoComm Technology Cluster"
            }
          ],
          "personId": 90401
        }
      ]
    },
    {
      "id": 90491,
      "typeId": 12457,
      "title": "The Community Game Development Toolkit",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565661"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1125",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "The Community Game Development Toolkit is a set of tools that\r\nprovide an accessible, intuitive work-flow within the Unity game\r\nengine for students, artists, researchers and community members to\r\ncreate their own visually rich, interactive 3D stories and immersive\r\nenvironments. The toolkit is designed to support diverse communi-\r\nties to represent their own traditions, rituals and heritages through\r\ninteractive, visual storytelling, drawing on community members’\r\nown visual assets such as photos, sketches and paintings, without\r\nrequiring the use of coding or other specialized game-design skills.\r\nProjects can be built for desktop, mobile and VR applications. This\r\npaper describes the background, implementation and planned fu-\r\nture developments of the toolkit, as well the contexts in which it\r\nhas been used.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Saint Peter",
              "institution": "Gustavus Adolphus College",
              "dsl": ""
            }
          ],
          "personId": 90387
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Galloway",
              "institution": "Stockton University",
              "dsl": "Digital Studies"
            }
          ],
          "personId": 90361
        }
      ]
    },
    {
      "id": 90492,
      "typeId": 12457,
      "title": "Exploration of inter-marker interactions in Tangible AR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565642"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1005",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Inter-marker interactions in marker-based Augmented Reality (AR) are defined primarily by the movement and placement of the markers. In this paper, we explore multiple inter-marker interactions for tangible AR along with their use cases. We developed prototypes that demonstrate primarily five inter-marker interactions; namely, proximity of two or more markers, placement of makers over each other, flipping of markers, marker as a toggle and marker as a controller. These interactions were designed such that they would be suitable for multiple contexts of application. To demonstrate their usage, visualizing lattice structures in Chemistry was chosen as the context. Using the prototypes and the insights from initial evaluations, we discuss the benefits and drawbacks of such interaction methods. We further outline the opportunities for using these interactions and the prospects of extending these concepts to other contexts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "IDC School of Design, Indian Institute of Technology Bombay",
              "dsl": ""
            }
          ],
          "personId": 90463
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Maharashtra",
              "city": "Mumbai",
              "institution": "Indian Institute of Technology Bombay",
              "dsl": "IDC School of Design"
            }
          ],
          "personId": 90374
        }
      ]
    },
    {
      "id": 90493,
      "typeId": 12457,
      "title": "Investigation of User Performance in Virtual Reality-based Annotation-assisted Remote Robot Control",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565687"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1126",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "This poster investigates the use of point cloud processing algorithms to provide annotations for robotic manipulation tasks completed remotely via Virtual Reality (VR). A VR-based system has been developed that receives and visualizes the processed data from real-time RGB-D camera feeds. A real-world robot model has also been developed to provide realistic reactions and control feedback. The targets and the robot model are reconstructed in a VR environment and presented to users in different modalities. The modalities and available information are varied between experimental settings,\r\nand the associated task performance is recorded and analyzed. The results accumulated from 192 experiments completed by 8 participants showed that point cloud data is sufficient for completing the task. Additional information, either image stream or preliminary processes presented as annotations, was found to not have a significant impact on the completion time. However, the combination of image stream and colored point cloud data visualization modalities was found to greatly enhance a user’s performance accuracy, with the number of target centers missed being reduced by 40%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Chippendale",
              "institution": "University of Technology Sydney",
              "dsl": "Faculty of Engineering and IT"
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Chippendale",
              "institution": "University of Technology Sydney",
              "dsl": "Faculty of Engineering and IT"
            }
          ],
          "personId": 90421
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Centre for Autonomous System"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "Centre for Autonomous System"
            }
          ],
          "personId": 90407
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "New South Wales",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": ""
            }
          ],
          "personId": 90434
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "School of Mechanical and Mechatronic Engineering"
            },
            {
              "country": "Australia",
              "state": "",
              "city": "Sydney",
              "institution": "University of Technology Sydney",
              "dsl": "School of Mechanical and Mechatronic Engineering"
            }
          ],
          "personId": 90328
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Ulitmo",
              "institution": "University of Technology, Sydney",
              "dsl": ""
            },
            {
              "country": "Australia",
              "state": "NSW",
              "city": "Ulitmo",
              "institution": "University of Technology, Sydney",
              "dsl": ""
            }
          ],
          "personId": 90352
        }
      ]
    },
    {
      "id": 90494,
      "typeId": 12457,
      "title": "A Mixed Reality Platform for Collaborative Technical Assembly Training",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565664"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1081",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We have developed a mixed reality (MR)-based platform for basic mechanical engineering concepts as a learning environment for collaborative assembly tasks. In our platform, multiple co-located users interact with virtual objects simultaneously, and during that time, the platform collects data related to participants' collaboration and team behavior. We implemented four main sections in the platform including setup, introduction, training, and assessment. The platform provides the opportunity for users to interact with virtual objects while also acquiring technical knowledge. Specifically, for the technical component of the platform, users are asked to assemble a hydraulic pump by manipulating and fitting various parts and pieces into a provided pre-assembled blueprint. We conducted a preliminary expert panel review composed of three experts and received positive feedback and suggestions for further development of the platform.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 90343
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "HRD Virtual Lab"
            }
          ],
          "personId": 90444
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "west lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 90355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 90376
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Technology Leadership & Innovation"
            }
          ],
          "personId": 90410
        }
      ]
    },
    {
      "id": 90495,
      "typeId": 12457,
      "title": "Visual Considerations for Augmented Reality in Urban Planning",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565681"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1082",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "The design process in architecture and urban planning has always been accompanied by a discourse on suitable visual representation. This resulted in both a wealth of visualisation styles and high sensi- tivity among planners regarding the visual communication of their work. Representation of projects throughout phases of the planning process often adheres to established visual standards, from concept sketch to high-end rendering. A look at contemporary Augmented Reality (AR) apps for urban planning indicates however that the quality and precision of representation seem to lag somewhat be- hind, entailing risks that projects are misinterpreted. This poster describes our design research on three urban planning apps devel- oped with Swiss municipalities and outlines results that improve visual representation in AR throughout different planning phases.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 90323
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "School of Information Technology"
            }
          ],
          "personId": 90408
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "School of Art and Design"
            }
          ],
          "personId": 90479
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lucerne",
              "institution": "Lucerne University of Applied Sciences and Arts",
              "dsl": "School of Art and Design"
            }
          ],
          "personId": 90470
        }
      ]
    },
    {
      "id": 90496,
      "typeId": 12457,
      "title": "Sign Language in Immersive VR: Design, Development,  and Evaluation of a Testbed Prototype",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565676"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1084",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Immersive Virtual Reality (IVR) systems support several \r\nmodalities such as body, finger, eye, and facial expressions \r\ntracking, thus they can support sign-language-based \r\ncommunication. The combined utilization of tracking technologies \r\nrequires careful evaluation to ensure high-fidelity transference of \r\nbody posture and gestures in real-time, whereas facial tracking is \r\nstill limited to the lower area of the face, thus excluding vital facial \r\nexpressions. This paper presents the design, development and \r\nevaluation of an IVR system utilizing state-of-the-art tracking \r\noptions. The system is evaluated by certified sign language teachers \r\nto detect usability issues and examine appropriate methodology for \r\nlarge-scale follow-up evaluation by users fluent in sign language.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "Syros, Cyclades",
              "city": "Hermoupolis",
              "institution": "University of the Aegean",
              "dsl": "Department of Product & Systems Design Engineering"
            }
          ],
          "personId": 90392
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Mytilene",
              "institution": "University of the Aegean",
              "dsl": "Department of Cultural Technology and Communication"
            }
          ],
          "personId": 90456
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Hermoupolis, Syros",
              "institution": "University of the Aegean",
              "dsl": "Product & Systems Design Engineering"
            }
          ],
          "personId": 90362
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Mytilene",
              "institution": "University of the Aegean",
              "dsl": "Department of Cultural Technology and Communication"
            }
          ],
          "personId": 90455
        }
      ]
    },
    {
      "id": 90497,
      "typeId": 12457,
      "title": "Dill Pickle: Interactive Theatre Play in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565678"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1085",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "``Dill Pickle'', is an interactive theatre experience in virtual reality. In the play, a volumetrically captured actor plays the character of Robert. The user interacts with Robert through utterances that are memorized or prompted with text or audio. The set was recreated through a process of photogrammetry. ``Dill Pickle'' is one of the first interactive theatre plays that implements interaction with volumetrically-captured characters.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "American University",
              "dsl": "Institute for IDEAS"
            }
          ],
          "personId": 90359
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "American University",
              "dsl": "Institute for IDEAS"
            }
          ],
          "personId": 90409
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "District of Columbia",
              "city": "Washington",
              "institution": "American University",
              "dsl": "Institute for IDEAS"
            }
          ],
          "personId": 90349
        }
      ]
    },
    {
      "id": 90498,
      "typeId": 12457,
      "title": "An Interactive Haptic Display System with Changeable Hardness Using Magneto-Rheological Fluid",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565673"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1041",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "We present a haptic display system with changeable  hardness using magneto-rheological (MR) fluid. The major component is the haptic device with layers of MR fluid, contact point and pressure sensors, and electromagnet. The system enables multi-modal interaction using this device with control circuits and a projector. We also developed two types of aiming for multi-modal virtual and mixed reality experiences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hiki",
              "institution": "Tokyo Denki University",
              "dsl": "Graduate School of Science and Engineering"
            }
          ],
          "personId": 90438
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Hiki",
              "institution": "Tokyo Denki University",
              "dsl": "Graduate School of Science and Engineering"
            }
          ],
          "personId": 90418
        }
      ]
    },
    {
      "id": 90499,
      "typeId": 12457,
      "title": "Dynamic X-Ray Vision in Mixed Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565675"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1086",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "X-ray vision, a technique that allows users to see through walls and other obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality (MR). \r\nIn this paper, we demonstrate a dynamic X-ray vision window that is rendered in real-time based on the user's current position and changes with movement in the physical environment. Moreover, the location and transparency of the window are also dynamically rendered based on the user's eye gaze. We build this X-ray vision window for a current state-of-the-art MR Head-Mounted Device (HMD) -- HoloLens 2 by integrating several different features: scene understanding, eye tracking, and clipping primitive.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Dallas",
              "institution": "Software Engineering department",
              "dsl": "The University of Texas at Dallas"
            }
          ],
          "personId": 90450
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Aberdeen Proving Ground",
              "institution": "U.S. Army Research Lab",
              "dsl": "Human Research and Engineering Directorate"
            }
          ],
          "personId": 90469
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Arlington",
              "institution": "U.S. DEVCOM Army Research Laboratory",
              "dsl": ""
            }
          ],
          "personId": 90437
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90414
        }
      ]
    },
    {
      "id": 90500,
      "typeId": 12457,
      "title": "MetaTwin: Synchronizing Physical and Virtual Spaces for Seamless World",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565647"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1043",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "This paper presents MetaTwin, a collaborative Metaverse platform that supports one-to-one spatiotemporal synchrony between physical and virtual spaces. The users can interact with other users and surrounding IoT devices without being tied to physical spaces. Resource sharing is implemented to allow users to share media, including presentation slides and music. We deploy MetaTwin in two different network environments (i.e., within the US, Korea-US international) and summarize users' feedback about the experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90340
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90397
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "ATEC"
            }
          ],
          "personId": 90420
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": "AI Research Lab"
            }
          ],
          "personId": 90459
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": "AI Research Lab"
            }
          ],
          "personId": 90457
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90440
        }
      ]
    },
    {
      "id": 90501,
      "typeId": 12457,
      "title": "Selection of Expanded Data Points in Immersive Analytics",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565645"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1120",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We propose a novel technique to facilitate the selection of data points, a type of data representation we often work with in immersive analytics. We designed and implemented this technique based on the expansion of data points following Fitt's law. A user study was conducted in an headset-based augmented reality environment. The results significantly highlight the performance of our technique in helping the user select data points and their subjective appreciation in working with the expendable data points.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN, VENISE Team",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN, VENISE Team",
              "dsl": ""
            }
          ],
          "personId": 90331
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 90404
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI, VENISE team",
              "dsl": ""
            },
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LIMSI, VENISE team",
              "dsl": ""
            }
          ],
          "personId": 90369
        }
      ]
    },
    {
      "id": 90502,
      "typeId": 12457,
      "title": "Leveraging multimodal sensory information in cybersickness prediction",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565667"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1088",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Cybersickness is one of the problems that undermines user experience in virtual reality. While many studies are trying to find ways to alleviate cybersickness, only a few have considered cybersickness through multimodal perspectives. In this paper, we propose a multimodal, attention-based cybersickness prediction model. Our model was trained based on a total of 24,300 seconds of data from 27 participants and yielded the F1-score of 0.82. Our study results highlight the potential to model cybersickness from multimodal sensory information with a high level of performance and suggest that the model should be extended using additional, diverse samples.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Artificial Intelligence"
            }
          ],
          "personId": 90163
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seoul",
              "institution": "Hanyang University",
              "dsl": "Department of Data Science"
            }
          ],
          "personId": 90271
        }
      ]
    },
    {
      "id": 90503,
      "typeId": 12457,
      "title": "TTTV2 makes it possible for people with shellfish allergies  to still enjoy the taste of crab virtually",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565669"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1080",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "In this paper, we reproduce the taste of a crab cream croquette, which are harmful to people with shellfish allergies using TTTV (Taste the TV), which is a taste-reproduction technology using the mixing and spraying of solutions. Participants recognized the food from only the taste. It is possible to safely taste flavors virtually by reproducing them through the mixing of safe materials that do not contain allergens.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90403
        }
      ]
    },
    {
      "id": 90504,
      "typeId": 12457,
      "title": "Evaluation of Pseudo-Haptics system feedbacking muscle activity",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565682"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1119",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Differences in perceptions between virtual reality (VR) and reality prevent immersion in VR. To improve immersion in VR, many methods have adopted haptic feedback in VR using pseudo-haptics. However, these methods have little evaluated the effect of force feedback on pseudo-haptics that reflect the user's state. This paper proposes and evaluates the pseudo-haptics system that manipulates the control/display (C/D) ratio between reality and VR using muscle activity measured. We conducted a user study under three conditions: the C/D ratio is constant, large, or small, depending on the muscle activity. Our results indicated that pseudo-haptics were effective for large C/D ratio settings during low myoelectric intensity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Chuo University",
              "dsl": ""
            }
          ],
          "personId": 90399
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Chuo University",
              "dsl": "School of Science and Engineering"
            }
          ],
          "personId": 90372
        }
      ]
    },
    {
      "id": 90505,
      "typeId": 12457,
      "title": "TTTV2 (Transform the Taste and Visual Appearance): Tele-eat with a seasoning home appliance that changes the taste and appearance of food or beverages",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565663"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1078",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "We prototyped a seasoning appliance that applies a “taste display” technology that employs a taste sensor to reproduce flavors via the spraying and subsequent mixing of colored, flavored liquids to create a printed image on the surface of another food. For example, when toasted bread is used as the medium, the appliance changes its appearance and taste into other food items, such as pizza or chocolate brownie, and the user can then virtually eat that food.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90403
        }
      ]
    },
    {
      "id": 90506,
      "typeId": 12457,
      "title": "Augmented Reality Patient-Specific Registration for Medical Visualization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565689"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1112",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "In recent years, medical research has made extensive use of Augmented Reality (AR) for visualization. These visualizations provide improved 3D understanding and depth perception for surgeons and medical staff during surgical planning, medical training, and procedures. Often, AR in medicine involves impractical and extensive instrumentation in order to provide the precision needed for clinical use. We propose a mobile AR 3D model registration system for use in a practical, non-instrumented hospital setting. Our registration system takes as input a patient-specific model and overlays it on the patient using an accurate pose registration technique that requires a single marker as a point of reference to initialize a point cloud-based pose refinement technique. Our method is automatic, easy to use, and runs in real-time on a mobile phone. We conduct quantitative and qualitative analysis of the registration. The results confirm that our AR pose registration system produces an accurate and visually correct overlay of the medical data in real-time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Computer Science"
            }
          ],
          "personId": 90461
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UC Irvine",
              "dsl": "iGravi Lab, School of ICS"
            }
          ],
          "personId": 90366
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "UCI",
              "dsl": "Computer science"
            }
          ],
          "personId": 90430
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Irvine",
              "institution": "University of California, Irvine",
              "dsl": "Interactive Graphics & Visualization Lab"
            }
          ],
          "personId": 90325
        }
      ]
    },
    {
      "id": 90507,
      "typeId": 12457,
      "title": "Virtual eating experience of poisonous mushrooms using TTTV2",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565668"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1079",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "VR content does not have to follow reality entirely, thus creating a variety of experiences, and the same should be true for content for taste displays. It may be worthwhile to create experiences that cannot be experienced in the real world. For example, many VR games create thrill and excitement through experiences that would be life-threatening in the real world. In this context, we developed taste content that allows users to safely experience the taste of poisonous mushrooms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90403
        }
      ]
    },
    {
      "id": 90508,
      "typeId": 12457,
      "title": "Visualizing Perceptions of Non-Player Characters in Interactive Virtual Reality Environments",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565680"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1114",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Visual effects and elements to visualize the perceptions of one's own virtual character (also referred to as Visual Delegates) are often used in video games, e.g., status bars visualize the character's sense of health, filters on the interface layer visualize the character's state of mind. It is still largely unexplored whether Visual Delegates can also be used to transfer the perception of non-player characters comprehensibly. Therefore, we developed a medical virtual reality scenario using five different types of Visual Delegates to visualize three different perceptions of a virtual non-player patient. We tested for character assignment in a qualitative user study (N = 20). Our results can be used to decide more effectively what types of Visual Delegates can be used to convey perceptions of non-player characters.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Hannover University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 90363
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Hannover University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 90395
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Hannover University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 90353
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Hannover University of Applied Sciences and Arts",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 90398
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Cologne",
              "institution": "TH Köln",
              "dsl": "Interactive Reality Experiences"
            }
          ],
          "personId": 90467
        }
      ]
    },
    {
      "id": 90509,
      "typeId": 12457,
      "title": "Improving Pedestrian Safety around Autonomous Delivery Robots in Real Environment with Augmented Reality.",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565644"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1038",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "In recent years, the use of autonomous vehicles and autonomous delivery robots (ADR) has increased due to technological progress. We can now imagine a future society where more and more ADRs roam around us, and we need to learn to coexist with ADRs in our lives. This paper explores how pedestrian safety around moving ADRs can be improved by increasing the user's understanding of ADR behavior. To reduce pedestrian anxiety, we proposed the display of various real-time information from the ADR in Augmented Reality (AR). In order to evaluate their effectiveness, a preliminary experiment was conducted in an outdoor environment where an ADR was running, within the coverage of a 5G network. We found that the real-time display of ADR information in AR has a positive effect in alleviating user anxiety around the ADR.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Mobile, Inc.",
              "dsl": ""
            }
          ],
          "personId": 90322
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Group, Inc.",
              "dsl": "Rakuten Institute of Technology"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Mobile, Inc.",
              "dsl": ""
            }
          ],
          "personId": 90389
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Group, Inc.",
              "dsl": "Rakuten Institute of Technology"
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Mobile, Inc.",
              "dsl": ""
            }
          ],
          "personId": 90449
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Faculty of Art and Design"
            }
          ],
          "personId": 90473
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Rakuten Institute of Technology, Rakuten, Inc.",
              "dsl": ""
            }
          ],
          "personId": 90373
        }
      ]
    },
    {
      "id": 90510,
      "typeId": 12457,
      "title": "Visualizing Machine Learning in 3D",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565688"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1118",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Understanding machine learning models can be difficult when the models at hand have many parts to it. Having a visual model can help aid in understanding how the model functions. A way to visualize these models is to use a 3D (three dimensional) game de- velopment application. An application that will have an interactive element allowing the users to interact with the model (rotating and scaling it) and see changes at run-time. An interactive element will keep the users engaged, understand, and see how a machine learning model looks like and behaves. This paper describes the process of visualizing a machine learning model in a 3D application.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Hunter College",
              "dsl": "VR REU"
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "New Rochelle",
              "institution": "Iona College",
              "dsl": ""
            }
          ],
          "personId": 90448
        }
      ]
    },
    {
      "id": 90511,
      "typeId": 12457,
      "title": "Exploring Vibration Intensity Map Of Hand Postures For Haptic Rendering In XR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565642"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1070",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Hand Posture may have an impact on your Haptic Experience with wearable haptic gloves. This may lead to both positive and negative results on one’s haptic experience in XR. In this study, we hypothesized that hand postures may have the potential to affect the sensation of tactile feedback and report the connection between the acceleration data and hand posture.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 90364
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Dresden",
              "institution": "Technische Universität Dresden",
              "dsl": "Faculty of Electrical and Computer Engineering"
            }
          ],
          "personId": 90478
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Daejeon",
              "city": "Yuseong-gu",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 90330
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 90425
        }
      ]
    },
    {
      "id": 90512,
      "typeId": 12457,
      "title": "Immersive Analytics for Spatio-Temporal Data on a Virtual Globe: Prototype and Emerging Research Challenges",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565656"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1072",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "We present our approach for the immersive analysis of spatio-temporal data, using a three-dimensional virtual globe. We display quantitative data as country-shaped elevated polygons and animate elevation levels over time to represent the temporal dimension. This approach allows us to investigate global patterns of behaviour, like pandemic infection data. By using a virtual reality setting, we intend to increase our understanding of spatial data and potential global relationships. Based on the development of our prototype, we outline research challenges we see emerging in this context. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": "Institute of Computer Graphics and Knowledge Visualisation"
            }
          ],
          "personId": 90342
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Vienna",
              "institution": "VRVis Forschungs-GmbH",
              "dsl": ""
            }
          ],
          "personId": 90371
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": ""
            }
          ],
          "personId": 90417
        }
      ]
    },
    {
      "id": 90513,
      "typeId": 12457,
      "title": "Guide Ring: Bi-directional Finger-worn Haptic Actuator for Rich Haptic Feedback",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565626"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1074",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We introduce a novel wearable haptic feedback device that magni-\r\nfies the visual experience of virtual and augmented environments\r\nthrough bidirectional vibrotactile feedback driven by electromag-\r\nnetic coils with permanent magnets. This device creates guidance\r\nhaptic effect through magnetic attraction and repulsion. Our proof-\r\nof-concept prototype enables haptic interaction through altering\r\nposition of wearable structure, vibrating with different intensity,\r\nand waveform pattern. Example applications illustrate how the\r\nproposed system promotes guided and rich haptic feedback.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Korea Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 90379
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "UVR Lab"
            }
          ],
          "personId": 90474
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "Graduate School of Culture Technology"
            }
          ],
          "personId": 90425
        }
      ]
    },
    {
      "id": 90514,
      "typeId": 12457,
      "title": "Ha and Fu: Interface to Breathe on a Smartphone ",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565643"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1030",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "This study used front-camera face tracking to realize interactions such as blowing on a smartphone to make it foggy or tapping on it to make it puff. We used the parameters of the lower jaw opening, the lip closing, and the lip stretching to identify these actions. Additionally, we enabled pointing based on the position and direction of the face and implemented four applications using this interface. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Meiji University",
              "dsl": ""
            }
          ],
          "personId": 90403
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Sony Group Corporation",
              "dsl": ""
            }
          ],
          "personId": 90339
        }
      ]
    },
    {
      "id": 90515,
      "typeId": 12457,
      "title": "Can Haptic Feedback on One Virtual Object Increase the Presence of Another Virtual Object?",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565658"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1076",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "This paper investigated whether increased presence from experiencing haptic feedback on one virtual object can transfer to another virtual object. Two similar studies were run in different environments: an immersive virtual environment and a mixed environment. Results showed that participants reported a high presence of a virtual object that was not touched when touching a virtual object in a virtual reality environment. On the other hand, it was difficult to confirm that such presence transfers occurred in an augmented reality environment. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 90445
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 90360
        }
      ]
    },
    {
      "id": 90516,
      "typeId": 12457,
      "title": "Finger Kinesthetic Haptic Feedback Device Using Shape Memory Alloy-based High-Speed Actuation Technique",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565662"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1077",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Compared to the study on tactile feedback gloves, the kinesthetic feedback device, which has been studied for the past several decades, has difficulties in interacting with the user owing to various problems, such as large size, low portability, and high power consumption. Herein, we present a bidirectional finger kinesthetic feedback device that can provide an immersive virtual reality experience using a shape memory alloy (SMA). The proposed device provides kinesthetic feedback without heterogeneity by integrating efficient power control of the SMA actuator, fast cooling of the SMA within one second, and high-precision motion-tracking technology. The implemented device delivers a gripping and hand spreading force of up to 10N each to the index and middle fingers.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "Daejeon",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": "Creative Content Research Division"
            }
          ],
          "personId": 90458
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": "Communication & Media Research Laboratory"
            }
          ],
          "personId": 90453
        }
      ]
    },
    {
      "id": 90517,
      "typeId": 12457,
      "title": " LivePose: Democratizing Pose Detection for Multimedia Arts and Telepresence Applications on Open Edge Devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565660"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1110",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We present LivePose: an open-source tool that democratizes pose detection for multimedia arts and telepresence applications, optimized for and distributed on open edge devices. We designed the architecture of LivePose with a 5-stage pipeline (frame capturing, pose estimation, dimension mapping, filtering, and output) sharing streams of data flow, distributable on networked nodes. We distribute LivePose and dependencies packages and filesystem images optimized for edge devices (NVIDIA Jetson). We showcase multimedia arts and telepresence applications enabled by LivePose.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technology",
              "dsl": "Metalab"
            }
          ],
          "personId": 90383
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technologies",
              "dsl": "Metalab"
            }
          ],
          "personId": 90426
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technologies",
              "dsl": "Metalab"
            }
          ],
          "personId": 90327
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Art and Technologies",
              "dsl": "Metalab"
            },
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "McGill University",
              "dsl": ""
            }
          ],
          "personId": 90452
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Society for Arts and Technology",
              "dsl": "Metalab"
            }
          ],
          "personId": 90354
        }
      ]
    },
    {
      "id": 90518,
      "typeId": 12457,
      "title": "Exploring Game Experience Variations Between Immersive and Non-Immersive RPGs",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565654"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1067",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "Comparing Non-Immersive Virtual Reality (N-IVR) and Immersive Virtual Reality (IVR) game experiences, while considering game genres, assists in understanding the specificities of IVR game design, providing future game designers with invaluable insights and guidelines. This work preliminary evaluates game experience variations between an N-IVR and an IVR version of a Role-Playing Game (RPG). Our results indicate genre-specific variations in game experience between RPGs and other game genres studied in a similar manner. Moreover, our study identifies prior experience with N-IVR games in general, as well as with N-IVR RPGs, as an important factor affecting game experience in IVR RPGs.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "Lesvos",
              "city": "Mytilene",
              "institution": "University of the Aegean",
              "dsl": "Department of Cultural Technology and Communication"
            }
          ],
          "personId": 90465
        },
        {
          "affiliations": [
            {
              "country": "Greece",
              "state": "",
              "city": "Mytilene",
              "institution": "University of the Aegean",
              "dsl": "Department of Cultural Technology and Communication"
            }
          ],
          "personId": 90456
        }
      ]
    },
    {
      "id": 90519,
      "typeId": 12457,
      "title": "Object Manipulation Method Using Eye Gaze and Hand-held Controller in AR Space",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565659"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1068",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "When manipulating virtual objects in AR space, the target object is often occluded by other objects partially or completely (hereinafter, occlusion problem). In addition, the hand-ray manipulation that is commonly used in many VR/AR devices requires the user to keep raising the arm within the range of the hand tracking sensor and causes the gorilla-arm problem. In this study, we propose an object manipulation method that combines eye gaze and a hand-held controller to mitigate the occlusion problem and gorilla-arm problem in AR environments. In the proposed method, the user controls the ray's direction with eye gaze, and the user adjusts the length of the ray and selects objects with the hand-held controller.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90429
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90377
        }
      ]
    },
    {
      "id": 90520,
      "typeId": 12457,
      "title": "Size Does Matter: An Experimental Study of Anxiety in Virtual Reality",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565683"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1104",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "The emotional response of users induced by VR scenarios has become a topic of interest, however, whether changing the size of objects in VR scenes induces different levels of anxiety remains a question to be studied. In this study, we conducted an experiment to initially reveal how the size of a large object in a VR environment affects changes in participants' (N = 38) anxiety level and heart rate. To holistically quantify the size of large objects in the VR visual field, we used the omnidirectional field of view occupancy (OFVO) criterion for the first time to represent the dimension of the object in the participant's entire field of view. The results showed that the participants' heartbeat and anxiety while viewing the large objects were positively and significantly correlated to OFVO. These study reveals that the increase of object size in VR environments is accompanied by a higher degree of user's anxiety.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "-Select-",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Doctoral Program in Design"
            },
            {
              "country": "Japan",
              "state": "-Select-",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Doctoral Program in Design"
            }
          ],
          "personId": 90344
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Center for Computational Sciences"
            },
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": "Center for Computational Sciences"
            }
          ],
          "personId": 90454
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "None/Not Applicable",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "None/Not Applicable",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90346
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90433
        }
      ]
    },
    {
      "id": 90521,
      "typeId": 12457,
      "title": "PlayMeBack - Cognitive Load Measurement using Different Physiological Cues in a VR Game",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565648"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1027",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We present a Virtual Reality (VR) game, PlayMeBack, to investigate cognitive load measurement in interactive VR environments using pupil dilation, Galvanic Skin Response (GSR), Electroencephalogram (EEG) and Heart Rate (HR). \r\n\r\nThe user is shown different patterns of tiles lighting up and is asked to replay the pattern back pressing the tiles in the same sequence they lit up. The task difficulty depends on the length of the observed pattern (3-6 keys). This task is designed to explore the effect of cognitive load on physiological cues, and if pupil dilation, EEG, GSR and HR can be used as measures of cognitive load.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Empathic Computing Lab"
            }
          ],
          "personId": 90368
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "The University of Auckland",
              "dsl": "Auckland Bioengineering Institute"
            }
          ],
          "personId": 90332
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Adelaide",
              "institution": "University of South Australia",
              "dsl": ""
            }
          ],
          "personId": 90427
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": ""
            }
          ],
          "personId": 90375
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute",
              "dsl": "Empathic Computing Lab"
            }
          ],
          "personId": 90447
        }
      ]
    },
    {
      "id": 90522,
      "typeId": 12457,
      "title": "Generating Leg Animation for Walking-in-Place Techniques using a Kinect Sensor",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565679"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1105",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "We present a kinematic approach based on animation rigging to generating real-time leg animation. Our main approach is to track vertical in-place foot movements of a user using a Kinect v2 sensor and map tracked foot height to inverse kinematics (IK) targets. We align two IK targets with an avatar’s feet and guide the virtual feet to perform cyclic walking motions using a set of kinematic equations. Preliminary testing shows that this approach can produce compelling real-time forward-backward leg animation during in-place walking.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "China Agricultural University",
              "dsl": "College of Information and Electrical Engineering"
            }
          ],
          "personId": 90337
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "China Agricultural University",
              "dsl": "College of Information and Electrical Engineering"
            }
          ],
          "personId": 90329
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "China Agricultural University",
              "dsl": "College of Information and Electrical Engineering"
            }
          ],
          "personId": 90351
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "China Agricultural University",
              "dsl": "College of Information and Electrical Engineering"
            }
          ],
          "personId": 90464
        }
      ]
    },
    {
      "id": 90523,
      "typeId": 12457,
      "title": "A Distance Learning System With Shareable Physical Information For Ski Training",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565655"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1029",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "Distance learning has been gaining attention owing to the spread of COVID-19. However, the distance learning environment for skill learning is still inadequate because the perceptual information provided to the user is greatly limited. \r\nThis study proposed a framework for a distance skill learning system with real-time feedback to share the current status between the student and the teacher. As an initial trial to use this framework, we developed a prototype of a distance learning system for skiing, including visual feedback, and verified its operation. The result indicated the system could be applied enough to a distance learning system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ishikawa",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 90341
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Nomi",
              "institution": "Japan Advanced Institute of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 90432
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 90402
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Tokyo Institute of Technology",
              "dsl": "School of Computing"
            }
          ],
          "personId": 90424
        }
      ]
    },
    {
      "id": 90524,
      "typeId": 12457,
      "title": "Haptic Interaction Module for VR Fishing Leisure Activity",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565677"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1060",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "This paper presents a tiny haptic interaction module which generates high resistive torque for VR fishing leisure activity. The presented haptic interaction module was developed by magnetic rheological fluids and optimizing its structure. The measured haptic torque was varied from 0.3 N·cm to 2.4 N·cm as the applied voltage increased from 0 V to 5 V. The performance of the proposed actuator was qualitatively evaluated by constructing virtual fishing environment where a user can feel not only the weight of a target object but also its motion.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            }
          ],
          "personId": 90462
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            }
          ],
          "personId": 90405
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            }
          ],
          "personId": 90439
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            }
          ],
          "personId": 90347
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Cheonan",
              "institution": "Koreatech",
              "dsl": ""
            }
          ],
          "personId": 90468
        }
      ]
    },
    {
      "id": 90525,
      "typeId": 12457,
      "title": "Data Abstraction for Visual and Haptic Representations in Flow Visualization",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565651"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1062",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90628
      ],
      "eventIds": [],
      "abstract": "This paper presents a new way of data abstraction for visual and haptic representations in immersive analytics using a mid-air haptic display. Visual and haptic abstraction is proposed to transform raw data (wind tunnel data) into another form of data for effective visual and haptic data mapping. Three main features are extracted: (i) Magnitude of Velocity, (ii) Recirculation Region, and (iii) Vorticity. For each feature, visual and haptic abstractions are defined based on data characterization and data reduction. A preliminary study shows a promising direction toward multimodal data interaction in immersive analytics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90397
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "ETRI",
              "dsl": "AI research lab"
            }
          ],
          "personId": 90457
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Richardson",
              "institution": "The University of Texas at Dallas",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 90440
        }
      ]
    },
    {
      "id": 90526,
      "typeId": 12457,
      "title": "A Method of Estimating the Object of Interest from 3D Object and User’s Gesture in VR",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565652"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1063",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "In VR, gaze information is useful for directly or indirectly analyzing a user's interest. However, there are inconveniences in using the eye tracking function included in the VR device. To overcome the drawback, we propose a method of estimating an object of interest from user’s gesture instead of eye tracking. Distance and angle-based feature are extracted from 3d information of the object and the position and rotation of the VR device. We compare accuracy of each feature for VR device combination, and find out that using all devices instead of individual devices and using angle-based feature instead of distance-based feature was more efficient with accuracy of 79.36%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 90324
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 90472
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 90345
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 90365
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute",
              "dsl": ""
            }
          ],
          "personId": 90358
        }
      ]
    },
    {
      "id": 90527,
      "typeId": 12457,
      "title": "The Effects of Gestural Filler in Reducing Perceived Waiting Time in Conversation with a Digital Human",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565657"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1065",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "The demand to utilize digital humans for conversation systems in multiple areas has increased. Inevitable time delays in a conversation with digital humans can be occurred because of network and natural language processing, which can deteriorate the users’ satisfaction and availability. In this study, we designed a gestural filler that can control the gesture of a digital human during a conversation and compared the effect of gestural filler with conversational filler.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 90419
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            },
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Busan",
              "institution": "Pusan National University",
              "dsl": ""
            }
          ],
          "personId": 90360
        }
      ]
    },
    {
      "id": 90528,
      "typeId": 12457,
      "title": "AirHaptics: Vibrotactile presentation method using an airflowfrom an audio speaker of smart devices",
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3562939.3565670"
        }
      },
      "isBreak": false,
      "importedId": "vrst22b-1022",
      "source": "PCS",
      "trackId": 11939,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        90529
      ],
      "eventIds": [],
      "abstract": "We perceive vibrotactile stimuli from smart devices such as smartphones when we use various applications. However, vibrators in these devices can present only a specific nearby resonant frequency with enough intensity to perceive, making it challenging to offer various vibrotactile stimuli. In this study, we propose a method to realize a vibrotactile presentation in a wide range of frequencies using airflow vibration generated by a built-in audio speaker of a smart device. We implemented a system based on the proposed method using a smartphone and experimented with measuring the airflow pressure. Moreover, we also propose the application of texture presentation using airflow.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90348
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90446
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Toyonaka",
              "institution": "Osaka University",
              "dsl": ""
            }
          ],
          "personId": 90396
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 90382
        }
      ]
    },
    {
      "id": 90646,
      "title": "Integrating interactive devices with the user's body",
      "isBreak": false,
      "importedId": "0",
      "source": "CSV",
      "trackId": 11985,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        90316
      ],
      "abstract": "The main question that drives my research is: what is the next interface paradigm that supersedes wearable devices? I argue that the new paradigm is one in which interactive devices will integrate with the user's biological senses and actuators.\nThis way of engineering devices that intentionally borrow parts of the user's biology puts forward a new generation of miniaturized devices; allowing us to circumvent traditional physical constraints. For instance, in the case of my devices based on electrical muscle stimulation, they demonstrate how our body-device integration circumvents the constraints imposed by the size of motors used in traditional haptic devices (e.g., robotic exoskeletons). Taking this further, we can apply this integrated approach to other modalities. For instance, we engineered a device that delivers chemicals to the user to generate temperature sensations, without the need to rely on cumbersome thermal actuators, such as air conditioners or heaters. My approach to miniaturizing devices is especially useful to advance mobile interactions, such as in virtual or augmented reality, where users have a desire to remain untethered & free.\nIntegrating devices with the user's body allows to give users new physical abilities. For example, we have engineered a device that allows users to locate odor sources by \"smelling in stereo\" as well as a device that physically accelerates the user's reaction time using muscle stimulation, which allows users to steer to safety or even catch a falling object that they would normally miss.\nWhile this integration can offer many benefits (e.g., faster reaction time, realistic simulations in VR/AR, or faster skill acquisition), it also requires tackling new challenges, such as the question of agency: do we feel in control when our body is integrated with an interface? Together with our colleagues in neuroscience, we have been measuring how our brain encodes agency to improve the design of this new type of integrated interfaces. We found that, even in the extreme case of interfaces that electrically control the user's muscles, it is possible to improve the sense of agency. More importantly, we found that it is only by preserving the user's sense of agency that these integrated devices provide benefits even after the user takes them out.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United State",
              "state": "Illinois",
              "city": "Chicago",
              "institution": "University of Chicago"
            }
          ],
          "personId": 90629
        }
      ]
    },
    {
      "id": 90647,
      "title": "The present location and future of the metaverse as seen through the development of \"Cluster\"",
      "isBreak": false,
      "importedId": "1",
      "source": "CSV",
      "trackId": 11985,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        90651
      ],
      "abstract": "The CEO of Cluster, the largest metaverse platform in Japan, will explain where the metaverse business is today and how to use it practically.\n- Appearance of the metaverse market\n- Transition of Cluster's business\n- Metaverse possibilities that have come to light while handling more than 100 projects per year\n- The reality of people and communities that make the metaverse their home",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "CEO, Cluster, Inc."
            }
          ],
          "personId": 90630
        }
      ]
    },
    {
      "id": 90648,
      "title": "Rhizomatiks Behind The Scenes",
      "isBreak": false,
      "importedId": "2",
      "source": "CSV",
      "trackId": 11985,
      "tags": [],
      "keywords": [],
      "sessionIds": [],
      "eventIds": [
        90317
      ],
      "abstract": "Rhizomatiks first became involved in the creation of artistic performances in 2005, and since 2010, have been active practitioners in the entertainment realm. Our work primarily revolves around the development and implementation of technologically-informed stage performances. In this presentation, I introduce behind the scenes of our performative work",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "city": "Tokyo",
              "institution": "Representative Director. mmmmm Co., Ltd,"
            }
          ],
          "personId": 90631
        }
      ]
    }
  ],
  "people": [
    {
      "id": 90122,
      "firstName": "Bogoan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "1k2XCnfJZiSreR-g6zEC3w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90123,
      "firstName": "Anatole",
      "lastName": "Lécuyer",
      "middleInitial": "",
      "importedId": "JerrJYQ2VgWHWr1cZ-hxxg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90124,
      "firstName": "Changkun",
      "lastName": "Ou",
      "middleInitial": "",
      "importedId": "EyP8fiBm_iH2ZxNtjh8Y9w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90125,
      "firstName": "Yuta",
      "lastName": "Itoh",
      "middleInitial": "",
      "importedId": "SL_W6xgAN8h-_ktvw92OUg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90126,
      "firstName": "Ryoichi",
      "lastName": "Kaku",
      "middleInitial": "",
      "importedId": "YgMAbhxOOhZ0VjuvYAH4bQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90127,
      "firstName": "Anna",
      "lastName": "Gerhardus",
      "middleInitial": "",
      "importedId": "cctrT3klC9dvW-cYFK1OgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90128,
      "firstName": "Myoung Gon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "f7s_Rh9yt1JA7IQnlLtNJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90129,
      "firstName": "Liwei",
      "lastName": "Chan",
      "middleInitial": "",
      "importedId": "KB4Ew3efN16WCB23RZP3hA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90130,
      "firstName": "Géry",
      "lastName": "Casiez",
      "middleInitial": "",
      "importedId": "DIpebubG6g2BUtnvQQ6Pcg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90131,
      "firstName": "Claudio",
      "lastName": "Pacchierotti",
      "middleInitial": "",
      "importedId": "JVJkDgsVFNUp9CpLclb0-w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90132,
      "firstName": "Aunnoy K",
      "lastName": "Mutasim",
      "middleInitial": "",
      "importedId": "7VrRlKHxdivIp7n_RDmwRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90133,
      "firstName": "Alexander",
      "lastName": "Kalus",
      "middleInitial": "",
      "importedId": "z34zA6WgKv1OBOv5nPw_Zg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90134,
      "firstName": "Xiaoyan",
      "lastName": "Zhou",
      "middleInitial": "",
      "importedId": "_n940mrWtxpLOyY2RIC5uQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90135,
      "firstName": "Joseph",
      "lastName": "LaViola",
      "middleInitial": "",
      "importedId": "6NYVtJkJmZFRips8wm-VXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90136,
      "firstName": "Mingon",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "ZMVonETxblPl6vwjROEVZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90137,
      "firstName": "Francesco",
      "lastName": "Chiossi",
      "middleInitial": "",
      "importedId": "s1Xyc1j9ixsl9NOK4VT4lQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90138,
      "firstName": "Richard",
      "lastName": "Göllner",
      "middleInitial": "",
      "importedId": "2KP6yF6mDF33Jlx1qyw60A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90139,
      "firstName": "Niels",
      "lastName": "Henze",
      "middleInitial": "",
      "importedId": "HOEWm6ALiUarpg2dwCcZSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90140,
      "firstName": "Christian",
      "lastName": "Wolff",
      "middleInitial": "",
      "importedId": "-WQ3zXmNlLxWo_bWvOLZDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90141,
      "firstName": "Greg",
      "lastName": "Welch",
      "middleInitial": "",
      "importedId": "lkkZ7KrKLEQK6RLXPh4x6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90142,
      "firstName": "Francisco",
      "lastName": "Ortega",
      "middleInitial": "Raul",
      "importedId": "FqBQvhiBHqCfL2wO-lHJmQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90143,
      "firstName": "Sebastian",
      "lastName": "Vizcay",
      "middleInitial": "",
      "importedId": "_5pyA40WHgXXUWrOVA_y-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90144,
      "firstName": "Hong",
      "lastName": "Gao",
      "middleInitial": "",
      "importedId": "8fbAAFKJl4JZ4AM_i_mHRA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90145,
      "firstName": "Simone",
      "lastName": "Kriglstein",
      "middleInitial": "",
      "importedId": "xCFrAOZBoCiEpsqJkolynw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90146,
      "firstName": "Chung Han",
      "lastName": "Liang",
      "middleInitial": "",
      "importedId": "OHL8Szv8AYaFiZ_9KZYfJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90147,
      "firstName": "Dongyun",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "mDj1LrfjaaVkQuvQbr_bSQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90148,
      "firstName": "Mario",
      "lastName": "Botsch",
      "middleInitial": "",
      "importedId": "_t2uR9-0YZ6w7iETc7sKWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90149,
      "firstName": "Valentin",
      "lastName": "Schwind",
      "middleInitial": "",
      "importedId": "RQzBxU_wVwvPHVcQsWWeXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90150,
      "firstName": "M. Rasel",
      "lastName": "Mahmud",
      "middleInitial": "",
      "importedId": "Kkr-yyZOK8OCg5XVXojW7g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90151,
      "firstName": "Stefan",
      "lastName": "Suette",
      "middleInitial": "",
      "importedId": "RpNDO6uneU0b2KKMSxTNag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90152,
      "firstName": "Barbara",
      "lastName": "Tversky",
      "middleInitial": "",
      "importedId": "xwUYXgECvoo6B6oyd2C51w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90153,
      "firstName": "Moaaz",
      "lastName": "Hudhud Mughrabi",
      "middleInitial": "",
      "importedId": "msJi9hQYvLi3duyE9F3e2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90154,
      "firstName": "Carl",
      "lastName": "Oechsner",
      "middleInitial": "",
      "importedId": "EDSiz7tRStDldjkMFgvJBQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90155,
      "firstName": "Enkelejda",
      "lastName": "Kasneci",
      "middleInitial": "",
      "importedId": "w-Y3_-0lnRuNdG4CAkTJQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90156,
      "firstName": "Marc Erich",
      "lastName": "Latoschik",
      "middleInitial": "",
      "importedId": "4yN36kev1hbUnLrcETD5iA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90157,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "-lrGWSm4EFKCsnZiKuIu3g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90158,
      "firstName": "Jennifer",
      "lastName": "Kim",
      "middleInitial": "G",
      "importedId": "TgQEc3ore7X_lFwtsmlxaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90159,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "importedId": "2Guk0EWER7zUs-hWrmCSsQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90160,
      "firstName": "Evan",
      "lastName": "Suma Rosenberg",
      "middleInitial": "",
      "importedId": "2SQvdqF_rOgw9h5fYmNZqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90161,
      "firstName": "Fabio",
      "lastName": "Sarto",
      "middleInitial": "",
      "importedId": "ID5oNmi2JhM1Ba9s74q7tQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90162,
      "firstName": "Julia",
      "lastName": "Schmid",
      "middleInitial": "",
      "importedId": "xpaorKt7lv6pFZnI5qIA_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90163,
      "firstName": "Dayoung",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "eheyKnFbbsYTpN8_LmFchA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90164,
      "firstName": "Nahal",
      "lastName": "Norouzi",
      "middleInitial": "",
      "importedId": "tEtQiWqz1OB_Xid3RqhzKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90165,
      "firstName": "Sho",
      "lastName": "Sakurai",
      "middleInitial": "",
      "importedId": "ghsprvNpyGZPfjoyt7sLDw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90166,
      "firstName": "Anil Ufuk",
      "lastName": "Batmaz",
      "middleInitial": "",
      "importedId": "EBYUW-F91fpOLGTE4b7d9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90167,
      "firstName": "Taehyung",
      "lastName": "Noh",
      "middleInitial": "",
      "importedId": "7MhUxDcuWwAKb5hDI1yNnQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90168,
      "firstName": "Seung-wook",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "NNKKRIGyrsp9qfyogeTy-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90169,
      "firstName": "George",
      "lastName": "Koulieris",
      "middleInitial": "Alex",
      "importedId": "vcUNmm9ixYTwjI4gZypqSg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90170,
      "firstName": "JunYoung",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "QoSIaD6N8fI-kOTsmxEt2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90171,
      "firstName": "Francisco",
      "lastName": "Guido-Sanz",
      "middleInitial": "",
      "importedId": "BrorC7W7oYLxESJwOwv6Xg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90172,
      "firstName": "Sung-In",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "JxmOWMAG7q1TGNFQi177QA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90173,
      "firstName": "Youngbo",
      "lastName": "Shim",
      "middleInitial": "Aram",
      "importedId": "uGiAmZuFdnys888PYi0DXA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90174,
      "firstName": "Simon",
      "lastName": "McCallum",
      "middleInitial": "James Robertson",
      "importedId": "68dkTCbSDBUL0Gz0pTKqzQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90175,
      "firstName": "Ruei-Che",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "fj-AI7DGbP0P11WOCt3b2w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90176,
      "firstName": "Hwajung",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "YkLIi9h4rzvE5V9lmvuTsA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90177,
      "firstName": "Ryota",
      "lastName": "Shijo",
      "middleInitial": "",
      "importedId": "p5stZ8_7WKKPH2pWWRdNlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90178,
      "firstName": "Won-Ki",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "WKLvGBqIoHw2MZBm0oIhxQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90179,
      "firstName": "Sven",
      "lastName": "Mayer",
      "middleInitial": "",
      "importedId": "aGMdv_UqvRAFIxAEt3rNsw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90180,
      "firstName": "Eunchan",
      "lastName": "Jo",
      "middleInitial": "",
      "importedId": "-cJRHrYijYc--C5HlThlZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90181,
      "firstName": "Panagiotis",
      "lastName": "Kourtesis",
      "middleInitial": "",
      "importedId": "zx-sGECQU1l-AgeIQH2XJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90182,
      "firstName": "Dominik",
      "lastName": "Schön",
      "middleInitial": "",
      "importedId": "Z4hyyGSPDtY18hRG6c3Umg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90183,
      "firstName": "So-youn",
      "lastName": "Jang",
      "middleInitial": "",
      "importedId": "Bnj4kNMu8i9-OYGUpRWWwg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90184,
      "firstName": "Alex",
      "lastName": "Rasla",
      "middleInitial": "",
      "importedId": "Fsfv1FQ2wIltgmkSP7y3fQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90185,
      "firstName": "Ziyi",
      "lastName": "Chang",
      "middleInitial": "",
      "importedId": "s09DiKHY_H8E2Zrppz73AA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90186,
      "firstName": "Mai",
      "lastName": "Otsuki",
      "middleInitial": "",
      "importedId": "CUAzjWZsVrlbaKXKoIAsbw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90187,
      "firstName": "Hyechan",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "yfQWI1RvND25OpzIwMjbJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90188,
      "firstName": "Patrick",
      "lastName": "BAERT",
      "middleInitial": "",
      "importedId": "lSh2sNm0qSj4c3ZJRBR5yg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90189,
      "firstName": "Gwendal",
      "lastName": "Fouché",
      "middleInitial": "",
      "importedId": "GYONAd2FjlPaES8EkwYQdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90190,
      "firstName": "Anne-Lise",
      "lastName": "Saive",
      "middleInitial": "",
      "importedId": "D79AcVMV23IaXJzhMcDkUQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90191,
      "firstName": "Jack",
      "lastName": "Ratcliffe",
      "middleInitial": "",
      "importedId": "Y_t63GVc3i1sze93RedaOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90192,
      "firstName": "Laurissa",
      "lastName": "Tokarchuk",
      "middleInitial": "",
      "importedId": "6-owT7GvxSVaS70KAOKJXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90193,
      "firstName": "Craig",
      "lastName": "Anslow",
      "middleInitial": "",
      "importedId": "WTJl-BhdB_f8UOtn0_BAfw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90194,
      "firstName": "Hesham",
      "lastName": "Elsayed",
      "middleInitial": "",
      "importedId": "XVG4m3thVii7RS-2pHTlWw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90195,
      "firstName": "Yerin",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "maKFsSTq4JsJqL9r4r_CCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90196,
      "firstName": "Kenneth",
      "lastName": "Kartono",
      "middleInitial": "",
      "importedId": "1TJM4ldSQXFuwjzP7Ca5Qg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90197,
      "firstName": "John",
      "lastName": "Quarles",
      "middleInitial": "",
      "importedId": "g5maG3eqSKqjwlLVn6LVkA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90198,
      "firstName": "Hideaki",
      "lastName": "Kuzuoka",
      "middleInitial": "",
      "importedId": "b7NCSK2IREioerbZhPPDhg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90199,
      "firstName": "Koichi",
      "lastName": "Hirota",
      "middleInitial": "",
      "importedId": "M06nQ6a-1TTHHS7A2sC0TA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90200,
      "firstName": "Tiffany",
      "lastName": "Do",
      "middleInitial": "D.",
      "importedId": "T9EYz_zECg9uuM0ke0SzLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90201,
      "firstName": "Charles",
      "lastName": "JAVERLIAT",
      "middleInitial": "",
      "importedId": "NdbL_e9rXpIb7CRZhDc61A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90202,
      "firstName": "Fei",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "toM0e2sEejELMbdbBl8HaA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90203,
      "firstName": "Efe",
      "lastName": "Bozkir",
      "middleInitial": "",
      "importedId": "l9nt37OSTR15diY4wqi2yw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90204,
      "firstName": "Miriam",
      "lastName": "Witte",
      "middleInitial": "",
      "importedId": "JS1xITRUWBHI1TIeBwVB5w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90205,
      "firstName": "Martin",
      "lastName": "Kocur",
      "middleInitial": "",
      "importedId": "Y5ftVlnXsxwlzIEps73qXQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90206,
      "firstName": "Gerd",
      "lastName": "Bruder",
      "middleInitial": "",
      "importedId": "tZQ7gLt4C6wd0tuTsCuv8A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90207,
      "firstName": "Jae-Won",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "ipbuYhdGVWqtzg1Dz5FNQA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90208,
      "firstName": "Hong-Sheng",
      "lastName": "Chen",
      "middleInitial": "",
      "importedId": "-ASoKBEN3rflXS-VeunuLQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90209,
      "firstName": "Donghan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "vH5FcPndhOFeWAT3RP5aVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90210,
      "firstName": "JungHyun",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "CAx9sFY-lgPi4-yQD9IsDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90211,
      "firstName": "Oyewole",
      "lastName": "Oyekoya",
      "middleInitial": "",
      "importedId": "-pCsQ7EfwUBcvK6vYnWoJw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90212,
      "firstName": "Jen-Shuo",
      "lastName": "Liu",
      "middleInitial": "",
      "importedId": "AyBxvEzlYCvX5RGt5hkyNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90213,
      "firstName": "Anthony",
      "lastName": "Steed",
      "middleInitial": "",
      "importedId": "L7vbmDhPr9tJmz7gPBK-nw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90214,
      "firstName": "Amelie",
      "lastName": "Karber",
      "middleInitial": "",
      "importedId": "eKM7bFX0Q64bMjDZivUNbQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90215,
      "firstName": "Joohwan",
      "lastName": "Chae",
      "middleInitial": "",
      "importedId": "jit_3Lrdpd5an6IJl2bXuw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90216,
      "firstName": "Isaac",
      "lastName": "Cho",
      "middleInitial": "",
      "importedId": "NVG_tJu416rMRnXdM5ABWg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90217,
      "firstName": "Jean-Marie",
      "lastName": "Normand",
      "middleInitial": "",
      "importedId": "3-_XwMguOW10Tduq4Kui-Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90218,
      "firstName": "Ching-Wen",
      "lastName": "Hung",
      "middleInitial": "",
      "importedId": "yddadgl3HkQf_5i5nWJjCA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90219,
      "firstName": "Daniel",
      "lastName": "Medeiros",
      "middleInitial": "",
      "importedId": "STkQ0KbP4yrwwsBQFvw5Tg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90220,
      "firstName": "Georg",
      "lastName": "Regal",
      "middleInitial": "",
      "importedId": "eenUZbN-XAyOvRAbf20J_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90221,
      "firstName": "Geehyuk",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "5oayHmDaeFMZJ_onK64caA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90222,
      "firstName": "Manuel",
      "lastName": "Mayer",
      "middleInitial": "",
      "importedId": "2NkuWiLhdN1LdCg6zd3pSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90223,
      "firstName": "Timo",
      "lastName": "Menzel",
      "middleInitial": "",
      "importedId": "JPnSqTmxQ92Hd2BiNkxHzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90224,
      "firstName": "Adam",
      "lastName": "Williams",
      "middleInitial": "Sinclair",
      "importedId": "i9suEDunW8-AFXHZ6AgNSA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90225,
      "firstName": "Charles",
      "lastName": "Kervrann",
      "middleInitial": "",
      "importedId": "efBfi-zvAPdkF-6O25Z40A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90226,
      "firstName": "DongHoon",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "Z4jhxQ2DRJ2C4VawsmgZAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90227,
      "firstName": "Wolfgang",
      "lastName": "Stuerzlinger",
      "middleInitial": "",
      "importedId": "wLJOcQYW6AQNHnW4Y2Dcyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90228,
      "firstName": "Wooseok",
      "lastName": "Jeong",
      "middleInitial": "",
      "importedId": "Br3GYXN3qGNjRJ4rhZTtCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90229,
      "firstName": "Jacob",
      "lastName": "Belga",
      "middleInitial": "",
      "importedId": "Dn2Mm3EiOETyEZVQ3S6RjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90230,
      "firstName": "Brian",
      "lastName": "Robinson",
      "middleInitial": "",
      "importedId": "GqyCVb9SQXmuErV6KbGDjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90231,
      "firstName": "Pierre-Philippe",
      "lastName": "ELST",
      "middleInitial": "",
      "importedId": "mbh7EyijPImd_LUOTUlIOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90232,
      "firstName": "Lisa",
      "lastName": "Hasenbein",
      "middleInitial": "",
      "importedId": "N_KKX07XaXnkehpcbeSkfQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90233,
      "firstName": "Julien",
      "lastName": "Cauquis",
      "middleInitial": "",
      "importedId": "advPcQPCRKkmAKpFg3TEww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90234,
      "firstName": "Max",
      "lastName": "Mühlhäuser",
      "middleInitial": "",
      "importedId": "IxtRKZB2rjeAvVGqICIm1w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90235,
      "firstName": "Mayra",
      "lastName": "Barrera Machuca",
      "middleInitial": "Donaji",
      "importedId": "kelrth_S2MBODQVWUWbO2g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90236,
      "firstName": "Hee Jeong",
      "lastName": "Yoo",
      "middleInitial": "",
      "importedId": "UAZLtcbacsfniSKKP1saXw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90237,
      "firstName": "Victor",
      "lastName": "Mercado",
      "middleInitial": "Rodrigo",
      "importedId": "eObRdH3XBm6KlgQU8_PfZw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90238,
      "firstName": "Takuya",
      "lastName": "Nojima",
      "middleInitial": "",
      "importedId": "cu5_LEHxH_ToOVSg6vfDGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90239,
      "firstName": "Gerard",
      "lastName": "Kim",
      "middleInitial": "Jounghyun",
      "importedId": "ZGhcicMSEzgS5hBiS0kmNQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90240,
      "firstName": "Naohiko",
      "lastName": "Morimoto",
      "middleInitial": "",
      "importedId": "s9x03ETNxy6tHOoOMRVXtQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90241,
      "firstName": "Ryan",
      "lastName": "Ghamandi",
      "middleInitial": "",
      "importedId": "vn14NtoTBqqamDZ0Xkx6Kw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90242,
      "firstName": "Hubert P. H.",
      "lastName": "Shum",
      "middleInitial": "",
      "importedId": "eWDq0onigkgAdC1acPnTOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90243,
      "firstName": "Yuichi",
      "lastName": "Hiroi",
      "middleInitial": "",
      "importedId": "vNuqKst4mZKU8N_lQ1s5VA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90244,
      "firstName": "Michinari",
      "lastName": "Kono",
      "middleInitial": "",
      "importedId": "WjbMaLTtyf9dDmByV3HGPA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90245,
      "firstName": "Michael",
      "lastName": "Stewart",
      "middleInitial": "",
      "importedId": "DrzJBHZj963_p3aFp9Y7kQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90246,
      "firstName": "Martin",
      "lastName": "Weigel",
      "middleInitial": "",
      "importedId": "b7OoAkw-1tCvVIv1wraijQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90247,
      "firstName": "Ryan",
      "lastName": "Schubert",
      "middleInitial": "",
      "importedId": "Cf4aP79T8YYKPE6vIVVKIw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90248,
      "firstName": "Steven",
      "lastName": "Feiner",
      "middleInitial": "",
      "importedId": "Aa5f6_SuhPINKZckqXg-zA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90249,
      "firstName": "Jiaheng",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "OO6GkmsXgb83fxJE-Q3Nww",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90250,
      "firstName": "Ryan P.",
      "lastName": "McMahan",
      "middleInitial": "",
      "importedId": "XE6QYaBBZp9ruyJdbOCCig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90251,
      "firstName": "Dennis",
      "lastName": "Dietz",
      "middleInitial": "",
      "importedId": "vgxLIoAibzGRpJdX-ltEUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90252,
      "firstName": "Aisha",
      "lastName": "Frampton - Clerk",
      "middleInitial": "",
      "importedId": "Xge61vXQLXMzeKIXwYBCAw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90253,
      "firstName": "Johanna",
      "lastName": "Bogon",
      "middleInitial": "",
      "importedId": "2Pq5Z_08lB_NSAxvH30GgA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90254,
      "firstName": "Taewan",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "cDWhQhTcrX5qDq9Ebltswg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90255,
      "firstName": "Manfred",
      "lastName": "Tscheligi",
      "middleInitial": "",
      "importedId": "H0xTlBTd7quX3RMy28fTEw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90256,
      "firstName": "Jakob",
      "lastName": "Uhl",
      "middleInitial": "Carl",
      "importedId": "-m0qOT0hxH5B0tOLN_8sTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90257,
      "firstName": "Martin",
      "lastName": "Schmitz",
      "middleInitial": "",
      "importedId": "2TAkNV6pUi3rf_iBjrN1nQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90258,
      "firstName": "Emmanuel",
      "lastName": "Faure",
      "middleInitial": "",
      "importedId": "cd6c2siM2J__C871kAVozQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90259,
      "firstName": "Nels",
      "lastName": "Numan",
      "middleInitial": "",
      "importedId": "hV7b-GaEhc2gFE6tK_2xTg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90260,
      "firstName": "Tzu-Yang",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "z5mxllJnJRyJZzB7xlP1DQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90261,
      "firstName": "Maud",
      "lastName": "Marchal",
      "middleInitial": "",
      "importedId": "qMER2F2ivZu1wOxuZb-8og",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90262,
      "firstName": "Sunbum",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "w92aTpJvY_RwolUGbFYGZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90263,
      "firstName": "Guillaume",
      "lastName": "Lavoué",
      "middleInitial": "",
      "importedId": "fw2Qf4IxSu0r5h1anBnLCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90264,
      "firstName": "Andreas",
      "lastName": "Butz",
      "middleInitial": "",
      "importedId": "CWYpLecThpHr6-Hh2dxUZA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90265,
      "firstName": "Michael",
      "lastName": "Beyeler",
      "middleInitial": "",
      "importedId": "wjBsdN_V-j_WqnrQvKadqQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90266,
      "firstName": "Ferran",
      "lastName": "Argelaguet Sanz",
      "middleInitial": "",
      "importedId": "hDnwHAn8UT3bSm9Ib36Hig",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90267,
      "firstName": "Elisabeth",
      "lastName": "Frankus",
      "middleInitial": "",
      "importedId": "mqS20WBnNx35ctnpVKJePw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90268,
      "firstName": "Joaquim",
      "lastName": "Jorge",
      "middleInitial": "",
      "importedId": "pYUAtq39KaBwxswF29fvQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90269,
      "firstName": "Alberto",
      "lastName": "Cordova",
      "middleInitial": "",
      "importedId": "ycf6mO6QJcSytjqyuZVFSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90270,
      "firstName": "Matt",
      "lastName": "Gottsacker",
      "middleInitial": "",
      "importedId": "2DvYg340wgRhBzjPWqP5RA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90271,
      "firstName": "Kyungsik",
      "lastName": "Han",
      "middleInitial": "",
      "importedId": "oVcowPaM8UEB0TFHi7Yztg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90322,
      "firstName": "Madoka",
      "lastName": "Inoue",
      "middleInitial": "",
      "importedId": "rVJPttKXskNBm1ZgukW_XQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90323,
      "firstName": "Dario",
      "lastName": "Lanfranconi",
      "middleInitial": "",
      "importedId": "TIJVG36_GjzdyHtONETUHA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90324,
      "firstName": "Sungjin",
      "lastName": "Hong",
      "middleInitial": "",
      "importedId": "J6z9hxqOetm8DzU1Kb3c2Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90325,
      "firstName": "Gopi",
      "lastName": "Meenakshisundaram",
      "middleInitial": "",
      "importedId": "s0BNlNUTMhUtNrTOGhnqzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90326,
      "firstName": "Kelvin",
      "lastName": "Sung",
      "middleInitial": "",
      "importedId": "Gya5Thek6NuHQv2DDnnILg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90327,
      "firstName": "Marie-Ève",
      "lastName": "Dumas",
      "middleInitial": "",
      "importedId": "91BmMGYOcadIuNzs0FBnNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90328,
      "firstName": "Dinh Tung",
      "lastName": "Le",
      "middleInitial": "",
      "importedId": "GvB6kCcB3XLD0WeX48tlmA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90329,
      "firstName": "Zhetao",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "rAkUqaz1qhGzGp7QyLJ58g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90330,
      "firstName": "Rachel",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "adq93-gArRee5xehxexA9Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90331,
      "firstName": "Inoussa",
      "lastName": "OUEDRAOGO",
      "middleInitial": "",
      "importedId": "6QxJgBnGiqKtD0IG-F6sEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90332,
      "firstName": "Huidong",
      "lastName": "Bai",
      "middleInitial": "",
      "importedId": "0RRfCcrZOAaUQeV84nf4GA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90333,
      "firstName": "Shih-Chieh",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "Ho2Lf10Qe0wFc3DO10SUOA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90334,
      "firstName": "Erica",
      "lastName": "Delhagen",
      "middleInitial": "",
      "importedId": "DACy-9uHYlw7jciRHcrtCw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90335,
      "firstName": "Nilanjan",
      "lastName": "Sarkar",
      "middleInitial": "",
      "importedId": "bgTdnr7ELvs5tKBYmOXapw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90336,
      "firstName": "Éva",
      "lastName": "Décorps",
      "middleInitial": "",
      "importedId": "bW05V29lF4ylyElDHupIow",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90337,
      "firstName": "Jingbo",
      "lastName": "Zhao",
      "middleInitial": "",
      "importedId": "pS5AHvgVxCIG4MWAxpWavQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90338,
      "firstName": "Takuya",
      "lastName": "Oka",
      "middleInitial": "",
      "importedId": "l95u0xWST4v6tWXnGU-TAQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90339,
      "firstName": "Hidenori",
      "lastName": "Aoki",
      "middleInitial": "",
      "importedId": "PF12keC5WhhXp9GREgRe-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90340,
      "firstName": "Henry",
      "lastName": "Kim",
      "middleInitial": "T",
      "importedId": "SVhXH_aA3kTjUm-idMbCnw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90341,
      "firstName": "Shigeharu",
      "lastName": "Ono",
      "middleInitial": "",
      "importedId": "4nlUMmGqWXStKAoaKDQNCQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90342,
      "firstName": "Simon",
      "lastName": "Kloiber",
      "middleInitial": "",
      "importedId": "c3TWlILR1fnSb6aseBz28g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90343,
      "firstName": "Minsoo",
      "lastName": "Choi",
      "middleInitial": "",
      "importedId": "4YkYdS4W3YSS8g3ZIvegBw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90344,
      "firstName": "Junyi",
      "lastName": "Shen",
      "middleInitial": "",
      "importedId": "fI7aX5oLCetOAIYDSwjUhw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90345,
      "firstName": "Cho-rong",
      "lastName": "Yu",
      "middleInitial": "",
      "importedId": "Gvd5hp6n2ON9W2tXIPsCNA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90346,
      "firstName": "Shinichi",
      "lastName": "Koyama",
      "middleInitial": "",
      "importedId": "W-mqXLi1RT9ZuhVEcksO4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90347,
      "firstName": "Gyubin",
      "lastName": "An",
      "middleInitial": "",
      "importedId": "-W4ACstasfHjsuZStoilzA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90348,
      "firstName": "Madoka",
      "lastName": "Ito",
      "middleInitial": "",
      "importedId": "Uz3lGfuNB39QwDSKH5tVEA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90349,
      "firstName": "Becky",
      "lastName": "Lake",
      "middleInitial": "",
      "importedId": "nxPmpP3k71Y_tWvJcpuxrw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90350,
      "firstName": "Genta",
      "lastName": "Ono",
      "middleInitial": "",
      "importedId": "xBwMcIhbgkF1PlmIpTLFkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90351,
      "firstName": "Yiqin",
      "lastName": "Peng",
      "middleInitial": "",
      "importedId": "PVoEvCiInqEughHRYNMZ1Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90352,
      "firstName": "Gavin",
      "lastName": "Paul",
      "middleInitial": "",
      "importedId": "f7WDgZ4PlMWV1SFDFaDa7Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90353,
      "firstName": "Madeline",
      "lastName": "Ebeling",
      "middleInitial": "",
      "importedId": "1m2SXSsMCVPONit4r0V2bA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90354,
      "firstName": "Emmanuel",
      "lastName": "Durand",
      "middleInitial": "",
      "importedId": "x65ozpyvjc5DQ0tOm8tSug",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90355,
      "firstName": "Farid",
      "lastName": "Breidi",
      "middleInitial": "",
      "importedId": "2mWXE9SvIhwSZ2gax0XPHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90356,
      "firstName": "Saki",
      "lastName": "Yamada",
      "middleInitial": "",
      "importedId": "L-2hIwSazMoMfHDCXDQZjQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90357,
      "firstName": "Jun Wei",
      "lastName": "Lim",
      "middleInitial": "",
      "importedId": "TEaP-mVEL0UeeI3y5ggNfg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90358,
      "firstName": "Youn-Hee",
      "lastName": "Gil",
      "middleInitial": "",
      "importedId": "fPhXPQXUq49Lv5pFGsDmyQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90359,
      "firstName": "Krzysztof",
      "lastName": "Pietroszek",
      "middleInitial": "",
      "importedId": "RQkW54W8iMuWzX50Zs-1_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90360,
      "firstName": "Myungho",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "IlMypZMz0rZWjbjhBlNh-A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90361,
      "firstName": "Daniel",
      "lastName": "Lichtman",
      "middleInitial": "",
      "importedId": "FvGGgplibk8DukGJnYzx6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90362,
      "firstName": "Spyros",
      "lastName": "Vosinakis",
      "middleInitial": "",
      "importedId": "JmKQHmrw8HaWFW4g2TPAkw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90363,
      "firstName": "Sebastian",
      "lastName": "Misztal",
      "middleInitial": "",
      "importedId": "miYmIc2X_Jwx8vfc8ghoQQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90364,
      "firstName": "Youjin",
      "lastName": "Sung",
      "middleInitial": "",
      "importedId": "-twTU8jAgdg5w58U7AVZ6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90365,
      "firstName": "Seong Min",
      "lastName": "Baek",
      "middleInitial": "",
      "importedId": "RCG1ajdxKvPMk4htzD6aUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90366,
      "firstName": "Muhammad Twaha",
      "lastName": "Ibrahim",
      "middleInitial": "",
      "importedId": "1EzbsicbSUizy44qjzplZg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90367,
      "firstName": "James",
      "lastName": "Oliver",
      "middleInitial": "",
      "importedId": "iYrtV4OZ5XkLouvXFtT7sw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90368,
      "firstName": "Mohammad",
      "lastName": "Ahmadi",
      "middleInitial": "",
      "importedId": "qHISaoQJPtdimLc0A0ZOwA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90369,
      "firstName": "Patrick",
      "lastName": "Bourdot",
      "middleInitial": "",
      "importedId": "W6TFUKBczgZtcou4wzykLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90370,
      "firstName": "Takuya",
      "lastName": "Kawashima",
      "middleInitial": "",
      "importedId": "uR3Q8TwuawR7PNGiOk-pHQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90371,
      "firstName": "Katharina",
      "lastName": "Krösl",
      "middleInitial": "",
      "importedId": "zaq6OciOU3I6zYIOVoGoDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90372,
      "firstName": "Akira",
      "lastName": "Kubota",
      "middleInitial": "",
      "importedId": "46HxqzET1mqIeR9WW6w-fw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90373,
      "firstName": "Soh",
      "lastName": "Masuko",
      "middleInitial": "",
      "importedId": "T6mJLgj-kID3Dguuyyk2VQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90374,
      "firstName": "Jayesh",
      "lastName": "Pillai",
      "middleInitial": "S.",
      "importedId": "_ke7UYKA7p8jGwrFlZmbTQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90375,
      "firstName": "Burkhard",
      "lastName": "Wuensche",
      "middleInitial": "",
      "importedId": "x4Ej9QZ-u2NH1SKJXqEiGA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90376,
      "firstName": "Christos",
      "lastName": "Mousas",
      "middleInitial": "",
      "importedId": "oKaU0gwfb1XBqAwCudlf4g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90377,
      "firstName": "Ikkaku",
      "lastName": "Kawaguchi",
      "middleInitial": "",
      "importedId": "K5BHzMVJJdf5oz0xCO_wmg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90378,
      "firstName": "Ines",
      "lastName": "Said",
      "middleInitial": "",
      "importedId": "S-mFuxlTT_KrHPF9bYtxbg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90379,
      "firstName": "Zofia",
      "lastName": "Marciniak",
      "middleInitial": "",
      "importedId": "wN7vr6Ria4pXD4QK7ldQ8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90380,
      "firstName": "Chiu-Pin",
      "lastName": "Kuo",
      "middleInitial": "",
      "importedId": "fM8iGv_AM2h0eNc-AvlB6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90381,
      "firstName": "Keita",
      "lastName": "Watanabe",
      "middleInitial": "",
      "importedId": "sKZhwjFHTo6NvDj3ZznH7w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90382,
      "firstName": "Takefumi",
      "lastName": "Hiraki",
      "middleInitial": "",
      "importedId": "m53sVakADN_xwoJWDCDtzg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90383,
      "firstName": "Christian",
      "lastName": "Frisson",
      "middleInitial": "",
      "importedId": "YTn7eZhL_IUbl01IfaFvRw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90384,
      "firstName": "Kaito",
      "lastName": "Yamaguchi",
      "middleInitial": "",
      "importedId": "KFyJNSbTLm5sXoAM2tResQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90385,
      "firstName": "Keisuke",
      "lastName": "Mizuno",
      "middleInitial": "",
      "importedId": "oiG-rtVmXLe150ZfI34OIA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90386,
      "firstName": "Hiiro",
      "lastName": "Okano",
      "middleInitial": "",
      "importedId": "KfiRPQrC2wuhnYVOhCD_bw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90387,
      "firstName": "Amelia",
      "lastName": "Roth",
      "middleInitial": "",
      "importedId": "hNVxJ2sr7vWIeCLn6_QjDA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90388,
      "firstName": "Margrethe",
      "lastName": "Horlyck-Romanovsky",
      "middleInitial": "",
      "importedId": "xzToLmQ0oQR-c8W0SvMmew",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90389,
      "firstName": "Kensuke",
      "lastName": "Koda",
      "middleInitial": "",
      "importedId": "w5nf80AdF_MOyPrdn8ejoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90390,
      "firstName": "Manoka",
      "lastName": "Waguri",
      "middleInitial": "",
      "importedId": "SAkexByybtsOrYUUvaoKXg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90391,
      "firstName": "Mohammadamin",
      "lastName": "Sanaei",
      "middleInitial": "",
      "importedId": "7SKLfEuzvJuAyI5m2X3rpQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90392,
      "firstName": "Elena",
      "lastName": "Dzardanova",
      "middleInitial": "",
      "importedId": "q-CHSf8jtStmmMYp00RuWA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90393,
      "firstName": "Akshith",
      "lastName": "Ullal",
      "middleInitial": "",
      "importedId": "KU0Ad7Wlho6_ZCTG76xX6w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90394,
      "firstName": "Amelia",
      "lastName": "Winger-Bearskin",
      "middleInitial": "",
      "importedId": "u6pSVlZlkJCPgNsElb9KlQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90395,
      "firstName": "Anna",
      "lastName": "Tarrach",
      "middleInitial": "",
      "importedId": "iNzOR-4xUjxuqMuyJkkTMw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90396,
      "firstName": "Hiroki",
      "lastName": "Ishizuka",
      "middleInitial": "",
      "importedId": "_gNXVFyflW0dJ1uvW1J_8Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90397,
      "firstName": "Ayush",
      "lastName": "Bhardwaj",
      "middleInitial": "",
      "importedId": "NLp2gaMiixMaRcPsNfUYKw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90398,
      "firstName": "Artur",
      "lastName": "Ritter",
      "middleInitial": "",
      "importedId": "IUclVKtW86PV3yqpppd3Lg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90399,
      "firstName": "Yoshihito",
      "lastName": "Tanaka",
      "middleInitial": "",
      "importedId": "rOzPDGgjxuo0rEfsZkeegw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90400,
      "firstName": "Haruna",
      "lastName": "Miyakawa",
      "middleInitial": "",
      "importedId": "ZnPmBVOBdb_tma74lozfUw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90401,
      "firstName": "Frank",
      "lastName": "Guan",
      "middleInitial": "Yunqing",
      "importedId": "xZtQGld_emOcUMBqh4wdSw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90402,
      "firstName": "Erwin",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "0fja23ihOgAOb3QPKRRbzw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90403,
      "firstName": "Homei",
      "lastName": "Miyashita",
      "middleInitial": "",
      "importedId": "8SUm8tZAhEI_3G3BIXTBiQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90404,
      "firstName": "Huyen",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "UH8kKG-jMBE5KZaF0JM13w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90405,
      "firstName": "Seongho",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "GK-oJSu412BHGThsNP4uLw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90406,
      "firstName": "Xinxing",
      "lastName": "Xia",
      "middleInitial": "",
      "importedId": "Gbud2tkPxBpL4QjSlckyCg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90407,
      "firstName": "Dac Dang Khoa",
      "lastName": "Nguyen",
      "middleInitial": "",
      "importedId": "9LlCZrXMNC2sEc3zKAe08Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90408,
      "firstName": "Richard",
      "lastName": "Wetzel",
      "middleInitial": "",
      "importedId": "Be57RjAJ7NzqUmDaBMuWEg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90409,
      "firstName": "Manuel",
      "lastName": "Rebol",
      "middleInitial": "",
      "importedId": "8vrrWQZfok2esgnEZyyrdA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90410,
      "firstName": "Mesut",
      "lastName": "Akdere",
      "middleInitial": "",
      "importedId": "LEQD8bf4iQIvm-G3sJlpZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90411,
      "firstName": "Ayaka",
      "lastName": "Yoshizawa",
      "middleInitial": "",
      "importedId": "4sOjoQAcLDIBQ-YjPNzzPQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90412,
      "firstName": "Tsukina",
      "lastName": "Matsuhashi",
      "middleInitial": "",
      "importedId": "7ym9cD6Wso0Bs9L-IFg0Rg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90413,
      "firstName": "Ming-Feng",
      "lastName": "Ke",
      "middleInitial": "",
      "importedId": "dOpP72CPJwWGOLZ6C8xhKg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90414,
      "firstName": "Balakrishnan",
      "lastName": "Prabhakaran",
      "middleInitial": "",
      "importedId": "VMdil79-KH8oHm_Dv9T2tA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90415,
      "firstName": "Ryoto",
      "lastName": "Uchihashi",
      "middleInitial": "",
      "importedId": "ox13RB2V4JBwShWqdihDOQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90416,
      "firstName": "Keiichi",
      "lastName": "Zempo",
      "middleInitial": "",
      "importedId": "5EXLcJOvdZpzRMR4gs1ZAg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90417,
      "firstName": "Tobias",
      "lastName": "Schreck",
      "middleInitial": "",
      "importedId": "SBpNr21aVy1j246jqIqQZQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90418,
      "firstName": "Akihiro",
      "lastName": "Matsuura",
      "middleInitial": "",
      "importedId": "aswnJ-ZbhbboR8eifm_Qpw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90419,
      "firstName": "Junyeong",
      "lastName": "Kum",
      "middleInitial": "",
      "importedId": "7EI2bvhC3KGwIvfsGYGbVg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90420,
      "firstName": "Brandon",
      "lastName": "Coffey",
      "middleInitial": "",
      "importedId": "508QCFZ99rekMX-Wj9NVJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90421,
      "firstName": "Thanh",
      "lastName": "Vu",
      "middleInitial": "Long",
      "importedId": "kAYw5AsCPepcQ6vTFX0yyg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90422,
      "firstName": "Takumi",
      "lastName": "Otsuka",
      "middleInitial": "",
      "importedId": "Jhm95XJPitu8DKS7ekTtEQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90423,
      "firstName": "Peggy",
      "lastName": "Wu",
      "middleInitial": "",
      "importedId": "TJ4vIvZTtZ-dzQv_4ig1kA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90424,
      "firstName": "Hideki",
      "lastName": "Koike",
      "middleInitial": "",
      "importedId": "RDQHw03ag01lqULc0xa4_w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90425,
      "firstName": "Sang Ho",
      "lastName": "Yoon",
      "middleInitial": "",
      "importedId": "V1nG1KeCiKE0S_kGh27BVw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90426,
      "firstName": "Gabriel N.",
      "lastName": "Downs",
      "middleInitial": "",
      "importedId": "07LfRZV8uabZifUCuUemOg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90427,
      "firstName": "Alex",
      "lastName": "Chatburn",
      "middleInitial": "",
      "importedId": "rvIvbGeRfrkiul3_4lzIJQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90428,
      "firstName": "Connor",
      "lastName": "Leonie",
      "middleInitial": "Wilding",
      "importedId": "74PbUuACk-5QzGWbVhvVUA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90429,
      "firstName": "Ryo",
      "lastName": "Ishibashi",
      "middleInitial": "",
      "importedId": "_eDlpBDh7JKLNbW6BkmItA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90430,
      "firstName": "Aditi",
      "lastName": "Majumder",
      "middleInitial": "",
      "importedId": "2_XWQGfiDODQitG2HfR_dQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90431,
      "firstName": "Jin Qi",
      "lastName": "Yeo",
      "middleInitial": "",
      "importedId": "fx3QagQ1xIot-Vh3nJhGoA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90432,
      "firstName": "Hideaki",
      "lastName": "Kanai",
      "middleInitial": "",
      "importedId": "-ziVRaNMzDQps3-Fc3vl6A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90433,
      "firstName": "Qiaoge",
      "lastName": "Li",
      "middleInitial": "",
      "importedId": "PROaJtboULSnSuCYXr8v2A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90434,
      "firstName": "Sheila",
      "lastName": "Sutjipto",
      "middleInitial": "",
      "importedId": "HicW-_KylTp-PGK2Www7Ng",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90435,
      "firstName": "James",
      "lastName": "Eubanks",
      "middleInitial": "Coleman",
      "importedId": "GuBtlGnOHSLepv-uEHCboA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90436,
      "firstName": "Yuya",
      "lastName": "Murakami",
      "middleInitial": "",
      "importedId": "gjBMU2CjJtpe2BkAMt4rgQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90437,
      "firstName": "Laura",
      "lastName": "Marusich",
      "middleInitial": "",
      "importedId": "5tUT59FbJRhq8vzkVeM2-g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90438,
      "firstName": "Yutaka",
      "lastName": "Nakanishi",
      "middleInitial": "",
      "importedId": "Ur8PW42_uqvK7bnziH052w",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90439,
      "firstName": "Juwon",
      "lastName": "Um",
      "middleInitial": "",
      "importedId": "7_gOKebLTKMrRWROGD2oag",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90440,
      "firstName": "Jin Ryong",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "VmNZR87New6oRuGF8A1vsg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90441,
      "firstName": "Austin",
      "lastName": "Stanbury",
      "middleInitial": "J.",
      "importedId": "gdFUgRfIItaELlGbGp5Quw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90442,
      "firstName": "Kosuke",
      "lastName": "Morimoto",
      "middleInitial": "",
      "importedId": "foa_znp0Yy4nHez_35rQMg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90443,
      "firstName": "Shu-Han",
      "lastName": "Liao",
      "middleInitial": "",
      "importedId": "oOGytDa3pCRj9ZODY60bJA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90444,
      "firstName": "Yeling",
      "lastName": "Jiang",
      "middleInitial": "",
      "importedId": "1p1gSU2WkW-6raOky1EQ_A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90445,
      "firstName": "Sooyeon",
      "lastName": "Lee",
      "middleInitial": "",
      "importedId": "JlyUW6mRlPvQ6Fgz8ihC5A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90446,
      "firstName": "Ryota",
      "lastName": "Sakuma",
      "middleInitial": "",
      "importedId": "V1-B4Yf4ANkwocBMzzGtkg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90447,
      "firstName": "Mark",
      "lastName": "Billinghurst",
      "middleInitial": "",
      "importedId": "QlX1SYXVMZkLEc_X2oWX1A",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90448,
      "firstName": "Diego",
      "lastName": "Rivera",
      "middleInitial": "",
      "importedId": "hS2XbPKDxi2s_9PxImi7AA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90449,
      "firstName": "Kelvin",
      "lastName": "Cheng",
      "middleInitial": "",
      "importedId": "xB2Uygl8hVifoIOr4Q9obA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90450,
      "firstName": "Hung-Jui",
      "lastName": "Guo",
      "middleInitial": "",
      "importedId": "4umLe4Mmo1TfSWj8y3z_gg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90451,
      "firstName": "Marielle",
      "lastName": "Machacek",
      "middleInitial": "G",
      "importedId": "rBVspaefCpcIzp62h_-Plw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90452,
      "firstName": "Farzaneh",
      "lastName": "Askari",
      "middleInitial": "",
      "importedId": "04vtAFTPOFpt-MfyRqTCGg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90453,
      "firstName": "SeungJoon",
      "lastName": "Kwon",
      "middleInitial": "",
      "importedId": "t_QgAKFWpsBjQxU4NcumRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90454,
      "firstName": "Itaru",
      "lastName": "Kitahara",
      "middleInitial": "",
      "importedId": "Tq2uvKaTzwX36IyPvq_vaw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90455,
      "firstName": "Konstantina",
      "lastName": "Psarrou",
      "middleInitial": "",
      "importedId": "1lUyYGy3msI1Ns5L32j_eQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90456,
      "firstName": "Vlasios",
      "lastName": "Kasapakis",
      "middleInitial": "",
      "importedId": "Qrm03y49nj11MuQh-6sfHw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90457,
      "firstName": "Sungjoo",
      "lastName": "Kang",
      "middleInitial": "",
      "importedId": "iwp1ir-CE88bt0SOz7tXRQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90458,
      "firstName": "Sang-Woo",
      "lastName": "Seo",
      "middleInitial": "",
      "importedId": "PNCbRoiAUBb54U-idby7DA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90459,
      "firstName": "Dongbeom",
      "lastName": "Ko",
      "middleInitial": "",
      "importedId": "cOmZlTLILZLDs3_xTkjzDQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90460,
      "firstName": "Stephen",
      "lastName": "Gilbert",
      "middleInitial": "B.",
      "importedId": "eCst3bbSJyPy5vVr4jQnGQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90461,
      "firstName": "Isabela",
      "lastName": "Figueira",
      "middleInitial": "",
      "importedId": "Zrc-AOo8qPZQsQtPtcR9qA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90462,
      "firstName": "Yong Hae",
      "lastName": "Heo",
      "middleInitial": "",
      "importedId": "qXpVfdp-eIvzC3zRniMI_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90463,
      "firstName": "Anurag Kumar",
      "lastName": "Singh",
      "middleInitial": "",
      "importedId": "rRRUkBVyf6IT8TSXSqZqyA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90464,
      "firstName": "Yaojun",
      "lastName": "Wang",
      "middleInitial": "",
      "importedId": "yCsTnOfzo1qxGtK7FNXKjA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90465,
      "firstName": "Ioannidis",
      "lastName": "Marios",
      "middleInitial": "",
      "importedId": "gt389TdYl3An3Gj7PEbT_Q",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90466,
      "firstName": "Chien-Hsing",
      "lastName": "Chou",
      "middleInitial": "",
      "importedId": "pdqcxE-bizhOaPTizXYIlA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90467,
      "firstName": "Jonas",
      "lastName": "Schild",
      "middleInitial": "",
      "importedId": "S5WOqalPEIP1xzoail7hQw",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90468,
      "firstName": "Sang-Youn",
      "lastName": "Kim",
      "middleInitial": "",
      "importedId": "0lrFSDF89YN5HyXgZC-BMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90469,
      "firstName": "Jonathan",
      "lastName": "Bakdash",
      "middleInitial": "",
      "importedId": "ty8Kk5LnbT21pwXIn5fBoQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90470,
      "firstName": "Christian",
      "lastName": "Schnellmann",
      "middleInitial": "",
      "importedId": "eNp-PArfSfAI9syb4NWlNg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90471,
      "firstName": "Yen-Hung",
      "lastName": "Lin",
      "middleInitial": "",
      "importedId": "mS_JqRAZIuV4SAXLCduQFQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90472,
      "firstName": "Heesook",
      "lastName": "Shin",
      "middleInitial": "",
      "importedId": "QvKsbQdH4xBfn_kSGa2pRg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90473,
      "firstName": "Toshimasa",
      "lastName": "Yamanaka",
      "middleInitial": "",
      "importedId": "qP50qM5j2nriFY1GsgSojA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90474,
      "firstName": "Seo Young",
      "lastName": "Oh",
      "middleInitial": "",
      "importedId": "Ktyujiy_lwEwP-o2xJVFLA",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90475,
      "firstName": "Robin",
      "lastName": "Angotti",
      "middleInitial": "",
      "importedId": "DQXu4Lu9VFNURe3qRUL4mg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90476,
      "firstName": "Youichi",
      "lastName": "Kamiyama",
      "middleInitial": "",
      "importedId": "44r8qfZSqfYmgPPbqVmaMQ",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90477,
      "firstName": "Yohei",
      "lastName": "Yanase",
      "middleInitial": "",
      "importedId": "86pDWL9ginFn27Rv44Vx_g",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90478,
      "firstName": "Yitian",
      "lastName": "Shao",
      "middleInitial": "",
      "importedId": "QH9LmzjxKZvM4ewZ1wMTJg",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90479,
      "firstName": "Tobias",
      "lastName": "Matter",
      "middleInitial": "",
      "importedId": "Ngem2OQYjI5uAud5XLMaog",
      "source": "PCS",
      "affiliations": []
    },
    {
      "id": 90629,
      "firstName": "Pedro",
      "lastName": "Lopes",
      "importedId": "0",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 90630,
      "firstName": "Naoto",
      "lastName": "Kato",
      "importedId": "1",
      "source": "CSV",
      "affiliations": []
    },
    {
      "id": 90631,
      "firstName": "Daito",
      "lastName": "Manabe",
      "importedId": "2",
      "source": "CSV",
      "affiliations": []
    }
  ],
  "recognitions": []
}