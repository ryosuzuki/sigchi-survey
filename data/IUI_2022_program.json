{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10075,
    "shortName": "IUI",
    "year": 2022,
    "startDate": 1647820800000,
    "endDate": 1648166400000,
    "name": "IUI 2022",
    "fullName": "27th Annual Conference on Intelligent User Interfaces",
    "url": "https://iui.acm.org/2022",
    "location": "Virtual from Helsinki, Finland",
    "timeZoneOffset": 120,
    "timeZoneName": "Europe/Helsinki",
    "logoUrl": "https://files.sigchi.org/conference/logo/17517869-ea0e-28ce-4559-71405bef4e6d.png",
    "noteToConference": "Note for attendees: If you have created your ACM account recently, you might not see the \"Registered\" badge at the top of this page immediately. Please wait for 24 hours and try again!"
  },
  "sponsors": [
    {
      "id": 10185,
      "name": "Reality Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/0c06b6c5-4a0d-383e-6213-c4a2a26f88cf.png",
      "levelId": 10135,
      "description": "<p>https://about.facebook.com/realitylabs/</p>",
      "order": 1
    },
    {
      "id": 10186,
      "name": "Google",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/37ca1831-6c30-3b97-dcf9-0d403985b39a.png",
      "levelId": 10136,
      "description": "<p>https://www.google.com/</p>",
      "order": 2
    },
    {
      "id": 10187,
      "name": "Goldman Sachs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/9c89aa4c-333b-5abd-1b79-fb62994184bb.png",
      "levelId": 10137,
      "description": "<p>https://design.gs.com/</p>",
      "order": 3
    },
    {
      "id": 10188,
      "name": "Nokia Bell Labs",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/d0a5a895-e3e5-9962-1499-7cd77002ed0e.png",
      "levelId": 10137,
      "description": "<p>https://www.bell-labs.com/</p>",
      "order": 4
    },
    {
      "id": 10189,
      "name": "Toyota Research Institute",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/db60cb64-86ce-9dea-54cb-97dfc08725bb.png",
      "levelId": 10137,
      "description": "<p>https://www.tri.global/</p>",
      "order": 5
    },
    {
      "id": 10190,
      "name": "Apple",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/2bd4b6a9-4469-22f5-3b5e-fcb8e9293642.png",
      "levelId": 10139,
      "description": "<p>https://www.apple.com/</p>",
      "order": 6
    },
    {
      "id": 10191,
      "name": "NVIDIA",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/f0394b94-a7e2-650a-fed7-d6c7fe59546f.png",
      "levelId": 10139,
      "description": "<p>https://www.nvidia.com/</p>",
      "order": 7
    },
    {
      "id": 10192,
      "name": "SILO AI",
      "logoUrl": "https://files.sigchi.org/conference/sponsor/logo/9b9a044a-e24e-1321-379c-13b9b18c38a7.png",
      "levelId": 10139,
      "description": "<p>https://silo.ai/</p>",
      "order": 8
    }
  ],
  "sponsorLevels": [
    {
      "id": 10130,
      "name": "Sponsors",
      "rank": 6,
      "isDefault": true
    },
    {
      "id": 10135,
      "name": "Platinum",
      "rank": 1,
      "isDefault": false
    },
    {
      "id": 10136,
      "name": "Gold",
      "rank": 2,
      "isDefault": false
    },
    {
      "id": 10137,
      "name": "Silver",
      "rank": 3,
      "isDefault": false
    },
    {
      "id": 10138,
      "name": "ACM",
      "rank": 5,
      "isDefault": false
    },
    {
      "id": 10139,
      "name": "Bronze",
      "rank": 4,
      "isDefault": false
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10614,
      "name": "Keynotes Room",
      "typeId": 12044,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10615,
      "name": "Posters & Demos Room",
      "typeId": 12089,
      "setup": "Rounds",
      "note": "Zoom"
    },
    {
      "id": 10616,
      "name": "Papers Room",
      "typeId": 12046,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10634,
      "name": "Workshop Room 6",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10635,
      "name": "Workshop Room 1",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10636,
      "name": "Workshop Room 5",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10637,
      "name": "Workshop Room 4",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10638,
      "name": "Workshop Room 3",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10639,
      "name": "Workshop Room 2",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10640,
      "name": "Doctoral Consortium Room",
      "typeId": 12049,
      "setup": "Theatre",
      "note": "Zoom"
    },
    {
      "id": 10653,
      "name": "Social Room",
      "typeId": 12044,
      "setup": "Theatre",
      "note": "Zoom"
    }
  ],
  "tracks": [
    {
      "id": 11623,
      "name": "IUI 2022 Papers",
      "typeId": 12046
    },
    {
      "id": 11765,
      "name": "IUI 2022 Posters and Demos",
      "typeId": 12089
    },
    {
      "id": 11766,
      "name": "IUI 2022 Workshops and Tutorials",
      "typeId": 12049
    },
    {
      "id": 11772,
      "typeId": 12065
    },
    {
      "id": 11790,
      "typeId": 12064
    },
    {
      "id": 11791,
      "typeId": 12044
    }
  ],
  "contentTypes": [
    {
      "id": 12041,
      "name": "Course",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 12042,
      "name": "Demo",
      "color": "#006d2c",
      "duration": 5,
      "displayName": "Demos"
    },
    {
      "id": 12043,
      "name": "Doctoral Consortium",
      "color": "#6baed6",
      "duration": 5
    },
    {
      "id": 12044,
      "name": "Event",
      "color": "#ffc034",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 12045,
      "name": "Late-Breaking Work",
      "color": "#8e008b",
      "duration": 5
    },
    {
      "id": 12046,
      "name": "Paper",
      "color": "#0d42cc",
      "duration": 12,
      "displayName": "Papers"
    },
    {
      "id": 12047,
      "name": "Poster",
      "color": "#ff7a00",
      "duration": 5,
      "displayName": "Posters"
    },
    {
      "id": 12048,
      "name": "Work-in-Progress",
      "color": "#26e5f1",
      "duration": 5
    },
    {
      "id": 12049,
      "name": "Workshop",
      "color": "#f60000",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 12050,
      "name": "Break",
      "color": "#7f6aff",
      "duration": 5
    },
    {
      "id": 12064,
      "name": "Panel",
      "color": "#32d923",
      "duration": 85,
      "displayName": "Panels"
    },
    {
      "id": 12065,
      "name": "Keynote",
      "color": "#ff99ca",
      "duration": 60
    },
    {
      "id": 12066,
      "name": "Opening",
      "color": "#969696",
      "duration": 85
    },
    {
      "id": 12067,
      "name": "Closing",
      "color": "#969696",
      "duration": 90
    },
    {
      "id": 12068,
      "name": "Industry",
      "color": "#969696",
      "duration": 60
    },
    {
      "id": 12089,
      "name": "Poster/Demo",
      "color": "#969696",
      "duration": 90
    }
  ],
  "timeSlots": [
    {
      "id": 12275,
      "type": "SESSION",
      "startDate": 1648045800000,
      "endDate": 1648050900000
    },
    {
      "id": 12276,
      "type": "BREAK",
      "startDate": 1648050900000,
      "endDate": 1648053000000
    },
    {
      "id": 12277,
      "type": "SESSION",
      "startDate": 1648053000000,
      "endDate": 1648058100000
    },
    {
      "id": 12278,
      "type": "BREAK",
      "startDate": 1648058100000,
      "endDate": 1648058400000
    },
    {
      "id": 12279,
      "type": "SESSION",
      "startDate": 1648058400000,
      "endDate": 1648063500000
    },
    {
      "id": 12280,
      "type": "BREAK",
      "startDate": 1648063500000,
      "endDate": 1648063800000
    },
    {
      "id": 12281,
      "type": "SESSION",
      "startDate": 1648063800000,
      "endDate": 1648068900000
    },
    {
      "id": 12282,
      "type": "BREAK",
      "startDate": 1648068900000,
      "endDate": 1648069200000
    },
    {
      "id": 12283,
      "type": "SESSION",
      "startDate": 1648069200000,
      "endDate": 1648074600000
    },
    {
      "id": 12284,
      "type": "SESSION",
      "startDate": 1648132200000,
      "endDate": 1648137300000
    },
    {
      "id": 12285,
      "type": "BREAK",
      "startDate": 1648137300000,
      "endDate": 1648139400000
    },
    {
      "id": 12286,
      "type": "SESSION",
      "startDate": 1648139400000,
      "endDate": 1648144500000
    },
    {
      "id": 12287,
      "type": "BREAK",
      "startDate": 1648144500000,
      "endDate": 1648144800000
    },
    {
      "id": 12288,
      "type": "SESSION",
      "startDate": 1648144800000,
      "endDate": 1648149900000
    },
    {
      "id": 12289,
      "type": "BREAK",
      "startDate": 1648149900000,
      "endDate": 1648150200000
    },
    {
      "id": 12290,
      "type": "SESSION",
      "startDate": 1648150200000,
      "endDate": 1648155300000
    },
    {
      "id": 12291,
      "type": "BREAK",
      "startDate": 1648155300000,
      "endDate": 1648155600000
    },
    {
      "id": 12292,
      "type": "SESSION",
      "startDate": 1648155600000,
      "endDate": 1648159200000
    },
    {
      "id": 12293,
      "type": "SESSION",
      "startDate": 1648218600000,
      "endDate": 1648223700000
    },
    {
      "id": 12294,
      "type": "BREAK",
      "startDate": 1648223700000,
      "endDate": 1648225800000
    },
    {
      "id": 12295,
      "type": "SESSION",
      "startDate": 1648225800000,
      "endDate": 1648230900000
    },
    {
      "id": 12296,
      "type": "BREAK",
      "startDate": 1648230900000,
      "endDate": 1648231200000
    },
    {
      "id": 12297,
      "type": "SESSION",
      "startDate": 1648231200000,
      "endDate": 1648236300000
    },
    {
      "id": 12298,
      "type": "BREAK",
      "startDate": 1648236300000,
      "endDate": 1648236600000
    },
    {
      "id": 12299,
      "type": "SESSION",
      "startDate": 1648236600000,
      "endDate": 1648242000000
    },
    {
      "id": 12302,
      "type": "SESSION",
      "startDate": 1648074600000,
      "endDate": 1648078200000
    },
    {
      "id": 12303,
      "type": "SESSION",
      "startDate": 1647966600000,
      "endDate": 1647971700000
    },
    {
      "id": 12304,
      "type": "BREAK",
      "startDate": 1647971700000,
      "endDate": 1647972000000
    },
    {
      "id": 12305,
      "type": "SESSION",
      "startDate": 1647972000000,
      "endDate": 1647977100000
    },
    {
      "id": 12306,
      "type": "BREAK",
      "startDate": 1647977100000,
      "endDate": 1647977400000
    },
    {
      "id": 12307,
      "type": "SESSION",
      "startDate": 1647977400000,
      "endDate": 1647982500000
    },
    {
      "id": 12308,
      "type": "BREAK",
      "startDate": 1647982500000,
      "endDate": 1647982800000
    },
    {
      "id": 12309,
      "type": "SESSION",
      "startDate": 1647982800000,
      "endDate": 1647988200000
    },
    {
      "id": 12336,
      "type": "SESSION",
      "startDate": 1647876600000,
      "endDate": 1647896400000
    },
    {
      "id": 12337,
      "type": "SESSION",
      "startDate": 1647946800000,
      "endDate": 1647961200000
    },
    {
      "id": 12338,
      "type": "SESSION",
      "startDate": 1647878400000,
      "endDate": 1647900000000
    },
    {
      "id": 12339,
      "type": "SESSION",
      "startDate": 1647880200000,
      "endDate": 1647900000000
    },
    {
      "id": 12340,
      "type": "SESSION",
      "startDate": 1647882000000,
      "endDate": 1647903600000
    },
    {
      "id": 12342,
      "type": "SESSION",
      "startDate": 1647964500000,
      "endDate": 1647966600000
    },
    {
      "id": 12343,
      "type": "SESSION",
      "startDate": 1647988200000,
      "endDate": 1647991800000
    },
    {
      "id": 12352,
      "type": "SESSION",
      "startDate": 1648159200000,
      "endDate": 1648162800000
    }
  ],
  "sessions": [
    {
      "id": 80020,
      "name": "Health, Well-being and Accessibility",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80432
      ],
      "contentIds": [
        79967,
        79985,
        79928,
        79942,
        79988,
        79968,
        79979
      ],
      "timeSlotId": 12305
    },
    {
      "id": 80021,
      "name": "Recommender Systems and Decision-Making",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        79725
      ],
      "contentIds": [
        79980,
        79983,
        79930,
        79984,
        79957,
        79990,
        79977
      ],
      "timeSlotId": 12307
    },
    {
      "id": 80022,
      "name": "Explainable AI (XAI) 1",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80653
      ],
      "contentIds": [
        79993,
        79952,
        79961,
        79933,
        79945,
        79999,
        79924
      ],
      "timeSlotId": 12275
    },
    {
      "id": 80023,
      "name": "Alternative Input Modes",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        79333
      ],
      "contentIds": [
        80672,
        79948,
        79986,
        79943,
        79995,
        79939,
        79926,
        79949
      ],
      "timeSlotId": 12277
    },
    {
      "id": 80024,
      "name": "Tools for AI Developers",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80143
      ],
      "contentIds": [
        79964,
        79972,
        79998,
        79971,
        79927,
        79963,
        79941
      ],
      "timeSlotId": 12283
    },
    {
      "id": 80025,
      "name": "Mobiles and Wearables",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80433
      ],
      "contentIds": [
        79936,
        79954,
        79938,
        79996,
        79931,
        79975,
        79934
      ],
      "timeSlotId": 12284
    },
    {
      "id": 80026,
      "name": "Interacting with Machine Learning",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80434
      ],
      "contentIds": [
        79978,
        79965,
        79940,
        79929,
        79989,
        79994,
        79987
      ],
      "timeSlotId": 12288
    },
    {
      "id": 80027,
      "name": "Learning and Playing",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        79760
      ],
      "contentIds": [
        79982,
        79969,
        79932,
        79925,
        79960,
        79944,
        79992
      ],
      "timeSlotId": 12290
    },
    {
      "id": 80028,
      "name": "Applications and Tools",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80435
      ],
      "contentIds": [
        79935,
        79958,
        79946,
        79966,
        79951,
        79959
      ],
      "timeSlotId": 12293
    },
    {
      "id": 80029,
      "name": "Explainable AI (XAI) 2",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        79711
      ],
      "contentIds": [
        79955,
        79937,
        79974,
        79962,
        79973,
        80000,
        79950
      ],
      "timeSlotId": 12295
    },
    {
      "id": 80030,
      "name": "Natural Language",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12046,
      "roomId": 10616,
      "chairIds": [
        80436
      ],
      "contentIds": [
        79970,
        79976,
        79991,
        79947,
        79956,
        79953,
        79997
      ],
      "timeSlotId": 12297
    },
    {
      "id": 80256,
      "name": "Posters and Demos",
      "addons": {},
      "isParallelPresentation": true,
      "typeId": 12089,
      "roomId": 10615,
      "chairIds": [],
      "contentIds": [
        80252,
        80250,
        80251,
        80244,
        80237,
        80245,
        80248,
        80247,
        80242,
        80241,
        80234,
        80246,
        80249,
        80240,
        80236,
        80239,
        80253,
        80254,
        80230,
        80233,
        80255,
        80238,
        80235,
        80243,
        80232,
        80628,
        80634,
        80635,
        80637,
        80632,
        80633,
        80640,
        80641,
        80631,
        80642,
        80636,
        80638,
        80627
      ],
      "timeSlotId": 12309
    },
    {
      "id": 80367,
      "name": "Doctoral Consortium",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12043,
      "roomId": 10640,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 12336
    },
    {
      "id": 80368,
      "name": "HEALTHI Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10635,
      "chairIds": [],
      "contentIds": [
        80363
      ],
      "timeSlotId": 12336
    },
    {
      "id": 80369,
      "name": "APEx-UI Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10639,
      "chairIds": [],
      "contentIds": [
        80364
      ],
      "timeSlotId": 12338
    },
    {
      "id": 80370,
      "name": "HAI-GEN Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10638,
      "chairIds": [],
      "contentIds": [
        80360
      ],
      "timeSlotId": 12338
    },
    {
      "id": 80371,
      "name": "TExSS Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10637,
      "chairIds": [],
      "contentIds": [
        80362
      ],
      "timeSlotId": 12340
    },
    {
      "id": 80372,
      "name": "SOCIALIZE Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10636,
      "chairIds": [],
      "contentIds": [
        80359
      ],
      "timeSlotId": 12337
    },
    {
      "id": 80373,
      "name": "Town Hall",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12044,
      "roomId": 10614,
      "chairIds": [
        80653
      ],
      "contentIds": [],
      "timeSlotId": 12286
    },
    {
      "id": 80402,
      "name": "Opening & Keynote",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12065,
      "roomId": 10614,
      "chairIds": [],
      "contentIds": [
        80397,
        80387
      ],
      "timeSlotId": 12303
    },
    {
      "id": 80403,
      "name": "Keynote",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://youtu.be/pD0Js0-J43Y"
        }
      },
      "isParallelPresentation": false,
      "typeId": 12065,
      "roomId": 10614,
      "chairIds": [],
      "contentIds": [
        80388
      ],
      "timeSlotId": 12292
    },
    {
      "id": 80404,
      "name": "Impact Panel",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12064,
      "roomId": 10614,
      "chairIds": [
        80392,
        80393
      ],
      "contentIds": [
        80399
      ],
      "timeSlotId": 12281
    },
    {
      "id": 80405,
      "name": "Panel: Digital Health",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://youtu.be/oAHOgmaNqRw"
        }
      },
      "isParallelPresentation": false,
      "typeId": 12064,
      "roomId": 10614,
      "chairIds": [],
      "contentIds": [
        80386
      ],
      "timeSlotId": 12279
    },
    {
      "id": 80406,
      "name": "Closing & Keynote",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://youtu.be/SYqVKrY8XpA"
        }
      },
      "isParallelPresentation": false,
      "typeId": 12065,
      "roomId": 10614,
      "chairIds": [],
      "contentIds": [
        80398,
        80389
      ],
      "timeSlotId": 12299
    },
    {
      "id": 80412,
      "name": "HUMANIZE Workshop",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12049,
      "roomId": 10637,
      "chairIds": [],
      "contentIds": [
        80361
      ],
      "timeSlotId": 12340
    },
    {
      "id": 80417,
      "name": "Meta Reality Labs Sponsor Session",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12044,
      "roomId": 10614,
      "chairIds": [
        80407
      ],
      "contentIds": [
        80410
      ],
      "timeSlotId": 12302
    },
    {
      "id": 80532,
      "name": "Social program: Mindfulness session",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12044,
      "roomId": 10653,
      "chairIds": [],
      "contentIds": [
        80645
      ],
      "timeSlotId": 12342
    },
    {
      "id": 80533,
      "name": "Social program: Mindfulness session",
      "addons": {},
      "isParallelPresentation": false,
      "typeId": 12044,
      "roomId": 10653,
      "chairIds": [],
      "contentIds": [
        80646
      ],
      "timeSlotId": 12343
    },
    {
      "id": 80664,
      "name": "Social Program",
      "addons": {
        "Live Stream": {
          "duringSessionOnly": false,
          "hideAfterConference": false,
          "hideBeforeConference": false,
          "isAvailableForRegisteredMembersOnly": false,
          "type": "video",
          "url": "https://youtu.be/pD0Js0-J43Y?t=3932"
        }
      },
      "isParallelPresentation": false,
      "typeId": 12044,
      "roomId": 10653,
      "chairIds": [],
      "contentIds": [
        80663
      ],
      "timeSlotId": 12352
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 79924,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Explaining conversational agents for use in criminal investigations",
      "addons": {
        "Presentation Video": {
          "duration": "527",
          "hideBeforeConference": true,
          "title": "TiiS: Explaining conversational agents for use in criminal investigations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jA2qiS06X7s"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Middlesex University London",
              "dsl": ""
            }
          ],
          "personId": 79893
        }
      ]
    },
    {
      "id": 79925,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Agenda- and Activity-Based Triggers for Microlearning",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511133"
        },
        "Presentation Video": {
          "duration": "470",
          "hideBeforeConference": true,
          "title": "Agenda- and Activity-Based Triggers for Microlearning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=fxIAcBU55Yg"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "The ubiquity of mobile devices has fueled the popularity of microlearning, namely informal self-directed learning during brief personal downtime. However, learner engagement is challenging to maintain and microlearning habits are hard to establish. Scheduled reminders are ineffective as they do not match the users' variable schedules and their intention or capacity to engage. In this paper, we propose a schedule-based and an activity-based trigger for microlearning. The first trigger is sensitive to the learners' agenda and device status, and includes a snooze mechanism. An four-week study (n=10) showed slightly lower response times when compared to triggers scheduled at a fixed time, but did not improve learner engagement. The second trigger initiates audio-based microlearning when plugging in headphones. Thus, we minimize the access to personal data and capture a moment where learners engage with their device for a listening activity. In an exploratory user study (n=10), the plugin trigger achieved higher compliance rates and was less likely to induce annoyance in users than lock screen notifications.\r\nWe conclude that intelligent reminders with simple interaction options can contribute to learner engagement.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 79766
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Eichenau",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 79895
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 79681
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 79901
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": "Institute for Informatics"
            },
            {
              "country": "Germany",
              "state": "North Rhine Westphalia",
              "city": "Dortmund",
              "institution": "IfADo-Leibniz Institute for Working Environment and Human Factors",
              "dsl": "Ergonomics"
            }
          ],
          "personId": 79774
        }
      ]
    },
    {
      "id": 79926,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Hazard Notifications for Cyclists: Comparison of Awareness Message Modalities in a Mixed Reality Study",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511127"
        },
        "Presentation Video": {
          "duration": "603",
          "hideBeforeConference": true,
          "title": "Hazard Notifications for Cyclists: Comparison of Awareness Message Modalities in a Mixed Reality Study",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SHDk-D7ZaQY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Cycling is an environmentally friendly means of transport with growing popularity. However, there is still potential for more road safety in the future. We argue that by making assistance systems available to cyclists, accidents could be prevented. In this paper, we focus on potential accidents caused by vehicle doors opening in a cyclist’s path of travel, which can lead to serious injuries to the cyclist. Using a mixed-methods approach, we investigated how messages informing about a potentially opening door ahead are perceived and understood regarding usability and intuitiveness in a bicycle simulator study (N=24). We investigated how visual messages, visual messages and auditory icons, and visual and voice messages on a head-mounted device are subjectively perceived compared to the baseline condition (no messages). We also assessed our participants’ attitudes toward using such systems and mixed reality simulations for bicycle safety research in general. Our results show that participants preferred visual messages and auditory cues and that they found these types of notifications more enjoyable than visual messages alone. Furthermore, the results suggest that such a system would be used while cycling. Participants agreed that mixed reality simulations are suitable for testing and evaluating novel support systems in the first step but confirmed that final real-world testing on the road is mandatory.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": ""
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 79715
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": ""
            }
          ],
          "personId": 79701
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 79807
        }
      ]
    },
    {
      "id": 79927,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Emblaze: Illuminating Machine Learning Representations through Interactive Comparison of Embedding Spaces",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511137"
        },
        "Presentation Video": {
          "duration": "608",
          "hideBeforeConference": true,
          "title": "Emblaze: Illuminating Machine Learning Representations through Interactive Comparison of Embedding Spaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xiCVPwkCEsg"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "Modern machine learning techniques commonly rely on complex, high-dimensional embedding representations to capture underlying structure in the data and improve performance. In order to characterize model flaws and choose a desirable representation, model builders often need to compare across multiple embedding spaces, a challenging analytical task supported by few existing tools. We first interviewed nine embedding experts in a variety of fields to characterize the diverse challenges they face and techniques they use when analyzing embedding spaces. Informed by these perspectives, we developed a novel system called Emblaze that integrates embedding space comparison within a computational notebook environment. Emblaze uses an animated, interactive scatter plot with a novel star trail augmentation to enable visual comparison. It also employs novel neighborhood analysis and clustering procedures to dynamically suggest groups of points with interesting changes between spaces. Through think-aloud sessions with ML experts, we demonstrate how interactive comparison with Emblaze can help gain new insights into embedding space structure.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79752
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79676
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79665
        }
      ]
    },
    {
      "id": 79928,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "The \"Artificial\" Colleague: Evaluation of Work Satisfaction in Collaboration with Non-human Coworkers",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511128"
        },
        "Presentation Video": {
          "duration": "603",
          "hideBeforeConference": true,
          "title": "The \"Artificial\" Colleague: Evaluation of Work Satisfaction in Collaboration with Non-human Coworkers",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_nUmzkIsItc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "The advance of AI-based technologies transforms work processes tremendously. Work forms a major part of life and its extent of meaningfulness can affect overall well-being. Previous research has investigated human-AI collaboration at work based on performance and cost outcomes. However, little attention is given to how collaboration with AI influences the meaningfulness of work and job satisfaction. In this paper, we present an online experiment, where we evaluated the perception of work meaningfulness and relationship to the collaborator across different task distributions and collaborators (human/AI). Our results indicate that people, in general, prefer working with a human, and having a meaningful task increases their satisfaction. Moreover, we found that people have a different perception of the relationship to their collaborator doing the same tasks with a human or an AI. These results provide preliminary implications for the design of collaboration with AI that consider job satisfaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Experience and Interaction"
            }
          ],
          "personId": 79786
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Siegen",
              "institution": "University of Siegen",
              "dsl": "Ubiquitous Design / Experience & Interaction"
            }
          ],
          "personId": 79666
        }
      ]
    },
    {
      "id": 79929,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Efficiently correcting machine learning: considering the role of example ordering in human-in-the-loop training of image classification models",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511110"
        },
        "Presentation Video": {
          "duration": "524",
          "hideBeforeConference": true,
          "title": "Efficiently correcting machine learning: considering the role of example ordering in human-in-the-loop training of image classification models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=z4xNPVlngyQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": " Arguably the most popular application task in artificial intelligence is image classification using transfer learning. Transfer learning enables models pre-trained on general classes of images, available in large numbers, to be refined for a specific application. This enables domain experts with their own---generally, substantially smaller---collections of images to build deep learning models. The good performance of such models poses the question of whether it is possible to further reduce the effort required to label training data by adopting a human-in-the-loop interface that presents the expert with the current predictions of the model on a new batch of data and only requires {\\em correction} of these predictions---rather than {\\em de novo} labelling by the expert---before retraining the model on the extended data.\r\n  This paper looks at how to order the data in this iterative training scheme to achieve the highest model performance while minimising the effort needed to correct misclassified examples. Experiments are conducted involving five methods of ordering, using four image classification datasets, and three popular pre-trained models. Two of the methods we consider order the examples \\textit{a priori} whereas the other three employ an active learning approach where the ordering is updated iteratively after each new batch of data and retraining of the model. The main finding is that it is important to consider accuracy of the model in relation to the number of corrections that are required: using accuracy in relation to the number of labelled training examples---as is common practice in the literature---can be misleading. More specifically, active methods require more cumulative corrections than \\textit{a priori} methods for a given level of accuracy. Within their groups, active and \\textit{a priori} methods perform similarly. Preliminary evidence is provided that suggests that for ``simple'' problems, i.e., those involving fewer examples and classes, no method improves upon random selection of examples. For more complex problems, an \\textit{a priori} strategy based on a greedy sample selection method known as ``kernel herding'' performs best.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Hamilton",
              "institution": "University of Waikato",
              "dsl": ""
            }
          ],
          "personId": 79844
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Hamilton",
              "institution": "University of Waikato",
              "dsl": ""
            }
          ],
          "personId": 79735
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Hamilton",
              "institution": "University of Waikato",
              "dsl": ""
            }
          ],
          "personId": 79860
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Hamilton",
              "institution": "University of Waikato",
              "dsl": ""
            }
          ],
          "personId": 79896
        }
      ]
    },
    {
      "id": 79930,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Recommendations as Challenges: Estimating Required Effort and User Ability for Health Behavior Change Recommendations",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511118"
        },
        "Presentation Video": {
          "duration": "623",
          "hideBeforeConference": true,
          "title": "Recommendations as Challenges: Estimating Required Effort and User Ability for Health Behavior Change Recommendations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=98Q8fBc1UX8"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": " Recommender Systems (RS) use implicit and explicit user feedback to recommend desired products or items online. When the recommendation item is a task or behavior change activity, several variables, such as the difficulty of the task and users' ability to achieve it, in addition to user preferences and needs, determine the suitability of the recommendations. This paper focuses on how user ability and task difficulty concepts can be integrated into the recommendation process to personalize health activity recommendations. To this end, we compare five approaches, some borrowed from the sports and gaming world, and explore their application, advantages, and drawbacks. Through a study of two weeks (N=185), we obtained a suitable dataset to investigate how these algorithms can be used for a health recommender system (HRS) and which one is the appropriate choice for an online HRS in terms of characteristics and flexibility required for behavior change related tailoring. We then compared this choice with a baseline as part of a fully functional HRS to assess the feasibility and impact of integrating the user ability and required effort concepts on the user engagement with the recommendations in an online study of two weeks (N=49). The results overall suggest that such integration is effective, and in addition to realizing health behavior change requirements, it improves user engagement with the recommendations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "NRW",
              "city": "Duisburg",
              "institution": "University of Duisburg-Essen",
              "dsl": "Interactive Systems Group"
            }
          ],
          "personId": 79762
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Duisburg",
              "institution": "Interactive Systems Research Group",
              "dsl": "University of Duisburg-Essen"
            }
          ],
          "personId": 79771
        }
      ]
    },
    {
      "id": 79931,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Hand Gesture Recognition for an Off-the-Shelf Radar by Electromagnetic Modeling and Inversion",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511107"
        },
        "Presentation Video": {
          "duration": "511",
          "hideBeforeConference": true,
          "title": "Hand Gesture Recognition for an Off-the-Shelf Radar by Electromagnetic Modeling and Inversion",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-ihfwGU6VaE"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Microwave radar sensors in human-computer interactions have several advantages compared to wearable and image-based sensors, such as privacy preservation, high reliability regardless of the ambient and lighting conditions, and larger field of view. However, the raw signals produced by such radars are high-dimension and relatively complex to interpret. Advanced data processing, including machine learning techniques, is therefore necessary for gesture recognition. While these approaches can reach high gesture recognition accuracy, using artificial neural networks requires a significant amount of gesture templates for training and calibration is radar-specific. \r\nTo address these challenges, we present a novel data processing pipeline for hand gesture recognition that combines advanced full-wave electromagnetic modelling (EM) and inversion with machine learning. In particular, the physical  model accounts for the radar source, radar antennas, radar-target interactions and target itself, i.e., the hand in our case. To make EM processing feasible, the hand is emulated by an equivalent infinite planar reflector, for which analytical Green's functions exist. The hand, located at a specific distance from the radar, is therefore characterized by an apparent dielectric permittivity. This apparent permittivity depends on the hand only (e.g., size, electric properties, orientation) and, together with the distance, determines wave reflection amplitude.\r\nThrough full-wave inversion of the radar data, the physical distance as well as this apparent permittivity are retrieved, thereby reducing by several orders of magnitude the dimension of the radar dataset, while keeping essential information. Using the estimated distance and apparent permittivity as a function of time is finally used to train the machine learning algorithm for gesture recognition. This dimension reduction enables the use of simple gesture recognition algorithms, such as template-matching recognizers, that can be trained in real time and provide competitive accuracy with only a few samples. We evaluate significant stages of our pipeline on a dataset of 16 gesture classes, with 5 templates per class, recorded with the Walabot, a lightweight, off-the-shelf array radar. We also compare these results with an ultra wideband radar made of a single horn antenna and lightweight vector network analyzer, and a Leap Motion Controller.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Louvain la Neuve",
              "institution": "Université catholique de Louvain",
              "dsl": "Louvain Research Institute in Management and Organizations"
            }
          ],
          "personId": 79764
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Louvain-la-Neuve",
              "institution": "Université catholique de Louvain",
              "dsl": "Earth and Life Institute"
            }
          ],
          "personId": 79661
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Louvain-la-Neuve",
              "institution": "Université catholique de Louvain",
              "dsl": "Louvain School of Management/LiLab"
            }
          ],
          "personId": 79694
        }
      ]
    },
    {
      "id": 79932,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Exploring the Role of Common Model of Cognition in Designing Adaptive Coaching Interactions for Health Behavior Change",
      "addons": {
        "Presentation Video": {
          "duration": "633",
          "hideBeforeConference": true,
          "title": "TiiS: Exploring the Role of Common Model of Cognition in Designing Adaptive Coaching Interactions for Health Behavior Change",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RU61lwU_9Xs"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Palo Alto Research Cetner",
              "dsl": "Interaction Analytics Lab"
            }
          ],
          "personId": 79818
        }
      ]
    },
    {
      "id": 79933,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Deep Learning Uncertainty in Machine Teaching",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511117"
        },
        "Presentation Video": {
          "duration": "598",
          "hideBeforeConference": true,
          "title": "Deep Learning Uncertainty in Machine Teaching",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H1S24WSD4OY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "Machine Learning models can output confident but incorrect predictions. To address this, ML researchers use various techniques to reliably estimate ML uncertainty, usually performed on controlled benchmarks once the model has been trained. \r\nWe explore how the two types of uncertainty—aleatoric and epistemic—can help non-expert users understand the strengths and weaknesses of a classifier in an interactive setting.  We are interested not only in their use of uncertainty to teach and understand the classifier, but also in their perception of the difference between aleatoric and epistemic uncertainty. We conducted an experiment where non-experts train a classifier to recognize card images, and are tested on their ability to predict classifier outcomes. Participants who used either larger or more varied training sets significantly improved their understanding of uncertainty, both epistemic or aleatoric. However, participants who relied on the uncertainty measure to guide their choice of training data did not significantly improve classifier training, nor were they better able to guess the classifier outcome. We identified three specific situations where participants successfully identified the difference between aleatoric and epistemic uncertainty: placing a new card in the exact same position as a training card; placing different cards next to each other; and placing a non-card, such as their hand, next to or on top of a card. We discuss our methodology for estimating uncertainty for Interactive Machine Learning systems and question the need for two-level uncertainty in Machine Teaching.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Gif-sur-Yvette",
              "institution": "Université Paris Saclay",
              "dsl": "LISN"
            }
          ],
          "personId": 79831
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 79763
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Champs-sur-Marne",
              "institution": "Université Gustave Eiffel",
              "dsl": "ESIPE-IMAC"
            }
          ],
          "personId": 79811
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Inria",
              "dsl": "ExSitu"
            }
          ],
          "personId": 79868
        }
      ]
    },
    {
      "id": 79934,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Understanding, Discovering, and Mitigating Habitual Smartphone Use in Young Adults",
      "addons": {
        "Presentation Video": {
          "duration": "597",
          "hideBeforeConference": true,
          "title": "TiiS: Understanding, Discovering, and Mitigating Habitual Smartphone Use in Young Adults",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nC56Uqi6iC4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Politecnico di Torino",
              "dsl": "Dipartimento di Automatica e Informatica"
            }
          ],
          "personId": 79749
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Politecnico di Torino",
              "dsl": "Dipartimento di Automatica e Informatica"
            }
          ],
          "personId": 79667
        }
      ]
    },
    {
      "id": 79935,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Interpretable Aesthetic Analysis Model for Intelligent Photography Guidance Systems",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511155"
        },
        "Presentation Video": {
          "duration": "445",
          "hideBeforeConference": true,
          "title": "Interpretable Aesthetic Analysis Model for Intelligent Photography Guidance Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Yf0LvdLPyDw"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "An aesthetics evaluation model is at the heart of predicting users' aesthetic experience and developing user interfaces with higher quality. However, previous methods on aesthetic evaluation largely ignore the interpretability of the model and are consequently not suitable for many human-computer interaction tasks. We solve this problem by using a hyper-network to learn the overall aesthetic rating as a combination of individual aesthetic attribute scores. We further introduce a specially designed attentional mechanism in attribute score estimators to enable the users to know exactly which parts/elements of visual inputs lead to the estimated score. We demonstrate our idea by designing an intelligent photography guidance system. Computational results and user studies demonstrate the interpretability and effectiveness of our method.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "The Dept of Computer Science and Technology"
            }
          ],
          "personId": 79880
        }
      ]
    },
    {
      "id": 79936,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Estimating 3D Finger Pose via 2D-3D Fingerprint Matching",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511123"
        },
        "Presentation Video": {
          "duration": "598",
          "hideBeforeConference": true,
          "title": "Estimating 3D Finger Pose via 2D-3D Fingerprint Matching",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Aq0PZr5a73I"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Touchscreens have become the primary input devices for smartphones, tablet computers, and other intelligent devices over the past decades. While for the most pervasive commercial devices, only 2D touch positions on the screen are utilized as interaction inputs. To extend the richness of the input vocabulary, some researchers have proposed several innovative interaction techniques, e.g. finger pose. However, due to the low resolution and lacking in information of capacitive images, only two angles, pitch and yaw, are considered in most finger pose estimation algorithms, and the accuracy is not sufficiently high for large scale applications in smartphones. With the rapid development of under-screen fingerprint sensing technology, a new input modality, fingerprint image, for 3D finger pose estimation is available from these fingerprint sensors. In this paper, we propose a finger specific algorithm for estimating 3D finger pose including roll, pitch, and yaw from fingerprint images. 3D finger surface is first reconstructed based on sequential fingerprint images captured in enrollment, and given this 3D surface model, 3D finger pose of a test fingerprint is estimated by matching keypoints between the 2D image and 3D point cloud and minimizing the projection error. The proposed approach is a non-learning algorithm with good generalization ability and robustness in real applications. To evaluate the performance of our method, a dataset of fingerprint images with their corresponding ground truth 3D angles is collected. Experimental results on this dataset demonstrate the effectiveness of introducing reconstructed 3D finger surface shape in 3D finger pose estimation. The average absolute errors of three angles are 10.74 for roll, 8.25 for pitch, and 7.38 for yaw, respectively. Extensive experiments are also conducted to explore the impact of touching area size and gallery size on performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Automation"
            }
          ],
          "personId": 79716
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Beijing",
              "city": "Beijing",
              "institution": "Department of Automation, BNRist, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 79773
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Automation"
            }
          ],
          "personId": 79733
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": "Department of Automation"
            }
          ],
          "personId": 79674
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Department of Automation, BNRist, Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 79787
        }
      ]
    },
    {
      "id": 79937,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511160"
        },
        "Presentation Video": {
          "duration": "598",
          "hideBeforeConference": true,
          "title": "Intuitively Assessing ML Model Reliability through Example-Based Explanations and Editing Model Inputs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8mpYOiH-S9M"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "Interpretability methods aim to help users build trust in and understand the capabilities of machine learning models. However, existing approaches often rely on abstract, complex visualizations that poorly map to the task at hand or require non-trivial ML expertise to interpret. Here, we present two interface modules that facilitate intuitively assessing model reliability. To help users better characterize and reason about a model’s uncertainty, we visualize raw and aggregate information about a given input’s nearest neighbors. Using an interactive editor, users can manipulate this input in semantically-meaningful ways, determine the effect on the output, and compare against their prior expectations. We evaluate our approach using an electrocardiogram beat classification case study. Compared to a baseline feature importance interface, we find that 14 physicians are better able to align the model's uncertainty with domain-relevant factors and build intuition about its capabilities and limitations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "EECS"
            }
          ],
          "personId": 79921
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79700
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79865
        }
      ]
    },
    {
      "id": 79938,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Multimodal Fusion for Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511142"
        },
        "Presentation Video": {
          "duration": "700",
          "hideBeforeConference": true,
          "title": "Multimodal Fusion for Driver Referencing: A Comparison of Pointing to Objects Inside and Outside the Vehicle",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=k6pdWu2sRRo"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Advanced in-cabin sensing technologies, especially vision based approaches, have tremendously progressed user interaction inside the vehicle, paving the way for new applications of natural user interaction. \r\nJust as humans use multiple modes to communicate with each other, we follow an approach which is characterized by simultaneously using multiple modalities to achieve natural human machine interaction for a specific task: pointing to or glancing towards objects inside as well as outside the vehicle for deictic references. By tracking the movements of eye-gaze, head and finger, we design a multimodal fusion architecture using a deep neural network to precisely identify the driver's referencing intent. Additionally, we use a speech command as a trigger to separate each referencing event. We observe differences in driver behavior in the two pointing use cases (i.e. for inside and outside objects), especially when analyzing the preciseness of the three modalities eye, head, and finger. We conclude that there is no single modality that is solely optimal for all cases as each modality reveals certain limitations. Fusion of multiple modalities exploits the relevant characteristics from each modality, hence overcoming the case dependent limitations of each individual modality. Ultimately, we propose a method to identity whether the driver's intended object lies inside or outside the vehicle, based on the predicted pointing direction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bayern",
              "city": "Munich",
              "institution": "BMW Group",
              "dsl": "Intelligent Personal Assistant"
            },
            {
              "country": "Germany",
              "state": "Saarland",
              "city": "Saarbrücken",
              "institution": "Saarland University",
              "dsl": "Graduate School of Computer Science"
            }
          ],
          "personId": 79654
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "BMW Group",
              "dsl": ""
            }
          ],
          "personId": 79710
        }
      ]
    },
    {
      "id": 79939,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Show of Hands: Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511153"
        },
        "Presentation Video": {
          "duration": "583",
          "hideBeforeConference": true,
          "title": "Show of Hands: Leveraging Hand Gestural Cues in Virtual Meetings for Intelligent Impromptu Polling Interactions",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=HdZKsT2XMaQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Greater ubiquity and improvements of virtual meeting software usage have allowed for people to meet more seamlessly when in-person meetings are not feasible. However, certain valuable interaction cues such as impromptu group polling are less optimally executed compared to in-person meetings due to less easily gauging meeting participants who are not co-located, while requiring prior meeting setup to set up built-in polling tools for automated counting. We propose a novel intelligent user interface approach for prevalent virtual meeting software, which allows for more seamlessly impromptu polling interactions similarly to in-person meetings. Our approach leverages recognized communicative hand gestures and video filters as real-time responses for more easily informing meeting participants on impromptu polling results, and computer vision techniques to automatically count responses with the software’s default gallery view. We first performed studies to motivate leveraging gestures and visual cues into our prototype virtual meeting, which were then evaluated on system performance and user task workload assessment. Our results demonstrated that our system was able to recognize attendees’ gestures and polled responses with reasonable accuracy, and demonstrated improvements in hosts’ task workload performance. From our findings, we developed an interface that can allow hosts to be better informed of valuable impromptu polling while maintaining organic gestural interaction cues with participants similarly performed in in-person meetings.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79772
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79576
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79525
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79675
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79333
        }
      ]
    },
    {
      "id": 79940,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Trade-offs in Sampling and Search for Early-stage Interactive Machine Learning",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511134"
        },
        "Presentation Video": {
          "duration": "595",
          "hideBeforeConference": true,
          "title": "Trade-offs in Sampling and Search for Early-stage Interactive Machine Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=vVefqcvSZwQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "For many automated classification tasks, collecting labeled data is the key barrier to training a useful supervised model.  Interfaces for interactive labeling tighten the loop of labeled data collection and model development, enabling a subject-matter expert to quickly establish the feasibility of a classifier for addressing a problem of interest. These interactive machine learning (IML) interfaces iteratively sample unlabeled data for annotation, train a new model, and display feedback on the model's estimated performance.  Different sampling strategies affect both the rate at which the model improves and the bias of performance estimates. We compare the performance of three sampling strategies in the \"early-stage\" of label collection, starting from zero labeled data. By simulating a user's interactions with an IML labeling interface, we demonstrate a trade-off between improving a text classifier's performance and computing unbiased estimates of that performance. We show that supplementing early-stage sampling with user-guided text search can effectively \"seed\" a classifier with positive documents without compromising generalization performance—particularly for imbalanced tasks where positive documents are rare. We argue for the benefits of incorporating search alongside active learning in IML interfaces and identify design trade-offs around the use of non-random sampling strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Minnesota",
              "city": "Minneapolis",
              "institution": "University of Minnesota",
              "dsl": "GroupLens"
            }
          ],
          "personId": 79916
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 79742
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": "Alexa Shopping"
            }
          ],
          "personId": 79883
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Amazon",
              "dsl": ""
            }
          ],
          "personId": 79738
        }
      ]
    },
    {
      "id": 79941,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Understanding Screen Relationships from Screenshots of Smartphone Applications",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511109"
        },
        "Presentation Video": {
          "duration": "459",
          "hideBeforeConference": true,
          "title": "Understanding Screen Relationships from Screenshots of Smartphone Applications",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1mNqX1qw-2A"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "All graphical user interfaces are comprised of one or more screens that may be shown to the user depending on their interactions.\r\nIdentifying different screens of an app and understanding the type of changes that happen on the screens is a challenging task that can be applied in many areas including automatic app crawling, playback of app automation macros and large scale app dataset analysis. For example, an automated app crawler needs to understand if the screen it is currently viewing is the same as any previous screen that it has encountered, so it can focus its efforts on portions of the app that it has not yet explored. Moreover, identifying the type of change on the screen, such as whether any dialogues or keyboards have opened or closed, is useful for an automatic crawler to handle such events while crawling. \r\nUnderstanding screen relationships is a difficult task as instances of the same screen may have visual and structural variation, for example due to different content in a database-backed application, scrolling, dialog boxes opening or closing, or content loading delays. At the same time, instances of different screens from the same app may share some similarities in terms of design, structure, and content.\r\nThis paper uses a dataset of screenshots from more than 1K iPhone applications to train two ML models that understand similarity in different ways: (1) a screen similarity model that combines a UI object detector with a transformer model architecture to recognize instances of the same screen from a collection of screenshots from a single app, and (2) a screen transition model that uses a siamese network architecture to identify both similarity and three types of events that appear in an interaction trace: the keyboard or a dialog box appearing or disappearing, and scrolling.  Our models achieve an F1 score of 0.83 on the screen similarity task, improving on comparable baselines, and an average F-1 score of 0.71 across all events in the transition task.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science department"
            }
          ],
          "personId": 79679
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79753
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple Inc",
              "dsl": ""
            }
          ],
          "personId": 79695
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 79871
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 79800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Apple",
              "dsl": ""
            }
          ],
          "personId": 79775
        }
      ]
    },
    {
      "id": 79942,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Crowdsourcing Thumbnail Captions Using Time-Constrained Methods",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511136"
        },
        "Presentation Video": {
          "duration": "552",
          "hideBeforeConference": true,
          "title": "Crowdsourcing Thumbnail Captions Using Time-Constrained Methods",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jGZDrbChpl8"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Blind and Visually Impaired (BVI) individuals rely on captions to consume images; however, current methods using single captions may be ineffective at browsing through large quantities of images. This is because longer captions require more time to consume, whereas shorter captions may hinder the ability to understand the content. We explore the collection of thumbnail captions–succinct image descriptions meant to be consumed quickly–and comprehensive captions that allow individuals to understand content in greater detail. We consider text-based and time-constrained methods to collect descriptions at these two levels of detail, thumbnail and comprehensive captions, and find that the time-constrained method is effective at collecting thumbnail captions while preserving the correctness of the captions. Additionally, we assess model-based metrics against human ratings and show that they yielded comparable results, indicating the potential of the automatic evaluation of caption qualities using model-based metrics.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": ""
            }
          ],
          "personId": 79795
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Intuitive Computing Lab"
            }
          ],
          "personId": 79755
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "Johns Hopkins University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79816
        }
      ]
    },
    {
      "id": 79943,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Brainwave-Augmented Eye Tracker: High-Frequency SSVEPs Improves Camera-Based Eye Tracking Accuracy",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511151"
        },
        "Presentation Video": {
          "duration": "612",
          "hideBeforeConference": true,
          "title": "Brainwave-Augmented Eye Tracker: High-Frequency SSVEPs Improves Camera-Based Eye Tracking Accuracy",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=sOiVnXzpqTc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "In this work, we leverage neural mechanisms of visual attention to improve the accuracy of a commercial eye tracker through the analysis of electroencephalography (EEG) waves. Gaze targets were rendered in a computer screen with imperceptible flickering stimuli (≥40Hz) that elicited attention-modulated steady-state visual evoked potentials (SSVEPs). Our hybrid system combines EEG and eye-tracking modalities to overcome accuracy limitations of the gaze-tracker alone. We integrate EEG and gaze data to efficiently exploit their complementary strengths driving a Bayesian probabilistic decoder that estimates the target gazed by the user. Our system's performance was analyzed across the screen with varying target sizes, spacings and dataset epoch lengths, using data from 10 subjects. Overall, our hybrid approach improves the classification accuracy of the eye tracker alone for all target parameters and dataset epoch lengths in 11 units on average. The system shows a larger impact at peripheral screen regions where performance enhancement is maximal, reaching improvements of over 45 units. The findings of this work demonstrate that the intrinsic accuracy limitations of camera-based eye-trackers can be corrected with the integration of EEG data, and opens opportunities for gaze tracking applications with higher target granularity.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Field Intelligence Lab"
            }
          ],
          "personId": 79878
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "MIT Open Learning"
            }
          ],
          "personId": 79825
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "Field Intelligence Lab"
            }
          ],
          "personId": 79889
        }
      ]
    },
    {
      "id": 79944,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "SoftVideo: Improving the Learning Experience of Software Tutorial Videos with Collective Interaction Data",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511106"
        },
        "Presentation Video": {
          "duration": "587",
          "hideBeforeConference": true,
          "title": "SoftVideo: Improving the Learning Experience of Software Tutorial Videos with Collective Interaction Data",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Iwp_EBpTP4w"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "Many people rely on tutorial videos when learning how to perform tasks using complex software. Watching the video for instructions and applying them to target software require frequent going back-and-forth between the two, which incurs cognitive overhead. Furthermore, users need to constantly compare the two to see if they are following correctly, sometimes missing out on subtle differences between the two. We propose SoftVideo, a prototype system that helps users plan ahead before watching each step in tutorial videos and provides feedback and help to users on their progress. SoftVideo provides information on each step such as its difficulty, lets users know if they completed or missed a step, and suggests tips such as relevant steps when it detects users struggling. Since experiences of previous learners with the same goal can provide insights into how they learned from the tutorial, we leveraged collective user logs to identify the difficulty and relatedness of each step. We collected and analyzed video interaction logs and the associated Photoshop usage logs for 2 tutorial videos from 120 users. We then defined six metrics that portray the difficulty of each step including the time taken to complete a step and the number of pauses in a step, which were also used to detect users' struggling moments by comparing their progress to the collected data. To investigate the feasibility and usefulness of using data-driven information and the suggested design, we ran a user study with 30 participants where participants followed one of the two Photoshop tutorial videos with SoftVideo. Results show that participants were able to proactively and effectively plan their pauses and playback speed, and vary their concentration level. They were also able to identify and recover from errors with the help SoftVideo provides.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": ""
            }
          ],
          "personId": 79886
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79754
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "School of Computing, KAIST",
              "dsl": ""
            }
          ],
          "personId": 79781
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79788
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Seongnam",
              "institution": "NAVER AI Lab",
              "dsl": ""
            }
          ],
          "personId": 79770
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "KAIST",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79673
        }
      ]
    },
    {
      "id": 79945,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "How Do People Rank Multiple Mutant Agents?",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511115"
        },
        "Presentation Video": {
          "duration": "575",
          "hideBeforeConference": true,
          "title": "How Do People Rank Multiple Mutant Agents?",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xI2QgMyj_8E"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "How might a person decide on which of several AI-powered sequential decision-making systems to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (e.g., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) a new XAI empirical task to measure explanations: \"the Ranking Task\"; 2) a new strategy for inducing controllable agent variations---Mutant Agent Generation; 3) novel explanations for sequential decision-making agents; 4) an adaptation to the AAR/AI assessment process; and 5) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user's perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent \"test selection\" strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of Electrical Engineering and Computer Science"
            }
          ],
          "personId": 79777
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of EECS"
            }
          ],
          "personId": 79693
        }
      ]
    },
    {
      "id": 79946,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "NewsPod: Automatic and Interactive News Podcasts",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511147"
        },
        "Presentation Video": {
          "duration": "602",
          "hideBeforeConference": true,
          "title": "NewsPod: Automatic and Interactive News Podcasts",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-kfJZD7GA8s"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "News podcasts are a popular medium to stay informed and dive deep into news topics. Today, most podcasts are handcrafted by professionals. In this work we advance the state-of-the-art in automatically generated podcasts, making use of recent advances in natural language processing and text-to-speech technology. We present NewsPod, an automatically generated, interactive news podcast. The podcast is divided into segments, each centered on a news event, with each segment structured as a Question and Answer  conversation, whose goal is to engage the listener. A key aspect of the design is the use of distinct voices for each role (questioner, responder), to better simulate a conversation. Another novel aspect of this system allows listeners to interact with the podcast by asking their own questions and receiving automatically generated answers. We validate the soundness of this system design through two usability studies, focused on evaluating the narrative style and interactions with the podcast, respectively. We find that NewsPod is preferred over a baseline by participants, with 80% claiming they would use the system in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 79750
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 79687
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 79670
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79691
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 79877
        }
      ]
    },
    {
      "id": 79947,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "KWickChat: A Multi-Turn Dialogue System for AAC Using Context-Aware Sentence Generation by Bag-of-Keywords",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511145"
        },
        "Presentation Video": {
          "duration": "574",
          "hideBeforeConference": true,
          "title": "KWickChat: A Multi-Turn Dialogue System for AAC Using Context-Aware Sentence Generation by Bag-of-Keywords",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=N2cZKFK-12Y"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "We present KWickChat (Keyword Quick Chat): a multi-turn augmentative and alternative communication (AAC) dialogue system for nonspeaking individuals with motor disabilities. The central objective of KWickChat is to reduce the communication gap between nonspeaking and speaking partners by exploring a sentence-based text entry system that automatically generates suitable sentences for the nonspeaking partner based on keyword entry. The system is underpinned by the state-of-the-art GPT-2 language model and leverages context information, including dialogue history and persona tags, to improve the quality of the generated responses. We evaluate the system by analyzing the functional design and decomposing it into key functions and parameters that are systematically investigated using envelope analysis. Our results show that with word prediction and with a threshold word error rate of 0.65, the keystroke savings of the KWickChat system is around 71%. We also recruited two human judges to evaluate the semantic consistency between 400 sentences generated by KWickChat and reference sentences. They both reported a median rating of 4 on a scale from 1 (very bad) to 5 (very good) for the best generated sentence in each exchange and achieved an inter-rater reliability of 0.92 across all 400 sentences judged.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge ",
              "dsl": "Department of Engineering "
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge ",
              "dsl": "Department of Engineering "
            }
          ],
          "personId": 79744
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Cambridge",
              "city": "University of Cambridge",
              "institution": "Department of Engineering",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "Cambridge",
              "city": "University of Cambridge",
              "institution": "Department of Engineering",
              "dsl": ""
            }
          ],
          "personId": 79789
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Engineering"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 79784
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Engineering"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "University of Cambridge",
              "dsl": "Department of Engineering"
            }
          ],
          "personId": 79683
        }
      ]
    },
    {
      "id": 79948,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Emotion Recognition in Conversations using Brain and Physiological Signals",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511148"
        },
        "Presentation Video": {
          "duration": "552",
          "hideBeforeConference": true,
          "title": "Emotion Recognition in Conversations using Brain and Physiological Signals",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=AVR3gMXZwuU"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Emotions are complicated psycho-physiological processes that are related to numerous external and internal changes in the body. They play an essential role in human-human interaction and can be important for human-machine interfaces. Automatically recognizing emotions in conversation could be applied in many application domains like health-care, education, social interactions, and entertainment. Facial expressions, speech, and body gestures are primary cues that have been widely used for recognizing emotions in conversation. However, these cues can be ineffective as they cannot reveal underlying emotions when a person involuntarily or deliberately conceals their emotions. Researchers have shown that analyzing brain activity and physiological signals can lead to more reliable emotion recognition since they generally cannot be controlled. However, these body responses in emotional situations have been rarely explored in interactive tasks like conversations. This paper explores and discusses the performance and challenges of using brain activity and other physiological signals in recognizing emotions in a face-to-face conversation. We present an experimental setup for stimulating spontaneous emotions during a face-to-face conversation while recording brain and physiological activity. We then describe our analysis strategies for recognizing emotions using Electroencephalography (EEG), Photoplethysmography (PPG), and Galvanic Skin Responses (GSR) signals in a subject-dependent and subject-independent approach. Finally, we describe new directions for future research in conversational emotion recognition, and the limitations and challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "Auckland",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute",
              "dsl": "Empathic Computing Laboratory"
            }
          ],
          "personId": 79688
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Department of Psychological Medicine"
            }
          ],
          "personId": 79697
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Empathic Computing Lab (ECl)",
              "dsl": ""
            }
          ],
          "personId": 79881
        },
        {
          "affiliations": [
            {
              "country": "Ethiopia",
              "state": "",
              "city": "Addis Ababa",
              "institution": "Addis Ababa University",
              "dsl": ""
            }
          ],
          "personId": 79723
        },
        {
          "affiliations": [
            {
              "country": "Ethiopia",
              "state": "-None-",
              "city": "Addis Ababa",
              "institution": "Addis Ababa University",
              "dsl": "Electrical and Computer Engineering"
            }
          ],
          "personId": 79794
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "Auckland Bioengineering Institute, The University of Auckland",
              "dsl": "Augmented Human Lab"
            }
          ],
          "personId": 79823
        },
        {
          "affiliations": [
            {
              "country": "New Zealand",
              "state": "",
              "city": "Auckland",
              "institution": "University of Auckland",
              "dsl": "Psychological Medicine"
            }
          ],
          "personId": 79696
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Mawson Lakes",
              "institution": "University of South Australia",
              "dsl": "ITMS"
            }
          ],
          "personId": 79890
        }
      ]
    },
    {
      "id": 79949,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Developing Persona Analytics Towards Persona Science",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511144"
        },
        "Presentation Video": {
          "duration": "458",
          "hideBeforeConference": true,
          "title": "Developing Persona Analytics Towards Persona Science",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=86BHK4QKTXI"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Much of the reported work on personas suffers from the lack of empirical evidence. To address this issue, we introduce Persona Analytics (PA), a system that tracks how users interact with data-driven personas. PA captures users’ mouse and gaze behavior to measure users’ interaction with data-driven personas and system features of an interactive persona system. Measuring these activities grants an understanding of the behaviors of a persona user, required for quantitative measurement of persona use to obtain scientifically valid evidence. Conducting a study with 144 participants, we demonstrate how PA can be deployed for remote user studies during exceptional times when physical user studies are difficult if not impossible.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "Hamad Bin Khalifa University",
              "dsl": "Qatar Computing Research Institute"
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Vaasa",
              "institution": "University of Vaasa",
              "dsl": ""
            }
          ],
          "personId": 79741
        },
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "Hamad Bin Khalifa University",
              "dsl": "Qatar Computing Research Institute"
            }
          ],
          "personId": 79797
        },
        {
          "affiliations": [
            {
              "country": "Qatar",
              "state": "",
              "city": "Doha",
              "institution": "Hamad Bin Khalifa University",
              "dsl": "Qatar Computing Research Institute"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Penn State",
              "dsl": "College of Information Science and Technology"
            }
          ],
          "personId": 79704
        }
      ]
    },
    {
      "id": 79950,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Contextualization and exploration of local feature importance as explanations to improve understanding and satisfaction of non-expert users",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511139"
        },
        "Presentation Video": {
          "duration": "528",
          "hideBeforeConference": true,
          "title": "Contextualization and exploration of local feature importance as explanations to improve understanding and satisfaction of non-expert users",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uc15oaOGJi0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "The increasing usage of complex ML models to make important decisions has raised interest in explainable artificial intelligence (XAI). In this work, we focus on providing accessible and useful explanations for non-expert users. More specifically, we propose generic XAI design principles for contextualizing and allowing the exploration of local feature importance explanations. To evaluate the effectiveness of these principles for improving users' objective understanding and satisfaction, we conduct a monitored user study with 80 participants using 4 different versions of our XAI interface, in the context of an insurance scenario. Our results show that contextualization is overall more effective than exploration to improve objective understanding and satisfaction. However, we obtain the highest satisfaction scores when contextualization and exploration are combined.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne University",
              "dsl": "LIP6"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "AXA Research",
              "dsl": ""
            }
          ],
          "personId": 79842
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "AXA Research",
              "dsl": ""
            }
          ],
          "personId": 79838
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne University",
              "dsl": "LIP6"
            }
          ],
          "personId": 79684
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Université Paris Lumière",
              "dsl": "EA 4004 - Cognitions Humaine et Artificielle"
            },
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "University Paris 08",
              "dsl": "CHArt-Lutin"
            }
          ],
          "personId": 79862
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "AXA Research",
              "dsl": ""
            }
          ],
          "personId": 79720
        }
      ]
    },
    {
      "id": 79951,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "AQX: Explaining Air Quality Forecast for Verifying Domain Knowledge using Feature Importance Visualization",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511150"
        },
        "Presentation Video": {
          "duration": "607",
          "hideBeforeConference": true,
          "title": "AQX: Explaining Air Quality Forecast for Verifying Domain Knowledge using Feature Importance Visualization",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=yXpxNFpwuYw"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "Air pollution forecast has become critical because of its direct impact on human health and its increased production caused by rapid industrialization. Machine learning (ML) solutions are being drastically explored in this domain because they can potentially produce highly accurate results with access to historical data. However, experts in the environmental area are skeptical about adopting ML solutions in real-world applications and policymaking due to their black-box nature. In contrast, despite having low accuracy sometimes, the existing traditional simulation model (e.g., CMAQ) are widely used and follows well-defined and transparent equations. Therefore, validating the ML model’s learning with the existing domain knowledge might aid in addressing their skepticism, building appropriate trust, and better utilizing ML models. In collaboration with three experts with an average of five years of research experience in the air pollution domain, we identified that feature (meteorological feature like wind) contribution, towards the final forecast as the major information to be verified with domain knowledge. In addition, the accuracy of ML models compared with traditional simulation models and raw wind trajectories are essential for domain experts to validate the feature contribution. Based on the identified information, we designed and developed AQX, a visual analytics system to help experts validate and verify the ML model’s learning with their domain knowledge. The system includes coordinated multiple views to present the contributions of input features at different levels of aggregation in both temporal and spatial dimensions. It also provides an accuracy comparison of ML and traditional models from the time dimension and spatial map, along with the animation of raw wind trajectories for the input period. We further demonstrated two case studies and conducted expert interviews with two domain experts to show the effectiveness and usefulness of AQX.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 79761
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 79822
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 79792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Computer Science and Engineering"
            },
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Notre Dame",
              "institution": "University of Notre Dame",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 79731
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "New Territories",
              "city": "Clear Water Bay",
              "institution": "The Hong Kong Uni of Sci and Technology",
              "dsl": "Division of Environment and Sustainability "
            },
            {
              "country": "Hong Kong",
              "state": "New Territories",
              "city": "Clear Water Bay",
              "institution": "The Hong Kong Uni of Sci and Technology",
              "dsl": "Division of Environment and Sustainability "
            }
          ],
          "personId": 79853
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            },
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "The Hong Kong University of Science and Technology",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 79759
        }
      ]
    },
    {
      "id": 79952,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511121"
        },
        "Presentation Video": {
          "duration": "603",
          "hideBeforeConference": true,
          "title": "Exploring the Effects of Machine Learning Literacy Interventions on Laypeople's Reliance on Machine Learning Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=r7QUj73_C94"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "Today, machine learning (ML) technologies have penetrated almost every aspect of people's lives, yet public understandings of these technologies are often limited. This highlights the urgent need of designing effective methods to increase people's machine learning literacy, as the lack of relevant knowledge may result in people's inappropriate usage of machine learning technologies. In this paper, we focus on an ML-assisted decision-making setting and conduct a human-subject randomized experiment to explore how providing different types of user tutorials as the machine learning literacy interventions can influence laypeople's reliance on ML models, on both in-distribution and out-of-distribution examples. We vary the existence, interactivity and scope of the user tutorial across different treatments in our experiment. Our results show that user tutorials, when presented in appropriate forms, can help some people rely on ML models more appropriately. For example, for those individuals who have relatively high ability in solving the decision-making task themselves, they tend to reduce their over-reliance on ML models on out-of-distribution examples after receiving most types of user tutorials, especially when such tutorial is interactive or presented as describing properties of ML models in general.  In contrast, low-performing individuals' reliance on the ML model is not affected by the presence or the type of user tutorial. Finally, we also find that people perceive the interactive tutorial to be more understandable and slightly more useful. We conclude by discussing the design implications of our study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 79906
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 79393
        }
      ]
    },
    {
      "id": 79953,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "iSEA : An Interactive Pipeline for Semantic Error Analysis of NLP Models",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511146"
        },
        "Presentation Video": {
          "duration": "582",
          "hideBeforeConference": true,
          "title": "iSEA : An Interactive Pipeline for Semantic Error Analysis of NLP Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=TbfazKzM5PQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "Error analysis in NLP models is essential to successful model development and deployment. One common approach for diagnosing errors is to identify subpopulations in the dataset where the model produces the most errors. However, existing approaches typically define subpopulations based on pre-defined features, which requires users to form hypotheses of errors in advance. To complement these approaches, we propose iSEA, an Interactive Pipeline for Semantic Error Analysis in NLP Models, which automatically discovers semantically-grounded subpopulations with high error rates in the context of a human-in-the-loop interactive system. iSEA enables model developers to learn more about their model errors through discovered subpopulations, validate the sources of errors through interactive analysis on the discovered subpopulations, and test hypotheses about model errors by defining custom subpopulations. The tool supports semantic descriptions of error-prone subpopulations at the token and concept level, as well as pre-defined higher-level features. Through use cases and expert interviews, we demonstrate how iSEA can assist error understanding and analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 79923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce Research",
              "dsl": ""
            }
          ],
          "personId": 79678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce",
              "dsl": "Salesforce"
            }
          ],
          "personId": 79726
        }
      ]
    },
    {
      "id": 79954,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "EyeSayCorrect: Eye Gaze and Voice Based Hands-free Text Correction for Mobile Devices",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511103"
        },
        "Presentation Video": {
          "duration": "521",
          "hideBeforeConference": true,
          "title": "EyeSayCorrect: Eye Gaze and Voice Based Hands-free Text Correction for Mobile Devices",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cSYQwaqchl0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Text correction on mobile devices usually requires precise and repetitive manual control. In this paper, we present EyeSayCorrect, an eye gaze and voice based hands-free text correction method for mobile devices. To correct text with EyeSayCorrect, the user first utilizes the gaze point on the screen to select a word, then speaks the new phrase. EyeSayCorrect would then infer the user's correction intention based on the inputs and the text context. EyeSayCorrect can accommodate ambiguities and noisy input signals. We used a Bayesian approach for determining the selected word given an eye-gaze trajectory. Given each sampling point in an eye-gaze trajectory, the posterior probability of selecting a word is calculated and accumulated. The target word would be selected when its accumulated interest is larger than a threshold. The misspelled words have higher priors. Our evaluation showed that EyeSayCorrect can correct text with promising performance. The mean +/- 95% CI of the task completion time (in seconds) with priors is 11.63 +/- 1.07 for large font size (28 pt) and 11.57 +/- 1.14 for small font size (14 pt). Using priors for misspelled words reduced the task-completion time of large text  by 9.26% and small text  by 23.79%, and it reduced the text-selecting time of large text by 23.49% and small text by 40.35%. The subjective ratings are also in favor of the method with priors for misspelled words. Overall, EyeSayCorrect utilizes the advantages of eye gaze and voice input, making hands-free text correction available and efficient on mobile devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79803
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Orangeburg",
              "institution": "Tappan Zee High School",
              "dsl": ""
            }
          ],
          "personId": 79851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79863
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University ",
              "dsl": "Dept of Computer Science"
            }
          ],
          "personId": 79891
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79748
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": ""
            }
          ],
          "personId": 79905
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79861
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "East Setauket",
              "institution": "Ward Melville High School",
              "dsl": ""
            }
          ],
          "personId": 79918
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook Medicine",
              "dsl": "Clinically Integrated Network"
            }
          ],
          "personId": 79758
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": ""
            }
          ],
          "personId": 79899
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79864
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Computer Science, Stony Brook University",
              "dsl": "Knowledge Systems Lab"
            }
          ],
          "personId": 79729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 79685
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79830
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79846
        }
      ]
    },
    {
      "id": 79955,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511122"
        },
        "Presentation Video": {
          "duration": "594",
          "hideBeforeConference": true,
          "title": "Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=UU5LAxF8-7Q"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "Embeddings mapping high-dimensional discrete input to lower-dimensional continuous vector spaces have been widely adopted in machine learning applications as a way to capture domain semantics. Interviewing 13 embedding users across disciplines, we find comparing embeddings is a key task for deployment or downstream analysis but unfolds in a tedious fashion that poorly supports systematic exploration. In response, we present the Embedding Comparator, an interactive system that presents a global comparison of embedding spaces alongside fine-grained inspection of local neighborhoods. It systematically surfaces points of comparison by computing the similarity of the k-nearest neighbors of every embedded object between a pair of spaces. Through case studies across multiple modalities, we demonstrate our system rapidly reveals insights, such as semantic changes following fine-tuning, language changes over time, and differences between seemingly similar models. In evaluations with 15 participants, we find our system accelerates comparisons by shifting from laborious manual specification to browsing and manipulating visualizations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79657
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79865
        }
      ]
    },
    {
      "id": 79956,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Does Using Voice Authentication in Multimodal Systems Correlate With Increased Speech Interaction During Non-critical Routine Tasks?",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511129"
        },
        "Presentation Video": {
          "duration": "601",
          "hideBeforeConference": true,
          "title": "Does Using Voice Authentication in Multimodal Systems Correlate With Increased Speech Interaction During Non-critical Routine Tasks?",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ghIPmO13PkQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "Multimodal systems offer their functionalities through multiple communication channels, with touch and speech being the most common modalities on commercial devices [13]. A messenger application may thus take either keyboard or voice input, and present incoming messages as text or audio output. This allows the users to communicate with their devices using the modality that best suits their current context and personal preference. Authentication is often the first point of interaction with an application. The users’ login behavior can therefore be used to immediately adapt the communication channel to their situation and preferences. Yet given the sensitive nature of authentication, this interaction may not be representative for the user’s inclination to use speech input in non-critical routine tasks. In this paper, we therefore test whether the interactions during authentication differ from non-critical routine tasks in a smart home application. Our findings indicate that, even in such a private space, the authentication behavior does not correlate with the use of speech input during non-critical routine task, or with the perceived usability of speech input. We further find that short interactions with the system are not indicative of the user’s attitude towards audio output, independent of whether authentication or non-critical tasks are performed. Since security concerns are minmized in the secure environment of private spaces, our findings can be generalized to other contexts where security threats are even more apparent.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "Information Systems II",
              "dsl": "University of Mannheim"
            }
          ],
          "personId": 79350
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": "Information Systems II"
            }
          ],
          "personId": 79706
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": "Information Systems II"
            }
          ],
          "personId": 79463
        }
      ]
    },
    {
      "id": 79957,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TastePaths: Enabling Deeper Exploration and Understanding of Personal Preferences in Recommender Systems",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511156"
        },
        "Presentation Video": {
          "duration": "602",
          "hideBeforeConference": true,
          "title": "TastePaths: Enabling Deeper Exploration and Understanding of Personal Preferences in Recommender Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=LFzAQ8Gxd94"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "Recommender systems are ubiquitous and influence the information we consume on a daily basis by helping us navigate vast catalogs of information like music databases. However, their linear approach of surfacing content in ranked lists limits their ability to help us grow and understand our personal preferences. In this paper, we study how we can better support users in exploring a novel space, specifically focusing on music genres. Informed by interviews with expert music listeners, we developed TastePaths: an interactive web-tool which helps users explore an overview of the genre-space via a graph of connected artists. We conducted a comparative user study with 16 participants where each of them used a personalized version of TastePaths (built with a set of artists the user listens to frequently) and a non-personalized one (based on a set of the most popular artists in the genre). We find that while participants employed various strategies to explore the space, overall they greatly preferred the personalized version as it helped anchor their exploration and provide recommendations that were more compatible with their personal taste. In addition to that,  TastePaths helped participants specify and articulate their interest in the genre and gave them a better understanding of the system's organization of music. Based on our findings, we discuss opportunities and challenges for incorporating more control and expressive feedback in recommendation systems, in order to help users explore spaces beyond their immediate interests and improve these systems' underlying algorithms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 79904
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Spotify",
              "dsl": ""
            }
          ],
          "personId": 79839
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "Spotify",
              "dsl": ""
            }
          ],
          "personId": 79707
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Spotify Research",
              "dsl": ""
            }
          ],
          "personId": 79745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Spotify",
              "dsl": ""
            }
          ],
          "personId": 79898
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Spotify",
              "dsl": ""
            }
          ],
          "personId": 79699
        }
      ]
    },
    {
      "id": 79958,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "VideoSticker: A Tool for Active Viewing and Visual Note-taking from Videos",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511132"
        },
        "Presentation Video": {
          "duration": "599",
          "hideBeforeConference": true,
          "title": "VideoSticker: A Tool for Active Viewing and Visual Note-taking from Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=doOW5NJiLP4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "Video is an effective medium for knowledge communication and learning. Yet active viewing and note-taking from videos remain a challenge.  Specifically, during note-taking, viewers find it difficult to extract essential information such as representation, composition, motion, and interactions of graphical objects and narration. Current approaches rely on creating static screenshots, manual clipping, and manual annotation/transcription. Additionally, note-takers may need to repeatedly pause and rewind the video, disrupting their active viewing process. We propose VideoSticker, a tool designed to support visual note-taking by extracting expressive content from videos as 'motion stickers'. VideoSticker implements automated object detection and tracking, linking objects to the transcript, and rapid extraction of stickers across space, time, and events of interest. VideoSticker's two-pass approach allows viewers to capture high-level information uninterrupted and later extract specific details. We demonstrate the usability of VideoSticker for a variety of videos and note-taking needs. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 79659
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 79332
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 79496
        }
      ]
    },
    {
      "id": 79959,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Utilizing Core-Query for Context-Sensitive Ad Generation Based on Dialogue",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511116"
        },
        "Presentation Video": {
          "duration": "602",
          "hideBeforeConference": true,
          "title": "Utilizing Core-Query for Context-Sensitive Ad Generation Based on Dialogue",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1mUuAEXaluc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "In this work, we present a system that sequentially generates advertisements within the context of a dialogue. Advertisements tailored to the user have long been displayed on the digital signage in stores, on web pages, and on smartphone applications, and they will work more effectively if they are aware of the context of the dialogue between the users. Creating an advertising sentence as a query and searching the web by using that query is one way to present a variety of advertisements, but there is currently no method to create an appropriate search query for the search according to the dialogue context. Therefore, we developed a method called the Conversational Context-sensitive Advertisement generator (CoCoA). The novelty of CoCoA is that advertisers simply need to prepare a few abstract phrases, called Core-Queries, and then CoCoA dynamically transforms the Core-Queries into complete search queries according to the dialogue context. The transformation is enabled by a masked word prediction technique that predicts a word that is hidden in a sentence. Our attempt is the first to apply masked word prediction to a web information retrieval framework that takes into account the dialogue context. We asked users to evaluate the search query presented by CoCoA against the dialogue text of multiple domains prepared in advance and found that CoCoA was able to present more contextual and effective advertisements than Google Suggest or a method without the query transformation. In addition, we found that CoCoA generated high-quality advertisements that advertisers had not expected when they created the Core-Queries.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 79874
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohanma",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 79739
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Dept. of Information & Computer Science/Keio University/Imai Lab"
            }
          ],
          "personId": 79813
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": ""
            }
          ],
          "personId": 79690
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokohama-shi",
              "institution": "Keio University",
              "dsl": "Faculty of Science and Technology"
            }
          ],
          "personId": 79682
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Kouhoku-ku, Yokohama-shi",
              "institution": "3-14-1 Hiyoshi",
              "dsl": "Keio University"
            }
          ],
          "personId": 79913
        }
      ]
    },
    {
      "id": 79960,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "An Intelligent Pedagogical Agent to Foster Computational Thinking in Open-Ended Game Design Activities",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511124"
        },
        "Presentation Video": {
          "duration": "563",
          "hideBeforeConference": true,
          "title": "An Intelligent Pedagogical Agent to Foster Computational Thinking in Open-Ended Game Design Activities",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=gRx0NLgLCXs"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "Free-form Game-Design (GD) environments show promise in fostering Computational Thinking (CT) skills at a young age. However, such environments can be challenging to some students due to their highly open-ended nature. Our long-term goal is to alleviate this difficulty via agents that can monitor the student interaction with the environment, detect when the student needs help and provide personalized support accordingly. In this paper, we present a preliminary evaluation of one such agent deployed in a real-word free-form GD learning environment to foster CT in the early K-12 education, Unity-CT. We focus on the effect of repetition by comparing student behaviors between no intervention, 1-shot intervention, and repeated intervention groups for two different errors that are known to be challenging in the online lessons of Unity-CT environment. Our findings showed that the agents were perceived very positively by the students and the repeated intervention showed promising results in terms of helping students making less errors and more correct behaviors, albeit only for one of the two target errors. Based on these results, we provide insights on how to improve the delivery of the IPA interventions in free-form GD environments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "University of British Columbia",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79722
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne University",
              "dsl": "LIP6"
            }
          ],
          "personId": 79708
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "British Columbia",
              "city": "Vancouver",
              "institution": "UBC",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79740
        }
      ]
    },
    {
      "id": 79961,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Explaining Call Recommendations in Nursing Homes: A User-Centered Design Approach for Interacting with Knowledge-Based Health Decision Support Systems",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511158"
        },
        "Presentation Video": {
          "duration": "602",
          "hideBeforeConference": true,
          "title": "Explaining Call Recommendations in Nursing Homes: A User-Centered Design Approach for Interacting with Knowledge-Based Health Decision Support Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EcUO0ErEP8k"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "Recommender systems are increasingly used in high-risk application domains, including healthcare. It has been shown that explanations are crucial in this context to support decision-making. In this paper, we explore how to explain call recommendations to nurses in nursing homes, providing insight into call priority, notifications, and resident information that may contribute to residents' safety and quality of care. We present the design and implementation of a recommender engine, and a mobile application designed to support call recommendations and explanations of these recommendations. More specifically, we report on the results of a user-centered design approach with residents (N=12) and healthcare professionals (N=4), and  a final evaluation (N=12) after four months of deployment. The results show that our design approach provides a valuable tool for more accurate and efficient decision-making. The overall system encourages nursing home staff to provide feedback and annotate, resulting in more confidence in the system. We discuss usability issues, challenges and reflections to be considered in future health recommender systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            },
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79776
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            },
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79478
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Sciences"
            },
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Sciences"
            }
          ],
          "personId": 79867
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            },
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79907
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            },
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79374
        }
      ]
    },
    {
      "id": 79962,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Do people engage cognitively with AI? Impact of AI assistance on incidental learning",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511138"
        },
        "Presentation Video": {
          "duration": "550",
          "hideBeforeConference": true,
          "title": "Do people engage cognitively with AI? Impact of AI assistance on incidental learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=oVYGAg1GgTo"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "When people receive advice while making difficult decisions, they often make better decisions in the moment and also increase their knowledge in the process. However, such incidental learning can only occur when people cognitively engage with the information they receive and process this information carefully and thoughtfully. How do people process the information and advice they receive from AI, and do they engage with it deeply enough to enable learning? To answer these questions, we conducted three experiments in which individuals were asked to make nutritional decisions and received simulated AI recommendations and explanations. In the first experiment, we found that when people were presented with both a recommendation and an explanation before making their choice, they made better decisions than they did when they received no such help, but they did not learn. In the second experiment, participants first made their own choice, and then examined a recommendation and an explanation from AI; this condition also resulted in improved decisions, but no learning. However, in our third experiment, participants were presented with just an AI explanation but no recommendation and had to arrive at their own decisions. This condition led to both more accurate decisions and learning gains. We hypothesize that learning gains in this condition were due to deeper engagement with explanations needed to arrive at the decisions. This work provides some of the most direct evidence to date that it may not be sufficient to provide people with AI-generated recommendations and explanations to ensure that people engage carefully with the AI-provided information. This work also presents one technique that enables incidental learning and, by implication, can help people process AI recommendations and explanations more carefully.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Harvard University",
              "dsl": "School of Engineering and Applied Sciences"
            }
          ],
          "personId": 79656
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Department of Biomedical Informatics"
            }
          ],
          "personId": 79833
        }
      ]
    },
    {
      "id": 79963,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511143"
        },
        "Presentation Video": {
          "duration": "588",
          "hideBeforeConference": true,
          "title": "Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3pmgrnXzHJY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "User interfaces (UI) of desktop, web, and mobile applications involve a hierarchy of objects (e.g. applications, screens, view class, and other types of design objects) with multimodal (e.g. textual, visual) and positional (e.g. spatial location, sequence order and hierarchy level) attributes. We can therefore represent a set of application UIs as a heterogeneous network with multimodal and positional attributes. Such a network not only represents how users understand the visual layout of UIs, but also influences how users would interact with applications through these UIs. To model the UI semantics well for different UI annotation, search, and evaluation tasks, this paper proposes the novel Heterogeneous Attention-based Multimodal Positional (HAMP) graph neural network model. HAMP combines graph neural networks with the scaled dot-product attention used in transformers to learn the embeddings of heterogeneous nodes and associated multimodal and positional attributes in a unified manner. HAMP is evaluated with classification and regression tasks conducted on three distinct real-world datasets. Our experiments demonstrate that HAMP significantly out-performs other state-of-the-art models on such tasks. We also report our ablation study results on HAMP.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": "Singapore Management University"
            }
          ],
          "personId": 79341
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "SMU",
              "dsl": "School of Information Systems"
            }
          ],
          "personId": 79581
        }
      ]
    },
    {
      "id": 79964,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "GridBook: Natural Language Formulas for the Spreadsheet Grid",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511161"
        },
        "Presentation Video": {
          "duration": "599",
          "hideBeforeConference": true,
          "title": "GridBook: Natural Language Formulas for the Spreadsheet Grid",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wf8dC3OfKH0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "Writing formulas on the spreadsheet grid is arguably the most widely practiced form of programming. Still, studies highlight the difficulties experienced by end-user programmers when learning and using traditional formulas, especially for slightly complex tasks. The purpose of GridBook is to ease these difficulties by supporting formulas expressed in natural language within the grid; it is the first system to do so. GridBook builds on a deep-learning based parser to understand analysis intents from the natural language input within a spreadsheet cell. GridBook also leverages the spatial context between cells to infer the analysis parameters underspecified in the natural language input. Natural language enables users to analyze data easily and flexibly, to build queries on the results of previous analyses, and to view results intelligibly within the grid—thus taking spreadsheets one step closer to computational notebooks. Our comparative lab study with 20 experienced data analysts, new only to GridBook, shows that data analysis with GridBook is significantly faster on average than with computational notebooks, and no slower than with conventional spreadsheets. Our study uncovers insights into the application of natural language as a special purpose programming language for end-user programming in spreadsheets.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 79820
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research Asia",
              "dsl": ""
            }
          ],
          "personId": 79796
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research Asia",
              "dsl": ""
            }
          ],
          "personId": 79653
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Edinburgh University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79866
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research Asia",
              "dsl": ""
            }
          ],
          "personId": 79798
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Microsoft Research Asia",
              "dsl": ""
            }
          ],
          "personId": 79746
        }
      ]
    },
    {
      "id": 79965,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "HINT: Integration Testing for AI-based features with Humans in the Loop",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511141"
        },
        "Presentation Video": {
          "duration": "606",
          "hideBeforeConference": true,
          "title": "HINT: Integration Testing for AI-based features with Humans in the Loop",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nmGTVCm6Qv4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "The dynamic nature of AI technologies makes testing human-AI interaction and collaboration challenging especially before such features are deployed in the wild. This presents a challenge for designers and AI practitioners as early feedback for iteration is often unavailable in the development phase. In this paper, we take inspiration from integration testing concepts in software development and present HINT (Human-AI INtegration Testing), a crowd-based framework for testing AI-based experiences integrated with a humans-in-the-loop workflow. HINT supports early testing of AI-based features within the context of realistic user tasks and utilizes successive sessions to simulate AI experiences that can evolve over-time and provides practitioners with reports to evaluate and compare aspects of these experiences.\r\n\r\nThrough a crowd-based study, we demonstrate the need for over-time testing where user behaviors evolve as they interact with an AI system. We also show that HINT is able to capture and reveal these distinct user behavior patterns across a variety of common AI performance modalities using two AI-based feature prototypes. We further evaluated HINT’s potential to support practitioners' evaluation of human-AI interaction experiences pre-deployment through semi-structured interviews with 13 practitioners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 79677
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 79911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "REDMOND",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 79894
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research AI",
              "dsl": ""
            }
          ],
          "personId": 79737
        }
      ]
    },
    {
      "id": 79966,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "CiteRead: Integrating Localized Citations into Scientific Paper Reading",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511162"
        },
        "Presentation Video": {
          "duration": "596",
          "hideBeforeConference": true,
          "title": "CiteRead: Integrating Localized Citations into Scientific Paper Reading",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=WjltByXFKAM"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80028
      ],
      "eventIds": [],
      "abstract": "When reading a reference paper, scientists oftentimes wish to understand how follow-on work has built on or engages with what they are reading. However, a paper itself can only discuss prior work; meanwhile, scientific search engines can provide a list of all citing papers but they are undifferentiated and disconnected from the contents of the original, reference paper. In this work, we introduce a novel paper reading experience that integrates relevant information about follow-on work directly into a paper, allowing readers to learn about newer work and see how a paper is discussed by follow-on work in the context of the reference paper. We built a tool, called \\sys, that implements the following three contributions: 1) automated techniques for selecting important citing papers, building on results from a formative study we conducted, 2) an automated process for localization of citing papers to a place in the reference paper, and 3) an interactive experience that allows readers to seamlessly alternate between the reference paper and citation sentences, or \"citances,'' and other information from citing papers in the margins.\r\nFrom a user study with 12 scientists, we found that in comparison to having just a list of citing papers and their citances, the use of \\sys\\ while reading allows for better comprehension and retention of information about follow-on work.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 79501
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 79686
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "CSE"
            }
          ],
          "personId": 79785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "Allen Institute for Artificial Intelligence",
              "dsl": "Semantic Scholar"
            }
          ],
          "personId": 79902
        }
      ]
    },
    {
      "id": 79967,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Towards Efficient Annotations for a Human-AI Collaborative, Clinical Decision Support System: A Case Study on Physical Stroke Rehabilitation Assessment",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511112"
        },
        "Presentation Video": {
          "duration": "568",
          "hideBeforeConference": true,
          "title": "Towards Efficient Annotations for a Human-AI Collaborative, Clinical Decision Support System: A Case Study on Physical Stroke Rehabilitation Assessment",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=O6_naHOFqN0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Machine learning (ML) algorithms are increasingly being explored to support various decision-making tasks in healthcare (e.g. rehabilitation assessment). However, the development of such ML-based decision support systems is challenging due to the expensive process to collect an annotated dataset. In this paper, we describe the development process of a human-in-the-loop, decision support system that augments an ML model with a rule-based (RB) model from therapists in the context of assessing physical stroke rehabilitation exercises. We conducted the empirical evaluation with the dataset of three exercises from 15 post-stroke patients and therapists. The results demonstrate that initially when an annotated dataset is not available, the RB model can be used to assess patients' quality of motion and identify samples with low confidence scores to support efficient annotations for training an ML model. Specifically, our system requires only 33\\% of annotations from therapists to train an ML model that achieves equally good performance with an ML model with all annotations from a therapist. Our work discusses the values of a human-in-the-loop approach for supporting a complex decision-making task and effectively collecting an annotated dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79711
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79703
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79841
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "Lisbon",
              "city": "Lisbon",
              "institution": "Instituto Superior Tecnico, University of Lisbon",
              "dsl": "Institute for Systems and Robotics"
            }
          ],
          "personId": 79828
        },
        {
          "affiliations": [
            {
              "country": "Portugal",
              "state": "",
              "city": "Funchal",
              "institution": "Universidade da Madeira",
              "dsl": "Faculdade de Ciências Exatas e da Engenharia / Madeira Interactive Technologies Institute"
            }
          ],
          "personId": 79780
        }
      ]
    },
    {
      "id": 79968,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Opportunities for Human-AI Collaboration in Remote Sighted Assistance",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511113"
        },
        "Presentation Video": {
          "duration": "605",
          "hideBeforeConference": true,
          "title": "Opportunities for Human-AI Collaboration in Remote Sighted Assistance",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wIa5zvGP2BM"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Remote sighted assistance (RSA) has emerged as a conversational assistive technology for people with vision impairments, where trained sighted workers (agents) provide realtime navigational assistance to users with visual impairments via video-chat-like communication. In this paper, we first identify the key challenges that adversely affect the agent-user interaction in RSA services through a literature review and an interview study with 12 RSA users. These challenges can be partly addressed by prior work that uses computer vision (CV) technologies, especially augmented reality-based 3D map construction and realtime localization, in RSA services. We argue that addressing the full spectrum of these challenges warrants new development in Human-CV collaboration, which we formalize as five emerging problems: making object recognition and obstacle avoidance algorithms blind-aware; localizing users under poor networks; recognizing digital content on LCD screens; recognizing texts on irregular surfaces; and predicting the trajectory of out-of-frame pedestrians or objects. Addressing these problems can usher in the next generation of RSA service. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "Information Sciences and Technology"
            }
          ],
          "personId": 79660
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 79782
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 79804
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park ",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 79829
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "University Park",
              "institution": "Pennsylvania State University",
              "dsl": "College of Information Sciences and Technology"
            }
          ],
          "personId": 79900
        }
      ]
    },
    {
      "id": 79969,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "``Rather Solve the Problem from Scratch'': Gamesploring Human-Machine Collaboration for Optimizing the Debris Collection Problem",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511163"
        },
        "Presentation Video": {
          "duration": "597",
          "hideBeforeConference": true,
          "title": "``Rather Solve the Problem from Scratch'': Gamesploring Human-Machine Collaboration for Optimizing the Debris Collection Problem",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=k_ziO4kZDzY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "Optimizing operations on critical infrastructure networks is key to reducing the impact of disruptive events. In this paper, we explore the potential of having humans and algorithms work together to address this difficult task. For this purpose, we used gamified experiments to build and assess this potential in the context of the debris collection problem (i.e., ``gamesploring''). We developed a digital game where players can request the help of the computer while facing a multi-objective problem of assigning contractors to road segments for clearing debris in a disaster area.\r\nThrough a within-subjects experimental study, we assessed how players optimized under various circumstances (e.g., initial solution vs. from scratch) compared to the computer on its own. The results are both surprising as well as insightful: they suggest that human-machine collaboration is indeed beneficial but also that more work is needed on how to appropriately guide this form of collaboration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 79832
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "College of Arts, Media and Design"
            }
          ],
          "personId": 79728
        },
        {
          "affiliations": [
            {
              "country": "Denmark",
              "state": "",
              "city": "Copenhagen",
              "institution": "modl.ai",
              "dsl": ""
            }
          ],
          "personId": 79709
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": "College of Engineering"
            }
          ],
          "personId": 79718
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Northeastern University",
              "dsl": ""
            }
          ],
          "personId": 79702
        }
      ]
    },
    {
      "id": 79970,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "A Dialogue-Based Interface for Active Learning of Activities of Daily Living",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511130"
        },
        "Presentation Video": {
          "duration": "497",
          "hideBeforeConference": true,
          "title": "A Dialogue-Based Interface for Active Learning of Activities of Daily Living",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=i3D9TO1NmGY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "While Human Activity Recognition (HAR) systems may benefit from Active Learning (AL) by allowing users to self-annotate their Activities of Daily Living (ADLs), many proposed methods for collecting such annotations are for short-term data collection campaigns for specific datasets. We present a reusable dialogue-based approach to user interaction for active learning in HAR systems, which utilises a dataset of natural language descriptions of common activities (which we make publicly available) and semantic similarity measures. Our approach involves system-initiated dialogue, including follow-up questions to reduce ambiguity in user responses where appropriate. We apply our work to an existing CASAS dataset in an active learning scenario, to demonstrate our work in context, in which a natural language interface provides knowledge that can help interpret other multi-modal sensor data. We provide results highlighting the potential of our dialogue- and semantic similarity-based approach. We evaluate our work: (i) technically, as an effective way to seek users' input for active learning of ADLs; and (ii) qualitatively, through a user study in which users were asked to use our approach and an established method, and subsequently compare the two. Results show the potential of our approach as a user-friendly mechanism for annotation of sensor data as part of an active learning system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Engineering & Physical Sciences"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "University of Edinburgh",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 79757
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Edinburgh",
              "institution": "Heriot-Watt University",
              "dsl": "School of Engineering & Physical Sciences"
            }
          ],
          "personId": 79887
        }
      ]
    },
    {
      "id": 79971,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Expressive Communication: Evaluating Developments in Generative Models and Steering Interfaces for Music Creation",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511159"
        },
        "Presentation Video": {
          "duration": "619",
          "hideBeforeConference": true,
          "title": "Expressive Communication: Evaluating Developments in Generative Models and Steering Interfaces for Music Creation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=McjDb44vh5o"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "There is an increasing interest from ML and HCI communities in empowering creators with better generative models and more intuitive interfaces with which to control them.\r\nIn music, ML researchers have focused on training models capable of generating pieces with increasing long-range structure and musical coherence, while HCI researchers have separately focused on designing steering interfaces that support user control and ownership.\r\nIn this study, we investigate through a common framework how developments in both models and user interfaces are important for empowering co-creation where the goal is to create music that communicates particular imagery or ideas (e.g., as is common for other purposeful tasks in music creation like establishing mood or creating accompanying music for another media). \r\nOur study is distinguished in that it measures communication through both composer's self-reported experiences, and how listeners evaluate this communication through the music. In an evaluation study with 26 composers creating 100+ pieces of music and listeners providing 1000+ head-to-head comparisons, we find that more expressive models and more steerable interfaces are important and complementary ways to make a difference in composers communicating through music and supporting their creative empowerment.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 79884
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Brain",
              "dsl": ""
            }
          ],
          "personId": 79850
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Quebec",
              "city": "Montreal",
              "institution": "Google Brain",
              "dsl": ""
            }
          ],
          "personId": 79840
        }
      ]
    },
    {
      "id": 79972,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Better Together? An Evaluation of AI-Supported Code Translation",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511157"
        },
        "Presentation Video": {
          "duration": "559",
          "hideBeforeConference": true,
          "title": "Better Together? An Evaluation of AI-Supported Code Translation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=uMBMdzoYUKw"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 79516
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 79539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": "Visual AI Lab"
            }
          ],
          "personId": 79447
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "Buenos Aires",
              "city": "La Plata",
              "institution": "IBM Argentina",
              "dsl": ""
            }
          ],
          "personId": 79557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79390
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79466
        }
      ]
    },
    {
      "id": 79973,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning",
      "addons": {
        "Presentation Video": {
          "duration": "586",
          "hideBeforeConference": true,
          "title": "TiiS: Learn, Generate, Rank, Explain: A Case Study of Visual Explanation by Generative Machine Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_OuD1xil650"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Oshawa",
              "institution": "Ontario Tech University",
              "dsl": ""
            }
          ],
          "personId": 79805
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "SRI International",
              "dsl": ""
            }
          ],
          "personId": 79826
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Oshawa",
              "institution": "Ontario Tech University",
              "dsl": ""
            }
          ],
          "personId": 79662
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Guelph",
              "institution": "University of Guelph",
              "dsl": ""
            }
          ],
          "personId": 79869
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New Jersey",
              "city": "Princeton",
              "institution": "SRI International",
              "dsl": ""
            }
          ],
          "personId": 79727
        }
      ]
    },
    {
      "id": 79974,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embeddings",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511104"
        },
        "Presentation Video": {
          "duration": "599",
          "hideBeforeConference": true,
          "title": "Similarity-Based Explanations meet Matrix Factorization via Structure-Preserving Embeddings",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KtIEtAq05hc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "Embeddings are core components of modern model-based Collaborative Filtering (CF) methods, such as Matrix Factorization (MF) and Deep Learning variations. In essence, embeddings are mappings of the original sparse representation of categorical features (e.g., user and items) to dense low-dimensional representations. A well-known limitation of such methods is that the learned embeddings are opaque and hard to explain to the users. On the other hand, a key feature of simpler KNN-based CF models (aka user/item-based CF) is that they naturally yield similarity-based explanations, i.e., similar users/items as evidence to support model predictions. Unlike related works that try to attribute explicit meaning to the learned embeddings, in this paper, we propose to equip the learned embeddings of MF with meaningful similarity-based explanations. First, we show that the learned user/item embeddings of MF do not preserve the distances between users (or items) in the original rating data. This may prevent meaningful similarity-based explanations. Next, we propose a novel approach that initializes SGD with user/item embeddings that preserve the structural properties of the original input data. We conduct a broad set of experiments and show that our method enables explanations, very similar to the ones provided by KNN-based approaches, without harming the prediction performance. Moreover, we show that fine-tuning the structure-preserving embeddings may unlock better local minima in the optimization space, leading simple vanilla MF to reach competitive performances with the best-known models for the rating prediction task",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "PB",
              "city": "Campina Grande",
              "institution": "Federal University of Campina Grande",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79815
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "",
              "city": "Campina Grande",
              "institution": "Federal University of Campina Grande",
              "dsl": "Unidade Acadêmica de Sistemas e Computação"
            }
          ],
          "personId": 79743
        },
        {
          "affiliations": [
            {
              "country": "Chile",
              "state": "",
              "city": "Santiago",
              "institution": "Pontificia Universidad Catolica de Chile",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79801
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "MG",
              "city": "Belo Horizonte",
              "institution": "Universidade Federal de Minas Gerais",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79848
        }
      ]
    },
    {
      "id": 79975,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with GreaseTerminator",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511152"
        },
        "Presentation Video": {
          "duration": "492",
          "hideBeforeConference": true,
          "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with GreaseTerminator",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1fUBFqTSfY4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Digital harms are widespread in the mobile ecosystem. As these devices gain ever more prominence in our daily lives, so too increases the potential for malicious attacks against individuals. The last line of defense against a range of digital harms – including digital distraction, political polarisation through hate speech, and children being exposed to damaging material – is the user interface. To tackle such interface-based harms, this work introduces GreaseTerminator to develop, deploy, and test interventions against these harms with end-users. We demonstrate the ease of intervention development and deployment, as well as the broad range of harms potentially covered with GreaseTerminator in five in-depth case studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79812
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79806
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79858
        }
      ]
    },
    {
      "id": 79976,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "BeParrot: Efficient Interface for Transcribing Unclear Speech via Respeaking",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511164"
        },
        "Presentation Video": {
          "duration": "634",
          "hideBeforeConference": true,
          "title": "BeParrot: Efficient Interface for Transcribing Unclear Speech via Respeaking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EcLO-rF8ZQQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "Transcribing speech from audio files to text is an important task not only for exploring the audio content in text form but also for utilizing the transcribed data as a source to train speech models, e.g., automated speech recognition (ASR) models. A post-correction approach has been frequently employed to reduce the time cost of transcription where users edit errors in the recognition results of ASR models. However, this approach assumes clear speech and is not designed for unclear speech (e.g., speech with high levels of noise or reverberation), which severely degrades the accuracy of ASR and requires many manual corrections. To construct an alternative approach to transcribe unclear speech, we introduce the idea of respeaking, which has primarily been used to create captions for television programs in real time. In respeaking, a proficient human respeaker repeats the heard speech as shadowing, and their utterances are recognized by an ASR model. While this approach can be effective for transcribing unclear speech, one problem is that respeaking is a highly cognitively demanding task and extensive training is often required to become a respeaker. We address this point with BeParrot, the first interface designed for respeaking that allows novice users to benefit from respeaking without extensive training through two key features, i.e, parameter adjustment and pronunciation feedback. Our user study involving 60 crowd workers demonstrated that they could transcribe different types of unclear speech 32.2 % faster with BeParrot than with a conventional approach without losing the accuracy of transcriptions. In addition, comments from the workers supported the design of the adjustment and feedback features, exhibiting a willingness to continue using BeParrot for transcription tasks. Our work demonstrates how we can leverage recent advances in machine learning techniques to overcome the area that is still challenging for computers themselves with the help of a human-in-the-loop approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79903
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 79821
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 79371
        }
      ]
    },
    {
      "id": 79977,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Holistic Transfer to Rank for Top-N Recommendation",
      "addons": {
        "Presentation Video": {
          "duration": "539",
          "hideBeforeConference": true,
          "title": "TiiS: Holistic Transfer to Rank for Top-N Recommendation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GvL7VbDd_pI"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Shenzhen",
              "institution": "Shenzhen University",
              "dsl": ""
            }
          ],
          "personId": 79814
        }
      ]
    },
    {
      "id": 79978,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511111"
        },
        "Presentation Video": {
          "duration": "597",
          "hideBeforeConference": true,
          "title": "Building Trust in Interactive Machine Learning via User Contributed Interpretable Rules",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cgcCwUnAY88"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "Machine learning technologies are increasingly being applied in many different domains in the real world. As autonomous machines and black-box algorithms begin making decisions previously entrusted to humans, great academic and public interest has been spurred to provide explanations that allow users to understand the decision-making process of the machine learning model. Besides explanations, Interactive Machine Learning (IML) seeks to leverage user feedback to iterate on an ML solution to correct errors and align decisions with those of the users. Despite the rise in explainable AI (XAI) and Interactive Machine Learning (IML) research, the links between interactivity, explanations, and trust have not been comprehensively studied in the machine learning literature. Thus, in this study, we develop and evaluate an explanation-driven interactive machine learning (XIML) system with the Tic-Tac-Toe game as a use case to understand how a XIML mechanism improves users’ satisfaction with the machine learning system. We explore different modalities to support user feedback through visual or rules-based corrections. Our online user study (n = 199) supports the hypothesis that allowing interactivity within this XIML system causes participants to be more satisfied with the system, while visual explanations play a less prominent (and somewhat unexpected) role. Finally, we leverage a user-centric evaluation framework to create a comprehensive structural model to clarify how subjective system aspects, which represent participants’ perceptions of the implemented interaction and visualization mechanisms, mediate the influence of these mechanisms on the system's user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            },
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79845
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79827
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79760
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Microsoft",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 79672
        },
        {
          "affiliations": [
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            },
            {
              "country": "Ireland",
              "state": "",
              "city": "Dublin",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79680
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            },
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79725
        }
      ]
    },
    {
      "id": 79979,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "FitNibble: A Field Study to Evaluate the Utility and Usability of Automatic Diet Monitoring in Food Journaling using an Eyeglasses-based Wearable",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511154"
        },
        "Presentation Video": {
          "duration": "599",
          "hideBeforeConference": true,
          "title": "FitNibble: A Field Study to Evaluate the Utility and Usability of Automatic Diet Monitoring in Food Journaling using an Eyeglasses-based Wearable",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7OxtDQLUXhY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Automatic diet monitoring systems (ADM) research has been aiming to increase the adoption of food journaling by making it as easy as counting steps with a smartwatch. Understanding the utility and usability of ADM systems is essential to inform new designs. This has been a challenging task since many ADM systems perform poorly in real-world settings. Therefore, the main focus of ADM research has been on improving ecological validity. This paper presents a preliminary evaluation of ADM’s utility and usability using an end-to-end system, FitNibble, which provides just-in-time notifications to remind users to journal as soon as they start eating. \r\n\r\nIn this evaluation, we conducted a long-term field study to compare traditional self-report journaling and journaling with ADM. We recruited 13 participants from diverse backgrounds and asked them to try each journaling method for 9 days. Our results showed that FitNibble improved adherence by significantly reducing the number of missed events (19.6% improvement, p=.0132). Results have shown that participants were highly dependent on FitNibble in maintaining their journals. Participants also reported increased awareness of their dietary patterns especially with snacking. All these results highlight the potential of ADM in improving the food journaling experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "HCII"
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Apple",
              "dsl": "Machine Intelligence"
            }
          ],
          "personId": 79714
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 79802
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Institute of Software Research"
            }
          ],
          "personId": 79920
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79872
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "School of Computer Science"
            }
          ],
          "personId": 79747
        }
      ]
    },
    {
      "id": 79980,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Explaining Recommendations in E-Learning, Effects on Adolescents' Trust",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511140"
        },
        "Presentation Video": {
          "duration": "586",
          "hideBeforeConference": true,
          "title": "Explaining Recommendations in E-Learning, Effects on Adolescents' Trust",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=7NAbSoT84Gs"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "Recommender systems are increasingly supporting explanations to increase trust in their recommendations. However, studies on explaining recommendations typically target adults in low-risk e-commerce or media contexts, and using explanations in e-learning has received little research attention. To address these limits, we investigated how explanations affect adolescents' trust in an exercise recommender on a mathematical e-learning platform. In a randomized controlled experiment with 37 adolescents, we compared real explanations with placebo and no explanations. Our results show that explanations can significantly increase initial trust when measured as a multidimensional construct of competence, benevolence, integrity, intention to return, and perceived transparency. Yet, as not all adolescents in our study attached equal importance to explanations, it remains important to tailor them. To study the impact of tailored explanations, we advise researchers to include placebo baselines in their studies as they may give more insights into how much transparency people actually need, compared to no-explanation baselines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79730
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79705
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79374
        }
      ]
    },
    {
      "id": 79982,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Robinhood's Forest: An Idle Game to Improve Investor Behavior",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511114"
        },
        "Presentation Video": {
          "duration": "728",
          "hideBeforeConference": true,
          "title": "Robinhood's Forest: An Idle Game to Improve Investor Behavior",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=UYjG6YzMRVo"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "Smartphone-based trading apps such as Robinhood and Webull have risen in popularity over the past few years. These apps allow investors, typically with little prior investing experience, easy and inexpensive (often commission-free) access to trading stocks, options, and other securities.\r\nHowever, non-expert investors using these apps often make poor investing decisions due to behavioral factors. In particular, they 1) trade more frequently leading to short-term speculation rather than reaping the long-term benefits of their investments, 2) make decisions made on sentiments rather than market fundamentals, and 3) under-diversify their portfolio and take unnecessarily large risks. Together, these actions reduce their gains and lead to devastating yet avoidable losses. \r\nThis paper introduces Robinhood's Forest, an idle game that facilitates improved investing behavior. Unlike interactive digital games that emphasize interactivity, idle games are designed for interpassivity. Such games are based on the premise that ``waiting is playing'' and players derive pleasure by repeating simple actions or automating them. As such, our system 1) provides recurring gratification from limiting investing actions and encouraging long-term investing, 2) abstracts representations of investments to reduce overreaction to market news and social pressure, and 3) encourages diversification by using strong visual metaphors.\r\nTwo small scale lab studies demonstrate that our system reduced participants' desire for frequent trading and encouraged them to diversify their portfolios. At the same time, participants still desired functionality like performance visualizations and market news updates that allowed them to keep up with the market. We also discuss the implications of our findings for other intelligent interactive systems that emphasize non-interaction.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 79668
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79769
        }
      ]
    },
    {
      "id": 79983,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
      "addons": {
        "Presentation Video": {
          "duration": "712",
          "hideBeforeConference": true,
          "title": "TiiS: Artificial Intelligence for Modeling Complex Systems: Taming the Complexity of Expert Models to Improve Decision Making",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=R7HyWJbHIc0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Marina del Rey",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 79835
        }
      ]
    },
    {
      "id": 79984,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Humanized Recommender Systems: State-of-the-Art and Research Issues",
      "addons": {
        "Presentation Video": {
          "duration": "607",
          "hideBeforeConference": true,
          "title": "TiiS: Humanized Recommender Systems: State-of-the-Art and Research Issues",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Ow3_yc-ExfU"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": "Institute for Software Technology"
            }
          ],
          "personId": 79892
        }
      ]
    },
    {
      "id": 79985,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Colorbo: Envisioned Mandala Coloring through Human-AI Collaboration",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511135"
        },
        "Presentation Video": {
          "duration": "601",
          "hideBeforeConference": true,
          "title": "Colorbo: Envisioned Mandala Coloring through Human-AI Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=H3tUPV1SQ1I"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Mandala coloring is popular with many people, from children to adults, and also many studies have revealed its benefits in mental well-being. However, our preliminary study results reveal difficulties in mandala coloring tasks, such as selecting harmonious colors/areas and envisioning how each selection affects the final output. This paper presents Colorbo, an interactive system to support mandala coloring by envision-based human-AI collaboration. Colorbo and its user colorize a mandala individually by watching each other’s work. The user shows a colored mandala in progress, and Colorbo fills the remaining areas by analyzing the patterns and color combinations of the user image. Colorbo then projects the complete mandala onto the paper the user is colorizing, and the user continues coloring by envisioning how it can go based on Colorbo’s images. We conducted a within-subject study to understand such envision-based human-AI collaboration. Our quantitative and qualitative analysis results show the participants’ positive experiences and concerns in coloring behavior with Colorbo and their preferred projection method for the envision. Finally, based on the findings,we discuss design implications for Human-AI collaboration for art.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang Univeristy",
              "dsl": ""
            }
          ],
          "personId": 79799
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 79857
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 79817
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University ERICA",
              "dsl": ""
            }
          ],
          "personId": 79655
        }
      ]
    },
    {
      "id": 79986,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Differentiating Endogenous and Exogenous Attention Shifts Based on Fixation-Related Potentials",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511149"
        },
        "Presentation Video": {
          "duration": "584",
          "hideBeforeConference": true,
          "title": "Differentiating Endogenous and Exogenous Attention Shifts Based on Fixation-Related Potentials",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cy7AB4FocGc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Attentional shifts can occur voluntarily (endogenous control) or reflexively (exogenous control). Previous studies have shown that the neural mechanisms underlying these shifts produce different activity patterns in the brain. Changes in visual-spatial attention are usually accompanied by eye movements and a fixation on the new center of attention. In this study, we analyze the fixation-related potentials in electroencephalographic recordings of 10 participants during computer screen-based viewing tasks. During task performance, we presented salient visual distractors to evoke reflexive attention shifts. Surrounding each fixation, 0.7-second data windows were extracted and labeled as “endogenous” or “exogenous”. Averaged over all participants, the balanced classification accuracy using a person-dependent Linear Discriminant Analysis reached 59.84%. In a leave-one-participant-out approach, the average classification accuracy reached 58.48%. Differentiating attention shifts, based on fixation-related potentials, could be used to deepen the understanding of human viewing behavior or as a Brain-Computer Interface for attention-aware user interface adaptations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 79712
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 79908
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 79404
        }
      ]
    },
    {
      "id": 79987,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: AOI-shapes: supporting interactive visualization of user-defined urban areas of interest",
      "addons": {
        "Presentation Video": {
          "duration": "592",
          "hideBeforeConference": true,
          "title": "TiiS: AOI-shapes: supporting interactive visualization of user-defined urban areas of interest",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=DgxkDuzK3Yo"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Science"
            }
          ],
          "personId": 79919
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": "School of Science"
            }
          ],
          "personId": 79671
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "University of Melbourne",
              "dsl": ""
            }
          ],
          "personId": 79837
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "College Park",
              "institution": "University of Maryland",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79664
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "RMIT University",
              "dsl": ""
            }
          ],
          "personId": 79751
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Hawthorn",
              "institution": "Swinburne University of Technology",
              "dsl": "Data Science Research Institute"
            }
          ],
          "personId": 79756
        }
      ]
    },
    {
      "id": 79988,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "InSupport: Proxy Interface for Enabling Efficient Non-Visual Interaction with Web Data Records",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511126"
        },
        "Presentation Video": {
          "duration": "586",
          "hideBeforeConference": true,
          "title": "InSupport: Proxy Interface for Enabling Efficient Non-Visual Interaction with Web Data Records",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_qcFqBaq1Ho"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80020
      ],
      "eventIds": [],
      "abstract": "Interaction with web data records such as shopping products and job listings typically involves accessing auxiliary webpage segments such as filters, sort options, search form, and multi-page links. As these segments are usually scattered all across the screen, it is arduous and tedious for blind users who rely on screen readers to access the segments, given that screen readers only support sequential content access via keyboard shortcuts. The extant techniques to overcome inefficient web screen reader interaction have mostly focused on general web content navigation, and as such they provide little to no support for data record-specific interaction activities such as filtering and sorting - activities that are equally important for enabling quick and easy access to the desired data records. To fill this void, we present InSupport, a browser extension that: (i) employs custom built machine learning models to automatically extract auxiliary segments on any webpage containing data records, and (ii) provides an instantly accessible proxy one-stop interface for easily navigating the extracted segments using basic screen reader shortcuts. An evaluation study with 14 blind participants showed significant reduction in interaction time and number of key presses with InSupport, when compared with state-of-the-art solutions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Norfolk",
              "institution": "Old Dominion University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79847
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Stony Brook",
              "institution": "Stony Brook University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79856
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Norfolk",
              "institution": "Old Dominion University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Norfolk",
              "institution": "Old Dominion University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79736
        }
      ]
    },
    {
      "id": 79989,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: MI3: Machine-Initiated Intelligent Interaction for Interactive Classification and Data Reconstruction",
      "addons": {
        "Presentation Video": {
          "duration": "602",
          "hideBeforeConference": true,
          "title": "TiiS: MI3: Machine-Initiated Intelligent Interaction for Interactive Classification and Data Reconstruction",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ihG8ZSnNS28"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79765
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Dept. Computer Science"
            }
          ],
          "personId": 79852
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Engineering Science"
            }
          ],
          "personId": 79783
        }
      ]
    },
    {
      "id": 79990,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: PRIME: A Personalized Recommender System for Information Visualization Methods via Extended Matrix Completion",
      "addons": {
        "Presentation Video": {
          "duration": "497",
          "hideBeforeConference": true,
          "title": "TiiS: PRIME: A Personalized Recommender System for Information Visualization Methods via Extended Matrix Completion",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JE780OUvWKA"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80021
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Kentucky",
              "city": "Louisville",
              "institution": "University of Louisville",
              "dsl": "Department of Industrial Engineering"
            }
          ],
          "personId": 79910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Grado Department of Industrial and Systems Engineering"
            }
          ],
          "personId": 79915
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Grado Department of Industrial and Systems Engineering"
            }
          ],
          "personId": 79790
        }
      ]
    },
    {
      "id": 79991,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Wordcraft: Story Writing With Large Language Models",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511105"
        },
        "Presentation Video": {
          "duration": "591",
          "hideBeforeConference": true,
          "title": "Wordcraft: Story Writing With Large Language Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Bl373hjWmKQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "The latest generation of large language models such as GPT-3 have achieved new levels of performance on benchmarks for language understanding and generation. These models have even demonstrated an ability to perform arbitrary tasks without explicit training. In this work we sought to learn how people might use such models in the process of creative writing. We built Wordcraft, a text editor in which users collaborate with a large language model to write a story. We evaluate Wordcraft with a user study in which participants wrote short stories with and without the tool. Our results show that large language models enable novel co-writing experiences, such as engaging in open-ended conversation with writers about their story, responding to writers' custom requests expressed in natural language (such as \"rewrite this text to be more Dickensian\"), and generating suggestions that, while imperfect, serve to unblock writers in the creative process. Based on these results, we discuss design implications for future human-AI co-writing systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Brain",
              "dsl": ""
            }
          ],
          "personId": 79698
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google",
              "dsl": ""
            }
          ],
          "personId": 79836
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 79719
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Mountain View",
              "institution": "Google Research",
              "dsl": ""
            }
          ],
          "personId": 79732
        }
      ]
    },
    {
      "id": 79992,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: A Real-time Interactive Visualizer for Large Classroom",
      "addons": {
        "Presentation Video": {
          "duration": "480",
          "hideBeforeConference": true,
          "title": "TiiS: A Real-time Interactive Visualizer for Large Classroom",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=CdNDAwTV_ic"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80027
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Assam",
              "city": "Guwahati",
              "institution": "Indian Institute of Technology Guwahati",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 79909
        }
      ]
    },
    {
      "id": 79993,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511108"
        },
        "Presentation Video": {
          "duration": "600",
          "hideBeforeConference": true,
          "title": "Do Humans Prefer Debiased AI Algorithms? A Case Study in Career Recommendation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=RZN5MXgxxe4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "Currently, there is a surge of interest in fair Artificial Intelligence (AI) and Machine Learning (ML) research which aims to mitigate discriminatory bias in AI algorithms, e.g. along lines of gender, age, and race. While most research in this domain focuses on developing fair AI algorithms, in this work, we examine the challenges which arise when  human- fair-AI interact.  Our results show that due to an apparent conflict between human preferences and fairness, a fair AI algorithm on its own may be insufficient to achieve its intended results in the real world. Using college major recommendation as a case study, we build a fair AI  recommender by employing  gender debiasing machine learning techniques.  Our offline evaluation showed that the debiased recommender makes fairer and more accurate college major recommendations.  Nevertheless, an online user study of more than 200 college students revealed that participants on average prefer the original biased system over the debiased system. Specifically, we found that the perceived gender disparity associated with a college major is a determining factor for the acceptance of a recommendation. In other words, our results demonstrate we cannot fully address the gender bias issue in AI recommendations without addressing the gender bias in humans. They also highlight the urgent need to extend the current scope of fair AI research from narrowly focusing on debiasing  AI algorithms to including new persuasion and bias explanation technologies in order to achieve intended societal impacts.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "The Harker School",
              "dsl": ""
            }
          ],
          "personId": 79689
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Clarksville",
              "institution": "River Hill High School",
              "dsl": ""
            }
          ],
          "personId": 79669
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore County",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 79885
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore County",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 79824
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore County",
              "institution": "University of Maryland",
              "dsl": ""
            }
          ],
          "personId": 79791
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Maryland",
              "city": "Baltimore",
              "institution": "University of Maryland, Baltimore County",
              "dsl": ""
            }
          ],
          "personId": 79721
        }
      ]
    },
    {
      "id": 79994,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: Theoretical, Measured and Subjective Responsibility in Aided Decision Making",
      "addons": {
        "Presentation Video": {
          "duration": "615",
          "hideBeforeConference": true,
          "title": "TiiS: Theoretical, Measured and Subjective Responsibility in Aided Decision Making",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KkquMgJzlk4"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80026
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Tel Aviv",
              "institution": "Tel Aviv University",
              "dsl": "Industrial Engineering"
            }
          ],
          "personId": 79724
        }
      ]
    },
    {
      "id": 79995,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Robust and Deployable Gesture Recognition for Smartwatches",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511125"
        },
        "Presentation Video": {
          "duration": "382",
          "hideBeforeConference": true,
          "title": "Robust and Deployable Gesture Recognition for Smartwatches",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Vfapab9bYe0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "abstract": "Gesture recognition on smartwatches is challenging not only due to resource constraints but the dynamically changing conditions of users. It is currently an open problem how to engineer gesture recognizers that are robust and yet deployable on smartwatches.Recent research has found that common everyday events, such as a user removing and reattaching smartwatch strap, can deteriorate recognition accuracy significantly. In this paper we suggest that prior understanding on causes behind everyday variability and false positives should be exploited in the development of recognizers. To this end, first, we present a data collection method that aims at diversifying gesture data in a representative way, in which users are taken to experimental conditions that resemble known causes of variability (e.g., walking while gesturing) and asked to produce deliberately varied, but realistic gestures. Second, we review known approaches in machine learning for recognizer design on constrained hardware. We propose convolution-based network variations for classifying raw sensor data, achieving greater than 98%accuracy reliably under both individual and situational variations where previous approaches have reported significant performance deterioration. This performance is achieved with a model that is two orders of magnitude less complex than previous state-of-the-art models. Our work suggests that deployable and robust recognition is feasible but requires systematic efforts in data collection and network design to address known causes of gesture variability.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 79873
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "School of Science"
            }
          ],
          "personId": 79870
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Munich",
              "institution": "Huawei Technologies",
              "dsl": "Munich Research Center"
            },
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Augsburg",
              "institution": "University of Augsburg",
              "dsl": "Chair of Embedded Intelligence for Health Care and Wellbeing"
            }
          ],
          "personId": 79778
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 79879
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "Huawei Technologies",
              "dsl": "Device Software Lab, Munich Research Center"
            }
          ],
          "personId": 79897
        },
        {
          "affiliations": [
            {
              "country": "Luxembourg",
              "state": "",
              "city": "Esch-sur-Alzette",
              "institution": "University of Luxembourg",
              "dsl": ""
            }
          ],
          "personId": 79875
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 79537
        }
      ]
    },
    {
      "id": 79996,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Multimodal Error Correction for Speech-to-Text in a Mobile Office Automated Vehicle: Results From a Remote Study",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511131"
        },
        "Presentation Video": {
          "duration": "605",
          "hideBeforeConference": true,
          "title": "Multimodal Error Correction for Speech-to-Text in a Mobile Office Automated Vehicle: Results From a Remote Study",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=mmKnNFZGQWk"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80025
      ],
      "eventIds": [],
      "abstract": "Future users of automated vehicles will demand the ability to perform diverse and extensive non-driving related tasks. However, prevailing restrictions in the car require new interaction concepts to enable productive office work. Intelligent voice-based interfaces may be a solution to facilitate productivity while at the same time keeping the ``driver in the loop'' and thereby maintaining safety. In this work, we investigated the repair problem of productive speech-to-text input in a highly automated vehicle. We examined the user experience of selecting/navigating to an incorrectly recognized word using only speech, pointing and clicking on a touchpad, and using mid-air hand gestures.\r\nResults indicate that hand gestures (condition VaG) have high hedonic quality but are not considered viable for error correction in productive text input. On the other hand, the unimodal (Voice-only baseline) and touchpad-based point-and-click (VaT) approaches to error correction were rated equally well in the hypothesized ``mobile office'' automated vehicle. The utilized remote study execution methodology proved to be a useful intermediary tool between pure online surveys and on-site studies for qualitative research during a pandemic but suffered from a lack of fidelity and options for objective usability and safety evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Linz",
              "institution": "Johannes Kepler University",
              "dsl": ""
            }
          ],
          "personId": 79779
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Bavaria",
              "city": "Ingolstadt",
              "institution": "Technische Hochschule Ingolstadt",
              "dsl": "Human-Computer Interaction Group"
            }
          ],
          "personId": 79807
        }
      ]
    },
    {
      "id": 79997,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: BONNIE: Building Online Narratives from Noteworthy Interaction Events",
      "addons": {
        "Presentation Video": {
          "duration": "593",
          "hideBeforeConference": true,
          "title": "TiiS: BONNIE: Building Online Narratives from Noteworthy Interaction Events",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rSmNc-uT9MY"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80030
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "RJ",
              "city": "Rio de Janeiro",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79882
        },
        {
          "affiliations": [
            {
              "country": "Brazil",
              "state": "RJ",
              "city": "Rio de Janeiro",
              "institution": "PUC-Rio",
              "dsl": "Department of Informatics"
            }
          ],
          "personId": 80127
        }
      ]
    },
    {
      "id": 79998,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "ODEN: Live Programming for Neural Network Architecture Editing",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511120"
        },
        "Presentation Video": {
          "duration": "632",
          "hideBeforeConference": true,
          "title": "ODEN: Live Programming for Neural Network Architecture Editing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=JQNZ1tPtfy0"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80024
      ],
      "eventIds": [],
      "abstract": "In deep learning application development, programmers tend to be trying different architectures and hyper-parameters until satisfied with the model performance.\r\nAlthough programmers may want to smoothly go back and forth between neural network(NN) architecture editing and experimentation, program crashes due to tensor shape mismatch and other issues prohibit them, especially novice programmers, from doing so.\r\nWe propose to leverage live programming techniques in NN architecture editing to show an always-on visualization.\r\nWhen the user edits the program, the visualization can synchronously display tensor states and provide a warning message by continuously executing the program to prevent program crashes during experimentation.\r\nWe implement the live visualization and integrate it into an IDE called ODEN that seamlessly supports the “edit→experiment→edit→···” repetition. \r\nWith ODEN, the user can construct the neural network with the live visualization and transits into experimentation to instantly train and test the NN architecture.\r\nAn exploratory user study is conducted to evaluate the usability, the limitations, and the potential of live visualization in ODEN.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79717
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79912
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79855
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 79808
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79479
        }
      ]
    },
    {
      "id": 79999,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "Investigating Explainability of Generative Models for Code through Scenario-based Design",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511119"
        },
        "Presentation Video": {
          "duration": "511",
          "hideBeforeConference": true,
          "title": "Investigating Explainability of Generative Models for Code through Scenario-based Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=QYHOoZcnoSE"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80022
      ],
      "eventIds": [],
      "abstract": "What does it mean for a generative AI model to be explainable? The emergent discipline of explainable AI (XAI) has made great strides in helping people understand discriminative models. Less attention has been paid to generative models that produce artifacts, rather than decisions, as output. Meanwhile, generative AI (GenAI) technologies are maturing and being applied to application domains such as software engineering. Using a scenario-based design approach, we explore users' explainability needs for GenAI in three software engineering use cases: natural language to code, code translation, and code auto-completion. We conducted 9 workshops with 43 software engineers in which real examples from state-of-the-art generative AI models were used to elicit explainability needs. Drawing from prior work, we also propose 4 types of XAI features for GenAI for code and gathered additional design ideas from participants. Our work begins to identify explainability needs for GenAI for code and demonstrates how human-centered approaches can drive the technical development of XAI in novel domains. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of Southern California",
              "dsl": ""
            }
          ],
          "personId": 79734
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79490
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 79539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79390
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 79516
        }
      ]
    },
    {
      "id": 80000,
      "typeId": 12046,
      "durationOverride": 12,
      "title": "TiiS: After-Action Review for AI (AAR/AI)",
      "addons": {
        "Presentation Video": {
          "duration": "557",
          "hideBeforeConference": true,
          "title": "TiiS: After-Action Review for AI (AAR/AI)",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=wJiU1t4Emtc"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80029
      ],
      "eventIds": [],
      "abstract": "",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State Un",
              "dsl": "EECS"
            }
          ],
          "personId": 79876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79819
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79922
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "CORVALLIS",
              "institution": "Oregon State University ",
              "dsl": ""
            }
          ],
          "personId": 79914
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79692
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79834
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79713
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79658
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79663
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of Electrical Engineering and Computer Science"
            }
          ],
          "personId": 79693
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79888
        }
      ]
    },
    {
      "id": 80230,
      "typeId": 12089,
      "title": "SummaryLens – A Smartphone App for Exploring Interactive Use of Automated Text Summarization in Everyday Life",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516471"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp3626.pdf"
        },
        "Presentation Video": {
          "duration": "73",
          "hideBeforeConference": true,
          "title": "SummaryLens – A Smartphone App for Exploring Interactive Use of Automated Text Summarization in Everyday Life",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=bWlD1nDzLa8"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "We present SummaryLens, a concept and prototype for a mobile tool that leverages automated text summarization to enables users to quickly scan and summarize physical text documents. We further combine this with a text-to-speech system to read out the summary on demand. With this concept, we propose and explore a concrete application case of bringing ongoing progress in AI and Natural Language Processing to a broad audience with interactive use cases in everyday life. Based on our implemented features, we describe a set of potential usage scenarios and benefits, including support for low-vision, low-literate and dyslexic users. A first usability study shows that the interactive use of automated text summarization in everyday life has noteworthy potential. We make the prototype available as an open-source project to facilitate further research on such tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 80132
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Research Group HCI + AI, Department of Computer Science"
            }
          ],
          "personId": 80225
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "HCI+AI"
            }
          ],
          "personId": 80182
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79513
        }
      ]
    },
    {
      "id": 80232,
      "typeId": 12089,
      "title": "Vietnamese Speech-based Question Answering over Car Manuals",
      "addons": {},
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "This paper presents a novel Vietnamese speech-based question answering system QA-CarManual that enables users to ask car-manual-related questions (e.g. how to properly operate devices and/or utilities within a car). Given a car manual written in Vietnamese as the main knowledge base, we develop QA-CarManual as a lightweight, real-time and interactive system that integrates state-of-the-art technologies in language and speech processing to (i) understand and interact with  users via speech commands and (ii) automatically query the knowledge base and return  answers in both forms of text and speech as well as visualization. To our best knowledge, QA-CarManual is the first Vietnamese question answering system that interacts with users via speech inputs and outputs. We perform a human evaluation to assess the quality of our QA-CarManual system and obtain promising results.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80191
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80158
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80213
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80149
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80218
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80137
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "Vinai Research",
              "dsl": ""
            }
          ],
          "personId": 80193
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80192
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80185
        },
        {
          "affiliations": [
            {
              "country": "Viet Nam",
              "state": "",
              "city": "Hanoi",
              "institution": "VinAI Research",
              "dsl": ""
            }
          ],
          "personId": 80207
        }
      ]
    },
    {
      "id": 80233,
      "typeId": 12089,
      "title": "The Diversity of Music Recommender Systems",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516474"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp4710.pdf"
        },
        "Presentation Video": {
          "duration": "143",
          "hideBeforeConference": true,
          "title": "The Diversity of Music Recommender Systems",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4W183NX5wqY"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "While the algorithms used by music streaming services to provide recommendations have often been studied in offline, isolated settings, little research has been conducted studying the nature of their recommendations within the full context of the system itself. This work seeks to compare the level of diversity of the real-world recommendations provided by five of the most popular music streaming services, given the same lists of low-, medium- and high-diversity input items. We contextualized our results by examining the reviews for each of the five services on the Google Play Store, focusing on users' perception of their recommender systems and the diversity of their output. We found that YouTube Music offered the most diverse recommendations, but the perception of the recommenders was similar across the five services. Consumers had multiple perspectives on the recommendations provided by their music service—ranging from not wanting any recommendations to applauding the algorithm for helping them find new music.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Daniel High School",
              "dsl": ""
            }
          ],
          "personId": 80206
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Evanston",
              "institution": "Northwestern University",
              "dsl": ""
            }
          ],
          "personId": 80222
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "HCC/School of Computing"
            }
          ],
          "personId": 80228
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "South Carolina",
              "city": "Clemson",
              "institution": "Clemson University",
              "dsl": "School of Computing"
            }
          ],
          "personId": 79725
        }
      ]
    },
    {
      "id": 80234,
      "typeId": 12089,
      "title": "ImCasting: Nonverbal Behaviour Reinforcement Learning of Virtual Humans through Adaptive Immersive Game",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516475"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp4112.pdf"
        },
        "Presentation Video": {
          "duration": "93",
          "hideBeforeConference": true,
          "title": "ImCasting: Nonverbal Behaviour Reinforcement Learning of Virtual Humans through Adaptive Immersive Game",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=0vHvPjzamaM"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "This research work puts a focus on the user experience of an alternative method to teach nonverbal behaviour to Embodied Conversational Agents in immersive environments. We overcome the limitations of the existing approaches by proposing an adaptive Virtual Reality game, called ImCasting, in which the player takes an active role in improving the learning models of the agents. Specifically, we based our approach on the Human-in-the-loop framework with human preferences to teach the nonverbal behaviour to the agents through the system. We introduce game mechanisms built around all the tasks of this Machine Learning framework, designing how a human should interact within this framework in real-time. The study explores how a game interaction in an immersive environment can improve the user experience in performing this interactive task, sharing the same space with the learning agents. In particular, we focus on the involvement of the players as well as the usability of the system. We conducted a preliminary evaluation to compare the design of our system with a baseline system which does not use any game mechanisms in teaching the nonverbal behaviour to virtual agents. Results suggest that our design concept and the game story are more engaging, increasing the satisfaction usability factor perceived by the participants.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 80156
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 80154
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 80181
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Orsay",
              "institution": "Université Paris-Saclay, CNRS, LISN",
              "dsl": ""
            }
          ],
          "personId": 80130
        }
      ]
    },
    {
      "id": 80235,
      "typeId": 12089,
      "title": "Using Wearables Data for Differentiating Between Injured and Non-Injured Athletes ",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516465"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp4573.pdf"
        },
        "Presentation Video": {
          "duration": "245",
          "hideBeforeConference": true,
          "title": "Using Wearables Data for Differentiating Between Injured and Non-Injured Athletes",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=XEWtxZfX-to"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Smartwatches nowadays are rich sensors that may provide their wearers with various data about their physical performance and physiological status. In this paper, we explore the possibility of using this data for identifying differences between athletes during a training program. We aim to distinguish between those who suffered musculoskeletal injuries to athletes that were not injured, by considering the external load and athletes’ heart rate. By Comparing the two groups, we found significant differences between the groups in the following features: Heart rate at rest and during sleep. In addition, percent time of REM and deep sleep were significantly different between the two groups and the external load expressed by distance was significantly lower in the injured group. Our findings suggest that by tracing heart rate and sleep quality during a training program, we were able to characterize athletes that were at risk of injuries. This may be a first step for further analysis we plan to explore the possibility to predict the risk of injuries to adapt the training loads accordingly to prevent injuries. In addition, upon such characteristics, user profiles can be built and used for personalized recommendations for avoiding injuries might during training may be provided.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "University of Haifa",
              "dsl": "Information Systems"
            }
          ],
          "personId": 80136
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "University of Haifa",
              "dsl": "Physical Therapy"
            }
          ],
          "personId": 80212
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "University of Haifa",
              "dsl": "Information Systems"
            }
          ],
          "personId": 80195
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "University of Haifa",
              "dsl": "Physical Therapy"
            }
          ],
          "personId": 80210
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "University of Haifa",
              "dsl": "Information Systems"
            }
          ],
          "personId": 80135
        }
      ]
    },
    {
      "id": 80236,
      "typeId": 12089,
      "title": "Neural Language Models as What If? -Engines for HCI Research",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516458"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp8162.pdf"
        },
        "Presentation Video": {
          "duration": "303",
          "hideBeforeConference": true,
          "title": "Neural Language Models as What If? -Engines for HCI Research",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=w7awPw2hJ74"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Collecting data is one of the bottlenecks of Human-Computer Interaction (HCI) and user experience (UX) research. In this short paper, we explore and critically evaluate the potential of large-scale neural language models like GPT-3 in generating synthetic research data such as participant responses to interview questions. We observe that in the best case, GPT-3 can create plausible reflections of video game experiences and emotions, and adapt its responses to given demographic information. Compared to real participants, such synthetic data can be obtained faster and at a lower cost. On the other hand, the quality of generated data has high variance, and future work is needed to rigorously quantify the human-likeness, limitations, and biases of the models in the HCI domain.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 80141
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 80157
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": ""
            }
          ],
          "personId": 80134
        }
      ]
    },
    {
      "id": 80237,
      "typeId": 12089,
      "title": "Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516463"
        },
        "Presentation Video": {
          "duration": "224",
          "hideBeforeConference": true,
          "title": "Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=8IyZh5jkQj8"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Distraction detection systems in automotive has great importance due to the prime safety of passengers. Earlier approaches confined\r\nto use indirect methods of driving performance metrics to detect visual distraction. Recent methods attempted to develop dedicated\r\nclassification models for gaze zone estimation whose cross-domain performance was not investigated. We adopt a more generic\r\nappearance-based gaze estimation approach where no assumption on setting or participant was made. We proposed MAGE-Net with\r\nless number of parameters while achieving on par performance with state of the art techniques on MPIIGaze dataset. We utilized the\r\nproposed MAGE-Net and performed a cross-domain evaluation in automotive setting with 10 participants. We observed that the gaze\r\nregion error using MAGE-Net for interior regions of car is 15.61 cm and 15.13 cm in x and y directions respectively. We utilized these\r\nresults and demonstrated the capability of proposed system to detect visual distraction using a driving simulator.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "I3D Lab, Centre for Product Design and Manufacturing"
            }
          ],
          "personId": 80139
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            },
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kalyani",
              "institution": "Indian Institute of Information Technology",
              "dsl": ""
            }
          ],
          "personId": 80221
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 80152
        }
      ]
    },
    {
      "id": 80238,
      "typeId": 12089,
      "title": "Touch-behavioral Authentication on Smartphones using Machine Learning",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516456"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp3290.pdf"
        },
        "Presentation Video": {
          "duration": "224",
          "hideBeforeConference": true,
          "title": "Touch-behavioral Authentication on Smartphones using Machine Learning",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1ZlMGjc1Ss4"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "The traditional authentication approaches for smartphones, such as PIN code or pattern-based password, are vulnerable of password hacking in public by shoulder-suffering or smudge attack. On the other side, advanced authentication approaches, such as fingerprints or retina-based recognition, required specific hardware or high computational power. In this work, we propose using the advancements in machine learning (ML) for providing the authentication mechanism without the requirement of any additional hardware. We propose the usage of users’ touch interaction behavior on smartphone screen to provide the required authentication mechanism. We propose the solution in two modes, i.e., using the supervised ML technique where the system is trained and then authorized the legitimate user using a set of simple shapes, and using the unsupervised ML technique where the system is trained on a user’s free touch interaction with the underlying device. Moreover, we conducted a preliminary user study with our supervised learning based system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Francisco",
              "institution": "San Francisco State University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 80143
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Rhineland-Palatinate",
              "city": "Kaiserslautern",
              "institution": "University of Kaiserslautern",
              "dsl": "Computer Graphics and HCI Group"
            }
          ],
          "personId": 80160
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "HILLSBORO",
              "institution": "INTEL Corporation",
              "dsl": "Intel Labs"
            }
          ],
          "personId": 80171
        }
      ]
    },
    {
      "id": 80239,
      "typeId": 12089,
      "title": "PARKS-Gaze - A Precision-focused Gaze Estimation Dataset in the Wild under Extreme Head Poses",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516467"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp9288.pdf"
        },
        "Presentation Video": {
          "duration": "156",
          "hideBeforeConference": true,
          "title": "PARKS-Gaze - A Precision-focused Gaze Estimation Dataset in the Wild under Extreme Head Poses",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Jr2OdzvM-KY"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Appearance-based gaze estimation systems adopt learning-based approach and hence their performance depend on their training\r\ndatasets. Most of the existing gaze estimation datasets were recorded in laboratory conditions. Further, datasets recorded in the wild\r\nconditions display limited head pose and intra-person variation. We proposed PARKS-Gaze, a large gaze estimation dataset with\r\n570 minutes of video data from 18 participants. We captured head pose range of ±50, [-40,60] degrees in yaw and pitch directions\r\nrespectively. We also captured multiple images for a single Point of Gaze enabling to carry out precision analysis of gaze estimation\r\nmodels. Our cross-dataset experiments revealed that the model trained on proposed dataset obtained lower test errors when compared\r\nto EYEDIAP, RT-GENE and MPIIGaze datasets, indicating its utility for developing real-world interactive gaze controlled applications",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bengaluru",
              "institution": "Indian Institute of Science",
              "dsl": "I3D Lab, Centre for Product Design and Manufacturing"
            }
          ],
          "personId": 80139
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": ""
            },
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kalyani",
              "institution": "Indian Institute of Information Technology",
              "dsl": ""
            }
          ],
          "personId": 80221
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "M. S. Ramaiah Institute of Technology",
              "dsl": "Dept. of Electronics and Instrumentation Engineering"
            }
          ],
          "personId": 80197
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Gujarat",
              "city": "Vallabh Vidyanagar",
              "institution": "G H Patel College of Engineering & Technology",
              "dsl": "Information Technology"
            }
          ],
          "personId": 80204
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Kanataka",
              "city": "Bangalore",
              "institution": "Indian Institute of Science",
              "dsl": "CPDM"
            }
          ],
          "personId": 80152
        }
      ]
    },
    {
      "id": 80240,
      "typeId": 12089,
      "title": "Leveraging Generative Conversational AI to Develop a Creative Learning Environment for Computational Thinking",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516473"
        },
        "Presentation Video": {
          "duration": "222",
          "hideBeforeConference": true,
          "title": "Leveraging Generative Conversational AI to Develop a Creative Learning Environment for Computational Thinking",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=-R4YQY7sQ4U"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "We explore how generative conversational AI can assist students' learning, creative, and sensemaking process in a visual programming environment where users can create comics from code. The process of visualizing code in terms of comics involves mapping programming language (code) to natural language (story) and then to visual language (of comics). While this process requires users to brainstorm code examples, metaphors, and story ideas, the recent development in generative models introduces an exciting opportunity for learners to harness their creative superpower and researchers to advance our understanding of how generative conversational agents can augment our intelligence in creative learning contexts. We provide an overview of the system and discuss multiple interaction scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "Computer Science"
            }
          ],
          "personId": 80166
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Ontario",
              "city": "Waterloo",
              "institution": "University of Waterloo",
              "dsl": "David R. Cheriton School of Computer Science"
            }
          ],
          "personId": 80202
        }
      ]
    },
    {
      "id": 80241,
      "typeId": 12089,
      "title": "Handwriting Messenger by Which the User Can Feel the Presence of Communication Partners",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516454"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp6013.pdf"
        },
        "Presentation Video": {
          "duration": "180",
          "hideBeforeConference": true,
          "title": "Handwriting Messenger by Which the User Can Feel the Presence of Communication Partners",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=1VdYmq3ilYY"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Communication technologies such as instant messaging and e-mail allow for a convenient, instantaneous communication between remote locations. However, since these technologies generally use fixed fonts, text messages are less expressive than those found in handwritten letters. To address this problem, a smartphone application and a pen plotter are proposed to incorporate expressive handwriting in communication. We hypothesize that the social presence of the communicating party can be better felt by the recipient by adding tactile information to the message in addition to the visual information discussed in previous studies. We introduce a haptic interface in which the message recipient can feel the handwriting and presence of the sender.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 80162
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "University of Tsukuba",
              "dsl": ""
            }
          ],
          "personId": 80163
        }
      ]
    },
    {
      "id": 80242,
      "typeId": 12089,
      "title": "GazeSync: Eye Movement Transfer Using an Optical Eye Tracker and Monochrome Liquid Crystal Displays",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516469"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp6356.pdf"
        },
        "Presentation Video": {
          "duration": "38",
          "hideBeforeConference": true,
          "title": "GazeSync: Eye Movement Transfer Using an Optical Eye Tracker and Monochrome Liquid Crystal Displays",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=cm3xJKFLm4Q"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Can we see the world through the eyes of somebody else? We present an early work to transfer eye gaze from one person to another. Imagine you can follow the eye gaze of an instructor while explaining a complex work step, or you can experience a painting like an expert would: Your gaze is directed to the important parts. In this work, we explore the possibility to transmit eye-gaze information in a subtle, unobtrusive fashion between two individuals. We present an early prototype consisting of an optical eye-tracker for the leader (person who shares the eye gaze) and two monochrome see-through displays for the follower (person who follows the eye gaze of the leader). We report the results of an initial user test and discuss future works. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Media Design"
            }
          ],
          "personId": 80145
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 80198
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 80151
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Graduate School of Media Design"
            }
          ],
          "personId": 80224
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University Graduate School of Media Design",
              "dsl": ""
            }
          ],
          "personId": 80150
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Keio University",
              "dsl": "KMD"
            }
          ],
          "personId": 80176
        }
      ]
    },
    {
      "id": 80243,
      "typeId": 12089,
      "title": "Videos2Doc: Generating Documents from a Collection of Procedural Videos",
      "addons": {},
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "The availability of user-generated multi-modal content, including videos and images, in abundance makes it easy for users to use it as a reference and source of information. However, several hours may be required for the consumption of this large corpus of data. Particularly, for authors and content creators abstracting out information from videos to then representing it in a textual format is a tedious task. The challenges are multiplied due to the diversity and the variety introduced when there are several videos associated with a given query or topic of interest. We present, Videos2Doc, a machine learning-based framework for automated document generation from a collection of procedural videos. Videos2Doc enables author-guided document generation for those looking for authoring assistance and an easy consumption experience for those preferring the text or document media over videos. Our proposed interface allows the users to choose several visual and semantic preferences for the output document allowing the generation of custom documents and webpage templates from a given set of inputs. Empirical and qualitative evaluations establish the utility of this work as well as the superiority over the current benchmarks. We believe, Videos2Doc will ease the task of making multimedia accessible through automation in conversion to alternate presentation modes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Adobe Systems",
              "dsl": "Adobe Research"
            }
          ],
          "personId": 80172
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kolkata",
              "institution": "Institute of Engineering and Management ",
              "dsl": "Computer Science and Engineering "
            }
          ],
          "personId": 80205
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 80155
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": ""
            }
          ],
          "personId": 80142
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 80217
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 80178
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 80190
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Adobe Systems",
              "dsl": "Adobe Research, India"
            }
          ],
          "personId": 80147
        }
      ]
    },
    {
      "id": 80244,
      "typeId": 12089,
      "title": "BareTQL: An Interactive System for Searching and Extraction of Open Data Tables",
      "addons": {},
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "There has been a plethora of research and commercial activities around extracting structured data from documents (e.g. web pages and scientific articles) and making them available to other applications. Many organizations and government bodies have been also making their data available to public. Despite the progress in many different aspects of table extraction and publishing, querying incomplete data in tables with little or no schema has been a challenge. This paper presents BareTQL, an interactive system for querying open data tables in the presence of the aforementioned challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": ""
            }
          ],
          "personId": 80175
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": ""
            }
          ],
          "personId": 80226
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": ""
            }
          ],
          "personId": 80220
        },
        {
          "affiliations": [
            {
              "country": "Canada",
              "state": "Alberta",
              "city": "Edmonton",
              "institution": "University of Alberta",
              "dsl": ""
            }
          ],
          "personId": 80167
        }
      ]
    },
    {
      "id": 80245,
      "typeId": 12089,
      "title": "Enabling Continuous Object Recognition in Mobile Augmented Reality",
      "addons": {},
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Mobile Augmented Reality (MAR) applications enable users to interact with physical environments through overlaying digital information on top of camera views. Detecting and classifying complex objects in the real world presents a critical challenge to enable immersive user experiences in MAR applications. Aiming to provide continuous MAR experiences, we address a key challenge of continuous object recognition, which requires accommodating an increasing number of recognition requests on different types of images in MAR systems and possible new types of images in emerging applications. Inspired by the latest advance in continual learning approaches in computer vision, this paper presents a novel MAR system to enhance its scalability with continual learning in realistic scenarios. Our experiments demonstrate that 1) the system enables efficiently recognising objects without requiring retraining from scratch; and 2) edge computing further reduces latency for continual object recognition.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Norway",
              "state": "",
              "city": "Gjøvik",
              "institution": "Norwegian University of Science and Technology",
              "dsl": "Department of Computer Science"
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Oulu",
              "institution": "University of Oulu",
              "dsl": "Center for Ubiquitous Computing"
            }
          ],
          "personId": 80129
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "Fife",
              "city": "St Andrews",
              "institution": "University of St. Andrews",
              "dsl": "Computer Science"
            }
          ],
          "personId": 80133
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "OULU",
              "institution": "University of Oulu",
              "dsl": ""
            }
          ],
          "personId": 80131
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong SAR",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 80187
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong University of Science and Technology",
              "dsl": ""
            },
            {
              "country": "Finland",
              "state": "Uusimaa",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": ""
            }
          ],
          "personId": 80196
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "St Andrews",
              "institution": "University of St Andrews",
              "dsl": ""
            }
          ],
          "personId": 80214
        }
      ]
    },
    {
      "id": 80246,
      "typeId": 12089,
      "title": "Inequity in Popular Speech Recognition Systems for Accented English Speech",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516457"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp4745.pdf"
        },
        "Presentation Video": {
          "duration": "168",
          "hideBeforeConference": true,
          "title": "Inequity in Popular Speech Recognition Systems for Accented English Speech",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4O2jFzH73i8"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Voice-enabled technology has become increasingly common in homes, businesses, and other parts of everyday life. The benefits of smart speakers, hands-free controllers, and digital assistants should be equally accessible to everyone, yet voice recognition performance can be frustratingly low for speakers with accents. In this work, we examine algorithmic bias in several voice recognition systems by measuring standard accuracy metrics on under-represented English dialects.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Department of Computer Science and Engineering"
            }
          ],
          "personId": 80215
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab, Department of Computer Science and Engineering"
            }
          ],
          "personId": 79583
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 79333
        }
      ]
    },
    {
      "id": 80247,
      "typeId": 12089,
      "title": "GAB - Gestures for Artworks Browsing",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516470"
        },
        "Presentation Video": {
          "duration": "68",
          "hideBeforeConference": true,
          "title": "GAB - Gestures for Artworks Browsing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=3MNKCkpqQS4"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Hands are an important tool for our daily communication with our peers and the world. They allow us to convey information through particular gestures that are either the product of social conventions or personal expressions. Thanks to the sophistication of sensing and computer vision technologies over the past decade, automated hands recognition can now be more easily used and integrated in simple web applications. In a context of digital artworks collections, it means that gestures can now be envisioned as a new browsing tool that goes beyond simple movements to navigate through a 3D digital space. The paper presents Gestures for Artwork Browsing(GAB), a web application which proposes to use hand motions as a way to directly query pictorial hand gestures from the past. Based on materials from a digitized collection of Renaissance paintings, GAB enables users to record a sequence with the movement of their choice and outputs an animation reproducing that same movement through painted hands. Fostering new research possibilities, the project is a novelty in terms of art database browsing and human-computer interaction, as it does not require a traditional research through text-based inputs based on metadata, and allows a direct communication with the content of the artworks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "University of Zurich",
              "dsl": "Digital Visual Studies"
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "Bibliotheca Hertziana - MPI für Kunstgeschichte",
              "dsl": ""
            }
          ],
          "personId": 80188
        }
      ]
    },
    {
      "id": 80248,
      "typeId": 12089,
      "title": "EV Life: A Counterfactual Dashboard Towards Reducing Carbon of Automotive Behaviors",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516451"
        },
        "Presentation Video": {
          "duration": "109",
          "hideBeforeConference": true,
          "title": "EV Life: A Counterfactual Dashboard Towards Reducing Carbon of Automotive Behaviors",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=_wLQTVy6IFA"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Adopting electric vehicles (EVs) is an important step towards meeting climate change targets. Despite the increased availability of EVs, many individuals are unfamiliar with the environmental and cost savings and how their driving behaviors might change (e.g., where and how to charge) when switching from a conventional fuel vehicle. While behavioral science research can identify what factors are barriers to EV adoption, there is a struggle to identify interventions that can help mitigate these barriers. We introduce EV Life, a mobile app for showing a counterfactual view of people’s automotive behaviors which introduces two functions. First, the app monitors a person’s driving trips in their current vehicle and provides a counterfactual dashboard that highlights what their trip would be like with an EV, including information about cost savings, reduction in carbon emissions, and charging locations. Second, the app provides a research platform for testing interventions for belief change using rule based or machine learning notification delivery.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80189
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80229
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80203
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80170
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80169
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80180
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Altos",
              "institution": "Toyota Research Institute",
              "dsl": ""
            }
          ],
          "personId": 80165
        }
      ]
    },
    {
      "id": 80249,
      "typeId": 12089,
      "title": "Language Models Can Generate Human-Like Self-Reports of Emotion",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516464"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp3571.pdf"
        },
        "Presentation Video": {
          "duration": "299",
          "hideBeforeConference": true,
          "title": "Language Models Can Generate Human-Like Self-Reports of Emotion",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GybnQ8foRJ8"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Computational interaction and user modeling is presently limited in the domain of emotions. We investigate a potential new approach to computational modeling of emotional response behavior, by using modern neural language models to generate synthetic self-report data, and evaluating the human-likeness of the results. More specifically, we generate responses to the PANAS questionnaire with four different variants of the recent GPT-3 model. Based on both data visualizations and multiple quantitative metrics, the human-likeness of the responses increases with model size, with the largest Davinci model variant generating the most human-like data. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 80157
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": ""
            }
          ],
          "personId": 80134
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 80141
        }
      ]
    },
    {
      "id": 80250,
      "typeId": 12089,
      "title": "Advanced Techniques for Preventing Thermal Imaging Attacks",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516472"
        },
        "Presentation Video": {
          "duration": "89",
          "hideBeforeConference": true,
          "title": "Advanced Techniques for Preventing Thermal Imaging Attacks",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=4AZO5WCF250"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Thermal cameras can be used to detect user's input on interfaces, such as touchscreens, keyboards and PIN pads. \r\nThis is done by recording the heat traces left by the users' fingers after interaction (e.g., typing a message or entering a PIN), and then using them to reconstruct the user's input. \r\nWhile previous work mitigated the thermal attacks by complicating input or distorting heat traces after input, our research is the first to propose \\textit{preventing} thermal attack using advanced deep learning (DL) techniques to prevent malicious use of thermal cameras. \r\nOur DL models detect interfaces and then obfuscate any heat traces on them. \r\nIn a planned user study, participants will try to identify user input from unmodified and obfuscated thermal images. \r\nOur preliminary findings show that the proposed framework can detect interfaces and eliminate authentication information from thermal images. \r\nAt the same time, our methods still reveal if an interface has been interacted with. Thus, our approach improves security without impacting the utility of the thermal camera. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 80161
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": "School of Computing Science"
            }
          ],
          "personId": 80148
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 80194
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "University of Glasgow",
              "dsl": ""
            }
          ],
          "personId": 80140
        }
      ]
    },
    {
      "id": 80251,
      "typeId": 12089,
      "title": "An Intelligent Color Recommendation Tool for Landing Page Design",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516450"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp9554.pdf"
        },
        "Presentation Video": {
          "duration": "91",
          "hideBeforeConference": true,
          "title": "An Intelligent Color Recommendation Tool for Landing Page Design",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=EOAjyFYD4_4"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Color plays an important role in users' attitudes and purchase intentions in the context of advertising. In landing page design, designers usually struggle with getting appropriate color palettes for multiple design elements, such as buttons, texts and icons. Therefore, we build a color recommendation system for landing page design. To learn different color palettes for each design element instead of a single palette for the overall design, we use a color sequence combining color palettes of all important elements in a design. We train a masked color model for color sequence completion and advertising performance prediction. Further, the system allows users to recolor a specified element in the image based on recommended colors. We conduct a user study by collecting qualitative feedback from professional graphic designers through interviews, which validates the usability of the proposed color recommendation system for landing page design.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "AI Lab, CyberAgent, Inc.",
              "dsl": ""
            }
          ],
          "personId": 80146
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "CyberAgent, inc.",
              "dsl": "AI Lab"
            }
          ],
          "personId": 80183
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "CyberAgent, Inc.",
              "dsl": ""
            }
          ],
          "personId": 80168
        }
      ]
    },
    {
      "id": 80252,
      "typeId": 12089,
      "title": "AF'fective Design: Supporting Atrial Fibrillation Post-treatment with Explainable AI",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516455"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp9555.pdf"
        },
        "Presentation Video": {
          "duration": "234",
          "hideBeforeConference": true,
          "title": "AF'fective Design: Supporting Atrial Fibrillation Post-treatment with Explainable AI",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Qdd-tvOgVvg"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "This paper reports the preliminary design results of AF'fective, an AI driven patient monitoring system designed to facilitate the post-treatment of patients with Atrial Fibrillation (AF). In-depth interviews, co-design and prototype testing sessions were carried out with 16 cardiologists to investigate the context surrounding AF treatment, evaluate different explainable AI strategies and better understand how explainable AI could be designed and used to support AF post-treatment. Through the design process, we learnt key lessons such as the pitfalls of over-justification and how augmenting machine explanations with data sources that allow for self-interpretation could enhance perceived control over the decision making process and increase user acceptance in the system.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Weill Cornell Medicine",
              "dsl": ""
            }
          ],
          "personId": 80173
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Prefectural University of Medicine",
              "dsl": "Department of Cardiac Arrhythmia Research and Innovation"
            }
          ],
          "personId": 80174
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Prefectural University of Medicine",
              "dsl": "Department of Cardiac Arrhythmia Research and Innovation"
            }
          ],
          "personId": 80144
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "KIT",
              "dsl": ""
            }
          ],
          "personId": 80227
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto Institute of Technology",
              "dsl": "School of Information and Human Science"
            }
          ],
          "personId": 80177
        }
      ]
    },
    {
      "id": 80253,
      "typeId": 12089,
      "title": "Predicting Persuasiveness of Participants in Multiparty Conversations",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516466"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp9753.pdf"
        },
        "Presentation Video": {
          "duration": "303",
          "hideBeforeConference": true,
          "title": "Predicting Persuasiveness of Participants in Multiparty Conversations",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=jEeOs1KVeyE"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Persuasiveness is an important skill in communicating with others. This study aims to estimate the persuasiveness of the participants in group discussions. First, human annotators rated the level of persuasiveness of each of four participants in group discussions. We then created multimodal and multiparty models for estimating the persuasiveness of each participant using speech, language, and visual (head pose) features using GRU-based neural network. The experiment results showed that multimodal and multiparty models are better than unimodal and single-person models. The best performing multimodal multiparty model achieved 80% accuracy in predicting high/low persuasiveness, and 77% accuracy in predicting the most persuasive participant in the group.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 80209
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 80184
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Minato-ku",
              "institution": "NTT Human Informatics Laboratories, NTT Corporation",
              "dsl": ""
            }
          ],
          "personId": 80219
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Musashino-shi",
              "institution": "Seikei University",
              "dsl": ""
            }
          ],
          "personId": 80199
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Kanagawa",
              "city": "Yokosuka",
              "institution": "NTT Corporation",
              "dsl": "NTT Human Informatics Laboratories"
            }
          ],
          "personId": 80179
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Minato-ku",
              "institution": "NTT Human Informatics Laboratories, NTT Corporation",
              "dsl": ""
            }
          ],
          "personId": 80211
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokosuka",
              "institution": "NTT Human Informatics Laboratories, NTT Corporation",
              "dsl": ""
            }
          ],
          "personId": 80200
        }
      ]
    },
    {
      "id": 80254,
      "typeId": 12089,
      "title": "Shape-Flexible Underwater Display System with Wirelessly Powered and Controlled Smart LEDs",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516461"
        },
        "Presentation Video": {
          "duration": "123",
          "hideBeforeConference": true,
          "title": "Shape-Flexible Underwater Display System with Wirelessly Powered and Controlled Smart LEDs",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=GtkExqXouOQ"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "This paper proposes a 3D shape-flexible underwater display system which allows users to built a 3D display easily. To attain the flexibility, this paper proposes and evaluates the underwater wireless powering and communication method, which eliminates cables connected to each node. Simulation results show that the proposed method can deliver 4.78 mW to a 8mm * 8mm * 8mm size node and the communication speed of 9600 baud is feasible.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 80208
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kyoto",
              "institution": "Kyoto University",
              "dsl": ""
            }
          ],
          "personId": 80201
        }
      ]
    },
    {
      "id": 80255,
      "typeId": 12089,
      "title": "ThermalDrive - Towards Situation Awareness over Thermal Feedback in Automated Driving Scenarios",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490100.3516453"
        },
        "Poster": {
          "type": "custom",
          "url": "https://iui.acm.org/2022/images/posters/dp8805.pdf"
        },
        "Presentation Video": {
          "duration": "46",
          "hideBeforeConference": true,
          "title": "ThermalDrive - Towards Situation Awareness over Thermal Feedback in Automated Driving Scenarios",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=SuJGwOMmBLU"
        }
      },
      "isBreak": false,
      "trackId": 11765,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "We present ThermalDrive, a thermal interface that provided situational awareness information using thermal feedback on the face of the driver. A prototype is built to simulate autonomous driving and the thermal interface in virtual reality.\r\n\r\nWe conduct an experiment to investigate the impact of displaying system situation awareness information via the thermal feedback in a VR driving simulation (16 participants). The initial results indicate that the thermal interface might be a suitable feedback mechanism to convey some information in autonomous driving. In particular, cold thermal feedback was effective in terms of notability and user preference.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Media Design"
            }
          ],
          "personId": 80216
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Media Design"
            }
          ],
          "personId": 80138
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Media Design"
            }
          ],
          "personId": 80151
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Media Design"
            }
          ],
          "personId": 80164
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Yokohama",
              "institution": "Keio University",
              "dsl": "Keio Media Design"
            }
          ],
          "personId": 80159
        }
      ]
    },
    {
      "id": 80359,
      "typeId": 12049,
      "title": "SOcial and Cultural IntegrAtion with PersonaLIZEd Interfaces (SOCIALIZE) 2022",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80372
      ],
      "eventIds": [],
      "abstract": "The SOCIALIZE workshop aims to bring together all those interested in the development of interactive techniques that may contribute to foster the social and cultural inclusion of a broad range of users. More specifically, we intend to attract research that takes into account the interaction peculiarities typical of different realities, with a focus on disadvantaged and at-risk categories (e.g., refugees and migrants) and vulnerable groups (e.g., children, elderly, autistic and disabled people). Among others, we are also interested in human-robot interaction techniques aimed at the development of social robots, that is, autonomous robots that interact with people by engaging in social-affective behaviors, abilities, and rules related to their collaborative role.\r\n\r\nWorkshop webpage: http://socialize2022.di.unito.it/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "Roma Tre University",
              "dsl": "Engineering"
            }
          ],
          "personId": 80354
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Torino",
              "institution": "Università di Torino",
              "dsl": "Dipartimento di Informatica"
            }
          ],
          "personId": 80358
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rome",
              "institution": "Roma Tre University",
              "dsl": "Engineering"
            }
          ],
          "personId": 80356
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Koper",
              "institution": "University of Primorska",
              "dsl": "Faculty of Mathematics, Natural Sciences and Information Technologies"
            }
          ],
          "personId": 80347
        }
      ]
    },
    {
      "id": 80360,
      "typeId": 12049,
      "title": "HAI-GEN 2022: 3rd Workshop on Human-AI Co-Creation with Generative Models",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80370
      ],
      "eventIds": [],
      "abstract": "Recent advances in generative AI through deep learning approaches such as generative adversarial networks (GANs), variational autoencoders (VAEs), and large language models will enable new kinds of user experiences around content creation, across a range of media types. These advances have enabled content to be produced with an unprecedented level of fidelity, for tasks such as generating faces, prose and poems, deep fake videos of celebrities, music, and even code. These examples also highlight some of the significant societal, ethical, and organizational challenges generative AI is posing. The goal of our workshop is to bring together researchers and practitioners from the domains of HCI & AI to establish a joint community to deepen our understanding of the human-AI co-creative process and to explore the opportunities and challenges of creating powerful user experiences with deep generative models.\r\n\r\nWorkshop webpage: https://hai-gen.github.io/2022/",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 79516
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "North Carolina",
              "city": "Charlotte",
              "institution": "University of North Carolina at Charlotte",
              "dsl": ""
            }
          ],
          "personId": 80340
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 80345
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Columbia University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 80355
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 80353
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79431
        }
      ]
    },
    {
      "id": 80361,
      "typeId": 12049,
      "title": "HUMANIZE'22: Sixth Edition",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80412
      ],
      "eventIds": [],
      "abstract": "HUMANIZE aims to investigate how intelligent, adaptive systems can benefit from combining quantitative, data-driven approaches with qualitative, theory-driven approaches. We in particular invite work from researchers that incorporate features grounded in psychological theory (e.g. personality, cognitive styles etc.) into the predictive models underlying their adaptive/intelligent systems (e.g. recommender systems, website morphing, etc.). Apart from research investigating how this approach can improve these systems, we are interested in research towards the potential of this approach in improving explainability, fairness and transparency and reducing bias in data or output of intelligent systems.\r\n\r\nWorkshop webpage: www.humanize-workshop.org",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Jönköping",
              "institution": "Jönköping University",
              "dsl": "Department of Computer Science and Informatics"
            }
          ],
          "personId": 80351
        },
        {
          "affiliations": [
            {
              "country": "Slovenia",
              "state": "",
              "city": "Koper",
              "institution": "University of Primorska",
              "dsl": "Faculty of Mathematics, Natural Sciences and Information Technologies"
            }
          ],
          "personId": 80347
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Walldorf",
              "institution": "SAP SE",
              "dsl": ""
            }
          ],
          "personId": 80357
        }
      ]
    },
    {
      "id": 80362,
      "typeId": 12049,
      "title": "TExSS: Transparency and Explanations in Smart Systems workshop",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80371
      ],
      "eventIds": [],
      "abstract": "Smart systems, such as decision support or recommender systems, continue to prove challenging for people to understand, but are nonetheless ever more pervasive based on the promise of harnessing rich data sources that are becoming available in every domain. These systems tend to be opaque, raising important concerns about how to discover and account for fairness or bias issues. The workshop on Transparency and Explanations in Smart Systems (TExSS) welcomes researchers and practitioners interested in exchanging ideas for overcoming the design, development, and evaluation issues in intelligent user interfaces. Specifically, we will focus on barriers preventing better reliability, trainability, usability, trustworthiness, fairness, accountability, and transparency. This year’s theme is “Responsible, Explainable AI for Inclusivity and Trust”, emphasizing the importance of responsibility that tech-industry and developers have towards the design, implementation and evaluation of explainable, inclusive and trustworthy human-AI interaction.\r\n\r\nWorkshop webpage: https://explainablesystems.comp.nus.edu.sg/2022/\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "The University of Haifa",
              "dsl": "Information Systems"
            }
          ],
          "personId": 80135
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79768
        },
        {
          "affiliations": [
            {
              "country": "Cyprus",
              "state": "",
              "city": "Nicosia",
              "institution": "Open University of Cyprus",
              "dsl": "Cyprus Centre for Algorithmic Transparency"
            }
          ],
          "personId": 80348
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "",
              "city": "Singapore",
              "institution": "National University of Singapore",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 80342
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Cambridge",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 80338
        },
        {
          "affiliations": [
            {
              "country": "Israel",
              "state": "",
              "city": "Haifa",
              "institution": "The University of Haifa",
              "dsl": "Information Systems"
            }
          ],
          "personId": 80350
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "City, University of London",
              "dsl": "Centre for HCI Design"
            }
          ],
          "personId": 80336
        }
      ]
    },
    {
      "id": 80363,
      "typeId": 12049,
      "title": "HEALTHI: The Second Workshop on Intelligent Healthy User Interfaces",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80368
      ],
      "eventIds": [],
      "abstract": "This second multidisciplinary workshop on Healthy Interfaces (HEALTHI) offers a forum that brings academics and industry researchers together and seeks submissions broadly related to the design of smart user interfaces for promoting health. It builds on the fields of psychology, behavioral health, human computer interaction, ubiquitous computing, and artificial intelligence. The workshop will discuss intelligent user interfaces such as screens, wearables, voices assistants, and chatbots in the context of accessibly and fairly supporting health, health behavior, and wellbeing.\r\n\r\nWorkshop webpage: https://sites.google.com/cornell.edu/healthi",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Connecticut",
              "city": "New Haven",
              "institution": "Yale University",
              "dsl": "School of Medicine"
            }
          ],
          "personId": 80339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "Cornell Tech",
              "dsl": "Computer Science"
            }
          ],
          "personId": 80352
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Zurich",
              "institution": "ETH Zurich",
              "dsl": "Centre for Digital Health Interventions, Department of Management, Technology, and Economics"
            },
            {
              "country": "Switzerland",
              "state": "",
              "city": "St.Gallen",
              "institution": "University of St.Gallen",
              "dsl": "Centre for Digital Health Interventions, Institute of Technology Management"
            }
          ],
          "personId": 80349
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "London",
              "institution": "Imperial College London",
              "dsl": "Dyson School of Design Engineering"
            }
          ],
          "personId": 80337
        }
      ]
    },
    {
      "id": 80364,
      "typeId": 12049,
      "title": "APEx-UI: Adaptive and Personalised Explainable User Interfaces",
      "addons": {},
      "isBreak": false,
      "trackId": 11766,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80369
      ],
      "eventIds": [],
      "abstract": "Adaptation and personalisation are crucial aspects of the design and development of successful artificial intelligence systems, from search engines and recommender systems to wearable devices. The increased desire for customisation inevitably leads to the need for the end-user to understand the rationale behind displaying that specific tailored content. The First Workshop on Adaptive and Personalised Explainable User Interfaces (APEx-UI 2022) aims to foster a cross-disciplinary and interdisciplinary discussion between experts from different fields (e.g. computer science, psychology, sociology, law, medicine, business, etc.) in order to answer a precise research question: \"How can we adapt and personalise explainable user interfaces to the needs, demands and requirements of different end-users, considering their distinct knowledge, background and expertise?\"\r\nWorkshop webpage: https://sites.google.com/view/apex-ui-2022",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Brunswick",
              "institution": "Georg Eckert Institute",
              "dsl": "Department Information and Research Infrastructures"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University Magdeburg",
              "dsl": "Faculty of Computer Science"
            }
          ],
          "personId": 80343
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "BA",
              "city": "Bari",
              "institution": "University of Bari Aldo Moro",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 80346
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bari ",
              "institution": "University of Bari",
              "dsl": ""
            }
          ],
          "personId": 80341
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Magdeburg",
              "institution": "Otto von Guericke University Magdeburg",
              "dsl": "Faculty of Computer Science"
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Brunswick",
              "institution": "Georg Eckert Institute - Leibniz Institute for International Textbook Research",
              "dsl": "Digital Information and Research Infrastructure department"
            }
          ],
          "personId": 80344
        }
      ]
    },
    {
      "id": 80386,
      "typeId": 12064,
      "title": "Digital Health: Which Roles for Patients, Professionals and Machines?",
      "addons": {},
      "isBreak": false,
      "trackId": 11790,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80405
      ],
      "eventIds": [],
      "abstract": "The objective of the panel is to have a discussion on the challenges and opportunities of digital health. In the past century, health has been the domain where inventions and innovations have greatly impacted society. More recently the pace of innovation has increased in the technology sector. Advances in technology include novel devices, sensors and of course the whole gamma of AI algorithms and systems. These new systems can greatly help to reach a high level of performance in healthcare and it has been shown in a variety of use cases. However, we have witnessed many failures and/or slow acceptance from the user end. The user groups in healthcare cover the patients, the healthcare professionals, the healthcare providers and the policy makers. There are many open questions on the design of technology that needs to fit processes that cover the whole patient journey. How technological systems can help motivate individuals or entire groups ( e.g. citizens ) to get better healthcare or join new therapeutic paths that include digital health ? Is there a human vs machine decision dilemma and what is that about ? These are some of the questions we will address with the help of a panel of international experts from academia and industry.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80379
        },
        {
          "affiliations": [],
          "personId": 80380
        },
        {
          "affiliations": [],
          "personId": 80381
        },
        {
          "affiliations": [],
          "personId": 80382
        },
        {
          "affiliations": [],
          "personId": 80383
        },
        {
          "affiliations": [],
          "personId": 80384
        }
      ]
    },
    {
      "id": 80387,
      "typeId": 12065,
      "title": "From Social to Prosocial Machines: A New Challenge for AI",
      "addons": {},
      "isBreak": false,
      "trackId": 11772,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80402
      ],
      "eventIds": [],
      "abstract": "Abstract: Throughout the past few years, artificial intelligence (AI) has become increasingly more present in our daily lives. A myriad of settings became the stage for AI applications, such as factories, roads, houses, hospitals and even schools. Given these new contexts, AI-powered machines must now place the human at the centre, and be designed to interact with humans in a natural way: AI is becoming social. But such a diverse use of AI also fosters change, especially in the way we behave and how with cooperate with each other and with machines. It is therefore important to reflect upon the impact that AI may have on humans’ societies, and consider its effects on supporting more collaboration, social action, and prosocial behavior. Prosocial behavior occurs when people and agents perform costly actions that benefit others. Acts such as helping others voluntarily, donating to charity, providing information or sharing resources, are all forms of prosocial behavior. Humans are inherently prosocial, and attributes, such as altruism or empathy, that affect decision-making and cooperation, are essential ingredients to more just and positive societies. However, the view of human decision-making prevalent in the design of AI is based on the homo economicus principle, where utility maximization and selfishness are the backbone for modeling behavior in autonomous behavior. In this talk I will be challenging this view and explore how to create AI (agents) that places prosocial behavior at the core, and while engaged in human settings, cultivates cooperation and fosters people into contributing for the social good. I will imagine a future where prosocial machines can be a reality and present three case studies from different areas: prosocial robotics; prosocial games, and social simulation. These simple examples illustrate how AI can play an active role by contributing to a kinder society.\n\nAbout the speaker: Ana Paiva is a Full Professor in the Department of Computer Engineering at Instituto Superior Técnico (IST) from the University of Lisbon and former Coordinator of GAIPS – \"Group on AI for People and Society\" at INESC-ID. Prof. Ana Paiva was also a 2020–2021 fellow at the Radcliffe Institute for Advanced Study at Harvard University. Prof. Paiva's addresses problems and develops techniques for creating social agents that can simulate human-like behaviors and interact with humans in a natural and social manner. Over the years she has addressed this problem by engineering agents that exhibit specific social capabilities, including emotions, personality, culture, non-verbal behavior, empathy, collaboration, and others. Her more recent research combines methods from AI, social robotics and social modelling to study hybrid societies of humans and machines, in particular to engineer agents that lead to more prosocial and altruistic societies. She has published extensively and received best paper awards in different conferences, in particular she won the first prize of the Blue Sky Awards at the AAAI in 2018. She has further advanced the area of artificial intelligence and social agents worldwide, having served for the Global Agenda Council in Artificial Intelligence and Robotics of the World Economic Forum and as a member of the Scientific Advisory Board of Science Europe. She is an EurAI fellow, an ELLIS fellow and member of the extended core group of the CLAIRE (Confederation of Laboratories for AI research in Europe).",
      "authors": [
        {
          "affiliations": [],
          "personId": 80376
        }
      ]
    },
    {
      "id": 80388,
      "typeId": 12065,
      "title": "Employing Social Media to Improve Mental Health: Pitfalls, Lessons Learned, and the Next Frontier",
      "addons": {},
      "isBreak": false,
      "trackId": 11772,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80403
      ],
      "eventIds": [],
      "abstract": "Abstract: Social media data is being increasingly used to computationally learn about and infer the mental health states of individuals and populations. Despite being touted as a powerful means to shape interventions and impact mental health recovery, little do we understand about the theoretical, domain, and psychometric validity of this novel information source, or its underlying biases, when appropriated to augment conventionally gathered data, such as surveys and verbal self-reports. This talk presents a critical analytic perspective on the pitfalls of social media signals of mental health, especially when they are derived from “proxy” diagnostic indicators, often removed from the real-world context in which they are likely to be used. Then, to overcome these pitfalls, this talk presents results from two case studies, where machine learning algorithms to glean mental health insights from social media were developed in a context-sensitive and human-centered way, in collaboration with domain experts and stakeholders. The first of these case studies, a collaboration with a health provider, focuses on the individual-perspective, and reveals the ability and implications of using social media data of consented schizophrenia patients to forecast relapse and support clinical decision-making. Scaling up to populations, in collaboration with a federal organization and towards influencing public health policy, the second case study seeks to forecast nationwide rates of suicide fatalities using social media signals, in conjunction with health services data. The talk concludes with discussions of the path forward, emphasizing the need for a collaborative, multi-disciplinary research agenda while realizing the potential of social media data and machine learning in mental health – one that incorporates methodological rigor, ethics, and accountability, all at once. \n\nAbout the speaker: Munmun De Choudhury is an Associate Professor of Interactive Computing at Georgia Tech. Dr. De Choudhury is best known for laying the foundation of a line of research that develops computational techniques to responsibly and ethically employ social media in understanding and improving our mental health. To do this work, she adopts a highly interdisciplinary approach, combining social computing, machine learning, and natural language analysis with insights and theories from the social, behavioral, and health sciences. Dr. De Choudhury has been recognized with the 2021 ACM-W Rising Star Award, 2019 Complex Systems Society – Junior Scientific Award, numerous best paper and honorable mention awards from the ACM and AAAI, and features and coverage in popular press like the New York Times, the NPR, and the BBC. Earlier, Dr. De Choudhury was a faculty associate with the Berkman Klein Center for Internet and Society at Harvard, a postdoc at Microsoft Research, and obtained her PhD in Computer Science from Arizona State University.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80374
        }
      ]
    },
    {
      "id": 80389,
      "typeId": 12065,
      "title": "Provably Beneficial Artificial Intelligence",
      "addons": {},
      "isBreak": false,
      "trackId": 11772,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80406
      ],
      "eventIds": [],
      "abstract": "Abstract: As AI advances in capabilities and moves into the real world, its potential to benefit humanity seems limitless. Yet we see serious problems including racial and gender bias, manipulation by social media, and an arms race in lethal autonomous weapons. Looking further ahead, Alan Turing predicted the eventual loss of human control over machines that exceed human capabilities. I will argue that Turing was right to express concern but wrong to think that doom is inevitable. Instead, we need to develop a new kind of AI that is provably beneficial to humans.\n\nAbout the speaker: Stuart Russell is a Professor of Computer Science at the University of California at Berkeley, holder of the Smith-Zadeh Chair in Engineering, and Director of the Center for Human-Compatible AI. He is a recipient of the IJCAI Computers and Thought Award and held the Chaire Blaise Pascal in Paris. In 2021 he received the OBE from Her Majesty Queen Elizabeth. He is an Honorary Fellow of Wadham College, Oxford, an Andrew Carnegie Fellow, and a Fellow of the American Association for Artificial Intelligence, the Association for Computing Machinery, and the American Association for the Advancement of Science. His book \"Artificial Intelligence: A Modern Approach\" (with Peter Norvig) is the standard text in AI, used in 1500 universities in 135 countries. His research covers a wide range of topics in artificial intelligence, with a current emphasis on the long-term future of artificial intelligence and its relation to humanity. He has developed a new global seismic monitoring system for the nuclear-test-ban treaty and is currently working to ban lethal autonomous weapons.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80385
        }
      ]
    },
    {
      "id": 80397,
      "typeId": 12065,
      "durationOverride": 25,
      "title": "Welcome",
      "addons": {},
      "isBreak": false,
      "trackId": 11772,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80402
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 80375
        },
        {
          "affiliations": [],
          "personId": 80377
        },
        {
          "affiliations": [],
          "personId": 80378
        },
        {
          "affiliations": [],
          "personId": 80391
        },
        {
          "affiliations": [],
          "personId": 80392
        },
        {
          "affiliations": [],
          "personId": 80393
        }
      ]
    },
    {
      "id": 80398,
      "typeId": 12065,
      "durationOverride": 25,
      "title": "Good bye",
      "addons": {},
      "isBreak": false,
      "trackId": 11772,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80406
      ],
      "eventIds": [],
      "authors": [
        {
          "affiliations": [],
          "personId": 80375
        },
        {
          "affiliations": [],
          "personId": 80377
        },
        {
          "affiliations": [],
          "personId": 80378
        },
        {
          "affiliations": [],
          "personId": 80391
        },
        {
          "affiliations": [],
          "personId": 80392
        },
        {
          "affiliations": [],
          "personId": 80393
        }
      ]
    },
    {
      "id": 80399,
      "typeId": 12064,
      "title": "Panel on Impact Award",
      "addons": {},
      "isBreak": false,
      "trackId": 11790,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80404
      ],
      "eventIds": [],
      "abstract": "Ana-Maria Popescu, Oren Etzioni, and Henry Kautz. 2003. Towards a theory of natural language interfaces to databases. In Proceedings of the 8th international conference on Intelligent user interfaces (IUI '03). Association for Computing Machinery, New York, NY, USA, 149–157. DOI:https://doi.org/10.1145/604045.604070",
      "authors": [
        {
          "affiliations": [],
          "personId": 80394
        },
        {
          "affiliations": [],
          "personId": 80395
        },
        {
          "affiliations": [],
          "personId": 80396
        }
      ]
    },
    {
      "id": 80410,
      "typeId": 12044,
      "durationOverride": 60,
      "title": "Intelligent User Interfaces @ Meta Reality Labs",
      "addons": {},
      "isBreak": false,
      "trackId": 11791,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80417
      ],
      "eventIds": [],
      "abstract": "HCI and AI researchers and engineers at Meta Reality Labs are focused on solving core interface and interaction problems that will enable future all-day wearable Mixed Reality (XR) interfaces. In this session, you will hear from several research scientists about ground-breaking research at the intersection of HCI and AI conducted at Reality Labs, including intelligent input, 3D interactions techniques, predictive systems, and adaptive and multimodal user interfaces. The presentation will be followed by a Q&A with other members of our team where we will discuss the numerous employment and collaboration opportunities.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80407
        },
        {
          "affiliations": [],
          "personId": 80408
        },
        {
          "affiliations": [],
          "personId": 80409
        },
        {
          "affiliations": [],
          "personId": 80437
        },
        {
          "affiliations": [],
          "personId": 80438
        },
        {
          "affiliations": [],
          "personId": 80439
        },
        {
          "affiliations": [],
          "personId": 80440
        },
        {
          "affiliations": [],
          "personId": 80441
        }
      ]
    },
    {
      "id": 80627,
      "typeId": 12089,
      "title": "iSEA : An Interactive Pipeline for Semantic Error Analysis of NLP Models",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511146"
        },
        "Presentation Video": {
          "duration": "286",
          "hideBeforeConference": true,
          "title": "iSEA : An Interactive Pipeline for Semantic Error Analysis of NLP Models",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=rgoe-Nal7Bo"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Error analysis in NLP models is essential to successful model development and deployment. One common approach for diagnosing errors is to identify subpopulations in the dataset where the model produces the most errors. However, existing approaches typically define subpopulations based on pre-defined features, which requires users to form hypotheses of errors in advance. To complement these approaches, we propose iSEA, an Interactive Pipeline for Semantic Error Analysis in NLP Models, which automatically discovers semantically-grounded subpopulations with high error rates in the context of a human-in-the-loop interactive system. iSEA enables model developers to learn more about their model errors through discovered subpopulations, validate the sources of errors through interactive analysis on the discovered subpopulations, and test hypotheses about model errors by defining custom subpopulations. The tool supports semantic descriptions of error-prone subpopulations at the token and concept level, as well as pre-defined higher-level features. Through use cases and expert interviews, we demonstrate how iSEA can assist error understanding and analysis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "New York",
              "institution": "New York University",
              "dsl": ""
            }
          ],
          "personId": 79923
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce Research",
              "dsl": ""
            }
          ],
          "personId": 79678
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Palo Alto",
              "institution": "Salesforce",
              "dsl": "Salesforce"
            }
          ],
          "personId": 79726
        }
      ]
    },
    {
      "id": 80628,
      "typeId": 12089,
      "title": "Better Together? An Evaluation of AI-Supported Code Translation",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511157"
        },
        "Presentation Video": {
          "duration": "271",
          "hideBeforeConference": true,
          "title": "Better Together? An Evaluation of AI-Supported Code Translation",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=NB4lxVt6nhs"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Generative machine learning models have recently been applied to source code, for use cases including translating code between programming languages, creating documentation from code, and auto-completing methods. Yet, state-of-the-art models often produce code that is erroneous or incomplete. In a controlled study with 32 software engineers, we examined whether such imperfect outputs are helpful in the context of Java-to-Python code translation. When aided by the outputs of a code translation model, participants produced code with fewer errors than when working alone. We also examined how the quality and quantity of AI translations affected the work process and quality of outcomes, and observed that providing multiple translations had a larger impact on the translation process than varying the quality of provided translations. Our results tell a complex, nuanced story about the benefits of generative code models and the challenges software engineers face when working with their outputs. Our work motivates the need for intelligent user interfaces that help software engineers effectively work with generative code models in order to understand and evaluate their outputs and achieve superior outcomes to working alone.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 79516
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 79539
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": "Visual AI Lab"
            }
          ],
          "personId": 79447
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "Buenos Aires",
              "city": "La Plata",
              "institution": "IBM Argentina",
              "dsl": ""
            }
          ],
          "personId": 79557
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79535
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79390
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79339
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 79466
        }
      ]
    },
    {
      "id": 80631,
      "typeId": 12089,
      "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with GreaseTerminator",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511152"
        },
        "Presentation Video": {
          "duration": "199",
          "hideBeforeConference": true,
          "title": "Mind-proofing Your Phone: Navigating the Digital Minefield with GreaseTerminator",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=PTs_CR-bTUA"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Digital harms are widespread in the mobile ecosystem. As these devices gain ever more prominence in our daily lives, so too increases the potential for malicious attacks against individuals. The last line of defense against a range of digital harms – including digital distraction, political polarisation through hate speech, and children being exposed to damaging material – is the user interface. To tackle such interface-based harms, this work introduces GreaseTerminator to develop, deploy, and test interventions against these harms with end-users. We demonstrate the ease of intervention development and deployment, as well as the broad range of harms potentially covered with GreaseTerminator in five in-depth case studies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79812
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79806
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Oxford",
              "institution": "University of Oxford",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 79858
        }
      ]
    },
    {
      "id": 80632,
      "typeId": 12089,
      "title": "Emblaze: Illuminating Machine Learning Representations through Interactive Comparison of Embedding Spaces",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511137"
        },
        "Presentation Video": {
          "duration": "301",
          "hideBeforeConference": true,
          "title": "Emblaze: Illuminating Machine Learning Representations through Interactive Comparison of Embedding Spaces",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KKnIPOMfnBE"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Modern machine learning techniques commonly rely on complex, high-dimensional embedding representations to capture underlying structure in the data and improve performance. In order to characterize model flaws and choose a desirable representation, model builders often need to compare across multiple embedding spaces, a challenging analytical task supported by few existing tools. We first interviewed nine embedding experts in a variety of fields to characterize the diverse challenges they face and techniques they use when analyzing embedding spaces. Informed by these perspectives, we developed a novel system called Emblaze that integrates embedding space comparison within a computational notebook environment. Emblaze uses an animated, interactive scatter plot with a novel star trail augmentation to enable visual comparison. It also employs novel neighborhood analysis and clustering procedures to dynamically suggest groups of points with interesting changes between spaces. Through think-aloud sessions with ML experts, we demonstrate how interactive comparison with Emblaze can help gain new insights into embedding space structure.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79752
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": ""
            }
          ],
          "personId": 79676
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 79665
        }
      ]
    },
    {
      "id": 80633,
      "typeId": 12089,
      "title": "Explaining Recommendations in E-Learning, Effects on Adolescents' Trust",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511140"
        },
        "Presentation Video": {
          "duration": "185",
          "hideBeforeConference": true,
          "title": "Explaining Recommendations in E-Learning, Effects on Adolescents' Trust",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xyNK4GYwfsQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Recommender systems are increasingly supporting explanations to increase trust in their recommendations. However, studies on explaining recommendations typically target adults in low-risk e-commerce or media contexts, and using explanations in e-learning has received little research attention. To address these limits, we investigated how explanations affect adolescents' trust in an exercise recommender on a mathematical e-learning platform. In a randomized controlled experiment with 37 adolescents, we compared real explanations with placebo and no explanations. Our results show that explanations can significantly increase initial trust when measured as a multidimensional construct of competence, benevolence, integrity, intention to return, and perceived transparency. Yet, as not all adolescents in our study attached equal importance to explanations, it remains important to tailor them. To study the impact of tailored explanations, we advise researchers to include placebo baselines in their studies as they may give more insights into how much transparency people actually need, compared to no-explanation baselines.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79730
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "Vlaams-Brabant",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79705
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79374
        }
      ]
    },
    {
      "id": 80634,
      "typeId": 12089,
      "title": "Colorbo: Envisioned Mandala Coloring through Human-AI Collaboration",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511135"
        },
        "Presentation Video": {
          "duration": "123",
          "hideBeforeConference": true,
          "title": "Colorbo: Envisioned Mandala Coloring through Human-AI Collaboration",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=KmsOsCOSOHg"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Mandala coloring is popular with many people, from children to adults, and also many studies have revealed its benefits in mental well-being. However, our preliminary study results reveal difficulties in mandala coloring tasks, such as selecting harmonious colors/areas and envisioning how each selection affects the final output. This paper presents Colorbo, an interactive system to support mandala coloring by envision-based human-AI collaboration. Colorbo and its user colorize a mandala individually by watching each other’s work. The user shows a colored mandala in progress, and Colorbo fills the remaining areas by analyzing the patterns and color combinations of the user image. Colorbo then projects the complete mandala onto the paper the user is colorizing, and the user continues coloring by envisioning how it can go based on Colorbo’s images. We conducted a within-subject study to understand such envision-based human-AI collaboration. Our quantitative and qualitative analysis results show the participants’ positive experiences and concerns in coloring behavior with Colorbo and their preferred projection method for the envision. Finally, based on the findings,we discuss design implications for Human-AI collaboration for art.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang Univeristy",
              "dsl": ""
            }
          ],
          "personId": 79799
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 79857
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University",
              "dsl": ""
            }
          ],
          "personId": 79817
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Ansan",
              "institution": "Hanyang University ERICA",
              "dsl": ""
            }
          ],
          "personId": 79655
        }
      ]
    },
    {
      "id": 80635,
      "typeId": 12089,
      "title": "Deep Learning Uncertainty in Machine Teaching",
      "award": "BEST_PAPER",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511117"
        },
        "Presentation Video": {
          "duration": "230",
          "hideBeforeConference": true,
          "title": "Deep Learning Uncertainty in Machine Teaching",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=e-2XLLVxjlg"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Machine Learning models can output confident but incorrect predictions. To address this, ML researchers use various techniques to reliably estimate ML uncertainty, usually performed on controlled benchmarks once the model has been trained. \r\nWe explore how the two types of uncertainty—aleatoric and epistemic—can help non-expert users understand the strengths and weaknesses of a classifier in an interactive setting.  We are interested not only in their use of uncertainty to teach and understand the classifier, but also in their perception of the difference between aleatoric and epistemic uncertainty. We conducted an experiment where non-experts train a classifier to recognize card images, and are tested on their ability to predict classifier outcomes. Participants who used either larger or more varied training sets significantly improved their understanding of uncertainty, both epistemic or aleatoric. However, participants who relied on the uncertainty measure to guide their choice of training data did not significantly improve classifier training, nor were they better able to guess the classifier outcome. We identified three specific situations where participants successfully identified the difference between aleatoric and epistemic uncertainty: placing a new card in the exact same position as a training card; placing different cards next to each other; and placing a non-card, such as their hand, next to or on top of a card. We discuss our methodology for estimating uncertainty for Interactive Machine Learning systems and question the need for two-level uncertainty in Machine Teaching.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Gif-sur-Yvette",
              "institution": "Université Paris Saclay",
              "dsl": "LISN"
            }
          ],
          "personId": 79831
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Sorbonne Université, CNRS, ISIR",
              "dsl": ""
            }
          ],
          "personId": 79763
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Champs-sur-Marne",
              "institution": "Université Gustave Eiffel",
              "dsl": "ESIPE-IMAC"
            }
          ],
          "personId": 79811
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Paris",
              "institution": "Inria",
              "dsl": "ExSitu"
            }
          ],
          "personId": 79868
        }
      ]
    },
    {
      "id": 80636,
      "typeId": 12089,
      "title": "ODEN: Live Programming for Neural Network Architecture Editing",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511120"
        },
        "Presentation Video": {
          "duration": "287",
          "hideBeforeConference": true,
          "title": "ODEN: Live Programming for Neural Network Architecture Editing",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=s2pMwSSJjGQ"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "In deep learning application development, programmers tend to be trying different architectures and hyper-parameters until satisfied with the model performance.\r\nAlthough programmers may want to smoothly go back and forth between neural network(NN) architecture editing and experimentation, program crashes due to tensor shape mismatch and other issues prohibit them, especially novice programmers, from doing so.\r\nWe propose to leverage live programming techniques in NN architecture editing to show an always-on visualization.\r\nWhen the user edits the program, the visualization can synchronously display tensor states and provide a warning message by continuously executing the program to prevent program crashes during experimentation.\r\nWe implement the live visualization and integrate it into an IDE called ODEN that seamlessly supports the “edit→experiment→edit→···” repetition. \r\nWith ODEN, the user can construct the neural network with the live visualization and transits into experimentation to instantly train and test the NN architecture.\r\nAn exploratory user study is conducted to evaluate the usability, the limitations, and the potential of live visualization in ODEN.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79717
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79912
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79855
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 79808
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            },
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 79479
        }
      ]
    },
    {
      "id": 80637,
      "typeId": 12089,
      "title": "Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples",
      "award": "HONORABLE_MENTION",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511122"
        },
        "Presentation Video": {
          "duration": "281",
          "hideBeforeConference": true,
          "title": "Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=Sn5IhuM8ae8"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Embeddings mapping high-dimensional discrete input to lower-dimensional continuous vector spaces have been widely adopted in machine learning applications as a way to capture domain semantics. Interviewing 13 embedding users across disciplines, we find comparing embeddings is a key task for deployment or downstream analysis but unfolds in a tedious fashion that poorly supports systematic exploration. In response, we present the Embedding Comparator, an interactive system that presents a global comparison of embedding spaces alongside fine-grained inspection of local neighborhoods. It systematically surfaces points of comparison by computing the similarity of the k-nearest neighbors of every embedded object between a pair of spaces. Through case studies across multiple modalities, we demonstrate our system rapidly reveals insights, such as semantic changes following fine-tuning, language changes over time, and differences between seemingly similar models. In evaluations with 15 participants, we find our system accelerates comparisons by shifting from laborious manual specification to browsing and manipulating visualizations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79793
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79657
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 79865
        }
      ]
    },
    {
      "id": 80638,
      "typeId": 12089,
      "title": "VideoSticker: A Tool for Active Viewing and Visual Note-taking from Videos",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511132"
        },
        "Presentation Video": {
          "duration": "294",
          "hideBeforeConference": true,
          "title": "VideoSticker: A Tool for Active Viewing and Visual Note-taking from Videos",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=xiJvQgP5xxE"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "Video is an effective medium for knowledge communication and learning. Yet active viewing and note-taking from videos remain a challenge.  Specifically, during note-taking, viewers find it difficult to extract essential information such as representation, composition, motion, and interactions of graphical objects and narration. Current approaches rely on creating static screenshots, manual clipping, and manual annotation/transcription. Additionally, note-takers may need to repeatedly pause and rewind the video, disrupting their active viewing process. We propose VideoSticker, a tool designed to support visual note-taking by extracting expressive content from videos as 'motion stickers'. VideoSticker implements automated object detection and tracking, linking objects to the transcript, and rapid extraction of stickers across space, time, and events of interest. VideoSticker's two-pass approach allows viewers to capture high-level information uninterrupted and later extract specific details. We demonstrate the usability of VideoSticker for a variety of videos and note-taking needs. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Diego",
              "institution": "University of California, San Diego",
              "dsl": "Cognitive Science"
            }
          ],
          "personId": 79659
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": ""
            }
          ],
          "personId": 79332
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 79496
        }
      ]
    },
    {
      "id": 80640,
      "typeId": 12089,
      "title": "HINT: Integration Testing for AI-based features with Humans in the Loop",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511141"
        },
        "Presentation Video": {
          "duration": "303",
          "hideBeforeConference": true,
          "title": "HINT: Integration Testing for AI-based features with Humans in the Loop",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=nInaUp1VTZA"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "The dynamic nature of AI technologies makes testing human-AI interaction and collaboration challenging especially before such features are deployed in the wild. This presents a challenge for designers and AI practitioners as early feedback for iteration is often unavailable in the development phase. In this paper, we take inspiration from integration testing concepts in software development and present HINT (Human-AI INtegration Testing), a crowd-based framework for testing AI-based experiences integrated with a humans-in-the-loop workflow. HINT supports early testing of AI-based features within the context of realistic user tasks and utilizes successive sessions to simulate AI experiences that can evolve over-time and provides practitioners with reports to evaluate and compare aspects of these experiences.\r\n\r\nThrough a crowd-based study, we demonstrate the need for over-time testing where user behaviors evolve as they interact with an AI system. We also show that HINT is able to capture and reveal these distinct user behavior patterns across a variety of common AI performance modalities using two AI-based feature prototypes. We further evaluated HINT’s potential to support practitioners' evaluation of human-AI interaction experiences pre-deployment through semi-structured interviews with 13 practitioners.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Paul G. Allen School of Computer Science & Engineering"
            }
          ],
          "personId": 79677
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 79911
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "REDMOND",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 79894
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research AI",
              "dsl": ""
            }
          ],
          "personId": 79737
        }
      ]
    },
    {
      "id": 80641,
      "typeId": 12089,
      "title": "How Do People Rank Multiple Mutant Agents?",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511115"
        },
        "Presentation Video": {
          "duration": "236",
          "hideBeforeConference": true,
          "title": "How Do People Rank Multiple Mutant Agents?",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=ol_s9AJvEJw"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "How might a person decide on which of several AI-powered sequential decision-making systems to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (e.g., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) a new XAI empirical task to measure explanations: \"the Ranking Task\"; 2) a new strategy for inducing controllable agent variations---Mutant Agent Generation; 3) novel explanations for sequential decision-making agents; 4) an adaptation to the AAR/AI assessment process; and 5) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user's perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent \"test selection\" strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "EECS"
            }
          ],
          "personId": 79843
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of Electrical Engineering and Computer Science"
            }
          ],
          "personId": 79777
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": ""
            }
          ],
          "personId": 79854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Oregon",
              "city": "Corvallis",
              "institution": "Oregon State University",
              "dsl": "School of EECS"
            }
          ],
          "personId": 79693
        }
      ]
    },
    {
      "id": 80642,
      "typeId": 12089,
      "title": "NewsPod: Automatic and Interactive News Podcasts",
      "addons": {
        "doi": {
          "hideBeforeConference": true,
          "type": "doiLink",
          "url": " https://dl.acm.org/doi/10.1145/3490099.3511147"
        },
        "Presentation Video": {
          "duration": "135",
          "hideBeforeConference": true,
          "title": "NewsPod: Automatic and Interactive News Podcasts",
          "type": "video",
          "url": "https://www.youtube.com/watch?v=G5AS0BYMBco"
        }
      },
      "isBreak": false,
      "trackId": 11623,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80256
      ],
      "eventIds": [],
      "abstract": "News podcasts are a popular medium to stay informed and dive deep into news topics. Today, most podcasts are handcrafted by professionals. In this work we advance the state-of-the-art in automatically generated podcasts, making use of recent advances in natural language processing and text-to-speech technology. We present NewsPod, an automatically generated, interactive news podcast. The podcast is divided into segments, each centered on a news event, with each segment structured as a Question and Answer  conversation, whose goal is to engage the listener. A key aspect of the design is the use of distinct voices for each role (questioner, responder), to better simulate a conversation. Another novel aspect of this system allows listeners to interact with the podcast by asking their own questions and receiving automatically generated answers. We validate the soundness of this system design through two usability studies, focused on evaluating the narrative style and interactions with the podcast, respectively. We find that NewsPod is preferred over a baseline by participants, with 80% claiming they would use the system in the future.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "Electrical Engineering and Computer Science"
            }
          ],
          "personId": 79750
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": "EECS"
            }
          ],
          "personId": 79687
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": ""
            }
          ],
          "personId": 79670
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "University of California, Berkeley",
              "dsl": "Computer Science"
            }
          ],
          "personId": 79691
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Berkeley",
              "institution": "UC Berkeley",
              "dsl": ""
            }
          ],
          "personId": 79877
        }
      ]
    },
    {
      "id": 80645,
      "typeId": 12044,
      "durationOverride": 35,
      "title": "Mindfulness Session",
      "isBreak": false,
      "trackId": 11791,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80532
      ],
      "eventIds": [],
      "abstract": "Warmly welcome to the IUI 2022 virtual Mindfulness Session! Mindfulness is a perfect tool for unwinding after work, to calm your nervous system and to enhance your capability to focus your mind better. Mindfulness instructor Joonatan Jääskeläinen from Avara Valmennus (www.avaravalmennus.fi) will guide you through a very short introduction to mindfulness, and of course, through some relaxing mindfulness practices which you can do by yourself. I recommend that you participate in an undisturbed environment if possible, where you can focus entirely on the content of the session – perhaps in a quiet place, perhaps leaving your cell phone and other distracting things away for the time being. We will do one practice lying down, so make sure you have a yoga mat, bed or even a carpet close by. If your lower back hurts while lying, please put some pillows under your knees. A blanket might be a good idea if there is a cool draft.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80644
        }
      ]
    },
    {
      "id": 80646,
      "typeId": 12044,
      "durationOverride": 60,
      "title": "Mindfulness Session",
      "addons": {},
      "isBreak": false,
      "trackId": 11791,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80533
      ],
      "eventIds": [],
      "abstract": "Warmly welcome to the IUI 2022 virtual Mindfulness Session! Mindfulness is a perfect tool for unwinding after work, to calm your nervous system and to enhance your capability to focus your mind better. Mindfulness instructor Joonatan Jääskeläinen from Avara Valmennus (www.avaravalmennus.fi) will guide you through a very short introduction to mindfulness, and of course, through some relaxing mindfulness practices which you can do by yourself. I recommend that you participate in an undisturbed environment if possible, where you can focus entirely on the content of the session – perhaps in a quiet place, perhaps leaving your cell phone and other distracting things away for the time being. We will do one practice lying down, so make sure you have a yoga mat, bed or even a carpet close by. If your lower back hurts while lying, please put some pillows under your knees. A blanket might be a good idea if there is a cool draft.",
      "authors": [
        {
          "affiliations": [],
          "personId": 80644
        }
      ]
    },
    {
      "id": 80663,
      "typeId": 12044,
      "durationOverride": 60,
      "title": "Music",
      "isBreak": false,
      "trackId": 11791,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80664
      ],
      "eventIds": [],
      "abstract": "Folknery (Ukrainian: Фолькнери) is a Ukrainian free folk band, founded in 2009 by Volodymyr Muliar and Yaryna Kvitka. The duo travel around Ukraine and other countries by bicycle, gathering traditional folk songs and recording them with additional musical elements",
      "authors": [
        {
          "affiliations": [],
          "personId": 80661
        },
        {
          "affiliations": [],
          "personId": 80662
        }
      ]
    },
    {
      "id": 80672,
      "typeId": 12050,
      "title": "Introduction",
      "isBreak": true,
      "tags": [],
      "keywords": [],
      "sessionIds": [
        80023
      ],
      "eventIds": [],
      "authors": []
    }
  ],
  "people": [
    {
      "id": 79332,
      "firstName": "Hariharan",
      "lastName": "Subramonyam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79333,
      "firstName": "Tracy",
      "lastName": "Hammond",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79339,
      "firstName": "Kartik",
      "lastName": "Talamadupula",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79341,
      "firstName": "Gary",
      "lastName": "Ang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79350,
      "firstName": "Melanie",
      "lastName": "Heck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79371,
      "firstName": "Masataka",
      "lastName": "Goto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79374,
      "firstName": "Katrien",
      "lastName": "Verbert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79390,
      "firstName": "Mayank",
      "lastName": "Agarwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79393,
      "firstName": "Ming",
      "lastName": "Yin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79404,
      "firstName": "Felix",
      "lastName": "Putze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79431,
      "firstName": "Werner",
      "lastName": "Geyer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79447,
      "firstName": "Steven",
      "lastName": "Ross",
      "middleInitial": "I.",
      "affiliations": []
    },
    {
      "id": 79463,
      "firstName": "Christian",
      "lastName": "Becker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79466,
      "firstName": "John",
      "lastName": "Richards",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79478,
      "firstName": "Nyi Nyi",
      "lastName": "Htun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79479,
      "firstName": "Takeo",
      "lastName": "Igarashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79490,
      "firstName": "Q. Vera",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79496,
      "firstName": "Eytan",
      "lastName": "Adar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79501,
      "firstName": "Napol",
      "lastName": "Rachatasumrit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79513,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79516,
      "firstName": "Justin",
      "lastName": "Weisz",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 79525,
      "firstName": "Josh",
      "lastName": "Cherian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79535,
      "firstName": "Stephanie",
      "lastName": "Houde",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79537,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79539,
      "firstName": "Michael",
      "lastName": "Muller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79557,
      "firstName": "Fernando",
      "lastName": "Martinez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79576,
      "firstName": "Samantha",
      "lastName": "Ray",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79581,
      "firstName": "Ee Peng",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79583,
      "firstName": "Seth",
      "lastName": "Polsley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79653,
      "firstName": "Yun",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79654,
      "firstName": "Abdul Rafey",
      "lastName": "Aftab",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79655,
      "firstName": "Minsam",
      "lastName": "Ko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79656,
      "firstName": "Krzysztof",
      "lastName": "Gajos",
      "middleInitial": "Z.",
      "affiliations": []
    },
    {
      "id": 79657,
      "firstName": "Brandon",
      "lastName": "Carter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79658,
      "firstName": "Caleb",
      "lastName": "Matthews",
      "middleInitial": "Robert",
      "affiliations": []
    },
    {
      "id": 79659,
      "firstName": "Yining",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79660,
      "firstName": "Sooyeon",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79661,
      "firstName": "Sébastien",
      "lastName": "Lambot",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79662,
      "firstName": "Christopher",
      "lastName": "Collins",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79663,
      "firstName": "Christopher",
      "lastName": "Perdriau",
      "middleInitial": "H",
      "affiliations": []
    },
    {
      "id": 79664,
      "firstName": "Hanan",
      "lastName": "Samet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79665,
      "firstName": "Adam",
      "lastName": "Perer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79666,
      "firstName": "Marc",
      "lastName": "Hassenzahl",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79667,
      "firstName": "Luigi",
      "lastName": "De Russis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79668,
      "firstName": "Sayan",
      "lastName": "Chaudhry",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79669,
      "firstName": "Andrew",
      "lastName": "Bian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79670,
      "firstName": "Srujay",
      "lastName": "Korlakunta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79671,
      "firstName": "Zhifeng",
      "lastName": "Bao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79672,
      "firstName": "Massimiliano",
      "lastName": "Mattetti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79673,
      "firstName": "Juho",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79674,
      "firstName": "Jiwen",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79675,
      "firstName": "Paul",
      "lastName": "Taele",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79676,
      "firstName": "Yiwei",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79677,
      "firstName": "Quan Ze",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79678,
      "firstName": "Jesse",
      "lastName": "Vig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79679,
      "firstName": "Shirin",
      "lastName": "Feiz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79680,
      "firstName": "Owen",
      "lastName": "Cornec",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79681,
      "firstName": "Manuela",
      "lastName": "Eska",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79682,
      "firstName": "Mitsuhiko",
      "lastName": "Kimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79683,
      "firstName": "Per Ola",
      "lastName": "Kristensson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79684,
      "firstName": "Marie-Jeanne",
      "lastName": "Lesot",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79685,
      "firstName": "Shumin",
      "lastName": "Zhai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79686,
      "firstName": "Jonathan",
      "lastName": "Bragg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79687,
      "firstName": "Elicia",
      "lastName": "Ye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79688,
      "firstName": "Nastaran",
      "lastName": "Saffaryazdi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79689,
      "firstName": "Clarice",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79690,
      "firstName": "Tomoyuki",
      "lastName": "Maekawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79691,
      "firstName": "John",
      "lastName": "Canny",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79692,
      "firstName": "Nicholas",
      "lastName": "Kiddle",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79693,
      "firstName": "Margaret",
      "lastName": "Burnett",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79694,
      "firstName": "Jean",
      "lastName": "Vanderdonckt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79695,
      "firstName": "Xiaoyi",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79696,
      "firstName": "Elizabeth",
      "lastName": "Broadbent",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79697,
      "firstName": "Yenushka",
      "lastName": "Goonesekera",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79698,
      "firstName": "Andy",
      "lastName": "Coenen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79699,
      "firstName": "Jennifer",
      "lastName": "Thom",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79700,
      "firstName": "John",
      "lastName": "Guttag",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79701,
      "firstName": "Thomas",
      "lastName": "Grauschopf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79702,
      "firstName": "Casper",
      "lastName": "Harteveld",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79703,
      "firstName": "Daniel P.",
      "lastName": "Siewiorek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79704,
      "firstName": "Bernard",
      "lastName": "Jansen",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 79705,
      "firstName": "Shotallo",
      "lastName": "Kato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79706,
      "firstName": "Seong Hyun",
      "lastName": "Shon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79707,
      "firstName": "Sarah",
      "lastName": "Mennicken",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79708,
      "firstName": "Sébastien",
      "lastName": "Lallé",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79709,
      "firstName": "Sam",
      "lastName": "Snodgrass",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79710,
      "firstName": "Michael",
      "lastName": "von der Beeck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79711,
      "firstName": "Min",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79712,
      "firstName": "Lisa-Marie",
      "lastName": "Vortmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79713,
      "firstName": "Sai",
      "lastName": "Raja",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79714,
      "firstName": "Abdelkareem",
      "lastName": "Bedri",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79715,
      "firstName": "Tamara",
      "lastName": "von Sawitzky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79716,
      "firstName": "Yongjie",
      "lastName": "Duan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79717,
      "firstName": "Chunqi",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79718,
      "firstName": "Ozlem",
      "lastName": "Ergun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79719,
      "firstName": "Ann",
      "lastName": "Yuan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79720,
      "firstName": "Marcin",
      "lastName": "Detyniecki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79721,
      "firstName": "Shimei",
      "lastName": "Pan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79722,
      "firstName": "Özge Nilay",
      "lastName": "Yalçın",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79723,
      "firstName": "Nebiyou",
      "lastName": "Hailemariam",
      "middleInitial": "Daniel",
      "affiliations": []
    },
    {
      "id": 79724,
      "firstName": "Joachim",
      "lastName": "Meyer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79725,
      "firstName": "Bart",
      "lastName": "Knijnenburg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79726,
      "firstName": "Nazneen",
      "lastName": "Rajani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79727,
      "firstName": "Mohamed",
      "lastName": "Amer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79728,
      "firstName": "Uttkarsh",
      "lastName": "Narayan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79729,
      "firstName": "Khiem",
      "lastName": "Phi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79730,
      "firstName": "Jeroen",
      "lastName": "Ooge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79731,
      "firstName": "Chaoli",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79732,
      "firstName": "Daphne",
      "lastName": "Ippolito",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79733,
      "firstName": "Jianjiang",
      "lastName": "Feng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79734,
      "firstName": "Jiao",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79735,
      "firstName": "Eibe",
      "lastName": "Frank",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79736,
      "firstName": "Vikas",
      "lastName": "Ashok",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79737,
      "firstName": "Saleema",
      "lastName": "Amershi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79738,
      "firstName": "F. Maxwell",
      "lastName": "Harper",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79739,
      "firstName": "Shoya",
      "lastName": "Matsumori",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79740,
      "firstName": "Cristina",
      "lastName": "Conati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79741,
      "firstName": "Joni",
      "lastName": "Salminen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79742,
      "firstName": "Chia-Jung",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79743,
      "firstName": "Júlio",
      "lastName": "Guedes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79744,
      "firstName": "Junxiao",
      "lastName": "Shen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79745,
      "firstName": "Samuel",
      "lastName": "Way",
      "middleInitial": "F",
      "affiliations": []
    },
    {
      "id": 79746,
      "firstName": "Dongmei",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79747,
      "firstName": "Mayank",
      "lastName": "Goel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79748,
      "firstName": "Wenzhe",
      "lastName": "Cui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79749,
      "firstName": "Alberto",
      "lastName": "Monge Roffarello",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79750,
      "firstName": "Philippe",
      "lastName": "Laban",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79751,
      "firstName": "Matt",
      "lastName": "Duckham",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79752,
      "firstName": "Venkatesh",
      "lastName": "Sivaraman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79753,
      "firstName": "Jason",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79754,
      "firstName": "Jisu",
      "lastName": "Yim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79755,
      "firstName": "Amama",
      "lastName": "Mahmood",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79756,
      "firstName": "Timos",
      "lastName": "Sellis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79757,
      "firstName": "Ronnie",
      "lastName": "Smith",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79758,
      "firstName": "Xia",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79759,
      "firstName": "Huamin",
      "lastName": "Qu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79760,
      "firstName": "Oznur",
      "lastName": "Alkan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79761,
      "firstName": "Reshika",
      "lastName": "Palaniyappan Velumani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79762,
      "firstName": "Helma",
      "lastName": "Torkamaan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79763,
      "firstName": "Baptiste",
      "lastName": "Caramiaux",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79764,
      "firstName": "Arthur",
      "lastName": "Sluÿters",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79765,
      "firstName": "Yu",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79766,
      "firstName": "Fiona",
      "lastName": "Draxler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79767,
      "firstName": "Harini",
      "lastName": "Suresh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79768,
      "firstName": "Jonathan",
      "lastName": "Dodge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79769,
      "firstName": "Chinmay",
      "lastName": "Kulkarni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79770,
      "firstName": "Minsuk",
      "lastName": "Chang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79771,
      "firstName": "Jürgen",
      "lastName": "Ziegler",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79772,
      "firstName": "Jung In",
      "lastName": "Koh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79773,
      "firstName": "Ke",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79774,
      "firstName": "Lewis",
      "lastName": "Chuang",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 79775,
      "firstName": "Jeffrey",
      "lastName": "Nichols",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79776,
      "firstName": "Francisco",
      "lastName": "Gutiérrez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79777,
      "firstName": "Matthew",
      "lastName": "Olson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79778,
      "firstName": "Moritz",
      "lastName": "Berghofer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79779,
      "firstName": "Clemens",
      "lastName": "Schartmüller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79780,
      "firstName": "Sergi",
      "lastName": "Bermúdez i Badia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79781,
      "firstName": "Aitolkyn",
      "lastName": "Baigutanova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79782,
      "firstName": "Rui",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79783,
      "firstName": "Min",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79784,
      "firstName": "John",
      "lastName": "Dudley",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 79785,
      "firstName": "Amy",
      "lastName": "Zhang",
      "middleInitial": "X.",
      "affiliations": []
    },
    {
      "id": 79786,
      "firstName": "Shadan",
      "lastName": "Sadeghian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79787,
      "firstName": "Jie",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79788,
      "firstName": "Seoyoung",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79789,
      "firstName": "Boyin",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79790,
      "firstName": "Nathan",
      "lastName": "Lau",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79791,
      "firstName": "James",
      "lastName": "Foulds",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79792,
      "firstName": "Jun",
      "lastName": "Han",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79793,
      "firstName": "Angie",
      "lastName": "Boggust",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79794,
      "firstName": "Ebasa",
      "lastName": "Temesgen",
      "middleInitial": "Girma",
      "affiliations": []
    },
    {
      "id": 79795,
      "firstName": "Carlos",
      "lastName": "Aguirre",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 79796,
      "firstName": "Zhitao",
      "lastName": "Hou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79797,
      "firstName": "Soon-Gyo",
      "lastName": "Jung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79798,
      "firstName": "Haidong",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79799,
      "firstName": "Eunseo",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79800,
      "firstName": "Titus",
      "lastName": "Barik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79801,
      "firstName": "Denis",
      "lastName": "Parra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79802,
      "firstName": "Yuchen",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79803,
      "firstName": "Maozheng",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79804,
      "firstName": "Jingyi",
      "lastName": "Xie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79805,
      "firstName": "Chris",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79806,
      "firstName": "Konrad",
      "lastName": "Kollnig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79807,
      "firstName": "Andreas",
      "lastName": "Riener",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79808,
      "firstName": "Jun",
      "lastName": "Kato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79809,
      "firstName": "IV",
      "lastName": "Ramakrishnan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79810,
      "firstName": "Roli",
      "lastName": "Khanna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79811,
      "firstName": "Pierre",
      "lastName": "Thiel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79812,
      "firstName": "Siddhartha",
      "lastName": "Datta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79813,
      "firstName": "Yosuke",
      "lastName": "Fukuchi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79814,
      "firstName": "Wanqi",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79815,
      "firstName": "Leandro Balby",
      "lastName": "Marinho",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79816,
      "firstName": "Chien-Ming",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79817,
      "firstName": "Hyuna",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79818,
      "firstName": "Shiwali",
      "lastName": "Mohan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79819,
      "firstName": "Kin-Ho",
      "lastName": "Lam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79820,
      "firstName": "Sruti",
      "lastName": "Srinivasa Ragavan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79821,
      "firstName": "Hiromu",
      "lastName": "Yakura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79822,
      "firstName": "Meng",
      "lastName": "Xia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79823,
      "firstName": "Suranga",
      "lastName": "Nanayakkara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79824,
      "firstName": "Kamrun Naher",
      "lastName": "Keya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79825,
      "firstName": "Andrés F.",
      "lastName": "Salazar-Gómez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79826,
      "firstName": "Xiao",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79827,
      "firstName": "Elizabeth",
      "lastName": "Daly",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 79828,
      "firstName": "Alexandre",
      "lastName": "Bernardino",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79829,
      "firstName": "Syed Masum",
      "lastName": "Billah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79830,
      "firstName": "Fusheng",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79831,
      "firstName": "Téo",
      "lastName": "Sanchez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79832,
      "firstName": "Aybike",
      "lastName": "Ulusan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79833,
      "firstName": "Lena",
      "lastName": "Mamykina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79834,
      "firstName": "Evan",
      "lastName": "Newman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79835,
      "firstName": "Yolanda",
      "lastName": "Gil",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79836,
      "firstName": "Emily",
      "lastName": "Reif",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79837,
      "firstName": "Farhana",
      "lastName": "Choudhury",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79838,
      "firstName": "Jonathan",
      "lastName": "Aigrain",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79839,
      "firstName": "Nediyana",
      "lastName": "Daskalova",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79840,
      "firstName": "Anna",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79841,
      "firstName": "Asim",
      "lastName": "Smailagic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79842,
      "firstName": "Clara",
      "lastName": "Bove",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79843,
      "firstName": "Andrew",
      "lastName": "Anderson",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 79844,
      "firstName": "Geoff",
      "lastName": "Holmes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79845,
      "firstName": "Lijie",
      "lastName": "Guo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79846,
      "firstName": "Xiaojun",
      "lastName": "Bi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79847,
      "firstName": "Md Javedul",
      "lastName": "Ferdous",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79848,
      "firstName": "Rodrygo",
      "lastName": "Santos",
      "middleInitial": "L. T.",
      "affiliations": []
    },
    {
      "id": 79849,
      "firstName": "Kathryn",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79850,
      "firstName": "Jesse",
      "lastName": "Engel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79851,
      "firstName": "Henry",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79852,
      "firstName": "Bob",
      "lastName": "Coecke",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79853,
      "firstName": "ALEXIS",
      "lastName": "LAU",
      "middleInitial": "K",
      "affiliations": []
    },
    {
      "id": 79854,
      "firstName": "Rupika",
      "lastName": "Dikkala",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79855,
      "firstName": "Tsukasa",
      "lastName": "Fukusato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79856,
      "firstName": "Hae-Na",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79857,
      "firstName": "Jeongmin",
      "lastName": "Hong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79858,
      "firstName": "Nigel",
      "lastName": "Shadbolt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79860,
      "firstName": "Dale",
      "lastName": "Fletcher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79861,
      "firstName": "Ananya",
      "lastName": "Goel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79862,
      "firstName": "Charles",
      "lastName": "Tijus",
      "middleInitial": "Albert",
      "affiliations": []
    },
    {
      "id": 79863,
      "firstName": "Zhi",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79864,
      "firstName": "Furqan",
      "lastName": "Baig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79865,
      "firstName": "Arvind",
      "lastName": "Satyanarayan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79866,
      "firstName": "Andrew D",
      "lastName": "Gordon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79867,
      "firstName": "Vero",
      "lastName": "Vanden Abeele",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79868,
      "firstName": "Wendy",
      "lastName": "Mackay",
      "middleInitial": "E.",
      "affiliations": []
    },
    {
      "id": 79869,
      "firstName": "Graham",
      "lastName": "Taylor",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79870,
      "firstName": "Sheetal",
      "lastName": "Borar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79871,
      "firstName": "Amanda",
      "lastName": "Swearngin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79872,
      "firstName": "Geoff",
      "lastName": "Kaufman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79873,
      "firstName": "Utkarsh",
      "lastName": "Kunwar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79874,
      "firstName": "Ryoichi",
      "lastName": "Shibata",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79875,
      "firstName": "Luis",
      "lastName": "Leiva",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 79876,
      "firstName": "Jed",
      "lastName": "Irvine",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79877,
      "firstName": "Marti",
      "lastName": "Hearst",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79878,
      "firstName": "Alexandre",
      "lastName": "Armengol-Urpi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79879,
      "firstName": "Julia",
      "lastName": "Kylmälä",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79880,
      "firstName": "Xiaoran",
      "lastName": "Wu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79881,
      "firstName": "Nafiseh",
      "lastName": "Saffaryazdi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79882,
      "firstName": "Vinícius",
      "lastName": "Segura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79883,
      "firstName": "Vanessa",
      "lastName": "Murdock",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79884,
      "firstName": "Ryan",
      "lastName": "Louie",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79885,
      "firstName": "Rashidul",
      "lastName": "Islam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79886,
      "firstName": "Saelyne",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79887,
      "firstName": "Mauro",
      "lastName": "Dragone",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79888,
      "firstName": "Alan",
      "lastName": "Fern",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79889,
      "firstName": "Sanjay E.",
      "lastName": "Sarma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79890,
      "firstName": "Mark",
      "lastName": "Billinghurst",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79891,
      "firstName": "Rui",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79892,
      "firstName": "Thi Ngoc Trang",
      "lastName": "Tran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79893,
      "firstName": "Sam",
      "lastName": "Hepenstal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79894,
      "firstName": "Besmira",
      "lastName": "Nushi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79895,
      "firstName": "Julia",
      "lastName": "Brenner",
      "middleInitial": "Maria",
      "affiliations": []
    },
    {
      "id": 79896,
      "firstName": "Corey",
      "lastName": "Sterling",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79897,
      "firstName": "Ilhan",
      "lastName": "Aslan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79898,
      "firstName": "Paul",
      "lastName": "Lamere",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79899,
      "firstName": "Sina",
      "lastName": "Rashidian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79900,
      "firstName": "John",
      "lastName": "Carroll",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 79901,
      "firstName": "Albrecht",
      "lastName": "Schmidt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79902,
      "firstName": "Daniel",
      "lastName": "Weld",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 79903,
      "firstName": "Riku",
      "lastName": "Arakawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79904,
      "firstName": "Savvas",
      "lastName": "Petridis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79905,
      "firstName": "Kajal",
      "lastName": "Toshniwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79906,
      "firstName": "Chun-Wei",
      "lastName": "Chiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79907,
      "firstName": "Robin",
      "lastName": "De Croon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79908,
      "firstName": "Moritz",
      "lastName": "Schult",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79909,
      "firstName": "Ujjwal",
      "lastName": "Biswas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79910,
      "firstName": "Xiaoyu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79911,
      "firstName": "Tobias",
      "lastName": "Schnabel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79912,
      "firstName": "I-Chao",
      "lastName": "Shen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79913,
      "firstName": "Michita",
      "lastName": "Imai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79914,
      "firstName": "Zhengxian",
      "lastName": "Lin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79915,
      "firstName": "Ran",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79916,
      "firstName": "Zachary",
      "lastName": "Levonian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79917,
      "firstName": "Sampath",
      "lastName": "Jayarathna",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79918,
      "firstName": "Andrew",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79919,
      "firstName": "Mingzhao",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79920,
      "firstName": "Sudershan",
      "lastName": "Boovaraghavan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79921,
      "firstName": "Kathleen",
      "lastName": "Lewis",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 79922,
      "firstName": "Theresa",
      "lastName": "Mai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 79923,
      "firstName": "Jun",
      "lastName": "Yuan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80127,
      "firstName": "Simone",
      "lastName": "Barbosa",
      "middleInitial": "D J",
      "affiliations": []
    },
    {
      "id": 80129,
      "firstName": "Xiang",
      "lastName": "Su",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80130,
      "firstName": "Brian",
      "lastName": "Ravenet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80131,
      "firstName": "Jacky",
      "lastName": "Cao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80132,
      "firstName": "Karim",
      "lastName": "Benharrak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80133,
      "firstName": "Ai",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80134,
      "firstName": "Anton",
      "lastName": "Kunnari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80135,
      "firstName": "Tsvi",
      "lastName": "Kuflik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80136,
      "firstName": "Maya",
      "lastName": "Reiner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80137,
      "firstName": "Tuan-Duy",
      "lastName": "Nguyen",
      "middleInitial": "Hien",
      "affiliations": []
    },
    {
      "id": 80138,
      "firstName": "Jiawen",
      "lastName": "Han",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80139,
      "firstName": "Murthy",
      "lastName": "L R D",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80140,
      "firstName": "Mohamed",
      "lastName": "Khamis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80141,
      "firstName": "Perttu",
      "lastName": "Hämäläinen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80142,
      "firstName": "Jeevana Kruthi",
      "lastName": "Karnuthala",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80143,
      "firstName": "Shah Rukh",
      "lastName": "Humayoun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80144,
      "firstName": "Hibiki",
      "lastName": "Iwakoshi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80145,
      "firstName": "Qing",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80146,
      "firstName": "Qianru",
      "lastName": "Qiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80147,
      "firstName": "Niyati",
      "lastName": "Chhaya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80148,
      "firstName": "Md Shafiqul",
      "lastName": "Islam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80149,
      "firstName": "Hieu",
      "lastName": "Tran",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80150,
      "firstName": "Yun Suen",
      "lastName": "Pai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80151,
      "firstName": "George",
      "lastName": "Chernyshov",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80152,
      "firstName": "Pradipta",
      "lastName": "Biswas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80154,
      "firstName": "Nicolas",
      "lastName": "Ladeveze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80155,
      "firstName": "Ayudh",
      "lastName": "Saxena",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80156,
      "firstName": "Miriam",
      "lastName": "Punzi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80157,
      "firstName": "Mikke",
      "lastName": "Tavast",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80158,
      "firstName": "Manh",
      "lastName": "Luong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80159,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80160,
      "firstName": "Ghazanfar",
      "lastName": "Abbas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80161,
      "firstName": "Norah",
      "lastName": "Alotaibi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80162,
      "firstName": "Takashi",
      "lastName": "Yonezawa",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80163,
      "firstName": "Fumihide",
      "lastName": "Tanaka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80164,
      "firstName": "Kirill",
      "lastName": "Ragozin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80165,
      "firstName": "Kent",
      "lastName": "Lyons",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80166,
      "firstName": "Sangho",
      "lastName": "Suh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80167,
      "firstName": "Arash",
      "lastName": "Dargahi Nobari",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80168,
      "firstName": "Yuki",
      "lastName": "Iwazaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80169,
      "firstName": "Kate",
      "lastName": "Glazko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80170,
      "firstName": "Laurent",
      "lastName": "Denoue",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80171,
      "firstName": "Ragaad",
      "lastName": "Al-Tarawneh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80172,
      "firstName": "Tripti",
      "lastName": "Shukla",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80173,
      "firstName": "Wan-Jou",
      "lastName": "She",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80174,
      "firstName": "Keitaro",
      "lastName": "Senoo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80175,
      "firstName": "Davood",
      "lastName": "Rafiei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80176,
      "firstName": "Kai",
      "lastName": "Kunze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80177,
      "firstName": "Panote",
      "lastName": "Siriaraya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80178,
      "firstName": "Bhanu Prakash Reddy",
      "lastName": "Guda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80179,
      "firstName": "Ryo",
      "lastName": "Ishii",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80180,
      "firstName": "Kalani",
      "lastName": "Murakami",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80181,
      "firstName": "Huyen",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80182,
      "firstName": "Hai",
      "lastName": "Dang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80183,
      "firstName": "Mayu",
      "lastName": "Otani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80184,
      "firstName": "Yukiko",
      "lastName": "Nakano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80185,
      "firstName": "Dat Quoc",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80187,
      "firstName": "Wenxiao",
      "lastName": "ZHANG",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80188,
      "firstName": "Valentine",
      "lastName": "Bernasconi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80189,
      "firstName": "David",
      "lastName": "Shamma",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 80190,
      "firstName": "Abhilasha",
      "lastName": "Sancheti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80191,
      "firstName": "Tin",
      "lastName": "Vo",
      "middleInitial": "Duy",
      "affiliations": []
    },
    {
      "id": 80192,
      "firstName": "Hung",
      "lastName": "Bui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80193,
      "firstName": "Thien",
      "lastName": "Nguyen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80194,
      "firstName": "Karola",
      "lastName": "Marky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80195,
      "firstName": "Veronika",
      "lastName": "Bogina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80196,
      "firstName": "Pan",
      "lastName": "Hui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80197,
      "firstName": "Ketan",
      "lastName": "Anand",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80198,
      "firstName": "Yifei",
      "lastName": "Huang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80199,
      "firstName": "Tatsuya",
      "lastName": "Sakato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80200,
      "firstName": "Takao",
      "lastName": "Nakamura",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80201,
      "firstName": "Masanori",
      "lastName": "Hashimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80202,
      "firstName": "Pengcheng",
      "lastName": "An",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80203,
      "firstName": "Alexandre",
      "lastName": "Filipowicz",
      "middleInitial": "L. S.",
      "affiliations": []
    },
    {
      "id": 80204,
      "firstName": "Shambhavi",
      "lastName": "Aggarwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80205,
      "firstName": "Aanisha",
      "lastName": "Bhattacharyya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80206,
      "firstName": "Ian",
      "lastName": "Baracskay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80207,
      "firstName": "Dinh",
      "lastName": "Phung",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80208,
      "firstName": "Ryo",
      "lastName": "Shirai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80209,
      "firstName": "Atsushi",
      "lastName": "Ito",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80210,
      "firstName": "Shany",
      "lastName": "Funk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80211,
      "firstName": "Atsushi",
      "lastName": "Fukayama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80212,
      "firstName": "Einat",
      "lastName": "Kodesh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80213,
      "firstName": "Duong",
      "lastName": "Le",
      "middleInitial": "Minh",
      "affiliations": []
    },
    {
      "id": 80214,
      "firstName": "Juan",
      "lastName": "Ye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80215,
      "firstName": "Chinaemere",
      "lastName": "Ike",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80216,
      "firstName": "Xiaru",
      "lastName": "Meng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80217,
      "firstName": "Abhinav",
      "lastName": "Bohra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80218,
      "firstName": "Nhan",
      "lastName": "Do",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80219,
      "firstName": "Fumio",
      "lastName": "Nihei",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80220,
      "firstName": "Thomas",
      "lastName": "Lafrance",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80221,
      "firstName": "Abhishek",
      "lastName": "Mukhopadhyay",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80222,
      "firstName": "Donald J Baracskay",
      "lastName": "III",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80224,
      "firstName": "Juling",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80225,
      "firstName": "Florian",
      "lastName": "Lehmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80226,
      "firstName": "Harrison",
      "lastName": "Fah",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80227,
      "firstName": "Noriaki",
      "lastName": "Kuwahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80228,
      "firstName": "Mehtab",
      "lastName": "Iqbal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80229,
      "firstName": "Matthew",
      "lastName": "Lee",
      "middleInitial": "L",
      "affiliations": []
    },
    {
      "id": 80336,
      "firstName": "Simone",
      "lastName": "Stumpf",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80337,
      "firstName": "Rafael",
      "lastName": "Calvo",
      "middleInitial": "A",
      "affiliations": []
    },
    {
      "id": 80338,
      "firstName": "Carina",
      "lastName": "Negreanu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80339,
      "firstName": "Katrin",
      "lastName": "Hänsel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80340,
      "firstName": "Mary Lou",
      "lastName": "Maher",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80341,
      "firstName": "Cataldo",
      "lastName": "Musto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80342,
      "firstName": "Brian",
      "lastName": "Lim",
      "middleInitial": "Y",
      "affiliations": []
    },
    {
      "id": 80343,
      "firstName": "Ernesto William",
      "lastName": "De Luca",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80344,
      "firstName": "Erasmo",
      "lastName": "Purificato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80345,
      "firstName": "Hendrik",
      "lastName": "Strobelt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80346,
      "firstName": "Pasquale",
      "lastName": "Lops",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80347,
      "firstName": "Marko",
      "lastName": "Tkalcic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80348,
      "firstName": "Styliani",
      "lastName": "Kleanthous Loizou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80349,
      "firstName": "Tobias",
      "lastName": "Kowatsch",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80350,
      "firstName": "Avital",
      "lastName": "Shulner-Tal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80351,
      "firstName": "Bruce",
      "lastName": "Ferwerda",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80352,
      "firstName": "Michael",
      "lastName": "Sobolev",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80353,
      "firstName": "David",
      "lastName": "Bau",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80354,
      "firstName": "Fabio",
      "lastName": "Gasparetti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80355,
      "firstName": "Lydia",
      "lastName": "Chilton",
      "middleInitial": "B",
      "affiliations": []
    },
    {
      "id": 80356,
      "firstName": "Giuseppe",
      "lastName": "Sansonetti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80357,
      "firstName": "Panagiotis",
      "lastName": "Germanakos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80358,
      "firstName": "Cristina",
      "lastName": "Gena",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 80374,
      "firstName": "Munmun",
      "lastName": "De Choudhury",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Georgia Tech"
        }
      ]
    },
    {
      "id": 80375,
      "firstName": "Giulio",
      "lastName": "Jacucci",
      "affiliations": [
        {
          "country": "Finland",
          "institution": "University of Helsinki"
        }
      ]
    },
    {
      "id": 80376,
      "firstName": "Ana",
      "lastName": "Paiva",
      "affiliations": [
        {
          "country": "Portugal",
          "institution": "Instituto Superior Técnico"
        }
      ]
    },
    {
      "id": 80377,
      "firstName": "Samuel",
      "lastName": "Kaski",
      "affiliations": [
        {
          "country": "Finland",
          "institution": "Aalto University"
        },
        {
          "country": "United Kingdom",
          "institution": "University of Manchester"
        }
      ]
    },
    {
      "id": 80378,
      "firstName": "Cristina",
      "lastName": "Conati",
      "affiliations": [
        {
          "country": "Canada",
          "institution": "University of British Columbia"
        }
      ]
    },
    {
      "id": 80379,
      "firstName": "Giuseppe",
      "lastName": "Riccardi",
      "affiliations": [
        {
          "country": "Italy",
          "institution": "University of Trento"
        }
      ]
    },
    {
      "id": 80380,
      "firstName": "Kenney",
      "lastName": "Ng",
      "affiliations": [
        {
          "country": "USA",
          "institution": "IBM Research Cambridge"
        }
      ]
    },
    {
      "id": 80381,
      "firstName": "David",
      "lastName": "Coyle",
      "affiliations": [
        {
          "country": "Ireland",
          "institution": "University College Dublin"
        }
      ]
    },
    {
      "id": 80382,
      "firstName": "Chiara",
      "lastName": "Longoni",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Boston University"
        }
      ]
    },
    {
      "id": 80383,
      "firstName": "Rosa",
      "lastName": "Baños",
      "affiliations": [
        {
          "country": "Spain",
          "institution": "University of Valencia"
        }
      ]
    },
    {
      "id": 80384,
      "firstName": "Arindam",
      "lastName": "Ghosh",
      "affiliations": [
        {
          "country": "Germany",
          "institution": "Oviva AG"
        }
      ]
    },
    {
      "id": 80385,
      "firstName": "Stuart",
      "lastName": "Russell",
      "affiliations": [
        {
          "country": "USA",
          "institution": "University of California at Berkeley"
        }
      ]
    },
    {
      "id": 80391,
      "firstName": "Simone",
      "lastName": "Stumpf",
      "affiliations": [
        {
          "country": "United Kingdom",
          "institution": "University of London"
        }
      ]
    },
    {
      "id": 80392,
      "firstName": "Tuukka",
      "lastName": "Ruotsalo",
      "affiliations": [
        {
          "country": "Finland",
          "institution": "University of Helsinki"
        },
        {
          "country": "Denmark",
          "institution": "University of Copenhagen"
        }
      ]
    },
    {
      "id": 80393,
      "firstName": "Krzysztof",
      "lastName": "Gajos",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Harvard University"
        }
      ]
    },
    {
      "id": 80394,
      "firstName": "Ana-Maria",
      "lastName": "Popescu",
      "affiliations": []
    },
    {
      "id": 80395,
      "firstName": "Oren",
      "lastName": "Etzioni",
      "affiliations": []
    },
    {
      "id": 80396,
      "firstName": "Henry",
      "lastName": "Kautz",
      "affiliations": []
    },
    {
      "id": 80407,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80408,
      "firstName": "Tanya",
      "lastName": "Jonker",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80409,
      "firstName": "Hrvoje",
      "lastName": "Benko",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80432,
      "firstName": "Carlos",
      "lastName": "Duarte",
      "affiliations": [
        {
          "country": "Portugal",
          "city": "Lisboa",
          "institution": "Universidade de Lisboa"
        }
      ]
    },
    {
      "id": 80433,
      "firstName": "Yi-Chi",
      "lastName": "Liao",
      "affiliations": [
        {
          "country": "Finland",
          "city": "Aalto",
          "institution": "Aalto University"
        }
      ]
    },
    {
      "id": 80434,
      "firstName": "Alison",
      "lastName": "Renner",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Dataminr"
        }
      ]
    },
    {
      "id": 80435,
      "firstName": "Heinrich",
      "lastName": "Hussmann",
      "affiliations": [
        {
          "country": "Germany",
          "city": "Munich",
          "institution": "Ludwig-Maximilians-University Munich"
        }
      ]
    },
    {
      "id": 80436,
      "firstName": "Yong",
      "lastName": "Wang",
      "affiliations": [
        {
          "country": "Singapore",
          "city": "Singapore",
          "institution": "Singapore Management University"
        }
      ]
    },
    {
      "id": 80437,
      "firstName": "Eric",
      "lastName": "Whitmire",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80438,
      "firstName": "Ting",
      "lastName": "Zhang",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80439,
      "firstName": "Aakar",
      "lastName": "Gupta",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80440,
      "firstName": "Ben",
      "lastName": "Lafreniere",
      "affiliations": [
        {
          "country": "Canada",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80441,
      "firstName": "Anna",
      "lastName": "Yu",
      "affiliations": [
        {
          "country": "USA",
          "institution": "Meta Reality Labs Research"
        }
      ]
    },
    {
      "id": 80644,
      "firstName": "Joonatan",
      "lastName": "Jääskeläinen",
      "affiliations": [
        {
          "country": "Finland",
          "institution": "Avara Valmennus"
        }
      ]
    },
    {
      "id": 80653,
      "firstName": "Simone",
      "lastName": "Stumpf",
      "affiliations": [
        {
          "country": "United Kingdom",
          "city": "Glasgow",
          "institution": "University of Glasgow"
        }
      ]
    },
    {
      "id": 80661,
      "firstName": "Volodymyr",
      "lastName": "Muliar",
      "affiliations": []
    },
    {
      "id": 80662,
      "firstName": "Yaryna",
      "lastName": "Kvitka",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 51,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": true,
    "publicationDate": "2022-04-05 03:16:32+00"
  }
}