{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10001,
    "startDate": 1520380800000,
    "endDate": 1520726400000,
    "shortName": "IUI",
    "name": "IUI 2018",
    "year": 2018,
    "fullName": "The 23rd Annual Meeting of the Intelligent User Interfaces Community",
    "url": "http://iui.acm.org/2018/",
    "location": "Hitotsubashi Hall (National Center of Sciences Building), Tokyo, Japan",
    "timeZoneOffset": 540,
    "logoUrl": "https://files.sigchi.org/conference/logo/06b10a8a-bb4d-52df-fe2a-ade870569112.png",
    "timeZoneName": "Asia/Tokyo"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10001,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [
    {
      "id": 10002,
      "name": "2 Floor March 11",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/7fb2f2d4-a84d-b80d-52de-6f7601730977.png",
      "roomIds": [
        10005,
        10001,
        10004,
        10002,
        10003,
        10000
      ]
    },
    {
      "id": 10003,
      "name": "2 Floor March 8-10",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/432a05fe-84f8-4367-1e1b-29dffdebf239.png",
      "roomIds": [
        10015,
        10014,
        10007,
        10006,
        10008
      ]
    },
    {
      "id": 10004,
      "name": "2 Floor March 7",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/0d7620c2-b1f2-1f8b-11bf-c1c96a305509.png",
      "roomIds": [
        10009
      ]
    },
    {
      "id": 10005,
      "name": "1 Floor March 7",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/5af1378e-4d1b-4e02-31e1-4b11e14c25df.png",
      "roomIds": [
        10016,
        10010
      ]
    },
    {
      "id": 10007,
      "name": "Lunch Map 2/2",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/9fd7e383-1f9b-572b-7704-e0d80397d1b6.png"
    },
    {
      "id": 10008,
      "name": "Lunch Map 1/2",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/68c38118-a24e-5877-3c82-d05dfc9cdc07.png"
    },
    {
      "id": 10009,
      "name": "Demo Map",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/a945a0c0-5fb3-a406-dc69-4a002810b9a7.png"
    },
    {
      "id": 10010,
      "name": "Poster Map 2/2",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/6e3181ac-042d-cd95-05c4-281817509a76.png"
    },
    {
      "id": 10011,
      "name": "Poster Map 1/2",
      "mapImageUrl": "https://files.sigchi.org/conference/floor/79f77406-02d5-300a-ce99-d3dc90a2033f.png"
    }
  ],
  "rooms": [
    {
      "id": 10015,
      "name": "Hitotsubashi Hall building (Floor 2F)",
      "typeId": 10018,
      "setup": "Special"
    },
    {
      "id": 10016,
      "name": "Just inside the main entrance of Hitotsubashi Hall building (Floor 1F)",
      "typeId": 10018,
      "setup": "Special"
    },
    {
      "id": 10147,
      "name": "Ryogoku Happyakuyacho Hananomai, Edo Tokyo Museum Branch (buses leave at 18:15 from Hitotsubashi Hall)",
      "typeId": 10245,
      "setup": "Special"
    },
    {
      "id": 10005,
      "name": "201",
      "typeId": 10020,
      "setup": "Special"
    },
    {
      "id": 10001,
      "name": "202 & 203",
      "typeId": 10020,
      "setup": "Special"
    },
    {
      "id": 10010,
      "name": "Get your ticket at registration. Please select one of: Dining Cafe Esperia, Sakura Suisan or Good Morning Cafe",
      "typeId": 10245,
      "setup": "Special"
    },
    {
      "id": 10009,
      "name": "Hitotsubashi Hall",
      "typeId": 10226,
      "setup": "Special"
    },
    {
      "id": 10014,
      "name": "Lunch not provided, food options are close by",
      "typeId": 10221,
      "setup": "Special"
    },
    {
      "id": 10004,
      "name": "Mid-size room 1",
      "typeId": 10020,
      "setup": "Special"
    },
    {
      "id": 10007,
      "name": "Mid-size room 1-2",
      "typeId": 10226,
      "setup": "Special"
    },
    {
      "id": 10006,
      "name": "Mid-size room 1-4",
      "typeId": 10226,
      "setup": "Special"
    },
    {
      "id": 10002,
      "name": "Mid-size room 2",
      "typeId": 10020,
      "setup": "Special"
    },
    {
      "id": 10003,
      "name": "Mid-size room 3",
      "typeId": 10020,
      "setup": "Special"
    },
    {
      "id": 10008,
      "name": "Mid-size room 3-4",
      "typeId": 10226,
      "setup": "Special"
    },
    {
      "id": 10000,
      "name": "Mid-size room 4",
      "typeId": 10020,
      "setup": "Special"
    }
  ],
  "tracks": [
    {
      "id": 10004,
      "typeId": 10771
    },
    {
      "id": 10005,
      "typeId": 10838
    },
    {
      "id": 10006,
      "typeId": 10861
    },
    {
      "id": 10007,
      "typeId": 10868
    }
  ],
  "contentTypes": [
    {
      "id": 10011,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 10012,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 10013,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 10015,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 10016,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 10017,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 10018,
      "name": "Paper",
      "color": "#08519c",
      "duration": 20,
      "displayName": "Papers"
    },
    {
      "id": 10019,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 10020,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 10014,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 10221,
      "name": "break",
      "duration": 0
    },
    {
      "id": 10246,
      "name": "ceremony",
      "duration": 0
    },
    {
      "id": 10251,
      "name": "demo",
      "duration": 0
    },
    {
      "id": 10226,
      "name": "oral",
      "duration": 0
    },
    {
      "id": 10222,
      "name": "poster",
      "duration": 0
    },
    {
      "id": 10245,
      "name": "social",
      "duration": 0
    },
    {
      "id": 10244,
      "name": "student consortium",
      "duration": 0
    },
    {
      "id": 10838,
      "name": "IUI 2018 Long & Short Papers",
      "duration": 0
    },
    {
      "id": 10868,
      "name": "IUI 2018 Posters & Demos",
      "duration": 0
    },
    {
      "id": 10771,
      "name": "IUI 2018 Student Consortium",
      "duration": 0
    },
    {
      "id": 10861,
      "name": "TiiS",
      "duration": 0
    }
  ],
  "timeSlots": [
    {
      "id": 10002,
      "type": "BREAK",
      "startDate": 1520422800000,
      "endDate": 1520427600000
    },
    {
      "id": 10001,
      "type": "SESSION",
      "startDate": 1520422800000,
      "endDate": 1520442000000
    },
    {
      "id": 10003,
      "type": "SESSION",
      "startDate": 1520427600000,
      "endDate": 1520434800000
    },
    {
      "id": 10004,
      "type": "BREAK",
      "startDate": 1520434800000,
      "endDate": 1520436000000
    },
    {
      "id": 10005,
      "type": "SESSION",
      "startDate": 1520436000000,
      "endDate": 1520441400000
    },
    {
      "id": 10006,
      "type": "SESSION",
      "startDate": 1520443800000,
      "endDate": 1520452800000
    },
    {
      "id": 10007,
      "type": "SESSION",
      "startDate": 1520500500000,
      "endDate": 1520532000000
    },
    {
      "id": 10008,
      "type": "SESSION",
      "startDate": 1520501400000,
      "endDate": 1520505900000
    },
    {
      "id": 10009,
      "type": "BREAK",
      "startDate": 1520505900000,
      "endDate": 1520507700000
    },
    {
      "id": 10011,
      "type": "SESSION",
      "startDate": 1520507700000,
      "endDate": 1520513100000
    },
    {
      "id": 10013,
      "type": "SESSION",
      "startDate": 1520513100000,
      "endDate": 1520516700000
    },
    {
      "id": 10012,
      "type": "BREAK",
      "startDate": 1520513100000,
      "endDate": 1520517600000
    },
    {
      "id": 10014,
      "type": "SESSION",
      "startDate": 1520517600000,
      "endDate": 1520524800000
    },
    {
      "id": 10016,
      "type": "BREAK",
      "startDate": 1520524800000,
      "endDate": 1520526600000
    },
    {
      "id": 10018,
      "type": "SESSION",
      "startDate": 1520526600000,
      "endDate": 1520532900000
    },
    {
      "id": 10019,
      "type": "SESSION",
      "startDate": 1520586900000,
      "endDate": 1520590500000
    },
    {
      "id": 10020,
      "type": "SESSION",
      "startDate": 1520586900000,
      "endDate": 1520618400000
    },
    {
      "id": 10021,
      "type": "BREAK",
      "startDate": 1520590500000,
      "endDate": 1520592300000
    },
    {
      "id": 10023,
      "type": "SESSION",
      "startDate": 1520592300000,
      "endDate": 1520599500000
    },
    {
      "id": 10024,
      "type": "BREAK",
      "startDate": 1520599500000,
      "endDate": 1520604000000
    },
    {
      "id": 10026,
      "type": "SESSION",
      "startDate": 1520604000000,
      "endDate": 1520611200000
    },
    {
      "id": 10027,
      "type": "BREAK",
      "startDate": 1520611200000,
      "endDate": 1520613000000
    },
    {
      "id": 10028,
      "type": "SESSION",
      "startDate": 1520613000000,
      "endDate": 1520619300000
    },
    {
      "id": 10029,
      "type": "SESSION",
      "startDate": 1520619300000,
      "endDate": 1520632800000
    },
    {
      "id": 10069,
      "type": "SESSION",
      "startDate": 1520673300000,
      "endDate": 1520679600000
    },
    {
      "id": 10068,
      "type": "SESSION",
      "startDate": 1520673300000,
      "endDate": 1520699400000
    },
    {
      "id": 10070,
      "type": "BREAK",
      "startDate": 1520679600000,
      "endDate": 1520681400000
    },
    {
      "id": 10071,
      "type": "SESSION",
      "startDate": 1520681400000,
      "endDate": 1520687700000
    },
    {
      "id": 10073,
      "type": "BREAK",
      "startDate": 1520687700000,
      "endDate": 1520692200000
    },
    {
      "id": 10074,
      "type": "SESSION",
      "startDate": 1520692200000,
      "endDate": 1520700300000
    },
    {
      "id": 10076,
      "type": "SESSION",
      "startDate": 1520759700000,
      "endDate": 1520765100000
    },
    {
      "id": 10077,
      "type": "SESSION",
      "startDate": 1520759700000,
      "endDate": 1520782200000
    },
    {
      "id": 10082,
      "type": "BREAK",
      "startDate": 1520765100000,
      "endDate": 1520766000000
    },
    {
      "id": 10086,
      "type": "SESSION",
      "startDate": 1520766000000,
      "endDate": 1520771400000
    },
    {
      "id": 10089,
      "type": "BREAK",
      "startDate": 1520771400000,
      "endDate": 1520775000000
    },
    {
      "id": 10092,
      "type": "SESSION",
      "startDate": 1520775000000,
      "endDate": 1520780400000
    },
    {
      "id": 10095,
      "type": "BREAK",
      "startDate": 1520780400000,
      "endDate": 1520781300000
    },
    {
      "id": 10096,
      "type": "SESSION",
      "startDate": 1520781300000,
      "endDate": 1520786700000
    }
  ],
  "sessions": [
    {
      "id": 1815,
      "name": "Registration/Support Desk",
      "roomId": 10016,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10001
    },
    {
      "id": 2481,
      "name": "Lunch",
      "typeId": 10221,
      "roomId": 10014,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10002
    },
    {
      "id": 1723,
      "name": "Poster session",
      "typeId": 10222,
      "chairIds": [
        15897,
        8690,
        14919
      ],
      "contentIds": [
        5441,
        3750,
        2983,
        3895,
        4257,
        6329,
        4459,
        7825,
        6823,
        5657,
        4776,
        6762,
        7714,
        6571,
        4993,
        7192,
        2701,
        2800,
        3523,
        3622,
        3842,
        4091,
        6912,
        6562,
        4003,
        2875,
        4370,
        4313,
        3216,
        3989,
        5405,
        4081,
        3697,
        3388,
        4909,
        5458,
        6721,
        5113,
        3098,
        4836,
        6563,
        5817,
        3186,
        3509,
        8025
      ],
      "timeSlotId": 10003
    },
    {
      "id": 2243,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10004
    },
    {
      "id": 1880,
      "name": "Keynote \"From On Body to Out of Body User Experience\" by Prof. James A. Landay",
      "typeId": 10226,
      "roomId": 10009,
      "chairIds": [
        24886
      ],
      "contentIds": [],
      "timeSlotId": 10005
    },
    {
      "id": 1125,
      "name": "Welcome reception",
      "typeId": 10245,
      "roomId": 10010,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10006
    },
    {
      "id": 1086,
      "name": "Registration/Support Desk",
      "roomId": 10015,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10007
    },
    {
      "id": 2164,
      "name": "Conference Welcome and Keynote \"Intelligent Music Interfaces\" by Prof. Masataka Goto",
      "typeId": 10226,
      "roomId": 10006,
      "chairIds": [
        23211
      ],
      "contentIds": [],
      "timeSlotId": 10008
    },
    {
      "id": 1744,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10009
    },
    {
      "id": 1950,
      "name": "1A: Recommender Systems",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        12727
      ],
      "contentIds": [
        7826,
        6278,
        5950,
        5915,
        4321
      ],
      "timeSlotId": 10011
    },
    {
      "id": 1994,
      "name": "1B: Multimodal Interfaces",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        9026
      ],
      "contentIds": [
        5336,
        3576,
        4278,
        3342,
        7689
      ],
      "timeSlotId": 10011
    },
    {
      "id": 1675,
      "name": "Lunch",
      "typeId": 10221,
      "roomId": 10014,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10012
    },
    {
      "id": 2128,
      "name": "Becoming a Volunteer at ACM SIGCHI 12.45-13.45 (Bento provided, registration required)",
      "roomId": 10007,
      "chairIds": [
        14553
      ],
      "contentIds": [],
      "timeSlotId": 10013
    },
    {
      "id": 2204,
      "name": "2B: Modelling and Predicting User Behavior",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        13619
      ],
      "contentIds": [
        8156,
        5428,
        3128,
        6973,
        3906,
        4466
      ],
      "timeSlotId": 10014
    },
    {
      "id": 1262,
      "name": "2A: Evaluation of IUIs",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        24831
      ],
      "contentIds": [
        6499,
        4653,
        6556,
        5838,
        5236,
        5562
      ],
      "timeSlotId": 10014
    },
    {
      "id": 1106,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10016
    },
    {
      "id": 1059,
      "name": "3A: XAI: Explainable IUIs",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        16526
      ],
      "contentIds": [
        3669,
        5239,
        7669,
        6738,
        6798
      ],
      "timeSlotId": 10018
    },
    {
      "id": 1698,
      "name": "3B: Interactive Machine Learning and Analysis",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        24847
      ],
      "contentIds": [
        6926,
        5153,
        5786,
        5500,
        7313
      ],
      "timeSlotId": 10018
    },
    {
      "id": 2028,
      "name": "Keynote \"Surveillance or Support: When Personalization Turns Creepy\" by Prof. Jennifer Golbeck",
      "typeId": 10226,
      "roomId": 10006,
      "chairIds": [
        23228
      ],
      "contentIds": [],
      "timeSlotId": 10019
    },
    {
      "id": 1693,
      "name": "Registration/Support Desk",
      "roomId": 10015,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10020
    },
    {
      "id": 1806,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10021
    },
    {
      "id": 2452,
      "name": "4A: Information Retrieval and Search",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        15799
      ],
      "contentIds": [
        3796,
        5637,
        4583,
        7246,
        6119,
        4005
      ],
      "timeSlotId": 10023
    },
    {
      "id": 1847,
      "name": "4B: Persuasive and Assistive IUIs",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        19173
      ],
      "contentIds": [
        4011,
        7380,
        7925,
        2973,
        5802,
        2805,
        7287
      ],
      "timeSlotId": 10023
    },
    {
      "id": 2538,
      "name": "Lunch",
      "typeId": 10221,
      "roomId": 10014,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10024
    },
    {
      "id": 1251,
      "name": "5A: IUIs for Wearable, Mobile, and Ubiquitous Computing",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        22047
      ],
      "contentIds": [
        6612,
        2888,
        4825,
        4757,
        4465,
        8033,
        7116
      ],
      "timeSlotId": 10026
    },
    {
      "id": 1929,
      "name": "5B: Intelligent Visualization and Smart Environments",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        11069
      ],
      "contentIds": [
        8012,
        4683,
        5811,
        6839,
        5506,
        5709,
        4670
      ],
      "timeSlotId": 10026
    },
    {
      "id": 2407,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10027
    },
    {
      "id": 2339,
      "name": "Student consortium",
      "typeId": 10244,
      "chairIds": [
        22085,
        24884
      ],
      "contentIds": [
        5458,
        4836,
        3186,
        3509,
        6721,
        3098,
        8025,
        5113,
        6563,
        5817
      ],
      "timeSlotId": 10028
    },
    {
      "id": 1818,
      "name": "Conference dinner",
      "typeId": 10245,
      "roomId": 10147,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10029
    },
    {
      "id": 2284,
      "name": "Registration/Support Desk",
      "roomId": 10015,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10068
    },
    {
      "id": 1386,
      "name": "Impact Award & Town hall meeting",
      "typeId": 10246,
      "roomId": 10006,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10069
    },
    {
      "id": 1362,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10070
    },
    {
      "id": 1497,
      "name": "6B: Social Media and Reccomenders",
      "typeId": 10226,
      "roomId": 10007,
      "chairIds": [
        13712
      ],
      "contentIds": [
        5583,
        7585,
        5016,
        6141,
        5155,
        8142
      ],
      "timeSlotId": 10071
    },
    {
      "id": 1013,
      "name": "6A: IUIs for Complex Tasks",
      "typeId": 10226,
      "roomId": 10008,
      "chairIds": [
        14662
      ],
      "contentIds": [
        6016,
        7690,
        3974,
        3878,
        7218,
        2890
      ],
      "timeSlotId": 10071
    },
    {
      "id": 2167,
      "name": "Lunch",
      "typeId": 10221,
      "roomId": 10014,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10073
    },
    {
      "id": 1811,
      "name": "Demo session",
      "typeId": 10251,
      "chairIds": [
        15897
      ],
      "contentIds": [
        3170,
        7686,
        3951,
        4044,
        5473,
        7854,
        5126,
        4364,
        4021,
        5271,
        6106,
        4946,
        6502,
        3039,
        7172,
        7049,
        3344,
        7667,
        4749,
        6501,
        3764,
        5941,
        3424,
        5505,
        5769,
        6277,
        8124,
        4487,
        3164
      ],
      "timeSlotId": 10074
    },
    {
      "id": 2190,
      "name": "W4: HUMANIZE",
      "typeId": 10020,
      "roomId": 10000,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 1631,
      "name": "W2: MILC",
      "typeId": 10020,
      "roomId": 10001,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 1057,
      "name": "Registration/Support Desk",
      "roomId": 10015,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10077
    },
    {
      "id": 1387,
      "name": "W5: WII",
      "typeId": 10020,
      "roomId": 10002,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 2377,
      "name": "W3: ESIDA",
      "typeId": 10020,
      "roomId": 10003,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 2275,
      "name": "W7: ExSS",
      "typeId": 10020,
      "roomId": 10004,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 1918,
      "name": "W6: UISTDA",
      "typeId": 10020,
      "roomId": 10005,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10076
    },
    {
      "id": 2005,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10082
    },
    {
      "id": 2249,
      "name": "W4: HUMANIZE",
      "typeId": 10020,
      "roomId": 10000,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 1512,
      "name": "W2: MILC",
      "typeId": 10020,
      "roomId": 10001,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 1024,
      "name": "W6: UISTDA",
      "typeId": 10020,
      "roomId": 10005,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 2441,
      "name": "W7: ExSS",
      "typeId": 10020,
      "roomId": 10004,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 1328,
      "name": "W5: WII",
      "typeId": 10020,
      "roomId": 10002,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 2370,
      "name": "W3: ESIDA",
      "typeId": 10020,
      "roomId": 10003,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10086
    },
    {
      "id": 1145,
      "name": "Lunch",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10089
    },
    {
      "id": 1419,
      "name": "W7: ExSS",
      "typeId": 10020,
      "roomId": 10004,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10092
    },
    {
      "id": 2047,
      "name": "W2: MILC",
      "typeId": 10020,
      "roomId": 10001,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10092
    },
    {
      "id": 1709,
      "name": "W5: WII",
      "typeId": 10020,
      "roomId": 10002,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10092
    },
    {
      "id": 2487,
      "name": "W3: ESIDA",
      "typeId": 10020,
      "roomId": 10003,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10092
    },
    {
      "id": 1036,
      "name": "W1: SymCollab",
      "typeId": 10020,
      "roomId": 10000,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10092
    },
    {
      "id": 2037,
      "name": "Coffee break",
      "typeId": 10221,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10095
    },
    {
      "id": 2529,
      "name": "W7: ExSS",
      "typeId": 10020,
      "roomId": 10004,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10096
    },
    {
      "id": 1139,
      "name": "W3: ESIDA",
      "typeId": 10020,
      "roomId": 10003,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10096
    },
    {
      "id": 2388,
      "name": "W5: WII",
      "typeId": 10020,
      "roomId": 10002,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10096
    },
    {
      "id": 2002,
      "name": "W1: SymCollab",
      "typeId": 10020,
      "roomId": 10000,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10096
    },
    {
      "id": 2030,
      "name": "W2: MILC",
      "typeId": 10020,
      "roomId": 10001,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 10096
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 6912,
      "typeId": 10868,
      "title": "Muscle-Wire Glove: Pressure-Based Haptic Interface",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This work follows a novel approach to information presentation through haptics using muscle-wire. The muscle-wire is extremely lightweight, silent and does not require complex circuitry. Its property to shrink in length when an electric current is applied, can have many interesting applications, some of which we present as a proof of concept of our vision. In this paper, we describe our initial prototype and a first series of user tests that demonstrates the possibility of representing discrete and continuous informations.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 20028
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 16463
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 20300
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 10475
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3842,
      "typeId": 10868,
      "title": "Gaze Data Clustering and Analysis",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Eye-Gaze contains useful information, especially, unconscious intention of human. However, it is difficult to distinguish between noise and intentional or unconscious movement from the gaze data. Moreover, matching stimuli with the data is not easy since many Area of Interest (AoI) clustering algorithms generate unwanted information. Therefore, the gaze data analysis and utilization are still limited. The intentions in the gaze data  can be categorized as navigational and informational. Hence, different visualization technique and analysis should be applied for each intention. In this paper, we study gaze data analysis using various data processing techniques, such as cluster and filter with corresponding visualization.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Sejong University"
            }
          ],
          "personId": 21540
        },
        {
          "affiliations": [
            {
              "institution": "Sejong University"
            }
          ],
          "personId": 12620
        },
        {
          "affiliations": [
            {
              "institution": "Data Visualization Lab, Sejong University"
            }
          ],
          "personId": 24135
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7172,
      "typeId": 10868,
      "title": "Detecting Utterance Scenes of a Specific Person",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a system that detects the scene, where a specific speaker is speaking in the video, and displays the site as a heat map in the video's timeline. This system enables users to skip to the timeline they want to hear by detecting scenes in a drama, talk show, or discussion TV program, where a specific speaker is speaking. To detect a specific speaker's utterance, we develop a deep neural network (DNN) to extract only a specific speaker from the original sound source. We also implement the detection algorithm based on the output of the proposed DNN and the interface for displaying the detection result.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 8505
        },
        {
          "affiliations": [
            {
              "institution": "CSL"
            },
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24886
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 5637,
      "typeId": 10838,
      "title": "Creative Writing with a Machine in the Loop: Case Studies on Slogans and Stories",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "As the quality of natural language generated by AI systems improves, writing interfaces can support interventions beyond grammar-checking and spell-checking, such as suggesting content to spark new ideas. To explore the possibility of machine-in-the-loop creative writing, we performed case studies using two system prototypes, one for slogan writing and one for short story writing. Participants in our study were asked to write with a machine in the loop, or alone (control condition). They assessed their writing and experience through surveys and an open-ended interview. We collected additional assessments of the writing from Amazon Mechanical Turk crowdworkers. Our findings indicate that participants found the process fun and helpful and could envision use cases for future systems. At the same time, the suggestions do not necessarily lead to better written artifacts. We therefore suggest novel natural language models and design implications for better machine-in-the-loop creative writing systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 22217
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 17855
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 24813
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 8694
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 18426
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 7686,
      "typeId": 10868,
      "title": "Gesture-based Mobile Communication System Providing Side-by-side Shopping Feeling",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This demo is a gesture-based communication framework designed for an immersive joint shopping scenario. It focuses on enhancing the mobile human-to-human interaction between two geographically separated users: an in-house user and an in-store user. By exploring the uses of depth-based tracking techniques and the integration of head-mounted displays and the spherical camera, we (1) construct an immersive virtual shopping environment for the in-house user remaining in a house and (2) offer a novel way for the users to achieve a real-time gestural communication in the physical shopping environment which the in-store user stays in, while (3) the latter gets an augmented reality experience. Through this demo, both users could share a feeling that they go for shopping side by side in the same place.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "WASEDA University"
            }
          ],
          "personId": 15134
        },
        {
          "affiliations": [
            {
              "institution": "Rakuten Institute of Technology, Rakuten, Inc."
            }
          ],
          "personId": 11678
        },
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 24176
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 5126,
      "typeId": 10868,
      "title": "Search Interface for Deep Thinking",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Today's search engines such as Google are designed to provide \"correct\" answers to queries in the shortest time. However, in real life, we encounter issues for which no clear answers might exist. Such issues demand deep thought. We developed a search interface that supports deep thinking. A user inputs a proposition in the form of sentences. The system decomposes a sentence into components and searches the web using those components. Consequently, a user will obtain a related, but not an exact answer that might be effective to consider a proposition from various perspectives. Furthermore, the system searches any Idea Notes that the user might have saved when using the web service. Reviewing the results and collecting key phrases from them will encourage a user to think. We discuss the design and some user results.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Lifull Inc, Ltd."
            }
          ],
          "personId": 13464
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 7689,
      "typeId": 10838,
      "title": "Eye Gaze Controlled MFD for Military Aviation ",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Multi-Function Displays (MFD) are essential part of glass cockpit of modern military and civilian aircraft. They can display more information in less space than traditional analog displays. Interacting with MFDs is still now limited to a joystick system attached to throttle (called Target Designation System). Modern military aircrafts already explored touchscreen and voice activation system although moving hand in high G maneuvers is difficult and speech recognition system was not extensively tested for non-native English speakers. This paper explored using gaze controlled interface for MFD and proposed two algorithms based on hotspots and adaptable zooming for improving response times in a gaze controlled interface. Three user studies confirmed gaze controlled MFD can significantly reduce response times for big peripheral buttons compared to touchscreen in a head down configuration and compared to existing TDS in a head up configuration. Using our set up that consists of helmet, gloves and HOTAS used in existing military aircraft, participants could undertake pointing and selection tasks at an average time of 2.43 secs for Head Down Display and 1.9 secs for Head Up Display. This time estimation includes context switching from primary flying to secondary pointing task and significantly better than existing TDS (2.67 secs) and touchscreen (2.8 secs).",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Indian Institute of Science"
            }
          ],
          "personId": 24887
        }
      ],
      "sessionIds": [
        1994
      ],
      "eventIds": []
    },
    {
      "id": 7690,
      "typeId": 10838,
      "title": "Two Tools are Better Than One: Tool Diversity as a Means of Improving Aggregate Crowd Performance",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Crowdsourcing is a common means of collecting image segmentation training data for use in a variety of computer vision applications. However, designing accurate crowd-powered image segmentation systems is challenging because defining object boundaries in an image requires significant fine motor skills and hand-eye coordination, which makes tasks error-prone. Typically, special segmentation tools are created, and then answers from multiple workers are aggregated to generate more accurate results. However, individual tool designs can bias how people make mistakes, resulting in shared errors that remain even after aggregation. In this paper, we introduce a novel crowdsourcing workflow that leverages multiple tools for the same task to increase output accuracy by reducing systematic error biases introduced by the tools themselves. When a task can no longer be broken down to more tractable subtasks (the conventional approach taken by microtask workflows), our multi-tool approach can be used to improve accuracy further by assigning different tools to different workers. We present a series of studies that evaluate the feasibility of our multi-tool approach, and show that it is able to significantly improve aggregate accuracy in semantic image segmentation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 24820
        },
        {
          "affiliations": [
            {
              "institution": "university of Michigan"
            }
          ],
          "personId": 20308
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 14430
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 11158
        },
        {
          "affiliations": [
            {
              "institution": "KAIST"
            }
          ],
          "personId": 24828
        },
        {
          "affiliations": [
            {
              "institution": "University of Michigan"
            }
          ],
          "personId": 24883
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 4364,
      "typeId": 10868,
      "title": "User Behaviour Analysis in a Simulated IoT Augmented Space",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present a demo application aimed at supporting the research  \r\nin the field of tourism and mobility support in IoT augmented areas. \r\nThe application collects tourists' choices while browsing Points of \r\nInterest (POIs) descriptions through a map-based interface that simulates user movement\r\nbetween POIs.\r\nCollected observations serve two purposes: the computation and \r\ntesting of recommendation strategies for POIs (both for on-line and off-line studies); the generation of simulated users' behaviour under alternative \r\nscenario and context conditions (e.g., weather, or the presence of a novel POI).",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Free University of Bozen-Bolzano"
            }
          ],
          "personId": 23513
        },
        {
          "affiliations": [
            {
              "institution": "Fondazione Bruno Kessler"
            }
          ],
          "personId": 10933
        },
        {
          "affiliations": [
            {
              "institution": "Free University of Bozen-Bolzano"
            }
          ],
          "personId": 24837
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 6926,
      "typeId": 10838,
      "title": "AnchorViz: Facilitating Classifier Error Discovery through Interactive Semantic Data Exploration",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In supervised interactive machine learning, human knowledge about the target concept can be a powerful reference to build a concept classifier that is robust to unseen items in the real world. The main challenge lies in finding unlabeled items that can either help discover or refine subconcepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare subconcepts that are hard to recall. This paper presents AnchorViz, an interactive visualization that facilitates error discovery through semantic data exploration. By creating example-based anchors, users create a topology to spread data based on their similarity to the anchors and examine the inconsistencies between data points that are semantically related. The results from our user study show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 13111
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 22970
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 17215
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 20178
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 20050
        },
        {
          "affiliations": [
            {
              "institution": "Microsoft Research"
            }
          ],
          "personId": 18318
        }
      ],
      "sessionIds": [
        1698
      ],
      "eventIds": []
    },
    {
      "id": 3342,
      "typeId": 10838,
      "title": "Bio-adaptive social VR to evoke affective interdependence -  DYNECOM",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "The effects of bio-adaptation-based visual cues in conveying affective information was studied while conducting social meditation in a virtual reality (VR) environment. In a laboratory study, 22 dyads practiced a short duration, modified version of empathy-evoking compassion meditation. The immersive VR provided electroencephalography (EEG) -based real-time visual feedback, as changing color, on the user’s empathy-related brain activation; the breathing rate of the users was visualized as a movement cue. In addition, the synchronization of the feedback signals between the users was also visualized. Initial results suggest that the perceived affective interdependence, a component of social presence, was increased when both of the bio-adaptive visual feedbacks were presented to the users, compared to when there was no visual feedback. Also, the EEG-based visual feedback led to higher affective interdependence than the respiration based visual feedback. The findings suggest that affective contagion may occur by mediated adaptive cues that are based on physiological signals. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalto University School of Business"
            }
          ],
          "personId": 18770
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University School of Business"
            }
          ],
          "personId": 23686
        },
        {
          "affiliations": [
            {
              "institution": "Helsinki Institute for Information Technology HIIT, University of Helsinki"
            }
          ],
          "personId": 10077
        },
        {
          "affiliations": [
            {
              "institution": "Helsinki Institute for Information Technology HIIT, University of Helsinki"
            }
          ],
          "personId": 23360
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University School of Business"
            }
          ],
          "personId": 11570
        },
        {
          "affiliations": [
            {
              "institution": "Helsinki Collegium for Advanced Studies"
            },
            {
              "institution": "Aalto University School of Business"
            }
          ],
          "personId": 17273
        },
        {
          "affiliations": [
            {
              "institution": "Helsinki Institute for Information Technology HIIT, University of Helsinki"
            }
          ],
          "personId": 11069
        }
      ],
      "sessionIds": [
        1994
      ],
      "eventIds": []
    },
    {
      "id": 3344,
      "typeId": 10868,
      "title": "Ether-Toolbars: Evaluating Off-Screen Toolbars for Mobile Interaction",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In mobile interaction, the use of touchscreen interaction, while beneficial from the perspective of portability, has limited spatial accuracy due to the ``fat finger problem''. As a result, an important challenge on mobile interaction is to find solutions to balance the size of individual widgets against the number of widgets needed during interaction. In this work, to address display space limitations, we explore the design of invisible off-screen toolbars (ether-toolbars) that leverage computer vision to expand application features by placing widgets adjacent to the display screen. We demonstrate a prototype system consisting of an inexpensive 3D printed mount for mirror that supports ether-toolbar implementations. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Lille, CNRS"
            },
            {
              "institution": "ESTIA"
            }
          ],
          "personId": 18028
        },
        {
          "affiliations": [
            {
              "institution": "University of Lille, CNRS"
            }
          ],
          "personId": 24877
        },
        {
          "affiliations": [
            {
              "institution": "INRIA"
            },
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 24824
        },
        {
          "affiliations": [
            {
              "institution": "University of Lille, CNRS"
            }
          ],
          "personId": 22517
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 4370,
      "typeId": 10868,
      "title": "The Effects of Virtual Agents’ Characteristics on User Impressions and Language Use",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual agent (VA) interaction is becoming more pervasive in work and home life. We present a study of text-based VA-user interaction in the context of a real-world job interview. Two VAs were designed with distinct personalities and different genders. Virtual interviews were conducted by the VAs with 316 job applicants for an entry-level position at a financial firm. We investigated how different VA characteristics affect impression of, and interaction with, the VA. Results show that when gender of users and VAs were dissimilar, VAs were perceived more extroverted and personable. When genders were similar, users tended to rate VAs as more trustworthy. We also found differences in authenticity by users who interacted with different VAs. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of California, Irvine"
            }
          ],
          "personId": 18616
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Irvine"
            }
          ],
          "personId": 12989
        },
        {
          "affiliations": [
            {
              "institution": "University of California, Irvine"
            }
          ],
          "personId": 24882
        },
        {
          "affiliations": [
            {
              "institution": "Juji, Inc."
            }
          ],
          "personId": 24868
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7192,
      "typeId": 10868,
      "title": "Assessing Cognitive Workload on Printed and Electronic Media using Eye-Tracker and EDA Wristband",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "With the expansion of e-learning platforms, we receive a great opportunity to learn and study just using an electronic device. In this paper, we measured the differences in information processing on screen and paper with 18 participants using an eye-tracker and an EDA wristband. Our findings show that the media type has a significant influence on cognitive workload and understandability of the content. The results of this work are of vital importance for the design of new intelligent user interfaces and reveal the necessity to take mental processes of users more into account. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence (DFKI)"
            }
          ],
          "personId": 12307
        },
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence (DFKI)"
            }
          ],
          "personId": 22452
        },
        {
          "affiliations": [
            {
              "institution": "Osaka Prefecture University"
            }
          ],
          "personId": 22697
        },
        {
          "affiliations": [
            {
              "institution": "Osaka Prefecture University"
            }
          ],
          "personId": 18096
        },
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence"
            }
          ],
          "personId": 20067
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5657,
      "typeId": 10868,
      "title": "A Configurable and Contextually Expandable Interactive Picture Exchange Communication System (PECS) for Chinese Children with Autism",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "The electronic versions of PECS (picture exchange communication system) have been introduced to non-verbal children with autism spectrum disorder (ASD) in the past decade. In this paper, we discuss some related issues and propose the design of more versatile electronic PECS (ePECS) as a comprehensive language training tool. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Wenzhou-Kean University"
            }
          ],
          "personId": 18547
        },
        {
          "affiliations": [
            {
              "institution": "Wenzhou-Kean University"
            }
          ],
          "personId": 18137
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3098,
      "typeId": 10771,
      "title": "Leveraging User Input and Feedback for Interactive Sound Event Detection and Annotation",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 11606
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 5915,
      "typeId": 10838,
      "title": "FontMatcher: Font Image Paring for Harmonious Digital Graphic Design",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "One of the important aspects in graphic design is choosing the font of the caption that matches aesthetically the associated image. To obtain a good match, users would exhaustively examine a long font list requiring them a substantial effort. This paper presents FontMatcher, which supports users to design digital graphic works harmoniously pairing fonts with an image. The system provides three features, recommendation, explaination and feedback. If a warm feeling image is given as input, the system recommends warm feeling fonts, and then explains what is the distinguishing features of the recommendation, e.g. a cursive shape. Users can also provide feedback to find fonts which correspond to their intention. The evaluation results show that the recommended fonts scored better than selected fonts by novices and provides competing results with the ones chosen by experienced graphic designers. The explanations help increasing the reliability of the recommended results.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Graduate School of Interdisciplinary Information Studies"
            }
          ],
          "personId": 14904
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 12264
        },
        {
          "affiliations": [
            {
              "institution": "University of Trento"
            }
          ],
          "personId": 17470
        }
      ],
      "sessionIds": [
        1950
      ],
      "eventIds": []
    },
    {
      "id": 5405,
      "typeId": 10868,
      "title": "In Tandem: Exploring Interactive Opportunities for Dual Input and Output on Two Smartwatches",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We introduce, explore, and prototype in this work new interactive opportunities for dual input and output on two smartwatches that users wear on both hands and operate simultaneously. To this end, we present practical implementations of wearable applications designed for two smartwatches working \"in tandem\" that demonstrate new ways to perform  generic input and output tasks, such as item selection from a list, text entry on a tiny soft keyboard, and generic content manipulation on small screens. We hope that this work will draw the community's attention towards the richness of interactions that become possible by utilizing not one, but two smartwatches in tandem and, consequently, will inspire new and rich interface designs for the computers that we wear on our wrists.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stefan cel Mare University"
            }
          ],
          "personId": 8775
        },
        {
          "affiliations": [
            {
              "institution": "University of Suceava"
            }
          ],
          "personId": 9741
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5153,
      "typeId": 10838,
      "title": "Interactive Document Clustering Revisited: A Visual Analytics Approach",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Document clustering is an efficient way to get insight into large text collections. Due to the personalized nature of document clustering, even the best fully automatic algorithms cannot create clusters that accurately reflect the user's perspectives. To incorporate the user's perspective in the clustering process and, at the same time, effectively visualize document collections to enhance user's sense making of the data, we propose a novel visual analytics system for interactive document clustering. We built our system on top of clustering algorithms that are interactive in nature or can adapt to user's feedback. In the first step, the initial clustering is created based on the user-defined number of clusters and the selected clustering algorithm. In the second step, the clustering result is visualized to the user. A collection of coordinated visualization modules and document projection is designed to guide the user towards a better insight into the document collection and the clusters. The user changes clusters and key-terms iteratively as a feedback to the clustering algorithm until the result is satisfactory. In key-term based interaction, the user assigns a set of key-terms to each target cluster to guide the clustering algorithm. A set of quantitative experiments, a use case, and a user study have been conducted to show the advantages of the approach for document analytics based on clustering.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18773
        },
        {
          "affiliations": [],
          "personId": 13290
        },
        {
          "affiliations": [],
          "personId": 24867
        },
        {
          "affiliations": [],
          "personId": 17676
        }
      ],
      "sessionIds": [
        1698
      ],
      "eventIds": []
    },
    {
      "id": 7714,
      "typeId": 10868,
      "title": "Customizing User Experience with Adaptive Virtual Reality",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Virtual reality is becoming more prominent than ever. The improved performance of mobile devices, along with the growing interest in more immersive user experiences, have allowed virtual reality to gain solid standing in this regard: nowadays, a smartphone and a pair of VR goggles are all that is needed for a full VR experience. The uses of VR systems extend beyond home entertainment: in particular, we are tackling the possibility that VR can be used by therapists to aid patients suffering from neuro-developmental disorders (NDD). To that, an adaptive system that can change different aspects of the VR environment, using machine learning techniques, can go a long way in increasing the efficiency of NDD treatments.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Politecnico di Milano"
            }
          ],
          "personId": 24244
        },
        {
          "affiliations": [
            {
              "institution": "Politecnico di Milano"
            }
          ],
          "personId": 12612
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5155,
      "typeId": 10838,
      "title": "Can We Predict the Scenic Beauty of Locations from Geo-tagged Flickr Images?",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In this work, we propose a novel technique to determine the aesthetic score of a location from social metadata of Flickr photos. In particular, we build machine learning classifiers to predict the class of a location where each class corresponds to a set of locations having equal aesthetic rating. These models are trained on two empirically build datasets containing locations in two different cities (Rome and Paris) where aesthetic ratings of locations were gathered from \\textit{TripAdvisor.com}. In this work we exploit the idea that in a location with higher aesthetic rating, it is more likely for an user to capture a photo and other users are more likely to interact with that photo. Our models achieved as high as 79.48\\% accuracy (78.60\\% precision and 79.27\\% recall) on Rome dataset and 73.78\\% accuracy(75.62\\% precision and 78.07\\% recall) on Paris dataset. The proposed technique can facilitate urban planning, tour planning and recommending aesthetically pleasing paths.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bangladesh University of Engineering and Technology"
            }
          ],
          "personId": 9602
        },
        {
          "affiliations": [
            {
              "institution": "Bangladesh University of Engineering and Technology"
            }
          ],
          "personId": 21558
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    },
    {
      "id": 3622,
      "typeId": 10868,
      "title": "ActiveMap: Visual Analysis of Temporal Activity in Social Media Sites",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Social media users are becoming more active by the day. Tracking their temporal activity patterns along time demands high processing and large storage volume. Analyzing and understanding both the global activity characteristics and personal activity patterns for different users is challenging. We examine here a visualization method that draws a snapshot of the temporal activity, specifically of content generated by users. Our visualization enables comparisons between sites and within site activities for different periods while giving both a global and a comparative personal view in a single snapshot. This inner comparative view also highlights irregular activities. We demonstrate our findings with three different social media datasets obtained from Amazon, IMDb and Twitter.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Haifa"
            }
          ],
          "personId": 16195
        },
        {
          "affiliations": [
            {
              "institution": "University of Haifa"
            }
          ],
          "personId": 20926
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3878,
      "typeId": 10838,
      "title": "Quester: A Speech-based Question Answering Support System for Oral Presentations",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Current slideware, such as PowerPoint, reinforces the delivery of linear oral presentations. In settings such as question answering sessions or review lectures, more extemporaneous and dynamic presentations are required. An intelligent system that can automatically identify and display the slides most related to the presenter’s speech, allows for more speaker flexibility in sequencing their presentation. We present Quester, a system that enables fast access to relevant presentation content during a question answering session and supports nonlinear presentations led by the speaker. Given the slides' contents and notes, the system ranks presentation slides based on semantic closeness to spoken utterances, displays the most related slides, and highlights the corresponding content keywords in slide notes. The design of our system was informed by findings from interviews with expert presenters and analysis of recordings of lectures and conference presentations. In a within-subjects study comparing our dynamic support system with a static slide navigation system during a question answering session, presenters expressed a strong preference for our system and answered the questions more efficiently using our system.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Northeastern University"
            }
          ],
          "personId": 10465
        },
        {
          "affiliations": [
            {
              "institution": "Northeastern University"
            }
          ],
          "personId": 9930
        },
        {
          "affiliations": [
            {
              "institution": "Northeastern University"
            }
          ],
          "personId": 16083
        },
        {
          "affiliations": [
            {
              "institution": "Northeastern University"
            }
          ],
          "personId": 24846
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 4653,
      "typeId": 10838,
      "title": "An Interactive Relevance Feedback Interface for Evidence-Based Health Care",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "We design, implement and evaluate EpistAid, an interactive relevance feedback system to support physicians towards a more efficient citation screening process for Evidence Based Health Care (EBHC). The system combines a relevance feedback algorithm with an interactive interface inspired by Tinder-like swipe interaction. To evaluate its efficiency and effectiveness in the citation screening process we conducted a user study with real users (senior medicine students) using a large EBHC dataset (Epistemonikos), with around 400,000 documents. We compared two relevance feedback algorithms, Rocchio and BM25-based. The combination of Rocchio relevance feedback with the document visualization obtained the best recall and F-1 scores, which are the most important metrics for EBHC document screening. In terms of cognitive demand and effort, BM25 relevance feedback without visualization was perceived as needing more physical and cognitive effort. EpistAid has the potential of improving the process for answering clinical questions by reducing the time needed to classify documents, as well as promoting user interaction. Our results can inform the development of intelligent user interfaces for screening research articles in the clinical domain and beyond.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Pontificia Universidad Católica de Chile"
            }
          ],
          "personId": 24869
        },
        {
          "affiliations": [
            {
              "institution": "Pontificia Universidad Catolica de Chile"
            }
          ],
          "personId": 24875
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 4909,
      "typeId": 10868,
      "title": "BoTest: a Framework to Test the Quality of Conversational Agents Using Divergent Input Examples",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Quality of conversational agents is important as users have high expectations. Consequently, poor interactions may lead to the user abandoning the system. In this paper, we propose a framework to test the quality of conversational agents. Our solution transforms working input that the conversational agent accurately recognises to generate divergent input examples that introduce complexity and stress the agent.  As the divergent inputs are based on known utterances for which we have the `normal' outputs, we can assess how robust the conversational agent is to variations in the input. To demonstrate our framework we built ChitChatBot, a simple conversational agent capable of making casual conversation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 20966
        },
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 13243
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7218,
      "typeId": 10838,
      "title": "EASEL: Easy Automatic Segmentation Event Labeler",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Video annotation is a vital part of research examining gestural and multimodal interaction as well as computer vision, machine learning, and interface design. However, annotation is a difficult, time-consuming task that requires high cognitive effort. Existing tools for labeling and annotation still require users to manually label most of the data, limiting their helpfulness. In this paper, we present the Easy Automatic Segmentation Event Labeler (EASEL), a tool supporting gesture analysis. EASEL streamlines the annotation process by introducing assisted annotation, using automatic gesture segmentation and recognition to automatically annotate gestures. To evaluate the efficacy of assisted annotation, we conducted a user study with 24 participants and found that assisted annotation decreased the time needed to annotate videos with no difference in accuracy compared with manual annotation. The results of our study demonstrate the benefit of adding computational intelligence to video and audio annotation tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Florida"
            }
          ],
          "personId": 23172
        },
        {
          "affiliations": [
            {
              "institution": "Colorado State University"
            }
          ],
          "personId": 9126
        },
        {
          "affiliations": [
            {
              "institution": "University of Florida"
            }
          ],
          "personId": 20352
        },
        {
          "affiliations": [
            {
              "institution": "Colorado State University"
            }
          ],
          "personId": 12670
        },
        {
          "affiliations": [
            {
              "institution": "Colorado State University"
            }
          ],
          "personId": 23566
        },
        {
          "affiliations": [
            {
              "institution": "University of Florida"
            }
          ],
          "personId": 21215
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 5428,
      "typeId": 10838,
      "title": "A Data-Driven Approach to Developing IoT Privacy-Setting Interfaces",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "User testing is often used to inform the development of user interfaces (UIs). But what if an interface needs to be developed for a system that does not yet exist? In that case, existing datasets can provide valuable input for UI development. We apply a data-driven approach to the development of a privacy-setting interface for Internet-of-Things (IoT) \r\ndevices. Applying machine learning techniques to an existing dataset of users' sharing preferences in IoT scenarios, we develop a set of \"smart\" default profiles. Our resulting interface asks users to choose among these profiles, which capture their preferences with an accuracy of 82% - a 14% improvement over a naive default setting and a 12% improvement over a single smart default setting for all users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 24818
        },
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 24840
        },
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 8662
        },
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 24181
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 5941,
      "typeId": 10868,
      "title": "Frame of Mind: Using Storytelling for Speech-Based Clustering of Family Pictures",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Mobile technologies, like tablets, allow complete family picture collections to be accessed from anywhere on a single, portable device, but still do not support browsing and reminiscing from family pictures as one would from an album. This is especially a problem for older adults who are more motivated to share their memories, but often have less access to their physical pictures due smaller living spaces. Frame of Mind offers simple tablet interactions for family picture reminiscence. The app uses the implicit speech interaction of oral storytelling to automatically organize pictures into album-like sets from the prompted memories. Users can modify and filter the sets, but do not need to manually sort and organize their pictures. This simplifies the overall process of family picture interactions by leveraging one enjoyable aspect to automate a more effortful one.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 8410
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            },
            {
              "institution": "University of Toronto Mississauga"
            }
          ],
          "personId": 24885
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3895,
      "typeId": 10868,
      "title": "Egocentric Video Multi-viewer for Analyzing Skilled Behaviors based on Gaze Object",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In many intellectual tasks, efficient succession of human physical and sensory skills is a long-standing issue. In order to analyze skilled behaviors, a useful approach is to compare same task scenes among workers or days and to understand these differences. In this paper, we propose an egocentric scene classification method based on objects which the worker turned the gaze to and a multi-viewer for egocentric videos comparison. We have experimented on proposed classification method with videos of painting watercolor.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Nagoya University"
            }
          ],
          "personId": 21180
        },
        {
          "affiliations": [
            {
              "institution": "Nagoya University"
            }
          ],
          "personId": 11002
        },
        {
          "affiliations": [
            {
              "institution": "Nagoya University"
            }
          ],
          "personId": 23371
        },
        {
          "affiliations": [
            {
              "institution": "Nagoya University"
            }
          ],
          "personId": 23258
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3128,
      "typeId": 10838,
      "title": "To Draw or Not to Draw: Stroke-Hover Intent Recognition in Bare-hand Mid-Air Curve Input",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Drawing curves in mid-air with fingers is a fundamental task with applications to 3D sketching, geometric modeling, handwriting recognition, and authentication. Mid-air curve input is most commonly accomplished through explicit user input; akin to click-and-drag, the user may use a hand posture (e.g. pinch) or a button-press on an instrumented controller to express the intention to start and stop sketching. In this paper, we present a novel approach to recognize the user’s intention to draw or not to draw in a mid-air sketching task without the use of postures or controllers. For every new point recorded in the user’s finger trajectory, the idea is to simply classify this point as either hover or stroke. Our work is motivated by a behavioral study that demonstrates the need for such an approach due to the lack of robustness and intuitiveness while using hand postures and instrumented devices. We captured sketch data from users using a haptics device and trained multiple binary classifiers using  feature vectors based on the local geometric and motion profile of the trajectory. We present a systematic comparison of these classifiers and discuss the advantages of our approach to spatial curve input applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 19688
        },
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 12965
        },
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 17150
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 2875,
      "typeId": 10868,
      "title": "Pair Matching: Transdisciplinary Study for Introducing Computational Intelligence to Guide Dog Associations",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "There have been attempts to tackle visual impairment issues in Human-Computer Interaction (HCI). However, we can scarcely see HCI research outcomes are widely used by the blind. In this research, we aim to investigate new design possibilities as a transdisciplinary approach for guide dog training and education. We work with guide dog trainers, the blind, and guide dogs. In detail, we focus on matching guide dogs with visually impaired people. This work contributes to igniting research on designing interactive systems for the community of the visually impaired and also other human-animal-matching processes.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology "
            }
          ],
          "personId": 20916
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 8765
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3388,
      "typeId": 10868,
      "title": "Scenario Context v/s Framing and Defaults in Managing Privacy in Household IoT",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "The Internet of Things provides household device users with an ability to connect and manage numerous devices over a common platform. However, the sheer number of possible privacy settings creates issues such as choice overload. This article outlines a data-driven approach to understand how users make privacy decisions in household IoT scenarios. We demonstrate that users are not just influenced by the specifics of the IoT scenario, but also by aspects immaterial to the decision, such as the default setting and its framing.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 24840
        },
        {
          "affiliations": [
            {
              "institution": "Eindhoven University of Technology"
            }
          ],
          "personId": 11194
        },
        {
          "affiliations": [
            {
              "institution": "Clemson University"
            }
          ],
          "personId": 24818
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 6973,
      "typeId": 10838,
      "title": "Webcam-based attention tracking in Online Learning: A Feasibility Study",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "A main weakness of the Massive Open Online Learning movement is retention:  a small minority of learners (on average 5-10%, in extreme cases <1%) that start a MOOC complete it  successfully.   There  are  many  reasons  why  learners  are unsuccessful, among the most important ones is the lack of self-regulation: learners are often not able to self-regulate their learning behavior. Designing tools that provide learners with a\r\ngreater awareness of their learning is vital to the future success of MOOC environments.  Detecting learners’ loss of focus during learning is particularly important, as this can allow us to intervene and return the learners’ attention to the learning materials.  One technological affordance to detect such loss of focus are Web-cams—ubiquitous pieces of hardware available in almost all laptops today.  Recently, researchers have begun to make use of Web-cams as part of complex machine learning-based solutions to detect inattention or loss of focus based on eye tracking and eye gaze data. However, those approaches tend to have a high detection lag, are inaccurate, and are complex to design and maintain. In contrast, in this paper, we explore the possibility to make use of simple metrics such as gaze presence or face presence to detect a loss of focus in the online learning setting.  To this end, we evaluate the performance of three consumer and professional eye-tracking frameworks using a benchmark suite we designed specifically\r\nfor this purpose:  it contains a set of common xMOOC user activities and behaviours. The results of our study show that already those simple metrics pose a significant challenge to current hard- and software based eye-tracking solutions.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tallinn University of Technology"
            }
          ],
          "personId": 21772
        },
        {
          "affiliations": [],
          "personId": 20878
        },
        {
          "affiliations": [
            {
              "institution": "Delft University of Technology"
            }
          ],
          "personId": 10158
        },
        {
          "affiliations": [
            {
              "institution": "TU Delft"
            }
          ],
          "personId": 16405
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 5950,
      "typeId": 10838,
      "title": "Discovering Surprising Documents with Context-Aware Word Representations",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "User experiences can be made more engaging by incorporating\r\nsurprise. For example, online shoppers may like to view unique\r\nproducts. In this paper we propose an approach for detecting\r\nsurprising documents, such as product titles.  \r\nAs the concept of surprise is subjective, there is currently\r\nno principled method for measuring the surprisingness score of a\r\ndocument. We present such a method; an unsupervised approach for\r\nautomatically discovering surprising documents in an unlabeled\r\ncorpus. Our approach is based on a probabilistic model of surprise,\r\nand a construction of effective distributional word embeddings, which can be\r\nadapted to the semantic context in which the word appears. As \r\nthe performance of our model does not degrade with the length of the\r\ndocument, it is particularly well suited for very short\r\ndocuments (even a single sentence). We evaluate our model both in\r\nsupervised and unsupervised settings, demonstrating its\r\nstate-of-the-art performance on two real-world data sets:\r\na collection of e-commerce products from eBay, and a corpus\r\nof NSF proposals. These experiments show that our surprisingness score\r\nexhibits high correlation with human annotated labels. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "UC Santa Cruz"
            }
          ],
          "personId": 16464
        },
        {
          "affiliations": [
            {
              "institution": "eBay inc"
            }
          ],
          "personId": 9826
        },
        {
          "affiliations": [
            {
              "institution": "eBay inc"
            }
          ],
          "personId": 8257
        }
      ],
      "sessionIds": [
        1950
      ],
      "eventIds": []
    },
    {
      "id": 4670,
      "typeId": 10861,
      "title": "Active Learning and Visual Analytics for Stance Classification with ALVA",
      "trackId": 10006,
      "tags": [],
      "keywords": [],
      "abstract": "The automatic detection and classification of stance (e.g., certainty or agreement) in text data using natural language processing and machine-learning methods creates an opportunity to gain insight into the speakers’ attitudes toward their own and other people’s utterances. However, identifying stance in text presents many challenges related to training data collection and classifier training. To facilitate the entire process of training a stance classifier, we propose a visual analytics approach, called ALVA, for text data annotation and visualization. ALVA’s interplay with the stance classifier follows an active learning strategy to select suitable candidate utterances for manual annotaion. Our approach supports annotation process management and provides the annotators with a clean user interface for labeling utterances with multiple stance categories. ALVA also contains a visualization method to help analysts of the annotation and training process gain a better understanding of the categories used by the annotators. The visualization uses a novel visual representation, called CatCombos, which groups individual annotation items by the combination of stance categories. Additionally, our system makes a visualization of a vector space model available that is itself based on utterances. ALVA is already being used by our domain experts in linguistics and computational linguistics to improve the understanding of stance phenomena and to build a stance classifier for applications such as social media monitoring.",
      "authors": [
        {
          "affiliations": [],
          "personId": 22013
        },
        {
          "affiliations": [],
          "personId": 19862
        },
        {
          "affiliations": [],
          "personId": 22941
        },
        {
          "affiliations": [],
          "personId": 20126
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 6721,
      "typeId": 10771,
      "title": "Machine Learning Behavioral Recognition to Support Neuropsychological Diagnosis of Cognitive Decline",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 21519
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 5441,
      "typeId": 10868,
      "title": "Cursor Entropy Reveals Decision Fatigue",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recognizing impairments of the user´s ability to make goal-related decisions is an important feature for intelligent user interfaces, because this points to a higher need to support the user by the intelligent system. Here, we introduce using the entropy of cursor movements as an indicator of decision fatigue. We report an empirical proof-of-concept study that manipulates the amount of decision fatigue in participants.  The results show that cursor entropy is increased for people with higher decision fatigue than for people with lower decision fatigue. Thus, intelligent user interfaces become capable of detecting decision fatigue on the basis of cursor entropy and could recognize when users need assistance in their decision making. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Julius-Maximilians-Universität Würzburg"
            }
          ],
          "personId": 24841
        },
        {
          "affiliations": [
            {
              "institution": "Julius-Maximilians-Universität"
            }
          ],
          "personId": 24856
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3906,
      "typeId": 10838,
      "title": "User-adaptive Support for Processing Magazine Style Narrative Visualizations: Identifying User Characteristics that Matter",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we present results from an exploratory user study to uncover which user characteristics (e.g., perceptual speed, verbal working memory, etc.) play a role in how users process textual documents with embedded visualizations (i.e., Magazine Style Narrative Visualizations). We present our findings as a step toward developing user-adaptive support, and provide suggestions on how our results can be leveraged for creating a set of meaningful interventions for future evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "UBC"
            }
          ],
          "personId": 16609
        },
        {
          "affiliations": [
            {
              "institution": "UBC"
            }
          ],
          "personId": 24870
        },
        {
          "affiliations": [
            {
              "institution": "UBC"
            }
          ],
          "personId": 11121
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 2888,
      "typeId": 10838,
      "title": "Responsive news summarization for ubiquitous consumption on multiple mobile devices",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "With the proliferation of online news read on devices ranging from desktops to smart watches, the need for meaningful summaries of long texts is growing. Manual summaries are labour-intensive and cannot be offered for all display sizes, whereas today’s abstracts of most news texts are teasers designed to attract the reader’s interest more than to provide an overview of an article’s content suited to the reader’s information needs. We propose responsive news summarization as a technological approach for filling this gap. Responsive news summarization provides an automatically generated content summary that has the right length for the device requesting the article, plus access to the full text. We describe the system prototype available at multisizenews.com along with the initial user study results and give an outlook on future work.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Southern Denmark"
            }
          ],
          "personId": 12560
        },
        {
          "affiliations": [
            {
              "institution": "Ben-Gurion University of the Negev"
            }
          ],
          "personId": 18938
        },
        {
          "affiliations": [
            {
              "institution": "KU Leuven"
            }
          ],
          "personId": 12239
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 2890,
      "typeId": 10838,
      "title": "An Evaluation of Inclusive Dialogue-Based Interfaces for the Takeover of Control in Autonomous Cars",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents an evaluation of dialogue-based interfaces, which mediate the driver to take back vehicle control from the autonomous mode of a car. Four concepts designed to increase driver Situation Awareness were evaluated in a driving simulator. They used dialogue-based interaction, where driving-related information was either asked from or repeated by the driver, with the alternative of a countdown timer interface with no additional information. An inclusive set of participants, with a wide age spectrum, tested the interfaces with their performance and views recorded. The shorter and simpler interaction of the countdown timer was most accepted. The interface seeking answers to driving-related questions came next, and the interface requiring repetition of driving-related information, even when augmented by visual and tactile cues, was least accepted. Design guidelines on utilizing dialogue as an effective means of keeping the driver in the loop when taking control from an autonomous vehicle were thus derived.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Cambridge"
            }
          ],
          "personId": 24861
        },
        {
          "affiliations": [
            {
              "institution": "University of Cambridge"
            }
          ],
          "personId": 24844
        },
        {
          "affiliations": [
            {
              "institution": "University of Cambridge"
            }
          ],
          "personId": 18951
        },
        {
          "affiliations": [
            {
              "institution": "University of Cambridge"
            }
          ],
          "personId": 17837
        },
        {
          "affiliations": [
            {
              "institution": "University of Cambridge"
            }
          ],
          "personId": 12993
        },
        {
          "affiliations": [
            {
              "institution": "University of Southampton"
            }
          ],
          "personId": 10425
        },
        {
          "affiliations": [
            {
              "institution": "University of Southampton"
            }
          ],
          "personId": 19639
        },
        {
          "affiliations": [
            {
              "institution": "University of Southampton"
            }
          ],
          "personId": 15879
        },
        {
          "affiliations": [
            {
              "institution": "University of Southampton"
            }
          ],
          "personId": 9955
        },
        {
          "affiliations": [
            {
              "institution": "Jaguar Land Rover"
            }
          ],
          "personId": 24855
        },
        {
          "affiliations": [
            {
              "institution": "Jaguar Land Rover"
            }
          ],
          "personId": 24827
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 4683,
      "typeId": 10838,
      "title": "Coupling Story to Visualization: Using Textual Analysis as a Bridge Between Data and Interpretation",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Online writers and journalism media are increasingly combining visualization and text to create narrative visualizations.  Often, however, the two tasks are carried out independently of one another.  We propose an approach to automatically and deeply integrate text and visualization elements.  We begin with a writer's story that presumably can be supported through substantial data evidence.  We leverage natural language processing, quantitative narrative analysis, and information visualization to (1) automatically extract narrative components (who, what, when, where) from data-rich stories, and (2) integrate the supporting data evidence with the text to develop a narrative visualization.   We also introduce interactive deep coupling to facilitate bidirectional interaction from text to visualization and visualization to text.  We demonstrate the approach with a case study in the data-rich field of sports journalism.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 15765
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 17803
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 19840
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 15215
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 8012,
      "typeId": 10838,
      "title": "Organic Visualization of Document Evolution",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Recent availability of data of writing processes at keystroke-granularity  has enabled research on the evolution of document writing. A natural step is to develop systems that can actually show this data and make it understandable. Here we propose a data structure that captures a document’s fine-grained history and an organic visualization that serves as an interface to it. We evaluate a proof-of-concept implementation of the system through a pilot study with documents written by students at a public university. Our results are promising and reveal facets such as general strategies adopted, local edition density and  hierarchical structure of the final text.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Universidad de Chile"
            }
          ],
          "personId": 23160
        },
        {
          "affiliations": [
            {
              "institution": "Universidad de Chile"
            }
          ],
          "personId": 17802
        },
        {
          "affiliations": [
            {
              "institution": "Universidad del Desarrollo"
            },
            {
              "institution": "Telefonica R&D"
            }
          ],
          "personId": 11196
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 5709,
      "typeId": 10838,
      "title": "Familiarisation: Restructuring Layouts with Visual Learning Models",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In domains where users are exposed to large variations in visuo-spatial features among designs, they often spend excess time searching for common elements (features) in familiar locations. This paper contributes computational approaches to restructuring layouts such that features on a previously unvisited interface can be found quicker. We explore four concepts of familiarisation, inspired by the human visual system (HVS), to automatically generate a representative (familiar) design for each user. Given a history of previously visited interfaces, we use this computed design to restructure the spatial layout of a new (unfamiliar) interface, with the goal of making features more easily findable. Familiariser is a browser-based implementation that automatically restructures webpage lay- outs based on the visual history of the user. Our evaluation with users provides first evidence favouring familiarisation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            },
            {
              "institution": "Hasselt University - tUL - imec"
            }
          ],
          "personId": 24839
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 14385
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 24849
        },
        {
          "affiliations": [
            {
              "institution": "Hasselt University - tUL - imec"
            }
          ],
          "personId": 21330
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 7246,
      "typeId": 10838,
      "title": "A Study on User-Controllable Social Exploratory Search",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Information-seeking tasks with learning or investigative purposes are usually referred to as exploratory search. Exploratory search unfolds as a dynamic process where the user, amidst navigation, trial-and-error and on-the-fly selections, gathers and organizes information (resources). A range of innovative interfaces with increased user control have been developed to support exploratory search process. In this work we present our attempt to increase the power of exploratory search interfaces by using ideas of social search, i.e., leveraging information left by past users of information systems. Social search technologies are highly popular nowadays, especially for improving ranking. However, current  approaches to social ranking do not allow users to decide to what extent social information should be taken into account for result ranking. This paper presents an interface that integrates social search functionality into an exploratory search system in a user-controlled way that is consistent with the nature of exploratory search. The interface incorporates control features that allow the user to (i) express information needs by selecting keywords and (ii) to express preferences for incorporating social wisdom based on tag matching and user similarity. The interface promotes search transparency through color-coded stacked bars and rich tooltips. In an online study investigating system accuracy and subjective aspects with a structural model we found that, when users actively interacted with all its control features, the hybrid system outperformed a baseline content-based-only tool and users were more satisfied.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Know-Center GmbH"
            }
          ],
          "personId": 22404
        },
        {
          "affiliations": [
            {
              "institution": "University of Pittsburgh"
            }
          ],
          "personId": 24819
        },
        {
          "affiliations": [
            {
              "institution": "Know Center"
            }
          ],
          "personId": 24874
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 5458,
      "typeId": 10771,
      "title": "A Technology for Computer-Assisted Stroke Rehabilitation",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 24873
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 6738,
      "typeId": 10838,
      "title": "Opportunity Team Builder for Sales Teams",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Sellers work together as a team on sales opportunities, using their expertise in different roles to increase the probability of a win. These roles include managing the relationship with the client, overall architecture support or deep knowledge of a particular product depending on the seller’s expertise, and the current opportunity requirements. Forming the right team for an incoming opportunity is vital and depends on several factors including understanding the required roles for the opportunity and assigning the right person to fulfil these roles, taking into consideration the seller’s social network. In this paper, we present the Opportunity Team Builder solution, which supports sellers in this work by dividing the process into the following sub-tasks; identifying the required roles for the opportunity based on the products that the client is interested in, recommending the best people to fulfil these roles, and providing a win probability figure to guide users in team formation. This supports the sellers in forming the best- fitting team for current opportunity dynamics. Each task in the solution is implemented as a model using historical data from previous sales opportunities. Models work in coordination with each other to ultimately maximize the probability of win over loss. The solution not only recommends the best person to join a team taking into account a combination of inferred skills and social relationships, but also the predicted impact the person can have on the overall performance of the team. We present how the whole solution is realized with an intelligent user interface enabling interaction with the user throughout the team formation process. Substantial experiments with real world data show that win/loss prediction is performed accurately and the Opportunity Team Builder solution can recommend teams that achieve a higher win probability.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 24821
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 24847
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 8542
        }
      ],
      "sessionIds": [
        1059
      ],
      "eventIds": []
    },
    {
      "id": 4946,
      "typeId": 10868,
      "title": "Interactive Online Shopping with Personalized Robot Agent",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, the interaction between human and intelligent agents has become an increasingly hot topic. In this area, entertainment has always played a very important role as an essential element in enriching the user experience. At this research we combined Robohon, a new generation of smart robots, to act as a new type of agent to achieve a better shopping experience through interaction with personal computers and human, which we called as tripartite guiding system. A lot of voice and action is used as the main elements to interact with humans. In the process, the intelligent robot acts as a guide to help people with a better shopping experience step by step.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 9588
        },
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 18754
        },
        {
          "affiliations": [
            {
              "institution": "Rakuten Institute of Technology, Rakuten, Inc."
            }
          ],
          "personId": 11678
        },
        {
          "affiliations": [
            {
              "institution": "Waseda University"
            }
          ],
          "personId": 24176
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3669,
      "typeId": 10838,
      "title": "Bringing Transparency Design into Practice",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Intelligent systems, which are on their way to becoming mainstream in everyday products, make recommendations and decisions for users based on complex computations. Researchers and policy makers increasingly raise concerns regarding the lack of transparency and comprehensibility of these computations from a user's perspective. Our aim is to advance existing UI guidelines for more transparency in complex real-world design scenarios involving multiple stakeholders. To this end, we contribute a stage-based process for designing transparent interfaces incorporating perspectives of users, designers, and providers, which we developed and validated with a commercial intelligent fitness coach. We further suggest exemplary participatory methods for this process to facilitate its application. With our work, we hope to provide guidance to practitioners and to pave the way for more transparency in intelligent systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24850
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 9020
        },
        {
          "affiliations": [
            {
              "institution": "Freeletics"
            }
          ],
          "personId": 22643
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 15248
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 8754
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24848
        }
      ],
      "sessionIds": [
        1059
      ],
      "eventIds": []
    },
    {
      "id": 8025,
      "typeId": 10771,
      "title": "Facilitating self-learning in behavior change through long-term intelligent conversational assistance",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 23744
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 10253
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 3164,
      "typeId": 10868,
      "title": "SHARKZOR: Human in the Loop ML for User-Defined Image Classification",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Sharkzor is a web-based user interface for user defined image classification. We present here a human in the loop system with interactions focusing on 3 main user tasks. The user triages a number of images by organizing them into arbitrary groups with few examples. Sharkzor's sophisticated few-shot learning back end then approximates the user's mental model and automates organization of the entire dataset. \r\n\r\nDEMO: http://bit.ly/iuidemo",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 15161
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 8518
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 18017
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 11972
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 12062
        },
        {
          "affiliations": [
            {
              "institution": "Pacific Northwest National Lab"
            }
          ],
          "personId": 12583
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3424,
      "typeId": 10868,
      "title": "Olfactory and Visual Presentation Using Olfactory Display Using SAW Atomizer and Solenoid Valves",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This article introduces our demonstration content employing a desktop type olfactory display and a head mount display(HMD). An olfactory display is a device that delivers fragrance to one or more users. We developed the olfactory display using a surface acoustic wave (SAW) device and high-speed solenoid valves. Since the SAW device can atomize liquid droplets forcibly based on SAW streaming phenomenon, even low-volatile odor compound can be presented. By using high-speed solenoid valves, it is possible to perform quantitative control in multichannel precisely, so that olfactory display can produce various fragrances. By using this olfactory display together with the HMD, more realistic feeling can be given to the user. We have developed the content using both visual and olfactory cues. Through the HMD, people walk in the maze and get cocktail ingredients in to virtual environment. In the maze goal people can enjoy the scent from olfactory display.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 11673
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 14453
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 12125
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 18322
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 8033,
      "typeId": 10838,
      "title": "A Model for Detecting and Locating Behaviour Changes in Mobile Touch Targeting Sequences",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Touch offset models capture users' targeting behaviour patterns across the screen. We present and evaluate the first extension of these models to explicitly address behaviour changes. We focus on user changes in particular: Given only a series of touch/target locations (x, y), our model detects 1) if the user has changed therein, and if so, 2) at which touch. We evaluate our model on smartphone targeting and typing data from the lab (N=28) and field (N=30). The results show that our model can exploit touch targeting sequences to reveal user changes. Our model outperforms existing non-sequence touch offset models and does not require training data. We discuss the model's limitations and ideas for further improvement. We conclude with recommendations for its integration into future touch biometric systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24859
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 5473,
      "typeId": 10868,
      "title": "CardBot: A Chatbot for Business Card Management",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This article presents an application which combines chatbot and OCR techniques. The core value of CardBot is helping people manage business card cleverly and intuitively, and providing personalized services as a virtual assistant.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Industrial Technology Research Institute"
            }
          ],
          "personId": 20221
        },
        {
          "affiliations": [
            {
              "institution": "Industrial Technology Research Institute"
            }
          ],
          "personId": 11630
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3170,
      "typeId": 10868,
      "title": "Automatic Tweet Detection based on Data Specified through News Production",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Posts made by individuals on social media have become an indispensable source of information for existing media. In this paper, we describe an automatic Tweet detection system for supporting news production to acquire useful information from social media such as Twitter. The system uses character-based bi-directional long short-term memory with an attention mechanism and learns newsworthy Tweets specified by news producers as training data on a continuous basis. The performance of the system is expected to be improved by learning additional training data generated from its own operation history in news production site.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NHK"
            }
          ],
          "personId": 13853
        },
        {
          "affiliations": [
            {
              "institution": "NHK"
            }
          ],
          "personId": 20134
        },
        {
          "affiliations": [
            {
              "institution": "NHK"
            }
          ],
          "personId": 18041
        },
        {
          "affiliations": [
            {
              "institution": "NHK"
            }
          ],
          "personId": 19952
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 6499,
      "typeId": 10838,
      "title": "Cubicle: An Adaptive Educational Gaming Platform for Training Spatial Visualization Skills",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Research has demonstrated that spatial visualization skills are crucial for success in STEM disciplines. With an increasing number of students entering STEM disciplines, the question of how to effectively train students' spatial visualization skills has become very important. While a scalable existing solution is to implement online workshops for students, the problem of how to motivate students to participate in these online workshops remains unsolved. In this study, we studied gamification as a way to motivate first year engineering students to take part in an online workshop designed to train their spatial visualization skills. Our game contains eight modules, each designed to train a different component of spatial visualization. The game records players' in-game behavior with high granularity, which allows us to provide automated, scalable feedback on players' problem-solving strategies. Ten students with different levels of spatial ability played our game and expressed a strong interest in using the game to train their spatial visualization skills in the future. In addition, our analysis of players' in-game behaviors shows the potential benefits of implementing adaptive and personalized learning guidance.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 24857
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois Urbana-Champaign"
            }
          ],
          "personId": 21317
        },
        {
          "affiliations": [
            {
              "institution": "Zhejiang University"
            }
          ],
          "personId": 19217
        },
        {
          "affiliations": [
            {
              "institution": "College of Computer Science and Technology"
            }
          ],
          "personId": 23894
        },
        {
          "affiliations": [
            {
              "institution": "Shanghai Jiao Tong University"
            }
          ],
          "personId": 9666
        },
        {
          "affiliations": [
            {
              "institution": "IIIS"
            }
          ],
          "personId": 21971
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 15520
        },
        {
          "affiliations": [
            {
              "institution": "university of illinois"
            }
          ],
          "personId": 24823
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 6501,
      "typeId": 10868,
      "title": "(author)rise: Artificial Intelligence Output via the Human Body",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We are increasingly offloading a lot of our mental and subjective tasks to machines. Over the last few years, with advances in machine learning, especially in deep learning, machines are becoming increasingly capable of modeling and being more intimately involved in our everyday tasks. Though a lot of the tasks we do today are related with manipulating virtual data, our bodies still belong in a physical space and make up a large part of our activities. Artificial intelligence (AI) interactions are generally limited to on-screen or audio format, and therefore lack support in our physical tasks where our bodies play a major role. Also, being behind the screen and virtual in nature, we often do not acknowledge the extent to which AI plays a role in a lot of our subjective tasks. We present (author)rise, a physical human-machine intelligence based handwriting system, where machines generate handwritten text in continuation to human handwriting and move the human hand on the paper to write it out, thereby tightly coupling human and machine intelligence in the same physical input-output space. ",
      "authors": [
        {
          "affiliations": [],
          "personId": 17018
        },
        {
          "affiliations": [],
          "personId": 22237
        },
        {
          "affiliations": [],
          "personId": 24879
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 6502,
      "typeId": 10868,
      "title": "Development of a Tsunami Evacuation Behavior Simulation System with Massive Evacuation Agents",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We present an evacuation behavior simulation system at the time of tsunami using the evacuation agent of a game engine. The tsunami evacuation behavior simulation system is a system that enables local governments and residents to work together to formulate disaster prevention plans. As a representation of a simple tsunami, in this system, a slightly oblique plane is inserted as the sea surface to a three-dimensional model of the terrain, building, and road, which is generated by geographical information system data. Mass agents of evacuees randomly placed on the road will move toward the nearest wide evacuation shelter or tsunami evacuation building. As a result, problems were found in the shape of some roads and the position of evacuation buildings.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 21884
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 13390
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 19382
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 18253
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 22065
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 19408
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 6762,
      "typeId": 10868,
      "title": "Can You Help Me without Knowing Much? Exploring Cued-Knowledge Sharing for Instructors’ Tutorial Generation",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "How-to tutorials and videos are valuable for self-learning. While it is common for task instructors to produce tutorials alone, sharing knowledge when performing physical tasks without external support can be challenging to the experts. The resulting tutorials may also appear incomprehensible to learners who are not involved in the process of tutorial generation. We investigate cued-knowledge sharing, which pairs instructors with partners of different levels of expertise who also participate tutorial generation and give cues (prompts, questions etc.) to the instructors to facilitate their knowledge externalization. In a laboratory study, experienced cooks performed a cooking task and think aloud with cues from three types of paired partners, another experienced cook, a novice cook, or no partner. We noted that experts produced more clarifications when being paired with novice-partners than experienced-partners. The results demonstrate that (1) having a partner participate and provide cues may influence the content of knowledge sharing, and (2) even if the partner doesn't know much about the task (i.e., novice), their external cues remain helpful, which shed light on developing intelligent support for instructors' tutorial generation in the procedural and physical domains.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Institute of Information Systems and Applications"
            }
          ],
          "personId": 24881
        },
        {
          "affiliations": [
            {
              "institution": "Institute of Information Systems and Applications"
            }
          ],
          "personId": 9055
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4459,
      "typeId": 10868,
      "title": "A Recommender System based on Detected Users' Complaints by Analyzing Reviews",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Even though the popularity of e-commerce recommender systems continues to spread, traditional recommender systems might not recommend alternatives that can solve the problems of the items flagged by users. Since users need to choose options on existing services, e.g., screen quality and size, it remains difficult to satisfy their requirements. Therefore, we propose a novel item recommender system based on our analysis of complaint data and review comments on e-commerce. Our system first generates negative feature vectors from complaint data and positive feature vectors from review comments. Next, it calculates the similarities of these two vectors and identifies reviews that can solve the complaints about the items checked by users, and provides alternatives for each user complaint. In this paper, we describe our proposed recommendation method based on complaint and review data.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Kwansei Gakuin University Graduate School"
            }
          ],
          "personId": 13983
        },
        {
          "affiliations": [
            {
              "institution": "Yamaguchi University"
            }
          ],
          "personId": 13187
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto Sangyo University"
            }
          ],
          "personId": 24852
        },
        {
          "affiliations": [
            {
              "institution": "Kwansei Gakuin University"
            }
          ],
          "personId": 19623
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 3951,
      "typeId": 10868,
      "title": "Computer Vision Based and FPRank Based Tag Recommendation for Social Popularity Enhancement",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In previous work, we have proposed Folk Popularity Rank, which can score text tags based on their influence to images' popularity. In this work, we have implemented a tag recommendation system which can assist users in the tagging process. And we have proved that our recommended tags can help users gain higher popularity.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 11372
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 15062
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 9589
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24092
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 12264
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3697,
      "typeId": 10868,
      "title": "Dynamic Path Planning of Flying Projector Considering Collision Avoidance with Observer and Bright Projection",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we propose to compute a quadcopter-mounted projector's position so that the luminance of the projected image is maximized while the image covers the entire surface. We also compute a path of the flying projector to move close to the surface while avoiding the collision with an observer. In addition, the projected image is geometrically corrected to display undistorted image on a projection surface. They are solutions to inherent problems of the flying projector. First, a small quadcopter has a low payload and cannot carry a bright projector, because such projector is typically heavy.Second, there is a danger of collision between quadcopter and observer. We confirmed the effectiveness of our methods through an experiment.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Osaka university"
            }
          ],
          "personId": 13256
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 11384
        },
        {
          "affiliations": [
            {
              "institution": "Osaka university"
            }
          ],
          "personId": 11703
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4465,
      "typeId": 10838,
      "title": "Can a Helmet-mounted Display Make Motorcycling Safer?",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper we investigated whether a helmet-mounted display for motorcyclists can provide similar benefits as the established in-car version: Head-up displays inform the driver efficiently and increase driving safety. In regard of their high safety risks, such a technology seems overdue for motorcyclists. Only recently, several crowdfunding projects raised substantial funding, however, none of them has led to commercial products yet. \r\n\r\nWe developed an easy-to-reproduce and low-cost helmet-mounted display prototype and compared it with a conventional display setup. The helmet-mounted display was rated more attractive and induced a lower workload. Motorcyclists who generally exceeded the speed limit also rode significantly slower when using the head-mounted display. We argue that an intelligent application that adapts to the rider's behavior and preferences is the safest solution. We encourage other researchers to replicate our setup and to further investigate the impact of HUDs on the motorcyclists' behavior and road safety. \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Munich (LMU) "
            }
          ],
          "personId": 20601
        },
        {
          "affiliations": [
            {
              "institution": "University of Munich (LMU)"
            },
            {
              "institution": "IAV GmbH"
            }
          ],
          "personId": 18595
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24843
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 3186,
      "typeId": 10771,
      "title": "Automatic Recognition of Hygiene Activities and Personalized Interventions for Chronic Care",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 15536
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 4466,
      "typeId": 10838,
      "title": "Detecting Memory-Based Interaction Obstacles with a Recurrent Neural Model of User Behavior",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "A memory-based interaction obstacle is a condition which impedes human memory during Human-Computer Interaction, for example a memory-loading secondary task. In this paper, we present an approach to detect the presence of such memory-based interaction obstacles from logged user behavior during system use. For this purpose, we use a recurrent neural network which models the resulting temporal sequences. To acquire a sufficient number of training episodes, we employ a cognitive user simulation. We evaluate the approach with data from a user test and on which we outperform a non-sequential baseline by more than 23%.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Bremen"
            }
          ],
          "personId": 24826
        },
        {
          "affiliations": [
            {
              "institution": "University of Bremen"
            }
          ],
          "personId": 11191
        },
        {
          "affiliations": [
            {
              "institution": "University of Bremen"
            }
          ],
          "personId": 21819
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 5236,
      "typeId": 10838,
      "title": "Comparing Speech and Text Input in Interactive Narratives",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Intelligent user interfaces are finding new applications in interactive narratives, where players take on the role of a character in a fictional storyline. A recent example is the interactive audio narrative \"Traveler\", in which a combination of technologies for speech recognition and unsupervised text classification allow players to navigate a branching storyline via open-vocabulary spoken input. We hypothesize that the affordances of audio-based interaction in interactive narratives are different than text-based interaction, and that these differences change the player experience and their understanding of their fictional role. To test this hypothesis, we conducted a controlled experiment (n=39) to compare player interaction in \"Traveler\" with a text-only variant of the same storyline. We found significant differences in the types of input provided by players, suggesting that interaction modality impacts how players conceive of their relation to narrators of fictional storylines.",
      "authors": [
        {
          "affiliations": [],
          "personId": 12038
        },
        {
          "affiliations": [],
          "personId": 19403
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 7287,
      "typeId": 10838,
      "title": "The I in Team: Mining Personal Social Interaction Routine with Topic Models from Long-Term Team Data",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Social interactions play an important role in assessing individual and team process dynamics. They become particularly critical in team performance when coupled with isolated, confined, and extreme conditions such as undersea missions. This work investigates how social interactions of individual members in a small team evolve during the course of a long duration mission. We propose to use a topic model to mine individual social interaction patterns and examine how the dynamics of these patterns have an effect on self-assessment of mood and team cohesion. Specifically, we analyzed data from a 6-person crew wearing Sociometric badges over a 4-month mission. Our results show that our method can extract the latent structure of social contexts without supervision. We demonstrate how the extracted patterns based on probabilistic models can provide insights on common behaviors at various temporal resolutions and exhibit links with self-report team cohesion and affective states.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "TU Delft"
            }
          ],
          "personId": 10795
        },
        {
          "affiliations": [
            {
              "institution": "Michigan State University"
            }
          ],
          "personId": 13094
        },
        {
          "affiliations": [
            {
              "institution": "Michigan State University"
            }
          ],
          "personId": 21246
        },
        {
          "affiliations": [
            {
              "institution": "Michigan State University"
            }
          ],
          "personId": 20447
        },
        {
          "affiliations": [
            {
              "institution": "TU Delft"
            }
          ],
          "personId": 10430
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 5239,
      "typeId": 10838,
      "title": "Toward Foraging for Understanding of StarCraft Agents: An Empirical Study",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Assessing and understanding intelligent agents is a difficult task for users that lack an AI background. A relatively new area, called \"Explainable AI,\" is emerging to help address this problem, but little is known about how users would forage through information an explanation system might offer. To inform the development of Explainable AI systems, we conducted a formative study -- using the lens of Information Foraging Theory -- into how experienced users foraged in the domain of StarCraft to assess an agent. Our results showed that participants faced difficult foraging problems. These foraging problems caused participants to entirely miss events that were important to them, reluctantly choose to ignore actions they did not want to ignore, and bear high cognitive, navigation, and information costs to access the information they needed.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 17111
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 24816
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 21138
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 21755
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 11370
        },
        {
          "affiliations": [
            {
              "institution": "Oregon State University"
            }
          ],
          "personId": 22096
        }
      ],
      "sessionIds": [
        1059
      ],
      "eventIds": []
    },
    {
      "id": 5500,
      "typeId": 10838,
      "title": "User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human-machine interaction; however, fewer studies have addressed the potential defects the designs can cause.  Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 21344
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 17913
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 22613
        },
        {
          "affiliations": [
            {
              "institution": "Aalto University"
            }
          ],
          "personId": 13488
        }
      ],
      "sessionIds": [
        1698
      ],
      "eventIds": []
    },
    {
      "id": 6016,
      "typeId": 10861,
      "title": "Effects of Speed, Cyclicity, and Dimensionality on Distancing, Time, and Preference in Human-Aerial Vehicle Interactions",
      "trackId": 10006,
      "tags": [],
      "keywords": [],
      "abstract": "This article will present a simulation-based approach to testing multiple variables in the behavior of a small Unmanned Aerial Vehicle (sUAV), inspired by insect and animal motions, to understand how these variables impact time of interaction, preference for interaction, and distancing in Human-Robot Interaction (HRI). Previous work has focused on communicating directionality of flight, intentionality of the robot, and perception of motion in sUAVs, while interactions involving direct distancing from these vehicles have been limited to a single study (likely due to safety concerns). This study takes place in a Cave Automatic Virtual Environment (CAVE) to maintain a sense of scale and immersion with the users, while also allowing for safe interaction. Additionally, the two-alternative forced-choice method is employed as a unique methodology to the study of collocated HRI in order to both study the impact of these variables on preference and allow participants to choose whether or not to interact with a specific robot. This article will be of interest to end-users of sUAV technologies to encourage appropriate distancing based on their application, practitioners in HRI to understand the use of this new methodology, and human-aerial vehicle researchers to understand the perception of these vehicles by 64 naive users. Results suggest that low speed (by 0.27m, p < 0.02) and high cyclicity (by 0.28m, p < 0.01) expressions can be used to increase distancing; that low speed (by 4.4s, p < 0.01) and three-dimensional (by 2.6s, p < 0.01) expressions can be used to decrease time of interaction; and low speed (by 10.4%, p < 0.01) expressions are less preferred for passability in human-aerial vehicle interactions.",
      "authors": [
        {
          "affiliations": [],
          "personId": 17560
        },
        {
          "affiliations": [],
          "personId": 19010
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 5505,
      "typeId": 10868,
      "title": "VocabChecker: Measuring Language Abilities for Detecting Early Stage Dementia",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, dementia patients have been increasing in number worldwide, necessitating the development of techniques to detect dementia as early as possible.\r\nConsidering that a typical symptom of dementia, especially Alzheimer’s disease, is language impairment, speech-based dementia detection approaches have drawn much attention.\r\nThis paper presents a smartphone-based dementia screening application, VocabChecker, which measures language abilities from a speech narrative via automatic speech recognition (ASR).\r\nIt measures four language abilities related to dementia:\r\nnumber of tokens (token), number of types (type), type token ratio (TTR), and potential vocabulary size (PVS).\r\nWe also reported that the use of VocabChecker has clearly distinguished dementia patients from elderly people.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 16557
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 24845
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 18109
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo University of Science, Suwa"
            }
          ],
          "personId": 23981
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 21451
        },
        {
          "affiliations": [
            {
              "institution": "Nara Institute of Science and Technology"
            }
          ],
          "personId": 21826
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 4993,
      "typeId": 10868,
      "title": "Omni-Gesture: A Hand Gesture Interface for Omnidirectional Devices",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We present “Omni-Gesture”, a new omnidirectional hand gesture interface for smart interactive devices. Unlike voice user interface, this interface recognizes the direction of a user’s gesture and provides visual feedback accordingly. We developed an experimental prototype system with Omni-Gesture in order to explore it in usability. The system has a cylindrical screen, implementing an interactive aquarium application. Omni-Gesture provides more accurate feedbacks for users in three stages. First, the system recognizes the gesture by analyzing camera images. Second, it integrates the recognized data into direction information. Last, it identifies the user’s control between “Access” and “Shift”, and then it provides feedback toward the direction. Through the preliminary study, we found that using Omni-Gesture for omnidirectional devices is promising if the recognition speed can be improved, especially when moving. We also discuss future work for developing various gesture sets to use alongside other interfaces and applying Omni-Gesture to other omnidirectional platforms.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "KIST"
            }
          ],
          "personId": 16214
        },
        {
          "affiliations": [
            {
              "institution": "KIST"
            }
          ],
          "personId": 20837
        },
        {
          "affiliations": [
            {
              "institution": "KIST"
            }
          ],
          "personId": 11964
        },
        {
          "affiliations": [
            {
              "institution": "KIST"
            }
          ],
          "personId": 21683
        },
        {
          "affiliations": [
            {
              "institution": "KIST"
            }
          ],
          "personId": 16889
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5506,
      "typeId": 10838,
      "title": "An Active Tangible User Interface Framework  for Teaching and Learning Artificial Intelligence",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Interactive and tangible computing platforms have garnered increased interest in the pursuit of embedding active learning pedagogies within curricula through educational technologies. Whilst Tangible User Interface (TUI) systems have successfully been developed to edutain children in various research, TUI architectures have seen limited deployment in more complex and abstract domains. In light of these limitations, this paper proposes an active TUI framework that addresses the challenges experienced in teaching and learning artificial intelligence (AI) within higher educational institutions. The proposal extends an aptly designed tabletop TUI architecture with the novel interactive paradigm of active tangible manipulatives to provide a more engaging and effective user interaction. The paper describes the deployment of the proposed TUI framework within an undergraduate laboratory session to aid in the teaching and learning of artificial neural networks. The experiment is assessed against currently adopted educational computer software and the obtained results highlight the potential of the proposed TUI framework to augment students’ gain in knowledge and understanding of abstracted threshold concepts in higher education.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Middlesex University Malta"
            }
          ],
          "personId": 16826
        },
        {
          "affiliations": [
            {
              "institution": "Middlesex University"
            }
          ],
          "personId": 23867
        },
        {
          "affiliations": [
            {
              "institution": "Middlesex University"
            }
          ],
          "personId": 24214
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 6277,
      "typeId": 10868,
      "title": "Language Density Driven Route Navigation System for Pedestrians based on Twitter Data",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Navigation systems often aid tourists traveling in unfamiliar cities. This work presents a novel navigation system for suggesting routes based on traveler density estimated from Twitter. We aim to recommend two types of routing: one is for tourist spots on a route frequently visited by large groups of tourists, and the other one is for suggesting a route with a small number of expected tourists. Our method is based on shallow processing of tweets and can be used in online applications. We demonstrate online route navigation system that recommends routes based on diversity of foreign languages spoken in two touristically popular cites: San Francisco and Kyoto.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Yamaguchi University"
            }
          ],
          "personId": 13187
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 24073
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto Sangyo University"
            }
          ],
          "personId": 19753
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto Sangyo University"
            }
          ],
          "personId": 24852
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 24834
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3974,
      "typeId": 10838,
      "title": "Visualizing Gaze Direction to Support Video Coding of Social Attention for Children with Autism Spectrum Disorder",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents a novel interface to support video coding of social attention in assessment for children with Autism Spectrum Disorder. Video-based evaluation of social attention in therapeutic activity poses efforts for observers to find target behaviors while handling the ambiguity of attention. Despite the recent advances in computer vision-based gaze estimation methods, fully automatic recognition of social attention under diverse environments is still challenging. The goal of this work is to investigate an approach using automatic video analysis in a supportive manner for guiding human judgement. The proposed interface displays visualization of gaze estimation results on videos, and provides GUI supports to allow users to define social attention labels in the video timeline for facilitating agreements within observers. Throughout user studies and expert reviews, we show how the interface helps observers to perform video coding of social attention and how it can be improved for more efficient support.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 15409
        },
        {
          "affiliations": [
            {
              "institution": "Tsukuba University"
            }
          ],
          "personId": 22003
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24836
        },
        {
          "affiliations": [
            {
              "institution": "Keio University"
            }
          ],
          "personId": 11922
        },
        {
          "affiliations": [
            {
              "institution": "Osaka University"
            }
          ],
          "personId": 24196
        },
        {
          "affiliations": [
            {
              "institution": "Keio Unievrsity"
            }
          ],
          "personId": 22353
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24858
        }
      ],
      "sessionIds": [
        1013
      ],
      "eventIds": []
    },
    {
      "id": 6278,
      "typeId": 10838,
      "title": "Ensemble Recommendations via Thompson Sampling: an Experimental Study within e-Commerce",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "This work presents an extension of Thompson Sampling bandit policy for orchestrating the collection of base recommendation algorithms for e-commerce. We focus on the problem of item-to-item recommendations, for which multiple behavioral and attribute-based predictors are provided to an ensemble learner. We show how to adapt Thompson Sampling to realistic situations when neither action availability nor reward stationarity is guaranteed. Furthermore, we investigate the effects of priming the sampler with pre-set parameters of reward probability distributions by utilizing the product catalog and/or event history, when such information is available. We report our experimental results based on the analysis of three real-world e-commerce datasets.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Malmö University"
            }
          ],
          "personId": 14996
        },
        {
          "affiliations": [
            {
              "institution": "Malmö University"
            }
          ],
          "personId": 20656
        },
        {
          "affiliations": [
            {
              "institution": "Apptus Technologies AB"
            }
          ],
          "personId": 18761
        },
        {
          "affiliations": [
            {
              "institution": "Apptus Technologies AB"
            }
          ],
          "personId": 19405
        }
      ],
      "sessionIds": [
        1950
      ],
      "eventIds": []
    },
    {
      "id": 4487,
      "typeId": 10868,
      "title": "Fluid UI for HIGH-dimensional Analysis of Social Networks",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": " Social Viewpoint Finder (SVF) is a visual analytics tool for social networks and complex networks. SVF lays out the input network data on a HIGH-dimensional (500-3,000 dimensional) Euclidean space, offers a simple, but unique dragging-based UI to trigger computation-intensive, high-dimensional rotation of the presented network, and let the user investigate the clustering structure of the network. This demonstration presents the effectiveness of SVF as well as the employed implementation techniques that enabled the fluid, complex user interaction. To achieve fluidness, SVF heavily relies on modern OpenGL technologies. It achieves massively parallel computing through the use of the compute shader, and graphic pipelines. A large volume of the network data and its layout information is stored in a shader storage buffer. A fragment shader-based, efficient object identification method is devised.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 22406
        },
        {
          "affiliations": [
            {
              "institution": "Tokyo Institute of Technology"
            }
          ],
          "personId": 20361
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 7049,
      "typeId": 10868,
      "title": "Exploring the Universe of Egregious Conversations in Chatbots",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "As part of the raging commercial development of chatbot systems, the ability to improve the quality of conversations quickly and consistently is crucial. In this work, we focus on egregious conversations and the dialog failure points that lead to these extremely bad dialogs. We present a tool that helps chatbot designers exploring and prioritizing failure points that need to be handled. Each failure is accompanied with an explanation and a suggested remedy.   \r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 24830
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 8379
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 19851
        },
        {
          "affiliations": [
            {
              "institution": "IBM Cloud"
            }
          ],
          "personId": 12371
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 5769,
      "typeId": 10868,
      "title": "Visual Analytics of Organizational Performance Network",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this demo paper, we present an integrated visualization application with an interactive user interface for analyzing organizational performance. The aim of this application is to provide holistic view of the performance of multiple teams and help initiate focused actions to improve it. The data about team structures, members and, their achievements against goals during different performance periods is visualized through various interactive interfaces. Further, the application analyzes and highlights performance variations across different metrics, teams and, members to provide actionable insights. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Infosys Ltd."
            }
          ],
          "personId": 17191
        },
        {
          "affiliations": [
            {
              "institution": "Infosys Ltd."
            }
          ],
          "personId": 19306
        },
        {
          "affiliations": [
            {
              "institution": "Infosys Ltd."
            }
          ],
          "personId": 11028
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 2701,
      "typeId": 10868,
      "title": "Trusting Strangers in Immersive Virtual Reality",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Social interactions in immersive virtual reality (IVR) benefit from more realistic designed avatars whilst head mounted displays (HMD) are simultaneously offering virtual reality experiences with improving levels of immersion and presence. The combination of these developments creates a need to understand how users remit trust towards avatars in IVR.\r\nWe evaluated trust towards two categories of avatars (robot vs. human-like) in VR by conducting a lab study (N=21) where participants had to play a trust game (TG) with each avatar. Our findings highlight that although the trust game revealed equal trust levels towards both categories of avatars, participants felt a significant sense of \"togetherness\" with the human-like avatar compared to the robot.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 14387
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24850
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 8750
        },
        {
          "affiliations": [
            {
              "institution": "LMU Munich"
            }
          ],
          "personId": 24848
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4749,
      "typeId": 10868,
      "title": "Medical 3D Images in Multimodal Virtual Reality",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We present a multimodal medical 3D image system for radiologists in an virtual reality (VR) environment. Users can walk freely inside the virtual room and interact with the system using speech, going through patient records, and manipulate 3D image data with hand gestures. Medical images are retrieved from the hospital's Picture and Archiving System (PACS) and displayed as 3D objects inside VR. Our system incorporates a dialogue-based decision support system for treatments. A central supervised patient database provides input to our predictive model and allows us, first, to add new examination reports by a pen-based mobile application on-the-fly, and second, to get therapy prediction results in real-time. This demo includes a visualisation of real patient records, 3D DICOM radiology image data, and real-time therapy predictions in VR. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence (DFKI GmbH)"
            }
          ],
          "personId": 22913
        },
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence (DFKI)"
            }
          ],
          "personId": 12566
        },
        {
          "affiliations": [
            {
              "institution": "German Research Center for Artificial Intelligence (DFKI)"
            }
          ],
          "personId": 21953
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 6798,
      "typeId": 10838,
      "title": "Combining Brain-Computer Interface and Eye Tracking for High-Speed Text Entry in Virtual Reality",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Gaze interaction provides an efficient way for users to communicate and control in virtual reality (VR) presented by head-mounted displays. In gaze-based text-entry systems, eye tracking and brain-computer interface (BCI) are the two most commonly used approaches. This paper presents a hybrid BCI system for text entry in VR by combining steady-state visual evoked potentials (SSVEP) and eye tracking. The user interface in VR designed a 40-target virtual keyboard using a joint frequency-phase modulation method for SSVEP. Eye position was measured by an eye-tracking accessory in the VR headset. Target-related gaze direction was detected by combining simultaneously recorded SSVEP and eye position data. Offline and online experiments indicate that the proposed system can type at a speed around 10 words per minute, leading to an information transfer rate (ITR) of 270 bits per minute. The results further demonstrate the superiority of the hybrid method over single-modality methods for VR applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Chinese Academy of Sciences"
            },
            {
              "institution": "Institute of Semiconductors, Chinese Academy of Sciences"
            }
          ],
          "personId": 17245
        },
        {
          "affiliations": [
            {
              "institution": "University of Chinese Academy of Sciences"
            },
            {
              "institution": "Institute of Semiconductors, Chinese Academy of Sciences"
            }
          ],
          "personId": 14521
        },
        {
          "affiliations": [
            {
              "institution": "University of Chinese Academy of Sciences"
            },
            {
              "institution": "Institute of Semiconductors, Chinese Academy of Sciences"
            }
          ],
          "personId": 12202
        },
        {
          "affiliations": [
            {
              "institution": "University of Chinese Academy of Sciences"
            },
            {
              "institution": "Institute of Semiconductors, Chinese Academy of Sciences"
            }
          ],
          "personId": 20469
        },
        {
          "affiliations": [
            {
              "institution": "University of Chinese Academy of Sciences"
            },
            {
              "institution": "Institute of Semiconductors, Chinese Academy of Sciences"
            }
          ],
          "personId": 9622
        }
      ],
      "sessionIds": [
        1059
      ],
      "eventIds": []
    },
    {
      "id": 3216,
      "typeId": 10868,
      "title": "Personality-Aware Decision Making In Educational Learning",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Personality, as one of the human factors, has been demonstrated as an influential element in decision makings. Its impact in educational learning is still under investigation and there are very few of available data sets in this area. In this paper, we introduce one data that is collected from user studies, describe our exploratory analysis and discuss the corresponding research topics. Furthermore, we encourage more discussions or ideas about the data collection, experimental design, promising topics and research challenges.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Illinois Institute of Technology"
            }
          ],
          "personId": 12977
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7825,
      "typeId": 10868,
      "title": "Impression-based Fabrication: A Framework to Reflect Personal Preferences in the Fabrication Process",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This study proposes impression-based fabrication, a new framework of digital fabrication that takes into account personal preferences and impressions. Based on users’ desired impressions, the two-component framework generates multiple designs for a target product. The first component, a design generation unit, is where the designs are generated. It converts the input desired impressions into physical features. These designs are then subjected to an impression evaluation model where their appropriateness to the desired impression is evaluated. The user then receives the 3D printing data of high-scoring designs. A custom design tool for picture frames was constructed as a case study to demonstrate the feasibility of the framework. A case study confirmed that the design tool was able to create designs that had the desired impression on users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Yamanashi"
            }
          ],
          "personId": 11749
        },
        {
          "affiliations": [
            {
              "institution": "University of Yamanashi"
            }
          ],
          "personId": 14555
        },
        {
          "affiliations": [
            {
              "institution": "University of Yamanashi"
            }
          ],
          "personId": 16225
        },
        {
          "affiliations": [
            {
              "institution": "University of Yamanashi"
            }
          ],
          "personId": 22932
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7313,
      "typeId": 10838,
      "title": "Who is the hero, the villain, and the victim? Detection of roles in news articles using natural language techniques",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Many news articles use narrative frames to present people, organizations, and facts. These narrative frames follow cultural archetypes, where readers can associate each one of the presented elements with familiar stereotypes, well-known characters, and recognizable outcomes. In this way, authors can cast real people or organizations as heroes, villains, or victims. We present a system that identifies the main entities of the article, and it uses dictionaries based on fictional characters and sentiment analysis to determine when an entity is being cast as a hero, villain, or a victim. This system interacts with news consumers directly through a browser extension. Our hope is that by informing readers when an entity is cast in one of these roles, we can make implicit bias explicit, and assist readers in applying their media literacy skills.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Pontificia Universidad Católica de Chile"
            },
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 24829
        },
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 18582
        },
        {
          "affiliations": [
            {
              "institution": "Northwestern University"
            }
          ],
          "personId": 23958
        }
      ],
      "sessionIds": [
        1698
      ],
      "eventIds": []
    },
    {
      "id": 7826,
      "typeId": 10838,
      "title": "FocusMusicRecommender: A System for Recommending Music to Listen to While Working",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "This paper proposes FocusMusicRecommender, an automated system recommending background music to listen to while working. Recommendation systems matching user preferences have been widely researched even though research has shown that music that listeners strongly like is not suitable background music because it interferes with their concentration. FocusMusicRecommender plays songs that users may \"neither like nor dislike\" instead of \"like very much.\" It is designed to by default summarize a song automatically so that users can give \"like very much\" feedback by pressing a \"keep listening\" button or \"dislike very much\" feedback by pressing a \"skip\" button. It uses this feedback, along with users' concentration levels estimated from their behavior history, to distinguish between the preference levels \"like\" and \"like very much.\" It then estimates the preference levels of unplayed songs and selects the most suitable song by considering the user's current concentration level. The effectiveness of the proposed feedback method and suitability of the recommendation results were verified experimentally and in user studies.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Tsukuba"
            }
          ],
          "personId": 23779
        },
        {
          "affiliations": [
            {
              "institution": "AIST"
            }
          ],
          "personId": 22523
        },
        {
          "affiliations": [
            {
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)"
            }
          ],
          "personId": 20487
        }
      ],
      "sessionIds": [
        1950
      ],
      "eventIds": []
    },
    {
      "id": 3989,
      "typeId": 10868,
      "title": "Explaining Social Recommendations to Casual Users: Design Principles and Opportunities",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems have become popular in recent years, and ordinary users are more likely to rely on such service when completing various daily tasks. The need to design and build explainable recommender interfaces is increasing rapidly. Most of the designs of such explanations are intended to reflect the underlying algorithms by which the recommendations are computed. These approaches have been shown to be useful for obtaining system transparency and trust. However, little is known about how to design explanation interfaces for causal (non-expert) users to achieve different explanatory goals. As a first step toward understanding the user interface design factors, we conducted an international (across 13 countries) online survey of 14 active users of a social recommender system. This study captures user feedback in the field and frames it in terms of design principles and opportunities. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Pittsburgh"
            }
          ],
          "personId": 24865
        },
        {
          "affiliations": [
            {
              "institution": "University of Pittsburgh"
            }
          ],
          "personId": 24819
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4757,
      "typeId": 10838,
      "title": "Investigating Interactions for Text Recognition using a Vibrotactile Werable Display",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Vibrotactile skin-reading uses wearable vibrotactile displays to convey dynamically generated textual information. Such wearable displays have potential to be used in a broad range of applications.\r\nNevertheless, the reading process is passive, and users have no control over the reading flow. To compensate for such drawback, this paper investigates what kind of interactions are necessary for vibrotactile skin reading and the modalities of such interactions.\r\nAn interaction concept for skin reading was designed taking reading as a process into account. We performed a formative study with 22 participants to assess reading behaviour in word and sentence reading using a six-channel wearable vibrotactile display. Our study shows that word based interactions in sentence reading are more often used and preferred by users compared to character-based interactions and that users prefer gesture-based interaction for skin reading. \r\nFinally, we discuss how such wearable vibrotactile displays could be extended with sensors that would enable recognition of such gesture-based interaction. This paper contributes a set of guidelines for the design of wearable haptic displays for text communication.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Know Center"
            }
          ],
          "personId": 24832
        },
        {
          "affiliations": [
            {
              "institution": "Know Center"
            }
          ],
          "personId": 24874
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 5271,
      "typeId": 10868,
      "title": "VisPod: Content-Based Audio Visual Navigation",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Current audio player interfaces generally provide brief information such as title and duration time and support basic playback control functions. These features alone are not sufficient for certain user tasks, such as quickly finding a previously-visited location or browsing the main topics covered in the audio content. We present VisPod, a visual audio player that visually displays the main topics and keywords extracted from the transcript. VisPod supports (1) audio content browsing, (2) topic-based and keyword-based navigation, (3) communication of transcript and speaker information in real time, and (4) content-based query. VisPod encodes audio as a donut chart comprised of topic segments, and uses text processing algorithms to segment the transcript into independent topics and utilizes a deep learning model to generate human-readable topic names. An informal study suggests users prefer VisPod over traditional audio playback approaches specifically with regards to its benefits for audio browsing and navigation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 19840
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 12869
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 18659
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 15765
        },
        {
          "affiliations": [
            {
              "institution": "University of Notre Dame"
            }
          ],
          "personId": 9162
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 5016,
      "typeId": 10838,
      "title": "Personal Recommendations for Raising Social Eminence in an Enterprise",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Social media sites have become very popular within large enterprises. Still, employees are experiencing difficulties in engaging efficiently. In this paper, we present a study of a personalized action recommendation system in an enterprise social network. Following a previous study on how to raise one’s social eminence in the enterprise and a set of interviews, we built an innovative recommendation system which provides employees with concrete personalized recommendations on how and where to engage. Differently from other systems, it presents recommendations in context of limiting social network behavioral patterns. The recommendations’ goal is to assist employees in growing out of these patterns. The paper presents the interview findings, the innovative recommendation system and results of a wide survey investigating the effectiveness of such a system.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 13819
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 20238
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 17455
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 17646
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 17297
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    },
    {
      "id": 5786,
      "typeId": 10838,
      "title": "Closing the Loop: User-Centered Design and Evaluation of a Human-in-the-Loop Topic Modeling System",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Human-in-the-loop topic modeling allows end users to guide the creation of topic models and improve the models' quality without requiring expertise in topic modeling algorithms. Prior work in this area either focuses on refinement implementation without understanding how users actually wish to improve the model or focuses on user wants without exposing them to a the effect user input has on the model. This work implements a set of user-preferred refinements identified from prior work. An interview study with twelve non-expert participants examines how end users are affected by issues that arise when the user is truly brought into the loop of the algorithm process. As these issues mirror those identified in interactive machine learning more broadly, such as unpredictability, latency, and trust, this work provides a mechanism for examining interactive machine learning challenges with non-expert end users through the lens of human-in-the-loop topic modeling. We find that although users experience unpredictability, their reactions vary from positive to negative, and surprisingly, we do not find any cases of distrust, but instead note instances where users perhaps trust the system too much or have too little confidence in themselves.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Decisive Analytics Corporation"
            },
            {
              "institution": "University of Maryland, College Park"
            }
          ],
          "personId": 14662
        },
        {
          "affiliations": [
            {
              "institution": "University of Maryland"
            }
          ],
          "personId": 13277
        },
        {
          "affiliations": [
            {
              "institution": "University of Maryland"
            }
          ],
          "personId": 24842
        },
        {
          "affiliations": [
            {
              "institution": "Brigham Young University"
            }
          ],
          "personId": 24871
        },
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            }
          ],
          "personId": 24814
        }
      ],
      "sessionIds": [
        1698
      ],
      "eventIds": []
    },
    {
      "id": 6556,
      "typeId": 10838,
      "title": "Modality Switching for Mitigation of Sensory Adaptation and Habituation in Personal Navigation Systems",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "When humans need to navigate across terrain accurately and quickly, they often use portable electronic navigation systems for directional guidance. Prior work in this field has focused on selecting either the visual or haptic sensory modality for providing such guidance and has indicated that either option may be preferable depending on the user's specific goals. However, basing the selection of visual or haptic guidance on static criteria of this type discounts important time-varying effects, primarily stimulus-specific adaptation (SSA) and habituation. Here, we propose a navigation system design that mitigates these detrimental effects by periodically switching between visual and haptic navigation guidance. While this is likely to incur an undesirable switching cost, we hypothesize that the long-term benefits of counteracting SSA and habituation will outweigh this cost. In this paper, we describe the design and results of a human-participant study intended to evaluate this hypothesis. Our findings indicate that modality switching results in a transient cost to performance, but also that switching modalities lessens the SSA and habituation effects over time as compared with single-modality systems. The results support the hypothesis that an alternating-modality system would outperform a single-modality system for long-duration navigation tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Massachusetts Institute of Technology"
            }
          ],
          "personId": 14795
        },
        {
          "affiliations": [
            {
              "institution": "MIT"
            }
          ],
          "personId": 10123
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 2973,
      "typeId": 10838,
      "title": "Assessing and Modeling Expertise in Assistive Navigation Interfaces for Blind People",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Evaluating the impact of expertise and route knowledge on task performance can guide the design of intelligent and adaptive navigation interfaces. Expertise has been relatively unexplored in the context of assistive indoor navigation interfaces for blind people. To quantify the complex relationship between the user's walking patterns, route learning, and adaptation to the interface, we conducted a study with 8 blind participants. The participants repeated a set of navigation tasks while using a smartphone-based turn-by-turn navigation guidance app. The results demonstrate the gradual evolution of user skill and knowledge throughout the route repetitions, significantly impacting the task completion time. In addition to the exploratory analysis, we take a step towards tailoring the navigation interface to each user's needs by proposing a personalized recurrent neural network-based behavior model for expertise level classification.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Robotics Institute"
            }
          ],
          "personId": 10333
        },
        {
          "affiliations": [
            {
              "institution": "Carnegie Mellon University"
            }
          ],
          "personId": 14766
        },
        {
          "affiliations": [
            {
              "institution": "Robotics Institute"
            }
          ],
          "personId": 17496
        },
        {
          "affiliations": [
            {
              "institution": "Robotics Institute"
            }
          ],
          "personId": 8749
        },
        {
          "affiliations": [
            {
              "institution": "Robotics Institute"
            }
          ],
          "personId": 12302
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 4257,
      "typeId": 10868,
      "title": "Convolutional Matrix Factorization for Recommendation Explanation",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we introduce a novel recommendation model, which harnesses a convolutional neural network to mine meaningful information from customer reviews, and integrates it with matrix factorization algorithm seamlessly. It is a valid method to improve the transparency of CF algorithms.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 20902
        },
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 24144
        },
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 23625
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7585,
      "typeId": 10838,
      "title": "Citicafe: An Interactive Interface for Citizen Engagement",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Community civic engagement is a new and emerging trend in urban cities driven by the mission of developing responsible citizenship. Technology is playing a vital role in helping this mission, for example, over the last couple of years, there have been a plethora of social media avenues to report civic issues and complaints. We present one such contribution of technology, in terms of an intelligent platform, \"Citicafe''. The platform has a conversation based interface that enhances citizen engagement by enabling a direct correspondence. The platform ingests data from different sources, which is exploited by a virtual agent to enable informed interactions. It can help citizens to (a) report problems and (b) gather information related to civic issues in different neighborhoods. We also report the results of a user study carried out to establish the effectiveness of our interface and draw a comparison with an existing platform. A detailed analysis (qualitative as well as quantitative) of the survey result shows a definite and statistically significant (p<0.05) preference for our interface over the existing platform",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 9682
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 13199
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 17936
        },
        {
          "affiliations": [
            {
              "institution": "IIT Madras"
            }
          ],
          "personId": 21667
        },
        {
          "affiliations": [
            {
              "institution": "University of Texas"
            }
          ],
          "personId": 15242
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 19595
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    },
    {
      "id": 6562,
      "typeId": 10868,
      "title": "How Personal Experience and Technical Knowledge Affect Using Conversational Agents",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Conversational agents (CA) use dialogues to interact with users so as to offer an experience of naturalistic interaction. However, due to the low transparency and poor explanability of mechanism inside CA, individual's understanding of CA's capabilities may affect how the individual interacts with CA and the sustainability of CA use. To examine how users' understanding affect perceptions and experiences of using CA, we conducted a laboratory study asking 41 participants performed a set of tasks using Apple Siri. We independently manipulated two factors: (1) personal experience of using CA, and (2) technical knowledge about CA's system model. We conducted mixed-method analyses of post-task usability measures and interviews, and confirmed that use experience and technical knowledge affects perceived usability and mental models differently. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Tsing-Hua University"
            }
          ],
          "personId": 22809
        },
        {
          "affiliations": [
            {
              "institution": "Institute of Information Systems and Applications"
            }
          ],
          "personId": 9055
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 6563,
      "typeId": 10771,
      "title": "User Preference Modeling and Exploitation in IoT Scenarios",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Free University of Bozen-Bolzano"
            }
          ],
          "personId": 23513
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 4003,
      "typeId": 10868,
      "title": "WAIVS: An Intelligent Interface for Visual Stylometry Using Semantic Workflows",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present initial work towards creating an intelligent interface that can act as an open access laboratory for visual stylometry called WAIVS, Workflows for Analysis of Images and Visual Stylometry. WAIVS allows scholars, students, and other interested parties to explore the nature of artistic style using cutting-edge research methods in visual stylometry. We create semantic workflows for this interface using various computer vision algorithms that not only facilitate artistically significant analyses but also impose intelligent semantic constraints on complex analyses.  In the interface, we combine these workflows with a manually-curated dataset for analysis of artistic style based on either the school of art or the  medium. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Fitchburg State University"
            }
          ],
          "personId": 20897
        },
        {
          "affiliations": [
            {
              "institution": "Fitchburg State University"
            }
          ],
          "personId": 9476
        },
        {
          "affiliations": [
            {
              "institution": "Boston College"
            }
          ],
          "personId": 17285
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4005,
      "typeId": 10838,
      "title": "Towards an optimal dialog strategy for information retrieval using both open-ended and close-ended questions",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "The emerging paradigm of dialogue interfaces for information retrieval systems opens new opportunities for interactively narrowing down users' information query and improving search results. Prior research has largely focused on methods that use a set of close-ended questions, such as decision tree, to learn about the user's search target. However, when there is a myriad of documents or items to search, solely relying on close-ended questions can lead to long and undesirable dialogues. We propose an adaptive dialogue strategy framework that incorporates open-ended questions at the optimal timing to reduce the length of the dialogues. We propose a method to estimate the information gain of open-ended questions, and in each dialog turn, we compare it with that of close-ended questions to decide which question to ask. We present experiments using several synthetic datasets designed to explore the behavior of such an adaptive dialogue strategy under different environments, and compare the system's performance with that of a decision-tree only strategy.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM T. J. Watson Research Center"
            }
          ],
          "personId": 24851
        },
        {
          "affiliations": [
            {
              "institution": "IBM T.J. Watson Research Center"
            }
          ],
          "personId": 24872
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research"
            }
          ],
          "personId": 11333
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 3750,
      "typeId": 10868,
      "title": "SinkAmp: Interactive Sink to Detect Living Habits for Healthcare and Quality of Life",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We have developed SinkAmp which detects living habits (e.g., hand wash, face wash, gargle) from the sound of water flow and talks interactively to users through synthetic sounds. Healthcare has been getting important recently, especially for the elderly. In such situations, a monitoring service is one of the solutions to watch a target person continuously through a camera and report whether the person is healthy or not through e-mail. However, camera-based monitoring makes people's activities limited, because the camera becomes a mental barrier to people. In contrast, our SinkAmp is set inside the sink where people wash their hands and often drink water, so invisible and ambient monitoring is possible. This paper describes our SinkAmp focusing on its implementation and initial evaluation.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "AIIT"
            }
          ],
          "personId": 8849
        },
        {
          "affiliations": [
            {
              "institution": "IS"
            }
          ],
          "personId": 13834
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 2983,
      "typeId": 10868,
      "title": "Supporting a Children's Workshop with Machine Translation",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Previous studies have investigated the characteristics of machine translation(MT)-mediated communication in lab settings and suggested various ways to improve it. Unfortunately, we still lack an understanding of how MT is used in real-world settings, particularly when people use it to support face-to-face communication. In this paper, we report on a field study of a multilingual workshop where children from various language regions used MT to communicate with each other. We investigate how children use various information such as non-verbal cues and drawings to compensate for the mistranslations of MT. For example, children tried to understand the mistranslated messages by reading alternative translations and used web browsers to search for pictures of unknown objects. Such findings provide insights for designing future multilingual support systems.  ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 16378
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 14835
        },
        {
          "affiliations": [
            {
              "institution": "NTT"
            }
          ],
          "personId": 24884
        },
        {
          "affiliations": [
            {
              "institution": "NPO Pangaea"
            }
          ],
          "personId": 13911
        },
        {
          "affiliations": [
            {
              "institution": "NPO Pangaea"
            }
          ],
          "personId": 19020
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 6823,
      "typeId": 10868,
      "title": "Information Display Method to Give the Non-Mechanical Impression by Imitating the Communication with Pets",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Display devices often present information in a mechanical manner to users. In this study, we propose an information display method providing a non-mechanical representation. The method makes the information display resemble communication with pets. As an example, we developed the Medaka Clock, which uses the position of killifish to respond to the user’s request for the time of day.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Miyagi University"
            }
          ],
          "personId": 9226
        },
        {
          "affiliations": [
            {
              "institution": "Miyagi University"
            }
          ],
          "personId": 15515
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4776,
      "typeId": 10868,
      "title": "Providing Adaptive and Personalized Visual Support based on Behavioral Tracking of Children with Autism for Assessing Reciprocity and Coordination Skills in a Joint Attention Training Application ",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recent works have demonstrated the applicability of the activity and behavioral pattern analysis mechanisms to assist therapists, care-givers and individuals with development disorders including those with autism spectrum disorder (ASD); the computational cost and sophistication of such behavioral modeling systems might prevent them from deploying. As such, in this paper, we proposed an easily deployable automatic system to train joint attention (JA) skills, assess the frequency and degree of reciprocity and provide visual cues accordingly. Our proposed approach is different from most of earlier attempts in that we do not capitalize the sophisticated feature-space construction methodology; instead, the simple design, in-game automatic data collection for adaptive visual supports offers hassle-free benefits especially for low-functioning ASD individuals and those with severe verbal impairments.  ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Wenzhou-Kean University"
            }
          ],
          "personId": 18547
        },
        {
          "affiliations": [
            {
              "institution": "Wenzhou-Kean University"
            }
          ],
          "personId": 18137
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5802,
      "typeId": 10838,
      "title": "AVEID: Automatic Video System For Measuring Engagement in Dementia",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Engagement in dementia is typically measured using behavior observational scales (BOS) that are tedious and involve intensive manual labor to annotate, and are therefore not easily scalable. We present AVEID, a low-cost and easy to use video-based engagement measurement tool to determine the level of engagement of a person with dementia (PwD) when interacting with a target object. We show that the objective behavioral measures computed via AVEID correlate well with subjective expert impressions for the popular MPES and OME BOS, confirming its viability and effectiveness.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National University of Singapore"
            }
          ],
          "personId": 8556
        },
        {
          "affiliations": [
            {
              "institution": "International Institute of Information Techology, Hyderabad"
            }
          ],
          "personId": 22383
        },
        {
          "affiliations": [
            {
              "institution": "University of Glasgow"
            }
          ],
          "personId": 15232
        },
        {
          "affiliations": [
            {
              "institution": "National University of Singapore"
            }
          ],
          "personId": 24880
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 6571,
      "typeId": 10868,
      "title": "On the Expression of Agent Emotions in Customer Support Dialogs in Social Media",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Providing customer support through social media channels is gaining popularity. In this context, it is important for the agent, either human or automated, to express emotions when needed. \r\nIn this paper we study the patterns of human agents emotional expression and question the need of Affective NLG. By analyzing real dialogs that were tagged by the crowd, we show that emotional responses contribute to the quality of the dialog and relate to customer satisfaction, and that human agents express emotions by simply adding emotional acknowledgment at the beginning of the response.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 24830
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 13043
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 8379
        },
        {
          "affiliations": [
            {
              "institution": "IBM Research AI"
            }
          ],
          "personId": 19851
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4011,
      "typeId": 10838,
      "title": "Burst Your Bubble! An Intelligent System for Improving Awareness of Diverse Social Opinions",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Social media users are overloaded with diverse opinions with different stances. Previous research shows that people often look for opinions that reinforce their pre-existing beliefs and stances, which may lead to social polarization. Traditional social media present opinions in a linear list format, which not only lacks structures for people to explore diverse viewpoints but also aggravates their selective exposure to agreeable opinions. To address this problem, we designed an intelligent system that improves awareness of diverse social opinions by providing visual hints and recommendations of opinions (e.g. news articles and comments) on different sides with different indicators. We evaluated our system with news articles about an issue related to Obamacare repeal and their corresponding user comments from Facebook. Results demonstrate that our system could increase people's awareness of their stances and opinion selection preferences, which mitigates selective exposure and thereby leads to a more balanced perception of social opinions.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 19083
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois Urbana-Champaign"
            }
          ],
          "personId": 23635
        },
        {
          "affiliations": [
            {
              "institution": "university of illinois"
            }
          ],
          "personId": 24823
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 7854,
      "typeId": 10868,
      "title": "Cylindrical M-sequence Markers and its Application to AR Fitting System for Kimono Obi",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This paper proposes an m-sequence cylindrical marker, which is an optical barcode marker that can be applied on cylindrical objects, such as parts of a human body and everyday things such as furniture, bottles, and cups. We use two cycles of an m-sequence barcode to maintain continuity at the joint of the cylindrical marker. By using the m-sequence characteristics, the marker is able to acquire the rotation angle by recognizing only a certain part of the barcode. To confirm feasibility, we have made a cylindrical marker that fits on the waist of a human, and have implemented an AR fitting application for kimono obi.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 17559
        },
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 18094
        },
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 24833
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 5811,
      "typeId": 10838,
      "title": "CARDINAL: Computer Assisted Authoring of Movie Scripts",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "We present Cardinal, a tool for computer-assisted authoring of movie scripts. Cardinal provides a means of viewing a script through a variety of perspectives, for interpretation as well as editing. This is made possible by virtue of intelligent automated analysis of these natural language scripts and generating different intermediate representations. We extract information about the actions occurring in the narrative, which then empowers these views. Cardinal generates 2-D and 3-D visualizations of the scripted narrative and also presents interactions in a timeline-based view. The user study reveals that users of the system demonstrated confidence and comfort using the system.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Swiss Federal Institute of Technology in Zurich (ETHZ)"
            },
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 12862
        },
        {
          "affiliations": [
            {
              "institution": "Swiss Federal Institute of Technology in Zurich (ETHZ)"
            },
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 24037
        },
        {
          "affiliations": [
            {
              "institution": "Swiss Federal Institute of Technology in Zurich (ETHZ)"
            },
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 13650
        },
        {
          "affiliations": [
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 16181
        },
        {
          "affiliations": [
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 18580
        },
        {
          "affiliations": [
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 11350
        },
        {
          "affiliations": [
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 24864
        },
        {
          "affiliations": [],
          "personId": 24862
        },
        {
          "affiliations": [
            {
              "institution": "Swiss Federal Institute of Technology in Zurich (ETHZ)"
            },
            {
              "institution": "Disney Research"
            }
          ],
          "personId": 24863
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 3764,
      "typeId": 10868,
      "title": "Automated Assistance for Creative Writing with an RNN Language Model",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "This work demonstrates an interface, Creative Help, that assists people with creative writing by automatically suggesting new sentences in a story. Authors can freely edit the generated suggestions, and the application tracks their modifications. We make use of a Recurrent Neural Network language model to generate suggestions in a simple probabilistic way. Motivated by the theorized role of unpredictability in creativity, we vary the degree of randomness in the probability distribution used to generate the sentences, and find that authors’ interactions with the suggestions are influenced by this randomness.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Southern California"
            }
          ],
          "personId": 12591
        },
        {
          "affiliations": [
            {
              "institution": "Institute for Creative Technologies, University of Southern California"
            }
          ],
          "personId": 19403
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3509,
      "typeId": 10771,
      "title": "An Intelligent Tutoring System for Situated Decision Making in Dental Surgery",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Faculty of ICT"
            }
          ],
          "personId": 22264
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 4021,
      "typeId": 10868,
      "title": "Implementation of an Interactive System for the Translation of Lyrics",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recent evolution of the Internet has made it possible to listen to music that is composed worldwide in foreign languages. However, lyrics that are entirely scripted in foreign languages are difficult to understand. An interactive system for the translation of lyrics into Japanese is proposed herein, and its implementation is explained.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 16210
        },
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 8878
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 4278,
      "typeId": 10838,
      "title": "Supporting Spatial Skill Learning with Gesture-Based Embodied Design",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Prior research has shown that spatial abilities are crucial for STEM achievement and attainment. The connection between the digital and physical worlds provided by embodied interaction has been shown to enhance performance and engagement in educational contexts. Spatial reasoning is a domain that lends itself naturally to embodied, physical interaction; however, there is little understanding of how embodied interaction could be incorporated into educational technology designed to train spatial reasoning skills. We propose several guidelines for gestural interaction design in spatial reasoning education games based on an empirical study with students at a local afterschool program using a custom-built computer game for training spatial skills. We present a series of gesture sets derived from an iterative design approach that are easy for children to acquire, show sufficient congruency to specific spatial operations, and enable robust recognition from the system. We also compared children's behaviors when playing the game with our gestural interface and a traditional mouse-based interface and found that children take more time but fewer steps to complete game levels when using gestures.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 23044
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois Urbana-Champaign"
            }
          ],
          "personId": 21317
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 24857
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 15520
        },
        {
          "affiliations": [],
          "personId": 24823
        }
      ],
      "sessionIds": [
        1994
      ],
      "eventIds": []
    },
    {
      "id": 6839,
      "typeId": 10838,
      "title": "Interactive Storytelling for Movie Recommendation through Latent Semantic Analysis",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Recommendation is essential to many online services, however current systems often provide limited interaction and visualization mechanisms. This paper presents an interactive recommendation approach for the general public without any knowledge of recommendation or visualization algorithms. Our approach emphasizes interactivity, explicit user input, and semantic information convey with the following two components.\r\nFirst, we propose a Latent Semantic Model that captures the statistical features of semantic concepts on 2D domains and abstracts user preferences for personal recommendation, so that high-dimensional spectral space from the rating records can be understood and interact with directly. Second, we propose an interactive recommendation approach through a storytelling mechanism for promoting the communication between the user and the recommendation system. We demonstrate and evaluate our approach with a real dataset. Our approach can also be extended to other applications including various online recommendation systems.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "UNCC"
            }
          ],
          "personId": 21718
        },
        {
          "affiliations": [],
          "personId": 24822
        },
        {
          "affiliations": [],
          "personId": 18853
        },
        {
          "affiliations": [],
          "personId": 12247
        }
      ],
      "sessionIds": [
        1929
      ],
      "eventIds": []
    },
    {
      "id": 6329,
      "typeId": 10868,
      "title": "A Deep Learning Based Method For 3D Human PoseEstimation From 2D Fisheye Images",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a deep learning based method to directly estimate the human joint positions in 3D space from 2D fisheye images captured in an egocentric manner. \r\nThe core of our method is a novel network architecture based on Inception-v3, featuring the asymmtric convolutional filter size, the long short-term memory module, and the anthropomorphic weights on the training loss. \r\nWe demonstrate our method outperform state-of-the-art method under different tasks.\r\nOur method can be helpful to develop useful deep learning network for human-machine interaction and VR/AR applications.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "NTU"
            }
          ],
          "personId": 11725
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 9396
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 24229
        },
        {
          "affiliations": [
            {
              "institution": "National Taiwan University"
            }
          ],
          "personId": 24878
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 5817,
      "typeId": 10771,
      "title": "Eye Tracking- Single Technology to Handle Multiple Domains",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 18224
        },
        {
          "affiliations": [
            {
              "institution": "Texas A&M University"
            }
          ],
          "personId": 16404
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 5562,
      "typeId": 10838,
      "title": "Eye Gaze-driven Prediction of Cognitive Differences during Graphical Password Composition",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Evidence suggests that individual cognitive differences affect users’ memorability, visual behavior and graphical passwords’ security. Such knowledge denotes the added value of personalizing graphical password schemes towards the unique cognitive characteristics of the users. However, real-time and accurate cognition-based predictive user models are necessary to reach such a break-through. In this paper, we present results of such an attempt, where an in-lab eye-tracking study was conducted with 36 participants who completed a recall-based graphical password composition task. We adopted a credible cognitive style theory, and investigated a variety of eye-tracking metrics to predict participants’ cognitive styles. Results’ analysis reveals that inferring individual cognitive differences in real-time during graphical password composition is feasible within a few seconds and that specific eye-tracking metrics correlate stronger with certain cognitive style groups. Findings further support the vision of incorporating real-time adaptive mechanisms in graphical password schemes for the benefit of service providers and end-users.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Patras"
            }
          ],
          "personId": 17602
        },
        {
          "affiliations": [
            {
              "institution": "University of Patras"
            }
          ],
          "personId": 18875
        },
        {
          "affiliations": [
            {
              "institution": "University of Ptras"
            }
          ],
          "personId": 19576
        },
        {
          "affiliations": [
            {
              "institution": "Cognitive UX GmbH"
            }
          ],
          "personId": 18265
        },
        {
          "affiliations": [
            {
              "institution": "University of Cyprus"
            }
          ],
          "personId": 21617
        },
        {
          "affiliations": [
            {
              "institution": "University of Patras"
            }
          ],
          "personId": 22711
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 8124,
      "typeId": 10868,
      "title": "Flufy: Recyclable and Edible Rapid Prototyping using Fluffed Sugar",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Low-Fab (low-fidelity fabrication) has allowed designers to speed up the initial prototyping of 3D objects. However, the method is used to quickly verify the key aspects of the model, and are not the final desired object. Therefore, the object generated by Low-Fab are temporal objects in which the disposability and/or reusability of the material should be considered. Here we present a novel prototyping method beyond the concept of Low-Fab, to quickly fabricate large, disposable, reusable and edible objects by using fluffed sugar for the material. We have implemented a fabrication system that supports the user to create desired objects in high-speed, which is based on the idea of cotton candy making. Our idea is to combine food fabrication techniques with rapid prototyping methods, which we believe that it can contribute to both designers and gastronomy.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 8261
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 15809
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            },
            {
              "institution": "Sony Computer Science Laboratories, Inc"
            }
          ],
          "personId": 18415
        },
        {
          "affiliations": [
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 22919
        },
        {
          "affiliations": [
            {
              "institution": "Inc"
            },
            {
              "institution": "The University of Tokyo"
            }
          ],
          "personId": 24886
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 3523,
      "typeId": 10868,
      "title": "An Intelligent User Interface for Efficient Semi-automatic Transcription of Historical Handwritten Documents",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Transcription of large-scale historical handwritten document images is a tedious task. Machine learning techniques, such as deep learning, are popularly used for quick transcription, but often require a substantial amount of pre-transcribed word examples for training. Instead of line-by-line word transcription, this paper proposes a simple training-free gamification strategy where all occurrences of each arbitrarily selected word is transcribed once, using an intelligent user interface implemented in this work. The proposed approach offers a fast and user-friendly semi-automatic transcription that allows multiple users to work on the same document collection simultaneously.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Department of Information Technology, vi2"
            }
          ],
          "personId": 19910
        },
        {
          "affiliations": [
            {
              "institution": "Information technology, Vi2"
            }
          ],
          "personId": 20515
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4044,
      "typeId": 10868,
      "title": "Support System to Review Manufacturing Workshop",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "With the increasing popularity of digital machine tools such as 3D printers, even common people are being involved in personal fabrication (Fab age). For example, manufacturing workshops using digital machine tools are often held by the Fab community (e.g., FabLab   ).  In such workshops, manufacturing processes are important experiences for users (both organizers and attendees). However, such processes are often difficult to record and review. This paper proposes a system for recording manufacturing workshops from multiple viewpoints, and helping users review manufacturing processes through multiple videos.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Systems Information Science"
            }
          ],
          "personId": 24377
        },
        {
          "affiliations": [
            {
              "institution": "Information Architecture"
            }
          ],
          "personId": 21766
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 7116,
      "typeId": 10838,
      "title": "Ether-Toolbars: Evaluating Off-Screen Toolbars for MobileInteraction",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "In mobile interaction, the use of touchscreen interaction, while beneficial from the perspective of portability, has limited spatial accuracy due to the ``fat finger problem''. As a result, an important challenge on mobile interaction is to find solutions to balance the size of individual widgets against the number of widgets needed during interaction. In this work, to address display space limitations, we explore the design of off-screen toolbars (ether-toolbars) that leverage computer vision to expand application features by placing widgets adjacent to the display screen.  We show how simple computer vision algorithms can be combined with a natural human ability to estimate physical placing to support highly accurate targeting.  Our ether-toolbar design promises targeting accuracy approximating on-screen widget accuracy while significantly expanding the interaction space of mobile devices. Through two experiments, we examine off-screen content placement metaphors and off-screen precision of participants accessing these toolbars. From the data of the second experiment, we provide a basic model that reflects how users perceive mobile surroundings for ether-widgets and validate it. We also demonstrate a prototype system consisting of an inexpensive 3D printed mount for mirror that supports ether-toolbar implementations. Finally, we discuss the implications of our work and potential design extensions that can increase the usability and the utility of ether-toolbars.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "CNRS, INRIA"
            }
          ],
          "personId": 18028
        },
        {
          "affiliations": [
            {
              "institution": "University of Lille Sci. & Tech, CNRS, INRIA"
            }
          ],
          "personId": 24877
        },
        {
          "affiliations": [
            {
              "institution": "University of Waterloo"
            }
          ],
          "personId": 24824
        },
        {
          "affiliations": [
            {
              "institution": "CNRS, INRIA"
            }
          ],
          "personId": 22517
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 5838,
      "typeId": 10838,
      "title": "Touch-Supported Voice Recording to Facilitate Forced Alignment of Text and Speech in an E-Reading Interface",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Reading a book together with a family member who has impaired vision or other difficulties reading is an important social bonding activity. However, for the person being read to, there is little support in making these experiences repeatable. While audio can easily be recorded, synchronizing it with the text for later playback requires the use of forced alignment algorithms, which do not perform well on amateur read-aloud speech. We propose a human-in-the-loop approach to augmenting such algorithms in the form of touch metaphors during collocated read-aloud sessions using tablet e-readers. The metaphor is implemented as a finger-follows-text tracker. We explore how this could better handle the variability of amateur reading, which poses accuracy challenges for existing forced alignment techniques. Data collected from users reading aloud as assisted by touch metaphors show increases in the accuracy of forced alignment algorithms and reveal opportunities for how to better support reading aloud.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 8410
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 24885
        },
        {
          "affiliations": [
            {
              "institution": "University of Alberta"
            }
          ],
          "personId": 21042
        },
        {
          "affiliations": [
            {
              "institution": "University of Toronto"
            }
          ],
          "personId": 11383
        },
        {
          "affiliations": [
            {
              "institution": "Toronto Rehabilitation Institute"
            }
          ],
          "personId": 21107
        }
      ],
      "sessionIds": [
        1262
      ],
      "eventIds": []
    },
    {
      "id": 8142,
      "typeId": 10838,
      "title": "Explaining Recommendations Using Contexts",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems support user decision-making, and explanations of recommendations further facilitate their usefulness. Previous explanation styles are based on similar users, similar items, demographics of users, and contents of items. Contexts, such as “usage scenarios” and “accompanying persons”, have not been used for explanations, although they influence user decisions. In this paper, we propose a context style explanation method, presenting contexts suitable for consuming recommended items. Expected impacts of context style explanations are 1) persuasiveness: recognition of suitable context for usage motivates users to consume items, and 2) usefulness: envisioning context help users’ right choice since values of items depend on contexts. We evaluate context style persuasiveness and usefulness by a crowdsourcing-based user study in a restaurant recommendation setting. The context style explanation is compared to demographic and content style explanations. We also combine context style and other explanation styles, confirming that hybrid styles improve persuasiveness and usefulness of explanation. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 11037
        },
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 18667
        },
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 12135
        },
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 14007
        },
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 12197
        },
        {
          "affiliations": [
            {
              "institution": "Fuji Xerox"
            }
          ],
          "personId": 23238
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    },
    {
      "id": 5583,
      "typeId": 10838,
      "title": "Helping customers make the most out of product reviews: A framework for visualizing service comparisons – A case study using restaurants",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Online customers’ opinions about products and services, in the form of reviews, are a major part of today’s web culture. However, customers, when looking for a product or service, do not have the time or the desire to read even a small part of the available product reviews. Moreover, they often would like to examine reviews of similar products, and get a comprehensive picture of how different aspects of these products compare. In this work, by introducing a generic framework for analyzing and presenting a visual summary based on comparative sentences extracted from customer reviews, we offer the user an easy and intuitive understanding of the differences between a set of products. \r\nThe contributions of this study if twofold: First, it focusses on reviews of services (using the restaurant domain as a case study), unlike most of the related studies that consider tangible products and second, it combines state-of-the-art text analysis techniques with an intuitive visualization into a n easy to use prototype to visualize summarized service comparisons to the users.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The University of Haifa"
            }
          ],
          "personId": 14327
        },
        {
          "affiliations": [
            {
              "institution": "The University of Haifa"
            }
          ],
          "personId": 24860
        },
        {
          "affiliations": [
            {
              "institution": "University of Haifa"
            }
          ],
          "personId": 9026
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    },
    {
      "id": 3796,
      "typeId": 10838,
      "title": "Cloud Menus, a Circular Adaptive Menu for Small Screens",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "This paper presents Cloud Menus, a split adaptive menu for small screens where the predicted menu items are arranged in a tag cloud with a location consistent with their corre-sponding position in the static menu and a font size depending on their prediction level. This layout results from a 3-step design process: (i) defining an initial design space on Bertin’s 8 visual variables and 4 quality properties, (ii) iden-tifying the most preferred layout based on agreement rate, and (iii) implementing it into Cloud Menus, a new widget for Android with cloud layout. A theoretical study investigates a model for estimating the item selection time. An empirical study suggests that cloud menus reduce item selection time and error rate when prediction is correct without penalizing it when prediction is incorrect, compared to two baselines: a non-adaptive static menu and an adaptive linear menu, where predicted items are arranged in a vertical list. From these studies, design guidelines for cloud menus are developed.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Université catholique de Louvain"
            }
          ],
          "personId": 24835
        },
        {
          "affiliations": [
            {
              "institution": "Orange Labs"
            }
          ],
          "personId": 23577
        },
        {
          "affiliations": [
            {
              "institution": "Grenoble Institute of Technology"
            }
          ],
          "personId": 23515
        },
        {
          "affiliations": [
            {
              "institution": "Orange Labs"
            }
          ],
          "personId": 19309
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 7380,
      "typeId": 10838,
      "title": "Visible Hearts, Visible Hands: A Smart Crowd Donation Platform",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "On existing crowdfunding platforms, the allocation of money is often not regulated, which leads to less-than-ideal distribution of resources. For example, recent donations to hurricane victims through their crowdfunding campaigns often lead to overfunding of certain victims while underfunding others. Inspired by algorithms from economic theories, our proposed Smart Crowd Donate system encourages donors to express preferences to multiple projects and reallocates funds dynamically across these preferences over time. \r\nWe conducted a user study in which recruited 452 participants to simulate a small scale of crowdfunding. The findings of our user study supported the idea that the Smart Crowd Donate system has potential to efficiently distribute funds to projects and allows more projects to receive the amount of money they need.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 24853
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 24812
        },
        {
          "affiliations": [
            {
              "institution": "university of illinois"
            }
          ],
          "personId": 24823
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 6612,
      "typeId": 10838,
      "title": "Write-it-Yourself with the Aid of Smartwatches: A Wizard-of-Oz Experiment with Blind People",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Working with non-digital, standard printed materials has always been a challenge for blind people, especially writing. Blind people very often depend on others to fill out printed forms, write checks, sign receipts and documents. Extant assistive technologies for working with printed material have exclusively focused on reading, with little to no support for writing. Also, these technologies employ special-purpose hardware that are usually worn on fingers, making them unsuitable for writing. In this paper, we explore the idea of using off-the-shelf smartwatches (paired with smartphones) to assist blind people in both reading and writing paper forms including checks and receipts. Towards this, we performed a Wizard-of-Oz evaluation of different smartwatch-based interfaces that provide user-customized audio-haptic feedback in real-time, to guide blind users to different form fields, narrate the field labels, and help them write straight while filling out these fields. Finally, we report the findings of this study including the technical challenges and user expectations that can potentially inform the design of Write-it-Yourself aids based on smartwatches.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 17275
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 21806
        },
        {
          "affiliations": [
            {
              "institution": "Stony Brook University"
            }
          ],
          "personId": 15664
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 5336,
      "typeId": 10838,
      "title": "AlterEgo: a Personalised Wearable Silent Speech Interface ",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "We present a wearable interface that allows a user to silently converse with a computing device without any voice or any discernible movements - thereby enabling the user to communicate with devices, AI assistants, applications or other people in a silent, concealed and seamless manner. A user’s intention to speak and internal speech is characterised by neuromuscular signals and subtle movements in internal speech articulators that are captured by the AlterEgo system to reconstruct this speech. We use this to facilitate a natural language user interface, where users can silently communicate in natural language and receive aural output (e.g - bone conduction headphones), thereby enabling a discreet, bi-directional interface with a computing device, and providing a seamless form of intelligence augmentation. The paper describes the architecture, design, implementation and operation of the entire system. Furthermore, we demonstrate the robustness of the system through user studies and report >91% accuracies. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "MIT Media Lab"
            }
          ],
          "personId": 9089
        },
        {
          "affiliations": [
            {
              "institution": "MIT Media Lab"
            }
          ],
          "personId": 20281
        },
        {
          "affiliations": [
            {
              "institution": "MIT Media Lab"
            }
          ],
          "personId": 24815
        }
      ],
      "sessionIds": [
        1994
      ],
      "eventIds": []
    },
    {
      "id": 4825,
      "typeId": 10838,
      "title": "Below the Surface: Unobtrusive Activity Recognition for Work Surfaces using RF-radar sensing",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Activity recognition is a core component of many intelligent and context-aware systems. In this paper, we present a solution for discreetly and unobtrusively recognizing common work activities above a work surface without using cameras. We demonstrate our approach, which utilizes an RF-radar sensor mounted under the work surface, in two work domains; recognizing work activities at a convenience-store counter (useful for post-hoc analytics) and recognizing common office deskwork activities (useful for real-time applications). We classify seven clerk activities with 94.9% accuracy using data collected in a lab environment, and recognize six common deskwork activities collected in real offices with 95.3% accuracy. We show that using multiple projections of RF signal leads to improved recognition accuracy. Finally, we show how smartwatches worn by users can be used to attribute an activity, recognized with the RF sensor, to a particular user in multi-user scenarios. We believe our solution can mitigate some of users’ privacy concerns associated with cameras and is useful for a wide range of intelligent systems.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Washington"
            },
            {
              "institution": "FXPAL"
            }
          ],
          "personId": 20225
        },
        {
          "affiliations": [
            {
              "institution": "FXPAL"
            }
          ],
          "personId": 14697
        },
        {
          "affiliations": [
            {
              "institution": "FUJI XEROX"
            }
          ],
          "personId": 10938
        },
        {
          "affiliations": [
            {
              "institution": "Harman International"
            }
          ],
          "personId": 23574
        }
      ],
      "sessionIds": [
        1251
      ],
      "eventIds": []
    },
    {
      "id": 4313,
      "typeId": 10868,
      "title": "Automatic Generation of Natural Language Explanations",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "An interesting challenge for explainable recommender systems is to provide successful interpretation of recommendations using structured sentences. It is well known that user-generated reviews, have strong influence on the users' decision. Recent techniques exploit user reviews to generate natural language  explanations. In this paper, we propose a character-level attention-enhanced long short-term memory model to generate natural language explanations. We empirically evaluated this network using two real-world review datasets. The generated text present readable and similar to a real user's writing, due to the ability of reproducing negation, misspellings, and domain-specific vocabulary.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Aalborg University"
            }
          ],
          "personId": 20623
        },
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 22610
        },
        {
          "affiliations": [
            {
              "institution": "Aalborg University"
            }
          ],
          "personId": 21859
        },
        {
          "affiliations": [
            {
              "institution": "University College Dublin"
            }
          ],
          "personId": 18942
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 6106,
      "typeId": 10868,
      "title": "MindScribe: Toward Intelligently Augmented Interactions in Highly Variable Early Childhood Environments",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Early childhood is a period of critical  development, with impacts that can last a lifetime. And inequities in the quality of care for this vulnerable population---especially for those at-risk  due to disability, family income, or  trauma---can perpetuate further downstream  health and school-readiness effects. Technology-enabled solutions have the ability to bridge quality-of-care gaps by intelligently augmenting daily activities. However, many traditional computational approaches to natural language interactions are not yet feasible nor affordable in highly variable and dynamic early childhood environments. Yet for rapidly developing preliterate young children, solutions  are needed now. We present MindScribe, an interactive robotic object that leverages open-ended `serve and return' natural language interactions to intelligently support reflective inquiry  and school-readiness in highly variable and imaginative early childhood environments.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 10608
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 19647
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 8747
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 15076
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 22023
        },
        {
          "affiliations": [
            {
              "institution": "University of Colorado Boulder"
            }
          ],
          "personId": 24817
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 8156,
      "typeId": 10838,
      "title": "Detecting low rapport during natural interactions in small groups from non-verbal behavior",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Rapport, the close and harmonious relationship in which interaction partners are “in sync” with each other, was shown to result in smoother social interactions, improved collaboration, and improved interpersonal outcomes. In this work, we are first to investigate automatic prediction of low rapport during natural interactions within small groups. This task is challenging given that rapport only manifests in subtle non-verbal signals that are, in addition, subject to influences of group dynamics as well as inter-personal idiosyncrasies. We record videos of unscripted discussions of three to four people using a multi-view camera system and microphones. We analyse a rich set of non-verbal signals for rapport detection, namely fa- cial expressions, hand motion, gaze, speaker turns, and speech prosody. Using facial features, we can detect low rapport with an average precision of 0.7 (chance level at 0.25), while incorporating prior knowledge of participants’ personalities can even achieve early prediction without a drop in performance. We further provide a detailed analysis of different feature sets and the amount of information contained in different temporal segments of the interactions. ",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Max Planck Institute for Informatics"
            }
          ],
          "personId": 24866
        },
        {
          "affiliations": [
            {
              "institution": "Max Planck Institute for Informatics"
            }
          ],
          "personId": 14225
        },
        {
          "affiliations": [
            {
              "institution": "Saarland Informatics Campus"
            }
          ],
          "personId": 24838
        }
      ],
      "sessionIds": [
        2204
      ],
      "eventIds": []
    },
    {
      "id": 3039,
      "typeId": 10868,
      "title": "Development of a Horror Game that Route Branches by the Player's Pulse Rate",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "In this study, we developed a horror game that changes the effect of fear by player's physiological information. We adopt pulse rate as physiological information in which fear emotions appear and decides to measure using a pulse rate meter as a physiological information sensor. In this system, we prepared a mechanism to select a route that gives fear emotions to players according to the pulse rate. As a result of the trial play, we were able to effectively give fear feelings to players who did not feel fear.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 23306
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 21973
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 18119
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 19382
        },
        {
          "affiliations": [
            {
              "institution": "Bunkyo University"
            }
          ],
          "personId": 21884
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 4321,
      "typeId": 10861,
      "title": "Interacting with Recommenders - Overview and Research Directions",
      "trackId": 10006,
      "tags": [],
      "keywords": [],
      "abstract": "Automated recommendations have become a ubiquitous part of today’s online user experience. These systems point us to additional items to purchase in online shops, they make suggestions to us on movies to watch, or recommend us people to connect with on social websites. In many of today’s applications, however, the only way for users to interact with the system is to inspect the recommended items. Often, no mechanisms are implemented for users to give the system feedback on the recommendations or to explicitly specify preferences, which can limit the potential overall value of the system for its users. \n Academic research in recommender systems is largely focused on algorithmic approaches for item selection and ranking. Nonetheless, over the years a variety of proposals were made on how to design more interactive recommenders. This work provides a comprehensive overview on the existing literature on user interaction aspects in recommender systems. We cover existing approaches for preference elicitation and result presentation, as well as proposals that consider recommendation as an interactive process. Throughout the work, we furthermore discuss examples of real-world systems and outline possible directions for future works.",
      "authors": [
        {
          "affiliations": [],
          "personId": 18260
        },
        {
          "affiliations": [],
          "personId": 12737
        }
      ],
      "sessionIds": [
        1950
      ],
      "eventIds": []
    },
    {
      "id": 4836,
      "typeId": 10771,
      "title": "Analysis of Interaction Design and Evaluation Methods in Full-Body Interaction for Special Needs",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Universitat Pompeu Fabra"
            }
          ],
          "personId": 9469
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 6119,
      "typeId": 10861,
      "title": "Discovering User Behavioral Features to Enhance Information Search on Big Data",
      "trackId": 10006,
      "tags": [],
      "keywords": [],
      "abstract": "Due to the emerging Big Data paradigm, driven by the increasing availability of intelligent services easily accessible by a large number of users (e.g., social networks), traditional data management techniques are inadequate in many real-life scenarios. In particular, the availability of huge amounts of data pertaining to user social interactions, user preferences, and opinions calls for advanced analysis strategies to understand potentially interesting social dynamics. Furthermore, heterogeneity and high speed of user-generated data require suitable data storage and management tools to be designed from scratch. This article presents a framework tailored for analyzing user interactions with intelligent systems while seeking some domain-specific information (e.g., choosing a good restaurant in a visited area). The framework enhances a user's quest for information by exploiting previous knowledge about their social environment, the extent of influence the users are potentially subject to, and the influence they may exert on other users. User influence spread across the network is dynamically computed as well to improve user search strategy by providing specific suggestions, represented as tailored faceted features. Such features are the result of data exchange activity (called data posting) that enriches information sources with additional background information and knowledge derived from experiences and behavioral properties of domain experts and users. The approach is tested in an important application scenario such as tourist recommendation, but it can be profitably exploited in several other contexts, for example, viral marketing and food education.",
      "authors": [
        {
          "affiliations": [],
          "personId": 8828
        },
        {
          "affiliations": [],
          "personId": 24353
        },
        {
          "affiliations": [],
          "personId": 23009
        },
        {
          "affiliations": [],
          "personId": 8683
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 4583,
      "typeId": 10838,
      "title": "Session-based Suggestion of Topics for Exploratory Search",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Exploratory information search can challenge users in the formulation of efficacious search queries to find the data they are interested in. Moreover, complex information spaces can disorient people, making it difficult to explore all the types of information relevant to their activities. In order address these issues, we propose a session-based concept suggestion model that, given the observed search queries, proposes context-dependent query expansions as a {\\em ``you might also be interested in''} function. Our model can be applied to incrementally generate suggestions during the search sessions. This can be employed for query expansion, and in general to guide users in the exploration of the possibly complex space of information categories managed by an information system.\r\n\r\nOur model is based on the generation of a concept co-occurrence graph that describes how frequently concepts are searched together in sessions. Starting from an ontological domain representation, we generated the graph by analyzing the query log of a major search engine. Moreover, we identified clusters of ontology concepts which frequently co-occur in users' searches via community detection on the graph.\r\nAn experiment carried out using the log provided satisfactory accuracy results.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Turin"
            }
          ],
          "personId": 8714
        },
        {
          "affiliations": [
            {
              "institution": "University of Torino"
            }
          ],
          "personId": 17726
        }
      ],
      "sessionIds": [
        2452
      ],
      "eventIds": []
    },
    {
      "id": 2800,
      "typeId": 10868,
      "title": "Spatio-Temporal Visualization of Tweet Data around Tokyo Disneyland Using VR",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Social media analysis is important to understand people behavior.  Human behavior in social media is often related to time and location, which is often difficult to understand the characteristics appropriately and quickly. We chose to visualize the date applying a virtual reality technology, which makes us easier to explore the data interactively and intuitively. This poster presents our visualization with tweets of microblogs with location information. Our system includes a three-dimensional temporal visualization which consists of the two-dimensional map and a time axis. In particular, we aggregate the number of tweets of each coordinate, calculate scores and display them as piled cubes. We highlight only specific cubes so that users can understand the overall tendency of datasets. We also developed user interfaces for operating these cubes and panels which indicate details of tweets.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 14433
        },
        {
          "affiliations": [
            {
              "institution": "Toyohashi University of Technology"
            }
          ],
          "personId": 9813
        },
        {
          "affiliations": [
            {
              "institution": "Ochanomizu University"
            }
          ],
          "personId": 8878
        },
        {
          "affiliations": [
            {
              "institution": "Monash University"
            }
          ],
          "personId": 17622
        },
        {
          "affiliations": [
            {
              "institution": "Monash University"
            }
          ],
          "personId": 9740
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 4081,
      "typeId": 10868,
      "title": "An Intelligent Educational Platform for Training Spatial Visualization Skills",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Current research has demonstrated the crucial role of spatial visualization skills in the Science, technology, engineering, and mathematics (STEM) related fields. Given the nationwide trend of the increasing population of university engineering students, more and more effort has been invested into how to effectively train students' spatial visualization skills. In this study, we designed and developed a scalable online platform for training spatial visualization skills. Our online platform is capable of intelligent learning features such as individualized learning trajectory and personalized hints. In the current stage of the project, we deployed the online platform with over 600 university students. By analyzing student's data, we identified a key question that has most predictive power on student's spatial visualization skill. The promising result shows a bright future for an intelligent learning platform.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 24857
        },
        {
          "affiliations": [
            {
              "institution": "University of Illinois at Urbana-Champaign"
            }
          ],
          "personId": 15520
        },
        {
          "affiliations": [
            {
              "institution": "university of illinois"
            }
          ],
          "personId": 24823
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 7667,
      "typeId": 10868,
      "title": "Augmented-Genomics: Protecting Privacy for Clinical Genomics with Inferential Interfaces",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Recent advances in genetic technology enable large-scale genome sequencing, creating new avenues for research and clinical care. At the same time, these advances raise growing concerns about data protection and privacy. In this demonstration, we present Augmented-Genomics, a system that puts the individuals in control over their genetic information. To envision user-controllable privacy in the hospital, and in other complex clinical situations, we demonstrate several techniques accessible through a mobile application. The system infers the risk of exposing certain parts of the genome and provide a simple interface for users to set their desired level of exposure. Patients and caregivers (such as doctors) exchange visual keys that are used to decrypt genomic data while indirectly fostering discussion and negotiation over the patient's privacy. After the patient provides the permission, the caregiver can access information about essential genes and mutations through a mobile interface or through an augmented reality glasses.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 9080
        },
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 9203
        },
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 12436
        },
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 20341
        },
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 9935
        },
        {
          "affiliations": [
            {
              "institution": "Tel Aviv University"
            }
          ],
          "personId": 14804
        }
      ],
      "sessionIds": [
        1811
      ],
      "eventIds": []
    },
    {
      "id": 2805,
      "typeId": 10838,
      "title": "Voice Input Tutoring System for Older Adults using Input Stumble Detection",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Many older adults are interested in smartphones but encounter difficulties in self-instruction and need support, especially text entry. Voice input is a useful option for text entry, but also presents some difficulties for older adults. In this paper, we propose a tutoring system for voice input that detects input stumbles using a statistical approach and provides instructions to overcome them. We construct the tutoring system for voice based on the data from a user study with novice older adults. In an evaluation experiment, the number of input stumble and the sentence completion time of the participants using the tutoring system were significantly smaller than those without it. The results showed that the tutoring system resulted in the improvement of the efficiency of voice input for novice older adults.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "KDDI Research, Inc"
            },
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 15455
        },
        {
          "affiliations": [
            {
              "institution": "KDDI Research"
            }
          ],
          "personId": 24854
        },
        {
          "affiliations": [
            {
              "institution": "Kyoto University"
            }
          ],
          "personId": 24876
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 7669,
      "typeId": 10838,
      "title": "Beyond the Ranked List: User-Driven Exploration and Diversification of Social Recommendation",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "The beyond-relevance objectives of recommender systems have been drawing more and more attention. For example, a diversity-enhanced interface has been shown to associate positively with overall levels of user satisfaction. However, little is known about the ways in which users adopt diversity-enhanced interfaces to accomplish various real-world tasks. In this paper, we present two attempts at creating a visual diversity-enhanced interface that presents recommendations beyond a simple ranked list. Our goal was to design a recommender system interface to help users explore the different relevance prospects of recommended items in parallel and to stress their diversity. Two within-subject user studies with real-life tasks were conducted to compare our visual interfaces to one another. Results from our user study show that the visual interface significantly reduced the exploration efforts required for explored tasks and helped users to perceive the overall recommendation diversity. We show that the users examined a diverse set of recommended items while experiencing an improvement in overall user satisfaction. Also, the users' subjective evaluations show significant improvement in many user-centric metrics. Experiences are discussed that shed light on avenues for future interface designs. ",
      "authors": [
        {
          "affiliations": [],
          "personId": 24865
        },
        {
          "affiliations": [],
          "personId": 24819
        }
      ],
      "sessionIds": [
        1059
      ],
      "eventIds": []
    },
    {
      "id": 7925,
      "typeId": 10838,
      "title": "Ev’ry Little Movement Has a Meaning of Its Own: Using Past Mouse Movements to Predict the Next Interaction",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "User experience could be enhanced if the computer could understand human interaction intention. For instance, it could react to intercept and prevent interaction errors. This paper presents an approach to predicting users’ intention in interaction tasks based on past mouse movements. We adopt a long short-term memory (LSTM) model to predict the users' intention via their next mouse click interaction, upon being trained with past mouse interaction behaviors. To evaluate, we consider two scenarios in daily computer usage: a more structured crowdsourcing annotation task and a more free-form, open-ended web search task. Our results indicate that we could predict the next interaction event with reasonable accuracy. We also conducted a pilot study to investigate the possibility of applying our model for non-intentional mouse click detection. We believe that our findings would be beneficial towards the development of better intelligent agents.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 11796
        },
        {
          "affiliations": [
            {
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 12267
        },
        {
          "affiliations": [
            {
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 9707
        },
        {
          "affiliations": [
            {
              "institution": "Saarland Informatics Campus"
            }
          ],
          "personId": 14225
        },
        {
          "affiliations": [
            {
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 19934
        },
        {
          "affiliations": [
            {
              "institution": "The Hong Kong Polytechnic University"
            }
          ],
          "personId": 20498
        }
      ],
      "sessionIds": [
        1847
      ],
      "eventIds": []
    },
    {
      "id": 3576,
      "typeId": 10838,
      "title": "Aging and Engaging: A Social Conversational Skills Training Program for Older Adults",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "We developed “Aging and Engaging,” a web-based intelligent interface, to improve communication skills among older adults. The interface allows users to practice conversations with a virtual assistant and receive feedback on eye contact, speaking volume, smiling, and valence of speech content. Feedback is generated automatically by analyzing the temporal properties of the conversation using the hidden Markov model. The interface was designed with the assistance of an expert advisory panel that works with geriatric patients, as well as a focus group of 12 older adults. To evaluate its effectiveness, we conducted a study with 25 older adults, each of whom participated in four conversations. Participants’ response times to questions, as well as the amount of positive feedback, increased gradually through these interactions, as assessed by human judges. Participants found the feedback useful, easy to interpret, and fairly accurate, and expressed their interest in using the system at home. We plan to enroll subjects with difficulties in social communication; have them use the system over time at home in a randomized, controlled study; and measure any changes in their behavior.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 24825
        },
        {
          "affiliations": [
            {
              "institution": "University of Rochester School of Medicine"
            }
          ],
          "personId": 19331
        },
        {
          "affiliations": [
            {
              "institution": "University of Utah"
            }
          ],
          "personId": 21493
        },
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 12926
        },
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 20243
        },
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 10546
        },
        {
          "affiliations": [
            {
              "institution": "University of Rochester"
            }
          ],
          "personId": 21690
        }
      ],
      "sessionIds": [
        1994
      ],
      "eventIds": []
    },
    {
      "id": 5113,
      "typeId": 10771,
      "title": "Suggestion Models in Geographical Exploratory Search",
      "trackId": 10004,
      "tags": [],
      "keywords": [],
      "authors": [
        {
          "affiliations": [
            {
              "institution": "University of Torino"
            }
          ],
          "personId": 8714
        }
      ],
      "sessionIds": [
        1723,
        2339
      ],
      "eventIds": []
    },
    {
      "id": 4091,
      "typeId": 10868,
      "title": "Using Spatialized Audio to Improve Human Spatial Knowledge Acquisition in Virtual Reality",
      "trackId": 10007,
      "tags": [],
      "keywords": [],
      "abstract": "Picture schematization, the construction of spatial object maps from images, has useful applications ranging from indoor exploration to augmented reality. Since using human spatial knowledge improves schematization, crowdsourcing workflows are introduced for extracting spatial information from pictures. As 360° pictures are now available in virtual reality (VR), crowdsourced 360° picture schematization also becomes essential. Yet, the vergence-accommodation conflict (VAC) in head-mounted displays (HMDs) causes inaccurate spatial perception in VR. We propose integration of spatial audio in VR as a cost-effective and intuitive feature to support spatial perception. This study indicates spatial audio cues in VR are naturally incorporated by humans to significantly improve human spatial knowledge accuracy and, subsequently, crowdsourcing schematization.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "National Tsing Hua University"
            }
          ],
          "personId": 11656
        },
        {
          "affiliations": [
            {
              "institution": "Institute of Information Systems and Applications"
            }
          ],
          "personId": 9055
        }
      ],
      "sessionIds": [
        1723
      ],
      "eventIds": []
    },
    {
      "id": 6141,
      "typeId": 10838,
      "title": "Exploring Improvement on User Experience by Exploiting Patterns inside Crowdsourced Time-Sync Comments",
      "trackId": 10005,
      "tags": [],
      "keywords": [],
      "abstract": "Time-Sync Comment (TSC) is a type of crowdsourced audience review embedded in online video websites, and it provides better real-time user interaction than traditional user comments. Considering strong temporal feature of TSC, traditional approaches on analyzing textual comments cannot be transfered directly to solving TSC-related research problems, and hence it gains increasing attention in the past few years. However, there are three major problems on existing TSC research. First, they did not show usefulness of TSC compared to traditional information in improving user experience. Second, experiments were conducted on inconsistent TSC datasets, so it is hard to reproduce their results; Third, performance of existing methods is not convincible because their results were manually evaluated by limited number of so-called experts in their experiments. This paper aims to explore the usefulness of TSC data in improving user experience for watching videos by exploiting a larger-scale TSC dataset with four-level structure and richer attributes. Meanwhile, a set of TSC-related research problems are well defined in the paper and are solved by adapted state-of-the-art methods and evaluated based on crowdsourced labels contained in the dataset.",
      "authors": [
        {
          "affiliations": [
            {
              "institution": "Tongji University"
            }
          ],
          "personId": 9558
        },
        {
          "affiliations": [
            {
              "institution": "Rutgers University"
            }
          ],
          "personId": 20455
        },
        {
          "affiliations": [
            {
              "institution": "Tongji University"
            }
          ],
          "personId": 19746
        },
        {
          "affiliations": [
            {
              "institution": "Tongji University"
            }
          ],
          "personId": 24218
        },
        {
          "affiliations": [
            {
              "institution": "Tongji University"
            }
          ],
          "personId": 20633
        }
      ],
      "sessionIds": [
        1497
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 20487,
      "firstName": "Masataka",
      "lastName": "Goto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9226,
      "firstName": "Suguru",
      "lastName": "Arinami",
      "affiliations": []
    },
    {
      "id": 10253,
      "firstName": "Gary",
      "lastName": "Hsieh",
      "affiliations": []
    },
    {
      "id": 12302,
      "firstName": "Chieko",
      "lastName": "Asakawa",
      "affiliations": []
    },
    {
      "id": 23566,
      "firstName": "Ross",
      "lastName": "Beveridge",
      "affiliations": []
    },
    {
      "id": 21519,
      "firstName": "Raniero",
      "lastName": "Lara-Garduno",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 20498,
      "firstName": "Hong Va",
      "lastName": "Leong",
      "affiliations": []
    },
    {
      "id": 12307,
      "firstName": "Iuliia",
      "lastName": "Brishtel",
      "affiliations": []
    },
    {
      "id": 16404,
      "firstName": "Tracy",
      "lastName": "Hammond",
      "affiliations": []
    },
    {
      "id": 16405,
      "firstName": "Claudia",
      "lastName": "Hauff",
      "affiliations": []
    },
    {
      "id": 23574,
      "firstName": "Sven",
      "lastName": "Kratz",
      "affiliations": []
    },
    {
      "id": 23577,
      "firstName": "Sara",
      "lastName": "Bouzit",
      "affiliations": []
    },
    {
      "id": 20515,
      "firstName": "Ekta",
      "lastName": "Vats",
      "affiliations": []
    },
    {
      "id": 21540,
      "firstName": "Sangbong",
      "lastName": "Yoo",
      "affiliations": []
    },
    {
      "id": 17455,
      "firstName": "Lior",
      "lastName": "Leiba",
      "affiliations": []
    },
    {
      "id": 14385,
      "firstName": "Jussi",
      "lastName": "Jokinen",
      "affiliations": []
    },
    {
      "id": 15409,
      "firstName": "Keita",
      "lastName": "Higuchi",
      "affiliations": []
    },
    {
      "id": 14387,
      "firstName": "Ceenu",
      "lastName": "George",
      "affiliations": []
    },
    {
      "id": 21558,
      "firstName": "Mohammed Eunus",
      "lastName": "Ali",
      "affiliations": []
    },
    {
      "id": 17470,
      "firstName": "Nicu",
      "lastName": "Sebe",
      "affiliations": []
    },
    {
      "id": 8257,
      "firstName": "Aamer",
      "lastName": "Hydrie",
      "affiliations": []
    },
    {
      "id": 11333,
      "firstName": "Biplav",
      "lastName": "Srivastava",
      "affiliations": []
    },
    {
      "id": 8261,
      "firstName": "Natsuki",
      "lastName": "Hamanishi",
      "affiliations": []
    },
    {
      "id": 23625,
      "firstName": "Barry",
      "lastName": "Smyth",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 13390,
      "firstName": "Yurie",
      "lastName": "Kaizu",
      "affiliations": []
    },
    {
      "id": 16463,
      "firstName": "George",
      "lastName": "Chernyshov",
      "affiliations": []
    },
    {
      "id": 16464,
      "firstName": "Michal",
      "lastName": "Derezinski",
      "affiliations": []
    },
    {
      "id": 22610,
      "firstName": "Sixun",
      "lastName": "Ouyang",
      "affiliations": []
    },
    {
      "id": 12371,
      "firstName": "Ora",
      "lastName": "Peled-Nakash",
      "affiliations": []
    },
    {
      "id": 23635,
      "firstName": "Hyo Jin",
      "lastName": "Do",
      "affiliations": []
    },
    {
      "id": 22613,
      "firstName": "Aki",
      "lastName": "Vehtari",
      "affiliations": []
    },
    {
      "id": 11350,
      "firstName": "Isabel",
      "lastName": "Simo",
      "affiliations": []
    },
    {
      "id": 17496,
      "firstName": "Dragan",
      "lastName": "Ahmetovic",
      "affiliations": []
    },
    {
      "id": 10333,
      "firstName": "Eshed",
      "lastName": "Ohn-Bar",
      "affiliations": []
    },
    {
      "id": 14430,
      "firstName": "Alan",
      "lastName": "Lundgard",
      "affiliations": []
    },
    {
      "id": 15455,
      "firstName": "Toshiyuki",
      "lastName": "Hagiya",
      "affiliations": []
    },
    {
      "id": 14433,
      "firstName": "Kaya",
      "lastName": "Okada",
      "affiliations": []
    },
    {
      "id": 11370,
      "firstName": "Logan",
      "lastName": "Simpson",
      "affiliations": []
    },
    {
      "id": 11372,
      "firstName": "Yiwei",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 21617,
      "firstName": "George",
      "lastName": "Samaras",
      "affiliations": []
    },
    {
      "id": 18547,
      "firstName": "Tiffany",
      "lastName": "Tang",
      "middleInitial": "Y.",
      "affiliations": []
    },
    {
      "id": 22643,
      "firstName": "Mark",
      "lastName": "Bilandzic",
      "affiliations": []
    },
    {
      "id": 14453,
      "firstName": "Masaaki",
      "lastName": "ISEKI",
      "affiliations": []
    },
    {
      "id": 11383,
      "firstName": "Yomna",
      "lastName": "Aly",
      "affiliations": []
    },
    {
      "id": 11384,
      "firstName": "Daisuke",
      "lastName": "Iwai",
      "affiliations": []
    },
    {
      "id": 19576,
      "firstName": "George",
      "lastName": "Raptis",
      "middleInitial": "E",
      "affiliations": []
    },
    {
      "id": 20601,
      "firstName": "Renate",
      "lastName": "Haeuslschmid",
      "affiliations": []
    },
    {
      "id": 23686,
      "firstName": "Simo",
      "lastName": "Järvelä",
      "affiliations": []
    },
    {
      "id": 19595,
      "firstName": "Gargi B",
      "lastName": "Dasgupta",
      "affiliations": []
    },
    {
      "id": 16526,
      "firstName": "Vera",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 20623,
      "firstName": "Felipe",
      "lastName": "Soares da Costa",
      "affiliations": []
    },
    {
      "id": 18580,
      "firstName": "Diana",
      "lastName": "Wotruba",
      "affiliations": []
    },
    {
      "id": 12436,
      "firstName": "Tal",
      "lastName": "Florentin",
      "affiliations": []
    },
    {
      "id": 18582,
      "firstName": "Miriam",
      "lastName": "Boon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17559,
      "firstName": "Mizuki",
      "lastName": "Okuyama",
      "affiliations": []
    },
    {
      "id": 17560,
      "firstName": "Brittany",
      "lastName": "Duncan",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 13464,
      "firstName": "Goro",
      "lastName": "Otsubo",
      "affiliations": []
    },
    {
      "id": 20633,
      "firstName": "Jiangfeng",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 15515,
      "firstName": "Yu",
      "lastName": "Suzuki",
      "affiliations": []
    },
    {
      "id": 15520,
      "firstName": "Yuqi",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 21667,
      "firstName": "Amol",
      "lastName": "Dumrewal",
      "affiliations": []
    },
    {
      "id": 18595,
      "firstName": "Benjamin",
      "lastName": "Fritzsche",
      "affiliations": []
    },
    {
      "id": 19623,
      "firstName": "Kazutoshi",
      "lastName": "Sumiya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22697,
      "firstName": "Olivier",
      "lastName": "Augereau",
      "affiliations": []
    },
    {
      "id": 16557,
      "firstName": "Daisaku",
      "lastName": "Shibata",
      "affiliations": []
    },
    {
      "id": 15536,
      "firstName": "Josh",
      "lastName": "Cherian",
      "affiliations": []
    },
    {
      "id": 13488,
      "firstName": "Samuel",
      "lastName": "Kaski",
      "affiliations": []
    },
    {
      "id": 20656,
      "firstName": "Bengt J.",
      "lastName": "Nilsson",
      "affiliations": []
    },
    {
      "id": 21683,
      "firstName": "Hyejin",
      "lastName": "Jang",
      "affiliations": []
    },
    {
      "id": 9396,
      "firstName": "Chia-Min",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 22711,
      "firstName": "Nikolaos",
      "lastName": "Avouris",
      "affiliations": []
    },
    {
      "id": 19639,
      "firstName": "Alexander",
      "lastName": "Eriksson",
      "affiliations": []
    },
    {
      "id": 18616,
      "firstName": "Fatema",
      "lastName": "Akbar",
      "affiliations": []
    },
    {
      "id": 14521,
      "firstName": "Zhaolin",
      "lastName": "Yao",
      "affiliations": []
    },
    {
      "id": 10425,
      "firstName": "James",
      "lastName": "Brown",
      "middleInitial": "W H",
      "affiliations": []
    },
    {
      "id": 21690,
      "firstName": "M. Ehsan",
      "lastName": "Hoque",
      "affiliations": []
    },
    {
      "id": 8379,
      "firstName": "Tommy",
      "lastName": "Sandbank",
      "affiliations": []
    },
    {
      "id": 10430,
      "firstName": "Hayley",
      "lastName": "Hung",
      "affiliations": []
    },
    {
      "id": 19647,
      "firstName": "Boskin",
      "lastName": "Erkocevic",
      "affiliations": []
    },
    {
      "id": 23744,
      "firstName": "Rafal",
      "lastName": "Kocielnik",
      "affiliations": []
    },
    {
      "id": 17602,
      "firstName": "Christina",
      "lastName": "Katsini",
      "affiliations": []
    },
    {
      "id": 17622,
      "firstName": "Tobias",
      "lastName": "Czauderna",
      "affiliations": []
    },
    {
      "id": 21718,
      "firstName": "Kodzo",
      "lastName": "Wegba",
      "affiliations": []
    },
    {
      "id": 14553,
      "firstName": "Aaron",
      "lastName": "Quigley",
      "affiliations": []
    },
    {
      "id": 8410,
      "firstName": "Benett",
      "lastName": "Axtell",
      "affiliations": []
    },
    {
      "id": 14555,
      "firstName": "Yuichiro",
      "lastName": "Kinoshita",
      "affiliations": []
    },
    {
      "id": 16609,
      "firstName": "Dereck",
      "lastName": "Toker",
      "affiliations": []
    },
    {
      "id": 10465,
      "firstName": "Reza",
      "lastName": "Asadi",
      "affiliations": []
    },
    {
      "id": 18659,
      "firstName": "Shuai",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 23779,
      "firstName": "Hiromu",
      "lastName": "Yakura",
      "affiliations": []
    },
    {
      "id": 19688,
      "firstName": "Umema",
      "lastName": "Bohari",
      "middleInitial": "Hakimuddin",
      "affiliations": []
    },
    {
      "id": 10475,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 18667,
      "firstName": "Budrul",
      "lastName": "Ahsan",
      "affiliations": []
    },
    {
      "id": 24812,
      "firstName": "YI-CHIEH",
      "lastName": "LEE",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24813,
      "firstName": "Chenhao",
      "lastName": "Tan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24814,
      "firstName": "Leah",
      "lastName": "Findlater",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17646,
      "firstName": "Avi",
      "lastName": "Kaplan",
      "affiliations": []
    },
    {
      "id": 24815,
      "firstName": "Pattie",
      "lastName": "Maes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24816,
      "firstName": "Jonathan",
      "lastName": "Dodge",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24817,
      "firstName": "Tom",
      "lastName": "Yeh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24818,
      "firstName": "Bart",
      "lastName": "Knijnenburg",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24819,
      "firstName": "Peter",
      "lastName": "Brusilovsky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24820,
      "firstName": "Jean",
      "lastName": "Song",
      "middleInitial": "Y",
      "affiliations": []
    },
    {
      "id": 24821,
      "firstName": "Oznur",
      "lastName": "ALKAN",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24822,
      "firstName": "Aidong",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24823,
      "firstName": "Wai",
      "lastName": "Fu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24824,
      "firstName": "Edward",
      "lastName": "Lank",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24825,
      "firstName": "Mohammad Rafayet",
      "lastName": "Ali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24826,
      "firstName": "Felix",
      "lastName": "Putze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21755,
      "firstName": "Andrew",
      "lastName": "Anderson",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 24827,
      "firstName": "Alexandros",
      "lastName": "Mouzakitis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24828,
      "firstName": "Juho",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 9469,
      "firstName": "Ciera",
      "lastName": "Crowell",
      "affiliations": []
    },
    {
      "id": 24829,
      "firstName": "Diego",
      "lastName": "Gómez-Zará",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24830,
      "firstName": "Michal",
      "lastName": "Shmueli-Scheuer",
      "affiliations": []
    },
    {
      "id": 24831,
      "firstName": "Simone",
      "lastName": "Stumpf",
      "affiliations": []
    },
    {
      "id": 24832,
      "firstName": "Granit",
      "lastName": "Luzhnica",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24833,
      "firstName": "Itiro",
      "lastName": "Siio",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24834,
      "firstName": "Adam",
      "lastName": "Jatowt",
      "affiliations": []
    },
    {
      "id": 24835,
      "firstName": "Jean",
      "lastName": "Vanderdonckt",
      "affiliations": []
    },
    {
      "id": 9476,
      "firstName": "Catherine",
      "lastName": "Buell",
      "affiliations": []
    },
    {
      "id": 24836,
      "firstName": "Rie",
      "lastName": "Kamikubo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24837,
      "firstName": "Francesco",
      "lastName": "Ricci",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21766,
      "firstName": "Koji",
      "lastName": "Tsukada",
      "affiliations": []
    },
    {
      "id": 24838,
      "firstName": "Andreas",
      "lastName": "Bulling",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24839,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24840,
      "firstName": "Paritosh",
      "lastName": "Bahirat",
      "affiliations": []
    },
    {
      "id": 24841,
      "firstName": "Daniel",
      "lastName": "Reinhardt",
      "affiliations": []
    },
    {
      "id": 24842,
      "firstName": "Jordan",
      "lastName": "Boyd-Graber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24843,
      "firstName": "Andreas",
      "lastName": "Butz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21772,
      "firstName": "Tarmo",
      "lastName": "Robal",
      "affiliations": []
    },
    {
      "id": 24844,
      "firstName": "Patrick",
      "lastName": "Langdon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 17676,
      "firstName": "Rosane",
      "lastName": "Minghim",
      "affiliations": []
    },
    {
      "id": 24845,
      "firstName": "Shoko",
      "lastName": "Wakamiya",
      "affiliations": []
    },
    {
      "id": 24846,
      "firstName": "Timothy",
      "lastName": "Bickmore",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24847,
      "firstName": "Elizabeth",
      "lastName": "Daly",
      "middleInitial": "M.",
      "affiliations": []
    },
    {
      "id": 24848,
      "firstName": "Heinrich",
      "lastName": "Hussmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12560,
      "firstName": "Rocio",
      "lastName": "Chongtay",
      "affiliations": []
    },
    {
      "id": 24849,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24850,
      "firstName": "Malin",
      "lastName": "Eiband",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24851,
      "firstName": "Yunfeng",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24852,
      "firstName": "Yukiko",
      "lastName": "Kawai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24853,
      "firstName": "Chi-Hsien",
      "lastName": "Yen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24854,
      "firstName": "Keiichiro",
      "lastName": "Hoashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12566,
      "firstName": "Michael",
      "lastName": "Barz",
      "affiliations": []
    },
    {
      "id": 24855,
      "firstName": "Lee",
      "lastName": "Skrypchuk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24856,
      "firstName": "Jörn",
      "lastName": "Hurtienne",
      "affiliations": []
    },
    {
      "id": 24857,
      "firstName": "Ziang",
      "lastName": "Xiao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 22809,
      "firstName": "Mei-Ling",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 24858,
      "firstName": "Yoichi",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24859,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24860,
      "firstName": "Tsvi",
      "lastName": "Kuflik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24861,
      "firstName": "Ioannis",
      "lastName": "Politis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24862,
      "firstName": "Mubbasir",
      "lastName": "Kapadia",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24863,
      "firstName": "Markus",
      "lastName": "Gross",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24864,
      "firstName": "Sasha",
      "lastName": "Schriber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24865,
      "firstName": "Chun-Hua",
      "lastName": "Tsai",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24866,
      "firstName": "Philipp",
      "lastName": "Müller",
      "middleInitial": "Matthias",
      "affiliations": []
    },
    {
      "id": 19746,
      "firstName": "Xiao",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 24867,
      "firstName": "Evangelos",
      "lastName": "Milios",
      "affiliations": []
    },
    {
      "id": 24868,
      "firstName": "Michelle",
      "lastName": "Zhou",
      "affiliations": []
    },
    {
      "id": 24869,
      "firstName": "Ivania",
      "lastName": "Donoso-Guzmán",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24870,
      "firstName": "Cristina",
      "lastName": "Conati",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24871,
      "firstName": "Kevin",
      "lastName": "Seppi",
      "affiliations": []
    },
    {
      "id": 12583,
      "firstName": "Nathan",
      "lastName": "Hodas",
      "affiliations": []
    },
    {
      "id": 24872,
      "firstName": "Q. Vera",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24873,
      "firstName": "Min Hun",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19753,
      "firstName": "Panote",
      "lastName": "Siriaraya",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24874,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24875,
      "firstName": "Denis",
      "lastName": "Parra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24876,
      "firstName": "Tatsuya",
      "lastName": "Kawahara",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24877,
      "firstName": "Yosra",
      "lastName": "Rekik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24878,
      "firstName": "Bing-Yu",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 21806,
      "firstName": "Vikas",
      "lastName": "Ashok",
      "affiliations": []
    },
    {
      "id": 24879,
      "firstName": "Yasuaki",
      "lastName": "Kakehi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 12591,
      "firstName": "Melissa",
      "lastName": "Roemmele",
      "affiliations": []
    },
    {
      "id": 24880,
      "firstName": "Shengdong",
      "lastName": "Zhao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 15664,
      "firstName": "IV",
      "lastName": "Ramakrishnan",
      "affiliations": []
    },
    {
      "id": 24881,
      "firstName": "Chi-Lan",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 10546,
      "firstName": "PAUL",
      "lastName": "DUBERSTEIN",
      "affiliations": []
    },
    {
      "id": 24882,
      "firstName": "Gloria",
      "lastName": "Mark",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 11570,
      "firstName": "Kristiina",
      "lastName": "Mannermaa",
      "affiliations": []
    },
    {
      "id": 24883,
      "firstName": "Walter",
      "lastName": "Lasecki",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 13619,
      "firstName": "Jeff",
      "lastName": "Nichols",
      "affiliations": []
    },
    {
      "id": 24884,
      "firstName": "Naomi",
      "lastName": "Yamashita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24885,
      "firstName": "Cosmin",
      "lastName": "Munteanu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24886,
      "firstName": "Jun",
      "lastName": "Rekimoto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 24887,
      "firstName": "Pradipta",
      "lastName": "Biswas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 8505,
      "firstName": "Kunihiko",
      "lastName": "Sato",
      "affiliations": []
    },
    {
      "id": 21819,
      "firstName": "Tanja",
      "lastName": "Schultz",
      "affiliations": []
    },
    {
      "id": 23867,
      "firstName": "Serengul",
      "lastName": "Smith",
      "affiliations": []
    },
    {
      "id": 17726,
      "firstName": "Liliana",
      "lastName": "Ardissono",
      "affiliations": []
    },
    {
      "id": 18754,
      "firstName": "Chunmeng",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 21826,
      "firstName": "Eiji",
      "lastName": "Aramaki",
      "affiliations": []
    },
    {
      "id": 12612,
      "firstName": "Pedro",
      "lastName": "Barcha",
      "affiliations": []
    },
    {
      "id": 14662,
      "firstName": "Alison",
      "lastName": "Smith",
      "affiliations": []
    },
    {
      "id": 8518,
      "firstName": "Nathan",
      "lastName": "Hilliard",
      "affiliations": []
    },
    {
      "id": 18761,
      "firstName": "Mikael",
      "lastName": "Hammar",
      "affiliations": []
    },
    {
      "id": 12620,
      "firstName": "Sujin",
      "lastName": "Jeong",
      "affiliations": []
    },
    {
      "id": 13650,
      "firstName": "Wojciech",
      "lastName": "Witoń",
      "affiliations": []
    },
    {
      "id": 18770,
      "firstName": "Mikko",
      "lastName": "Salminen",
      "affiliations": []
    },
    {
      "id": 18773,
      "firstName": "Ehsan",
      "lastName": "Sherkat",
      "affiliations": []
    },
    {
      "id": 11606,
      "firstName": "Bongjun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 9558,
      "firstName": "Zhenyu",
      "lastName": "Liao",
      "affiliations": []
    },
    {
      "id": 23894,
      "firstName": "Hanfei",
      "lastName": "Ren",
      "affiliations": []
    },
    {
      "id": 8542,
      "firstName": "Inge",
      "lastName": "Vejsbjerg",
      "affiliations": []
    },
    {
      "id": 21859,
      "firstName": "Peter",
      "lastName": "Dolog",
      "affiliations": []
    },
    {
      "id": 20837,
      "firstName": "Dongeon",
      "lastName": "Lee",
      "affiliations": []
    },
    {
      "id": 14697,
      "firstName": "Mitesh",
      "lastName": "Patel",
      "affiliations": []
    },
    {
      "id": 8556,
      "firstName": "Pin Sym",
      "lastName": "Foong",
      "affiliations": []
    },
    {
      "id": 11630,
      "firstName": "Zih-Hong",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 10608,
      "firstName": "Layne",
      "lastName": "Hubbard",
      "middleInitial": "Jackson",
      "affiliations": []
    },
    {
      "id": 9588,
      "firstName": "Xuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 9589,
      "firstName": "Shunmpei",
      "lastName": "Sano",
      "affiliations": []
    },
    {
      "id": 21884,
      "firstName": "Yasuo",
      "lastName": "Kawai",
      "affiliations": []
    },
    {
      "id": 12670,
      "firstName": "Bruce",
      "lastName": "Draper",
      "affiliations": []
    },
    {
      "id": 19840,
      "firstName": "Qiyu",
      "lastName": "Zhi",
      "affiliations": []
    },
    {
      "id": 22913,
      "firstName": "Alexander",
      "lastName": "Prange",
      "affiliations": []
    },
    {
      "id": 9602,
      "firstName": "Ch. Md. Rakin",
      "lastName": "Haider",
      "affiliations": []
    },
    {
      "id": 22919,
      "firstName": "Takashi",
      "lastName": "Miyaki",
      "affiliations": []
    },
    {
      "id": 11656,
      "firstName": "Seraphina",
      "lastName": "Yong",
      "affiliations": []
    },
    {
      "id": 17802,
      "firstName": "Claudio",
      "lastName": "Gutierrez",
      "affiliations": []
    },
    {
      "id": 19851,
      "firstName": "David",
      "lastName": "Konopnicki",
      "affiliations": []
    },
    {
      "id": 17803,
      "firstName": "Walter",
      "lastName": "Scheirer",
      "affiliations": []
    },
    {
      "id": 20878,
      "firstName": "Yue",
      "lastName": "Zhao",
      "affiliations": []
    },
    {
      "id": 13712,
      "firstName": "Takayuki",
      "lastName": "Itoh",
      "affiliations": []
    },
    {
      "id": 22932,
      "firstName": "Kentaro",
      "lastName": "Go",
      "affiliations": []
    },
    {
      "id": 15765,
      "firstName": "Ronald",
      "lastName": "Metoyer",
      "affiliations": []
    },
    {
      "id": 9622,
      "firstName": "Hongda",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 23958,
      "firstName": "Larry",
      "lastName": "Birnbaum",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19862,
      "firstName": "Carita",
      "lastName": "Paradis",
      "affiliations": []
    },
    {
      "id": 11673,
      "firstName": "Shiori",
      "lastName": "Itou",
      "affiliations": []
    },
    {
      "id": 22941,
      "firstName": "Magnus",
      "lastName": "Sahlgren",
      "affiliations": []
    },
    {
      "id": 11678,
      "firstName": "Soh",
      "lastName": "Masuko",
      "affiliations": []
    },
    {
      "id": 20897,
      "firstName": "Ricky",
      "lastName": "Sethi",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 18853,
      "firstName": "Yuemeng",
      "lastName": "Li",
      "affiliations": []
    },
    {
      "id": 20902,
      "firstName": "Yichao",
      "lastName": "Lu",
      "affiliations": []
    },
    {
      "id": 17837,
      "firstName": "Mike",
      "lastName": "Bradley",
      "affiliations": []
    },
    {
      "id": 23981,
      "firstName": "Mai",
      "lastName": "Miyabe",
      "affiliations": []
    },
    {
      "id": 14766,
      "firstName": "João",
      "lastName": "Guerreiro",
      "affiliations": []
    },
    {
      "id": 20916,
      "firstName": "June",
      "lastName": "Han",
      "affiliations": []
    },
    {
      "id": 11703,
      "firstName": "Kosuke",
      "lastName": "Sato",
      "affiliations": []
    },
    {
      "id": 12727,
      "firstName": "Denis",
      "lastName": "Parra",
      "affiliations": []
    },
    {
      "id": 15799,
      "firstName": "Cecile",
      "lastName": "Paris",
      "affiliations": []
    },
    {
      "id": 22970,
      "firstName": "Jina",
      "lastName": "Suh",
      "affiliations": []
    },
    {
      "id": 16826,
      "firstName": "Clifford",
      "lastName": "De Raffaele",
      "affiliations": []
    },
    {
      "id": 18875,
      "firstName": "Christos",
      "lastName": "Fidas",
      "affiliations": []
    },
    {
      "id": 20926,
      "firstName": "Osnat",
      "lastName": "Mokryn",
      "affiliations": []
    },
    {
      "id": 17855,
      "firstName": "Anne",
      "lastName": "Ross",
      "affiliations": []
    },
    {
      "id": 12737,
      "firstName": "Dietmar",
      "lastName": "Jannach",
      "affiliations": []
    },
    {
      "id": 21953,
      "firstName": "Daniel",
      "lastName": "Sonntag",
      "affiliations": []
    },
    {
      "id": 15809,
      "firstName": "Michinari",
      "lastName": "Kono",
      "affiliations": []
    },
    {
      "id": 9666,
      "firstName": "Lei",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 19910,
      "firstName": "Anders",
      "lastName": "Hast",
      "affiliations": []
    },
    {
      "id": 14795,
      "firstName": "Kyle",
      "lastName": "Kotowick",
      "affiliations": []
    },
    {
      "id": 11725,
      "firstName": "Ching-Chun",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 9682,
      "firstName": "Shubham",
      "lastName": "Atreja",
      "affiliations": []
    },
    {
      "id": 21971,
      "firstName": "Shiliang",
      "lastName": "Zuo",
      "affiliations": []
    },
    {
      "id": 14804,
      "firstName": "Noam",
      "lastName": "Shomron",
      "affiliations": []
    },
    {
      "id": 21973,
      "firstName": "Tachi",
      "lastName": "Ikeda",
      "affiliations": []
    },
    {
      "id": 8662,
      "firstName": "Abhilash",
      "lastName": "Menon",
      "affiliations": []
    },
    {
      "id": 19934,
      "firstName": "Grace",
      "lastName": "Ngai",
      "affiliations": []
    },
    {
      "id": 23009,
      "firstName": "Chiara",
      "lastName": "Pulice",
      "affiliations": []
    },
    {
      "id": 24037,
      "firstName": "Jodok",
      "lastName": "Vieli",
      "affiliations": []
    },
    {
      "id": 11749,
      "firstName": "Takashi",
      "lastName": "Totsuka",
      "affiliations": []
    },
    {
      "id": 20966,
      "firstName": "Elayne",
      "lastName": "Ruane",
      "affiliations": []
    },
    {
      "id": 8683,
      "firstName": "Domenico",
      "lastName": "Saccà",
      "affiliations": []
    },
    {
      "id": 9707,
      "firstName": "Erin You",
      "lastName": "Wu",
      "affiliations": []
    },
    {
      "id": 19952,
      "firstName": "Kiminobu",
      "lastName": "Makino",
      "affiliations": []
    },
    {
      "id": 8690,
      "firstName": "Masahiro",
      "lastName": "Hamasaki",
      "affiliations": []
    },
    {
      "id": 22003,
      "firstName": "Soichiro",
      "lastName": "Matsuda",
      "affiliations": []
    },
    {
      "id": 14835,
      "firstName": "Toru",
      "lastName": "Ishida",
      "affiliations": []
    },
    {
      "id": 8694,
      "firstName": "Yangfeng",
      "lastName": "Ji",
      "affiliations": []
    },
    {
      "id": 17913,
      "firstName": "Tomi",
      "lastName": "Peltola",
      "affiliations": []
    },
    {
      "id": 16889,
      "firstName": "Ji-hyung",
      "lastName": "Park",
      "affiliations": []
    },
    {
      "id": 18938,
      "firstName": "Mark",
      "lastName": "Last",
      "affiliations": []
    },
    {
      "id": 13819,
      "firstName": "Shiri",
      "lastName": "Kremer-Davidson",
      "affiliations": []
    },
    {
      "id": 22013,
      "firstName": "Kostiantyn",
      "lastName": "Kucher",
      "affiliations": []
    },
    {
      "id": 18942,
      "firstName": "Angus",
      "lastName": "Lawlor",
      "affiliations": []
    },
    {
      "id": 23044,
      "firstName": "Po-Tsung",
      "lastName": "Chiu",
      "affiliations": []
    },
    {
      "id": 22023,
      "firstName": "Andrea",
      "lastName": "Chamorro",
      "affiliations": []
    },
    {
      "id": 18951,
      "firstName": "Damilola",
      "lastName": "Adebayo",
      "affiliations": []
    },
    {
      "id": 15879,
      "firstName": "Kirsten",
      "lastName": "Revell",
      "affiliations": []
    },
    {
      "id": 24073,
      "firstName": "Yihong",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 8714,
      "firstName": "Noemi",
      "lastName": "Mauro",
      "affiliations": []
    },
    {
      "id": 13834,
      "firstName": "Koichi",
      "lastName": "Miyazaki",
      "affiliations": []
    },
    {
      "id": 9740,
      "firstName": "Kingsley",
      "lastName": "Stephens",
      "affiliations": []
    },
    {
      "id": 9741,
      "firstName": "Radu-Daniel",
      "lastName": "Vatavu",
      "affiliations": []
    },
    {
      "id": 17936,
      "firstName": "Prateeti",
      "lastName": "Mohapatra",
      "affiliations": []
    },
    {
      "id": 11796,
      "firstName": "Tiffany C.K.",
      "lastName": "Kwok",
      "affiliations": []
    },
    {
      "id": 15897,
      "firstName": "Li",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 24092,
      "firstName": "Toshihiko",
      "lastName": "Yamasaki",
      "affiliations": []
    },
    {
      "id": 13853,
      "firstName": "Jun",
      "lastName": "Goto",
      "affiliations": []
    },
    {
      "id": 22047,
      "firstName": "Kai",
      "lastName": "Kunze",
      "affiliations": []
    },
    {
      "id": 8747,
      "firstName": "Dylan",
      "lastName": "Cassady",
      "affiliations": []
    },
    {
      "id": 10795,
      "firstName": "Yanxia",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 8749,
      "firstName": "Kris",
      "lastName": "Kitani",
      "affiliations": []
    },
    {
      "id": 8750,
      "firstName": "Michael",
      "lastName": "Hufnagel",
      "affiliations": []
    },
    {
      "id": 22065,
      "firstName": "Satoshi",
      "lastName": "Otsuka",
      "affiliations": []
    },
    {
      "id": 21042,
      "firstName": "Carrie",
      "lastName": "Demmans Epp",
      "affiliations": []
    },
    {
      "id": 8754,
      "firstName": "Mareike",
      "lastName": "Haug",
      "affiliations": []
    },
    {
      "id": 14904,
      "firstName": "Saemi",
      "lastName": "Choi",
      "affiliations": []
    },
    {
      "id": 20028,
      "firstName": "Cedric",
      "lastName": "Caremel",
      "affiliations": []
    },
    {
      "id": 8765,
      "firstName": "Tom",
      "lastName": "Hope",
      "affiliations": []
    },
    {
      "id": 12862,
      "firstName": "Marcel",
      "lastName": "Marti",
      "affiliations": []
    },
    {
      "id": 19010,
      "firstName": "Robin",
      "lastName": "Murphy",
      "middleInitial": "R.",
      "affiliations": []
    },
    {
      "id": 12869,
      "firstName": "Suwen",
      "lastName": "Lin",
      "affiliations": []
    },
    {
      "id": 22085,
      "firstName": "Jason",
      "lastName": "Orlosky",
      "affiliations": []
    },
    {
      "id": 24135,
      "firstName": "Yun",
      "lastName": "Jang",
      "affiliations": []
    },
    {
      "id": 8775,
      "firstName": "Petru",
      "lastName": "Cioată",
      "middleInitial": "Vasile",
      "affiliations": []
    },
    {
      "id": 14919,
      "firstName": "Masataka",
      "lastName": "Imura",
      "affiliations": []
    },
    {
      "id": 19020,
      "firstName": "Yumiko",
      "lastName": "Mori",
      "affiliations": []
    },
    {
      "id": 24144,
      "firstName": "Ruihai",
      "lastName": "Dong",
      "affiliations": []
    },
    {
      "id": 22096,
      "firstName": "Margaret",
      "lastName": "Burnett",
      "affiliations": []
    },
    {
      "id": 20050,
      "firstName": "Steven",
      "lastName": "Drucker",
      "affiliations": []
    },
    {
      "id": 9813,
      "firstName": "Mitsuo",
      "lastName": "Yoshida",
      "affiliations": []
    },
    {
      "id": 13911,
      "firstName": "Toshiyuki",
      "lastName": "Takasaki",
      "affiliations": []
    },
    {
      "id": 18017,
      "firstName": "Nancy",
      "lastName": "O'Brien",
      "middleInitial": "D",
      "affiliations": []
    },
    {
      "id": 9826,
      "firstName": "Khashayar",
      "lastName": "Rohanimanesh",
      "affiliations": []
    },
    {
      "id": 20067,
      "firstName": "Andreas",
      "lastName": "Dengel",
      "affiliations": []
    },
    {
      "id": 18028,
      "firstName": "Hanae",
      "lastName": "Rateau",
      "affiliations": []
    },
    {
      "id": 24176,
      "firstName": "Jiro",
      "lastName": "Tanaka",
      "affiliations": []
    },
    {
      "id": 21107,
      "firstName": "Frank",
      "lastName": "Rudzicz",
      "affiliations": []
    },
    {
      "id": 24181,
      "firstName": "Yangyang",
      "lastName": "He",
      "affiliations": []
    },
    {
      "id": 23160,
      "firstName": "Ignacio",
      "lastName": "Pérez-Messina",
      "affiliations": []
    },
    {
      "id": 18041,
      "firstName": "Yuka",
      "lastName": "Takei",
      "affiliations": []
    },
    {
      "id": 17018,
      "firstName": "Harshit",
      "lastName": "Agrawal",
      "affiliations": []
    },
    {
      "id": 8828,
      "firstName": "Nunziato",
      "lastName": "Cassavia",
      "affiliations": []
    },
    {
      "id": 12926,
      "firstName": "Shuyang",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 23172,
      "firstName": "Isaac",
      "lastName": "Wang",
      "middleInitial": "Decheng",
      "affiliations": []
    },
    {
      "id": 24196,
      "firstName": "Yusuke",
      "lastName": "Sugano",
      "affiliations": []
    },
    {
      "id": 19083,
      "firstName": "Mingkun",
      "lastName": "Gao",
      "affiliations": []
    },
    {
      "id": 8849,
      "firstName": "Hiroaki",
      "lastName": "Tobita",
      "affiliations": []
    },
    {
      "id": 11922,
      "firstName": "Takuya",
      "lastName": "Enomoto",
      "affiliations": []
    },
    {
      "id": 21138,
      "firstName": "Claudia",
      "lastName": "Hilderbrand",
      "affiliations": []
    },
    {
      "id": 14996,
      "firstName": "Dimitris",
      "lastName": "Paraschakis",
      "affiliations": []
    },
    {
      "id": 24214,
      "firstName": "Orhan",
      "lastName": "Gemikonakli",
      "affiliations": []
    },
    {
      "id": 24218,
      "firstName": "Chenxi",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 20126,
      "firstName": "Andreas",
      "lastName": "Kerren",
      "affiliations": []
    },
    {
      "id": 13983,
      "firstName": "Toshinori",
      "lastName": "Hayashi",
      "affiliations": []
    },
    {
      "id": 24229,
      "firstName": "I-Chao",
      "lastName": "Shen",
      "affiliations": []
    },
    {
      "id": 12965,
      "firstName": "Ting-Ju",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 20134,
      "firstName": "Taro",
      "lastName": "Miyazaki",
      "affiliations": []
    },
    {
      "id": 23211,
      "firstName": "Yoshinori",
      "lastName": "Hijikata",
      "affiliations": []
    },
    {
      "id": 8878,
      "firstName": "Takayuki",
      "lastName": "Itoh",
      "affiliations": []
    },
    {
      "id": 18094,
      "firstName": "Yasushi",
      "lastName": "Matoba",
      "affiliations": []
    },
    {
      "id": 18096,
      "firstName": "Koichi",
      "lastName": "Kise",
      "affiliations": []
    },
    {
      "id": 12977,
      "firstName": "Yong",
      "lastName": "Zheng",
      "affiliations": []
    },
    {
      "id": 24244,
      "firstName": "Bachar",
      "lastName": "Senno",
      "affiliations": []
    },
    {
      "id": 10933,
      "firstName": "Elena",
      "lastName": "Not",
      "affiliations": []
    },
    {
      "id": 14007,
      "firstName": "Takashi",
      "lastName": "Sonoda",
      "affiliations": []
    },
    {
      "id": 10938,
      "firstName": "Yusuke",
      "lastName": "Yamaura",
      "affiliations": []
    },
    {
      "id": 21180,
      "firstName": "Yuki",
      "lastName": "Umezawa",
      "affiliations": []
    },
    {
      "id": 11964,
      "firstName": "MIN GYEONG",
      "lastName": "KIM",
      "affiliations": []
    },
    {
      "id": 23228,
      "firstName": "Shlomo",
      "lastName": "Berkovsky",
      "affiliations": []
    },
    {
      "id": 12989,
      "firstName": "Ted",
      "lastName": "Grover",
      "affiliations": []
    },
    {
      "id": 18109,
      "firstName": "Kaoru",
      "lastName": "Ito",
      "affiliations": []
    },
    {
      "id": 12993,
      "firstName": "P. John",
      "lastName": "Clarkson",
      "affiliations": []
    },
    {
      "id": 11972,
      "firstName": "Artem",
      "lastName": "Yankov",
      "affiliations": []
    },
    {
      "id": 23238,
      "firstName": "Tomoko",
      "lastName": "Ohkuma",
      "affiliations": []
    },
    {
      "id": 18119,
      "firstName": "Takumi",
      "lastName": "Ozawa",
      "affiliations": []
    },
    {
      "id": 22217,
      "firstName": "Elizabeth",
      "lastName": "Clark",
      "affiliations": []
    },
    {
      "id": 9930,
      "firstName": "Ha",
      "lastName": "Trinh",
      "affiliations": []
    },
    {
      "id": 9935,
      "firstName": "Daya",
      "lastName": "Sellman",
      "affiliations": []
    },
    {
      "id": 20178,
      "firstName": "Gonzalo",
      "lastName": "Ramos",
      "affiliations": []
    },
    {
      "id": 16083,
      "firstName": "Harriet",
      "lastName": "Fell",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 15062,
      "firstName": "Jiani",
      "lastName": "Hu",
      "affiliations": []
    },
    {
      "id": 17111,
      "firstName": "Sean",
      "lastName": "Penney",
      "affiliations": []
    },
    {
      "id": 18137,
      "firstName": "Pinata",
      "lastName": "Winoto",
      "affiliations": []
    },
    {
      "id": 23258,
      "firstName": "Kenji",
      "lastName": "Mase",
      "affiliations": []
    },
    {
      "id": 22237,
      "firstName": "JUNICHI",
      "lastName": "YAMAOKA",
      "affiliations": []
    },
    {
      "id": 21215,
      "firstName": "Jaime",
      "lastName": "Ruiz",
      "affiliations": []
    },
    {
      "id": 9955,
      "firstName": "Neville",
      "lastName": "Stanton",
      "affiliations": []
    },
    {
      "id": 15076,
      "firstName": "Chen Hao",
      "lastName": "Cheng",
      "affiliations": []
    },
    {
      "id": 19173,
      "firstName": "Brian",
      "lastName": "Lim",
      "middleInitial": "Y",
      "affiliations": []
    },
    {
      "id": 13043,
      "firstName": "Jonathan",
      "lastName": "Herzig",
      "affiliations": []
    },
    {
      "id": 22264,
      "firstName": "Narumol",
      "lastName": "Vannaprathip",
      "affiliations": []
    },
    {
      "id": 11002,
      "firstName": "Takatsugu",
      "lastName": "Hirayama",
      "affiliations": []
    },
    {
      "id": 20221,
      "firstName": "Meng-Chieh",
      "lastName": "Ko",
      "affiliations": []
    },
    {
      "id": 21246,
      "firstName": "Chu-Hsiang",
      "lastName": "Chang",
      "affiliations": []
    },
    {
      "id": 17150,
      "firstName": "Vinayak",
      "lastName": "Vinayak",
      "affiliations": []
    },
    {
      "id": 20225,
      "firstName": "Daniel",
      "lastName": "Avrahami",
      "affiliations": []
    },
    {
      "id": 12038,
      "firstName": "Diego",
      "lastName": "Gonzalez",
      "affiliations": []
    },
    {
      "id": 23306,
      "firstName": "Hayato",
      "lastName": "Araki",
      "affiliations": []
    },
    {
      "id": 20238,
      "firstName": "Inbal",
      "lastName": "Ronen",
      "affiliations": []
    },
    {
      "id": 19217,
      "firstName": "Zeya",
      "lastName": "Peng",
      "affiliations": []
    },
    {
      "id": 20243,
      "firstName": "VIET-DUY",
      "lastName": "NGUYEN",
      "affiliations": []
    },
    {
      "id": 11028,
      "firstName": "Anjaneyulu",
      "lastName": "Pasala",
      "affiliations": []
    },
    {
      "id": 11037,
      "firstName": "Masahiro",
      "lastName": "Sato",
      "affiliations": []
    },
    {
      "id": 15134,
      "firstName": "Minghao",
      "lastName": "Cai",
      "affiliations": []
    },
    {
      "id": 12062,
      "firstName": "Court",
      "lastName": "Corley",
      "affiliations": []
    },
    {
      "id": 24353,
      "firstName": "Elio",
      "lastName": "Masciari",
      "affiliations": []
    },
    {
      "id": 13094,
      "firstName": "Jeffrey",
      "lastName": "Olenick",
      "affiliations": []
    },
    {
      "id": 17191,
      "firstName": "Gopakumar",
      "lastName": "Gopalakrishnan",
      "affiliations": []
    },
    {
      "id": 18224,
      "firstName": "Adil Hamid",
      "lastName": "Malla",
      "affiliations": []
    },
    {
      "id": 16181,
      "firstName": "Rushit",
      "lastName": "Sanghrajka",
      "affiliations": []
    },
    {
      "id": 13111,
      "firstName": "Nan-Chen",
      "lastName": "Chen",
      "affiliations": []
    },
    {
      "id": 24377,
      "firstName": "Nakae",
      "lastName": "Kazuya",
      "affiliations": []
    },
    {
      "id": 20281,
      "firstName": "Shreyas",
      "lastName": "Kapur",
      "affiliations": []
    },
    {
      "id": 15161,
      "firstName": "Meg",
      "lastName": "Pirrung",
      "affiliations": []
    },
    {
      "id": 9020,
      "firstName": "Hanna",
      "lastName": "Schneider",
      "affiliations": []
    },
    {
      "id": 11069,
      "firstName": "Giulio",
      "lastName": "Jacucci",
      "affiliations": []
    },
    {
      "id": 17215,
      "firstName": "Johan",
      "lastName": "Verwey",
      "affiliations": []
    },
    {
      "id": 23360,
      "firstName": "Janne",
      "lastName": "Timonen",
      "affiliations": []
    },
    {
      "id": 9026,
      "firstName": "Osnat",
      "lastName": "Mokryn",
      "affiliations": []
    },
    {
      "id": 16195,
      "firstName": "hagit",
      "lastName": "ben shoshan",
      "affiliations": []
    },
    {
      "id": 21317,
      "firstName": "Helen",
      "lastName": "Wauck",
      "middleInitial": "C",
      "affiliations": []
    },
    {
      "id": 23371,
      "firstName": "Yu",
      "lastName": "Enokibori",
      "affiliations": []
    },
    {
      "id": 20300,
      "firstName": "Gemma",
      "lastName": "Liu",
      "affiliations": []
    },
    {
      "id": 18253,
      "firstName": "Youhei",
      "lastName": "Obuchi",
      "affiliations": []
    },
    {
      "id": 22353,
      "firstName": "Junichi",
      "lastName": "Yamamoto",
      "affiliations": []
    },
    {
      "id": 21330,
      "firstName": "Kris",
      "lastName": "Luyten",
      "affiliations": []
    },
    {
      "id": 16210,
      "firstName": "Ayano",
      "lastName": "Nishimura",
      "affiliations": []
    },
    {
      "id": 20308,
      "firstName": "Raymond",
      "lastName": "Fok",
      "affiliations": []
    },
    {
      "id": 18260,
      "firstName": "Michael",
      "lastName": "Jugovac",
      "affiliations": []
    },
    {
      "id": 16214,
      "firstName": "Heesun",
      "lastName": "Kim",
      "affiliations": []
    },
    {
      "id": 18265,
      "firstName": "Marios",
      "lastName": "Belk",
      "affiliations": []
    },
    {
      "id": 17245,
      "firstName": "Xinyao",
      "lastName": "Ma",
      "affiliations": []
    },
    {
      "id": 10077,
      "firstName": "Antti",
      "lastName": "Ruonala",
      "affiliations": []
    },
    {
      "id": 12125,
      "firstName": "Shingo",
      "lastName": "Kato",
      "affiliations": []
    },
    {
      "id": 9055,
      "firstName": "Hao-Chuan",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 21344,
      "firstName": "Pedram",
      "lastName": "Daee",
      "affiliations": []
    },
    {
      "id": 16225,
      "firstName": "Shota",
      "lastName": "Shiraga",
      "affiliations": []
    },
    {
      "id": 12135,
      "firstName": "Koki",
      "lastName": "Nagatani",
      "affiliations": []
    },
    {
      "id": 19306,
      "firstName": "Madhusudhan",
      "lastName": "Aithal",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 19309,
      "firstName": "Denis",
      "lastName": "Chêne",
      "affiliations": []
    },
    {
      "id": 22383,
      "firstName": "Viral",
      "lastName": "Parekh",
      "affiliations": []
    },
    {
      "id": 15215,
      "firstName": "Bartosz",
      "lastName": "Janczuk",
      "affiliations": []
    },
    {
      "id": 11121,
      "firstName": "Giuseppe",
      "lastName": "Carenini",
      "affiliations": []
    },
    {
      "id": 20341,
      "firstName": "Dan",
      "lastName": "Linenberg",
      "affiliations": []
    },
    {
      "id": 9080,
      "firstName": "Eran",
      "lastName": "Toch",
      "affiliations": []
    },
    {
      "id": 17273,
      "firstName": "Niklas",
      "lastName": "Ravaja",
      "affiliations": []
    },
    {
      "id": 17275,
      "firstName": "Syed Masum",
      "lastName": "Billah",
      "affiliations": []
    },
    {
      "id": 20352,
      "firstName": "Jesse",
      "lastName": "Smith",
      "affiliations": []
    },
    {
      "id": 15232,
      "firstName": "Ramanathan",
      "lastName": "Subramanian",
      "affiliations": []
    },
    {
      "id": 9089,
      "firstName": "Arnav",
      "lastName": "Kapur",
      "affiliations": []
    },
    {
      "id": 13187,
      "firstName": "Yuanyuan",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 19331,
      "firstName": "Kimberly",
      "lastName": "Van Orden",
      "affiliations": []
    },
    {
      "id": 22404,
      "firstName": "Cecilia",
      "lastName": "di Sciascio",
      "affiliations": []
    },
    {
      "id": 17285,
      "firstName": "William",
      "lastName": "Seeley",
      "affiliations": []
    },
    {
      "id": 22406,
      "firstName": "Riku",
      "lastName": "Takano",
      "affiliations": []
    },
    {
      "id": 20361,
      "firstName": "Ken",
      "lastName": "Wakita",
      "affiliations": []
    },
    {
      "id": 15242,
      "firstName": "Anwesh",
      "lastName": "Basu",
      "affiliations": []
    },
    {
      "id": 10123,
      "firstName": "Julie",
      "lastName": "Shah",
      "affiliations": []
    },
    {
      "id": 18318,
      "firstName": "Patrice",
      "lastName": "Simard",
      "affiliations": []
    },
    {
      "id": 13199,
      "firstName": "Pooja",
      "lastName": "Aggarwal",
      "affiliations": []
    },
    {
      "id": 15248,
      "firstName": "Julian",
      "lastName": "Fazekas-Con",
      "affiliations": []
    },
    {
      "id": 14225,
      "firstName": "Michael Xuelin",
      "lastName": "Huang",
      "affiliations": []
    },
    {
      "id": 17297,
      "firstName": "Maya",
      "lastName": "Barnea",
      "affiliations": []
    },
    {
      "id": 18322,
      "firstName": "Takamichi",
      "lastName": "Nakamoto",
      "affiliations": []
    },
    {
      "id": 11158,
      "firstName": "Fan",
      "lastName": "Yang",
      "affiliations": []
    },
    {
      "id": 12197,
      "firstName": "Qian",
      "lastName": "Zhang",
      "affiliations": []
    },
    {
      "id": 9126,
      "firstName": "Pradyumna",
      "lastName": "Narayana",
      "affiliations": []
    },
    {
      "id": 12202,
      "firstName": "Yijun",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 10158,
      "firstName": "Christoph",
      "lastName": "Lofi",
      "affiliations": []
    },
    {
      "id": 22452,
      "firstName": "Shoya",
      "lastName": "Ishimaru",
      "affiliations": []
    },
    {
      "id": 19382,
      "firstName": "Kenta",
      "lastName": "Kawahara",
      "affiliations": []
    },
    {
      "id": 11191,
      "firstName": "Mazen",
      "lastName": "Salous",
      "affiliations": []
    },
    {
      "id": 11194,
      "firstName": "Qizhang",
      "lastName": "Sun",
      "affiliations": []
    },
    {
      "id": 13243,
      "firstName": "Anthony",
      "lastName": "Ventresque",
      "affiliations": []
    },
    {
      "id": 11196,
      "firstName": "Eduardo",
      "lastName": "Graells-Garrido",
      "affiliations": []
    },
    {
      "id": 13256,
      "firstName": "Takumi",
      "lastName": "Kawahara",
      "affiliations": []
    },
    {
      "id": 9162,
      "firstName": "Nitesh V",
      "lastName": "Chawla",
      "affiliations": []
    },
    {
      "id": 19403,
      "firstName": "Andrew",
      "lastName": "Gordon",
      "middleInitial": "S",
      "affiliations": []
    },
    {
      "id": 21451,
      "firstName": "Ayae",
      "lastName": "Kinoshita",
      "affiliations": []
    },
    {
      "id": 19405,
      "firstName": "Björn",
      "lastName": "Broden",
      "affiliations": []
    },
    {
      "id": 12239,
      "firstName": "Bettina",
      "lastName": "Berendt",
      "affiliations": []
    },
    {
      "id": 19408,
      "firstName": "Shiori",
      "lastName": "Tomimatsu",
      "affiliations": []
    },
    {
      "id": 12247,
      "firstName": "Wencheng",
      "lastName": "Wang",
      "affiliations": []
    },
    {
      "id": 23513,
      "firstName": "David",
      "lastName": "Massimo",
      "affiliations": []
    },
    {
      "id": 23515,
      "firstName": "Gaelle",
      "lastName": "Calvary",
      "affiliations": []
    },
    {
      "id": 13277,
      "firstName": "Varun",
      "lastName": "Kumar",
      "affiliations": []
    },
    {
      "id": 20447,
      "firstName": "Steve",
      "lastName": "Kozlowski",
      "affiliations": []
    },
    {
      "id": 20455,
      "firstName": "Yikun",
      "lastName": "Xian",
      "affiliations": []
    },
    {
      "id": 12264,
      "firstName": "Kiyoharu",
      "lastName": "Aizawa",
      "affiliations": []
    },
    {
      "id": 13290,
      "firstName": "Seyednaser",
      "lastName": "Nourashrafeddin",
      "affiliations": []
    },
    {
      "id": 12267,
      "firstName": "Eugene Yujun",
      "lastName": "Fu",
      "affiliations": []
    },
    {
      "id": 18415,
      "firstName": "Shunichi",
      "lastName": "Suwa",
      "affiliations": []
    },
    {
      "id": 9203,
      "firstName": "Netta",
      "lastName": "Rager",
      "affiliations": []
    },
    {
      "id": 22517,
      "firstName": "Laurent",
      "lastName": "Grisoni",
      "affiliations": []
    },
    {
      "id": 20469,
      "firstName": "Weihua",
      "lastName": "Pei",
      "affiliations": []
    },
    {
      "id": 21493,
      "firstName": "Kimberly",
      "lastName": "Parkhurst",
      "affiliations": []
    },
    {
      "id": 14327,
      "firstName": "Yaakov",
      "lastName": "Danone",
      "affiliations": []
    },
    {
      "id": 16378,
      "firstName": "Mondheera",
      "lastName": "Pituxcoosuvarn",
      "affiliations": []
    },
    {
      "id": 18426,
      "firstName": "Noah",
      "lastName": "Smith",
      "affiliations": []
    },
    {
      "id": 22523,
      "firstName": "Tomoyasu",
      "lastName": "Nakano",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 18,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-02-10 13:50:15+00"
  }
}