{
  "schemeVersion": 7,
  "cc_licence": "Content of this file is licensed under a CC BY-NC-SA 4.0 license. For details see https://creativecommons.org/licenses/by-nc-sa/4.0/",
  "conference": {
    "id": 10061,
    "startDate": 1618272000000,
    "endDate": 1618617600000,
    "shortName": "IUI",
    "name": "IUI 2021",
    "year": 2021,
    "fullName": "26th ACM International Conference on Intelligent User Interfaces",
    "url": "https://iui.acm.org/2021",
    "location": "Virtual (Hosted by Texas A&M University)",
    "timeZoneOffset": -300,
    "logoUrl": "https://files.sigchi.org/conference/logo/36917748-3457-2951-d31e-3979d02f270a.png",
    "timeZoneName": "America/Chicago"
  },
  "sponsors": [],
  "sponsorLevels": [
    {
      "id": 10095,
      "name": "Sponsors",
      "rank": 1,
      "isDefault": true
    }
  ],
  "floors": [],
  "rooms": [
    {
      "id": 10482,
      "name": "Papers Room",
      "typeId": 11793,
      "setup": "Theatre",
      "note": "Virtual"
    },
    {
      "id": 10483,
      "name": "Events Room",
      "typeId": 11794,
      "setup": "Theatre",
      "note": "Virtual"
    }
  ],
  "tracks": [
    {
      "id": 11171,
      "name": "IUI 2021 Papers",
      "typeId": 11793
    }
  ],
  "contentTypes": [
    {
      "id": 11786,
      "name": "SIG",
      "color": "#7a0177",
      "duration": 90
    },
    {
      "id": 11787,
      "name": "Case Study",
      "color": "#993404",
      "duration": 20,
      "displayName": "Case Studies"
    },
    {
      "id": 11788,
      "name": "Course",
      "color": "#e6550d",
      "duration": 90,
      "displayName": "Courses"
    },
    {
      "id": 11789,
      "name": "Event",
      "color": "#fecc5c",
      "duration": 0,
      "displayName": "Events"
    },
    {
      "id": 11790,
      "name": "Invited Talk",
      "color": "#66c2a4",
      "duration": 90,
      "displayName": "Invited Talks"
    },
    {
      "id": 11791,
      "name": "Operations",
      "color": "#006d2c",
      "duration": 90
    },
    {
      "id": 11792,
      "name": "Panel",
      "color": "#6baed6",
      "duration": 90,
      "displayName": "Panels"
    },
    {
      "id": 11794,
      "name": "Plenary",
      "color": "#756bb1",
      "duration": 90
    },
    {
      "id": 11795,
      "name": "Workshop",
      "color": "#de2d26",
      "duration": 240,
      "displayName": "Workshops"
    },
    {
      "id": 11793,
      "name": "Paper",
      "color": "#08519c",
      "duration": 10,
      "displayName": "Papers"
    }
  ],
  "timeSlots": [
    {
      "id": 11923,
      "type": "SESSION",
      "startDate": 1618657500000,
      "endDate": 1618661100000
    },
    {
      "id": 11925,
      "type": "SESSION",
      "startDate": 1618665900000,
      "endDate": 1618669500000
    },
    {
      "id": 11931,
      "type": "SESSION",
      "startDate": 1618477200000,
      "endDate": 1618480800000
    },
    {
      "id": 11922,
      "type": "SESSION",
      "startDate": 1618653900000,
      "endDate": 1618656900000
    },
    {
      "id": 11907,
      "type": "SESSION",
      "startDate": 1618395000000,
      "endDate": 1618398600000
    },
    {
      "id": 11908,
      "type": "SESSION",
      "startDate": 1618399200000,
      "endDate": 1618402800000
    },
    {
      "id": 11910,
      "type": "SESSION",
      "startDate": 1618407600000,
      "endDate": 1618411200000
    },
    {
      "id": 11911,
      "type": "SESSION",
      "startDate": 1618411800000,
      "endDate": 1618415400000
    },
    {
      "id": 11926,
      "type": "SESSION",
      "startDate": 1618670100000,
      "endDate": 1618673700000
    },
    {
      "id": 11912,
      "type": "SESSION",
      "startDate": 1618481400000,
      "endDate": 1618485000000
    },
    {
      "id": 11913,
      "type": "SESSION",
      "startDate": 1618485600000,
      "endDate": 1618489200000
    },
    {
      "id": 11915,
      "type": "SESSION",
      "startDate": 1618494000000,
      "endDate": 1618497600000
    },
    {
      "id": 11916,
      "type": "SESSION",
      "startDate": 1618498200000,
      "endDate": 1618501800000
    },
    {
      "id": 11927,
      "type": "SESSION",
      "startDate": 1618403400000,
      "endDate": 1618407000000
    },
    {
      "id": 11929,
      "type": "SESSION",
      "startDate": 1618576200000,
      "endDate": 1618579800000
    },
    {
      "id": 11930,
      "type": "SESSION",
      "startDate": 1618661700000,
      "endDate": 1618665300000
    },
    {
      "id": 11932,
      "type": "SESSION",
      "startDate": 1618563600000,
      "endDate": 1618567200000
    },
    {
      "id": 11933,
      "type": "SESSION",
      "startDate": 1618390800000,
      "endDate": 1618394400000
    },
    {
      "id": 11917,
      "type": "SESSION",
      "startDate": 1618567800000,
      "endDate": 1618571400000
    },
    {
      "id": 11918,
      "type": "SESSION",
      "startDate": 1618572000000,
      "endDate": 1618575600000
    },
    {
      "id": 11920,
      "type": "SESSION",
      "startDate": 1618580400000,
      "endDate": 1618584000000
    },
    {
      "id": 11921,
      "type": "SESSION",
      "startDate": 1618584600000,
      "endDate": 1618588200000
    },
    {
      "id": 11928,
      "type": "SESSION",
      "startDate": 1618489800000,
      "endDate": 1618493400000
    },
    {
      "id": 11934,
      "type": "SESSION",
      "startDate": 1618650000000,
      "endDate": 1618653600000
    }
  ],
  "sessions": [
    {
      "id": 58108,
      "name": "AI in Education & Intelligent Tutoring System",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        58000,
        58002,
        58001,
        57981
      ],
      "timeSlotId": 11925
    },
    {
      "id": 58109,
      "name": "Conversational AI & Human-AI Partnership",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57994,
        58008,
        57965,
        57953
      ],
      "timeSlotId": 11916
    },
    {
      "id": 58110,
      "name": "Evaluation",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        58005,
        57992,
        57964,
        57959
      ],
      "timeSlotId": 11907
    },
    {
      "id": 58111,
      "name": "Fairness & Explanation",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57984,
        57999,
        57969,
        57972,
        57957
      ],
      "timeSlotId": 11910
    },
    {
      "id": 58112,
      "name": "Gestures & Accessibility",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        58012,
        58003,
        57955,
        57950
      ],
      "timeSlotId": 11913
    },
    {
      "id": 58113,
      "name": "Human-in-the-Loop",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        58009,
        57993,
        57951,
        57982,
        57991
      ],
      "timeSlotId": 11920
    },
    {
      "id": 58114,
      "name": "Interactive Design",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57958,
        57952,
        57977,
        58006,
        57989
      ],
      "timeSlotId": 11912
    },
    {
      "id": 58115,
      "name": "Media, Art and Interactive Technology",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57985,
        57962,
        57995,
        57971
      ],
      "timeSlotId": 11923
    },
    {
      "id": 58116,
      "name": "Medical/Healthcare AI",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57960,
        57987,
        57983
      ],
      "timeSlotId": 11921
    },
    {
      "id": 58117,
      "name": "ML Model Visualization and Interaction",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57996,
        57980,
        57968,
        57978
      ],
      "timeSlotId": 11908
    },
    {
      "id": 58118,
      "name": "Multimodal Interaction",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57963,
        57949,
        57961,
        57966
      ],
      "timeSlotId": 11915
    },
    {
      "id": 58119,
      "name": "Search and Recommender Systems",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57997,
        58004,
        58007,
        57954
      ],
      "timeSlotId": 11911
    },
    {
      "id": 58120,
      "name": "Textual & Natural Language Interfaces",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57973,
        58013,
        57970
      ],
      "timeSlotId": 11926
    },
    {
      "id": 58121,
      "name": "User Experience",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57976,
        58011,
        57990,
        57986
      ],
      "timeSlotId": 11917
    },
    {
      "id": 58122,
      "name": "User Modeling",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57979,
        57988,
        57998,
        57974
      ],
      "timeSlotId": 11918
    },
    {
      "id": 58123,
      "name": "Wearable & Ubiquitous Computing",
      "typeId": 11793,
      "roomId": 10482,
      "chairIds": [],
      "contentIds": [
        57967,
        57975,
        58010,
        57956
      ],
      "timeSlotId": 11922
    },
    {
      "id": 58124,
      "name": "Keynote:  Meredith Ringel Morris",
      "typeId": 11794,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11927
    },
    {
      "id": 58126,
      "name": "Keynote: Andrea G. Parker, Ph.D.",
      "typeId": 11794,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11932
    },
    {
      "id": 58127,
      "name": "Keynote: Kwan-Liu Ma",
      "typeId": 11794,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11934
    },
    {
      "id": 58128,
      "name": "Town Hall",
      "typeId": 11789,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11930
    },
    {
      "id": 58130,
      "name": "Panel: Diversity & Inclusion at IUI",
      "typeId": 11792,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11929
    },
    {
      "id": 58131,
      "name": "Social Hour",
      "typeId": 11789,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11928
    },
    {
      "id": 58125,
      "name": "Keynote: Juan Gilbert",
      "typeId": 11794,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11931
    },
    {
      "id": 58129,
      "name": "Welcome",
      "typeId": 11794,
      "roomId": 10483,
      "chairIds": [],
      "contentIds": [],
      "timeSlotId": 11933
    }
  ],
  "events": [],
  "contents": [
    {
      "id": 57984,
      "typeId": 11793,
      "title": "Perception of Fairness in Group Music Recommender Systems",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Fairness is an important aspect in group recommender systems (GRSs). They must ensure that potentially diverse preferences of all group members are taken into consideration when providing recommendations. Previous work has proposed a number of conflict elicitation and merging techniques to produce preferable recommendations for group members. However, we have yet to understand the influence of user personality on the perception of fairness in GRSs. To examine this gap, we use music recommendation as an example domain. We have developed a web-based group music recommender system using the Spotify API and two simple ranking algorithms: one based on the time the songs were voted by users (time-based) and the other based on a dissimilarity score (dissimilarity-based). A within-subjects experiment was conducted with 45 participants divided into groups of 3 (15 groups). Results showed that openness personality has a negative correlation with the perception that fairness is important in groups.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57839
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57906
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57735
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450642"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5ZdYkzd5nrA",
          "title": "Perception of Fairness in Group Music Recommender Systems",
          "duration": 304,
          "type": "video"
        }
      },
      "sessionIds": [
        58111
      ],
      "eventIds": []
    },
    {
      "id": 57985,
      "typeId": 11793,
      "title": "Learning to Engage in Interactive Digital Art",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "The aim of this study was to determine whether reinforcement learning could increase user engagement in interactive art installations. In collaboration with an interactive art company, reinforcement learning was integrated into a web application which was adapted from a physical interactive art piece. The original installation consisted of real plants that visitors could touch to produce sounds. The sounds played by the installation are decided by the artist beforehand based on considerations such as expected audience or location. A digital model and interface was developed to simulate the physical installation. A user study was conducted with 178 participants. Three modes were examined: the original settings of the installation as designed by the artist; a predetermined fixed schedule of consistently changing sound banks; and a reinforcement learning mode, where an agent changes the interactive behaviours to maximise user engagement. User engagement was estimated by comparing the number of touches by an individual over successive time intervals. From the trial, it was found that reinforcement learning was able to improve average engagement levels of users by nearly 27%. However, reinforcement learning was not able to increase the average duration users interacted with the installation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Clayton",
              "institution": "Monash University",
              "dsl": ""
            }
          ],
          "personId": 57705
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Department of Electrical and Computer Systems Engineering"
            }
          ],
          "personId": 57695
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450691"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=iZGpPZshpLI",
          "title": "Learning to Engage in Interactive Digital Art",
          "duration": 277,
          "type": "video"
        }
      },
      "sessionIds": [
        58115
      ],
      "eventIds": []
    },
    {
      "id": 57986,
      "typeId": 11793,
      "title": "How to Support Users in Understanding Intelligent Systems? Structuring the Discussion",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "The opaque nature of many intelligent systems violates established usability principles and thus presents a challenge for human-computer interaction. Research in the field therefore highlights the need for transparency, scrutability, intelligibility, interpretability and explainability, among others. While all of these terms carry a vision of supporting users in understanding intelligent systems, the underlying notions and assumptions about users and their interaction with the system often remain unclear. We review the literature in HCI through the lens of implied user questions to synthesise a conceptual framework integrating user mindsets, user involvement, and knowledge outcomes to reveal, differentiate and classify current notions in prior work. This framework aims to resolve conceptual ambiguity in the field and enables researchers to clarify their assumptions and become aware of those made in prior work. We thus hope to advance and structure the dialogue in the HCI research community on supporting users in understanding intelligent systems.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57860
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bayreuth",
              "institution": "University of Bayreuth",
              "dsl": ""
            }
          ],
          "personId": 57874
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57887
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450694"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=I-UBy12Fp5g",
          "title": "How to Support Users in Understanding Intelligent Systems? Structuring the Discussion",
          "duration": 512,
          "type": "video"
        }
      },
      "sessionIds": [
        58121
      ],
      "eventIds": []
    },
    {
      "id": 57987,
      "typeId": 11793,
      "title": "Stress Management Training using Biofeedback guided by Social Agents",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Coping with stress is critical to mental health. Prolonged mental stress is the psychological and physiological response to a high frequency of or continuous stressors, which has a negative impact on health. This paper presents a virtual stress management training using biofeedback derived from the cardiovascular response of the heart rate variability (HRV) with an interactive social agent as biofeedback trainer. The evaluation includes both, a subject-matter expert interview and an experiment with 71 participants. In the experiment, we compared our novel stress management training to a stress management training using stress diaries. The results indicate that our social agent-based stress management training using biofeedback significantly decreased the self-assessed stress levels immediately after the training, as well as in a socially stressful task. Moreover, we found a significant correlation between stress level and the assessment of one’s performance in a socially stressful task. Participants that received our training assessed their performance higher than participants getting stress diaries. Taken this together, our novel virtual stress management training with an interactive social agent as a trainer can be evaluated as a valid method for learning techniques on how to cope with stressful situations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus, Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57762
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus, Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57766
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus, Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57850
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus, Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57788
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450683"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=mkPUAbdVESo",
          "title": "Stress Management Training using Biofeedback guided by Social Agents",
          "duration": 602,
          "type": "video"
        }
      },
      "sessionIds": [
        58116
      ],
      "eventIds": []
    },
    {
      "id": 57988,
      "typeId": 11793,
      "title": "Non-Linear Consumption of Videos Using a Sequence of Personalized Multimodal Fragments",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "As videos progressively take a central role in conveying information on the Web, current  linear-consumption methods that involve spending time proportional to the duration of the video, need to be revisited. In this work, we present NoVoExp, a method that enables a Non-linear Video Consumption Experience by generating a sequence of multimodal fragments that represents the content in different segments of the videos in a succinct fashion. These fragments can help understand the content of the video without watching it in entirety and also serve as pointers to different segments of the video, enabling a new mechanism to interact/consume the video. We also design several baselines by building on top of video captioning and video summarization works to better understand the relative advantages and disadvantages of NoVoExp, and compare the performances across different video durations (short, medium, long) and categorises (entertainment, lectures, tutorials). We find that the sequences of multimodal fragments generated by NoVoExp have higher relevance to the video, are more diverse and yet coherent. Our extensive evaluation using several automated metrics as well as human studies show that our multimodal fragments are not only good at representing the contents of the video, but also align well with targeted viewer preferences.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Bangalore",
              "institution": "Adobe Research",
              "dsl": ""
            }
          ],
          "personId": 57760
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "IIT Guwahati",
              "dsl": ""
            }
          ],
          "personId": 57869
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "IIT Guwahati",
              "dsl": ""
            }
          ],
          "personId": 57747
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "IIT Guwahati",
              "dsl": ""
            }
          ],
          "personId": 57914
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Guwahati",
              "institution": "IIT Guwahati",
              "dsl": ""
            }
          ],
          "personId": 57883
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "Karnataka",
              "city": "Bangalore",
              "institution": "Adobe Research",
              "dsl": "Big data Experience Lab"
            }
          ],
          "personId": 57928
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450672"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4MD9g6-KCXI",
          "title": "Non-Linear Consumption of Videos Using a Sequence of Personalized Multimodal Fragments",
          "duration": 594,
          "type": "video"
        }
      },
      "sessionIds": [
        58122
      ],
      "eventIds": []
    },
    {
      "id": 57989,
      "typeId": 11793,
      "title": "From Philosophy to Interfaces: an Explanatory Method and a Tool Inspired by Achinstein’s Theory of Explanation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "We propose a new method for explanations in Artificial Intelligence (AI) and a tool to test its expressive power within a user interface. In order to bridge the gap between philosophy and human-computer interfaces, we show a new approach for the generation of interactive explanations based on a sophisticated pipeline of AI algorithms for structuring natural language documents into knowledge graphs, answering questions effectively and satisfactorily. Among the mainstream philosophical theories of explanation we identified one that in our view is easily applicable as a practical model for user-centric tools: Achinstein's Theory of Explanation.\r\nWith this work we aim to prove that the realist theory proposed by Achinstein can be actually implemented into a real software application, in an epistemic fashion, as an interactive process answering questions.\r\nTo this end we found a way to handle the generic (archetypal) questions that implicitly characterise an explanatory processes as preliminary overviews rather than as answers to explicit questions, as commonly understood. To show the expressive power of this approach we designed and implemented a pipeline of AI algorithms for the generation of interactive explanations under the form of overviews, focusing on this aspect of explanations rather than on existing interfaces and presentation logic layers for question answering.\r\nAccordingly, through the identification of a minimal set of archetypal questions it is possible to create a generator of explanatory overviews that is generic enough to significantly ease the acquisition of knowledge by humans, regardless of the specificities of the users outside of a minimum set of very broad requirements (e.g. people able to read and understand English and capable of performing basic common-sense reasoning).\r\nWe tested our hypothesis on a well-known XAI-powered credit approval system by IBM, comparing CEM, a static explanatory tool for post-hoc explanations, with an extension we developed adding interactive explanations based on our model.\r\nThe results of the user study, involving 100 participants, showed that our proposed solution produced a statistically relevant improvement on effectiveness (U=931.0, p=0.036) over the baseline, thus giving evidence in favour of our theory.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": ""
            }
          ],
          "personId": 57752
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Bologna",
              "institution": "University of Bologna",
              "dsl": ""
            }
          ],
          "personId": 57926
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450655"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=s9vQ_-Zv30E",
          "title": "From Philosophy to Interfaces: an Explanatory Method and a Tool Inspired by Achinstein's Theory of Explanation",
          "duration": 586,
          "type": "video"
        }
      },
      "sessionIds": [
        58114
      ],
      "eventIds": []
    },
    {
      "id": 57990,
      "typeId": 11793,
      "title": "Visual, textual or hybrid: the effect of user experience on different explanations",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "As the use of AI algorithms keeps rising continuously, so does the need for their transparency and accountability. However, literature often adopts a one-size-fits-all approach for developing explanations when in practice, the type of explanations needed depends on the type of end-user. This research will look at user expertise as a variable to see how different levels of expertise influence the understanding of explanations. The first iteration consists of developing two common types of explanations (visual and textual explanations) that explain predictions made by a general class of predictive model learners. These explanations are then evaluated by users of different expertise backgrounds to compare the understanding and ease-of-use of each type of explanation with respect to the different expertise groups. Results show strong differences between experts and lay users when using visual and textual explanations, as well as lay users having a preference for visual explanations which they perform significantly worse with. To solve this problem, the second iteration of this research focuses on the shortcomings of the first two explanations and tries to minimize the difference in understanding between both expertise groups. This is done through the means of developing and testing a candidate solution in the form of hybrid explanations, which essentially combine both visual and textual explanations. This hybrid form of explanations shows a significant improvement in terms of correct understanding (for lay users in particular) when compared to visual explanations, whilst not compromising on ease-of-use at the same time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57872
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57922
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Leuven",
              "institution": "KU Leuven",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57735
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450662"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=5kmxQaMvqk8",
          "title": "Visual, textual or hybrid: the effect of user expertise on different explanations",
          "duration": 531,
          "type": "video"
        }
      },
      "sessionIds": [
        58121
      ],
      "eventIds": []
    },
    {
      "id": 57991,
      "typeId": 11793,
      "title": "The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Recently, AI Security researchers have identified a new way crowdsourced data can be intentionally compromised. Backdoor attacks are a process through which an adversary creates a vulnerability in a machine learning model by “poisoning’' the training set by selectively mislabelling images containing a backdoor object. The model continues to perform well on standard testing data but misclassifies on the inputs that contain the backdoor chosen by the adversary. In this paper, we present the design and development of the Backdoor Game, the first game in which users can interact with different poisoned classifiers and upload their own images containing backdoor objects in an engaging way. We conduct semi-structured interviews with eight different participants who interacted with a first version of the Backdoor Game and deploy the game to Mechanical Turk users (N=68) to demonstrate how users interacted with the backdoor objects. We present results including novel types of interactions that emerged as a result of game play and design recommendations for the improvement of the system. The combined design, development and deployment of our system can help AI Security researchers to study this emerging concept, from determining the effectiveness of different backdoor objects to the collection of diverse and unique backdoor objects from the public, increasing the safety of future AI systems. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": "Thomas J. Watson Center"
            }
          ],
          "personId": 57806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI X-Lab"
            }
          ],
          "personId": 57912
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57691
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Boston",
              "institution": "Fjord Design",
              "dsl": ""
            }
          ],
          "personId": 57901
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 57825
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57834
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "SAN JOSE",
              "institution": "IBM Research, Almaden",
              "dsl": ""
            }
          ],
          "personId": 57822
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "SAN JOSE",
              "institution": "IBM Research, Almaden",
              "dsl": ""
            }
          ],
          "personId": 57854
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57792
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57842
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450647"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Gsk9k2i0Dyk",
          "title": "The Design and Development of a Game to Study Backdoor Poisoning Attacks: The Backdoor Game",
          "duration": 588,
          "type": "video"
        }
      },
      "sessionIds": [
        58113
      ],
      "eventIds": []
    },
    {
      "id": 57992,
      "typeId": 11793,
      "title": "A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Co-speech gestures, gestures that accompany speech, play an important role in human communication. Automatic co-speech gesture generation is thus a key enabling technology for embodied conversational agents (ECAs), since humans expect ECAs to be capable of multi-modal communication. Research into gesture generation is rapidly gravitating towards data-driven methods. Unfortunately, individual research efforts in the field are difficult to compare: there are no established benchmarks, and each study tends to use its own dataset, motion visualisation, and evaluation methodology. To address this situation, we launched a gesture-generation challenge, wherein participating teams built automatic gesture-generation systems on a common dataset, and the resulting systems were evaluated in parallel in a large, crowdsourced user study using the same motion-rendering pipeline. Since differences in evaluation outcomes between systems now are solely attributable to differences between the motion-generation methods, this enables benchmarking recent approaches against one another in order to get a better impression of the state of the art in the field. This paper reports on the purpose, design, and results of our challenge.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH Royal Institute of Technology in Stockholm",
              "dsl": "Robotics, Perception and Learning"
            }
          ],
          "personId": 57799
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 57727
        },
        {
          "affiliations": [
            {
              "country": "Korea, Republic of",
              "state": "",
              "city": "Daejeon",
              "institution": "Electronics and Telecommunications Research Institute (ETRI)",
              "dsl": ""
            }
          ],
          "personId": 57818
        },
        {
          "affiliations": [
            {
              "country": "Belgium",
              "state": "",
              "city": "Ghent",
              "institution": "Ghent University - imec",
              "dsl": "IDLab"
            }
          ],
          "personId": 57856
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Stockholm",
              "institution": "KTH",
              "dsl": ""
            }
          ],
          "personId": 57724
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450692"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=QmaoKRzoVwM",
          "title": "A large, crowdsourced evaluation of gesture generation systems on common data: The GENEA Challenge 2020",
          "duration": 584,
          "type": "video"
        }
      },
      "sessionIds": [
        58110
      ],
      "eventIds": []
    },
    {
      "id": 57993,
      "typeId": 11793,
      "title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Labeling data is an important step in the supervised machine learning lifecycle. It is a laborious human activity comprised of repeated decision making: the human labeler decides which of several potential labels to apply to each example. Prior work has shown that providing AI assistance can improve the accuracy of binary decision tasks. However, the role of AI assistance in more complex data-labeling scenarios with a larger set of labels has not yet been explored. We designed an AI labeling assistant that uses a semi-supervised learning algorithm to predict the most probable labels for each example. We leverage these predictions to provide assistance to the labeler in two ways: (i) providing a label recommendation and (ii) reducing the labeler’s decision space by focusing their attention on only the most probable labels. We conducted a user study (n=54) to evaluate an AI-assisted interface for data labeling in this context.Our results highlight that the AI assistance improves both labeler accuracy and speed, especially when the labeler finds the correct label in the reduced label space. We discuss findings related to the presentation of AI assistance and design implications for intelligent labeling interfaces",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57810
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 57900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": "Thomas J. Watson Center"
            }
          ],
          "personId": 57806
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57800
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57941
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Hastings on Hudson",
              "institution": "Mrs.",
              "dsl": "IBM Research"
            }
          ],
          "personId": 57899
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights,",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57917
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 57889
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57910
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM",
              "dsl": "IBM Research"
            }
          ],
          "personId": 57717
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57842
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450698"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=1w8oO1hA1kM",
          "title": "Increasing the Speed and Accuracy of Data Labeling Through an AI Assisted Interface",
          "duration": 692,
          "type": "video"
        }
      },
      "sessionIds": [
        58113
      ],
      "eventIds": []
    },
    {
      "id": 57994,
      "typeId": 11793,
      "title": "Towards Guidelines for Designing Human-in-the-Loop Machine Training Interfaces",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Supervised machine learning approaches commonly require good availability and quality of training data. In applications that depend on human-labeled data, especially from experts, or that depend on contextual knowledge for training data sets, the human-in-the-loop presents a serious bottleneck to the scalability of training efforts. Even if human labeling is generally feasible, sustained human performance and high-quality labels in larger quantities are challenging. Interactive Machine Learning can help solve usability problems in traditional machine learning by giving users agency in deciding how systems learn from data. Yet, the field lacks clear design guidelines for such interfaces, specifically regarding the scaling of training processes. In this paper, we present results from a pilot study in which participants interacted with several interface variants of a recommender engine and evaluated them on interaction and efficiency parameters. Based on the performance of these different learning system implementations we propose design guidelines for the design of such systems and a score for comparative evaluation, in which we combine interaction experience and system learning efficiency into one relative scoring unit.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 57868
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Industrial Design"
            }
          ],
          "personId": 57940
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450668"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=sb13AJx3ZwI",
          "title": "Towards Guidelines for Designing Human-in-the-Loop Machine Training Interfaces",
          "duration": 302,
          "type": "video"
        }
      },
      "sessionIds": [
        58109
      ],
      "eventIds": []
    },
    {
      "id": 57995,
      "typeId": 11793,
      "title": "Dynamic Manipulation of Player Performance with Music Tempo in Tetris",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "As music tempo can influence human actions pace, we use tempi variations to vary the difficulty in gaming situations based on the synchronization of events, like actions on moving objects in arcade games or waves of enemies in FPS. In this work, musical tempi are exploited to hinder or help Tetris players. Over the first phase of this work involving 44 players and more than 230 Tetris games we discovered surprising interactions between different tempo characteristics influencing the player's performance. The positive or negative effects of specific tempi settings we discovered were validated in a second phase involving 19 players and 50 Tetris games. Once the effect was chosen, it was dynamically triggered according to certain conditions that were validated during the first phase of this work. Results show that a transition from a staircase increase to a more gradual increase in tempo significantly hinders Tetris players when both tempi are synchronous with the gameplay whereas the same transition help players when both tempi are not synchronized with game actions. Our approach provides new and valuable insight to varying video-game difficulty when gaming situations ask the player to increasingly synchronize with the pace of the game.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Guer",
              "institution": "Écoles de Coëtquidan",
              "dsl": "CREC Saint Cyr"
            }
          ],
          "personId": 57764
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Guer",
              "institution": "Écoles de Coëtquidan",
              "dsl": "CREC Saint Cyr"
            }
          ],
          "personId": 57739
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Guer",
              "institution": "Écoles de Coëtquidan",
              "dsl": "CREC Saint Cyr"
            }
          ],
          "personId": 57725
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450684"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=2RmctXCKK_E",
          "title": "Dynamic Manipulation of Player Performance with Music Tempo in Tetris",
          "duration": 582,
          "type": "video"
        }
      },
      "sessionIds": [
        58115
      ],
      "eventIds": []
    },
    {
      "id": 57996,
      "typeId": 11793,
      "title": "Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Automated Machine Learning (AutoML) is a rapidly growing set of technology to automate the model development pipeline, by automatically searching the model space and generating candidate models. A critical final step of AutoML is to have the users, often data scientists, selecting the final model from dozens of candidates. In current AutoML systems the selection is supported by performance metrics.  Prior work has shown that in practices people make choice of ML model based on many criteria beyond prediction accuracy, including whether the way a model makes decision is reasonable or reliable. It is possible that AutoML users are interested in further understanding and comparing how these candidate models work. We also hypothesize the comparison may happen at various levels of granularity, from prediction distribution, feature importance to how the models judge selected instances. Based on these hypotheses, we developed Model LineUpper, supporting interactive model comparison for AutoML users by integrating multiple explainable AI (XAI) and visualization techniques. We conducted a user study with 14 data scientists, both to evaluate the design of Model LineUpper, and to use it as a design probe to understand how users perform model comparison with an AutoML system. We discuss the design implications for utilizing explainable AI techniques for model comparison, and supporting the unique user needs to compare candidate models generated by AutoML.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Troy",
              "institution": "Rensselaer Polytechnic Institute",
              "dsl": "Tetherless World Constellation"
            }
          ],
          "personId": 57867
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM T. J. Watson Research Center",
              "dsl": ""
            }
          ],
          "personId": 57715
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57835
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 57877
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450658"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=mMDS7Yahqh4",
          "title": "Model LineUpper: Supporting Interactive Model Comparison at Multiple Levels for AutoML",
          "duration": 303,
          "type": "video"
        }
      },
      "sessionIds": [
        58117
      ],
      "eventIds": []
    },
    {
      "id": 57997,
      "typeId": 11793,
      "title": "Augmenting Visual Information in Knowledge Graphs for Recommendations",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Knowledge graphs (KGs) have been popularly used in recommender systems due to their strength in revealing high-order connections between entities. KGs are typically constructed based on semantic information derived from metadata. Entity such as item images are very useful for those domains where visual factors are influential such as fashion recommendations. In this paper, we propose an approach to augment visual factors of item images into knowledge graphs with five popularly used image feature extraction methods. A user representation learning approach is proposed to learn hybrid user profiles that combine both semantic and visual preferences. The proposed approaches have been applied to recommender systems to make personalized recommendations. The experiments conducted on two real-world data sets show the effectiveness of the proposed approaches. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Reading",
              "institution": "University of Reading",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57925
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Reading",
              "institution": "University of Reading",
              "dsl": ""
            }
          ],
          "personId": 57742
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450686"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=btmwos3lt3k",
          "title": "Augmenting Visual Information in Knowledge Graphs for Recommendations",
          "duration": 300,
          "type": "video"
        }
      },
      "sessionIds": [
        58119
      ],
      "eventIds": []
    },
    {
      "id": 57998,
      "typeId": 11793,
      "title": "Detection and Differentiation of Obstacles in Repeated Adaptive Human-Computer Interactions from Brain Activity and User Behavior",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Human Computer Interaction can be impeded by various interaction obstacles, impacting a user's perception or cognition. In this work, we detect and discriminate such interaction obstacles from different data modalities to compensate for them through User Interface (UI) adaptation. For example, we detect memory-based obstacles from brain activity and compensate through repetition of information in the UI; we detect visual obstacles from user behavior and compensate by complementing visual with auditory information in the UI. Online cognitive adaptive systems should be able to decide the most suitable UI adaptation given inputs from several obstacles detectors. In this paper, we employ a Bayesian fusion approach upon different underlying obstacles detectors over multiple consecutive interaction sessions. Experimental results show that the model promisingly outperforms the baseline in the first interaction with an average accuracy of 72.5% and further improves drastically in subsequent interactions with additional information, with an average accuracy of 98%.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 57933
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Bremen",
              "institution": "University of Bremen",
              "dsl": "Cognitive Systems Lab"
            }
          ],
          "personId": 57765
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450641"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=eEETeme0BRE",
          "title": "Detection and Differentiation of Obstacles in Repeated Adaptive Human-Computer Interactions from Brain Activity and User Behavior",
          "duration": 633,
          "type": "video"
        }
      },
      "sessionIds": [
        58122
      ],
      "eventIds": []
    },
    {
      "id": 57999,
      "typeId": 11793,
      "title": "I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Unintended consequences of deployed AI systems fueled the call for more interpretability in AI systems.\r\nOften explainable AI (XAI) systems provide users with simplifying local explanations for individual predictions but leave it up to them to construct a global understanding of the model behavior.\r\nIn this work, we examine if non-technical users of XAI fall for an illusion of explanatory depth when interpreting additive local explanations.\r\nWe applied a mixed methods approach consisting of a moderated study with 40 participants and an unmoderated study with 107 crowd workers using a spreadsheet-like explanation interface based on the SHAP framework.\r\nWe observed what non-technical users do to form their mental models of global AI model behavior from local explanations and how their perception of understanding changes when it is examined.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57771
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57860
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57697
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57930
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Munich",
              "institution": "LMU Munich",
              "dsl": ""
            }
          ],
          "personId": 57945
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450644"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=38YWL6DlKB8",
          "title": "I Think I Get Your Point, AI! The Illusion of Explanatory Depth in Explainable AI",
          "duration": 572,
          "type": "video"
        }
      },
      "sessionIds": [
        58111
      ],
      "eventIds": []
    },
    {
      "id": 58000,
      "typeId": 11793,
      "title": "EnglishBot: An AI-Powered Conversational Interface for Second Language Learning",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Today, many students learn to speak a foreign language by listening to and repeating pre-recorded materials. This is due to the lack of practice opportunities with human partners. Leveraging recent advancements in AI, Speech, and NLP, we developed EnglishBot, a language learning chatbot that converses with students interactively on college-related topics and provides adaptive feedback. We evaluated EnglishBot against a traditional listen-and-repeat interface with 56 Chinese college students through two six-day user studies under both voluntary and fixed-usage conditions. Results show that students were more engaged with EnglishBot and voluntarily spent 2.1 times more time interacting with it. Students’ fluency also improved more with EnglishBot under the IELTS grading standard. Our results suggest that chatbots are an effective learning tool to engage students and have great potential to enhance foreign learners’ speaking abilities.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57787
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science & Engineering"
            },
            {
              "country": "United States",
              "state": "Washington",
              "city": "Seattle",
              "institution": "University of Washington",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 57936
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Graduate School of Education"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Graduate School of Education"
            }
          ],
          "personId": 57785
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57743
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Stanford",
              "institution": "Stanford University",
              "dsl": "Computer Science Department"
            }
          ],
          "personId": 57852
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 57713
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            },
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Tsinghua University",
              "dsl": ""
            }
          ],
          "personId": 57831
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450648"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=JzUQzaUBw4w",
          "title": "EnglishBot: An AI-Powered Conversational System for Second Language Learning",
          "duration": 598,
          "type": "video"
        }
      },
      "sessionIds": [
        58108
      ],
      "eventIds": []
    },
    {
      "id": 58001,
      "typeId": 11793,
      "title": "Data-centric disambiguation for data transformation with programming-by-example",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Programming-by-example (PBE), can be a powerful tool to reduce manual work in repetitive data transformation tasks. However, few examples often leave ambiguity and may cause undesirable data transformation by the system. This ambiguity can be resolved by allowing the user to directly edit the synthesized programs; however, this is difficult for non-programmers. Here, we present a novel approach: data-centric disambiguation for data transformation, where users resolve the ambiguity in data transformation by examining and modifying the output rather than the program. The key idea is to focus on the given set of data the user wants to transform instead of pursuing the synthesized program's generality or completeness. Our system provides visualization and interaction methods that allow users to efficiently examine and fix the transformed outputs, which is much simpler than understanding and modifying the program itself. The user study suggests that our system can successfully help non-programmers to more easily and efficiently process data.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Amherst",
              "institution": "University of Massachusetts, Amherst",
              "dsl": "College of Information and Computer Sciences"
            }
          ],
          "personId": 57779
        },
        {
          "affiliations": [
            {
              "country": "France",
              "state": "",
              "city": "Strasbourg",
              "institution": "Université de Strasbourg",
              "dsl": ""
            }
          ],
          "personId": 57782
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": "Graduate School of Information Science and Technology"
            }
          ],
          "personId": 57836
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57840
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450680"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rAAQaJTiszg",
          "title": "Data-centric disambiguation for data transformation with programming-by-example",
          "duration": 592,
          "type": "video"
        }
      },
      "sessionIds": [
        58108
      ],
      "eventIds": []
    },
    {
      "id": 58002,
      "typeId": 11793,
      "title": "Improving Artificial Teachers by Considering How People Learn and Forget",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "This paper studies a novel model-based method for artificial teaching; that is, the problem of selecting teaching interventions in interaction with humans. Previous work has either focused on the individualization of teaching or the optimization of teaching intervention sequences. We converge these two lines of research in an individualized model-based planning approach. In model-based planning, a user memory model's parameters are learned interactively and used to pick best interventions. New to our approach is the use of a model that can account for some key individual and material-specific characteristics related to recall/forgetting, along with a planning technique that considers users' practice schedules. Using a rule-based approach as a baseline, we evaluate the benefits of this approach in a controlled study of artificial teaching in second language vocabulary learning.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 57734
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 57846
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communications and Networking"
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "University of Helsinki",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57904
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 57898
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450696"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=8wTsJ5Fnzkg",
          "title": "Improving Artificial Teachers by Considering How People Learn and Forget",
          "duration": 518,
          "type": "video"
        }
      },
      "sessionIds": [
        58108
      ],
      "eventIds": []
    },
    {
      "id": 58003,
      "typeId": 11793,
      "title": "DeepNAG: Deep Non-Adversarial Gesture Generation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Synthetic data generation to improve classification performance (data augmentation) is a well-studied problem. Recently, generative adversarial networks (GAN) have shown superior image data augmentation performance, but their suitability in gesture synthesis has received inadequate attention. Further, GANs prohibitively require simultaneous generator and discriminator network training. We tackle both issues in this work. We first discuss a novel, device-agnostic GAN model for gesture synthesis called DeepGAN. Thereafter, we formulate DeepNAG by introducing a new differentiable loss function based on dynamic time warping and the average Hausdorff distance, which allows us to train DeepGAN's generator without requiring a discriminator. Through evaluations, we compare the utility of DeepGAN and DeepNAG against two alternative techniques for training five recognizers using data augmentation over six datasets. We further investigate the perceived quality of synthesized samples via an Amazon Mechanical Turk user study based on the HYPE∞ benchmark. We find that DeepNAG outperforms DeepGAN in accuracy, training time (up to 17x faster), and realism, thereby opening the door to a new line of research in generator network design and training for gesture synthesis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Santa Clara",
              "institution": "NVIDIA",
              "dsl": ""
            }
          ],
          "personId": 57784
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": "Interactive Systems and User Experience Lab"
            }
          ],
          "personId": 57821
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Orlando",
              "institution": "University of Central Florida",
              "dsl": ""
            }
          ],
          "personId": 57708
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450675"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=fGhsANyK4zw",
          "title": "DeepNAG: Deep Non-Adversarial Gesture Generation",
          "duration": 588,
          "type": "video"
        }
      },
      "sessionIds": [
        58112
      ],
      "eventIds": []
    },
    {
      "id": 58004,
      "typeId": 11793,
      "title": "Critiquing for Music Exploration in Conversational Recommender Systems",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Dialogue-based conversational recommender systems allow users to give language-based feedback on the recommended item, which has great potential for supporting users to explore the space of recommendations through conversation. In this work, we consider incorporating critiquing techniques into conversational systems to facilitate users' exploration of music recommendations. Thus, we have developed a music chatbot with three system variants, which are respectively featured with three different critiquing techniques, i.e., user-initiated critiquing (UC), progressive system-suggested critiquing (Progressive SC), and cascading system-suggested critiquing (Cascading SC). We conducted a between-subject study (N=107) to compare these three types of systems with regards to music exploration in terms of user perception and user interaction. Results show that both UC and SC are useful for music exploration, while users perceive higher diversity of recommendations with the system that offers Cascading SC and perceive more serendipitous with the system that offers Progressive SC. In addition, we find that the critiquing techniques significantly moderate the relationships between some interaction metrics (e.g., number of listened songs, number of dialogue turns) and users' perceived helpfulness and serendipity during music exploration.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hong Kong",
              "institution": "Hong Kong Baptist University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57775
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Lenovo Research",
              "dsl": ""
            }
          ],
          "personId": 57915
        },
        {
          "affiliations": [
            {
              "country": "Hong Kong",
              "state": "",
              "city": "Kowloon",
              "institution": "Hong Kong Baptist University",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57749
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450657"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=xJplJUquAiM",
          "title": "Critiquing for Music Exploration in Conversational Recommender Systems",
          "duration": 604,
          "type": "video"
        }
      },
      "sessionIds": [
        58119
      ],
      "eventIds": []
    },
    {
      "id": 58005,
      "typeId": 11793,
      "title": "Increasing the User Experience in Autonomous Driving through different Feedback Modalities",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Within the ongoing process of defining autonomous driving solutions, experience design may represent an important interface between humans and the autonomous vehicle. This paper presents an empirical study that uses different ways of unimodal communication in autonomous driving to communicate awareness and intent of autonomous vehicles. The goal is to provide recommendations for feedback solutions within holistic autonomous driving experiences. 22 test subjects took part in four autonomous, simulated virtual reality shuttle rides and were presented with different unimodal feedback in the form of light, sound, visualisation, text and vibration.The empirical study showed that, compared to a no-feedback baseline ride, light and visualisation were able to create a positive user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Stuttgart",
              "institution": "Stuttgart Media University",
              "dsl": "Mobile Media"
            }
          ],
          "personId": 57880
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "Games Institute",
              "dsl": "Hochschule der Medien "
            }
          ],
          "personId": 57813
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Glasgow",
              "institution": "The Glasgow School of Art",
              "dsl": "School of Simulation and Visualisation"
            }
          ],
          "personId": 57888
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "Baden-Württemberg",
              "city": "Stuttgart",
              "institution": "Stuttgart Media University",
              "dsl": "Mobile Media"
            }
          ],
          "personId": 57704
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450687"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Qam24bKrxSU",
          "title": "Increasing the User Experience in Autonomous Driving through different Feedback Modalities",
          "duration": 300,
          "type": "video"
        }
      },
      "sessionIds": [
        58110
      ],
      "eventIds": []
    },
    {
      "id": 58006,
      "typeId": 11793,
      "title": "Interactive Layout Transfer",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "During the design of graphical user interfaces (GUIs), one typical objective is to ensure compliance with pertinent style guides, ongoing design practices and design systems. However, designing compliant layouts is challenging, time-consuming, and can distract creative thinking in design. This paper presents a method for interactive layout transfer,\r\nwhere a source draft layout -- typically an initial rough working draft -- is transferred automatically using a selected reference/template layout, while complying with relevant guidelines. Our integer programming (IP) method extends previous work in two ways; First, by showing how to transform a rough draft into the final target layout using a reference template and, second, by extending IP-based approaches to adhere to guidelines. We demonstrate how to integrate the method into a real-time interactive GUI sketching tool. Evaluation results are presented from a case study and also from an online experiment where the perceived quality of layouts was assessed.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communications and Networking"
            }
          ],
          "personId": 57838
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 57855
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "User Interfaces Group"
            }
          ],
          "personId": 57795
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communications and Networking"
            },
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Finnish Center for Artificial Intelligence FCAI",
              "dsl": ""
            }
          ],
          "personId": 57707
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": "Department of Communications and Networking"
            }
          ],
          "personId": 57919
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Helsinki",
              "institution": "Aalto University",
              "dsl": ""
            }
          ],
          "personId": 57898
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450652"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=jJt949DoziA",
          "title": "Interactive Layout Transfer",
          "duration": 459,
          "type": "video"
        }
      },
      "sessionIds": [
        58114
      ],
      "eventIds": []
    },
    {
      "id": 58007,
      "typeId": 11793,
      "title": "HowDIY: Towards Meta-Design Tools to Support Anyone to 3D Print Anywhere",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "The promise of anyone being able to 3D print anywhere relies on both technological advances and incremental shifts in social organizations to trigger changes in human behavior. While research has focused on how people learn aspects of printing processes, such as expressively utilizing design-software and operating fabrication-machinery, this work explores how anyone may gain an understanding of what can be 3D printed through computationally-guided exploration of online resources and 3D printing facilities. Investigations surrounding online printing services reveal accessible 3D printing processes that do not require end-users to have experience with design-software or fabrication-machinery, only requiring end-users to specify printable ideas.  We present these accessible printing processes alongside associated technologies in a meta-design framework for supporting end-users’ specification of 3D printing ideas. Informed by this framework and a series of formative studies, we designed the website HowDIY to introduce anyone to 3D printing by encouraging and facilitating the intelligent exploration of various online resources. HowDIY was deployed over several weeks with diverse newcomers to 3D printing, validating that intelligent user interfaces can support anyone to participate in the utilization and design of 3D printing tools and processes. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M university",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57803
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Industrial & Systems Engineering"
            }
          ],
          "personId": 57720
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "TEILab"
            }
          ],
          "personId": 57913
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Visualization"
            }
          ],
          "personId": 57866
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 57701
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450638"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=9QYEaESiKlU",
          "title": "HowDIY: Towards Meta-Design Tools to Support Anyone to 3D Print Anywhere",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        58119
      ],
      "eventIds": []
    },
    {
      "id": 58008,
      "typeId": 11793,
      "title": "Key Qualities of Conversational Chatbots – the PEACE Model",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Open-domain chatbots engage in natural conversations with the user to socialize and establish bonds. However, designing and developing an effective open-domain chatbot is challenging. It is unclear what qualities of such chatbots most correspond to users’ expectations. Even though existing work has considered a wide range of aspects, some key components are still missing. More importantly, the consistency and validity of the combined criteria have not been tested. In this paper, we describe a large-scale survey using a consolidated model to elicit users’ preferences, expectations, and concerns. We apply structural equation modeling methods to further validate the data collected from the user survey. The outcome supports the consistency, validity, and reliability of the model, which we call PEACE (Politeness, Entertainment, Attentive Curiosity, and Empathy). PEACE, therefore, defines the key determinants most predictive of user acceptance. This has allowed us to develop a set of design guidelines useful for the development of compelling open-domain chatbots.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "Ecole polytechnique fédérale de Lausanne",
              "dsl": "Human Computer Interaction Group"
            }
          ],
          "personId": 57844
        },
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "-- Select One --",
              "city": "Lausanne",
              "institution": "Ecole polytechnique fédérale de Lausanne",
              "dsl": "Human Computer Interaction Group"
            }
          ],
          "personId": 57843
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450643"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=y1CRzvT3fTc",
          "title": "Key Qualities of Conversational Chatbots - the PEACE Model",
          "duration": 538,
          "type": "video"
        }
      },
      "sessionIds": [
        58109
      ],
      "eventIds": []
    },
    {
      "id": 58009,
      "typeId": 11793,
      "title": "Human-in-the-loop  Pose  Estimation  via  Shared  Autonomy",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Reliable, efficient shared autonomy requires balancing human operation and robot automation on complex tasks, such as for dexterous manipulation. Adding to the difficulty of shared autonomy is a robot’s limited ability to perceive the 6 degree-of-freedom pose of objects, which is essential to perform manipulation affordances. Inspired by Monte Carlo Localization, we propose a generative human-in-the-loop approach to estimating object pose. We characterize the performance of our mixed-initiative 3D registration approach using 2D pointing devices via a user study. Seeking an analog for Fitts’s Law for 3D registration, we introduce a new evaluation framework that takes the entire registration process into account instead of only the outcome. When combined with estimates of registration confidence, we posit mixed-initiative registration will reduce the human workload while maintaining or improving final pose estimation accuracy.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan, Ann Arbor",
              "dsl": "Computer Science & Engineering"
            }
          ],
          "personId": 57879
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57849
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57773
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Webster",
              "institution": "TRACLabs",
              "dsl": ""
            }
          ],
          "personId": 57893
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Electrical & Computer Engineering"
            }
          ],
          "personId": 57769
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57790
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57714
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450654"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=svFk6--eVyo",
          "title": "Human-in-the-loop Pose Estimation via Shared Autonomy",
          "duration": 283,
          "type": "video"
        }
      },
      "sessionIds": [
        58113
      ],
      "eventIds": []
    },
    {
      "id": 58010,
      "typeId": 11793,
      "title": "Designing a Personalised Sensor Glove Using Deep-Learning",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "When designing a smart glove for gesture recognition, the set of sensors available and their layout on the gloveare crucial. However, once a computational model reaches acceptable recognition accuracy, it is often notclear which sensors are more important for the task. Nor whether some sensors can be strategically removedwhile retaining similar performance in order to save cost. Furthermore, when aiming for a personalized setup,there can be minor deviation in how gestures are performed by each participant, and so the importance of asensor may vary between participants. In this paper, we use explainable AI to explore whether a personalisedglove can be produced, and whether the set of significant sensors persist between users. We present a deeplearning algorithm which utilises a layer of weights to estimate the importance of each sensor in relation toeach other. Besides estimating importance in relation to recognition accuracy, it is demonstrated how theimportance estimates can be extended to take into account factors external to the computational model, suchas costs. This allows for a cost effective elimination of sensors to reduce hardware redundancy whilst having acontrolled impact on performance. We provide 2 methods:genericorspecific. The generic method exploits theimportance estimate from all participants to select a set of sensors for removal. Whereas the specific methodestimates importance, and removes sensors based on individuals to provide a personalised setup.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Know Center GmbH",
              "dsl": "Knowledge Visualisation "
            }
          ],
          "personId": 57737
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Graz University of Technology",
              "dsl": "Interactive Systems and Data Science"
            },
            {
              "country": "Austria",
              "state": "",
              "city": "Graz",
              "institution": "Know Center GmbH",
              "dsl": "Knowledge Visualization"
            }
          ],
          "personId": 57903
        },
        {
          "affiliations": [
            {
              "country": "Austria",
              "state": "Styria",
              "city": "Graz",
              "institution": "Know-Center GmbH",
              "dsl": "Knowledge Visualisation"
            }
          ],
          "personId": 57815
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450665"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=meHsxRSv0Xg",
          "title": "Designing a Personalised Sensor Glove Using Deep-Learning",
          "duration": 460,
          "type": "video"
        }
      },
      "sessionIds": [
        58123
      ],
      "eventIds": []
    },
    {
      "id": 58011,
      "typeId": 11793,
      "title": "The Subconscious Director: Dynamically Personalizing Videos Using Gaze Data",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Watching TV has become a side event rather than a deliberate pastime. Movie directors thus struggle to find new ways to sustain the attention of their audience. Interactive movies usually require the viewer to actively decide how the plot progresses, creating an experience more akin to video games than film.\r\nIn this paper, we propose a system that analyses gaze data to personalize the plot of a video without the viewer's active intervention. User preferences are inferred from their gaze allocation to different elements in a scene. The subsequent scene is then dynamically tailored towards the user's predicted preference. In a user study (N = 175), we evaluate the effectiveness of the system with regard to user engagement. Our findings show that personalized videos have a positive effect on focused attention and involvement, whereas novelty perception is not significantly affected.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": ""
            }
          ],
          "personId": 57711
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": "Chair of Information Systems II"
            }
          ],
          "personId": 57826
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": ""
            }
          ],
          "personId": 57946
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Mannheim",
              "institution": "University of Mannheim",
              "dsl": "Information Systems II"
            }
          ],
          "personId": 57824
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450679"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=RrzwuRm428Q",
          "title": "The Subconscious Director: Dynamically Personalizing Videos Using Gaze Data",
          "duration": 549,
          "type": "video"
        }
      },
      "sessionIds": [
        58121
      ],
      "eventIds": []
    },
    {
      "id": 58012,
      "typeId": 11793,
      "title": "Positioning Left-hand Movement in Violin Performance: A System and User Study of Fingering Pattern Generation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Organizing fingerings, i.e., choosing which fingers to press on which positions and strings, is a crucial step for playing the violin. As the violin fingering comprises several components, the mapping of a musical phrase to the corresponding fingering arrangement is not unique, and it requires comprehensive musical knowledge for organizing adequate fingerings. In this paper, we study the human-machine cooperative approach to the generation of violin fingering, aiming to build an intelligent system which can provide multiple generation paths and yield adaptable fingering arrangements. For this sake, we compile a new dataset with fingering annotations of multiple versions of performance, propose a deep neural network with conditions on the left-hand movement for fingering generation, and conduct an in-depth user study for detailed responses. Result shows that the proposed system can yield various fingering arrangements according to different performance requirements, though a single generation may not satisfy all the requirements at a time. This highlights the importance of multi-path and human-in-the-loop architecture for violin fingering generation.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Switzerland",
              "state": "",
              "city": "Lausanne",
              "institution": "EPFL",
              "dsl": ""
            }
          ],
          "personId": 57796
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Institute of Information Science, Academia Sinica",
              "dsl": "Music and Culture Technology Lab"
            }
          ],
          "personId": 57878
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Taipei National University of the Arts",
              "dsl": ""
            }
          ],
          "personId": 57921
        },
        {
          "affiliations": [
            {
              "country": "Taiwan",
              "state": "",
              "city": "Taipei",
              "institution": "Institute of Information Science",
              "dsl": ""
            }
          ],
          "personId": 57710
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450661"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=6e1ZxmkrAhM",
          "title": "Positioning left-hand movement in violin performance: system and user study on fingering pattern generation",
          "duration": 304,
          "type": "video"
        }
      },
      "sessionIds": [
        58112
      ],
      "eventIds": []
    },
    {
      "id": 58013,
      "typeId": 11793,
      "title": "DIY: Helping People Assess the Correctness of Natural Language to SQL Systems",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Designing natural language interfaces for querying databases remains an important goal pursued by researchers in natural language processing, databases, and HCI. These systems receive natural language as input, translate it into a formal database query, and execute the query to compute a result. Because the responses from these systems are not always correct, it is important to provide people with mechanisms to assess the correctness of the generated query and computed result. However, this assessment can be challenging for people who lack expertise in query languages. We present Debug-It-Yourself (DIY), an interactive technique that enables users to assess the responses from a state-of-the-art NL2SQL system for correctness and, if possible, fix errors. DIY provides users with a sandbox where they can interact with (1) the mappings between the question and the generated query, (2) a small-but-relevant subset of the underlying database, and (3) a multi-modal explanation of the generated query by employing a back-of-the-envelope calculation, end-user debugging strategy on the system's responses. Through an exploratory study with 12 users, we investigate how DIY helps users assess the correctness of the system’s answers and detect & fix errors. Our observations reveal the benefits of DIY while providing insights about end-user debugging strategies and underscore opportunities for further improving the user experience.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 57733
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": "Microsoft Research AI"
            }
          ],
          "personId": 57774
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 57767
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "KIRKLAND",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 57902
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450667"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=p2OOKU1y_LI",
          "title": "DIY: Helping People Assess the Correctness of Natural Language to SQL Systems",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        58120
      ],
      "eventIds": []
    },
    {
      "id": 57949,
      "typeId": 11793,
      "title": "Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "This work proposes a new method for guiding a user's attention towards objects of interest in a cyber-physical environment (CPE). CPEs are environments that contain several computing systems that interact with each other and with the physical world. These environments contain several sensors (cameras, eye trackers, etc.) and output devices (lamps, screens, speakers, etc.). These devices can be used to first track the user's position, orientation, and focus of attention to then find the most suitable output device to guide the user's attention towards a target object. We argue that the most suitable device in this context is the one that attracts attention closest to the target and is salient enough to capture the user's attention. The method is implemented as a function which estimates the \"closeness\" and \"salience\" of each visual and auditive output device in the environment. Some parameters of this method are then evaluated through a user study in the context of a virtual reality supermarket. The results show that multi-modal guidance can lead to better guiding performance. However, this depends on the set parameters.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Cognitive Assistants"
            }
          ],
          "personId": 57931
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": "Cognitive Assistants"
            }
          ],
          "personId": 57798
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450678"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=acSYB_0zdKw",
          "title": "Multi-modal Multi-scale Attention Guidance in Cyber-Physical Environments",
          "duration": 513,
          "type": "video"
        }
      },
      "sessionIds": [
        58118
      ],
      "eventIds": []
    },
    {
      "id": 57950,
      "typeId": 11793,
      "title": "Spinning Icons: Introducing a Novel SSVEP-BCI Paradigm Based on Rotation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Steady-State-Visually-Evoked-Potential (SSVEP) Brain-Computer Interfaces (BCIs) make use of flickering stimuli to determine the target a user is looking at and select commands accordingly. Those types of BCI can be operated with little to no training, achieve high classification accuracies and are robust in application. A drawback of this approach is the reduced user comfort due to the constant flickering of the stimuli which can be annoying and tiring to look at. Existing studies addressing this issue try to make use of motion to disguise the oscillating patterns. However, this makes them look abstract and restricts the design of those applications as those patterns do not blend in to conventional user interfaces. In this work we introduce the concept of spinning icons to evoke SSVEPs. The icons are rotating in a certain frequency around their vertical axis and are supposed to appear more natural and be less stressing for the human eye. Furthermore this concept is not bound to any kind of abstract motion based pattern but rather supposed to work with any type of icon or image. The newly designed stimuli were evaluated in an application-oriented scenario and compared to standard and state-of-the-art movement-based SSVEP stimuli regarding the classification accuracy and experienced visual fatigue. The results show that the newly created stimuli performed equally well and partially even better in terms of classification accuracy and were  rated throughout better concerning visual fatigue by the study participants. This work therefore lays the foundation for more comfortable SSVEP-BCIs which can be used with basically every icon or UI element spinning around their vertical axis.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57871
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "German Research Center for Artificial Intelligence",
              "dsl": ""
            }
          ],
          "personId": 57793
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarbrücken",
              "institution": "DFKI, Saarland Informatics Campus",
              "dsl": ""
            }
          ],
          "personId": 57758
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450646"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=fVO7Tt0LiVc",
          "title": "Spinning Icons: Introducing a Novel SSVEP-BCI Paradigm Based on Rotation",
          "duration": 684,
          "type": "video"
        }
      },
      "sessionIds": [
        58112
      ],
      "eventIds": []
    },
    {
      "id": 57951,
      "typeId": 11793,
      "title": "Perfection Not Required? Human-AI Partnerships in Code Translation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Generative models have become adept at producing artifacts such as images, videos, and prose at human-like levels of proficiency. New generative techniques, such as unsupervised neural machine translation (NMT), have recently been applied to the task of generating source code, translating it from one programming language to another. The artifacts produced in this way may contain imperfections, such as compilation or logical errors. We examine the extent to which software engineers would tolerate such imperfections and explore ways to aid the detection and correction of those errors. Using a design scenario approach, we interviewed 11 software engineers to understand their reactions to the use of an NMT model in the context of application modernization, focusing on the task of translating source code from one language to another. Our three-stage scenario sparked discussions about the utility and desirability of working with an imperfect AI system, how acceptance of that system's outputs would be established, and future opportunities for generative AI in application modernization. Our study highlights how UI features such as confidence highlighting and alternate translations help software engineers work with and better understand generative NMT models.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research AI",
              "dsl": ""
            }
          ],
          "personId": 57877
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": "AI Interactions"
            }
          ],
          "personId": 57900
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57896
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57827
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research AI",
              "dsl": "Visual AI Lab"
            }
          ],
          "personId": 57808
        },
        {
          "affiliations": [
            {
              "country": "Argentina",
              "state": "Buenos Aires",
              "city": "La Plata",
              "institution": "IBM Argentina",
              "dsl": ""
            }
          ],
          "personId": 57918
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57751
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": "TJ Watson Research Center"
            }
          ],
          "personId": 57700
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450656"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=CMR02z8tBRM",
          "title": "Perfection Not Required? Human-AI Partnerships in Code Translation",
          "duration": 583,
          "type": "video"
        }
      },
      "sessionIds": [
        58113
      ],
      "eventIds": []
    },
    {
      "id": 57952,
      "typeId": 11793,
      "title": "ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "When prototyping AI-powered interfaces, designers seek to build useful and usable ways to support end-user tasks with AI capabilities. In the process, they need to consider the potential capabilities and constraints of the AI services and build interface adaptations such as explainability, error recovery, and feedback. Unfortunately, in assuming a black-box view of AI, current prototyping tools fail to support the design of AI capabilities and adaptations. Designers need to work with separate tools to explore machine learning models, understand model performance across diverse inputs, and align model behavior and interface choices. This introduces friction to rapid and iterative prototyping. We propose Model-Informed Prototyping (MIP), a workflow that combines model exploration with UI prototyping tasks. Our system, ProtoAI, allows designers to directly incorporate model outputs into interface design, evaluate design choices across different inputs, and iteratively revise their design by analyzing model breakdowns. We demonstrate how ProtoAI can readily operationalize several human-AI design guidelines. Our user study finds that designers can effectively engage in MIP to create AI-infused prototypes.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 57693
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "U Michigan",
              "dsl": "Psychology"
            }
          ],
          "personId": 57791
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 57857
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450640"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=unswhdV0Wt4",
          "title": "ProtoAI: Model-Informed Prototyping for AI-Powered Interfaces",
          "duration": 549,
          "type": "video"
        }
      },
      "sessionIds": [
        58114
      ],
      "eventIds": []
    },
    {
      "id": 57953,
      "typeId": 11793,
      "title": "Do You Have Time for a Quick Chat? Designing a Conversational Interface for Sexual Harassment Prevention Training",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Sexual harassment (SH) incidents are increasing and call into question the effectiveness of traditional SH prevention training. In this paper, we introduce a proof-of-concept design of a conversational interface (CI) for understanding SH cases. Key features of the interface include that it engages the learner in a dyadic conversation, prompts the learner for guidance, and tells a story of SH from a first-person perspective. From a mixed-methods study (N=32), learners experiencing a SH vignette using the conversational interface reported feeling less overwhelmed with the content, more engaged with the situation, and more comfortable discussing the topic compared to reading the same vignette online. Participants also reported that using a first-person narrative made the vignette feel realistic and relatable. However, there was no difference in empathy between the conditions. We discuss these results and implications for designing effective SH prevention training.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana-Champaign",
              "institution": "University of Illinois Urbana-Champaign",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57746
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign",
              "institution": "university of illinois",
              "dsl": ""
            }
          ],
          "personId": 57778
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Champaign ",
              "institution": "University of Illinois at Urbana Champaign ",
              "dsl": ""
            }
          ],
          "personId": 57876
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "urbana",
              "institution": "university of illinois",
              "dsl": ""
            }
          ],
          "personId": 57939
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Illinois",
              "city": "Urbana",
              "institution": "University of Illinois",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57845
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450659"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=BMOy8mEB4rY",
          "title": "Do You Have Time for a Quick Chat? Designing a Conversational Interface for Sexual Harassment Prevention Training",
          "duration": 626,
          "type": "video"
        }
      },
      "sessionIds": [
        58109
      ],
      "eventIds": []
    },
    {
      "id": 57954,
      "typeId": 11793,
      "title": "Rapid Assisted Visual Search: Supporting Digital Pathologists with Imperfect AI",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "The performance of AI models on medical images is showing promise to aid physicians in improving patient outcomes. However, identifying and navigating trade-offs between exploiting AI capabilities and mitigating its imperfections makes human-AI interaction difficult to design. Starting from a diagnostic task with strong but imperfect algorithmic results, we conducted a design exploration of how we might design an AI-powered tool for clinical use. In this paper, we describe our design process and motivate an assisted visual search concept with incremental sensitivity. In evaluation with six pathologists, diagnostic time was reduced with maintained quality. Our system afforded a progressive build-up of trust and model understanding. We discuss implications for future human-AI interaction design on trust, explanations, learning from use and designing to afford efficient collaboration strategies.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Sectra AB",
              "dsl": ""
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Norrkoping",
              "institution": "Department of Science and Technology",
              "dsl": "Linkoping University"
            }
          ],
          "personId": 57753
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Sectra AB",
              "dsl": ""
            },
            {
              "country": "Sweden",
              "state": "",
              "city": "Linköping",
              "institution": "Linköping University",
              "dsl": "Center for Medical Image Science and Visualization"
            }
          ],
          "personId": 57789
        },
        {
          "affiliations": [
            {
              "country": "Sweden",
              "state": "",
              "city": "Norrkoping",
              "institution": "Department of Science and Technology",
              "dsl": "Linkoping University"
            }
          ],
          "personId": 57828
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450681"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MSEEkKvKemQ",
          "title": "Rapid Assisted Visual Search: Supporting Digital Pathologists with Imperfect AI",
          "duration": 596,
          "type": "video"
        }
      },
      "sessionIds": [
        58119
      ],
      "eventIds": []
    },
    {
      "id": 57955,
      "typeId": 11793,
      "title": "An Intelligent System to Analyze Sketched Solutions to Open-Ended Truss Problems",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Engineering students need practical, open-ended problems to help them build their problem-solving skills and design abilities. However, large class sizes create a grading challenge for instructors as there is simply not enough time nor support to provide adequate feedback on many design problems. In this work, we describe an intelligent user interface to provide automated real-time feedback on hand-drawn free body diagrams that is capable of analyzing the internal forces of a sketched truss to evaluate open-ended design problems. The system is driven by sketch recognition algorithms developed for recognizing trusses and a robust linear algebra approach for analyzing trusses. Students in an introductory statics course were assigned a truss design problem as a homework assignment using either paper or our software. We used conventional content analysis on four focus groups totaling 16 students to identify key aspects of their experiences with the design problem and our software. We found that the software correctly analyzed all student submissions, students enjoyed the problem compared to typical homework assignments, and students found the problem to be good practice. Additionally, students using our software reported less difficulty understanding the problem, and the majority of all students said they would prefer the software approach over pencil and paper. We also evaluated the recognition performance on a set of 3000 sketches resulting in an f-score of 0.997. We manually reviewed the submitted student work which showed the handful of student complaints about recognition were largely due to user error.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 57935
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab, Department of Computer Science and Engineering"
            }
          ],
          "personId": 57944
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 57882
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College STation",
              "institution": "Texas A&M University",
              "dsl": "Department of Psychological & Brain Sciences"
            }
          ],
          "personId": 57712
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Georgia Institute of Technology",
              "dsl": "iDREEM Lab"
            }
          ],
          "personId": 57897
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 57694
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450651"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Vbwq_zv1_8o",
          "title": "An Intelligent System to Analyze Sketched Solutions to Open-Ended Truss Problems",
          "duration": 640,
          "type": "video"
        }
      },
      "sessionIds": [
        58112
      ],
      "eventIds": []
    },
    {
      "id": 57956,
      "typeId": 11793,
      "title": "TeethTap: Recognizing Discrete Teeth Gestures using Motion and Acoustic Sensing on an Earpiece",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Teeth gestures become an alternative input modality for different situations and accessibility purposes. In this paper, we present TeethTap, a novel eyes-free and hands-free input technique, which can recognize up to 13 discrete teeth tapping gestures. TeethTap adopts a wearable 3D printed earpiece with an IMU sensor and a contact microphone behind both ears, which works in tandem to detect jaw movement and sound data, respectively. TeethTap uses a support vector machine to classify gestures from noise by fusing acoustic and motion data, and implements K-Nearest-Neighbor (KNN) with a Dynamic Time Warping (DTW) distance measurement using motion data for gesture classification. A user study with 11 participants demonstrated that TeethTap could recognize 13 gestures with a real-time classification accuracy of 90.9% in a laboratory environment. We further uncovered the accuracy differences on different teeth gestures when having sensors on single vs. both sides. Moreover, we explored the activation gesture under real-world environments, including eating, speaking, walking and jumping. Based on our findings, we further discussed potential applications and practical challenges of integrating TeethTap into future devices.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of Software Chinese Academy of Sciences",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 57881
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human-Computer Interaction Institute"
            }
          ],
          "personId": 57729
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Ithaca",
              "institution": "Cornell University",
              "dsl": ""
            }
          ],
          "personId": 57864
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Hefei",
              "institution": "University of Science and Technology of China",
              "dsl": ""
            }
          ],
          "personId": 57811
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "",
              "city": "Beijing",
              "institution": "Institute of software, Chinese Academy of Sciences",
              "dsl": ""
            }
          ],
          "personId": 57768
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "ITHACA",
              "institution": "Cornell University",
              "dsl": "Computing and Information Science"
            }
          ],
          "personId": 57870
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450645"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rCwFrD3kreU",
          "title": "TeethTap: Recognizing Discrete Teeth Gestures using Motion and Acoustic Sensing on an Earpiece",
          "duration": 603,
          "type": "video"
        }
      },
      "sessionIds": [
        58123
      ],
      "eventIds": []
    },
    {
      "id": 57957,
      "typeId": 11793,
      "title": "Anchoring Bias Affects Mental Model Formation and User Reliance in Explainable AI Systems",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "EXplainable Artificial Intelligence (XAI) approaches are used to bring transparency to machine learning and artificial intelligence models, and hence, improve the decision-making process for their end-users. While these methods aim to improve human understanding and thier mental models, cognitive biases can still influence a user's mental model and decision-making in ways that system designers do not anticipate. This paper presents research on cognitive biases due to ordering effects in intelligent systems. We conducted a controlled user study to understand how the order of observing system weaknesses and strengths can affect the user's mental model, task performance, and reliance on the intelligent system, and we investigate the role of explanations in addressing this bias. Using an explainable video activity recognition tool in the cooking domain, we asked participants to verify whether a set of kitchen policies are being followed, with each policy focusing on a weakness or a strength. We controlled the order of the policies and the presence of explanations to test our hypotheses. Our main finding shows that those who observed system strengths early-on were more prone to automation bias and made significantly more errors due to positive first impressions of the system, while they built a more accurate mental model of the system competencies. On the other hand, those who encountered weaknesses earlier made significantly fewer errors since they tended to rely more on themselves, while they also underestimated model competencies due to having a more negative first impression of the model. Our work presents strong findings that aims to make intelligent system designers aware of such biases when designing such tools.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": "INDIE LAB - Department of Computer & Information Science & Engineering"
            }
          ],
          "personId": 57948
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Dallas",
              "institution": "University of Texas in Dallas",
              "dsl": ""
            }
          ],
          "personId": 57770
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Department of Computer & Information Science & Engineering",
              "dsl": "INDIE Lab"
            }
          ],
          "personId": 57853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Department of Computer & Information Science & Engineering",
              "dsl": "University of Florida, INDIE Lab"
            }
          ],
          "personId": 57731
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Dallas",
              "institution": "University of Texas in Dallas",
              "dsl": "Computer Science"
            }
          ],
          "personId": 57696
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 57726
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Dallas",
              "institution": "University of Texas in Dallas",
              "dsl": ""
            }
          ],
          "personId": 57837
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450639"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=akEOZDLcBK8",
          "title": "Anchoring Bias Affects Mental Models and User Reliance in Explainable AI Systems",
          "duration": 537,
          "type": "video"
        }
      },
      "sessionIds": [
        58111
      ],
      "eventIds": []
    },
    {
      "id": 57958,
      "typeId": 11793,
      "title": "Interactive Exploration-Exploitation Balancing for Generative Melody Composition",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Recent content creation systems allow users to generate various high-quality content (e.g., images, 3D models, and melodies) by just specifying a parameter set (e.g., a latent vector of a deep generative model). The task here is to search for an appropriate parameter set that produces the desired content. To facilitate this task execution, researchers have investigated user-in-the-loop optimization, where the system samples candidate solutions, asks the user to provide preferential feedback on them, and iterates this procedure until finding the desired solution. In this work, we investigate a novel approach to enhance this interactive process: allowing users to control the sampling behavior. More specifically, we allow users to adjust the balance between exploration (i.e., favoring diverse samples) and exploitation (i.e., favoring focused samples) in each iteration. To evaluate how this approach affects the user experience and optimization behavior, we implement it into a melody composition system that combines a deep generative model with Bayesian optimization. Our experiments suggest that this approach could improve the user's engagement and optimization performance.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57777
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 57909
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Ibaraki",
              "city": "Tsukuba",
              "institution": "National Institute of Advanced Industrial Science and Technology (AIST)",
              "dsl": ""
            }
          ],
          "personId": 57732
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57840
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450663"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=NbGTorxBcdE",
          "title": "Interactive Exploration-Exploitation Balancing for Generative Melody Composition",
          "duration": 303,
          "type": "video"
        }
      },
      "sessionIds": [
        58114
      ],
      "eventIds": []
    },
    {
      "id": 57959,
      "typeId": 11793,
      "title": "Wait, But Why?: Assessing Behavior Explanation Strategies for Real-Time Strategy Games",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Work in AI-based explanation systems has uncovered an interesting contradiction: people prefer and learn best from 'why' explanations but expert esports commentators primarily answer 'what' questions when explaining complex behavior in real-time strategy games. Three possible explanations for this contradiction are: 1.) broadcast audiences are well-informed and do not need 'why' explanations; 2.) consuming 'why' explanations in real-time is too cognitively demanding for audiences; or 3.) producing live 'why' explanations is too difficult for commentators. We answer this open question by investigating the effects of explanation types and presentation modalities on audience recall and cognitive load in the context of an esports broadcast. We recruit 131 Dota 2 players and split them into three groups: the first group views a Dota 2 broadcast, the second group has the addition of an interactive map that provides 'what' explanations, and the final group receives the interactive map with detailed 'why' explanations. We find that participants who receive short interactive text prompts that provide 'what' explanations outperform the other two groups on a multiple-choice recall task. We also find that participants who receive detailed 'why' explanations submit the highest reports of cognitive load. Our evidence supports the conclusion that informed audiences benefit from explanations but do not have the cognitive resources to process 'why' answers in real-time. It also supports the conclusion that stacked explanation interventions across different modalities, like audio, interactivity, and text, can aid real-time comprehension when attention resources are limited. Together, our results indicate that interactive multimedia interfaces can be leveraged to quickly guide attention and provide low-cost explanations to improve intelligibility when time is too scarce for cognitively demanding 'why' explanations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Kennesaw State University",
              "dsl": ""
            }
          ],
          "personId": 57722
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": ""
            }
          ],
          "personId": 57927
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Digital Creativity Labs, Dept. of Theatre, Film, TV and Interactive Media"
            }
          ],
          "personId": 57776
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Theatre, Film, Television and Interactive Media"
            }
          ],
          "personId": 57706
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": ""
            }
          ],
          "personId": 57847
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "North Yorkshire",
              "city": "York",
              "institution": "Dept. of Theatre, Film and Television, University of York",
              "dsl": "Digital Creativity Labs"
            }
          ],
          "personId": 57823
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": ""
            }
          ],
          "personId": 57755
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": "Weavr"
            }
          ],
          "personId": 57858
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": ""
            }
          ],
          "personId": 57934
        },
        {
          "affiliations": [
            {
              "country": "United Kingdom",
              "state": "",
              "city": "York",
              "institution": "University of York",
              "dsl": ""
            }
          ],
          "personId": 57891
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450699"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=rQ7PBK6AZOw",
          "title": "Wait, But Why?: Assessing Behavior Explanation Strategies for Real-Time Strategy Games",
          "duration": 515,
          "type": "video"
        }
      },
      "sessionIds": [
        58110
      ],
      "eventIds": []
    },
    {
      "id": 57960,
      "typeId": 11793,
      "title": "OralViewer: 3D Demonstration of Dental Surgeries for Patient Education with Oral Cavity Reconstruction from a 2D Panoramic X-ray",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we present OralViewer---the first interactive application that enables dentist's demonstration of dental surgeries in 3D to promote patients' understanding. \r\nOralViewer takes a single 2D panoramic dental X-ray to reconstruct patient-specific 3D teeth structures, which are then assembled with registered gum and jaw bone models for complete oral cavity modeling. \r\nDuring the demonstration, OralViewer enables dentists to show surgery steps with virtual dental instruments that can animate effects on a 3D model in real-time. \r\nA technical evaluation shows our deep learning based model achieves a mean Intersection over Union (IoU) of 0.771 for 3D teeth reconstruction. \r\nA patient study with 12 participants shows OralViewer can improve patients' understanding of surgeries. \r\nAn expert study with 3 board-certified dentists further verifies the clinical validity of our system. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            }
          ],
          "personId": 57809
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            }
          ],
          "personId": 57745
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "University of Pittsburgh",
              "dsl": ""
            }
          ],
          "personId": 57756
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "Design Automation Lab, ECE dept"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "Design Automation Lab, ECE dept"
            }
          ],
          "personId": 57929
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            }
          ],
          "personId": 57780
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            }
          ],
          "personId": 57924
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": ""
            }
          ],
          "personId": 57875
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California",
              "dsl": ""
            }
          ],
          "personId": 57709
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 57801
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "University of California, Los Angeles",
              "dsl": ""
            }
          ],
          "personId": 57947
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450695"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=zkYdLRqtlsM",
          "title": "OralViewer: 3D Demonstration of Dental Surgeries for Patient Education with Oral Cavity Reconstruction from a 2D Panoramic X-ray",
          "duration": 656,
          "type": "video"
        }
      },
      "sessionIds": [
        58116
      ],
      "eventIds": []
    },
    {
      "id": 57961,
      "typeId": 11793,
      "title": "Learning Network-Based Multi-Modal Mobile User Interface Embeddings",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Rich multi-modal information - text, code, images, categorical and numerical data - co-exist in the user interface (UI) design of mobile applications. UI designs are composed of UI entities supporting different functions which together enable the application. To support effective search and recommendation applications over mobile UIs, we need to be able to learn UI representations that integrate latent semantics. In this paper, we propose a novel unsupervised model - Multi-modal Attention-based Attributed Network Embedding (MAAN) model. MAAN is designed to capture both multi-modal and structural network information. Based on the encoder-decoder framework, MAAN aims to learn UI representations that allow UI design reconstruction. The generated embedding can be applied to a variety of tasks: predicting UI elements associated with UI screens, inferring missing UI screen and element attributes, predicting UI user ratings, and retrieving UIs. Extensive experiments, including user evaluations, conducted on two datasets from RICO, a rich real-world mobile UI repository, demonstrates that MAAN out-performs other state-of-the-art models. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": "School of Information Systems"
            },
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "Singapore Management University",
              "dsl": "School of Information Systems"
            }
          ],
          "personId": 57702
        },
        {
          "affiliations": [
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "SMU",
              "dsl": "School of Information Systems"
            },
            {
              "country": "Singapore",
              "state": "Singapore",
              "city": "Singapore",
              "institution": "SMU",
              "dsl": "School of Information Systems"
            }
          ],
          "personId": 57942
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450693"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=TVSOVm6k-Ss",
          "title": "Learning Network-Based Multi-Modal Mobile User Interface Embeddings",
          "duration": 605,
          "type": "video"
        }
      },
      "sessionIds": [
        58118
      ],
      "eventIds": []
    },
    {
      "id": 57962,
      "typeId": 11793,
      "title": "Continuous and Gradual Style Changes of Graphic Designs with Generative Model",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Creating a high-quality layout design from scratch is difficult for novices. Therefore, novices often consult the works of other skilled designers for ideas regarding layout designs. Researchers have previously investigated methods to support the layout design process; these works mainly focused on retrieval methods for similar layout designs, or refinement of existing layouts. To enhance user creativity in designing layouts, assistance is needed for exploring various designs. Herein, we propose a novel deep generative model that enables the generation of various layout designs and guarantees continuous and gradual changes in layouts, for effectively exploring graphic designs. Accordingly, we present an adversarial training method with dual critic networks; we trained our model by a public graphic design dataset. We developed another interaction method that allows the user to change the graphic designs between two different layout styles and categories parametrically. We demonstrated the efficacy of the proposed method in generating rich layout variations with representation of latent space by comparing the layout designs generated by our model with by an existing model.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "LINE corp",
              "dsl": ""
            }
          ],
          "personId": 57907
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "National Institute of Informativs",
              "dsl": ""
            }
          ],
          "personId": 57772
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450666"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=_oh5cwXtXbA",
          "title": "Continuous and Gradual Style Changes of Graphic Designs with Generative Model",
          "duration": 576,
          "type": "video"
        }
      },
      "sessionIds": [
        58115
      ],
      "eventIds": []
    },
    {
      "id": 57963,
      "typeId": 11793,
      "title": "Multimodal Emotion Recognition of Hand-Object Interaction",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we investigate whether information related to touches and rotations impressed to an object can be effectively used to classify the emotion of the agent manipulating it. We specifically focus on sequences of basic actions (e.g., grasping, rotating), which are constituents of daily interactions. We use the iCube, a 5 cm cube covered with tactile sensors and embedded with an accelometer, to collect a new dataset including 11 persons performing action sequences associated with 4 emotions: anger, sadness, excitement and gratitude. Next, we propose 17 high-level hand-crafted features based on the tactile and kinematics data derived from the iCube. Twelve of these features vary significantly as a function of the emotional context in which the action sequence was performed.In particular, a larger surface of the object is engaged in physical contact for anger and excitement, than for sadness. Furthermore, the average duration of  interactions labelled as sad, is longer than for the remaining 3 emotions. More rotations are performed for anger and excitement than for sadness and gratitude. The accuracy of a classification experiment in the case of four emotions reaches 0.75. This result shows that the  emotion recognition during hand-object interactions is possible and it may foster development of new intelligent user interfaces.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Rovereto",
              "institution": "University of Trento",
              "dsl": "University of Trento"
            },
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Istituto Italiano di Tecnologia",
              "dsl": ""
            }
          ],
          "personId": 57698
        },
        {
          "affiliations": [
            {
              "country": "Italy",
              "state": "",
              "city": "Genova",
              "institution": "Italian Institute of Technology",
              "dsl": "RBCS"
            }
          ],
          "personId": 57804
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450636"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=R09U1TJHPQA",
          "title": "Multimodal Emotion Recognition of Hand-Object Interaction",
          "duration": 406,
          "type": "video"
        }
      },
      "sessionIds": [
        58118
      ],
      "eventIds": []
    },
    {
      "id": 57964,
      "typeId": 11793,
      "title": "Quantitative Evaluation of Machine Learning Explanations: A Human-Grounded Benchmark",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Research in interpretable machine learning proposes different approaches to evaluate model saliency or attribution explanations. These approaches measure different qualities of explanations to achieve divers goals in designing interpretable machine learning systems. In this paper, we propose an online available human attention benchmark for image and text domains using multi-layer human attention heatmaps drawn from multiple human annotators. We then present an evaluation study for the correctness or trustworthiness of model saliency explanations obtained using Grad-cam and LIME techniques. We demonstrate our benchmark's utility for quantitative evaluation of model explanations and compare it with 1) human subjective rating and 2) objective single-layer feature mask evaluations. Our study results show the threshold agnostic evaluation with the human attention baseline is more effective compared to object localization metrics. Our benchmark also reveals user biases between the subjective rating of different explanation error types.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Computer Science and Engineering "
            }
          ],
          "personId": 57741
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "Department of Computer & Information Science & Engineering",
              "dsl": "INDIE Lab"
            }
          ],
          "personId": 57853
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Florida",
              "city": "Gainesville",
              "institution": "University of Florida",
              "dsl": ""
            }
          ],
          "personId": 57726
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450689"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=T9sDjx-ANNg",
          "title": "Quantitative Evaluation of Machine Learning Explanations: A Human-Grounded Benchmark",
          "duration": 560,
          "type": "video"
        }
      },
      "sessionIds": [
        58110
      ],
      "eventIds": []
    },
    {
      "id": 57965,
      "typeId": 11793,
      "title": "Knowledge Graph Completion-based Question Selection for Acquiring Domain Knowledge through Dialogues",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Building a perfect knowledge base in a certain domain is practically impossible, so it is effective for dialogue systems to acquire knowledge for enhancing an imperfect knowledge base through natural language dialogues with users. This paper proposes a framework for selecting questions for such knowledge acquisition when a knowledge graph is used as the knowledge base. The framework exploits knowledge graph completion (KGC) for predicting new links that are likely to be correct and selects questions on the basis of the KGC scores. One of the problems with this framework is that questions with incorrect content might be selected, which often occur when the link prediction performance is low, and this would reduce the users’ willingness to engage in the dialogues. To alleviate this problem, this paper presents two modifications to the KGC training: 1) creating pseudo entities having substrings of the names of the entities in the graph so that the entities whose names share substrings are connected and 2) limiting the range of negative sampling. Cross validation-based experiments we conducted showed that these modifications improved KGC performance.  We also conducted a user study with crowdsourcing to investigate the subjective perception of the correctness of the predicted links. The results suggest that the model trained with the modifications is capable of avoiding questions with incorrect content.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": "The Institute of Scientific and Industrial Research (ISIR)"
            }
          ],
          "personId": 57781
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": "The Institute of Scientific and Industrial Research (ISIR)"
            }
          ],
          "personId": 57895
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Osaka",
              "city": "Ibaraki",
              "institution": "Osaka University",
              "dsl": "The Institute of Scientific and Industrial Research (ISIR)"
            }
          ],
          "personId": 57884
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Gunma",
              "city": "Maebashi",
              "institution": "Gunma University",
              "dsl": ""
            }
          ],
          "personId": 57841
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "Tokyo",
              "city": "Shinjuku",
              "institution": "C4A Research Institute",
              "dsl": ""
            }
          ],
          "personId": 57797
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450653"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=4r1a7aVBKSk",
          "title": "Knowledge Graph Completion-based Question Selection for Acquiring Domain Knowledge through Dialogues",
          "duration": 555,
          "type": "video"
        }
      },
      "sessionIds": [
        58109
      ],
      "eventIds": []
    },
    {
      "id": 57966,
      "typeId": 11793,
      "title": "Understanding Driving Distractions: A Multimodal Analysis on Distraction Characterization",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Distracted driving is a leading cause of accidents worldwide. The tasks of distraction detection and recognition have been traditionally addressed as computer vision problems. However, distracted behaviors are not always expressed in a visually observable way. In this work, we introduce a novel multimodal dataset of distracted driver behaviors, consisting of data collected using twelve information channels coming from visual, acoustic, near-IR, thermal, physiological and linguistic modalities. The data were collected from 45 subjects while being exposed to four different distractions, three cognitive and one physical. For the purposes of this paper, we experiment with visual and physiological information and explore the potential of multimodal modeling for distraction recognition. In addition, we analyze the value of different modalities by identifying specific visual and physiological groups of features that contribute the most to distraction characterization. Our results highlight the advantage of multimodal representations and reveal valuable insights for the role played by the two modalities on identifying different types of driving distractions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 57761
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "DEARBORN",
              "institution": "University of Michigan",
              "dsl": "Computer and information science "
            }
          ],
          "personId": 57894
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Dearborn",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 57865
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 57938
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Dearborn",
              "institution": "University of Michigan",
              "dsl": ""
            }
          ],
          "personId": 57719
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "Computer Science Engineering"
            }
          ],
          "personId": 57757
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Flint",
              "institution": "University of Michigan-Flint",
              "dsl": ""
            }
          ],
          "personId": 57759
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450635"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=VULZ-aH1P_U",
          "title": "Understanding Driving Distractions: A Multimodal Analysis on Distraction Characterization",
          "duration": 691,
          "type": "video"
        }
      },
      "sessionIds": [
        58118
      ],
      "eventIds": []
    },
    {
      "id": 57967,
      "typeId": 11793,
      "title": "Exploring Smartphone Keyboard Interactions for Experience Sampling Method driven Probe Generation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Keyboard interaction patterns on a smartphone is the input for many intelligent emotion-aware applications, such as adaptive interface, optimized keyboard layout, automatic emoji recommendation in IM applications. The simplest approach, called the Experience Sampling Method (ESM), is to systematically gather self-reported emotion labels from users, which act as the ground truth labels, and build a supervised prediction model for emotion inference. However, as manual self-reporting is fatigue-inducing and attention-demanding, the self-report requests are to be scheduled at favorable moments to ensure high fidelity response.We, in this paper, perform fine-grain keyboard interaction analysis to determine suitable probing moments. Keyboard interaction patterns, both cadence, and latency between strokes, nicely translate to frequency and time domain analysis of the patterns. In this paper, we perform a 3-week in-the-wild study (N = 22) to log keyboard interaction patterns and self-report details indicating (in)opportune probing moments. Analysis of the dataset reveals that time-domain features (e.g., session length, session duration) and frequency-domain features (e.g., number of peak amplitudes, value of peak amplitude) vary significantly between opportune and inopportune probing moments. Driven by these analyses, we develop a generalized (all-user) Random Forest based model, which can identify the opportune probing moments with an average F-score of 93%. We also carry out the explainability analysis of the model using SHAP (SHapley Additive exPlanations), which reveals that the session length and peak amplitude have strongest influence to determine the probing moments.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "Indian Institute of Technology Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57721
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "CSE"
            }
          ],
          "personId": 57786
        },
        {
          "affiliations": [
            {
              "country": "India",
              "state": "West Bengal",
              "city": "Kharagpur",
              "institution": "IIT Kharagpur",
              "dsl": "Computer Science and Engineering"
            }
          ],
          "personId": 57794
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Georgia",
              "city": "Atlanta",
              "institution": "Microsoft",
              "dsl": ""
            }
          ],
          "personId": 57848
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450669"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=1IvQfedkhkQ",
          "title": "Exploring Smartphone Keyboard Interactions for Experience Sampling Method driven Probe Generation",
          "duration": 319,
          "type": "video"
        }
      },
      "sessionIds": [
        58123
      ],
      "eventIds": []
    },
    {
      "id": 57968,
      "typeId": 11793,
      "title": "TextFlow: Screenless Access to Non-Visual Smart Messaging ",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Texting relies on screen-centric prompts designed for sighted users, still posing significant barriers to people who are blind and visually impaired (BVI). Can we re-imagine texting untethered from a visual display? In an interview study, 20 BVI adults shared situations surrounding their texting practices, recurrent topics of conversations, and challenges. Informed by these insights, we introduce Textflow: a mixed-initiative context-aware system that generates entirely auditory message options relevant to the users’ location, activity, and time of the day. Users can browse and select suggested aural messages using finger-taps supported by an off-the-shelf finger-worn device, without having to hold or attend to a mobile screen. In an evaluative study, 10 BVI participants successfully interacted with TextFlow to browse and send messages in screen-free mode. The experiential response of the users shed light on the importance of bypassing the phone and accessing rapidly controllable messages at their fingertips, while preserving privacy and accuracy with respect to speech or screen-based input. We discuss how non-visual access to proactive, contextual messaging can support the blind in a variety of daily scenarios.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis",
              "institution": "Indiana University Purdue University Indianapolis",
              "dsl": "School of informatics"
            }
          ],
          "personId": 57816
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis",
              "institution": "Indiana University Purdue University Indianapolis",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57814
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "Indianapolis",
              "institution": "Indiana University Purdue University Indianapolis",
              "dsl": "School of Informatics"
            }
          ],
          "personId": 57730
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450697"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=qYmBEM0-CLA",
          "title": "TextFlow: Screenless Access to Non-Visual Smart Messaging",
          "duration": 570,
          "type": "video"
        }
      },
      "sessionIds": [
        58117
      ],
      "eventIds": []
    },
    {
      "id": 57969,
      "typeId": 11793,
      "title": "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "This paper contributes to the growing literature in empirical evaluation of explainable AI (XAI) methods by presenting a comparison on the effects of a set of established XAI methods in AI-assisted decision making. Specifically, based on our review of previous literature, we highlight three desirable properties that ideal AI explanations should satisfy---improve people's understanding of the AI model, help people recognize the model uncertainty, and support people's calibrated trust in the model. Through a randomized human-subject experiment, we evaluate whether four types of common model-agnostic explainable AI methods satisfy these properties on two types of decision making tasks where people perceive themselves as having different levels of prior knowledge in (i.e., recidivism prediction and forest cover prediction). Our results show that the effects of AI explanations are largely different on decision making tasks where people have varying levels of domain knowledge in, and many AI explanations do not satisfy any of the desirable properties for tasks that people have little prior knowledge in. Further, for decision making tasks that people are more knowledgeable, feature contribution explanation is shown to satisfy more desiderata of AI explanations, while the explanation that is considered to resemble how human explain decisions (i.e., counterfactual explanation) does not seem to improve calibrated trust. We conclude by discussing the implications of our study for improving the design of XAI methods to better support human decision making.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 57692
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Indiana",
              "city": "West Lafayette",
              "institution": "Purdue University",
              "dsl": ""
            }
          ],
          "personId": 57754
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450650"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=w2ikdgNlto0",
          "title": "Are Explanations Helpful? A Comparative Study of the Effects of Explanations in AI-Assisted Decision-Making",
          "duration": 556,
          "type": "video"
        }
      },
      "sessionIds": [
        58111
      ],
      "eventIds": []
    },
    {
      "id": 57970,
      "typeId": 11793,
      "title": "Forsense: Accelerating online research through sensemaking integration and machine research support.",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Online research is a frequent and important activity people perform on the Internet, yet current support for this task is basic, fragmented and not well integrated into web browser experiences.\r\n\r\nGuided by sensemaking theory, we present ForSense, a browser extension for accelerating people’s online research experience. \r\nThe two primary sources of novelty of ForSense are the integration of multiple stages of online research and providing machine assistance to the user by leveraging recent advances in neural-driven machine reading.\r\n\r\nWe use ForSense as a design probe to explore (1) the benefits of integrating multiple stages of online research, (2) the opportunities to accelerate online research using current advances in machine reading, and (3) the opportunities to support online research tasks under the presence of imprecise machine suggestions. In our study, we observe people performing online research tasks, and see that they benefit from \\app's integration and machine support for online research. \r\n\r\nFrom our study, we derive and share key design recommendations in designing and supporting imprecise machine assistance for research tasks.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Pennsylvania",
              "city": "Pittsburgh",
              "institution": "Carnegie Mellon University",
              "dsl": "Human Computer Interaction Institute"
            }
          ],
          "personId": 57862
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 57902
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 57859
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "REDMOND",
              "institution": "Microsoft Corporation",
              "dsl": ""
            }
          ],
          "personId": 57833
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Washington",
              "city": "Redmond",
              "institution": "Microsoft Research",
              "dsl": ""
            }
          ],
          "personId": 57744
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450649"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=oi2bJLAlZNQ",
          "title": "Forsense: Accelerating online research through sensemaking integration and machine research support.",
          "duration": 620,
          "type": "video"
        }
      },
      "sessionIds": [
        58120
      ],
      "eventIds": []
    },
    {
      "id": 57971,
      "typeId": 11793,
      "title": "The Sound Sketchpad: Expressively Combining Large and Diverse Audio Collections",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Software tools for media production have largely been adapted from physical media paradigms, offering blank canvases upon which to import, combine, and process content. In music production, this increasingly involves meticulous manual assembly of audio clips often carefully curated from diverse sources. As collections of audio content scale upwards in sample size, diversity, and number, creative projects require exponentially more time, effort, and attention to effectively shape them. New tools must find new ways to contend with this abundance of content. We propose the Sound Sketchpad, an algorithm-in-the-loop audio-graphical system and interface for combining sounds from a database into new music. It allows a user to sketch broad musical ideas by making sound, and then interactively modify and refine the resulting composition by drawing visual paths. We discuss the design, implementation, and advantages of this approach.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "Massachusetts Institute of Technology",
              "dsl": "MIT Media Lab"
            }
          ],
          "personId": 57829
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450688"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=NirGAucDxLc",
          "title": "The Sound Sketchpad: Expressively Combining Large and Diverse Audio Collections",
          "duration": 266,
          "type": "video"
        }
      },
      "sessionIds": [
        58115
      ],
      "eventIds": []
    },
    {
      "id": 57972,
      "typeId": 11793,
      "title": "XAlgo: a Design Probe of Explaining Algorithms' Internal States via Question-Answering",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Algorithms often appear as 'black boxes' to non-expert users. While prior work focuses on explainable representations and expert-oriented exploration, we propose and study an interactive approach using question answering to explain deterministic algorithms to non-expert users who need to understand the algorithms' internal states (e.g., students learning algorithms, operators monitoring robots, admins troubleshooting network routing). We construct XAlgo---a formal model that first classifies the type of question based on a taxonomy and generates an answer based on a set of rules that extract information from representations of an algorithm's internal states, e.g., the pseudocode. A design probe in an algorithm learning scenario with 18 participants (9 for a Wizard-of-Oz XAlgo and 9 as a control group) reports findings and design implications based on what kinds of questions people ask, how well XAlgo responds, and what remain as challenges to bridge users' gulf of understanding algorithms.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "HCI Research, UCLA",
              "dsl": ""
            }
          ],
          "personId": 57703
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 57805
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 57718
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "Los Angeles",
              "institution": "UCLA",
              "dsl": "HCI Research"
            }
          ],
          "personId": 57801
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450676"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Qj6da-CNMEc",
          "title": "XAlgo: a Design Probe of Explaining Algorithms' Internal States via Question-Answering",
          "duration": 730,
          "type": "video"
        }
      },
      "sessionIds": [
        58111
      ],
      "eventIds": []
    },
    {
      "id": 57973,
      "typeId": 11793,
      "title": "Facilitating Knowledge Sharing from Domain Experts to Data Scientists for Building NLP Models",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Data scientists face a steep learning curve in understanding a new domain for which they want to build machine learning (ML) models. While input from domain experts could offer valuable help, such input is often limited, expensive, and generally not in a form readily consumable by a model development pipeline. In this paper, we propose Ziva, an interfaces framework to guide domain experts in sharing essential domain knowledge to data scientists for building NLP models. With Ziva, experts are able to distill and share their domain knowledge using domain concepts extractors and five types of label justification over a representative data sample. Ziva is especially useful in cold-start situations (no training data available), and lowers the communication barrier between domain experts and data scientists. The design of Ziva is informed by preliminary interviews with data scientists and ML engineers, in order to understand current practices of knowledge sharing from domain experts for ML development projects. \r\n\r\nTo assess our design, we run a mix-method case-study to evaluate how Ziva can automatically facilitate interaction of domain experts and data scientists. Our results highlight that (1) domain experts are able to use Ziva provide rich domain knowledge, while maintaining low mental load and stress levels;  and (2) data scientists find Ziva's output helpful for learning essential information about the domain, offering scalability of information, and lowering the burden on domain experts to share knowledge directly. We conclude this work by experimenting with building  NLP models using the domain-knowledge concepts and justifications output by our case study.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Massachusetts",
              "city": "Cambridge",
              "institution": "MIT",
              "dsl": "CSAIL"
            }
          ],
          "personId": 57863
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Michigan",
              "city": "Ann Arbor",
              "institution": "University of Michigan",
              "dsl": "School of Information"
            }
          ],
          "personId": 57736
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "IBM research",
              "dsl": ""
            }
          ],
          "personId": 57750
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57851
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "New York",
              "city": "Yorktown Heights",
              "institution": "IBM Research",
              "dsl": ""
            }
          ],
          "personId": 57890
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "California",
              "city": "San Jose",
              "institution": "IBM Research",
              "dsl": "Almaden Research Lab"
            }
          ],
          "personId": 57920
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450637"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=nnaHbZeko-E",
          "title": "Facilitating Knowledge Sharing from Domain Experts to Data Scientists for Building NLP Models",
          "duration": 584,
          "type": "video"
        }
      },
      "sessionIds": [
        58120
      ],
      "eventIds": []
    },
    {
      "id": 57974,
      "typeId": 11793,
      "title": "User Preferences for Automated Curation of Snackable Content",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "As the volume of  content and the connectivity of social media have grown, snackable content has increasingly become an enjoyable and engaging way to share content. Snackable content is a shortened form of original content focusing on a single theme or motif for entertainment and quick understanding of a video moment.   For content owners with a large library of long-form content (movies, television series, documentaries, etc.), one challenge in accommodating snackable content in social media uses is the correct identification and cutting of interesting regions.  Related problems have been studied for algorithmic discovery of content for movie trailers, short-duration meme content, and medium duration news stories, but none of these approaches included user preferences as explicit drivers for cuts.  This paper analyzes both human and automatic methods for creating snackable clips across different categories of content with two comprehensive user studies.  Contrary to initial expectations, findings amongst the surveyed population indicate a preference for slightly longer snackable clips (60-90 seconds) and those that began or ended with a human character.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            }
          ],
          "personId": 57885
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "AT&T Chief Data Office",
              "dsl": "Data Science and AI Research"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "AT&T",
              "dsl": "Data Science and AI Research"
            }
          ],
          "personId": 57783
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "Austin",
              "institution": "AT&T",
              "dsl": "Design Technology"
            }
          ],
          "personId": 57820
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450690"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=0AV8Dlll3ps",
          "title": "User Preferences for Automated Curation of Snackable Content full strip notes",
          "duration": 313,
          "type": "video"
        }
      },
      "sessionIds": [
        58122
      ],
      "eventIds": []
    },
    {
      "id": 57975,
      "typeId": 11793,
      "title": "GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "People spend an enormous amount of time and effort looking for lost objects. Various computational systems have been developed to help remind people of the location of lost objects by providing information on their locations. However, prior systems for assisting people in finding objects require users to register the target objects in advance.\r\nThis requirement imposes a cumbersome burden on the users, and the system cannot help remind them of unexpectedly lost objects. We propose a registration-free wearable camera based system for assisting people in finding an arbitrary number of objects based on two key features: automatic discovery of hand-held objects and image-based candidate selection. Given a video taken from a wearable camera, our system automatically detects and groups hand-held objects to form a visual timeline of the objects. Users can retrieve the last appearance of the object by browsing the timeline through a smartphone app. We conducted a user study to investigate how users benefit from using our system. We confirmed improved accuracy and reduced mental load regarding the object search task by providing clear visual cues on object locations.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57763
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57911
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Kanagawa",
              "institution": "FUJITSU LABORATORIES LTD.",
              "dsl": ""
            }
          ],
          "personId": 57832
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "Softbank Corp.",
              "dsl": ""
            }
          ],
          "personId": 57723
        },
        {
          "affiliations": [
            {
              "country": "Japan",
              "state": "",
              "city": "Tokyo",
              "institution": "The University of Tokyo",
              "dsl": ""
            }
          ],
          "personId": 57861
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450664"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=munisKFzzx0",
          "title": "GO-Finder: A Registration-Free Wearable System for Assisting Users in Finding Lost Objects via Hand-Held Object Discovery",
          "duration": 600,
          "type": "video"
        }
      },
      "sessionIds": [
        58123
      ],
      "eventIds": []
    },
    {
      "id": 57976,
      "typeId": 11793,
      "title": "The Effect of Surrounding Scenery Complexity on the Transfer of Control Time in Highly Automated Driving",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "One challenge in highly automated driving is the safe transfer of the control (ToC). A safe ToC requires estimating the take-over time depending on the driver's state in different environmental conditions, to adapt timing and design of the ToC request. We introduce the environmental complexity as one factor that affects the ToC time. In a driving simulator experiment (N=12), the participants drove in five scenes having different environmental complexities (i.e. density and height of the background objects) with and without a secondary task. The results revealed that the ToC time is proportional to the environmental complexity. Thus, in the same driving task and the same traffic, an increasing environmental complexity yields higher ToC times in both conditions with and without secondary task. Our model of environmental complexity is a first step towards a better prediction of ToC times in highly automated driving to measure the complexity of real environments.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": ""
            }
          ],
          "personId": 57819
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Stuttgart",
              "institution": "P3 Group GmbH",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": ""
            }
          ],
          "personId": 57748
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": ""
            }
          ],
          "personId": 57916
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Saarland Informatics Campus",
              "institution": "German Research Center for Artificial Intelligence (DFKI)",
              "dsl": ""
            }
          ],
          "personId": 57758
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450677"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=-6qFfJsMv4M",
          "title": "The Effect of Surrounding Scenery Complexity on the Transfer of Control Time in Highly Automated Driving",
          "duration": 304,
          "type": "video"
        }
      },
      "sessionIds": [
        58121
      ],
      "eventIds": []
    },
    {
      "id": 57977,
      "typeId": 11793,
      "title": "Auto-Icon: An Automated Code Generation Tool for Icon Designs Assisting in UI Development",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Approximately 50% of development resources are devoted to UI development tasks [62]. Occupied a large proportion of development resources, developing icons can be a time-consuming task, because developers need to consider not only effective implementation methods but also easy-to-understand descriptions. In this study, we define 100 icon classes through an iterative open coding for the existing icon design sharing website. Based on a deep learning model and computer vision methods, we propose an approach to automatically convert icon images to fonts with descriptive labels, thereby reducing the laborious manual effort for developers and facilitating UI development. We quantitatively evaluate the quality of our method in the real world UI development environment and demonstrate that our method offers developers accurate, efficient, readable, and usable code for icon designs, in terms of saving 65.2% developing time.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 57873
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "VIC",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 57943
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Alibaba Group",
              "dsl": ""
            }
          ],
          "personId": 57716
        },
        {
          "affiliations": [
            {
              "country": "Australia",
              "state": "Victoria",
              "city": "Melbourne",
              "institution": "Monash University",
              "dsl": "Faculty of Information Technology"
            }
          ],
          "personId": 57830
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Alibaba Group",
              "dsl": ""
            }
          ],
          "personId": 57807
        },
        {
          "affiliations": [
            {
              "country": "China",
              "state": "Zhejiang",
              "city": "Hangzhou",
              "institution": "Alibaba Group",
              "dsl": ""
            }
          ],
          "personId": 57923
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450671"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=Xgzk-T_CDuk",
          "title": "Auto-Icon: An Automated Code Generation Tool for Icon Designs Assisting In UI Development",
          "duration": 471,
          "type": "video"
        }
      },
      "sessionIds": [
        58114
      ],
      "eventIds": []
    },
    {
      "id": 57978,
      "typeId": 11793,
      "title": "DeepSI: Interactive Deep Learning for Semantic Interaction",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "In this paper, we design novel interactive deep learning methods to improve semantic interactions in visual analytics applications.\r\nThe ability of semantic interaction to infer analysts' precise intents during sensemaking is dependent on the quality of the underlying data representation.\r\nWe propose the DeepSI-finetune framework that integrates deep learning into the human-in-the-loop interactive sensemaking pipeline, with two important properties.\r\nFirst, deep learning extracts meaningful representations from raw data, which improves semantic interaction inference. \r\nSecond, semantic interactions are exploited to fine tune the deep learning representations, which then further improves semantic interaction inference. This feedback loop between human interaction and deep learning enables efficient learning of user- and task-specific representations. \r\nTo evaluate the advantage of embedding the deep learning within the semantic interaction loop, we compare DeepSI-finetune against a more basic use of deep learning as only a feature extractor pre-processed outside of the interactive loop.\r\nResults of two complementary studies, a human-centered qualitative case study and an algorithm-centered simulation-based quantitative experiment, show that  DeepSI-finetune more accurately captures users' complex mental models with fewer interactions.\r\n",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57932
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Virginia",
              "city": "Blacksburg",
              "institution": "Virginia Tech",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57728
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450670"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=jhyRnajOskw",
          "title": "DeepSI: Interactive Deep Learning for Semantic Interaction",
          "duration": 580,
          "type": "video"
        }
      },
      "sessionIds": [
        58117
      ],
      "eventIds": []
    },
    {
      "id": 57979,
      "typeId": 11793,
      "title": "Decision rule elicitation for domain adaptation",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. We show in simulated and real-user studies that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert's knowledge to the AI model. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Helsinki Institute for Information Technology HIIT, Department of Computer Science"
            }
          ],
          "personId": 57905
        },
        {
          "affiliations": [
            {
              "country": "Finland",
              "state": "",
              "city": "Espoo",
              "institution": "Aalto University",
              "dsl": "Helsinki Institute for Information Technology HIIT, Department of Computer Science"
            },
            {
              "country": "United Kingdom",
              "state": "",
              "city": "Manchester",
              "institution": "University of Manchester",
              "dsl": "Department of Computer Science"
            }
          ],
          "personId": 57699
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450682"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=gvL7r6c_7nI",
          "title": "Decision rule elicitation for domain adaptation",
          "duration": 379,
          "type": "video"
        }
      },
      "sessionIds": [
        58122
      ],
      "eventIds": []
    },
    {
      "id": 57980,
      "typeId": 11793,
      "title": "Interactive music genre exploration with visualization and mood control",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Recommender systems can be used to help users discover novel items and explore new tastes, for example in music genre exploration. However, little work has studied how to improve users' understandability and acceptance of novel items. In this paper, we investigate how visualization as well as mood control affects the perceived control, informativeness and understandability of the recommender, to improve the helpfulness for new music genre exploration. Specifically, we compare a bar chart visualization as used in earlier work to a contour plot which allows users to compare their musical preferences with the recommended tracks as well as the new genre. In addition, mood control is implemented with two sliders to set a preferred mood on energy and valence features (that correlate with psychological mood dimensions). The user study was conducted online, in which mood control was manipulated between subjects, while the proposed contour plot was compared with bar charts within subjects. During the study (N=102), we measured users' subjective perceptions and experiences as well as their interaction data. Our results show that a contour plot is perceived more helpful for users to explore new genres than a bar chart because it is perceived to be more informative and understandable. Users spent significantly more time in the contour plot condition and used the mood control more than in the bar chart condition. Mood control only significantly influences perceived helpfulness when paired with the contour plot. Overall, our results show that contour plot type of visualization combined with mood control is the most helpful for genre exploration.  ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Jheronimus Academy of Data Science",
              "institution": " ’s-Hertogenbosch",
              "dsl": ""
            }
          ],
          "personId": 57802
        },
        {
          "affiliations": [
            {
              "country": "Netherlands",
              "state": "",
              "city": "Eindhoven",
              "institution": "Eindhoven University of Technology",
              "dsl": "Human-Technology Interaction Group"
            },
            {
              "country": "Netherlands",
              "state": "",
              "city": "'s-Hertogenbosch",
              "institution": "Jheronimus Academy of Data Science",
              "dsl": ""
            }
          ],
          "personId": 57892
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450700"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=MHjnXpLhCnc",
          "title": "Interactive Music Genre Exploration with Visualization and Mood Control",
          "duration": 596,
          "type": "video"
        }
      },
      "sessionIds": [
        58117
      ],
      "eventIds": []
    },
    {
      "id": 57981,
      "typeId": 11793,
      "title": "Crowdsourcing Scholarly Discourse Annotations",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "The number of scholarly publications grows steadily every year and it becomes harder to find, assess and compare scholarly knowledge effectively. Scholarly knowledge graphs have the potential to address these challenges. However, creating such graphs remains a complex task. We propose a method to crowdsource structured scholarly knowledge from paper authors with a web-based user interface supported by artificial intelligence. The interface enables authors to select key sentences for annotation. It integrates multiple machine learning algorithms to assist authors during the annotation, including class recommendation and key sentence highlighting. We envision that the interface is integrated in paper submission processes for which we define three main task requirements: The task has to be (1) straightforward (2) time efficient (3) well-defined. We evaluated the interface with a user study in which participants were assigned the task to annotate one of their own articles. With the resulting data, we determined whether the participants were successfully able to perform the task. Furthermore, we evaluated the interface's usability and the participant's attitude towards the interface with a survey. The results suggest that sentence annotation is a feasible task for researchers and that they do not object to annotate their articles during the submission process. ",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "L3S Research Center",
              "dsl": ""
            },
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "TIB - German National Library of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 57817
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "TIB - German National Library of Science and Technology",
              "dsl": ""
            }
          ],
          "personId": 57812
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Hannover",
              "institution": "Technische Informationsbibliothek",
              "dsl": ""
            }
          ],
          "personId": 57908
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450685"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=fuTLUHp5_uM",
          "title": "Crowdsourcing Scholarly Discourse Annotations",
          "duration": 586,
          "type": "video"
        }
      },
      "sessionIds": [
        58108
      ],
      "eventIds": []
    },
    {
      "id": 57982,
      "typeId": 11793,
      "title": "Comparing Apples and Oranges: Human and Computer Clustered Affinity Diagrams Under the Microscope",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Affinity diagramming is a crucial yet time-consuming part of user research in human-centered design. To support designers in this process, as a first contribution, we explored seven models for pre-clustering affinity notes and suggest fastText as most appropriate. Since affinity diagrams are not deterministic, there is no established measure to assess their quality. \r\nOur second contribution is, therefore, a thorough examination of the potential of fastText-clusters for design teams regarding technical, psychological and performance-related measures. Compared to reference human built affinity diagrams, the fastText-clusters resulted in an overlap index of M = .694 (SD = .034). A study with four design teams clustering small sets (112 notes) of pre-clustered or randomized affinity notes indicated an increased discussion overhead caused by algorithmic support that led to a decrease in both, efficiency and quality. \r\nAs a third contribution, we report qualitative data from the instances, where algorithmic support failed human expectations. We conclude that more research on the appropriate time and manner of pre-clustered data presentation is required to harness the full potential of algorithmic support while preserving the spirit of affinity diagramming.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Karlsruhe",
              "institution": "Karlsruhe Institute of Technology",
              "dsl": ""
            }
          ],
          "personId": 57740
        },
        {
          "affiliations": [
            {
              "country": "Germany",
              "state": "",
              "city": "Würzburg ",
              "institution": "Julius-Maximilians-Universität ",
              "dsl": ""
            }
          ],
          "personId": 57738
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450674"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=eVafi1dEexg",
          "title": "Comparing Apples and Oranges: Human and Computer Clustered Affinity Diagrams Under the Microscope",
          "duration": 505,
          "type": "video"
        }
      },
      "sessionIds": [
        58113
      ],
      "eventIds": []
    },
    {
      "id": 57983,
      "typeId": 11793,
      "title": "An Activity Recognition System for Taking Medicine Using In-The-Wild Data to Promote Medication Adherence",
      "trackId": 11171,
      "tags": [],
      "keywords": [],
      "abstract": "Nearly half of people prescribed medication to treat chronic or short-term conditions do not take their medicine as prescribed. This leads to worse treatment outcomes, higher hospital admission rates, increased healthcare costs, and increased morbidity and mortality rates. While some instances of medication non-adherence are a result of problems with the treatment plan or barriers caused by the health care provider, many are instances caused by patient-related factors such as forgetting, running out of medication, and not understanding the required dosages. This presents a clear need for patient-centered systems that can reliably increase medication adherence. To that end, in this work we describe an activity recognition system capable of recognizing when individuals take medication in an unconstrained, real-world environment. Our methodology uses a modified version of the Bagging ensemble method to suit unbalanced data and a classifier trained on the prediction probabilities of the Bagging classifier to identify when individuals took medication during a real-time full-day study. Using this methodology we are able to recognize when individuals took medication with an F-measure of 0.77. Our system is a first step towards developing personal health interfaces that are capable of providing personalized medication adherence interventions.",
      "authors": [
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 57886
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": ""
            }
          ],
          "personId": 57937
        },
        {
          "affiliations": [
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            },
            {
              "country": "United States",
              "state": "Texas",
              "city": "College Station",
              "institution": "Texas A&M University",
              "dsl": "Sketch Recognition Lab"
            }
          ],
          "personId": 57694
        }
      ],
      "addons": {
        "doi": {
          "type": "doiLink",
          "url": "https://doi.org/10.1145/3397481.3450673"
        },
        "Talk": {
          "url": "https://www.youtube.com/watch?v=BJ5jlYM_vBI",
          "title": "An Activity Recognition System for Taking Medicine Using In-The-Wild Data to Promote Medication Adherence",
          "duration": 530,
          "type": "video"
        }
      },
      "sessionIds": [
        58116
      ],
      "eventIds": []
    }
  ],
  "people": [
    {
      "id": 57856,
      "firstName": "Pieter",
      "lastName": "Wolfert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57857,
      "firstName": "Eytan",
      "lastName": "Adar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57858,
      "firstName": "Simon",
      "lastName": "Demediuk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57859,
      "firstName": "Jina",
      "lastName": "Suh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57860,
      "firstName": "Malin",
      "lastName": "Eiband",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57861,
      "firstName": "Yoichi",
      "lastName": "Sato",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57862,
      "firstName": "Napol",
      "lastName": "Rachatasumrit",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57863,
      "firstName": "Soya",
      "lastName": "Park",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57864,
      "firstName": "Benjamin",
      "lastName": "Steeper",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57865,
      "firstName": "Andrew",
      "lastName": "Gasiorowski",
      "middleInitial": "Brian",
      "affiliations": []
    },
    {
      "id": 57866,
      "firstName": "Francis",
      "lastName": "Quek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57867,
      "firstName": "Shweta",
      "lastName": "Narkar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57868,
      "firstName": "Almar",
      "lastName": "van der Stappen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57869,
      "firstName": "Trikay",
      "lastName": "Nalamada",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57870,
      "firstName": "Cheng",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57871,
      "firstName": "Maurice",
      "lastName": "Rekrut",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57872,
      "firstName": "Maxwell",
      "lastName": "Szymanski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57873,
      "firstName": "Sidong",
      "lastName": "Feng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57874,
      "firstName": "Daniel",
      "lastName": "Buschek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57875,
      "firstName": "Yiting",
      "lastName": "Shao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57876,
      "firstName": "Boogyoung",
      "lastName": "Choi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57877,
      "firstName": "Justin",
      "lastName": "Weisz",
      "middleInitial": "D.",
      "affiliations": []
    },
    {
      "id": 57878,
      "firstName": "Tsung-Ping",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57879,
      "firstName": "Zhefan",
      "lastName": "Ye",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57880,
      "firstName": "Tobias",
      "lastName": "Schneider",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57881,
      "firstName": "Wei",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57882,
      "firstName": "Blake",
      "lastName": "Williford",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57883,
      "firstName": "Aman",
      "lastName": "Mishra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57884,
      "firstName": "Keisuke",
      "lastName": "Nakashima",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57885,
      "firstName": "Allyson",
      "lastName": "King",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57886,
      "firstName": "Josh",
      "lastName": "Cherian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57887,
      "firstName": "Heinrich",
      "lastName": "Hussmann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57888,
      "firstName": "Steve",
      "lastName": "Love",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57889,
      "firstName": "Michelle",
      "lastName": "Brachman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57890,
      "firstName": "David",
      "lastName": "Piorkowski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57891,
      "firstName": "Oluseyi",
      "lastName": "Olarewaju",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57892,
      "firstName": "Martijn",
      "lastName": "Willemsen",
      "middleInitial": "C.",
      "affiliations": []
    },
    {
      "id": 57893,
      "firstName": "Stephen",
      "lastName": "Hart",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57894,
      "firstName": "Kais",
      "lastName": "Riani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57895,
      "firstName": "Yuma",
      "lastName": "Fujioka",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57896,
      "firstName": "Stephanie",
      "lastName": "Houde",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57897,
      "firstName": "Julie",
      "lastName": "Linsey",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57898,
      "firstName": "Antti",
      "lastName": "Oulasvirta",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57899,
      "firstName": "Kristina",
      "lastName": "Brimijoin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57900,
      "firstName": "Michael",
      "lastName": "Muller",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57901,
      "firstName": "Ingrid",
      "lastName": "Lange",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57902,
      "firstName": "Gonzalo",
      "lastName": "Ramos",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57903,
      "firstName": "Eduardo",
      "lastName": "Veas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57904,
      "firstName": "Carlos",
      "lastName": "de la Torre-Ortiz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57905,
      "firstName": "Alexander",
      "lastName": "Nikitin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57906,
      "firstName": "Elisa",
      "lastName": "Lecluse",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57907,
      "firstName": "Michihiko",
      "lastName": "Ueno",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57908,
      "firstName": "Sören",
      "lastName": "Auer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57909,
      "firstName": "Yuki",
      "lastName": "Koyama",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57910,
      "firstName": "Aabhas",
      "lastName": "Sharma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57911,
      "firstName": "Takumi",
      "lastName": "Nishiyasu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57912,
      "firstName": "James",
      "lastName": "Johnson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57913,
      "firstName": "Joshua",
      "lastName": "Howell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57914,
      "firstName": "Pranav",
      "lastName": "Goel",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57915,
      "firstName": "Yucheng",
      "lastName": "Jin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57916,
      "firstName": "Florian",
      "lastName": "Daiber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57917,
      "firstName": "Catherine",
      "lastName": "Finegan-Dollak",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57918,
      "firstName": "Fernando",
      "lastName": "Martinez",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57919,
      "firstName": "Jingzhou",
      "lastName": "Du",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57920,
      "firstName": "Marina",
      "lastName": "Danilevsky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57921,
      "firstName": "Shih-Wei",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57922,
      "firstName": "Martijn",
      "lastName": "Millecamp",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57923,
      "firstName": "Yankun",
      "lastName": "Zhen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57924,
      "firstName": "Jiawei",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57925,
      "firstName": "Thanet",
      "lastName": "Markchom",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57926,
      "firstName": "Fabio",
      "lastName": "Vitali",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57927,
      "firstName": "Athanasios",
      "lastName": "Kokkinakis",
      "middleInitial": "Vasileios",
      "affiliations": []
    },
    {
      "id": 57928,
      "firstName": "Balaji Vasan",
      "lastName": "Srinivasan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57929,
      "firstName": "Zhujun",
      "lastName": "Fang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57930,
      "firstName": "Adrian",
      "lastName": "Krüger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57931,
      "firstName": "Guillermo",
      "lastName": "Reyes",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57932,
      "firstName": "Yail",
      "lastName": "Bian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57933,
      "firstName": "Mazen",
      "lastName": "Salous",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57934,
      "firstName": "Anders",
      "lastName": "Drachen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57935,
      "firstName": "Matthew",
      "lastName": "Runyon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57936,
      "firstName": "Liwei",
      "lastName": "Jiang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57937,
      "firstName": "Samantha",
      "lastName": "Ray",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57938,
      "firstName": "Yan",
      "lastName": "Sun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57939,
      "firstName": "Wayne",
      "lastName": "Fu",
      "middleInitial": "T.",
      "affiliations": []
    },
    {
      "id": 57940,
      "firstName": "Mathias",
      "lastName": "Funk",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57941,
      "firstName": "Evelyn",
      "lastName": "Duesterwald",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57942,
      "firstName": "Ee Peng",
      "lastName": "Lim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57943,
      "firstName": "Suyu",
      "lastName": "Ma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57944,
      "firstName": "Seth",
      "lastName": "Polsley",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57945,
      "firstName": "Andreas",
      "lastName": "Butz",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57946,
      "firstName": "Jonathan",
      "lastName": "Bünemann",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57947,
      "firstName": "Lei",
      "lastName": "He",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57948,
      "firstName": "Mahsan",
      "lastName": "Nourani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57691,
      "firstName": "Dustin",
      "lastName": "Torres",
      "middleInitial": "Ramsey",
      "affiliations": []
    },
    {
      "id": 57692,
      "firstName": "Xinru",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57693,
      "firstName": "Hariharan",
      "lastName": "Subramonyam",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57694,
      "firstName": "Tracy",
      "lastName": "Hammond",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57695,
      "firstName": "Dana",
      "lastName": "Kulic",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57696,
      "firstName": "Tahrima",
      "lastName": "Rahman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57697,
      "firstName": "Felicitas",
      "lastName": "Buchner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57698,
      "firstName": "Radoslaw",
      "lastName": "Niewiadomski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57699,
      "firstName": "Samuel",
      "lastName": "Kaski",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57700,
      "firstName": "Kartik",
      "lastName": "Talamadupula",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57701,
      "firstName": "Jeeeun",
      "lastName": "Kim",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57702,
      "firstName": "Gary",
      "lastName": "Ang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57703,
      "firstName": "Juan",
      "lastName": "Rebanal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57704,
      "firstName": "Ansgar",
      "lastName": "Gerlicher",
      "middleInitial": "R.S.",
      "affiliations": []
    },
    {
      "id": 57705,
      "firstName": "Zoe",
      "lastName": "Tong",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57706,
      "firstName": "Ben",
      "lastName": "Kirman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57707,
      "firstName": "Kashyap",
      "lastName": "Todi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57708,
      "firstName": "Joseph",
      "lastName": "LaViola",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57709,
      "firstName": "Kun",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57710,
      "firstName": "Li",
      "lastName": "Su",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57711,
      "firstName": "Melanie",
      "lastName": "Heck",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57712,
      "firstName": "Sin-Ning",
      "lastName": "Liu",
      "middleInitial": "Cindy",
      "affiliations": []
    },
    {
      "id": 57713,
      "firstName": "Qianyao",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57714,
      "firstName": "Odest",
      "lastName": "Jenkins",
      "middleInitial": "Chadwicke",
      "affiliations": []
    },
    {
      "id": 57715,
      "firstName": "Yunfeng",
      "lastName": "Zhang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57716,
      "firstName": "Jinzhong",
      "lastName": "Yu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57717,
      "firstName": "Narendra Nath",
      "lastName": "Joshi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57718,
      "firstName": "Yuqi",
      "lastName": "Tang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57719,
      "firstName": "Mohamed",
      "lastName": "Abouelenien",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57720,
      "firstName": "Ketan",
      "lastName": "Thakare",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57721,
      "firstName": "Surjya",
      "lastName": "Ghosh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57722,
      "firstName": "Justus",
      "lastName": "Robertson",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57723,
      "firstName": "Moe",
      "lastName": "Matsuki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57724,
      "firstName": "Gustav Eje",
      "lastName": "Henter",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57725,
      "firstName": "Éric",
      "lastName": "Jacopin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57726,
      "firstName": "Eric",
      "lastName": "Ragan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57727,
      "firstName": "Patrik",
      "lastName": "Jonell",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57728,
      "firstName": "Chris",
      "lastName": "North",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57729,
      "firstName": "Franklin Mingzhe",
      "lastName": "Li",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57730,
      "firstName": "Davide",
      "lastName": "Bolchini",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57731,
      "firstName": "Donald",
      "lastName": "Honeycutt",
      "middleInitial": "R",
      "affiliations": []
    },
    {
      "id": 57732,
      "firstName": "Masataka",
      "lastName": "Goto",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57733,
      "firstName": "Arpit",
      "lastName": "Narechania",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57734,
      "firstName": "Aurelien",
      "lastName": "Nioche",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57735,
      "firstName": "Katrien",
      "lastName": "Verbert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57736,
      "firstName": "April",
      "lastName": "Wang",
      "middleInitial": "Yi",
      "affiliations": []
    },
    {
      "id": 57737,
      "firstName": "Jeremy",
      "lastName": "Chan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57738,
      "firstName": "Stephan",
      "lastName": "Huber",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57739,
      "firstName": "Stéphane",
      "lastName": "Cardon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57740,
      "firstName": "Parzival",
      "lastName": "Borlinghaus",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57741,
      "firstName": "Sina",
      "lastName": "Mohseni",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57742,
      "firstName": "Huizhi",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57743,
      "firstName": "Emma",
      "lastName": "Brunskill",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57744,
      "firstName": "Christopher",
      "lastName": "Meek",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57745,
      "firstName": "Liang",
      "lastName": "Qiu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57746,
      "firstName": "Hyo Jin",
      "lastName": "Do",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57747,
      "firstName": "Keerti",
      "lastName": "Harpavat",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57748,
      "firstName": "Baris",
      "lastName": "Cakar",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57749,
      "firstName": "Li",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57750,
      "firstName": "Ban",
      "lastName": "Kawas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57751,
      "firstName": "Mayank",
      "lastName": "Agarwal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57752,
      "firstName": "Francesco",
      "lastName": "Sovrano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57753,
      "firstName": "Martin",
      "lastName": "Lindvall",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57754,
      "firstName": "Ming",
      "lastName": "Yin",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57755,
      "firstName": "Sagarika",
      "lastName": "Patra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57756,
      "firstName": "Tiancheng",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57757,
      "firstName": "Rada",
      "lastName": "Mihalcea",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57758,
      "firstName": "Antonio",
      "lastName": "Krüger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57759,
      "firstName": "Mihai",
      "lastName": "Burzo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57760,
      "firstName": "Gaurav",
      "lastName": "Verma",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57761,
      "firstName": "Michalis",
      "lastName": "Papakostas",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57762,
      "firstName": "Tanja",
      "lastName": "Schneeberger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57763,
      "firstName": "Takuma",
      "lastName": "Yagi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57764,
      "firstName": "Aline",
      "lastName": "Hufschmitt",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57765,
      "firstName": "Felix",
      "lastName": "Putze",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57766,
      "firstName": "Naomi",
      "lastName": "Sauerwein",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57767,
      "firstName": "Bongshin",
      "lastName": "Lee",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57768,
      "firstName": "Feng",
      "lastName": "Tian",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57769,
      "firstName": "Jorge",
      "lastName": "Vilchis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57770,
      "firstName": "Chiradeep",
      "lastName": "Roy",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57771,
      "firstName": "Michael",
      "lastName": "Chromik",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57772,
      "firstName": "Shin’ichi",
      "lastName": "Satoh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57773,
      "firstName": "Zhiqiang",
      "lastName": "Sui",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57774,
      "firstName": "Adam",
      "lastName": "Fourney",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57775,
      "firstName": "Wanling",
      "lastName": "CAI",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57776,
      "firstName": "Jonathan",
      "lastName": "Hook",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57777,
      "firstName": "Yijun",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57778,
      "firstName": "Seon Hye",
      "lastName": "Yang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57779,
      "firstName": "Minori",
      "lastName": "Narita",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57780,
      "firstName": "Dezhan",
      "lastName": "Tu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57781,
      "firstName": "Kazunori",
      "lastName": "Komatani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57782,
      "firstName": "Nolwenn",
      "lastName": "Maudet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57783,
      "firstName": "Eric",
      "lastName": "Zavesky",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57784,
      "firstName": "Mehran",
      "lastName": "Maghoumi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57785,
      "firstName": "Glenn",
      "lastName": "Davis",
      "middleInitial": "M",
      "affiliations": []
    },
    {
      "id": 57786,
      "firstName": "Salma",
      "lastName": "Mandi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57787,
      "firstName": "Sherry",
      "lastName": "Ruan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57788,
      "firstName": "Patrick",
      "lastName": "Gebhard",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57789,
      "firstName": "Claes",
      "lastName": "Lundström",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57790,
      "firstName": "Walter",
      "lastName": "Lasecki",
      "middleInitial": "S.",
      "affiliations": []
    },
    {
      "id": 57791,
      "firstName": "Colleen",
      "lastName": "Seifert",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57792,
      "firstName": "Werner",
      "lastName": "Geyer",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57793,
      "firstName": "Tobias",
      "lastName": "Jungbluth",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57794,
      "firstName": "Bivas",
      "lastName": "Mitra",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57795,
      "firstName": "Lukas",
      "lastName": "Brückner",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57796,
      "firstName": "Yi-Hsin",
      "lastName": "Jen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57797,
      "firstName": "Mikio",
      "lastName": "Nakano",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57798,
      "firstName": "Alexandra",
      "lastName": "Alles",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57799,
      "firstName": "Taras",
      "lastName": "Kucherenko",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57800,
      "firstName": "Casey",
      "lastName": "Dugan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57801,
      "firstName": "Xiang 'Anthony'",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57802,
      "firstName": "Yu",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57803,
      "firstName": "Alexander",
      "lastName": "Berman",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57804,
      "firstName": "Alessandra",
      "lastName": "Sciutti",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57805,
      "firstName": "Jordan",
      "lastName": "Combitsis",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57806,
      "firstName": "Zahra",
      "lastName": "Ashktorab",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57807,
      "firstName": "TingTing",
      "lastName": "Zhou",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57808,
      "firstName": "Steven",
      "lastName": "Ross",
      "middleInitial": "I.",
      "affiliations": []
    },
    {
      "id": 57809,
      "firstName": "Yuan",
      "lastName": "Liang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57810,
      "firstName": "Michael",
      "lastName": "Desmond",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57811,
      "firstName": "Songlin",
      "lastName": "Xu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57812,
      "firstName": "Markus",
      "lastName": "Stocker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57813,
      "firstName": "Sabiha",
      "lastName": "Ghellal",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57814,
      "firstName": "Emanuele",
      "lastName": "Plebani",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57815,
      "firstName": "Jörg",
      "lastName": "Simon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57816,
      "firstName": "Pegah",
      "lastName": "Karimi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57817,
      "firstName": "Allard",
      "lastName": "Oelen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57818,
      "firstName": "Youngwoo",
      "lastName": "Yoon",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57819,
      "firstName": "Frederik",
      "lastName": "Wiehr",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57820,
      "firstName": "Michael",
      "lastName": "Gonzales",
      "middleInitial": "J",
      "affiliations": []
    },
    {
      "id": 57821,
      "firstName": "Eugene",
      "lastName": "Taranta",
      "middleInitial": "Matthew",
      "affiliations": []
    },
    {
      "id": 57822,
      "firstName": "Bryant",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57823,
      "firstName": "Marian",
      "lastName": "Ursu",
      "middleInitial": "F",
      "affiliations": []
    },
    {
      "id": 57824,
      "firstName": "Christian",
      "lastName": "Becker",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57825,
      "firstName": "Benjamin",
      "lastName": "Hoover",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57826,
      "firstName": "Janick",
      "lastName": "Edinger",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57827,
      "firstName": "John",
      "lastName": "Richards",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57828,
      "firstName": "Jonas",
      "lastName": "Löwgren",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57829,
      "firstName": "Nikhil",
      "lastName": "Singh",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57830,
      "firstName": "Chunyang",
      "lastName": "Chen",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57831,
      "firstName": "Zhiyuan",
      "lastName": "Liu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57832,
      "firstName": "Kunimasa",
      "lastName": "Kawasaki",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57833,
      "firstName": "Rachel",
      "lastName": "Ng",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57834,
      "firstName": "Heiko",
      "lastName": "Ludwig",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57835,
      "firstName": "Dakuo",
      "lastName": "Wang",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57836,
      "firstName": "Yi",
      "lastName": "Lu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57837,
      "firstName": "Vibhav",
      "lastName": "Gogate",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57838,
      "firstName": "Niraj",
      "lastName": "Dayama",
      "middleInitial": "Ramesh",
      "affiliations": []
    },
    {
      "id": 57839,
      "firstName": "Nyi Nyi",
      "lastName": "Htun",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57840,
      "firstName": "Takeo",
      "lastName": "Igarashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57841,
      "firstName": "Katsuhiko",
      "lastName": "Hayashi",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57842,
      "firstName": "Qian",
      "lastName": "Pan",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57843,
      "firstName": "Pearl",
      "lastName": "Pu",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57844,
      "firstName": "Ekaterina",
      "lastName": "Svikhnushina",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57845,
      "firstName": "Brian",
      "lastName": "Bailey",
      "middleInitial": "P",
      "affiliations": []
    },
    {
      "id": 57846,
      "firstName": "Pierre-Alexandre",
      "lastName": "Murena",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57847,
      "firstName": "Florian",
      "lastName": "Block",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57848,
      "firstName": "Pradipta",
      "lastName": "De",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57849,
      "firstName": "Jean",
      "lastName": "Song",
      "middleInitial": "Y",
      "affiliations": []
    },
    {
      "id": 57850,
      "firstName": "Manuel S.",
      "lastName": "Anglet",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57851,
      "firstName": "Q. Vera",
      "lastName": "Liao",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57852,
      "firstName": "James",
      "lastName": "Landay",
      "middleInitial": "A.",
      "affiliations": []
    },
    {
      "id": 57853,
      "firstName": "Jeremy E",
      "lastName": "Block",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57854,
      "firstName": "Nathalie",
      "lastName": "Baracaldo",
      "middleInitial": "",
      "affiliations": []
    },
    {
      "id": 57855,
      "firstName": "Simo",
      "lastName": "Santala",
      "middleInitial": "",
      "affiliations": []
    }
  ],
  "recognitions": [],
  "publicationInfo": {
    "hideLinksBeforeConference": false,
    "version": 20,
    "publicationStatus": "PUBLISHED",
    "isProgramEnabled": true,
    "isDraft": false,
    "isRegistrationEnabled": false,
    "publicationDate": "2021-04-29 08:56:43+00"
  }
}